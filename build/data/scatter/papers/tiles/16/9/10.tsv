id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
7397b5868948ba7708ee79670d33f92ea77084ab	hierarchical residue number systems with small moduli and simple converters	resztowy system liczbowy;arytmetyka cyfrowa;vlsi;digital arithmetic;residue number system;digital circuits;uklad cyfrowy	In this paper, a new class of Hierarchical Residue Number Systems (HRNSs) is proposed, where the numbers are represented as a set of residues modulo factors of 2 ± 1 and modulo 2 . The converters between the proposed HRNS and the positional binary number system can be built as 2-level structures using efficient circuits designed for the RNS (2 − 1, 2, 2 + 1). This approach allows using many small moduli in arithmetic channels without large conversion overhead. The advantages resulting from the use of the proposed HRNS depend on the possibility of factorisation of moduli 2 ± 1.	binary number;like button;mod (video gaming);modulo operation;monoid factorisation;overhead (computing);residue number system	Tadeusz Tomczak	2011	Applied Mathematics and Computer Science	10.2478/v10006-011-0013-2	arithmetic;residue number system;discrete mathematics;pure mathematics;mathematics;very-large-scale integration;digital electronics	Logic	16.099833626960827	43.65707348825245	47479
3a004f2164811092b3470eefca327ea09e31b7ed	connected components in o(\lg^3/2 |v|) parallel time for the crew pram	graph theory;connected components;crew pram;crew processors parallel time crew pram connected components undirected graph vertices edges;parallel algorithms computational complexity graph theory;crew processors;vertices;phase change random access memory concurrent computing parallel algorithms process design algorithm design and analysis educational institutions writing joining processes mathematics computer science;undirected graph;computational complexity;edges;parallel time;parallel algorithms	Computing the connected components of an undirected graph G=(V, E) on mod V mod =n vertices and mod E mod =m edges is addressed. An efficient and simple algorithm that runs in O(lg/sup 3/2/ n) time using n+m CREW processors is presented. >	connected component (graph theory)	Donald B. Johnson;Panagiotis Takis Metaxas	1991		10.1109/SFCS.1991.185437	edge;vertex;combinatorics;connected component;level structure;computer science;graph theory;theoretical computer science;pseudoforest;mathematics;distributed computing;parallel algorithm;graph;computational complexity theory;tree;algorithm;strength of a graph	HPC	14.712356156832112	33.21796846765707	47628
7b52c69aa2f439ca49841de52d6cd4fc5b2f6710	performance evaluation of a parallel algorithm for determining all optimal solutions of the 1d array partitioning problem		We address the parallelization of the determination of all optimal solutions (DAOS) for the 1D array partitioning problem, an easy combinatorial optimization problem that may be solved by dynamic programming. It turns out that the designed DAOS algorithm is a polyhedral algorithm (PA), i.e. a FOR-loop nest with affine loop bounds, whose iteration space that has to be scanned may be of exponential cardinality in the array size hence too time consuming. Thus, after a cardinality reducing procedure leading to an improved PA (IPA), we propose to parallelize the IPA through a specific approach. This latter first consists in a dependence analysis within the IPA leading to detect that all its loops are parallel. We then apply a loop interchange on the IPA in order to have a new IPA whose first loop has the largest number of parallel iterations. Another alternative consists in moving to the first position the loop whose number of iterations is the closest to the number of available processors. We detail an analysis of the theoretical performances of the derived parallel IPA by proposing several versions fitting the number of available processors and reducing the overhead due to processor synchronizations.	automatic parallelization;central processing unit;combinatorial optimization;dependence analysis;dynamic programming;for loop;iteration;loop interchange;mathematical optimization;optimization problem;overhead (computing);parallel algorithm;parallel computing;partition problem;performance evaluation;polyhedron;simulation;time complexity	Hajer Salhi;Bchira Ben Mabrouk;Zaher Mahjoub	2017	2017 IEEE/ACS 14th International Conference on Computer Systems and Applications (AICCSA)	10.1109/AICCSA.2017.179	cardinality;real-time computing;dependence analysis;computer science;combinatorial optimization;loop interchange;dynamic programming;parallel algorithm;affine transformation;algorithm;exponential function	HPC	10.116179349668869	32.94596094037366	47689
22b8dd25f08769097ae5c15016f554a76282e143	computing powers in parallel	powers of polynomials;10a30;general and miscellaneous mathematics computing and information science;mathematics;symbolic manipulation;efficiency;68c20;algebraic computing;12c05;circuit depth;functional programming;polynomials;finite field;powers of integers;programming 990230 mathematics mathematical models 1987 1989;computer calculations;supercomputers 1987 1989;parallel computer;functions;parallel processing	Fast parallel computations are presented for large powers modulo an element that has only small prime factors. They work for integers and polynomials over small finite fields.		Joachim von zur Gathen	1987	SIAM J. Comput.	10.1137/0216060	parallel processing;mathematical optimization;combinatorics;discrete mathematics;computer science;theoretical computer science;mathematics;efficiency;functional programming;finite field;function;algorithm;polynomial;algebra	Theory	15.34064969138607	36.80655125230166	47719
f74e6c780930448727216c95af5af706c5e8a0b5	odd even shifts in simd hypercubes	computers;index termsimage template matching;general and miscellaneous mathematics computing and information science;data routing;technology utilization;linear time algorithm;computerised pattern recognition;complexity;odd length circular shifts;mathematical logic;parallel algorithms computational complexity computer vision computerised pattern recognition computerised picture processing;computer vision;complexity image template matching odd even shifts odd length circular shifts odd shifts data routing parallel processing parallel algorithms linear time algorithm two dimensional convolution n sup 2 processor simd hypercube;computational complexity;hypercubes routing convolution broadcasting computer vision concurrent computing sorting linear algebra registers clocks;odd shifts;two dimensional convolution;computerised picture processing;programming 990200 mathematics computers;algorithms;n sup 2 processor simd hypercube;hypercube computers;template matching;memory devices;odd even shifts;parallel processing;parallel algorithms;two dimensional calculations	A linear-time algorithm is developed to perform all odd (even) length circular shifts of data in an SIMD (single-instruction-stream, multiple-data-stream) hypercube. As an application, the algorithm is used to obtain an O(M/sup 2/+log N) time and O(1) memory per processor algorithm to compute the two-dimensional convolution of an N*N image and an M*M template on an N/sup 2/ processor SIMD hypercube. This improves the previous best complexity of O(M/sup 2/ log M+log N). >	simd	Sanjay Ranka;Sartaj Sahni	1990	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.80126	parallel processing;mathematical logic;parallel computing;complexity;template matching;computer science;theoretical computer science;distributed computing;parallel algorithm;computational complexity theory;algorithm	Arch	12.144324425503203	35.21286631058019	48213
c5aa7851c16dc6e98e414b70f6c7ab930bce4b52	a power- and area-efficient sram core architecture for super-parallel video processing	sram core architecture;segmentation free access;800 muw;video signal processing;video processing;motion estimation;motion estimation processor;130 nm;130 nm sram core architecture super parallel video processing segmentation free access extra x decoders motion estimation processor 800 muw;super parallel video processing;parallel architecture;random access memory motion estimation hdtv parallel architectures systolic arrays image segmentation mpeg 4 standard image coding pixel filters;video signal processing motion estimation sram chips;extra x decoders;sram chips	For super-parallel video processing, we proposed a power- and area-efficient SRAM core architecture with a segmentation-free access, which means accessibility to arbitrary consecutive pixels, and horizontal/vertical access. To achieve these flexible accesses, a spirally-connected local-wordline select signal and multi-selection scheme in wordlines are proposed, so that extra X-decoders in the conventional multi-division SRAM can be eliminated. Consequently, the proposed SRAM reduces an area and power by 69% and 59%, respectively, when it is applied to a 128 parallel architecture. The proposed 160-kbit SRAM with 16-read ports (eight-division and 2-read port SRAM) is implemented to a search window buffer for an H.264 motion estimation processor core which dissipates 800 muW for QCIF 15-fps in a 130-nm technology	accessibility;h.264/mpeg-4 avc;intel core (microarchitecture);motion estimation;multi-core processor;parallel computing;pixel;static random-access memory;video processing	Junichi Miyakoshi;Yuichiro Murachi;Masaki Hamamoto;Takahiro Iinuma;Tomokazu Ishihara;Hiroshi Kawaguchi;Masahiko Yoshimoto;Tetsuro Matsuno	2006	2006 IFIP International Conference on Very Large Scale Integration	10.1109/VLSISOC.2006.313232	electronic engineering;parallel computing;real-time computing;computer science;motion estimation;video processing	EDA	12.313865896753516	40.323348820581884	48495
19e1b36f742c233125327c57b4a41a299a581c6a	efficient gröbner basis reductions for formal verification of galois field multipliers	computer algebra-based approach;163-bit circuits;163-bit circuit;grobner basis reductions;term order;multiplier circuit modeling;multiplying circuits;algebraic geometry-based approach;bner base;bner basis reduction;network topology;galois field computation;galois field;verification problem;digital arithmetic;multiplier hardware implementation;circuit topology;galois fields;galois field multiplier;bner basis;bug detection;polynomial system;polynomials;efficient gr;process algebra;galois field arithmetic;formal verification;computer algebra;computer bugs;elliptic curve cryptography;logic gate;error correction code;logic gates;boolean function;boolean functions;data structures;integrated circuit;algebraic geometry;signal processing;data structure	Galois field arithmetic finds application in many areas, such as cryptography, error correction codes, signal processing, etc. Multiplication lies at the core of most Galois field computations. This paper addresses the problem of formal verification of hardware implementations of (modulo) multipliers over Galois fields of the type F2k, using a computer-algebra/algebraic-geometry based approach. The multiplier circuit is modeled as a polynomial system in F2k[x1, x2, ···, xd] and the verification problem is formulated as a membership test in a corresponding (radical) ideal. This requires the computation of a Gröbner basis, which can be computationally intensive. To overcome this limitation, we analyze the circuit topology and derive a term order to represent the polynomials. Subsequently, using the theory of Gröbner bases over Galois fields, we prove that this term order renders the set of polynomials itself a Gröbner basis of this ideal -- thus significantly improving verification. Using our approach, we can verify the correctness of, and detect bugs in, upto 163-bit circuits in F2163; whereas contemporary approaches are infeasible.	circuit topology;code;correctness (computer science);cryptography;ecc memory;error detection and correction;formal verification;gröbner basis;linear algebra;modulo operation;rendering (computer graphics);signal processing;software bug;symbolic computation;system of polynomial equations	Jinpeng Lv;Priyank Kalla;Florian Enescu	2012	2012 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedding problem;data structure;normal basis;logic gate;generic polynomial;computer science;theoretical computer science;boolean function;programming language;algorithm;resolvent;field norm	EDA	14.820383357827668	44.96955928678252	48770
806befbf89b95862cf1505c95b9842b1b7fc105b	simple collision-based chemical logic gates with adaptive computing	bz system;chemical logical gates;belousov zhabotinsky;collision based computing;adaptive computing;logic gate	We present a method that is capable of implementing information transfer without any rigidly controlled architecture using the light-sensitive Belousov-Zhabotinsky (BZ) reaction system. Chemical wave fragments are injected into a subexcitable area and their collisions result in annihilation, fusion or quasielastic interactions depending on their initial positions. The fragments of excitation both pre and post collision possess a considerable freedom of movement when compared to previous implementations of information transfer in chemical systems. We propose that the collision of such wave fragments can be controlled automatically through adaptive computing. By extension, forms of unconventional computing, i.e., massively parallel non-linear computers, can be realised by such an approach. In this study we present initial results from using a simple evolutionary algorithm to design Boolean logic gates within the BZ system.	belousov–zhabotinsky reaction;boolean algebra;computer;evolutionary algorithm;interaction;logic gate;nonlinear system;unconventional computing	Rita Toth;Christopher Stone;Ben de Lacy Costello;Andrew Adamatzky;Larry Bull	2009	IJNMC	10.4018/jnmc.2009070101	electronic engineering;real-time computing;logic gate;computer science;engineering;theoretical computer science;algorithm	HPC	18.6828367381791	42.10349432863219	48791
7e2a76795a7abb8faabf4edfd9cedd7803decefe	dsp implementation of fast fir filtering algorithms using short fft's	adsp 2100 dsp implementation fast fir filtering algorithms short fft real time application processing delay arithmetic complexity pointers;efficient implementation;fir filter;digital filters;digital signal processor;fast fourier transforms;digital signal processing chips;digital arithmetic;fir filters;digital signal processing finite impulse response filter filtering algorithms delay digital arithmetic digital signal processors registers timing signal processing algorithms convolution;real time application;digital filters digital signal processing chips fir filters fast fourier transforms real time systems delays digital arithmetic;delays;real time systems	This paper proposes an efficient implementation of fast FIR filtering algorithms which us,eful characteristics for real-time application. They maintain a low processing delay, independent of the filter length. The difficulty is to keep as much as possible the improvement brought by the reduction of the arithmetic complexity of these fast FIR filtering algorithms without exceeding the Digital Signal Processor (DSP) resources (number of regisl.ers, pointers, memory, ...). A particular attention is devoted to the heavy use of pointers which represents a crucial problem. It is solved in this paper by an optimal organisation of data in memory. Improvements of more than 70% in actual timings on an ADSP-2100 COInFcIfed to the classical algorithm of convolution are obtained, even for very short blocks.	algorithm;convolution;digital signal processor;fast fourier transform;finite impulse response;processing delay;real-time clock;real-time computing	Anissa Zergaïnoh-Mokraoui;Pierre Duhamel;Jean Pierre Vidal	1995		10.1109/ISCAS.1995.521490	multidimensional signal processing;computer vision;electronic engineering;parallel computing;real-time computing;digital down converter;digital signal;computer science;digital signal processing;finite impulse response;digital image processing;linear filter;digital delay line;half-band filter	Theory	12.49896681347945	43.22716949428975	48814
420e161321900c5d383a89a59730cffe75215272	parallel computer architectures and problem solving strategies for the consistent labeling problem	processor intercommunication computer simulation consistant labeling problem parallel architectures problem solving strategies;parallel computer architecture;parallel architectures;consistant labeling problem;processor intercommunication;computer simulation;problem solving;problem solving strategies	Parallel computer architectures and problem solving strategies for the consistent labeling problem are studied. Problem solving factors include: processor intercommunication methods, passing order, and selection of the initial processor to receive the problem.	artificial intelligence;central processing unit;cluster analysis;communications of the acm;computer architecture;computer graphics;computer science;computer vision;cybernetics;data compression;emoticon;image processing;interrupt;kerrison predictor;machine vision;parallel computing;pattern recognition;problem solving;randomness;robert haralick;simulation;spatial analysis;systems theory;the circle (file system)	Jeanette Tyler McCall;Joseph G. Tront;F. Gail Gray;Robert M. Haralick;William M. McCormack	1985	IEEE Transactions on Computers	10.1109/TC.1985.1676530	computer simulation;computational problem;mathematical optimization;computer science;theoretical computer science;distributed computing	Theory	11.213566981913921	35.31136398488722	48831
eefa5766379417cc530962b07c4324e871929354	algorithms for defining mixed radix fft flow graphs	helium;geometry;signal processors;flow graphs;fast fourier transform;signal processing fast fourier transforms;indexing;organizing;signal processing;joining processes;fast fourier transforms;mixed radix fft flow graphs;phase rotation;flow graphs signal processing algorithms indexing equations geometry signal processing organizing frequency joining processes;intraconnection;signal processing algorithms;frequency;signal processors mixed radix fft flow graphs intraconnection phase rotation fast fourier transforms high order language;high order language	A unified set of algorithms is presented to define the intraconnection and phase rotation structure of flow graphs for arbitrary fast Fourier transforms (FFTs). The same set of basic equations is applied to arbitrary-length FFTs; mixed-radix FFTs; either decimation-in-time or decimation-in-frequency FFTs; and FFTs with ordered inputs, ordered outputs, or both. These equations, which define input and output point indexing of each FFT stage and the twiddle factors used in each stage, are based on forward and backward products of radixes. The same basic products of radixes can be used to represent a wide variety of FFT structures. These equations also permit HOL (high-order language) specification of FFTs for programming signal processors. >	algorithm;fast fourier transform	Gordon L. DeMuth	1989	IEEE Trans. Acoustics, Speech, and Signal Processing	10.1109/29.31290	arithmetic;computer vision;fast fourier transform;parallel computing;computer science;theoretical computer science;signal processing;quantum mechanics	Arch	16.580347458872733	42.185671363247515	49128
d7fdcfaa9ad289dafc8a30a553f8e9ec848d1877	low cost and memoryless cavld architecture for h.264/avc decoder	size 0 18 mum h 264 avc decoder context adaptive variable length decoder video coding standard look up tables high power dissipation syntax element decoding vhdl altera stratix ii fpga hardware resources consumption;syntax element decoding;architectural design;look up table;variable length codes;random access memory;adaptive decoding;memory management;codecs;decoding;hardware resources consumption;real time;hardware description languages;video compression;tree data structures;architectural design video compression cavld h 264 avc standard;look up tables;size 0 18 mum;memory access;video coding;computer architecture;automatic voltage control;h 264 avc standard;high power dissipation;memory architecture;vhdl;power dissipation;tree structure;context adaptive variable length decoder;costs memory architecture automatic voltage control decoding hardware video coding power dissipation tree data structures field programmable gate arrays throughput;hardware design;field programmable gate arrays;cavld;table lookup;video coding standard;high power;video coding adaptive decoding codecs field programmable gate arrays hardware description languages table lookup variable length codes;throughput;altera stratix ii fpga;hardware;h 264 avc decoder	This paper presents a low cost and memoryless hardware design for the Context Adaptive Variable Length Decoder (CAVLD) of the H.264/AVC video coding standard. Usually, a large number of memory bits and memory accesses are required to decode the CAVLD symbols in H.264/AVC since a great number of syntax elements are decoded based on look-up tables. This is an important problem given the high hardware cost and the high power dissipation caused by the large number of memory accesses. Thus, to solve this problem, we designed an efficient decoding of syntax elements using tree structures. The architecture designed was described in VHDL and synthesized to Altera Stratix II FPGA and to TSMC 0.18μm standard-cells technologies. The results obtained show that our architecture has significant savings in hardware resources consumption and in the number of memory accesses in comparison to other published works. Our design reached the necessary throughput to decode SDTV videos (720x576 pixels) in real-time.	codec;data compression;field-programmable gate array;h.264/mpeg-4 avc;lookup table;pixel;real-time clock;standard-definition television;stratix;throughput;tree structure;vhdl;video coding format	Thaísa Leal da Silva;João Alberto Vortmann;Luciano Volcan Agostini;Altamiro Amadeu Susin;Sergio Bampi	2009	2009 IEEE Computer Society Annual Symposium on VLSI	10.1109/ISVLSI.2009.20	parallel computing;real-time computing;computer science;theoretical computer science	Arch	12.40331259944469	40.893658360100915	49363
03fdf193a1d4336712278032bf0c0a0a0689cccb	generalized algorithm for parallel sorting on product networks	multiprocessor interconnection networks;parallel algorithms parallel sorting product networks dd even merge sorting generalized algorithm asymptotic complexity time complexity deterministic algorithm factor graphs interconnection networks;product networks;parallel sorting;generic algorithm;time complexity;factor graph;sorting;parallel algorithms computational complexity multiprocessor interconnection networks sorting;upper bound;computational complexity;sorting hypercubes computer architecture upper bound multiprocessor interconnection networks parallel algorithms tree graphs binary trees routing very large scale integration;interconnection networks;algorithms;odd even merge;sorting algorithm;production network;parallel algorithms	We generalize the well-known odd-even merge sorting algorithm, originally due to Batcher [2], and show how this generalized algorithm can be applied to sorting on product networks. If G is an arbitrary factor graph with N nodes, its r-dimensional product contains N r nodes. Our algorithm sorts N r keys stored in the r-dimensional product of G in O r F N ( ( )) 2 time, where F(N) depends on G. We show that, for any factor graph G, F(N) is, at most, O(N), establishing an upper bound of O r N ( ) 2 for the time complexity of sorting N r keys on any product network. For product networks with bounded r (e.g., for grids), this leads to the asymptotic complexity of O(N) to sort N r keys, which is optimal for several instances of product networks. There are factor graphs for which F N O N ( ) (log ) 2 , which leads to the asymptotic running time of O N (log ) 2 to sort N r keys. For networks with bounded N (e.g., in the hypercube N = 2, fixed), the asymptotic complexity becomes O r ( ) 2 . We show how to apply the algorithm to several cases of well-known product networks, as well as others introduced recently. We compare the performance of our algorithm to well-known algorithms developed specifically for these networks, as well as others. The result of these comparisons led us to conjecture that the proposed algorithm is probably the best deterministic algorithm that can be found in terms of the low asymptotic complexity with a small constant. Index Terms —Sorting, interconnection networks, product networks, algorithms, odd-even merge. —————————— ✦ ——————————	binary logarithm;computational complexity theory;de bruijn graph;deterministic algorithm;factor graph;interconnection;merge sort;randomized algorithm;sorting algorithm;time complexity;whole earth 'lectronic link	Antonio Fernández;Nancy Eleser;Kemal Efe	1995		10.1109/71.640013	time complexity;parallel computing;genetic algorithm;computer science;sorting;theoretical computer science;sorting algorithm;factor graph;distributed computing;parallel algorithm;upper and lower bounds;computational complexity theory;algorithm	Theory	15.662195726591737	32.68301226170868	49507
a9c8f76733178b8e0e3aefae07d1bc0439c16643	high performance ip core for hevc quantization	quantization signal computer architecture transforms pipelines hardware encoding video coding;video coding cmos integrated circuits quantisation signal video codecs;quantization signal;video coding;computer architecture;frequency 374 mhz high performance ip core hevc quantization unified quantizer nonpipelined implementation cmos process uhdtv video sequences size 90 nm;pipelines;transforms;encoding;hardware	A new class of quantization architectures suitable for the realization of high performance and hardware efficient forward, inverse and unified quantizers for HEVC is presented. The proposed structures are based on a highly flexible and optimized integer datapath that can be configured to provide several pipelined and non-pipelined implementations, offering distinct trade-offs between performance and hardware cost, which makes them highly suitable for most video coding application domains. The experimental results obtained using a 90 nm CMOS process show that the proposed class of quantization architectures is able to process 4k UHDTV video sequences in real-time (3840 × 2160 @ 30fps), with a power consumption as low as 3.9 mW when the unified architecture is operated at 374 MHz.	application domain;application-specific integrated circuit;cmos;color quantization;computation;data compression;datapath;high efficiency video coding;pipeline (computing);quantization (signal processing);real-time clock;semiconductor intellectual property core	Tiago Dias;Nuno Roma;Leonel Sousa	2015	2015 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2015.7169275	electronic engineering;real-time computing;computer science;theoretical computer science;pipeline transport;encoding;multiview video coding	Arch	11.841625730014917	41.07084705147802	49605
298e633b7660f3412b14e41ccf60f3976f9d69ca	a novel vlsi concurrent dual multiplier-dual adder architecture for image and video coding applications	processing element;multiplying circuits;data compression;logic design;video signal processing;very large scale integration;efficient algorithm;signal design;concurrent dual multiplier dual adder architecture;video coding applications;minimization methods;additives;video coding;compressors;very large scale integration video coding logic design pulse inverters additives compressors minimization methods video signal processing signal processing algorithms signal design;computational complexity;computation time vlsi concurrent dual multiplier dual adder architecture video coding applications high throughput image coding carry save 4 2 compressors;adders;carry save 4 2 compressors;performance analysis;vlsi;pulse inverters;digital signal processing chips;computation time;high throughput;signal processing algorithms;digital signal processing chips vlsi multiplying circuits adders video coding data compression computational complexity;high throughput image coding	In this paper, an efficient algorithm for concurrent computation of two real multiplications and/or two real additions usually required for high-throughput image and video coding applications is described. The proposed algorithm is mapped onto a novel concurrent dual multiplier-dual adder cell based on carry-save 4:2 compressors. A detailed performance analysis of the the proposed cell shows reductions ranging from 15% to 60% in the computation time and area when compared with the conventional processing elements making it highly attractive for VLSI implementation.	adder (electronics);data compression	D. V. Poornaiah;P. V. Ananda Mohan	1996		10.1109/ICVD.1996.489458	electronic engineering;real-time computing;computer science;electrical engineering;theoretical computer science;very-large-scale integration;algorithm	Arch	12.693913463544432	43.01556678468983	49777
060342bef0c04225c72548c746f5b126d2e8bdd3	some linear-time algorithms for systolic arrays	greatest common divisor;cluster computing;eigenvalue problem;parallel algorithm;image processing;linear array;systolic array;linear time algorithm;chip;finite field;numerical analysis;error correction code;error correction;signal processing;parallel computer;symmetric eigenvalue problem;technical report;computer science;linear equations;data structure;jacobi method	We survey some recent results on linear-time algorithms for systolic arrays. In particular, we show how the greatest common divisor (GCD) of two polynomials of degree n over a finite field can be computed in time O(n) on a linear systolic array of O(n) cells; similarly for the GCD of two n-bit binary numbers. We show how n by n Toeplitz systems of linear equations can be solved in time O(n) on a linear array of O(n) cells, each of which has constant memory size (independent of n). Finally, we outline how a two-dimensional square array of O(n) by O(n) cells can be used to solve (to working accuracy) the eigenvalue problem for a symmetric real n by n matrix in time O(nS(n)). Here S(n) is a slowly growing function of n; for practical purposes S(n) can be regarded as a constant. In addition to their theoretical interest, these results have potential applications in the areas of error-correcting codes, symbolic and algebraic computations, signal processing and image processing. For example, systolic GCD arrays for error correction have been implemented with the microprogrammable “PSC” chip.	binary number;charge-coupled device;code;computation;error detection and correction;forward error correction;image processing;linear algebra;linear equation;microcode;multi-function printer;polynomial greatest common divisor;signal processing;system of linear equations;systolic array;time complexity;toeplitz hash algorithm	Richard P. Brent;Franklin T. Luk;H. T. Kung	1983			mathematical optimization;combinatorics;discrete mathematics;error detection and correction;data structure;image processing;computer science;theoretical computer science;signal processing;mathematics;algorithm	Theory	12.937909628517222	35.42973690308091	49983
90d9ce5f4c55aa1feec625e2060ddbe0325c8001	generalized parallel selection in sorted matrices	selection problem;parallel algorithm;concurrent computing;matrix algebra parallel algorithms computational complexity;helium;information technology;phase change random access memory;matrix algebra;selection problem parallel selection sorted matrices parallel algorithm erew pram sorted matrix;sorted matrix;sorted matrices;upper bound;symmetric matrices;parallel algorithms phase change random access memory helium symmetric matrices concurrent computing information technology australia computer science upper bound;computational complexity;computer science;erew pram;australia;parallel selection;parallel algorithms	This paper presents a parallel algorithm running in time O(logmlog* m(log1ogm + log(n/m))) time on an ERE W PRAM with O(m/(log m log* m)) processors for the problem of selection in an m x n matrix with sorted rows and columns, m 5 n. Our algorithm generalizes the result of Sarnath and He [13] for selection in a sorted matrix of equal dimensions, and thus answers the open question they posted. The algorithm is work-optimal when n 2 mlogm, and near optimal within O(log1og m) factor otherwise. We show that our algorithm can be generalized to solve the selection problem on a set of sorted matrices of arbitrary dimensions.	central processing unit;column (database);parallel algorithm;selection algorithm	Hong Shen	1996		10.1109/SPDP.1996.570345	parallel computing;concurrent computing;computer science;theoretical computer science;distributed computing;parallel algorithm;information technology;algorithm	Theory	14.519250457916344	33.29900899572405	50275
bdd28ed10e8c1e12d08466154612904ddeb618ff	the fundamental primitives with fault-tolerance in quantum-dot cellular automata	quantum-dot cellular automata;three and five-input majority gates;adder;fault tolerance	Since conventional CMOS technology has met its development bottleneck, an alternative technology, quantum-dot cellular automata (QCA), attracted researchers’ attention and was studied extensively. The manufacturing process of QCA, however, is immature for commercial production because of the high defect rate. Seeking for designs that display excellent performance shows significant potentials for practical realizations. In the paper we propose a 5 × 5 module, which not only can implement three-input majority gate but also can realize five-input majority gate by adding another two inputs. A comprehensive analysis is made in terms of area, number of cells, energy dissipation and fault tolerance against single-cell omission defects. In order to testify the superiority of the proposed designs, preexisting related designs are tested and compared. Weighing up above four kinds of factors and technical feasibility, proposed majority gates perform fairly well. Further, we take full adders and multi-bit adders as illustrations to display the practical application of proposed majority gates. The detailed comparisons with previous adders reveal that proposed 5 × 5 module behaves well in circuits, especially the high degree of fault tolerance and the relatively small area, complexity and QCA cost, thereby making it more suitable for practical realizations in large circuit designs.	fault tolerance;quantum dot cellular automaton	Mengbo Sun;Hongjun Lv;Yongqiang Zhang;Guangjun Xie	2018	J. Electronic Testing	10.1007/s10836-018-5723-z	computer science;electronic engineering;fault tolerance;electronic circuit;alternative technology;cellular automaton;quantum dot cellular automaton;adder;bottleneck;cmos	Theory	15.99815637443057	45.866612624984285	50492
80e9b5b6dc6994dd00b944c810441f4166826930	fpga accelerated multipliers over binary composite fields constructed via low hamming weight irreducible polynomials	fpga accelerated multipliers;field programmable gate array;multiplicador binario;digit serial multipliers;bit parallel structure;multiplying circuits;hybrid multipliers;hamming weight;composite material;linear feedback shift registers;xilinx ise 7 1 fpga accelerated multipliers low hamming weight irreducible polynomials digit serial multipliers binary composite fields irreducible pentanomial conventional construction pentanomials linear feedback shift registers and xor arrays irreducible trinomials bit parallel structure hybrid architecture hybrid multipliers post place and route timing analysis;polynomials field programmable gate arrays poles and towers elliptic curve cryptography complexity theory generators logic gates;pentanomials;material compuesto;red puerta programable;reseau porte programmable;hamming distance;xilinx ise 7 1;retroaccion;parallel architectures;logic gates;post place and route timing analysis;retroaction;architecture parallele;multiplying circuits field programmable gate arrays logic gates;distance hamming;feedback regulation;irreducible polynomial;multiplicateur binaire;hybrid architecture;registro dispersion;conventional construction;binary composite fields;binary multiplier;low hamming weight irreducible polynomials;field programmable gate arrays;irreducible trinomials;irreducible pentanomial;registre decalage;distancia hamming;shift register;and xor arrays;materiau composite	The efficient design of digit-serial multipliers for special binary composite fields, F2nm where gcd(n, m) 1⁄4 1, is presented. These composite fields can be constructed via an irreducible pentanomial of degree nm but not an irreducible trinomial of degree nm. The conventional construction method for such digit-serial multipliers is to exploit the simplicity of pentanomials to obtain efficent linear feedback shift registers together with AND–XOR arrays. In this approach, these binary fields are constructed via irreducible trinomials of degree m with respect to F2n which in turn are also constructed via an irreducible trinomial (Hybrid I) or pentanomial (Hybrid II) over F2. The bit-serial structure to the tower field and applying the bit-parallel structure to the ground field are applied to obtain the hybrid architecture. Three kinds of multipliers (conventional, Hybrid I and Hybrid II) are implemented using the same FPGA device. Since at least one level is constructed via a trinomial instead of a pentanomial, the hybrid multipliers are 10–33% more efficient than the conventional ones according to the post-place-and-route-timing analysis via Xilinx-ISE 7.1.	american and british english spelling differences;exclusive or;field-programmable gate array;hamming weight;irreducibility;irreducible polynomial;linear-feedback shift register;place and route;serial communication;static timing analysis;trinomial;window function;xilinx ise	Chang Shu;Soonhak Kwon;Kris Gaj	2008	IET Computers & Digital Techniques	10.1049/iet-cdt:20060168	embedded system;electronic engineering;computer science;field-programmable gate array	ECom	11.058927128362082	44.22672269392054	50646
29b8d88d3e4fe66d70c4781ff9605daf480d27dd	a cordic arctangent fpga implementation for a high-speed 3d-camera system	approche cordic;field programmable gate array;fotografia rapida;movie camera;red puerta programable;reseau porte programmable;fpga implementation;high speed photography;camara;three dimensional system;grande vitesse;systeme 3 dimensions;systeme camera 3 dimensions;sistema 3 dimensiones;gran velocidad;printed circuit board;high speed;algorithm design;camera;photographie rapide	This paper presents the design and FPGA implementation of a pipelined CORDIC arctangent unit suitable for use in a 3D camera system. The end use for this application is in the assembly of printed circuit boards where there is a need for high-speed 3D height inspection of solder paste. FPGAs are chosen as the implementation platform, firstly for their quick turnaround to a final prototype; secondly for their reprogrammability to meet advances in algorithm design via software rather than hardware; thirdly footprint compatible higher speed grade FPGAs can be used to adapt the system to improved sensor technologies as they become available; finally the latest FPGAs offer a wide range of resources, including SDRAM drivers, ZBT SRAM drivers, fast carry logic and interfaces such as LVTTL and LVDS.	algorithm design;cordic;field-programmable gate array;low-voltage differential signaling;paste;printed circuit board;printing;prototype;static random-access memory;transistor–transistor logic	Stephen J. Bellis;William P. Marnane	2000		10.1007/3-540-44614-1_53	embedded system;algorithm design;computer hardware;computer science;operating system;printed circuit board;high-speed photography;field-programmable gate array	EDA	14.45914730082178	40.572718574644036	50810
89ddace897f9f4bc58e813960d38b94c16ff5c48	laying out midimew networks with constant dilation	average distance;interconnection network;parallel computer;graph embedding;circulant graph	Abst rac t . Midimew networks [1] are mesh-connected networks derived from a subset of degree-4 circulant graphs. They have minimum diameter and average distance among all degree-4 circulant graphs, and are better than some of the most common topologies for parallel computers in terms of various cost measures. Among the many midimew networks, the rectangular ones appear to be most suitable for practical implementation. Unfortunately, with the normal way of laying out these networks on a 2-D plane, long cross wires that grow with the size of the network appear. In this paper, we propose ways to embed rectangular midimew networks in a 2-D grid so that the length of the longest wire is at most a small constant.	circulant matrix;computer;dilation (morphology);parallel computing	Guihai Chen;Francis C. M. Lau	1994		10.1007/3-540-58430-7_67	graph power;factor-critical graph;combinatorics;geometric graph theory;discrete mathematics;graph embedding;graph bandwidth;regular graph;distance-regular graph;theoretical computer science;simplex graph;mathematics;voltage graph;windmill graph;butterfly graph;quartic graph;line graph;strength of a graph;circulant graph;coxeter graph;friendship graph	ML	24.30203794580047	35.510883533022444	50945
dcfba3daf8672cbaf7f9889bdf0b6ecb82335cf6	deterministic distributed resource discovery (brief announcement)	distributed system;resource discovery;time complexity;communication complexity;connected graph;concurrency;public key;community networks;directed graph;timing optimization;randomized algorithm;parallel programs;distributed algorithm;web caching	The <italic>resource discovery problem</italic> was introduced by Harchol-Balter, Leighton and Lewin in [HLL99], as a part of their work on web caching. They developed a randomized algorithm for the problem in the weakly connected directed graph model, that was implemented within LCS at MIT, and then licensed to Akamai Technologies. The directed graph is a logical one (as opposed to the underlying communication graph). It represents the nodes' “knowledge” about the topology of the underlying communication network. In [HLL99] the notion of topology “knowledge” is simplified, by modeling it as a “knowledge” of an <italic>ID</italic> of another node. In the general case such a “knowledge” may a include whole route, as well as any additional information needed in order to establish a connection (e.g. a cryptographic public key). It is assumed (here, and in [HLL99]) that the logical graph <italic>G</italic> is weakly connected. This can result from topology changes: e.g., a loss of a connection to a name server, or a gain of new knowledge that is uneven between different nodes, since it is obtained by distributed algorithms, and in non-homogeneous network.  Note that weak connectivity is a necessary condition for the solvability of the problem. Dealing efficiently with the weakly connected graph was, in fact, the main contribution in [HLL99]. The alternative of transforming the graph into an undirected one, and then solving the problem on the resulting undirected graph, is possible. If <italic>E</italic><subscrpt>0</subscrpt> = <italic>O</italic>(<italic>n</italic>) it even leads to efficient solutions: A time optimal algorithm (for <italic>undirected graphs</italic>) appears in [SV82], It was designed for CRCW PRAM, but can be translated into the model of [HLL99] using <italic>O</italic>(|<italic>E</italic><subscrpt>0</subscrpt>| log <italic>n</italic>) messages. The assumption that |<italic>E</italic><subscrpt>0</subscrpt>| = <italic>O</italic>(<italic>n</italic>) may be suitable if topology changes are assumed to be limited. Many practical distributed systems, however, attempt to deal with the case that the network may be partitioned. Thus, <italic>E</italic><subscrpt>0</subscrpt> should be as big as possible, to enable the disconnected components to regain connectivity.  The current paper proposes a <italic>deterministic</italic> algorithm for the problem in the same model as that of [HLL99]. Our algorithm is near optimal in all the measures: time, message, and communication complexities. Each previous algorithm had a complexity that was higher at least in one of the measures. Specifically, previous deterministic solutions required either time linear in the diameter of the initial network, or communication complexity <italic>O</italic>(<italic>n</italic><supscrpt>3</supscrpt>) (with message complexity <italic>O</italic>(<italic>n</italic><supscrpt>2</supscrpt>)), or message complexity <italic>O</italic>(|<italic>E</italic><subscrpt>0</subscrpt>| log <italic>n</italic>) (where <italic>E</italic><subscrpt>0</subscrpt> is the edge set of the initial graph). Compared to the main randomized algorithm of the Harchol-Balter, Leighton, and Lewin, the time complexity is reduced from <italic>O</italic>(log<supscrpt>2</supscrpt><italic>n</italic>) to <italic>O</italic>(log <italic>n</italic> log<supscrpt>*</supscrpt> <italic>n</italic>), the message complexity from <italic>O</italic>(<italic>n</italic> log <supscrpt>2</supscrpt><italic>n</italic>) to <italic>O</italic>(<italic>n</italic> log <italic>n</italic> log <supscrpt>*</supscrpt> <italic>n</italic>), and the communication complexity from <italic>O</italic>(<italic>n</italic><supscrpt>2</supscrpt> log<supscrpt>3</supscrpt> <italic>n</italic>) to <italic>O</italic>(|<italic>E</italic><subscrpt>0</subscrpt>|log<supscrpt>2</supscrpt> <italic>n</italic> + <italic>n</italic><supscrpt>2</supscrpt> log <italic>n</italic>).	communication complexity;connectivity (graph theory);directed graph;distributed algorithm;distributed computing;graph (discrete mathematics);parallel random-access machine;public-key cryptography;randomized algorithm;server (computing);telecommunications network;time complexity;weak ai;web cache	Shay Kutten;David Peleg	2000		10.1145/343477.362152	time complexity;distributed algorithm;combinatorics;directed graph;concurrency;computer science;connectivity;theoretical computer science;communication complexity;mathematics;distributed computing;graph;windmill graph;public-key cryptography;randomized algorithm;algorithm	Theory	16.902287507759894	34.87841166382468	51150
faf8cf89a1521ceddd903dd6a32339b6b55dcdc7	a high performance soft decision viterbi decoder for wlan and broadband applications	broadband networks;decoding;design support;logic design;saturation arithmetic viterbi decoder convolution encoder soft decision parallel processing;add compare select;circuit design wlan applications broadband applications configurable 3 bit soft decision viterbi decoder sdvd realization add compare select units vhdl simulation matlab simulation environment;circuit design;hardware description languages;telecommunication computing;wireless lan broadband networks circuit simulation hardware description languages logic design parallel architectures telecommunication computing viterbi decoding;vhdl simulation;hardware architecture;viterbi algorithm decoding circuit simulation throughput wireless lan built in self test mathematical model hardware circuit synthesis field programmable gate arrays;circuit simulation;saturation arithmetic;matlab simulation environment;built in self test;parallel architectures;configurable 3 bit soft decision viterbi decoder;viterbi algorithm;viterbi decoder;functional model;mathematical model;sdvd realization;wlan applications;wireless lan;add compare select units;field programmable gate arrays;convolution encoder;high performance;soft decision;viterbi decoding;parallel processing;broadband applications;simulation environment;circuit synthesis;throughput;hardware	This paper presents a configurable 3-bit soft decision Viterbi decoder implementation that meets the requirements for WLAN and broadband applications. The programmable design supports a constraint length K=7 soft decision Viterbi decoder (SDVD) realization with a code rate (R) of 1/2 and traceback lengths (TBL) of 35 and 50 symbols. To assure a throughput of 155 Mbps, an architecture incorporating 32 add compare select (ACS) units operating in parallel has been selected. The design incorporates a built-in self-test for operation at the rated throughput. The VHDL simulation results are verified against a functional model of the Viterbi decoder reflecting the hardware architecture in the Matlab simulation environment. The decoder architecture is defined in VHDL and the circuit is simulated, synthesized, and implemented on a Xilinx XC2VP100-1704ff-5 FPGA device	built-in self-test;code rate;convolutional code;data rate units;defining length;field-programmable gate array;function model;matlab;requirement;simulation;throughput;vhdl;viterbi decoder	Abdul-Rafeeq Abdul-Shakoor;Valek Szwarc	2006	2006 Canadian Conference on Electrical and Computer Engineering	10.1109/CCECE.2006.277834	parallel processing;electronic engineering;parallel computing;real-time computing;soft-decision decoder;computer science;hardware architecture;viterbi decoder;statistics	EDA	11.813175950564977	46.194181877579354	51163
7b4fd7442881564a832f4b8672e7ab8682943d03	a new rns scaler for {2n − 1, 2n, 2n + 1}	digital signal processing;wide dynamic range;complexity theory;integrated circuit;time complexity;design criteria;pervasive computing;circuit design;residue number systems;inner product;global position system;digital camera;set theory;discrete cosine transform;chip;fast fourier transform;computer architecture;chinese remainder theorem;logic gates;computational complexity;adders;system integration;fir filter;power dissipation;dynamic range;digital signal processor;radio frequency identification;digital signal processing rns scaler rns scaling algorithm balanced special moduli set scaling constant three moduli set new chinese remainder theorem i new crt i complicated modulo reduction operations scaling error multioperand modulo adder area time complexity residue number system;ubiquitous computing;digital signal processing chips;residue number system;set theory computational complexity digital signal processing chips residue number systems;logic gate;real time application;algorithm design;delay logic gates computer architecture algorithm design and analysis dynamic range complexity theory adders;algorithm design and analysis	This paper presents an efficient RNS scaling algorithm for the balanced special moduli set {2n−1, 2n, 2n+1}. By exploiting the relationship between the scaling constant and the residues of the three-moduli set using the New Chinese Remainder Theorem I (New CRT-I), the complicated modulo reduction operations for large integer scaling in RNS can be greatly simplified. The scaling constant has been chosen as 2n(2n+1)such that all residues of the scaled integer are identical and equal to the scaled integer output. This is particularly useful as no expensive and slow residue-to-binary converter is required for interfacing with conventional number system after the digital signal processing and scaling in RNS domain. The scaling error occurs only conditionally and is proven to be at most unity. The proposed design can be implemented entirely based on full adders with complexity commensurate with a multi-operand modulo 2n −1 adder. Its area-time complexity is at least 86% lower than one of the fastest ROM-based scaler designs for the same moduli set over a wide dynamic range of 15 bits and above.	adder (electronics);algorithm;cathode ray tube;digital signal processing;dynamic range;fastest;image scaling;modulo operation;modulus of continuity;operand;read-only memory;residue number system;scalability;time complexity	Jeremy Yung Shern Low;Chip Hong Chang	2011	2011 IEEE International Symposium of Circuits and Systems (ISCAS)	10.1109/ISCAS.2011.5937842	embedded system;algorithm design;electronic engineering;discrete mathematics;logic gate;computer science;theoretical computer science;mathematics;ubiquitous computing;algorithm	Arch	12.64706185829681	43.498081189527554	51733
b556ea31562f9940ea68276b98cc35a7f26d6577	balanced parallel scheduling for video encoding with adaptive gop structure	encoding complexity theory streaming media instruction sets processor scheduling dynamic scheduling video coding;complexity theory;processor scheduling;adaptive gop structure;video coding batch processing computers scheduling;video coding;streaming media;scheduling;batch processing computers;parallel scheduling adaptive gop structure;encoding;article;balanced parallel scheduling batch processing gradual scene change detection algorithm block based abrupt dynamic gop structure temporal influence coding complexity thread priority assignment balanced frame level parallel scheduling algorithms dynamic group adaptive gop structure video encoding;parallel scheduling;dynamic scheduling;instruction sets	Due to the nature of a dynamic group of picture (GOP) structure, parallel scheduling for video encoding becomes challenging. To address this, the balanced frame-level parallel scheduling algorithms are developed. The proposed approaches first determine the frame priority and then the thread priority assignment for scheduling. The concept of the algorithms lies in the analysis of coding complexity, temporal influence, and the required temporal burden to finish coding. To complete the scheduling with the dynamic GOP structure, a block-based abrupt and gradual scene change detection algorithm is also proposed to determine the GOP structure adaptively. The experiments show that the scheduling performance is close to the optimal. In addition, the concept of batch processing is incorporated so that the required buffer can be reduced.	algorithm;batch processing;brute-force search;computation;data compression;experiment;overhead (computing);parallel computing;scheduling (computing);speedup	Hsu-Feng Hsiao;Chen-Tsang Wu	2013	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2013.7	fair-share scheduling;fixed-priority pre-emptive scheduling;parallel computing;real-time computing;earliest deadline first scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;operating system;two-level scheduling;instruction set;distributed computing;round-robin scheduling;scheduling;encoding	HPC	11.213813745896331	39.04414221142247	52002
47836c9ad4ffbe06df1fc1be754b54e884a94760	framework and vlsi architecture of measurement-domain intra prediction for compressively sensed visual contents			very-large-scale integration	Jian-Bin Zhou;Dajiang Zhou;Li Guo;Takeshi Yoshimura;Satoshi Goto	2017	IEICE Transactions		architecture;compressed sensing;mathematics;theoretical computer science;very-large-scale integration	Visualization	13.218135302445273	40.00522219861291	52420
3a27674a47f144e33c15e1fbd756f9a265d99e85	architecture and implementation of multi-processor socs for advanced set-top box and digital tv systems	emergency service;digital tv;digital television;integrated circuit design;digital tv decoding multimedia systems streaming media hardware system on a chip hdtv auditory displays cmos technology engines;media processing;system on chip;programmable circuits;integrated circuit design digital television multimedia communication multiprocessing systems programmable circuits system on chip;multimedia communication;multiprocessing systems;web browsing;pnx 8500 multimedia soc multi processor soc set top box systems digital tv systems movie quality pictures cd quality sound dtv programmable hardware system on chip media processing soc nx 2700;verification and validation;set top box	Movie-quality picture and CD-quality sound are really only the tip of the iceberg because, even though these are the primary benefits from a consumer perspective, digital TV (DTV) technology promises much more. For example, DTV can be used to transmit large amounts of 'non-television' data into the home, which may be accessible by using the computer or the television set. This opens up the possibility of virtually a limitless range of enhanced services such as video telephony, video e-mail, web browsing, games, subtitles, captioning, and data-casting, to name a few. Moreover, new applications and services are invented almost every day. Clearly, in order to support these emerging services in an evolving market that is still a bit uncertain about and debating over format and content, it is necessary to implement and provide customers with a solution that is very flexible. This talk focuses on the design of programmable hardware that provides such flexible solutions. Starting with an introduction to the current trends in system-on-chip (SoC) designs, the talk details the architecture, implementation, verification, and validation of two highly-integrated, programmable, media-processing SoCs that have been designed at Philips for digital television and advanced set-top box applications. Some of the challenges faced and solutions devised are also discussed.	multiprocessing;set-top box;system on a chip	Santanu Dutta	2003		10.1109/SBCCI.2003.1232820	embedded system;electronic engineering;real-time computing;digital television;computer hardware;telecommunications;computer science;electrical engineering;operating system	HPC	10.455410660395154	41.80884493491882	52658
aeacc72271504ec050168d19f31354dbdcfec046	a novel modulo $2^{n}-2^{k}-1$ adder for residue number system	logic design;residue number systems;adders;vlsi	Modular adder is one of the key components for the application of residue number system (RNS). Moduli set with the form of 2n-2k-1 (1 ≤ k ≤ n-2) can offer excellent balance among the RNS channels for multi-channels RNS processing. In this paper, a novel algorithm and its VLSI implementation structure are proposed for modulo 2n-2k-1 adder. In the proposed algorithm, parallel prefix operation and carry correction techniques are adopted to eliminate the re-computation of carries. Any existing parallel prefix structure can be used in the proposed structure. Thus, we can get flexible tradeoff between area and delay with the proposed structure. Compared with same type modular adder with traditional structures, the proposed modulo 2n-2k-1 adder offers better performance in delay and area.	adder (electronics);algorithm;computation;modulo operation;modulus robot;parallel computing;preprocessor;residue number system;trie	Shang Ma;Jianhao Hu;Chen-Hao Wang	2013	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2013.2252639	arithmetic;electronic engineering;logic synthesis;computer science;engineering;electrical engineering;serial binary adder;mathematics;very-large-scale integration;carry-save adder;algorithm;adder	EDA	12.2666422442952	44.53022081797736	52920
b7b35ce826f43485745c458d644494f9686f41be	qca systolic array design	matrix multiplier;quantum dot cellular automata;multiplying circuits;logic design;clocks;systolic arrays;semiconductor quantum dots;systolic array;systolic arrays cellular automata cmos logic circuits delays galois fields integrated circuit design integrated circuit interconnections logic design matrix multiplication multiplying circuits pipeline processing quantum gates semiconductor quantum dots semiconductor quantum wells;wires;layout;quantum gates;arrays;integrated circuit design;nonhomogeneous media;logic gates;systolic array structure quantum dot cellular automata technology cmos technology qca characteristics digital circuit design approaches wire delay pipelined architectures complicated control system design parallelism semiconductor qca technology galois field multiplier matrix multiplier coplanar crossings multilayer crossings semiconductor qca systolic array design methodology;integrated circuit interconnections;cmos logic circuits;multilayer crossover quantum dot cellular automata systolic array matrix multiplier galois field multiplier coplanar crossing;multilayer crossover;clocks arrays wires layout delay nonhomogeneous media logic gates;matrix multiplication;cellular automata;galois field multiplier;galois fields;pipeline processing;delays;semiconductor quantum wells;coplanar crossing	Quantum-dot Cellular Automata (QCA) technology is a promising potential alternative to CMOS technology. To explore the characteristics of QCA and suitable design methodologies, digital circuit design approaches have been investigated. Due to the inherent wire delay in QCA, pipelined architectures appear to be a particularly suitable design technique. Also, because of the pipeline nature of QCA technology, it is not suitable for a complicated control system design. Systolic arrays take advantage of pipelining, parallelism, and simple local control. Therefore, an investigation into these architectures in semiconductor QCA technology is provided in this paper. Two case studies, (a matrix multiplier and a Galois Field multiplier) are designed and analyzed based on both multilayer and coplanar crossings. The performance of these two types of interconnections are compared and it is found that even though coplanar crossings are currently more practical, they tend to occupy a larger design area and incur slightly more delay. A general semiconductor QCA systolic array design methodology is also proposed. It is found that by applying a systolic array structure in QCA design, significant benefits can be achieved particularly with large systolic arrays, even more so than when applied in CMOS-based technology.	cmos;control system;crossing number (graph theory);digital electronics;integrated circuit design;parallel computing;pipeline (computing);qualitative comparative analysis;quantum cellular automaton;quantum dot cellular automaton;semiconductor;systems design;systolic array	Liang Lu;Weiqiang Liu;Máire O'Neill;Earl E. Swartzlander	2013	IEEE Transactions on Computers	10.1109/TC.2011.234	cellular automaton;layout;logic synthesis;logic gate;systolic array;matrix multiplication;computer science;theoretical computer science;mathematics;finite field;algorithm;algebra;integrated circuit design;quantum gate	EDA	15.553785835696702	45.89643631615202	52975
9b577184095c30df068536587ac84a7d70753724	design framework for jpeg2000 system architecture		Abstract For the exploration of system architecture dedicated to JPEG2000 coding, decoding and codec, a novel design framework is constructed. In order to utilize the scalability of JPEG2000 algorithm aggressively in system implementation, three types of modules are prepared for JPEG2000 coding/decoding/codec processes, i.e. softwaze, software accelerated with user-defined instructions, and dedicated hardware. Specifically, dedicated hazdware modules for forward and inverse discrete wavelet transformation (shortly DWT), entropy coder, entropy decoder, and entropy codec as well as software acceleration for the DWT process are devised to be used in the framework. Furthermore, a JPEG2000 encoder LSI, which consists of a configurable processor Xtensa, the DWT module, and the entropy coder, is fabricated to exemplify the system implementation designed through the use of proposed framework.	jpeg 2000	Hiroshi Tsutsui;Takahiko Masuzaki;Yoshiteru Hayashi;Yoshitaka Taki;Tomonori Izumi;Takao Onoye;Yukihiro Nakamura	2006	Intelligent Automation & Soft Computing	10.1080/10798587.2006.10642936	enterprise architecture framework;reference architecture;software architecture;the open group architecture framework;space-based architecture;rm-odp;operational view;tafim;architecture domain;applications architecture;solution architecture;software architecture description;architecture framework;view model;data architecture;systems architecture	Robotics	10.454806657577757	40.87174199813644	52978
2541024491876f7df8e82d1a3efd70757b2b04d2	reversible circuit optimization via leaving the boolean domain	computers;logic design;boolean functions;circuit designs;reversible circuits;quantum reversible circuit community;circuit design;logic gates quantum computing quantum mechanics circuit optimization computers registers;emerging technology;quantum gates;quantum gate;quantum gates boolean functions circuit optimisation logic design quantum computing;logic synthesis;logic gates;quantum physics;quantum mechanics;registers;quantum circuits;reversible circuit optimization;reversible circuits circuit optimization logic synthesis quantum circuits;circuit optimisation;quantum computing;boolean domain;auxiliary quantum bits;circuit optimization;circuit designs reversible circuit optimization boolean domain quantum reversible circuit community auxiliary quantum bits quantum gates	For years, the quantum/reversible circuit community has been convinced that: 1) the addition of auxiliary quantum bits (qubits) is instrumental in constructing a smaller quantum circuit, and 2) the introduction of quantum gates inside reversible circuits may result in more efficient designs. This paper presents a systematic approach to optimizing reversible (and quantum) circuits via the introduction of auxiliary qubits and quantum gates inside circuit designs. This advances our understanding of what may be achieved with 1) and 2).	algorithm;computation;mathematical optimization;multistage amplifier;quantum circuit;quantum gate;qubit;reversible computing	Dmitri Maslov;Mehdi Saeedi	2011	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2011.2105555	electronic engineering;quantum information;logic synthesis;computer science;theoretical computer science;quantum network;quantum circuit;mathematics;quantum convolutional code;algorithm;quantum mechanics;quantum gate;quantum error correction	EDA	17.127345049338913	45.3769059120625	53003
4c5ebba565ea9b84f10330c074271a05eea50a77	a low power radix-2 fft accelerator for fpga	field programmable gate array low power radix 2 fft accelerator radix 2 algorithm 8 parallel multipath delay commutator hardware accelerators software fft routines dsp processors radix 2 multipath delay commutator fft accelerator r2mdc fft accelerator 8 parallel processing hardware utilization parallel butterflies clock frequency mdc accelerator gate capacitance xilinx virtex fpga area measurement frequency measurement latency measurement throughput measurement power measurement fast fourier transform;8 parallel;fft;accelerator;dif;accelerator fft dif dsp r2mdc 8 parallel;computer architecture hardware throughput delays clocks ofdm field programmable gate arrays;low power electronics digital arithmetic digital signal processing chips fast fourier transforms field programmable gate arrays;r2mdc;dsp	This paper presents a low power FFT accelerator using a Radix-2 algorithm with an 8-parallel multi-path delay commutator. Hardware accelerators can achieve better performance and throughput compared to software FFT routines. Thus, FFT accelerators are used in many DSP processors. In this paper, a Radix-2 Multipath Delay Commutator (R2MDC) FFT accelerator is designed with 8-parallel processing of the input samples. The hardware utilization of the architecture is 100% requiring only 4 parallel butterflies. It increases the throughput to eight times that of the traditional R2MDC. Thus, it can achieve similar throughput while running at one eighth of the clock frequency for a regular MDC accelerator while roughly increasing the gate capacitance by 4 times. Thus, it can operate at a lower power than the regular Radix-2 MDC accelerator. We implement our design on the Xilinx Virtex FPGA and measure area, frequency, latency, throughput and power. We show that our design can operate at a similar rate while reducing the power by 25% on an FPGA compared to R2MDC.	algorithm;butterfly diagram;central processing unit;clock rate;data structure;digital signal processor;fast fourier transform;field-programmable gate array;hardware acceleration;multipath propagation;profiling (computer programming);throughput;virtex (fpga)	Soumak Mookherjee;Linda DeBrunner;Victor E. DeBrunner	2015	2015 49th Asilomar Conference on Signals, Systems and Computers	10.1109/ACSSC.2015.7421167	embedded system;electronic engineering;parallel computing;computer science	Arch	11.62178086488389	44.59823569577212	53388
86ee9511c8585cffe76f9b63c7e473d89abc7de0	parallel algorithms for the medial axis transform on linear arrays with a reconfigurable pipelined bus system	medial axis transform;image coding;parallel algorithm;image processing;data compression;binary image;reconfigurable architectures;linear array;computer vision;system buses;parallel algorithms phase change random access memory concurrent computing optical arrays computational modeling pixel information management educational institutions optical computing image processing;image compression;computational complexity;parallel computer;image processing reconfigurable pipelined bus optical transmission electronic computation parallel algorithm medial axis transform binary image computer vision image compression;computational complexity parallel algorithms data compression image coding reconfigurable architectures system buses;parallel algorithms	We show that knowledge-based techniques are as effective as mathematical techniques when satisfying constraints for solving manpower allocation problems. These techniques can be used to fulfill the corresponding local and global constraints based on the dynamic programming algorithm. It uses tools borrowed from genetic and simulated annealing algorithms, and fuzzy logic methodologies. The schedules produced by our algorithm match the best schedules produced by human experts.	dynamic programming;fuzzy logic;genetic algorithm;knowledge-based systems;medial graph;optic axis of a crystal;parallel algorithm;schedule (computer science);simulated annealing	Horng-Ren Tsai	2002		10.1109/ICPADS.2002.1183388	parallel computing;image processing;computer science;theoretical computer science;distributed computing;parallel algorithm	EDA	11.834376963058784	35.84625147811815	53417
df77e4473f80fc9f9f7765f10d94df3328e58ccd	the mixed-radix chinese remainder theorem and its applications to residue comparison	residue to binary conversions;mixed radix chinese remainder theorem;residue number systems;fpga;residue number systems field programmable gate arrays;cathode ray tubes delay algorithm design and analysis read only memory field programmable gate arrays parallel processing;residue comparator;sequential process;fpga implementation;computer arithmetic;chinese remainder theorem;fpga mixed radix chinese remainder theorem residue comparison mixed radix conversion binary correspondence sequential process residue to binary conversions parallel processing;fpga computer arithmetic residue number system rns chinese remainder theorem residue comparison mixed radix conversion;residue number system;rns;field programmable gate arrays;mixed radix conversion;read only memory;algorithm design and analysis;binary correspondence;cathode ray tubes;parallel processing;residue comparison	The Chinese remainder theorem (CRT) and mixed-radix conversion (MRC) are two classic theorems used to convert a residue number to its binary correspondence for a given moduli set {<i>P</i> <sub>n</sub>,...,<i>P</i> <sub>2</sub>, <i>P</i> <sub>1</sub>}. The MRC is a weighted number system and it requires operations modulo <i>P</i> <sub>i</sub> only and hence magnitude comparison is easily performed. However, the calculation of the mixed-radix coefficients in the MRC is a strictly sequential process and involves complex divisions. Thus the residue-to-binary (R/B) conversions and residue comparisons based on the MRC require large delay. In contrast, the R/B conversion and residue comparison based on the CRT are fully parallel processes. However, the CRT requires large operations modulo <i>M</i> = <i>P</i> <sub>n</sub>,...,<i>P</i> <sub>2</sub> <i>P</i> <sub>1</sub>. In this paper, a new mixed-radix CRT is proposed which possesses both the advantages of the CRT and the MRC, which are parallel processing, small operations modulo <i>P</i> <sub>i</sub> only, and the efficiency of making modulo comparison. Based on the proposed CRT, new residue comparators are developed for the three-moduli set {2<sup>n</sup> - 1, 2<sup>n</sup>, 2<sup>n</sup> + 1}. The FPGA implementation results show that the proposed modulo comparators are about 20% faster and smaller than one of the previous best designs.	cathode ray tube;coefficient;comparator;field-programmable gate array;modulo operation;parallel computing;polynomial remainder theorem	Shaoqiang Bi;Warren J. Gross	2008	IEEE Transactions on Computers	10.1109/TC.2008.126	arithmetic;embedded system;parallel processing;discrete mathematics;computer science;operating system;mathematics;field-programmable gate array;algebra	DB	12.905927550023883	44.06970641649801	53529
8a978cc8c357ec3f2091cb381cf83ba17a2a47f6	"""transistor realization of reversible """"zs"""" series gates and reversible array multiplier"""	reversible array multiplier;cmos technology;zs series gates;time complexity;journal;rf gate;pass transistor;power consumption	"""In order to reduce the redundant Toffoli gates and the line-crossings in the classical reversible full adder appearing in the present literatures, this paper gives a reconstructive structure of Fredkin gate, called RF gate, the corresponding quantum equivalent realization and electronic circuitry construction based on CMOS technology and pass-transistor of this gate are also designed in this paper. With the assistance of the RF gate and the basic reversible gates (including NOT gate, CNOT gate and Toffoli gate), we design new 4i?4 reversible gates called """"ZS"""" series gates and its corresponding electronic circuitry construction. The proposed """"ZS"""" series gates have the ability to operate reversible add operation between two signed numbers by a single gate and at lower power consumption. At the same time, as an application of """"ZS"""" series gates, this paper also designs reversible array multiplier in order to achieve the signed multiplication. It can be theoretically proved that the proposed reversible array multiplier can eliminate power loss associated with the irreversible operation of classical computer, and will be exponentially lower than reversible parallel multiplier with respect to time complexity."""	transistor	Rigui Zhou;Yang Shi;Hui'an Wang;Jian Cao	2011	Microelectronics Journal	10.1016/j.mejo.2010.11.008	time complexity;electronic engineering;nor logic;toffoli gate;three-input universal logic gate;gate equivalent;electrical engineering;pass transistor logic;controlled not gate;quantum circuit;xnor gate;cmos;algorithm;tc0;quantum gate	EDA	16.534505446910305	45.03640992326765	53789
4f92000120e39fab70de2e8b175fa745c84d385a	novel approximate synthesis flow for energy-efficient fir filter	generators;approximation algorithms;finite impulse response filters;adders;power demand;approximate computing;algorithm design and analysis	The portability of emerging computing systems demands further reduction in the power consumption of their components. Approximate computing can reduce power consumption by using a simplified or an inaccurate circuit. In this paper, the energy efficiency of a finite impulse response (FIR) filter is improved through approximate computing. We propose an approximate synthesis technique for an energy-efficient FIR filter with an acceptable level of accuracy. We employ the common subexpression elimination (CSE) algorithm to implement the FIR filter and replace conventional adder/subtractors with approximate ones. While yielding acceptable rates of accuracy, the proposed flow can attain a maximum energy saving of 50.7% in comparison with conventional FIR filter designs.	adder (electronics);approximate computing;approximation algorithm;common subexpression elimination;finite impulse response;software portability	Yesung Kang;Jaewoo Kim;Seokhyeong Kang	2016	2016 IEEE 34th International Conference on Computer Design (ICCD)	10.1109/ICCD.2016.7753266	algorithm design;mathematical optimization;electronic engineering;computer science;theoretical computer science;approximation algorithm;algorithm;adder;half-band filter	EDA	14.064792642684369	42.662699268730464	54238
0e581028b7f2ace4d02aa7edb6a178c2a1c886f7	in-camera detection of fabric defects	structural defects;video streaming;industrial inspection cameras;fabrics cameras computer industry inspection streaming media field programmable gate arrays electrical equipment industry electronics industry industrial control machinery production industries;machinery production industries;medium small size fpga in camera detection fabric defects industrial inspection cameras web processes video rate data rate video stream preprocessing defect information co occurrence matrices structural defects processing power;matrix algebra;computer industry;electrical equipment industry;fabric defects;inspection;image texture;web processes;co occurrence matrices;video rate;streaming media;electronics industry;data rate;industrial control;fabrics;defect information;in camera detection;video stream preprocessing;field programmable gate arrays;processing power;cameras;medium small size fpga;field programmable gate arrays image texture inspection cameras matrix algebra fabrics	Industrial inspection cameras for Web processes have a very high output video rate. This output data rate can be reduced by preprocessing the video stream inside the camera in order to only send pixels that have a high probability of containing defect information. Co-occurrence matrices are quite successful in detecting structural defects in fabrics; however, they require high processing power. This paper discusses simplification to these algorithms, which are simple enough to fit into a medium-small size FPGA.	algorithm;field-programmable gate array;level of detail;pixel;preprocessor;sensor;software bug;streaming media;uncompressed video	Ibrahim Cem Baykal;Graham A. Jullien	2004	2004 IEEE International Symposium on Circuits and Systems (IEEE Cat. No.04CH37512)	10.1109/ISCAS.2004.1328901	image texture;embedded system;computer vision;electronic engineering;inspection;computer science;engineering;electrical engineering;instructions per second;field-programmable gate array	Embedded	14.571941474521402	39.380765982059025	54322
3f95aa655fae92a31fcc74038be57a168780f4ae	high-performance low-power approximate wallace tree multiplier				Sa'ed Abed;Yasser Khalil;Mahdi Modhaffar;Imtiaz Ahmad	2018	I. J. Circuit Theory and Applications	10.1002/cta.2540	electronic engineering;field-programmable gate array;wallace tree;parallel computing;mathematics;multiplier (economics)	Logic	16.435834784110103	44.60934812835425	54327
7e95ef2446b3964450a5f7f5b8235575e8f9b62f	boolean function representation using parallel-access diagrams	logic function;nondeterministic device;pad graph;nondeterministic counterpart;benchmark circuit;parallel-access diagrams;boolean function representation;language recognizers;benchmark function;robddsare conceptually;ordered binary decision diagrams;traditional robdds;data structures;robustness;automata;boolean function;function point;logic programming;deterministic finite automata;finite automata;automata theory;polynomials;function representation;redundancy;boolean functions;canonical form	In this paper we introduce a nondeterministic counterpart to Reduced, Ordered Binary Decision Diagrams for the representation and manipulation of logic functions. ROBDDs are conceptually related to deterministic finite automata (DFA), accepting the language formed by the minterms of a function. This analogy suggests the use of nondeterministic devices as language recognizers. Unlike ROBDDs, the diagrams introduced in this paper allow multiple outgoing edges with the same label. By suitably restricting the degree of nondeterminism, we still obtain a canonical form for logic functions. Using PADs, we are able to reduce the memory occupation with respect to traditional ROBDDs for several benchmark functions. Moreover, the analysis of the PAD graphs allowed us to sometimes identify new and better variable ordering for several benchmark circuits.	automata theory;benchmark (computing);binary decision diagram;deterministic finite automaton;finite-state machine;function representation	Valeria Bertacco;Maurizio Damiani	1996			mathematical optimization;electronic engineering;discrete mathematics;data structure;computer science;theoretical computer science;mathematics;finite-state machine;boolean function;programming language;algorithm	AI	18.386808849130734	46.162955003202846	54479
30e9e337f5e45493bda290283a87ac59a8fd3c05	state assignment techniques in multiple-valued logic	xor;field programmable gate array;educational institutions switches fourier transforms systems engineering and theory mechanical engineering argon field programmable gate arrays logic functions boolean algebra automata;fourier transform;state assignment;zero coefficients;argon;systems engineering and theory;boolean algebra;mechanical engineering;not;automata;efficient implementation;field programmable gate arrays multiple valued logic state assignment not and xor zero coefficients;fourier transforms;logic functions;and;field programmable gate arrays;multivalued logic;switches;reed muller;transform matrix;field programmable gate arrays state assignment multivalued logic;galois fields;multiple valued logic	Multiple-Valued Logic (MVL) functions are implemented via Boolean multiple-wire arrangements where a careful state assignment methodology is used to ensure efficient implementation regimes. A ‘power of N’ module is proposed for GF (2 ). The method avoids the need to factorize the polynomial and circuits can be realised using a combination of NOT AND and XOR functions. In addition, a novel transform over GF (2 ) is proposed which shows promise when compared to the Reed-MullerFourier transform, in its capacity to produce zero coefficients. A possible implementation strategy, using Field Programmable Gate Arrays (FPGAs) is briefly discussed.	coefficient;exclusive or;field-programmable gate array;grammatical framework;polynomial	K. J. Adams;J. G. Campbell;Liam P. Maguire;J. A. C. Webb	1999		10.1109/ISMVL.1999.779720	fourier transform;electronic engineering;discrete mathematics;computer science;theoretical computer science;mathematics;algorithm;field-programmable gate array;algebra	EDA	17.714581143601826	44.11688946036454	54694
c43369f43ed4b0bc22f286db4fdd9f970e7fdfb5	time efficient vlsi artwork analysis algorithms in goalie2	plan masse;architecture systeme;performance;circuit vlsi;modo exploracion;network analysis;algorithme;algorithm;layout plan;output sorting time efficient analysis polygon trapezoid decomposition vlsi artwork analysis algorithms goalie2 circuit extraction system vlsi layout geometries trapezoids scanline management;vlsi circuit;integrated circuit technology;linear time;pattern recognition;vlsi;circuit layout cad;arquitectura sistema;very large scale integration algorithm design and analysis robustness sorting computational geometry design automation circuit analysis computing routing compaction central processing unit;systeme goalie 2;reconnaissance forme;rendimiento;vlsi layout;circuito vlsi;reconocimiento patron;system architecture;analyse circuit;scanning mode;mode balayage;plano masa;analisis circuito;algoritmo;vlsi circuit layout cad integrated circuit technology	New algorithms used in the GOALIE2 circuit extraction system, based on representing VLSI layout geometries as trapezoids, are presented in this paper. These include polygon-to-trapezoid decomposition, scanline management, and output sorting. In particular, the scanline algorithm virtually eliminates all redundant computation present in similar systems. These algorithms enable us to perform VLSI layout analysis in nearly linear time.	algorithm;circuit extraction;computation;scan line;scanline rendering;sorting;time complexity;very-large-scale integration	Kuang-Wei Chiang;Surendra Nahar;Chi-Yuan Lo	1988		10.1109/43.31520	time complexity;embedded system;electronic engineering;network analysis;performance;computer science;very-large-scale integration;engineering drawing;algorithm;systems architecture	Theory	17.851265514565544	38.7467254130575	54967
06ed1b373fb8cdcd8acb29ecee34ee6fad0410ca	routing in networks with low doubling dimension	silicon;shortest path;labeling scheme;metric space;routing intelligent networks extraterrestrial measurements computer science silicon costs books upper bound;routing;compact routing;mobile object;books;upper bound;scale free;intelligent networks;computer science;extraterrestrial measurements;lower bound;aspect ratio	This paper studies compact routing schemes for networks with low doubling dimension. Two variants are explored, name-independent routing and labeled routing. The key results obtained for this model are the following. First, we provide the first name-independent solution. Specifically, we achieve constant stretch and polylogarithmic storage. Second, we obtain the first truly scale-free solutions, namely, the network’s aspect ratio is not a factor in the stretch. Scale-free schemes are given for three problem models: name-independent routing on graphs, labeled routing on metric spaces, and labeled routing on graphs. Third, we prove a lower bound requiring linear storage for stretch \gt 3 schemes. This has the important ramification of separating for the first time the name-independent problem model from the labeled model for these networks, since compact stretch-1+e labeled schemes are known to be possible.	period-doubling bifurcation;polylogarithmic function;ramification problem;routing	Ittai Abraham;Cyril Gavoille;Andrew V. Goldberg;Dahlia Malkhi	2006	26th IEEE International Conference on Distributed Computing Systems (ICDCS'06)	10.1109/ICDCS.2006.72	dsrflow;telecommunications;computer science;theoretical computer science;destination-sequenced distance vector routing;distributed computing;upper and lower bounds	Theory	21.597648058258397	36.001765537327856	54996
d24291ec7bc74b5bc328e92528a153f24fad6d79	test pattern generation for column compression multiplier	exhaustive test test pattern generation column compression multiplier building cell parallel multiplier 4 2 counter tree vlsi implementation regular structure;integrated circuit testing automatic test pattern generation multiplying circuits parallel architectures vlsi logic testing;multiplying circuits;automatic test pattern generation;parallel architectures;logic testing;integrated circuit testing;vlsi;test pattern generator;test pattern generators counting circuits very large scale integration circuit testing microelectronics buildings electronic switching systems computer applications digital signal processing application software	When used as the building cell of a parallel multiplier, the (4,2) counter tree is better suited than a Wallace tree for a VLSI implementation because of its more regular structure. In this paper test pattern generation for the CC multipliers is presented following a brief introduction to the structure of the (4,2) counter and the column compression multiplier using a (4,2) counter as its building cell. In conclusion, less test patterns are enough to exhaustively test the CC multiplier.		Pingying Zeng;Zhigang Mao;Yizheng Ye;Yuliang Deng	1998		10.1109/ATS.1998.741663	computer architecture;electronic engineering;computer science;engineering;theoretical computer science;automatic test pattern generation;test compression;very-large-scale integration	Vision	14.333347355314674	45.41426494553605	55256
e7234b114b5fbee66a55d7353725c9792163a40c	"""correction to """"multiple constrained folding of programmable logic arrays: theory and applications"""""""	logic arrays;programmable logic arrays;programmable logic array;constraint theory	In the above-named paper [ibid., vol. CAD-2, pp. 151-167, July 1983], a misprint makes the correct understanding of the constrained multiple folding problem and its solutions almost impossible. In page 162, the lines between the 23rd row from the top and the 6th from the bottom (We state the column folding with ordered connection-row . ... and the ordered folding set O = {(C8, C9)}. should be moved to page 163 immediately after Example 5.3.1 and before Remark 5.3. In addition, Fig. 17 in Example 5.3.2 should be replaced by Fig. 16 and Fig. 16 of Example 5.3.1 should be replaced by Fig. 17.	programmable logic device	Giovanni De Micheli;Alberto L. Sangiovanni-Vincentelli	1984	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/TCAD.1984.1270083	erasable programmable logic device;embedded system;discrete mathematics;logic synthesis;macrocell array;logic optimization;diode–transistor logic;logic gate;logic family;programmable logic array;computer science;theoretical computer science;programmable logic device;complex programmable logic device;simple programmable logic device;digital electronics;programmable array logic;algorithm	EDA	18.95162947411496	44.31663167351063	55300
4d334798575dd72b4db4b92695e2f635f440fb64	a new algorithm based on givens rotations for solving linear equations on fault-tolerant mesh-connected processors	multiprocessor interconnection networks;linear algebra;parallel algorithm;fault tolerant;algorithm based fault tolerance;algorithm based fault tolerance linear equations fault tolerant mesh connected processors parallel algorithm fault tolerant scheme;fault tolerant computing;multiprocessor interconnection networks fault tolerant computing parallel algorithms linear algebra;fault tolerance;givens rotations;linear equations;fault tolerance equations linear systems parallel algorithms fault tolerant systems phased arrays systolic arrays neural networks very large scale integration hardware;mesh connected processor array;parallel algorithms	In this paper, we propose a new I/O overhead free Givens rotations based parallel algorithm for solving a system of linear equations. The algorithm uses a new technique called two-sided elimination and requires an N  (N + 1) mesh-connected processor array to solve N linear equations in (5N log N 4) time steps. The array is well suited for VLSI implementation as identical processors with simple and regular interconnection pattern are required. We also describe a fault-tolerant scheme based on an algorithm based fault tolerance (ABFT) approach. This scheme has small hardware and time overhead and can tolerate up to N processor failures.	central processing unit;fault tolerance;givens rotation;input/output;interconnection;linear equation;linear system;network topology;overhead (computing);parallel algorithm;processor array;system of linear equations;very-large-scale integration	K. N. Balasubramanya Murthy;K. Bhuvaneswari;Chebiyyam Siva Ram Murthy	1998	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.706053	fault tolerance;parallel computing;real-time computing;computer science;linear algebra;distributed computing;parallel algorithm	Arch	13.504703217215564	35.72462566135979	56070
9fd46a84b42613b9c6ac7bb63c52ea3d3ce545bc	reconfigurable buses with shift switching: concepts and applications	algoritmo paralelo;prefix sums;architectural design;switching;parallel algorithm;algorithm performance;list ranking;sorting;parallel architectures system buses reconfigurable architectures;reconfigurable architectures;linear array;parallel counters;procesador panel;tria;reconfigurable bus systems;indexing terms;array processor;algorithme parallele;system buses;processeur tableau;computer architecture;broadcasting parallel machines delay sorting computer science optical switches very large scale integration algorithm design and analysis parallel architectures counting circuits;architecture ordinateur;parallel architectures;resultado algoritmo;triage;conmutacion;performance algorithme;arquitectura ordenador;concepcion arquitectural;systeme parallele;parallel system;parallel architecture;conception architecturale;commutation;sistema paralelo;list ranking shift switching broadcast buses reconfigurable buses prefix sums algorithm parallel architectures parallel counters sorting	We propose to enhance traditional broadcast buses by the addition of a new feature that we call shift switching. We show that on a linear array of processors enhanced with shift switching, the prefix sums of n bits can be computed in [log(n+1)/log w] broadcasts, each over n switches, assuming a global bus of width w. Next our prefix sums algorithm is used in conjunction with broadcasting on short buses to obtain several efficient architectural designs for the following fundamental problems: 1) ranking linked lists, 2) counting the number of 1's in a sequence of n bits, and 3) sorting small sets. We see our main contribution in showing that the new bus feature leads to designs that are both theoretically interesting and practically relevant. >		Rong Lin;Stephan Olariu	1995	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.363407	parallel computing;real-time computing;index term;computer science;sorting;theoretical computer science;operating system;distributed computing;parallel algorithm;algorithm	Visualization	12.394852775561217	34.78086554245202	56202
aa68a69f56d24829aeffc7245286833463931680	the inverse matrix for the conversion between standard and normal bases	normal base;computer aided design;standard base;design automation;galois fields elliptic curve cryptography hardware design automation equations gaussian processes mathematics very large scale integration bridges application software;matrix algebra;coprocessors;finite field;normal basis;inverse matrix;coding theory;vlsi computer aided design;cryptography;standard basis;vlsi;vlsi circuit cad coprocessors cryptography encoding matrix algebra;cryptography systems;random counter;circuit cad;base conversion;random counter inverse matrix standard base normal base finite field cryptography systems vlsi computer aided design coding theory base conversion;encoding;normal basis base conversion computer aided design finite field standard basis	The mathematics of finite field has been widely applied on the design of core modules for cryptography systems. This paper presents one of the module designs of a VLSI computer aided design (CAD) system to bridge the gap between the finite field and the hardware for coding theory. It aims on the design automation of base-conversion between standard and normal bases. A random counter is proposed as the reference to do fast computation. This computation for the inverse matrix can be easily implemented by both software and hardware	coding theory;computation;computer-aided design;cryptography;very-large-scale integration	Ming-Haw Jing;Jyun-Min Wang;Zih-Heng Chen;Yan-Haw Chen	2006	APCCAS 2006 - 2006 IEEE Asia Pacific Conference on Circuits and Systems	10.1109/APCCAS.2006.342460	embedded system;electronic engineering;discrete mathematics;normal basis;electronic design automation;computer science;standard basis;cryptography;theoretical computer science;computer aided design;mathematics;finite field;algorithm;encoding;coding theory	EDA	10.405791153527518	43.933643499162535	56611
a469c0188d756a960b84f6c9f6f78a67ec31ecba	optimizing latency under throughput requirements for streaming applications on cluster execution	streaming applications;latency optimisation;temporal approach;data compression;spatial approach;video coding data compression parallel processing task analysis;video compression;task assignment latency optimisation streaming applications cluster execution task mapping exploiting pipeline execution under time constraints algorithm spatial approach temporal approach video compression;video coding;task analysis;task mapping;task assignment;cluster execution;delay throughput parallel processing streaming media pipelines video compression signal processing algorithms adaptive signal processing video signal processing scalability;exploiting pipeline execution under time constraints algorithm;parallel processing	Parallelism in applications that act on a stream of input data can be exploited with two different approaches, spatial and temporal. In this paper we propose a new task mapping algorithm, called EXPERT, to exploit temporal parallelism efficiently when the streaming application is running in a pipeline fashion. We compare the performance of spatial and temporal approaches, in terms of latency and throughput for a video compression application. The results show that the pipeline execution with the task assignment provided by EXPERT algorithm, significantly overcomes spatial parallelism. Additionally, this temporal parallelism presents better scalability results when the dimension of the problem is augmented	algorithm;data compression;optimizing compiler;parallel computing;scalability;simulation;throughput	Fernando Guirado;Ana Ripoll;Concepció Roig;Emilio Luque	2005	2005 IEEE International Conference on Cluster Computing	10.1109/CLUSTR.2005.347051	data compression;parallel processing;parallel computing;real-time computing;computer science;theoretical computer science;data parallelism;instruction-level parallelism;statistics;task parallelism	HPC	11.03951481998015	39.22898809609303	56694
4d6db8bf3881f5fc8f2915ea48ec53d8f8742775	a h.264/mpeg-2 dual mode video decoder chip supporting temporal/spatial scalable video	temporal-spatial scalable video;dual mode video decoder;64-bit adjustable memory bus;um cmos technology;h.264-mpeg-2;sram chips;cmos technology;hd1080 video;design automation environment;video coding;adjustable memory bus width;mpeg-2 dual mode video;sram;dual mode video decoder chip;decoder chip;spatial scalability;size 0.13 mum;proposed design;decoding cif;video codecs;spatial scalable video;static var compensator;chip;design automation;decoding;scalability;transform coding	This paper proposes a dual mode video decoder with 4-level temporal/spatial scalability and 32/64-bit adjustable memory bus width. A design automation environment for simulation and verification is established to automatically verify the correctness and completeness of the proposed design. Using a 0.13 um CMOS technology, it comprises 439Kgates/10.9KB SRAM and consumes 2~328mW in decoding CIF~HD1080 videos at 3.75~30fps when operating at 1~150MHz, respectively.	cmos;correctness (computer science);memory bus;scalability;server message block;simulation;static random-access memory;unified model;video decoder	Cheng-An Chien;Yao-Chang Yang;Hsiu-Cheng Chang;Jiawei Chen;Cheng-Yen Chang;Jiun-In Guo;Jinn-Shyan Wang;Ching-Hwa Cheng	2011	16th Asia and South Pacific Design Automation Conference (ASP-DAC 2011)		chip;embedded system;electronic engineering;real-time computing;scalability;transform coding;electronic design automation;telecommunications;computer science;electrical engineering;static var compensator	EDA	11.682007653924526	40.614588116247276	56987
46ae883e761ef96dfb6f13f041b1edacc597df02	a subworld-parallel multiplication and sum-of-squares unit	multiplying circuits;logic design;digital arithmetic multiplying circuits adders logic design parallel architectures;general purpose processor;multiplier subworld parallel multiplication sum of squares unit digital signal processors multimedia processors general purpose processors operands subword partitioning product mapping techniques;parallel architectures;adders;sum of squares;digital signal processor;digital arithmetic;delay estimation;digital signal processing delay estimation very large scale integration digital signal processors signal mapping hardware signal design equations computer society	Several recent digital signal processors, multimedia processors, and general-purpose processors with multimedia extensions support subword parallelism. With subword parallelism, each operand is partitioned into multiple lower-precision operands, called subwords. A single subword-parallel instruction performs the same operation on multiple sets of subwords in parallel. This paper presents the design of a subword-parallel multiplication and sum-of-squares unit (SPMSSU). The SPMSSU uses novel subword partitioning and partial product mapping techniques to perform one 32-bit, two 16-bit, or four 8-bit multiplications or sum-of-squares operations in parallel. The SPMSSU efficiently performs subword-parallel operations with area and delay estimates that are comparable to those of a conventional 32-bit multiplier.	16-bit;32-bit;8-bit;central processing unit;digital signal processor;general-purpose markup language;operand;parallel computing;substring;windows 3.0	Shankar Krithivasan;Michael J. Schulte;C. John Glossner	2004	IEEE Computer Society Annual Symposium on VLSI	10.1109/ISVLSI.2004.1339554	computer architecture;parallel computing;digital signal;computer science;theoretical computer science	Arch	12.238914572241727	43.87930044863866	57051
3a660fbb4f8e560c2590a465ed7e5a287bccad2e	don't cares and multi-valued logic network minimization	logic function;single multi-valued output;multi-valued logic network minimization;multi-valued function node;binary logic;compatible set;multi-valued logic function;multi-valued logic;multi-valued input;multi-level combinational logic network;experimental multi-valued network;inductance;value function;observability;combinational circuits;vlsi	We address optimizing multi-valued (MV) logic functions in a multi-level combinational logic network. Each node in the network, called an MV-node, has multi-valued inputs and single multi-valued output. The notion of don't cares used in binary logic is generalized to multi-valued logic. It contains two types of flexibility: incomplete specification and non-determinism. We generalize the computation of observability don't cares for a multi-valued function node. Methods are given to compute (a) the maximum set of observability don't cares, and (b) the compatible set of observability don't cares for each MV-node. We give a recursive image computation to transform the don't cares into the space of local inputs of the node to be minimized. The methods are applied to some experimental multi-valued networks, and demonstrate reduction in the size of the tables that represent multi-valued logic functions.	combinational logic;computation;don't-care term;emoticon;mv-algebra;nondeterministic algorithm;recursion	Yunjian Jiang;Robert K. Brayton	2000			electronic engineering;discrete mathematics;logic synthesis;logic optimization;observability;logic level;logic gate;logic family;programmable logic array;inductance;computer science;electrical engineering;pass transistor logic;mathematics;sequential logic;very-large-scale integration;bellman equation;combinational logic;digital electronics;register-transfer level;algorithm	EDA	19.167418991438854	45.5141973917643	57121
d3d3dfaed6d6d985e822ff5bb8b2aad5f5adfbcb	design of a compact reversible read-only-memory with mos transistors		Energy conservative devices are the need of the modern technology which leads to the development of reversible logic. The synthesis of reversible logic has become an intensely studied area as it overcomes the problem of power dissipation associated with irreversibility. Storage device such as Read-Only-Memory (ROM) can be realized in a reversible way with low power dissipation. The reversibility of ROM has not been yet realized in literature and hence, this paper presents a novel reversible ROM with its Complementary Metal Oxide Semiconductor (CMOS) realization. On the way to present the architecture of reversible ROM, we propose a new reversible gate named as Nowrin Papiya (NP) gate. All the proposed circuits and gates are realized with CMOS based pass transistor logic. Finally, an algorithm as well as several theorems on the numbers of gates, transistors and garbage outputs have been presented to show the optimality of the reversible ROM. Simulations using Microwind DSCH software has been shown to verify the correctness of the proposed design. The comparative results prove that the proposed designs are efficient and optimized in terms of numbers of gates, transistors, garbage outputs, quantum cost and delay.	algorithm;cmos;cpu power dissipation;computer simulation;correctness (computer science);pass transistor logic;read-only memory;reversible computing;semiconductor	Sadia Nowrin;Papiya Nazneen;Lafifa Jamal	2015	CoRR		electronic engineering;three-input universal logic gate;computer science;electrical engineering;algorithm	EDA	16.583711970818104	45.27296959398173	57330
ce6ee9b11777eb0ae70e4a4a9cd08ec74583d332	a heuristic for k-broadcasting in arbitrary networks	arbitrary network;graph theory;optimisation;network design;time measurement;time complexity;grid graph;network design model arbitrary network heuristic algorithm optimal k broadcast time grid graph two dimensional torus graph 1 broadcast algorithm;two dimensional torus graph;network design model;computational complexity;optimal k broadcast time;heuristic algorithms;1 broadcast algorithm;message passing;councils;intelligent networks broadcasting computer science heuristic algorithms mesh generation algorithm design and analysis councils time measurement;intelligent networks;computer science;broadcasting;mesh generation;algorithm design and analysis;telecommunication networks;heuristic algorithm;optimisation broadcasting computational complexity graph theory telecommunication networks message passing	In this paper, we present a heuristic algorithm for kbroadcasting in an arbitrary network. This heuristic generates optimal k-broadcast time in grid graph when k ≥ 2. In two-dimensional torus graph, it also generates optimal k-broadcast time when k ≥ 3, while giving a bound of m2 + n2 + 1 when k = 2, where m and n are the number of rows and columns in the graph. In practice, the new heuristic outperforms best known 1-broadcast algorithm for three different network design models. The new algorithm runs fast. The time complexity of the algorithm is O(R · m), where R represents the rounds of broadcasting, and m stands for the total number of edges in the graph.	algorithm;column (database);heuristic (computer science);lattice graph;network planning and design;time complexity;toroidal graph	Hovhannes A. Harutyunyan;Bin Shao	2003		10.1109/IV.2003.1217992	combinatorics;computer science;theoretical computer science;distributed computing	Theory	15.498074402178299	32.782728932662195	57644
35451a4c38b5ed560fa02c83fb4006bfd65109da	new approach to lut implementation and accumulation for memory-based multiplication	direct shift accumulation approach;digital signal processing;look up table;random access memory;computational delay;multiplying circuits;semiconductor memory;combinational circuit complexity;integrated memory circuits;table lookup costs digital signal processing combinational circuits delay throughput semiconductor memory system on a chip random access memory logic devices;shift save accumulation scheme;storage management;system on a chip;circuit complexity;table lookup circuit complexity combinational circuits digital arithmetic integrated memory circuits multiplying circuits storage management;arrays;computer architecture;logic gates;registers;high precision multiplication;adders;lut based multiplier implementation;digital arithmetic;memory size reduction;combinational circuit;table lookup;memory based multiplication;area delay product;direct shift accumulation approach lut based multiplier implementation look up table memory based multiplication memory size reduction combinational circuit complexity area delay product computational delay high precision multiplication shift save accumulation scheme input operand;input operand;logic devices;throughput;combinational circuits	A new approach to look-up-table (LUT) implementation for memory-based multiplication is presented, where the memory-size is reduced to half at the cost of some increase in combinational circuit complexity. The proposed design offers a saving of nearly 42% area and 38% area-delay product (ADP) at the cost of 6% increase in computational delay for memory-based multiplication of 8-bit inputs with 16-bit coefficient. For high-precision multiplication, a shift-save-accumulation scheme is proposed to accumulate the LUT outputs corresponding to the segments of input-operand, which requires nearly 1.5 times more area, but offers more than twice the throughput and nearly two-third the ADP of direct shift-accumulation approach.	16-bit;8-bit;circuit complexity;coefficient;combinational logic;integrated circuit;logic gate;lookup table;operand;throughput;tree accumulation;usb on-the-go	Pramod Kumar Meher	2009	2009 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2009.5117783	electronic engineering;parallel computing;computer science;theoretical computer science;combinational logic;algorithm	Arch	12.490392212662627	44.68990358025642	57659
a3260f453200fa0198bc6ac8817ff3b73dd1579a	rap-cla: a reconfigurable approximate carry look-ahead adder	adders delays power demand logic gates multiplexing error correction generators;quality of service approximate computing carry look ahead adder reconfigurable multi operating mode	In this brief, we propose a fast yet energy-efficient reconfigurable approximate carry look-ahead adder (RAP-CLA). This adder has the ability of switching between the approximate and exact operating modes making it suitable for both error-resilient and exact applications. The structure, which is more area and power efficient than state-of-the-art reconfigurable approximate adders, is achieved by some modifications to the conventional carry look ahead adder (CLA). The efficacy of the proposed RAP-CLA adder is evaluated by comparing its characteristics to those of two state-of-the-art reconfigurable approximate adders as well as the conventional (exact) CLA in a 15 nm FinFET technology. The results reveal that, in the approximate operating mode, the proposed 32-bit adder provides up to 55% and 28% delay and power reductions compared to those of the exact CLA, respectively, at the cost of up to 35.16% error rate. It also provides up to 49% and 19% lower delay and power consumption, respectively, compared to other approximate adders considered in this brief. Finally, the effectiveness of the proposed adder on two image processing applications of smoothing and sharpening is demonstrated.	32-bit;adder (electronics);approximation algorithm;carry-lookahead adder;image processing;rapid refresh;smoothing	Omid Akbari;Mehdi Kamal;Ali Afzali-Kusha;Massoud Pedram	2018	IEEE Transactions on Circuits and Systems II: Express Briefs	10.1109/TCSII.2016.2633307	error detection and correction;parallel computing;look-ahead;serial binary adder;logic gate;electronic engineering;smoothing;adder;carry-save adder;computer science	EDA	14.08608457293367	42.704208215688006	57809
14250079a8290845b303a47933d6f6167d3604d4	assigning labels in unknown anonymous networks (extended abstract)	asymptotic optimality;distributed computing;compact routing;interval routing;broadcasting;efficiency measurement	We consider the task of distributedly assigning distinct labels to nodes of an unknown anonymous network. A priori, nodes do not have any identities (anonymous network) and do not know the topology or the size of the network (unknown network). They execute identical algorithms, apart from a distinguished node, called the source, which starts the labeling process. Our goal is to assign short labels, as fast as possible. The quality of a labeling algorithm is measured by the range from which the algorithm picks the labels, or alternatively, the length of the assigned labels. Natural efficiency measures are the time, i.e., the number of rounds required for the label assignment, and the message and bit complexities of the label assignment protocol, i.e., the total number of messages (resp., bits) circulating in the network. We present label assignment algorithms whose time and message complexity are asymptotically optimal and which assign short labels. On the other hand, we establish inherent trade-offs between quality and efficiency for labeling algorithms.	asymptotically optimal algorithm	Pierre Fraigniaud;Andrzej Pelc;David Peleg;Stéphane Pérennes	2000		10.1145/343477.343527	real-time computing;computer science;theoretical computer science;distributed computing;broadcasting	Theory	17.55760343412778	34.26947656451473	58045
e7313fc5cd845f89f9c4db8649d3b89c02dc3c99	low-power data transfer and storage exploration for h.263 video decoder system	distributed memory;storage exploration;c specification;distributed memory systems;instruments;memory management;motion compensation;decoding;logic design;very large scale integration;storage management;videophone systems;formalized high level memory management;code standards;h 263 video decoding demonstrator application;transform coding;very large scale integrated;indexing terms;public domain;predicted frames;video coding;reference design;low power;worst case mode;prediction theory;telenor research;energy consumption;memory architecture;power system management;telecommunication standards;bidirectional control;decoding energy consumption memory management instruments memory architecture bidirectional control environmental management power system management logic design very large scale integration;atomium environment;predictive coding low power data transfer storage exploration h 263 video decoder system h 263 video decoding demonstrator application c specification telenor research public domain distributed memory organization power consumption reduction reference design worst case mode bidirectional frames predicted frames memory power consumption formalized high level memory management atomium environment motion compensated hybrid coding transform coding;transform coding video coding code standards telecommunication standards decoding storage management distributed memory systems image sequences motion compensation prediction theory;memory power consumption;distributed memory organization;power consumption;low power data transfer;bidirectional frames;environmental management;h 263 video decoder system;predictive coding;motion compensated hybrid coding;power consumption reduction;data transfer;image sequences	| We describe a power exploration methodology for data-dominated applications using a H.263 video decoding demonstrator application. The starting point for our exploration is a C speciication of the video decoder, available in the public domain from Telenor Research. We have transformed the data transfer scheme in the speciication and have optimised the distributed memory organisation. This results in a memory architecture with signiicantly reduced power consumption. For the worst-case mode using Predicted (P) frames, memory power consumption is reduced by a factor of 7 when compared to the reference design. For the worst-case mode using Predicted and Bi-directional (PB) frames, memory power consumption is reduced by a factor of 9. To achieve these results, we make use of our formalised high-level memory management methodology , partly supported in our ATOMIUM environment.	best, worst and average case;distributed memory;high- and low-level;memory management;memory organisation;reference design;video decoder	Lode Nachtergaele;Francky Catthoor;Bhanu Kapoor;Stefan Janssens;Dennis Moolenaar	1998	IEEE Journal on Selected Areas in Communications	10.1109/49.650925	embedded system;public domain;logic synthesis;real-time computing;transform coding;index term;distributed memory;telecommunications;computer science;electrical engineering;operating system;very-large-scale integration;motion compensation;memory management	EDA	12.71453085353494	39.89229376946917	58252
d11e992ea3a6680d2edc8e163f8b4497cbe5f89b	design of generic floating point multiplier and adder/subtractor units	pipelined architecture floating point fpga;detectors;floating point unit;virtex5 fpga;multiplying circuits;virtex2 fpga;adder subtractor units;multiplication operator;neck;clocks;generic floating point multiplier design;signal processing algorithms field programmable gate arrays frequency neck floating point arithmetic hardware computational modeling computer simulation detectors computer architecture;fpga;computer architecture;multiplication operation;leading one detector algorithm;computational modeling;multiplying circuits adders field programmable gate arrays floating point arithmetic;vhdl code;adders;virtex4 fpga;floating point;virtex5 fpga generic floating point multiplier design adder subtractor units generic floating point unit multiplication algorithm multiplication operation leading one detector algorithm vhdl code virtex2 fpga virtex4 fpga;floating point arithmetic;field programmable gate arrays;signal processing algorithms;frequency;generic floating point unit;computer simulation;high speed;algorithm design and analysis;multiplication algorithm;pipelined architecture;hardware	A high speed generic Floating Point Unit (FPU) consisting of a multiplier and adder/subtractor units is proposed. A novel multiplication algorithm is proposed and used in the multiplier implementation. The new algorithm depends on dividing the multiplication operation into several smaller multiplications performed in parallel. The output from these multiplications is then manipulated in a manner to give the final result of the original multiplication operation. The adder/subtractor unit is implemented using the Leading One Detector (LOD) algorithm. In order to achieve high maximum, speed, both units were deeply pipelined. The design is written using VHDL code and mapped to Virtex2, Virtex4 and Virtex5 FPGAs. Both units can operate at more than 400 MHz on Virtex4.	adder (electronics);field-programmable gate array;floating-point unit;multiplication algorithm;pipeline (computing);subtractor;vhdl	Lamiaa Sayed Abdel Hamid;Khaled Ali Shehata;Hassan El-Ghitani;Mohamed ElSaid	2010	2010 12th International Conference on Computer Modelling and Simulation	10.1109/UKSIM.2010.117	electronic engineering;parallel computing;computer hardware;computer science;subtractor;adder	EDA	11.396877284436744	44.91194181052716	58801
64ce8e09f9c98d9a70588a0a919f60663ef92d24	a parallel and area-efficient architecture for deblocking filter and adaptive loop filter	pixel wiener filter filtering decoding random access memory hardware computer architecture;filtering;random access memory;decoding;real time;video coding adaptive filters field programmable gate arrays hardware description languages parallel architectures parallel memories shared memory systems;hardware description languages;frequency 211 mhz parallel architecture deblocking filter adaptive loop filter video coding alf decoder shared hybrid organized memory architecture verilog hdl xilinx virtex 5 fpga;video coding;computer architecture;adaptive filters;shared memory systems;parallel architectures;memory architecture;pixel;wiener filter;field programmable gate arrays;parallel memories;hardware	Adaptive Loop Filter (ALF) has been developed lately to improve the video coding performance. It is inserted between deblocking and inter-prediction, which makes deblocking and ALF very time-critical because they are conducted sequentially. In this paper, we propose an efficient architecture integrating deblocking and ALF for the decoder. The architecture not only implements deblocking and ALF in parallel but also reduces area cost as much as possible. These are achieved by shared hybrid organized memory architecture and one-block-two-edge parallel strategy using a novel filter order. The proposed architecture is implemented in verilog HDL and can achieve real-time decoding for 1080p @ 30 fps applications by working at 211MHz in a Xilinx Virtex-5 FPGA.	data compression;deblocking filter;field-programmable gate array;hardware description language;real-time clock;verilog;window of opportunity	Juan Du;Lu Yu	2011	2011 IEEE International Symposium of Circuits and Systems (ISCAS)	10.1109/ISCAS.2011.5937723	filter;adaptive filter;embedded system;parallel computing;real-time computing;computer science;deblocking filter;wiener filter;hardware description language;pixel;field-programmable gate array;statistics	Arch	12.178005004622067	40.40718063689446	58818
c03f6861d5b8a3d0ab53e590fdde8fd937386388	a theoretical framework for quantum image representation and data loading scheme		Two fundamental problems exist in the use of quantum computation to process an image or signal. The first one is how to represent giant data, such as image data, using quantum state without losing information. The second one is how to load a colossal volume of data into the quantum registers of a quantum CPU from classical electronic memory. Researches on these two questions are rarely reported. Here an entangled state is used to represent an image (or vector) for which two entangled registers are used to store a vector component and its classical address. Using the representation, n 1 + n 2 + 8 qubits are used to store the whole information of the gray image that has a $2^{n_1 } \times 2^{n_2 } $ size at a superposition of states, a feat is not possible with a classic computer. The way of designing a unitary operation to load data, such as a vector (or image), into the quantum registers of a quantum CPU from electronic memory is defined herein as a quantum loading scheme (QLS). In this paper, the QLS with time complexity O(log2 N) is presented where N denotes the number of vector components, a solution that would break through the efficiency bottleneck of loading data. QLS would enable a quantum CPU to be compatible with electronic memory and make possible quantum image compression and quantum signal processing that has classical input and output.	central processing unit;common language infrastructure;computation;computer;digital image;image compression;input/output;quantum computing;quantum entanglement;quantum register;quantum state;quantum superposition;qubit;radiology;semiconductor memory;signal processing;time complexity;vector graphics	Ben-Qiong Hu;Xudong Huang;Rigui Zhou;Yanyu Wei;Qun Wan;Chao-Yang Pang	2013	Science China Information Sciences	10.1007/s11432-013-4866-x	quantum operation;discrete mathematics;quantum information;telecommunications;quantum t-design;computer science;amplitude damping channel;theoretical computer science;quantum network;quantum capacity;mathematics;qubit;quantum channel;quantum computer;quantum process;quantum algorithm;algorithm;one-way quantum computer;quantum mechanics;quantum phase estimation algorithm;quantum sort;quantum error correction	Theory	15.849413354158045	38.68659204396604	59000
a1321b98f0c831069b0683dd0e64b4356f076db7	on resource placements in 3d tori	ordinateur parallele;irregular torus;interconnection network;tore irregulier;deplacement distance d parfaite;toro;torus;tore;ordenador paralelo;tore regulier;parallel computer;interconnection networks;tore 3 dimensions;regular torus;red interconexion;perfect distance d placements;reseau interconnexion	Perfect distance-d placements in 3D tori is investigated. It is proved that there exists no linear perfect distance-1 placement other than the known ones so far. Perfect distance-d placements in 3D tori, where at least one of the dimensions is 2, are presented. Furthermore, quasi-perfect placements for 3D tori are introduced.		Bader F. AlBdaiwi;Bella Bose	2003	J. Parallel Distrib. Comput.	10.1016/S0743-7315(03)00122-9	discrete mathematics;parallel computing;topology;torus;mathematics;geometry	HPC	23.85121373567527	35.5143471700354	59229
0ddaa5ff4ef342023b7cbd5feb1ea6b22324267c	evolutionary algorithms and theirs use in the design of sequential logic circuits	state assignment;sequential circuits;evolvable hardware;qa75 electronic computers computer science;genetic algorithm;evolutionary algorithm;logic gate;finite state machine;state transition;tk electrical engineering electronics nuclear engineering	In this paper an approach based on an evolutionary algorithm to design synchronous sequential logic circuits with minimum number of logic gates is suggested. The proposed method consists of four main stages. The first stage is concerned with the use of genetic algorithms (GA) for the state assignment problem to compute optimal binary codes for each symbolic state and construct the state transition table of finite state machine (FSM). The second stage defines the subcircuits required to achieve the desired functionality. The third stage evaluates the subcircuits using extrinsic Evolvable Hardware (EHW). During the fourth stage, the final circuit is assembled. The obtained results compare favourably against those produced by manual methods and other methods based on heuristic techniques.	and gate;assignment problem;binary code;boolean algebra;circa;combinational logic;evolutionary algorithm;evolvable hardware;execution unit;finite-state machine;function-level programming;genetic algorithm;heuristic;lu decomposition;logic gate;sequential logic;software release life cycle;state transition table	Behloul Ali;A. E. A. Almaini;Tatiana Kalganova	2004	Genetic Programming and Evolvable Machines	10.1023/B:GENP.0000017009.11392.e2	genetic algorithm;logic gate;computer science;artificial intelligence;theoretical computer science;machine learning;evolutionary algorithm;sequential logic;digital electronics;algorithm	EDA	18.71119126094015	43.88245497751892	59700
5d4c8db5842a8633e8bbb910d5447ff69a2d52ad	memristor-cntfet based ternary logic gates		Abstract Multilevel electronic systems offer the reduction of implementation’ complexity, power consumption, and area. Ternary system is a very promising system where more information is represented in the same number of digits compared to the binary systems. In this paper, ternary logic gates and some of their ternary circuit applications are presented using memristors and CNTFET inverter. This integration between memristors and CNTFET offers low static power, small area and high performance. The proposed circuits do not require refreshment like the previously published circuits and are not initial state dependent because the memristors switch between the low resistance and high resistance states according to each input. In addition, we investigate the mathematical analysis of the proposed memristor ternary logic gates circuits. In the circuit simulations, a VTEAM model is used to verify the proposed circuits. Finally, a comparison between the proposed circuits with the previously published implementations is discussed showing better performance in terms of power, delay and area.	logic gate;memristor;three-valued logic	Nancy S. Soliman;Mohammed E. Fouda;Ahmed Gomaa Radwan	2018	Microelectronics Journal	10.1016/j.mejo.2017.12.008	memristor;engineering;electronic engineering;ternary operation;carbon nanotube field-effect transistor;electronic circuit;ternary numeral system;inverter;logic gate;binary number	EDA	16.283054555866105	45.51776142748602	59802
217af1561dca623bccd449b149e19f4b812fda4c	max-consensus in open multi-agent systems with gossip interactions		We study the problem of distributed maximum computation in an open multi-agent system, where agents can leave and arrive during the execution of the algorithm. The main challenge comes from the possibility that the agent holding the largest value leaves the system, which changes the value to be computed. The algorithms must as a result be endowed with mechanisms allowing to forget outdated information. The focus is on systems in which interactions are pairwise gossips between randomly selected agents. We consider situations where leaving agents can send a last message, and situations where they cannot. For both cases, we provide algorithms able to eventually compute the maximum of the values held by agents.	algorithm;computation;interaction;max;multi-agent system;randomness	Mahmoud Abdelrahim;Julien M. Hendrickx;W. P. M. H. Heemels	2017	2017 IEEE 56th Annual Conference on Decision and Control (CDC)	10.1109/CDC.2017.8264362	mathematical optimization;computer science;wireless sensor network;algorithm design;computation;distributed computing;gossip;pairwise comparison;control system;multi-agent system	AI	17.662224950416636	34.45588807139878	59877
071f811955c970410d61143dd33208cb446e154e	ultra-low power dlms adaptive filter for hearing aid applications	ultra low power;least mean square;hearing aids;least mean squares methods;adaptive filters;mos digital integrated circuits;adaptive signal processing;parallel architectures;digital filters;low power electronics;digital signal processing chips;power delay product;parallel architecture;adaptive filters auditory system cmos logic circuits mos devices parallel architectures voltage throughput delay clocks signal processing;digital signal processing chips low power electronics adaptive filters least mean squares methods hearing aids adaptive signal processing parallel architectures digital filters mos digital integrated circuits pipeline processing;adaptive filter;hearing aid;pipeline processing;sub cmos;sub threshold operation;dlms adaptive filter;sub pseudo nmos;22 khz ultra low power dlms adaptive filter delayed least mean square adaptive filter hearing aid applications sub threshold operation parallel architecture pseudo nmos logic style clock rate reduction supply voltage reduction power delay product voice signal processing 400 mv	We present an ultra-low power DLMS (delayed least mean square) adaptive filter working in the sub-threshold region for hearing aid applications. Sub-threshold operation was accomplished by using a parallel architecture with pseudo NMOS logic style. The parallel architecture enabled us to run the system at a lower clock rate with a reduced supply voltage, while maintaining the same throughput. Pseudo NMOS logic operating in the sub-threshold region (SubPseudo NMOS) provided better power-delay product than subthreshold CMOS (Sub-CMOS) logic. Simulation results show that the system can process voice signals at a throughput of 22kHz with a supply voltage of 400mV and achieve 91% improvement in energy compared to the non-parallel architecture using standard CMOS logic.	adaptive filter;cmos;clock rate;mean squared error;nmos logic;parallel computing;power–delay product;sampling (signal processing);simulation;throughput	Chris Hyung-Il Kim;Kaushik Roy	2001		10.1145/383082.383182	adaptive filter;embedded system;electronic engineering;real-time computing;computer science;engineering	Arch	13.695782130076177	46.015638790003386	59924
d4b17e2db97896a7413b91428f5a1b7f56df52b5	space complexity of self-stabilizing leader election in population protocol based on k-interaction	theoretical computer science;computer science all	Population protocol (PP) is a distributed computing model for passively mobile systems, in which a computation is executed by interactions between two agents. This paper is concerned with an extended model, population protocol based on interactions of at most k agents (PP k ). Beauquier et al. (2012) recently introduced the model, and showed a hierarchy of computational powers of PP k with respect toak; a PP k + 1 is strictly more powerful than a PP k . Motivated by a further understanding of the model, this paper investigates the space complexity of PP k for self-stabilizing leader election (SS-LE), which is a fundamental problem for a distributed system. Cai et al. (2012) showed that the space complexity of SS-LE for n agents by a PP (i.e., PP2) is exactly n. This paper shows that the space complexity of SS-LE for n agents by a PP k is exactly i¾?(n '—' 1)/(k '—' 1)i¾? + 1.	dspace;leader election;population protocol	Xiaoguang Xu;Yukiko Yamauchi;Shuji Kijima;Masafumi Yamashita	2013		10.1007/978-3-319-03089-0_7	computer science;artificial intelligence;theoretical computer science	Theory	16.745339117505985	35.0697245739148	60105
3aba9c3213cb6f5d87feb8be86cfd2880c197357	a hardware implementation of a content-based motion estimation algorithm for real-time mpeg-4 video coding	content based;tecnologia electronica telecomunicaciones;real time;motion estimation;video coding;mpeg 4;tecnologias;grupo a;power efficient;hardware implementation	Power efficiency and real-time processing capability are two major issues in today's mobile video applications. We proposed a novel Motion Estimation (ME) engine for power-efficient real-time MPEG-4 video coding based on our previously proposed content-based ME algorithm [8], [13]. By adopting Full Search (FS) and Three Step Search (TSS) alternatively according to the nature of video contents, this algorithm keeps the visual quality very close to that of FS with only 3% of its computational power. We designed a flexible Block Matching (BM) Unit with 16-PE SIMD data path so that the adaptive ME can be performed at a much lower clock frequency and hardware cost as compared with previous FS based work. To reduce the energy cost caused by excessive external memory access, on-chip SRAM is also utilized and optimized for parallel processing in the BM Unit. The ME engine is fabricated with TSMC 0.18 μm technology. When processing QCIF (15 fps) video, the estimated power is 2.88 mW@4.16 MHz (supply voltage: 1.62 V). It is believed to be a favorable contribution to the video encoder LSI design for mobile applications.	algorithm;motion estimation;real-time transcription	Shen Li;Takeshi Ikenaga;Hideki Takeda;Masataka Matsui;Satoshi Goto	2006	IEICE Transactions	10.1093/ietfec/e89-a.4.932	embedded system;computer vision;real-time computing;computer hardware;telecommunications;computer science;coding tree unit;motion estimation;block-matching algorithm;motion compensation;mpeg-4;multiview video coding	Vision	13.426523037327495	40.5090465607956	60190
08afb859211da883dd41804656a3e6fd8100c9cb	a note on digital filter implementation using hybrid rns-binary arithmetic	vlsi architectures;inner product;digital filter;digital filters;inner products;rns arithmetic;lookup table;vlsi architecture	Binary logic implementation of residue arithmetic using longer intermediate pseudo-residues has been shown to offer advantages over standard binary and lookup-table RNS realizations. In this note, we show that a small modification to one such proposed scheme leads to even greater advantages.	binary number;bitwise operation;digital filter;lookup table;residue number system	Behrooz Parhami	1996	Signal Processing	10.1016/0165-1684(96)00031-X	arithmetic;electronic engineering;digital filter;computer science;electrical engineering;theoretical computer science;saturation arithmetic;mathematics	Mobile	15.864794143823268	43.93934856611863	60268
429d88889bdda9f517110ffb96a42f86e4554e2d	rendezvous of mobile agents without agreement on local orientation	distributed coordination;mobile agents;connected graph;synchronization;network model;incomparable labels;leader election;rendezvous;qualitative modeling;mobile agent;anonymous networks	The exploration of a connected graph by multiple mobile agents has been previously studied under different conditions. A fundamental coordination problem in this context is the gathering of all agents at a single node, called the Rendezvous problem. To allow deterministic exploration, it is usually assumed that the edges incident to a node are locally ordered according to a fixed function called local orientation. We show that having a fixed local orientation is not necessary for solving rendezvous; Two or more agents having possibly distinct local orientation functions can rendezvous in all instances where rendezvous is solvable under a common local orientation function. This result is surprising and extends the known characterization of solvable instances for rendezvous and leader election in anonymous networks. On one hand, our model is more general than the anonymous port-to-port network model and on the other hand it is less powerful than qualitative model of Barrière et al. [4,9] where the agents have distinct labels. Our results hold even in the simplest model of communication using identical tokens and in fact, we show that using two tokens per agent is necessary and sufficient for solving the problem.	connectivity (graph theory);decision problem;leader election;local variable;mobile agent;network model	Jérémie Chalopin;Shantanu Das	2010		10.1007/978-3-642-14162-1_43	synchronization;simulation;computer science;connectivity;network model;mobile agent;leader election;mathematics;distributed computing	AI	17.826266127187356	34.90676276326037	60410
2cf3208aefcb41c073f67de53683b052c93ea30a	ires: an integrated software and hardware interface framework for reconfigurable embedded system	software;field programmable gate array;evaluation performance;conception conjointe;video compression applications;hardware software codesign;performance evaluation;data compression;systeme embarque;integrated circuit;logiciel;diseno conjunto;reconfigurable architectures;evaluacion prestacion;circuito integrado;task oriented operating objects;hardware management unit;red puerta programable;reseau porte programmable;video compression applications hardware software codesign embedded system ires system reconfigurable computing architectures central processing unit reconfigurable field programmable gate array integration linker boot loader task oriented operating objects hardware management unit;embedded system;video coding;embedded systems;codificacion;compression image;image compression;codesign;object oriented;reconfigurable field programmable gate array;coding;oriente objet;logicial;ires system;video coding data compression embedded systems field programmable gate arrays hardware software codesign reconfigurable architectures;procesador;field programmable gate arrays;processeur;boot loader;orientado objeto;architecture reconfigurable;reconfigurable computing architectures;integration linker;processor;circuit integre;central processing unit;codage;compresion imagen	Hardware/software co-design is an interesting topic for most embedded system architects. However, designers find integrating hardware and software communications interface challenging. A framework for integrating the software and hardware communication interface for computing in reconfigurable embedded systems, called IRES, is proposed. The framework supports reconfigurable computing architectures, based on traditional central processing unit and the reconfigurable field programmable gate array, and composed of the integration linker, the boot loader, small task-oriented operating objects and the hardware management unit. The integration linker enables the IRES to link hardware net-list files and tasks into one execution file, called the executor, constructed with the boot loader, the task-oriented operating kernel, the application tasks and the accelerating hardware functions. Task and hardware functions are segregated by program segment prefixes, designed to record interaction information of hardware and software resources. When the executor operates on the target-embedded environment, the implicit hardwire-call will be supported to invoke hardware functions in the task codes. The IRES successfully implements in the realised hardware platform, and this work verifies communication effectiveness between hardware and software through video compression applications.	booting;central processing unit;code;control unit;data compression;electrical connector;embedded system;field-programmable gate array;hardware interface design;integrated software;interaction information;operating system;reconfigurable computing	Jih-Ching Chiu;Ta-Li Yeh	2010	IET Computers & Digital Techniques	10.1049/iet-cdt.2009.0010	hardware compatibility list;embedded system;parallel computing;real-time computing;hardware acceleration;computer science;operating system;hardware architecture;open source hardware;hardware register;field-programmable gate array	Arch	10.456992293047133	40.926431870808855	60449
ac40d789f4024790c7b06db0dab8e5b6f0cd06e1	a fast pipelined fft unit	reconfiguration;silicon;processing element;inf;optimisation;multiplier;fault tolerance fast pipelined fft unit vlsi butterfly processing element serial arithmetic full fractional two s complement form serial complex multiplier optimisation complex multiplication partial products multiplier performance evaluation reconfiguration fault detection;image coding;vlsi butterfly processing element;performance evaluation;fault tolerant;clocks;very large scale integration;complex multiplication;pipeline architecture;fft;full fractional two s complement form;fast fourier transform;computer architecture;partial products;fault tolerant computing;parallel architectures;serial arithmetic;fault detection;fault tolerance;fault detection computer architecture performance evaluation very large scale integration clocks silicon image coding switches arithmetic merging;fast pipelined fft unit;merging;vlsi;fast fourier transforms;arithmetic;digital arithmetic;serial complex multiplier;switches;optimisation hypercube networks digital arithmetic fast fourier transforms parallel architectures fault tolerant computing;hypercube networks	This paper is dedicated to the presentation ofthe architecture ofa VLSZ butterfly processing element, for computing FFT in aerial arithmetic. This butterfly PE uses complez samples and weights, with real and imaginary parts repreaented separately in full fractional two's complement form. The PE is based on a compact serial/pamllel to serial complez multiplier, which optimises complez multiplication by merging the generation and accumulation of partial products. The structure o j the multiplier and the PE is presented; their performances are evaluated, including the possibility of reconfiguration, fault detection and fault tolemnce.	aerial photography;fast fourier transform;fault detection and isolation;fault tolerance;imaginary time;performance;tree accumulation;two's complement	Luca Breveglieri;Vincenzo Piuri	1994		10.1109/ASAP.1994.331808	fast fourier transform;computer architecture;parallel computing;real-time computing;computer science;theoretical computer science;algorithm	Robotics	12.955831248132203	43.27785169688935	61112
cf7ecfa8e0d0aa84919eea33055df7c376bc76e0	realizing complex boolean functions with simple groups	boolean function;simple group	"""In an earlier paper, Maurer and Rhodes (1965) proved the following result: Let G be a finite simple nonabelian group and f: G"""" --> G a map of the set of n-tuples of elements of G into G. Then there is an integer m and an m-tuple t ( t I , t 2 , . . . , t m ) of elements of G 0 { 1 , . . , n } such that for all g = (g l , . . . , g , ) C G ~, f ( g ) --E ( t , g) , where E ( t , g) is the product gl p . . . g~' C G of elements g t C G defined by letting for eachj C {1, . . . , m} g j = t~ if tj C G org j = gtj i f t~ C {1, --,n}. While this result is of some intrinsic algebraic interest, the motivation behind the result was its application to automata theory. I t is the purpose of this paper to explore this connection, in particular, we set forth here a new method for the generation of boolean functions. Currently, the maior method used for that purpose is one developed by Shannon which, while well-known, can require in hardware realizations circuitry too cumbersome for some applications. The alternative method set forth here involves the use of a fixed simple nonabelian group machine (which can be realized in hardware by n flip-flops, where 2"""" exceeds the order of the group involved) and, for each boolean function, a storage sequence, the length of which depends upon the boolean function and the group machine used. A few examples will convince the reader that there is little correlation between the length of storage sequences and the amount of circuitry necessary to reMize the corresponding functions"""	automata theory;electronic circuit;flops;flip-flop (electronics);shannon (unit);ueli maurer (cryptographer)	Kenneth Krohn;Ward Douglas Maurer;John L. Rhodes	1966	Information and Control	10.1016/S0019-9958(66)90229-4	implicant;boolean algebra;boolean circuit;and-inverter graph;combinatorics;circuit minimization for boolean functions;discrete mathematics;reed–muller expansion;boolean domain;boolean expression;simple group;product term;computer science;maximum satisfiability problem;karp–lipton theorem;boolean algebras canonically defined;mathematics;boolean function;complete boolean algebra;algorithm;two-element boolean algebra;free boolean algebra;parity function;algebra	Theory	19.574623626223797	44.119914037652904	61340
fc4581199abd055672fcd3c9a4674065dd718732	read-polarity-once boolean functions	logic design boolean functions;boolean functions algorithm design and analysis data structures runtime equations benchmark testing input variables;logic design;boolean functions;read polarity once boolean functions domain transformation binate variables independent unate variables binate functions rpo functions positive polarity negative polarity logic synthesis ro expressions isf incompletely specified boolean functions boolean expression exact factoring algorithms;read polarity once function boolean function factoring logic synthesis incompletely specified functions read once function	Efficient exact factoring algorithms are limited to read-once (RO) functions, where each variable appears once in the final Boolean expression. However, these algorithms present two important constraints: (1) they do not consider incompletely specified Boolean functions (ISF); and (2) they are not suitable for binate functions. To overcome the first drawback, an algorithm that finds RO expressions for ISF, whenever possible, is proposed. With respect to the second limitation, we propose a domain transformation that splits existing binate variables into two independent unate variables. Such domain transformation leads to ISF, which can be efficiently factored by applying the proposed algorithm. The combination of both contributions gives optimal results for a novel broader class of Boolean functions called read-polarity-once (RPO) functions, where each polarity (positive and negative) of a variable appears at most once in the factored form. Experimental results carried out over ISCAS'85 benchmark circuits have shown that RPO functions are significantly more frequent than RO functions.	algorithm;benchmark (computing);boolean algebra;boolean expression;ink serialized format;integer factorization;path ordering (term rewriting);polish notation;unate function;vii	Vinicius Callegaro;Mayler G. A. Martins;Renato P. Ribas;André Inácio Reis	2013	2013 26th Symposium on Integrated Circuits and Systems Design (SBCCI)	10.1109/SBCCI.2013.6644862	boolean algebra;embedded system;boolean circuit;and-inverter graph;circuit minimization for boolean functions;logic synthesis;boolean network;boolean domain;boolean expression;product term;standard boolean model;computer science;maximum satisfiability problem;theoretical computer science;complexity index;boolean function;complete boolean algebra;algorithm;two-element boolean algebra;parity function	EDA	18.24053361153314	46.29428864148816	61382
6a2dbb234f99bbe8a1f69f36c133e6dffd0be8e2	necessary and sufficient conditions for t-diagnosability of multiprocessor computer systems for various models of nonreliable testing established using the system graph-theoretical model		Consideration was given to the graph-theoretical model of self-diagnosis at the system level under multiple permanent faults. A group of testing models based on complete unreliable tests was analyzed. For the models of this group, the necessary and sufficient conditions for t-diagnosability without repair were established. A critical review of the results obtained in the publications of various authors was given, and the noticed errors concerning the proof of the conditions for t-diagnosability without repair were corrected.	multiprocessing;theory	Yurij K. Dimitriev	2016	Automation and Remote Control	10.1134/S0005117916060096	reliability engineering;simulation;engineering;algorithm	Embedded	23.86900894286177	44.74004789862193	61861
c2a646201e1a5730ab2f7125c2e136a199ee1c5e	a 1080p h.264/avc baseline residual encoder for a fine-grained many-core system	voltage control;quantization;variable length codes;data compression;parallel programming asap cavlc fine grained many core system h 264 avc;h 264 avc;parallel programming;adaptive codes;cavlc;encoding automatic voltage control transforms program processors quantization streaming media parallel processing;quantisation signal;video coding adaptive codes data compression high definition television image sequences microprocessor chips quantisation signal transforms variable length codes;video coding;power 293 mw h 264 avc baseline residual encoder programmable fine grained many core processing array software encoder integer transform context based adaptive variable length coding functions quantization fine grained data task level parallelism processors video sequences high definition television average power consumption workload based optimal clock frequency ti c642 digital signal processing platform multiple data architecture energy efficiency area efficiency video encoding application specific integrated circuit application specific hardware;automatic voltage control;streaming media;fine grained many core system;transforms;asap;parallel programs;encoding;program processors;parallel processing;high definition television;microprocessor chips;image sequences	This paper presents a baseline residual encoder for H.264/AVC on a programmable fine-grained many-core processing array that utilizes no application-specific hardware. The software encoder contains integer transform, quantization, and context-based adaptive variable length coding functions. By exploiting fine-grained data and task-level parallelism, the residual encoder is partitioned and mapped to an array of 25 small processors. The proposed encoder encodes video sequences with variable frame sizes and can encode 1080p high-definition television at 30 f/s with 293 mW average power consumption by adjusting each processor to workload-based optimal clock frequencies and dual supply voltages-a 38.4% power reduction compared to operation with only one clock frequency and supply voltage. In comparison to published implementations on the TI C642 digital signal processing platform, the design has approximately 2.9-3.7 times higher scaled throughput, 11.2-15.0 times higher throughput per chip area, and 4.5-5.8 times lower energy per pixel. Compared to a heterogeneous single instruction, multiple data architecture customized for H.264, the presented design has 2.8-3.6 times greater throughput, 4.5-5.9 times higher area efficiency, and similar energy efficiency. The proposed fine-grained parallelization methodology provides a new approach to program a large number of simple processors allowing for a higher level of parallelization and energy-efficiency for video encoding than conventional processors while avoiding the cost and design time of implementing an application specific integrated circuit or other application-specific hardware.	algorithm;application-specific integrated circuit;array data structure;baseline (configuration management);central processing unit;clock rate;context-adaptive variable-length coding;data architect;data compression;digital signal processing;digital signal processor;encode;encoder;general-purpose modeling;h.264/mpeg-4 avc;hdmi;manycore processor;multiprocessing;parallel computing;pixel;quantization (signal processing);simd;ti-nspire series;task parallelism;throughput;variable-length code	Zhibin Xiao;Bevan M. Baas	2011	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2011.2133290	data compression;parallel processing;parallel computing;real-time computing;quantization;computer hardware;computer science;context-adaptive variable-length coding;encoding	Arch	12.068202464234888	40.35242688558189	62191
42fae98882b2d0599b2bdeadccc95b942c36fe1a	distributed computing with channel noise		A group of n users want to run a distributed protocol π over a network where communication occurs via private point-to-point channels. Unfortunately, an adversary, who knows π, is able to maliciously flip bits on the channels. Can we efficiently simulate π in the presence of such an adversary? We show that this is possible, even when L, the number of bits sent in π, and T , the number of bits flipped by the adversary are not known in advance. In particular, we show how to create a robust version of π that 1) fails with probability at most δ, for any δ > 0; and 2) sends Õ(L+ T ) bits, where the Õ notation hides a log(nL/δ) term multiplying L. Additionally, we show how to improve this result when the average message size α is not constant. In particular, we give an algorithm that sends O(L(1 + (1/α) log(nL/δ) + T ) bits. This algorithm is adaptive in that it does not require a priori knowledge of α. We note that if α is Ω (log(nL/δ)), then this improved algorithm sends only O(L + T ) bits, and is therefore within a constant factor of optimal.	adversary (cryptography);algorithm;distributed computing;free protocol;graph coloring;point-to-point protocol;simulation	Abhinav Aggarwal;Varsha Dani;Thomas P. Hayes;Jared Saia	2017	IACR Cryptology ePrint Archive		discrete mathematics;adversary;mathematics;flip;communication channel	Theory	16.51729277469267	33.86106669060453	62277
0a336aff264e0b65044e8eb9d672fdef9c4d9c36	"""corrections to """"an rns to binary converter in a three moduli set with common factors"""""""	common factor;equations cathode ray tubes hardware;cathode ray tubes;hardware		residue number system	B. Premkumar	2004	IEEE Trans. on Circuits and Systems	10.1109/TCSII.2003.821518	cathode ray tube;electronic engineering;computer science;electrical engineering;physics	EDA	14.491016120865723	44.68873313085329	62396
7e63f24479d794ffe3fff91a0e21624b47dfdcbb	a novel cavlc architecture for h.264 video encoding at high bit-rate	pipeline techniques;encoding bit rate engines codecs pipeline processing clocks frequency real time systems hardware delay;variable length codes;codecs;clocks;bitrates;video coding variable length codes video codecs;real time;component level parallelism;variable length code;bit rate;video coding;engines;context based adaptive variable length codes;syntax coding;syntax coding cavlc architecture h 264 video encoding variants context based adaptive variable length codes bitrates component level parallelism pipeline techniques video data codec;codec;video codecs;variants;h 264 video encoding;cavlc architecture;video data;frequency;encoding;pipeline processing;hardware;real time systems	In H.264/AVC and the variants, the coding of context-based adaptive variable length codes (CAVLC) is one of the demanding operations, particularly for high bitrates such as 100 Mbps. This paper presents a novel architecture that exploits component-level parallelism and pipeline techniques capable of processing high-bitrate video data in a macroblock(MB)-level pipelined CODEC architecture. Additionally, some techniques for efficient CAVLC coding is presented because CAVLC is a dominant part of the syntax coding. The resulting architecture, merged in a MB-level pipelined CODEC system, is capable of coding up to 100 Mbps bitstreams in real-time, thus, accommodating the real-time encoding of 1080p 60 Hz video.	baseline (configuration management);code word;codec;coefficient;context-adaptive variable-length coding;data compression;data rate units;encode;experiment;h.264/mpeg-4 avc;macroblock;megabit;parallel computing;pipeline (computing);rlc circuit;real-time clock;streaming media;variable-length code	Yongseok Yi;Byung Cheol Song	2008	2008 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2008.4541460	codec;parallel computing;real-time computing;computer hardware;computer science;context-adaptive variable-length coding;context-adaptive binary arithmetic coding;statistics	Arch	12.265208157017538	40.02235930580165	62629
2afae566877d7cccfb735e0c9d0cd59c8ef53b51	an o(1) time parallel algorithm for the dominance counting and 3d block-based medial axis transform on arob	medial axis transform;parallel algorithm;concurrent computing;image processing;application software;binary image;optical computing;phase change random access memory;null;three dimensional;computer vision;shape;optical arrays;pixel;parallel algorithms optical arrays image processing concurrent computing optical computing shape computer vision application software pixel phase change random access memory;parallel algorithms	In this paper, we present a constant time algorithm for solving three dimensional block-based medial axis transform (3D BB-MAT, for short) on the array with reconfigurable optical buses (AROB). The main contribution of this work is to exploit the dominance counting method for solving the 3D BB-MAT that was never done previously in the literature. With the advantages of both optical transmission and electronic computation on the AROB model, the constant algorithm is proposed for the medial axis transform of a 3D N x N x N binary image on a 4D AROB using N^4 processors. To the best of our knowledge, this presented result of the dominance counting technique for medial axis transform of block-based is the best O(1) time algorithm known.	apache axis;binary image;central processing unit;computation;medial graph;parallel algorithm;time complexity	Shih-Ying Lin;Shi-Jinn Horng;Tzong-Wann Kao;Yuh-Rau Wang	2005	Sixth International Conference on Parallel and Distributed Computing Applications and Technologies (PDCAT'05)	10.1109/PDCAT.2005.87	computer vision;parallel computing;concurrent computing;image processing;computer science;theoretical computer science;distributed computing;parallel algorithm	Robotics	11.824092788633743	35.499801213384266	62759
641980a60d72d5f8fe142480de202c279b410d05	a parallel matching algorithm based on image gray scale	computers;image gray scale matching;image matching;data division processing algorithm;parallel matching algorithm;artificial neural networks;message passing interface;parallel image processing;pixel;cluster system;message passing;cluster parallel processing;message passing interface parallel matching algorithm cluster parallel processing image gray scale matching data division processing algorithm mpi parallel image processing;mpi;parallel processing concurrent computing clustering algorithms image processing image matching hardware information science large scale systems workstations digital images;correlation;parallel processing image matching message passing;parallel processing;hardware	In this paper, the idea of cluster parallel processing is taken in image gray scale matching. A data division processing algorithm based on MPI is applied to the known image gray scale matching. Experiment results show that parallel processing can shorten the overhead of gray scale matching significantly, and high speedup and efficiency can be acquired. This paper is expected to take technical insights into further investigation for parallel image processing based on cluster system.	algorithm;grayscale;image processing;overhead (computing);parallel computing;speedup	Liang Zong;Yanhui Wu	2009	2009 International Joint Conference on Computational Sciences and Optimization	10.1109/CSO.2009.131	parallel processing;parallel computing;computer science;message passing interface;theoretical computer science;distributed computing;artificial neural network	Robotics	11.216390278312906	35.54293648452586	63441
0e30f7d7cd6ef80666c3149939d581d0b6ac1455	quantum fpga architecture design	logic design;four qubit quantum fourier transform quantum fpga architecture design qfpga architecture programmable quantum computing hybrid architecture measurement based quantum computation qubus system quantum logic blocks qlb quantum routing channels qrc four qubit phase shift module;quantum fourier transform quantum fpga architecture qlb qrc grover s algorithm;quantum computing field programmable gate arrays fourier transforms logic design;fourier transforms;field programmable gate arrays;quantum computing	A Quantum FPGA (QFPGA) architecture is presented for programmable quantum computing, which is a hybrid architecture combining the advantages of the measurement-based quantum computation and the qubus system. QFPGA consists of Quantum Logic Blocks (QLBs) and Quantum Routing Channels (QRCs). The QLB is used to realize a small quantum logic while the QRC is to combine them properly for larger logic realization. There are two types of buses in QFPGA, the local bus in the QLB and the global bus in the QRC, which are to generate the cluster states and general multiqubit rotations around the z axis respectively. However for some applications such as Grover's algorithm and n-qubit quantum Fourier transform, one QLB can be configured for four-qubit phase shift module and four-qubit quantum Fourier transform respectively.	apache axis;computation;field-programmable gate array;grover's algorithm;local bus;quantum fourier transform;quantum computing;quantum logic;qubit;routing	Jialin Chen;Lingli Wang;Bin Wang	2013	2013 International Conference on Field-Programmable Technology (FPT)	10.1109/FPT.2013.6718386	embedded system;fourier transform;quantum fourier transform;quantum information;logic synthesis;computer science;theoretical computer science;quantum network;quantum circuit;quantum convolutional code;qubit;quantum computer;quantum algorithm;one-way quantum computer;quantum phase estimation algorithm;field-programmable gate array;quantum gate;quantum error correction	EDA	10.088420907680634	45.78912949928356	63868
a78f37abd7267324b9b9ca9f3ec0b242f5b53479	leveraging channel diversity to gain efficiency and robustness for wireless broadcast	different algorithm;log log n;leveraging channel diversity;wireless broadcast;primary question;single communication channel;unavoidable cost;large wireless network;lower bound showing;multiple communication channel	This paper addresses two primary questions: (i) How much faster can we disseminate information in a large wireless network if we have multiple communication channels available (as compared to relying on only a single communication channel)? (ii) Can we still disseminate information reliably, even if some subset of the channels are disrupted? In answer to the first question, we reduce the cost of broadcast to O(log log n) rounds/hop, approximately, for sufficiently many channels. We answer the second question in the affirmative, presenting two different algorithms, while at the same time proving a lower bound showing that disrupted channels have unavoidable costs. In more detail, we present upper and lower bounds for the multihop broadcast problem in the tdisrupted radio network model. This model assumes that in every round: each processes can participate on 1 out of C available communication channels, of which up to t < C might be locally disrupted, preventing communication. This model captures the unpredictable message loss that plagues real radio networks. We begin by studying the case with no disruption (t = 0) and present a randomized algorithm that solves broadcast in O((D + log n)(log C + logn C ) rounds, w.h.p., where D is the network diameter and n is the network size. Notice, for a single channel (C = 1), our algorithm has the same running time as the canonical Bar-Yehuda et al. algorithm [3], but as the number of channels increases so does our algorithm’s performance advantage. We then prove that for a sufficiently large number of channels, our algorithm is within a O(log log n) factor of optimal. Having shown that multiple channels yield efficiency, we next turn our attention to showing that they also yield robustness. We now consider a setting with adversarial disruption (t > 0) and a common source of randomness. We present a randomized algorithm that solves broadcast in O((D + log n)(C log C log log C C−t + logn C−t )) rounds. For t up to a constant factor of C, this algorithm performs only a factor of O(log log C) slower than the no disruption case: that is, even with significant disruption, our multi-channel algorithm still outperforms solutions that assume a single, perfectly reliable channel. For completeness, we conclude by considering the case with disruption and no common randomness. We demonstrate a clear separation with the common randomness case by proving a lower bound of Ω((D+log n) Ct C−t ) rounds, and then presenting an almost matching randomized upper bound that solves broadcast in O((D + log n) Ct C−t log ( n t )) rounds, w.h.p. Contact author: Calvin Newport Address: MIT CSAIL, the Stata Center 32 Vassar St. (32-G918) Cambridge, MA 02139 E-mail: cnewport@csail.mit.edu Phone: 617-253-7328 Regular paper ∗Ben-Gurion University, dolev@cs.bgu.ac.il †National University of Singapore, seth.gilbert@comp.nus.edu.sg ‡University of Winnipeg, majid@ubc.ca §MIT CSAIL, cnewport@csail.mit.edu	ct scan;channel (communications);denial-of-service attack;mit computer science and artificial intelligence laboratory;network model;randomized algorithm;randomness;robustness (computer science);stata;time complexity	Shlomi Dolev;Seth Gilbert;Majid Khabbazian;Calvin C. Newport	2011		10.1007/978-3-642-24100-0_25	telecommunications;computer science;data mining;distributed computing	Theory	17.969531293615617	32.9488180571746	63965
e09c54e4debae32b6f77a5ab89ecdfff694222ba	atomic and optical realizations of cluster quantum computation			computation	Jaewoo Joo	2007				Theory	17.385322888179083	41.969129438454814	64385
0676f562dd12249f709e23c6a25722e80229443c	smoothed analysis of dynamic networks	smoothed analysis;dynamic networks;distributed algorithms;flooding;aggregation;random walks	In this paper, we generalize the technique of smoothed analysis to apply to distributed algorithms in dynamic networks in which the network graph can change from round to round. Whereas standard smoothed analysis studies the impact of small random perturbations of input values on algorithm performance metrics, our proposed dynamic network version of smoothed analysis studies the impact of random perturbations of the underlying changing network topologies. Similar to the original application of smoothed analysis, our goal is to study whether known strong lower bounds in these models are robust or fragile: do they withstand small (random) perturbations, or do such deviations push the graphs far enough from a precise pathological instance to enable much better performance? Fragile lower bounds are likely not relevant for real-world deployment, while robust lower bounds represent a true difficulty caused by dynamic behavior. We apply this technique to three standard dynamic network problems with known strong worst-case lower bounds: random walks, flooding, and aggregation. We prove that these bounds provide a spectrum of robustness when subjected to smoothing—some are fragile (random walks), some are moderately fragile (flooding), and some are robust (aggregation).	best, worst and average case;distributed algorithm;network topology;robustness (computer science);smoothed analysis;smoothing;software deployment	Michael Dinitz;Jeremy T. Fineman;Seth Gilbert;Calvin C. Newport	2017	Distributed Computing	10.1007/s00446-017-0300-8	mathematical optimization;statistics	Theory	19.343065281851143	35.85711140304437	64543
05e15465c33023dc8096257e08c043a1fe76088a	a design of high-performance multiplier for digital video transmission	multiplying circuits;visual communication;high level synthesis;fir filter;digital video;fir filters;high performance;design methodology	| A high performance design methodology is described for a multiplier to be used dedicatedly for digital video transmission. The key factor for such a multiplier is to operate at the speed of 30{ 100MHz but with the precision of 8{10 bits, since it is intended for FIR ltering of digital video data. In terms of implementing an FIR lter with more than ten taps, the same number of multipliers are required to be integrated. Moreover, for the preloadability of coe cients to the lter, each coe cient can be treated as a constant during the ltering operation. Motivated by these requirements and functionalities, a novel multiplier architecture is described, which is to be synthesized with the use of a high level synthesis tool PARTHENON in conjunction with manually designed macroblocks. Design results of the multiplier are also shown.	digital video;finite impulse response;high-level programming language;high-level synthesis;macroblock;naruto shippuden: clash of ninja revolution 3;performance;requirement;very-large-scale integration;wiring	Keisuke Okada;Shun Morikawa;Isao Shirakawa;Sumitaka Takeuchi	1995		10.1145/224818.224947	electronic engineering;computer hardware;computer science;finite impulse response;computer graphics (images)	EDA	11.417203292028653	42.29039419161295	64559
3bf8972664359838e185b27ed351a30a539203c8	an efficient memory-based fft architecture	random access storage fast fourier transforms performance evaluation vlsi digital signal processing chips;random access memory;ram;performance evaluation;chaos;very large scale integration;memory size requirement reduction;memory based fft architecture;maintenance engineering;radix 2 fft architecture;memory architecture;multiplier utilization;signal processing;pipelines;ofdm modulation;proceedings paper;vlsi;fast fourier transforms;random access storage;digital signal processing chips;memory architecture pipelines random access memory chaos maintenance engineering signal processing ofdm modulation real time systems very large scale integration parallel processing;address generator;parallel processing;ram memory based fft architecture radix 2 fft architecture memory size requirement reduction address generator multiplier utilization;real time systems	This paper proposes an efficient memory-based radix-2 FFT architecture, which greatly improves the memorybased FFT [5], [6] by reducing 50% memory size requirement, while maintaining a simple address generator. Specifically the memory size is reduced to 1.25N words. In addition, the multiplier utilization rate is 100%.	fast fourier transform	Chao-Kai Chang;Chung-Ping Hung;Sau-Gee Chen	2003		10.1109/ISCAS.2003.1205910	maintenance engineering;uniform memory access;parallel processing;computer vision;interleaved memory;electronic engineering;parallel computing;computer hardware;computer science;electrical engineering;signal processing;very-large-scale integration;flat memory model;registered memory	Arch	12.279367668232704	44.26247941456182	64571
199a6e7a0a17de0e818034076d2f708a0b9bdb9b	partial product reduction for parallel cubing	0 6 micron;cubing circuits;concurrent computing;logic design;ambient intelligence;0 6 micron partial product reduction parallel cubing operand cube computing counters cubing circuits ami c5n technology;parallel cubing;logic design digital arithmetic;counting circuits;delay concurrent computing function approximation hardware table lookup parallel processing counting circuits ambient intelligence signal processing cryptography;function approximation;cryptography;signal processing;ami c5n technology;digital arithmetic;counters;operand cube computing;partial product reduction;table lookup;parallel processing;hardware	A new technique for computing the cube of an operand of any length is proposed, implemented, analyzed, and compared to existing techniques. The new proposed method is faster than previously proposed methods that compute the cube of an operand in parallel with the disadvantage that more counters are utilized to perform partial product reduction. Cubing circuits using the proposed techniques are implemented with several operand lengths and analyzed with regard to area consumption and latency. Results are shown for several designs in AMI C5N 0.6 mum technology	operand	James E. Stine;Jeff M. Blank	2007	IEEE Computer Society Annual Symposium on VLSI (ISVLSI '07)	10.1109/ISVLSI.2007.78	parallel computing;computer science;theoretical computer science;distributed computing	Arch	12.109446344718194	44.1803045188611	64644
572934fb744c97feb3682ec436c7ace097d5f8eb	searching in discrete universes	discrete universe	• second order Always output the same search technique. For example, always a binary search. Output the best of several search techniques. For example, either a binary search or a sequential search. Output a combination of several search techniques. For example, a search that bruits the number of possible key-da ta pairs to a small set using a binary search and then switches to a sequential search.	binary search algorithm;linear search;network switch	George H. Roberts	1991	SIGPLAN Notices	10.1145/122167.122170	computer science;theoretical computer science;universe	AI	21.20111502262392	41.025421183961086	64804
cff2ee25380217ae76f1d3d79dba658051feb1dc	memory-aware multiple reference frame motion estimation for the h.264/avc standard	mrf;clocks;real time;data reuse;reference frame;motion estimation;automatic voltage control videos clocks transform coding;transform coding;video coding field programmable gate arrays motion estimation;memory access;video coding;automatic voltage control;h 264 avc standard;memory aware;memory aware h 264 avc standard motion estimation mrf vlsi architecture;external memory;field programmable gate arrays;high definition;video coding memory aware multiple reference frame motion estimation h 264 avc standard mrf me current block parallelism fpga device processing high definition videos h 264 avc mvc extension;videos;vlsi architecture	This paper presents an architecture for Multiple Reference Frame Motion Estimation (MRF-ME) targeting H.264/AVC standard. MRF introduces issues regarding processing throughput and memory access. In this context, this work proposes a memory-aware architecture for MRF-ME that relies on data reuse and current block parallelism. The data reuse scheme guarantees a reduction of almost 70% in the number of external memory accesses when compared with a traditional approach. The architecture was synthesized targeting a Xilinx FPGA device and it is capable of processing high definition (HD) videos in real time (30fps), with very good processing rates results when compared with related works. This solution is based on one single view, but it is being used as base to design an architecture which will be able to process multi view videos as defined in the H.264/AVC MVC extension.	asp.net mvc;field-programmable gate array;h.264/mpeg-4 avc;markov random field;motion estimation;parallel computing;reference frame (video);throughput	Mateus Grellert;Felipe Sampaio;Bruno Hecktheuer;Júlio C. B. de Mattos;Luciano Volcan Agostini	2010	2010 17th IEEE International Conference on Electronics, Circuits and Systems	10.1109/ICECS.2010.5724577	embedded system;computer vision;real-time computing;computer science	EDA	11.991110127058455	40.128482293233766	64843
fbfc608268ba084a2cf070f23085e7e01a0bf9f9	a systolic array based architecture for implementing multivariate polynomial interpolation tasks	reverse engineering problem for gene networks;resource utilization;interpolation;finite field model;multivariate polynomial addition;reverse engineering problem for gene networks multivariate polynomial interpolation systolic arrays parameterizable architectures bioinformatic applications;boolean functions;systolic arrays;bioinformatic applications;systolic array;parameterizable architectures;gene network;systolic arrays polynomials interpolation computer architecture genetics field programmable gate arrays computer networks reverse engineering galois fields hardware;fpga;data mining;polynomials;genetic networks;genetics;hardware architecture;finite field;arrays;multivariate polynomial interpolation;systolic array based hardware architecture;multivariate polynomial;resource utilization systolic array based hardware architecture multivariate polynomial interpolation tasks reverse engineering genetic networks finite field model genetic data complexity fpga boolean cover multivariate polynomial addition software implementation;genetic network;systolic arrays boolean functions field programmable gate arrays interpolation;genetic data complexity;boolean cover;field programmable gate arrays;multivariate polynomial interpolation tasks;software implementation;reverse engineering;hardware	Multivariate polynomial interpolation is a key computation for the reverse engineering of genetic networks modeled by finite fields. Faster implementations of such algorithms are needed to cope with the increasing quantity and complexity of genetic data. Our implementation of an interpolation methodology to FPGA has led us to identify a systolic array-based hardware architecture that is useful for performing at least three interpolation sub-tasks: Boolean cover, uniqueness, and multivariate polynomial addition. We present a generalization of these algorithms that simplifies mapping to the systolic-array structure, as well as control and storage considerations to guarantee correct results when the input sequence is longer than the processing array. The three interpolation sub-tasks were modeled and implemented to FPGA using the proposed structure, obtaining speedups up to 172x when compared to a software implementation, while achieving low resource utilization.	algorithm;computation;field-programmable gate array;gene regulatory network;polynomial interpolation;reverse engineering;systolic array	Rafael A. Arce-Nazario;Edusmildo Orozco;Dorothy Bollman	2009	2009 International Conference on Reconfigurable Computing and FPGAs	10.1109/ReConFig.2009.70	embedded system;computer science;theoretical computer science;hardware architecture;field-programmable gate array	EDA	10.426021583117219	44.37067579501797	65138
bf8d16f01e29b8f124cf97da51f3bf85bd2482f3	on the equivalence of the sequence pair for rectangle packing to the dimension of partial orders [floorplanning]	minimisation;sequences;minimum area layout;partial order dimensions;lsi layout sequence pair equivalence rectangle packing partial order dimensions floorplanning problem representation minimum area layout cad slicing structure partially ordered set dimensions mathematical background;integrated circuit layout;floorplanning problem representation;cad;set theory;sequence pair equivalence;simulated annealing;mathematical background;lsi layout;slicing structure;partially ordered set;large scale integration;partially ordered set dimensions;circuit layout cad;rectangle packing;large scale integration simulated annealing lapping;set theory circuit layout cad integrated circuit layout large scale integration sequences minimisation;lapping;partial order	A sequence pair can be used to represent a set of instances of a floorplanning problem that includes the minimum-area layout. The sequence pair has attracted much attention from CAD researchers because it provides them with an efficient representation of the instances of the floorplanning problem that do not necessarily have a slicing structure. In this respect, the sequence pair is the first representation that can adequately handle realistic requirements. This paper relates the sequence pair to the notion of dimension of partially ordered sets and clarifies its mathematical background.	floorplan (microelectronics);set packing;turing completeness	H. Miyashita;Y. Kajitani	2002		10.1109/APCCAS.2002.1115263	partially ordered set;mathematical optimization;combinatorics;attribute–value pair;mathematics;engineering drawing	Theory	22.42989144263317	42.04851878351639	65264
87ff85a4730af25df7c3b275f975854e38949dc8	low cost floating-point unit design for audio applications	digital signal processing;cmos digital integrated circuits floating point arithmetic vlsi audio signal processing digital signal processing chips;audio systems;audio signal processing;floating point unit;cmos technology;decoding;area overhead;0 25 micron floating point unit design audio applications digital audio processing applications critical path delay hardware resources integer datapath area overhead cmos w3 aac test vector generation method verification time 6 ns;6 ns;hardware resources;aac;costs digital audio players delay floating point arithmetic digital signal processing dynamic range cmos technology audio systems signal processing algorithms decoding;floating point unit design;fixed point;general methods;cmos digital integrated circuits;critical path;dynamic range;verification time;vlsi;digital signal processing chips;floating point;critical path delay;floating point arithmetic;digital audio players;signal processing algorithms;audio applications;cmos;integer datapath;0 25 micron;digital audio processing applications;test vector generation method	This paper presents a low-cost, single-cycle floating-point unit developed for digital audio processing applications. In the unit, the serial steps of floating-point operations are paralleled to reduce critical path delay, and hardware resources are shared with the integer datapath to minimize area overhead. Its area overhead is as small as 38% of the fixed-point datapath, and the critical path delay is 6ns in 0.25μm CMOS technology, which makes it well suited to modern audio applications such as MP3 and AAC. The floating-point unit is verified through an efficient test vector generation method developed to reduce verification time significantly.	advanced audio coding;audio signal processing;cmos;critical path method;datapath;floating-point unit;mp3;overhead (computing);test vector	Sung-Won Lee;In-Cheol Yong Park	2002		10.1109/ISCAS.2002.1009979	embedded system;electronic engineering;computer hardware;execution unit;computer science;floating point;operating system;cmos	EDA	12.019358569746133	43.43112113638601	65648
01e964f3160634c8a920e5485597eb477cb69e96	explicit constructions of depth-2 majority circuits for comparison and addition	arithmetique ordinateur;68rxx;integrated circuit;comparison function;estudio comparativo;boolean function;conception;circuito integrado;calculo automatico;computing;calcul automatique;etude comparative;computer arithmetic;error correcting codes;comparative study;diseno;addition;aritmetica ordenador;68q15;design;threshold function;majority circuits;circuit integre;addition function;adiccion	All Boolean variables here range over the two element set {−1, 1}. Given n Boolean variables x1, . . . , xn, a non-monotone MAJORITY gate (in the variables xi) is a Boolean function whose value is the sign of ∑n i=1 ixi, where each i is either 1 or −1. The COMPARISON function is the Boolean function of two n-bits integers X and Y whose value is −1 iff X ≥ Y . We construct an explicit sparse polynomial whose sign computes this function. Similar polynomials are constructed for computing all the bits of the summation of the two numbers X and Y . This supplies explicit constructions of depth-2 polynomial-size circuits computing these functions, which use only non-monotone MAJORITY gates. These constructions are optimal in terms of the depth and can be used to obtain the best known explicit constructions of MAJORITY circuits for other functions like the product of two n-bit numbers and the maximum of n nbit numbers. A crucial ingredient is the construction of a discrete version of a sparse “delta polynomial”—one that has a large absolute value for a single assignment and extremely small absolute values for all other assignments. ∗Research supported in part by the Fund for Basic Research administered by the Israel Academy of Sciences	academy;assignment (computer science);majority function;polynomial;sparse matrix;monotone	Noga Alon;Jehoshua Bruck	1994	SIAM J. Discrete Math.	10.1137/S0895480191218496	design;combinatorics;computing;discrete mathematics;integrated circuit;mathematics;geometry;boolean function;addition;algorithm;algebra	Theory	17.067372196408876	40.189414763194044	66046
f8ca5566927ef7e3a761b7d0f2807b4c44f456b4	index generation functions: tutorial		Given a set of k distinct binary vectors of n bits, for each vector assign a unique integer from 1 to k. An incompletely specified index generation function produces an index for a given vector. This tutorial first introduces index generation functions, which are useful for pattern matching in communication circuits. Then, it shows a method to represent a given index generation function using fewer variables. A linear transformation is used to reduce the number of variables. An extension to the multiple-valued case is also presented.	index (publishing);pattern matching	Tsutomu Sasao	2014	Multiple-Valued Logic and Soft Computing		mathematical optimization;computer science;pattern matching;binary number;linear map;electronic circuit;integer	Theory	19.014865144166542	45.03071572477548	66268
51f2ffb3b70dceb8367ba82dd84c58aca8eb6064	parallel vision computing on a network of workstation clusters		the advanced technology of computer network has made Vision computing involves the execution of a large number of operations on large sets of structured data. In this paper we demonstrate that such vision tasks can be implemented in parallel on a network of workstation clusters for fast processing. We introduce some techniques used in distributed systems and adopt a divide-and-conquer policy to schedule the complex vision tasks for parallelism. The visionrelated algorithms for mask convolution, feature extraction, discrete Fourier transform and image matching are implemented in parallel using PVM(paralle1 virtual machine). In addition, a hierarchical object recognition system is described to conclude that a general distributed system can be applied to parallel vision computing at a low cost.	algorithm;computer cluster;convolution;discrete fourier transform;distributed computing;feature extraction;image registration;outline of object recognition;parallel computing;real-time clock;speedup;virtual machine;workstation	Jane You;Abdul Sattar;Ljubo Vlavic	1998			light flashing;computer vision;artificial intelligence;computer architecture;mathematics;computer hardware;cluster (physics);microphone;workstation	HPC	10.536578152082663	35.625149677508595	66529
b7bb051c2376345f5c5e80f165b15f2f2e68ecc9	minimizing the communication time for matrix multiplication on multiprocessors	minimisation;parallelisme;complexite;programa paralelo;minimization;analisis numerico;multiprocessor;complejidad;computer and information science;array of processor nodes;complexity analysis;minimizacion;complexity;experimental implementation;binary cube networks;three dimensional;analyse numerique;algorithme;algorithm;parallelism;numerical analysis;matrices;paralelismo;degeneration;temps communication;optimal communication;matrix multiplication;multiplicacion;multiprocesador;data och informationsvetenskap;matrice;multiplication;parallel program;algoritmo;programme parallele;multiprocesseur	We present one matrix multiplication algorithm for two{dimensional arrays of processing nodes, and one algorithm for three{dimensional nodal arrays. One{dimensional nodal arrays are treated as a degenerate case. The algorithms are designed to utilize fully the communications bandwidth in high degree networks in which the one{, two{, or three{dimensional arrays may be embedded. For binary n-cubes, our algorithms ooer a speedup of the communication over previous algorithms for square matrices and square two{dimensional arrays by a factor of n 2. Connguring the N = 2 n processing nodes as a three-dimensional array may reduce the communication complexity by a factor of N 1 6 compared to a two{dimensional nodal array. The three{dimensional algorithm requires temporary storage proportional to the length of the nodal array axis aligned with the axis shared between the multiplier and the multiplicand. The optimal two{dimensional nodal array shape with respect to communication has a ratio between the numbers of node rows and columns equal to the ratio between the numbers of matrix rows and columns of the product matrix, with the product matrix accumulated in{place. The optimal three{dimensional nodal array shape has a ratio between the lengths of the machine axes equal approximately to the ratio between the lengths of the three axes in matrix multiplication. For product matrices of extreme shape, one{dimensional nodal array shapes are optimal when N=n < columns of the product matrix. All our algorithms use standard communication functions.	column (database);communication complexity;embedded system;matrix multiplication algorithm;olap cube;optic axis of a crystal;speedup;tiling array	S. Lennart Johnsson	1993	Parallel Computing	10.1016/0167-8191(93)90029-K	three-dimensional space;minimisation;combinatorics;parallel computing;complexity;multiprocessing;numerical analysis;matrix multiplication;theoretical computer science;mathematics;multiplication;algorithm;matrix;algebra	Theory	12.585712431100884	34.11701078806794	66675
32b54b34a98d5f188eec5230839dbf5e8b50f2fc	a theory of galois switching functions	minimal expansion;reed muller expansion;finite field;disjunctive normal expression;partial ordering relations;multiple output switching functions;reed muller expansion disjunctive normal expression galois field minimal expansion multiple output switching functions partial ordering relations;reed muller;galois field;partial order	Galois switching functions (GSF's) are generalizations of binary functions in that the input and output variables can assume values over any finite field. The concepts of minterms, k-cubes and minimal complexity realizations as related to GSF are introduced.	generic sensor format;input/output;olap cube	Dhiraj K. Pradhan	1978	IEEE Transactions on Computers	10.1109/TC.1978.1675077	partially ordered set;galois theory;combinatorics;discrete mathematics;normal basis;mathematics;galois extension;finite field;algebra;field norm	Visualization	20.41574417551752	42.25952584042505	66842
54881738b7be6550a0a9cba58b4f4359d71103cb	every edge lies on cycles embedding in folded hypercubes with both vertex and edge faults	embedding;fault tolerant;folded hypercubes;interconnection networks;hypercubes;cycle;fault free	The folded hypercube is a well-known variation of hypercube structure and can be constructed from a hypercube by adding an edge to every pair of vertices with complementary addresses. Let FFv (respectively, FFe) denote the set of faulty vertices (respectively, faulty edges) in an n-dimensional folded hypercube FQn. In the case that all edges in FQn are fault-free, Cheng et al. [Cycles embedding on folded hypercubes with faulty vertices, Discrete Appl. Math. 161 (2013) 2894–2900] has shown that (1) every fault-free edge of FQn −FFv lies on a fault-free cycle of every even length from 4 to 2n − 2|FF v| if |FFv|≤ n − 2, where n ≥ 3; and (2) every fault-free edge of FQn −FFv lies on a fault-free cycle of every odd length from n + 1 to 2n − 2|FF v|− 1 if |FFv|≤ n − 2, where n ≥ 2 is even. In this paper, we extend Cheng’s result to obtain two further properties, which consider both vertex and edge faults, as follows: (1) Every fault-free edge of FQn −FFv −FFe lies on a fault-free cycle of every even length from 4 to 2n − 2|FF v| if |FFv| + |FFe|≤ n − 2, where n ≥ 3; (2) Every fault-free edge of FQn −FFv −FFe lies on a fault-free cycle of every odd length from n + 1 to 2n − 2|FF v|− 1 if |FFv| + |FFe|≤ n − 2, where n ≥ 2 is even.		Che-Nan Kuo	2016	Discrete Math., Alg. and Appl.	10.1142/S1793830916500014	fault tolerance;combinatorics;discrete mathematics;topology;embedding;mathematics;geometry;hypercube	Theory	24.49673308119043	34.40282122246521	66850
aca8718e0b035b445fb748aa0217ca75c2d11324	message terminate algorithms for anonymous rings of unknown size	pattern search;probabilistic algorithm;symmetry breaking;leader election	We consider a ring of unknown number of anonymous processors. We restrict ourselves to algorithms that are message terminate, i.e. the algorithm terminates when no more messages are present in the system but the processors may lack the ability to detect this situation. The work addresses algorithms (both deterministic and probabilistic) that always terminate with the correct result. We show the following:          A deterministic algorithm for orientation that requires a symmetry breaking marking on the links and uses O(n log2 n) bits for communication and O(n) time. A Las-Vegas version of this algorithm that uses probability to break symmetry has the same average communication and time cost.           A deterministic algorithm for pattern searching that uses O(n · ¦S¦) communication bits for a pattern of length ¦S¦. Computing AND and OR are simple cases of that algorithm.           A probabilistic algorithm for dividing an even ring to pairs that uses O(n log n) communication bits and time.           The impossibility of computing a class of functions called nonsym-metric that includes: leader election, XOR and finding the ring size. The same technique can be applied to prove the impossibility of dividing an odd ring to a maximal number of pairs.          	terminate (software)	Israel Cidon;Yuval Shavitt	1992		10.1007/3-540-56188-9_18	theoretical computer science;leader election;mathematics;distributed computing;algorithm	Crypto	17.630121384628428	34.16229811781311	67091
07e6b35c9cc73cffd26eef7e3ede0546a54c005e	local algorithms for finding interesting individuals in large networks	graph theory;local algorithm;preferential attachment;topological properties;social network;keyword search;clustering coefficient;upper and lower bounds;power law	We initiate the study of local, sublinear time algorithms fo r finding vertices with extreme topological properties — such as high degree or clustering c oeffi ient — in large social or other networks. We introduce a new model, called the Jump and Crawl model, in which algorithms are permitted only two graph operations. The Jumpoperation returns a randomly chosen vertex, and is meant to m odel the ability to discover “new” vertices via keyword search in the Web, shared hobbies or interests in social networks such as Facebook, and other mechanisms that ma y return vertices that are distant from all those currently known. The Crawl operation permits an algorithm to explore the neighbors of a ny currently known vertex, and has clear analogous in many mode rn n tworks. We give both upper and lower bounds in the Jump and Crawl model f r the problems of finding vertices of high degree and high clustering coefficient. We consider b oth arbitrary graphs, and specializations in which some common assumptions are made on the global topolog y (such as power law degree distributions or generation via preferential attachment). We als o examine local algorithms for some related vertex or graph properties, and discuss areas for future inv estigation.	attachments;cluster analysis;clustering coefficient;graph operations;graph property;randomness;search algorithm;social network;time complexity;vertex (geometry);vertex (graph theory);world wide web	Mickey Brautbar;Michael Kearns	2010			power law;combinatorics;discrete mathematics;graph theory;machine learning;clustering coefficient;mathematics;upper and lower bounds;statistics;social network	Theory	20.08309949260331	35.18211356016859	67166
8e9d8084fbd50c72d4d3aa70196cc04f258d8904	divide-by-n and divide-by-n/n+1 prescalers based on a shift register and a multi-input nor gate	divider;prescaler;linear feedback shift register;dual modulus divider;pll	This paper describes the architecture of a divide-by-N prescaler and a divide-by-N/N + 1 dual-modulus prescaler based on a shift register and a multi-input NOR gate. The divide-by-N prescaler has a circuit style similar to a linear feedback shift register (LFSR), except for the fact that a multi-input NOR gate is used instead of XOR gates. This architecture can be applied to various division ratios of N ≥ 2 by changing the numbers of flip-flops and NOR-gate inputs according to specific rules, which will be explained in this paper. The state of the prescaler runs through the correct loop without requiring a reset signal or an initialization circuit.	and gate;dual-modulus prescaler;exclusive or;flops;flip-flop (electronics);like button;linear-feedback shift register;modulus of continuity;modulus robot;nor gate	Seon-Woo Hwang;Yongsam Moon	2012	IEICE Electronic Express	10.1587/elex.9.1611	frequency divider;electronic engineering;phase-locked loop;control theory;linear feedback shift register;dual-modulus prescaler	EDA	21.39522928755463	45.302421275140524	67204
670a2cb508500040046f27207b80504230f6df65	multiply accumulate unit optimised for fast dot-product evaluation	signed digit;frequency 220 mhz fast dot product evaluation asymmetric signed number digit redundant number representation optimal partial product packing adder low pass fir filter bit word length altera cyclone ii fpga family field programmable gate arrays multiply and accumulate unit filter clock speed;low pass;word length;adders;fir filter;low pass filters adders digital arithmetic field programmable gate arrays fir filters;digital arithmetic;low pass filters;fir filters;field programmable gate arrays;high throughput;field programmable gate arrays signal processing algorithms array signal processing throughput finite impulse response filter cyclones clocks signal design algorithm design and analysis microprocessors	A fast dot-product unit suitable for long word lengths is shown. Its implementation is based on computing only the significant partial products and exploiting the properties of the asymmetric signed digit redundant number representation. Optimal partial product packing and a carry propagation free adder combine to yield a MAC with high throughput. An example design of a low-pass FIR filter of 51 taps of 32 bit word-length was synthesised for the Altera cyclone II FPGA family. A filter clock speed of 220 MHz and a throughput of 12.9 MSamples/s was achieved.	32-bit;adder (electronics);clock rate;cyclone;field-programmable gate array;finite impulse response;low-pass filter;multiply–accumulate operation;set packing;software propagation;throughput	William Kamp;Andrew Bainbridge-Smith	2007	2007 International Conference on Field-Programmable Technology	10.1109/FPT.2007.4439283	embedded system;parallel computing;low-pass filter;computer hardware;computer science;finite impulse response	Robotics	11.823374413437442	44.39867048965812	67210
109cbdcaa31165874c99088ccd1664809d78de94	multi-level simulation of a translinear analog adaptive filter	computer languages educational institutions;least mean square;computer languages;bepress selected works;adaptive algorithm;levels of abstraction;spice;adaptive filter;matlab;computer languages educational institutions matlab spice four tap analog adaptive filter;four tap analog adaptive filter	In this paper, we briefly discuss a methodology for synthesizing analog systems from a high-level behavioral specification using a class of circuits called dynamic translinear circuits. We illustrate this method by synthesizing a Least-Mean-Square (LMS) adaptation algorithm used in an analog adaptive filter. The resulting systems can be simulated at various levels of abstraction during the design phase. As an example, we present simulation results from a four-tap analog adaptive filter simulated using Matlab and SPICE.	adaptive filter;algorithm;high- and low-level;least mean squares filter;matlab;principle of abstraction;spice;simulation;translinear circuit	Eric J. McDonald;Bradley A. Minch	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5745532	adaptive filter;computer vision;real-time computing;digital filter;least mean squares filter;computer science;theoretical computer science	EDA	21.459960924393865	46.052267431701374	67416
0d4f3eb84e2d7b741afa10c6ac78b84583886eb6	deterministic leader election among disoriented anonymous sensors		We address the Leader Election (LE) problem in networks of anonymous sensors sharing no kind of common coordinate system. Leader Election is a fundamental symmetry breaking problem in distributed computing. Its goal is to assign value 1 (leader) to one of the entities and value 0 (non-leader) to all others. In this paper, assuming n > 1 disoriented anonymous sensors, we provide a complete characterization on the sensors positions to deterministically elect a leader, provided that all the sensors’ positions are known by every sensor. More precisely, our contribution is twofold: First, assuming n anonymous sensors agreeing on a common handedness (chirality) of their own coordinate system, we provide a complete characterization on the sensors positions to deterministically elect a leader. Second, we also provide such a complete chararacterization for sensors devoided of a common handedness. Both characterizations rely on a particular object from combinatorics on words, namely the Lyndon Words.	apache axis;chirality (chemistry);circular polarization;computer security;deterministic algorithm;distributed computing;entity;leader election;mobile agent;planar (computer graphics);sensor;symmetry breaking	Yoann Dieudonné;Florence Levé;Franck Petit;Vincent Villain	2012	CoRR		artificial intelligence;leader election	Theory	17.579157270991068	34.87038410758334	68154
e273c92d2ddf60e23338604ff017682e1c121515	computing rooted communication reliability in an almost acyclic digraph	grafo aciclico;fiabilidad;reliability;digraph;graphe acyclique;reseau;acyclic graph;red;directed graph;fiabilite;vertex graph;graphe oriente;grafo orientado;cuspide grafico;sommet graphe;network	This article considers the reliability problem of computing the probability that the rootvertex of a directed network can communicate with all other vertices of the network when arcs are subject to random failure. Ball and Provan have observed that when the network contains no directed cycles this probability can be computed in linear time. This paper considers the problem when the network is almost acyclic, that is, when the number of simple directed cycles in the network is small. A new algorithm for cornputing network reliability is presented. The algorithm is based on expressing the reliability in terms of the derivative of the network reliability with respect to the reliability of an arc. Given a class of networks with a fixed number of simple cycles c, the computation requirements of the new algorithm are O(lElc+') , where IEl is the number of arcs. Comparison of coded versions of this algorithm and one of Ball's as well as theoretical comparison with one of Buzacott's confirm that this algorithm is to be preferred when the number of directed cycles is small in comparison to the number of vertices.	algorithm;ball project;computation;cycle (graph theory);directed acyclic graph;directed graph;requirement;time complexity;vertex (geometry)	Jane N. Hagstrom	1991	Networks	10.1002/net.3230210507	combinatorics;discrete mathematics;directed graph;mathematics;algorithm	Theory	24.035735195155585	33.45842518315647	68464
2ed4278cf41df984411c102d447f92c5bc080393	tolerance determination for algorithm-based checks using simplified error analysis techniques	reliability;error coverage;tolerance determination;fault tolerant;algorithm based checks;application software;algorithm based fault tolerance;systolic arrays;error expressions;contracts;thresholding tolerance determination algorithm based checks error analysis techniques roundoff errors error bounds error expressions fault tolerant encodings error coverage data sets;spectrum;error analysis techniques;error analysis;redundancy;fault tolerant systems;fault tolerance;roundoff errors;error bounds;fault tolerant encodings;thresholding;error bound;data sets;error analysis hardware fault tolerance roundoff errors contracts fault tolerant systems reliability systolic arrays redundancy application software;hardware	A scheme for dealing with roundoff errors in algorithm-based fault tolerance methods which complicate the check phases of the algorithm is presented. The method is based on error analysis incorporating some simplifications which result in easier derivation of error bounds and more useful error expressions in some cases where the theoretical error bound may be too wide to be of much use as a tolerance in the check phase. The methods are used to derive error bounds for three applications, and it is shown that the fault-tolerant encodings for these applications using the authors error expressions achieve high error coverage and no false alarms for a wide spectrum of data sets at low overheads. The authors contrast their tolerance bound calculation method with an earlier method and show that their method is much more robust. The authors believe that, to be of practical use, algorithm-based fault-tolerance techniques need to pay close attention to the thresholding issue, and they point out that their scheme is the first one which achieves good results across a wide range of data sets.	algorithm;error analysis (mathematics);fault tolerance;key derivation function;robustness (computer science);thresholding (image processing)	Amber Roy-Chowdhury;Prithviraj Banerjee	1993	FTCS-23 The Twenty-Third International Symposium on Fault-Tolerant Computing	10.1109/FTCS.1993.627332	human error assessment and reduction technique;real-time computing;computer science;theoretical computer science;error bar;algorithm	Embedded	22.806299535642925	44.34999348374455	68484
4d5441e6383aecf4287e0cb16f450d81955e7f98	parallel algorithm for mapping parallel program into pyramidal multiprocessor	parallel program;pyramidal multiprocessor;parallel algorithm;recursive algorithm;parallel computer;objective function	A problem of mapping of an information graph of a complex algorithm into the pyramidal interprocessor network of a parallel computer system is considered. The parallel recursive algorithm for optimal or suboptimal solution of the mapping problem, the objective functions for mapping and experimental results for the pyramidal multiprocessor system MEMSY are presented.	multiprocessing;parallel algorithm	O. G. Monakhov	1995		10.1007/3-540-60902-4_46	parallel algorithm	HPC	11.546319235302533	34.63523035233611	68682
7d792b527c9001792eb215158c9146fd8cdabf0f	a low fragmentation heuristic for task placement in 2d rtr hw management	field programmable gate array;diseno circuito;reconfigurable architectures;localization;heuristic method;circuit design;metric;metodo heuristico;localizacion;fragmentacion;red puerta programable;reseau porte programmable;run time reconfigurable;localisation;modelo 2 dimensiones;modele 2 dimensions;multithread;metrico;conception circuit;multitâche;methode heuristique;fragmentation;architecture reconfigurable;multitarea;metrique;two dimensional model	A novel technique is proposed for the management of a two-dimensional run-time reconfigurable device in order to get true hardware multitasking. The proposed technique uses a Vertex List Set to keep track of the available free area, and of the candidate locations to place the arriving tasks. Each Vertex List describes the contour of each unoccupied area fragment in the reconfigurable device. Several heuristics are proposed to solve the problem of selecting one of the vertices to place the task. The heuristic that gives best results is based on a novel fragmentation metric. This metric estimates for each alternative location the suitability of the resulting free device area to accept future incoming tasks. Finally, we show that our approach, with a reasonable complexity, gives better results, in terms of device fragmentation and efficiency, than other techniques.	fragmentation (computing);heuristic;real-time recovery	Jesús Tabero;Julio Septién;Hortensia Mecha;Daniel Mozos	2004		10.1007/978-3-540-30117-2_26	embedded system;real-time computing;simulation;internationalization and localization;metric;computer science;operating system;circuit design;fragmentation;algorithm;field-programmable gate array	Robotics	11.44493846680885	36.60425318454604	68697
14e069c09704a3aee3d3dfa4e127b445e2cd813f	analysis of the matrix processing (mxp) architecture	ti tms320c55x;vector manipulations;digital signal processing;data parallel;filtering;dsp architecture;ti tms320c64x matrix processing architecture dsp architecture data parallelism hardwired matrix operations mxp matrix computation multidimensional transforms instruction memory requirements vector manipulations video processing standards dct motion estimation ti tms320c55x;multidimensional transforms;clocks;computer architecture signal processing algorithms coprocessors digital signal processing filtering discrete cosine transforms motion estimation clocks streaming media iso standards;iso standards;data parallelism;dsp architectures;video processing;motion estimation;matrix algebra;mxp matrix computation;coprocessors;computer architecture;matrix computation;streaming media;video standards dsp architectures;discrete cosine transforms;ti tms320c64x;transforms;matrix processing architecture;instruction memory requirements;digital signal processing chips;video processing standards;transforms digital signal processing chips matrix algebra;signal processing algorithms;hardwired matrix operations;dct;video standards	In this paper, we describe and evaluate a new DSP architecture called the matrix processing (MxP™). MxP exploits data parallelism using hardwired matrix operations and instructions. The MxP matrix computation capabilities are optimized for multidimensional transforms and filtering. A detailed analysis of the MxP performance, in terms of precision, execution time, and instruction memory requirements, is presented. We focus particularly on matrix and vector manipulations of the type embedded in video processing standards, e.g., filtering, DCT, and motion estimation. We present comparison tables of the performance of the MxP with other widely used DSPs such as the TI TMS320c55x™, and the TI TMS320c64x™.	codec;computation;data compression;data parallelism;digital signal processor;discrete cosine transform;embedded system;filter (signal processing);finite impulse response;h.264/mpeg-4 avc;image compression;instruction cycle;mad;motion estimation;numerical linear algebra;parallel computing;peterson's algorithm;requirement;run time (program lifecycle phase);texas instruments tms320;the matrix;video processing	Lakshmi Girija;Andreas Spanias	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.366251	filter;computer vision;parallel computing;computer hardware;computer science;theoretical computer science;digital signal processing;discrete cosine transform;motion estimation;data parallelism;video processing;numerical linear algebra;coprocessor	HPC	11.736794710264363	40.03422315902421	68806
8aa80f96ced336ce20fd212dedacded6b6df5c59	a yield study of vlsi adders	adders;carry-lookahead adder;hybrid adder;carry-skip adder;vlsi adders;64 bit;yield study;layout modification;floating point arithmetic;process design;circuits;very large scale integration	Several 64-bit adders have been designed and their ezpected yield has been estimated. OUT Tesults show that the yield of VLSI adders can be improved by modifying the layout of the ol.iginal design and/or by choosing a dzfferent layout and circuit structure. In certain situations, these approaches can improve the yield by 10% to 17%.	64-bit computing;very-large-scale integration	Zhan Chen;Israel Koren	1994			process design;electronic circuit;electronic engineering;parallel computing;computer hardware;computer science;floating point;engineering;electrical engineering;very-large-scale integration;adder	HCI	15.2152563099694	46.17439557163479	68869
e05fab968a2076418db6ee050b27a5012101e2c4	an efficient implementation of batcher's odd-even merge on a simd hypercube	parallelisme;complexite;hypercube;evaluation performance;performance evaluation;evaluacion prestacion;complejidad;machine parallele;complexity;calculateur simd;parallel computation;parallelism;calculo paralelo;paralelismo;efficient implementation;simd computer;parallel computer;calcul parallele;hipercubo	Abstract   We present an efficient Θ(log  N ) implementation of Batcher′s odd-even merge on a SIMD hypercube. (The hypercube model assumes that all communications are restricted to one fixed dimension at a time.) The best previously known implementation of odd-even merge on a SIMD hypercube requires Θ(log 2  N ) time. The performance of our odd-even merge implementation is comparable to that of bitonic merge. (If the input sequences are both in ascending order and the architecture provides half-duplex communication, then our algorithm runs faster than bitonic merge by a factor of   4  3  .) A generalization of our technique has led to an efficient  O (log  N ) algorithm for a wider class of parallel computations, called ±2  b  -descend, on a SIMD hypercube [11]. This class includes odd-even merge and many other algorithms. In this paper, we briefly discuss the main ideas of this paradigm.	ken batcher;simd	David Nassimi;Yuh-Dong Tsai	1993	J. Parallel Distrib. Comput.	10.1006/jpdc.1993.1090	computer architecture;parallel computing;complexity;computer science;distributed computing;merge algorithm;algorithm;hypercube;polyphase merge sort	HPC	11.908917766430363	34.367965784039534	69099
72b2071f05193d95de2fadec8f78f6007623c31e	vlsi design and implementation of 2-d inverse discrete wavelet transform	discrete wavelet transforms abstracts computer architecture schedules;xilinx virtex2 fpga vlsi design 2d inverse discrete wavelet transform jpeg 2000 compliant architecture row based schedule single processor routing complexity;vlsi discrete wavelet transforms integrated circuit design inverse transforms	This paper proposes a JPEG-2000 compliant architecture capable of computing the 2 -D Inverse Discrete Wavelet Transform. The proposed architecture uses a single processor and a row-based schedule to minimize control and routing complexity and to ensure that processor utilization is kept at 100%. The design incorporates the handling of borders through the use of symmetric extension. The architecture has been implemented on the Xilinx Virtex2 FPGA.	discrete wavelet transform;field-programmable gate array;jpeg 2000;routing;very-large-scale integration	Paul V. McCanny;John V. McCanny;Shahid Masud	2002	2002 11th European Signal Processing Conference	10.5281/zenodo.38144	computer architecture;parallel computing;second-generation wavelet transform;computer science;theoretical computer science;cascade algorithm;wavelet packet decomposition;lifting scheme	Robotics	11.748139616755498	41.834634624704	69166
8f898a29fc804274961f04e36b1ec1cbdac1ca63	a probabilistic simulation of prams on a bounded degree network	tratamiento paralelo;traitement parallele;algorithm analysis;multiprocessor;simulation;aproximacion probabilista;simulacion;probabilistic approach;approche probabiliste;analyse algorithme;multiprocesador;analisis algoritmo;parallel processing;multiprocesseur	Abstract   A simulation scheme for ( n, m )-PRAM computation is devised, based on an interconnection network organized in the form of a mash-of-trees (MT) of side  n . The  m  memory cells are subdivided in  n  modules, each local to one of the  n  PRAM processors. The MT roots contain these processors and the memory modules, while the other MT nodes have the mere functions of packet switchers. Time complexity is probabilistically analyzed. It is shown that, as  n  goes to infinity, the slow-down for each step of PRAM computation is O(log  n ) with probability tending to 1 and that, as either  n  or  k  go to infinity, the simulation time for  k  consecutive steps is O( k  log  n ) with probability tending to 1. Area requirements are finally studied according to the VLSI grid model.	simulation	Fabrizio Luccio;Geppino Pucci;Andrea Pietracaprina	1988	Inf. Process. Lett.	10.1016/0020-0190(88)90160-3	parallel processing;multiprocessing;computer science;artificial intelligence;theoretical computer science;algorithm	DB	12.86666301148496	34.57197974123247	69182
a18d612542d4f495c4514850721f9f233bae6f66	a high throughput parallel avc/h.264 context-based adaptive binary arithmetic decoder	frames per second;adaptive decoding;viterbi decoding adaptive codes adaptive decoding arithmetic codes parallel processing video coding;decoding;adaptive codes;viterbi decoder parallelization;h 264 context based adaptive binary arithmetic decoder;cabac decoder;context model;video coding;arithmetic codes;viterbi algorithm;top down design methodology;viterbi decoder;bin decoding high throughput parallel avc h 264 context based adaptive binary arithmetic decoder top down design methodology data flow modeling viterbi decoder cabac decoder;parallelization;data flow modeling;high throughput;encoding;context modeling;high throughput parallel avc;bin decoding;viterbi decoding;context;parallel processing;decoding parallel processing context encoding context modeling throughput viterbi algorithm;context based adaptive binary arithmetic coding;throughput;design methodology	In this paper, based on the proposed parallelization scheme of binary arithmetic decoding, a parallel AVC/H.264 context-based adaptive binary arithmetic coding (CABAC) decoder with high throughput is proposed. Following the top-down design methodology, algorithm analyzing and dataflow modeling in both high and low granularities are performed to achieve the proposed architecture. According to the analysis for algorithm, the similarity between CABAC decoder and Viterbi decoder is found to extend the degree of parallelism for binary arithmetic decoding. The application of proposed design is specified to support AVC/H.264 High Profile, 4.2 Level, and 1920×1088 resolution at 64 frames per second. By increasing the degree of parallelism of bin decoding, the throughput of the proposed architecture is shown by the experiments to have improved 3.5 times as compared to the sequential bin decoding, and the decoded bin per second can reach 378M at clock speed 108MHz.	algorithm;binary number;clock rate;context-adaptive binary arithmetic coding;dataflow;degree of parallelism;experiment;h.264/mpeg-4 avc;parallel computing;throughput;top-down and bottom-up design;viterbi decoder	Jia-Wei Liang;He-Yuan Lin;Gwo Giun Lee	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5946835	parallel processing;parallel computing;real-time computing;soft-decision decoder;computer science;theoretical computer science;context-adaptive variable-length coding;context model;context-adaptive binary arithmetic coding;viterbi decoder;statistics	Robotics	12.431305506607226	39.968407564709075	69198
34a909d37d6e8f3ee4bb21b8e386e3dad797b77e	performability analysis of k-to-l-out-of-n computing systems using binary decision diagrams	ordering heuristic performability analysis large computing systems k to l out of n model binary decision diagram bdd;boolean functions data structures computational modeling nickel analytical models fault trees computers	Modern computing systems typically utilize a large number of computing nodes to perform coordinated computations in parallel or simultaneously. They can exhibit multiple performance states or levels due to statuses or failures of their consistent nodes. Performability analysis is concerned with assessing the probability that the computing system performs at a particular performance level. In the context of performability analysis, these computing systems can be modeled using k-to-l-out-of-n structures. This paper proposes new analytical methods based on binary decision diagrams (BDD) for the performability analysis of large computing systems with unrepairable computing nodes. A new and efficient BDD algorithm that makes full uses of the special k -to-l-out-of-n structure is first proposed for systems with computing node having identical computing powers. New simplification rules are further proposed to generate compact and canonical BDD models for systems with heterogeneous computing nodes characterized by different computing powers. Ordering heuristic is also explored to further reduce the size of BDD models. Examples are provided to illustrate the proposed BDD-based performability analysis methodology as well as its efficiency in analyzing large-scale computing systems.	algorithm;binary decision diagram;bottom-up proteomics;cloud computing;computation;data center;dynamical system;heterogeneous computing;heuristic;level of detail;mathematical optimization;optimization problem;recursion (computer science);redundancy (engineering);reliability engineering;requirement;software propagation;top-down and bottom-up design;ubiquitous computing	Yuchang Mo;Liudong Xing;Joanne Bechta Dugan	2018	IEEE Transactions on Dependable and Secure Computing	10.1109/TDSC.2015.2504092	computer science;distributed computing;boolean function;symmetric multiprocessor system;fault tree analysis;computation;theoretical computer science;data structure;heuristic;binary decision diagram	HPC	24.24051388925587	45.43437864445205	69609
089e9f97473e9cffd0d6f11cd20bd67eeb6ec2eb	comment on lawler's multilevel boolean minimization	minimization;generalized prime implicants;incompletely specified functions;logic design;minimal forms;multilevel logic design	Lawler [1] proposed a method of multilevel Boolean minimization which is a generalization of the conventional prime implicant approach for two-level minimization. In his article, he presented two lemmas (Lemmas 1 and 2) that are important in reducing the number of calculations involved in obtaining a solution. These lemmas can be extended to increase the efficiency even more. In particular, it is possible to lower the upper limit of the function interval for an incompletely specified function [2]. Later, the extended version of these lemmas will be shown to be of value in several aspects of the minimization procedure.	boolean circuit;circuit minimization for boolean functions	Ronald C. de Vries	1970	Commun. ACM	10.1145/362258.362301	combinatorics;discrete mathematics;logic synthesis;computer science;mathematics;algorithm	EDA	20.949321803310365	43.815652239049896	69857
901c632a9c5a0a1460e0428f252c5649e6e0e62f	a reduced memory bandwidth and high throughput hdtv motion compensation decoder for h.264/avc high 4:2:2 profile	h 264 avc 4 2 2 high profile;motion compensation;enhanced video quality;real time video coding;hardware design	This article presents the HP422-MoCHA: optimized Motion Compensation hardware architecture for the High 4:2:2 profile of H.264/AVC video coding standard. The proposed design focuses on real-time decoding for HDTV 1080p (1,920 × 1,080 pixels) at 30 fps. It supports multiple sample bit-width (8, 9, or 10 bits) and multiple chroma sub-sampling formats (4:0:0, 4:2:0, and 4:2:2) to provide enhanced video quality experience. The architecture includes an optimized sample interpolator that processes luma and chroma samples in two parallel datapaths and features quarter sample accuracy, bi-prediction and weighted prediction. HP422-MoCHA also includes a hardwired Motion Vector Predictor, supporting temporal and spatial direct predictions. A novel memory hierarchy implemented as a 3-D Cache reduces the frame memory access, providing, on average, 62% of bandwidth and 80% of clock cycles reduction. The design was implemented in a Xilinx Virtex-II PRO FPGA, and also in an ASIC with a TSMC 0.18 μm standard cells technology. The ASIC implementation occupies 102 K equivalent gates and 56.5 KB of on-chip SRAM in a 3.8 × 3.4 mm2 area. It presents a power consumption of 130 mW. Both implementations reach a maximum operation frequency of ~100 MHz, being able to motion compensate 37 bi-predictive frames or 69 predictive fps. The minimum required frequency to ensure the real-time decoding for HD1080p at 30 fps is 82 MHz. Since HP422-MoCHA is the first Motion Compensation architecture for the High 4:2:2 profile found in the literature, a Main profile MoCHA was used for comparison purposes, showing the highest throughput among all presented works. However, the HP422-MoCHA architecture also reaches the highest throughput when compared with the other published Main profile MC solutions, even considering the significantly higher complexity of the High 4:2:2 profile.	32-bit;algorithm;application-specific integrated circuit;chroma subsampling;clock signal;data compression;field-programmable gate array;h.264/mpeg-4 avc;interpolation;macroblock;memory bandwidth;memory hierarchy;motion compensation;pixel;real-time clock;real-time computing;sampling (signal processing);static random-access memory;throughput;video coding format;virtex (fpga)	Bruno Zatt;Leandro M. de L. Silva;Arnaldo Azevedo;Luciano Volcan Agostini;Altamiro Amadeu Susin;Sergio Bampi	2011	Journal of Real-Time Image Processing	10.1007/s11554-011-0216-7	embedded system;real-time computing;quarter-pixel motion;computer science;motion compensation;algorithm	EDA	12.632900815839479	40.62601409766944	69865
ed6facbc5a1ea37dd12635fb35624ba47ad57f28	efficient 45nm asic architecture for full-search free intra prediction in real-time h.264/avc decoder	h 264 avc;intra prediction;45nm asic;video	The standard H.264/AVC Intra frame encoding process has several data dependent and computational intensive coding methodologies that limit the overall encoding speed. It causes not only a high degree of computational complexity but also an unacceptable delay especially for the real-time video applications. Based on DCT properties and spatial activity analysis, low power hardware architecture for high throughput Full-Search Free (FSF) Intra mode selection and direction prediction algorithm is proposed. The FSF Intra prediction Algorithm significantly reduces the computational complexity and the processing run-time required for the H.264/AVC Intra frame prediction process. The ASIC implementation for the proposed architecture is carried out and synthesizing results are obtained. The heavily tested 45nm ASIC design is able to achieve an operating frequency of 140 MHz while limiting the overall power consumption to 9.01 mW, which nominates our proposed FSF Intra prediction architecture for interactive real-time H.264/AVC mobile video decoders. T. A. Elarabi (B) · R. Ayoubi · H. Mahmoud · M. A. Bayoumi The Center for Advanced Computer Studies (CACS), University of Louisiana at Lafayette, Lafayette, LA, USA e-mail: tae9963@cacs.louisiana.edu Randa Ayoubi e-mail: rxa3595@cacs.louisiana.edu M. Bayoumi e-mail: mab@cacs.louisiana.edu	algorithm;application-specific integrated circuit;clock rate;coefficient;computational complexity theory;discrete cosine transform;elegant degradation;email;h.264/mpeg-4 avc;low-power broadcasting;peak signal-to-noise ratio;pixel;real-time clock;real-time transcription;throughput	Tarek A. Elarabi;Randa Ayoubi;Hanan A. Mahmoud;Magdy A. Bayoumi	2013	Signal Processing Systems	10.1007/s11265-012-0700-8	embedded system;real-time computing;video;telecommunications;computer science	Graphics	12.780943721079328	40.904925911578246	69869
2d87d9cb3e3be2fe0a860e1ae0b8f1fa40fe3ee4	"""constrained """"modern"""" floorplanning"""	floorplanning;rectilinear polygons;relative position;input constraint;network flow	"""This paper presents algorithms for a constrained version of the """"modern"""" floorplanning problem proposed by Kahng in """"Classical Floorplanning Harmful?"""" [1]. Specically, the constrained modern floorplanning problem (CMFP) is suitable when die-size is fixed, modules are permitted to have rectilinear shapes, and, in addition, the approximate relative positions of the modules are known. This formulation is particularly useful in two scenarios: (1) assisting an expert floorplan architect in a semi-automated floorplan methodology and (2) in incremental floorplanning. CMFP is shown to be NP hard. An algorithm based on a max-flow network formulation quickly identifies input constraints that are impossible to meet, thus permitting the floorplan architect to modify these constraints. Three algorithms (BFS, IBFS, CBFS) based on using BFS numbers to assign costs in a min-cost max-flow network formulation are presented. Experiments on standard benchmarks demonstrate that BFS and IBFS are fast and obtain zero whitespace floorplans."""	approximation algorithm;be file system;benchmark (computing);breadth-first search;floorplan (microelectronics);flow network;maximum flow problem;multistage interconnection networks;np (complexity);regular grid;semiconductor industry	Yan Feng;Dinesh P. Mehta;Hannah Honghua Yang	2003		10.1145/640000.640030	floorplan;mathematical optimization;flow network;computer science;mathematics;algorithm	EDA	19.75727468903112	39.552829997381586	69904
3bcb6c329fd2f47c4e9d080cdd16c959fe16949f	network movement games	speed of convergence;price of anarchy;movement problems;price of stability;network creation games	We introduce a new class of games, called Network Movement Games, naturally related to the movement problems of 7, which models the spontaneous creation of multi-hop communication networks by the distributed and uncoordinated movement of k selfish mobile devices placed on the nodes of an underlying graph. Devices are players aiming to find the final positions which achieve a global property of the induced subnetwork. We actually focus on solutions (i.e. Nash equilibria) connecting all the players while minimizing the distances from their home locations. We show that the game always admits a pure Nash equilibrium, and that the convergence after a finite number of improving moves is guaranteed only when players perform their best possible moves; in this case, we provide tight bounds on the speed of convergence to Nash equilibria, both when initial positions are arbitrary and when players start at their home positions. As for the Nash equilibria performances, we first prove that the price of stability is 1 (i.e., an optimal solution is also a Nash equilibrium). Furthermore, we provide tight bounds on the price of anarchy, also showing that better performances are obtained when players start at their home positions. Finally, through extensive experiments, we show that high bounds on the price of anarchy as well as high convergence time of best move dynamics to Nash equilibria are only due to pathological worst cases. Moreover, quite often improving move dynamics (i.e., where players do not necessarily perform a best possible move) converge to Nash equilibria on the tested instances even though the class of instances does not guarantee the convergence.	graph (discrete mathematics);image scaling;loss function;many-to-many;multicast;nash equilibrium;preprocessor	Michele Flammini;Vasco Gallotti;Giovanna Melideo;Gianpiero Monaco;Luca Moscardelli	2017	Theor. Comput. Sci.	10.1016/j.tcs.2016.12.029	price of stability;epsilon-equilibrium;mathematical optimization;simulation;best response;folk theorem;risk dominance;mathematical economics;equilibrium selection;price of anarchy;nash equilibrium	ECom	18.362698102497824	36.14430589800999	69992
a535b250be06acc5103d828c6d66904b698f6b14	collision-free network exploration	mobile agents;synchronous agents;network exploration	A set of mobile agents is placed at different nodes of a n-node network. The agents synchronously move along the network edges in a collision-free way, i.e., in no round may two agents occupy the same node. In each round, an agent may choose to stay at its currently occupied node or to move to one of its neighbors. An agent has no knowledge of the number and initial positions of other agents. We are looking for the shortest possible time required to complete the collision-free network exploration, i.e., to reach a configuration in which each agent is guaranteed to have visited all network nodes and has returned to its starting location. We first consider the scenario when each mobile agent knows the map of the network, as well as its own initial position. We establish a connection between the number of rounds required for collision-free exploration and the degree of the minimum-degree spanning tree of the graph. We provide tight (up to a constant factor) lower and upper bounds on the collision-free exploration time in general graphs, and the exact value of this parameter for trees. For our second scenario, in which the network is unknown to the agents, we propose collision-free exploration strategies running in O(n) rounds for tree networks and in O(n log n) rounds for general networks. keywords: mobile agents, network exploration, synchronous agents		Jurek Czyzowicz;Dariusz Dereniowski;Leszek Gasieniec;Ralf Klasing;Adrian Kosowski;Dominik Pajak	2017	J. Comput. Syst. Sci.	10.1016/j.jcss.2016.11.008	real-time computing;simulation;distributed computing	ECom	17.973509640803258	34.31880261934329	70279
3ee80cdf32c73cdfc3dd76ecc72f76811f6655b9	on the self-stabilization of mobile robots in graphs	distributed system;cluster computing;building block;mobile robot;agent modeling;distributed computing;transient fault;leader election;data structure	Self-stabilization is a versatile technique to withstand any transient fault in a distributed system. Mobile robots (or agents) are one of the emerging trends in distributed computing as they mimic autonomous biologic entities. The contribution of this paper is threefold. First, we present a new model for studying mobile entities in networks subject to transient faults. Our model differs from the classical robot model because robots have constraints about the paths they are allowed to follow, and from the classical agent model because the number of agents remains fixed throughout the execution of the protocol. Second, in this model, we study the possibility of designing self-stabilizing algorithms when those algorithms are run by mobile robots (or agents) evolving on a graph. We concentrate on the core building blocks of robot and agents problems: naming and leader election. Not surprisingly, when no constraints are given on the network graph topology and local execution model, both problems are impossible to solve. Finally, using minimal hypothesis with respect to impossibility results, we provide deterministic and probabilistic solutions to both problems, and show equivalence of these problems by an algorithmic reduction mechanism. Key-words: mobile agents, mobile robots, graphs, self-stabilization, leader election, naming Sur l’Auto-stabilisation de Robots Mobiles dans un	algorithm;autonomous robot;distributed computing;entity;leader election;mobile agent;mobile robot;self-stabilization;topological graph theory;turing completeness	Lélia Blin;Maria Gradinariu Potop-Butucaru;Sébastien Tixeuil	2007		10.1007/978-3-540-77096-1_22	mobile robot;mathematical optimization;combinatorics;simulation;data structure;computer cluster;computer science;theoretical computer science;leader election;distributed computing;algorithm	AI	16.918451783957043	35.067894982644574	70426
92a7115b708910dd2c93d6beff8c8b15f6d06460	the deletable bloom filter: a new member of the bloom family	hachage;false negative free deletions;mathematics;memory management;probabilistic filter applications;bloom filter;negativo falso;helium;set theory data structures;memory consumption;radiation detectors;information filtering;construction industry;false negative;filtre de bloom;packet switching;probabilistic approach;set theory;data structures counting circuits information filtering information filters encoding costs mathematics hardware roads;conmutacion por paquete;arrays;counting circuits;codificacion;bloom filter deletions packet forwarding;dlbf design deletable bloom filter data structure encoding false negative free deletions memory consumption probabilistic filter applications;hashing;roads;data structures;enfoque probabilista;approche probabiliste;estructura datos;coding;structure donnee;middleboxes;probabilistic logic;information filters;encoding;dlbf design;data structure;artigo de periodico;faux negatif;commutation paquet;packet forwarding;deletions;deletable bloom filter;codage;hardware	We introduce the Deletable Bloom filter (DlBF) as a new spin on the popular data structure based on compactly encoding the information of where collisions happen when inserting elements. The DlBF design enables false-negative-free deletions at a fraction of the cost in memory consumption, which turns to be appealing for certain probabilistic filter applications.	be file system;bloom filter;dspace;data structure;network packet	Christian Esteve Rothenberg;Carlos Alberto Braz Macapuna;Fábio Luciano Verdi;Maurício F. Magalhães	2010	IEEE Communications Letters	10.1109/LCOMM.2010.06.100344	hash function;data structure;telecommunications;computer science;theoretical computer science;bloom filter;packet forwarding;probabilistic logic;coding;helium;particle detector;algorithm;packet switching;encoding;set theory;memory management	DB	10.08372606657015	37.00252779735515	70501
b652a52fd7ee0f97d47cb3d49b14cd5508b1e969	profiling of dataflow programs using post mortem causation traces	profiling;avc h 264 video decoders dataflow programs post mortem causation traces data stream natural representation media processing applications iso video coding standards signal processing algorithms real world applications mpeg 4 sp video decoders;decoding algorithm design and analysis transform coding measurement optimization video coding;video coding;dataflow;data flow computing;video codecs;profiling dataflow causation trace;video coding data flow computing video codecs;causation trace	The natural representation of data streams, parallelism, and composition has made dataflow an attractive programming model for expressing a wide range of stream and media processing applications, and has led MPEG and ISO to base their latest video coding standards on this model. This paper describes and compares methodologies and metrics for the optimization of signal processing algorithms represented as dataflow programs. Our approach is based on the analysis of traces and addresses some of the complexity challenges that arise from the very large data sets that are required for evaluating real-world applications. The methodology and experimental results are demonstrated and evaluated in two at-size case studies, an MPEG-4 SP and an AVC/H.264 video decoders.	algorithm;bottleneck (software);causality;computation;correlation does not imply causation;critical path method;data compression;dataflow programming;digital footprint;h.264/mpeg-4 avc;mathematical optimization;moving picture experts group;network congestion;parallel computing;profiling (computer programming);programming model;signal processing;slack variable;time complexity;tracing (software);video coding format	Simone Casale Brunet;Marco Mattavelli;Jörn W. Janneck	2012	2012 IEEE Workshop on Signal Processing Systems	10.1109/SiPS.2012.54	scalable video coding;parallel computing;real-time computing;computer science;theoretical computer science;operating system;dataflow;profiling;multiview video coding	Embedded	11.245381122493624	39.29311868024312	71098
23963d2eaba199a7227cb85c0a5b82f2b7b0e34e	a codesign case study: implementing arithmetic functions in fpga's	arithmetic field programmable gate arrays software prototyping design methodology system performance image analysis software performance hardware prototypes bandwidth;image sequences field programmable gate arrays digital arithmetic digital signal processing chips circuit cad software engineering;design methods;fpga;3d image analysis;software engineering;arithmetic function;design method;codesign;digital signal processing chips;digital arithmetic;circuit cad;field programmable gate arrays;arithmetic functions;300 mbyte arithmetic functions fpga technology codesign integers system performance dedicated 3d image analysis prototype system vector length calculation board total computing time image data network bandwidth;image sequences	Diierent way of implementing and designing arithmetic functions for 16/32 bit integers in FPGA technology are studied. This also includes a comparison of four diierent design methods. The results are used to increase the overall system performance in a dedicated 3D image analysis prototype system by moving a vector length calculation from software to hardware. The conclusion is that by adding one relatively simple board containing two FPGA's in the prototype setup, the total computing time is reduced by 30 %. The total amount of image data, in this case 300 Mbyte which has to be transmitted via network, is reduced by a factor of two, and the required network bandwidth is reduced similarly.	field-programmable gate array;image analysis;megabyte;prototype	Ilya V. Klotchkov;S. Pedersen	1996		10.1109/ECBS.1996.494565	embedded system;computer architecture;electronic engineering;design methods;computer science;arithmetic function;field-programmable gate array;computer engineering	Arch	12.339845458194908	43.17211954945484	71158
d2756e95d0286fe76f6f83223ae3de81c71bfa27	decimal addition on fpga based on a mixed bcd/excess-6 representation		Abstract Decimal arithmetic has recovered the attention in the field of computer arithmetic due to decimal precision requirements of application domains like financial, commercial and internet. In this paper, we propose a new decimal adder on FPGA based on a mixed BCD/excess-6 representation that improves the state-of-the-art decimal adders targeting high-end FPGAs. Using the proposed decimal adder, a multioperand adder and a mixed binary/decimal adder are also proposed. The results show that the new decimal adder is very efficient improving the area and delay of previous state of the art decimal adders, multioperand decimal addition and binary/decimal addition.	binary-coded decimal;field-programmable gate array	Horácio C. Neto;Mário P. Véstias	2017	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2017.10.004	decimal32 floating-point format;decimal64 floating-point format;decimal128 floating-point format;parallel computing;binary integer decimal;decimal floating point;decimal;computer science;adder;binary number	EDA	10.780421922813678	43.91807999858301	71564
022fc7de462cec63d2fb40a167229a43ceccd18c	a power-aware motion estimation architecture using content-based subsampling	content management;estensibilidad;processing element;switching activity;power aware system;estimation mouvement;multimedia;image processing;data compression;compression video;edge detection;low frequency;estimacion movimiento;adaptive control;circuit vlsi;video compression;systolic array;procesamiento imagen;turn off;edge extraction;motion estimation;gestion contenido;traitement image;deteccion contorno;detection contour;vlsi circuit;compression image;image compression;macro block;gestion contenu;extensibilite;scalability;compresion dato;power consumption;circuito vlsi;article;compression donnee;compresion imagen;vlsi architecture	This paper presents a novel power-aware motion estimation architecture for battery-powered multimedia devices. As the battery status changes, the proposed architecture adaptively performs graceful tradeoffs between power consumption and compression quality. The tradeoffs are considered to be graceful in that the proposed architecture is scalable with changing conditions and the compression quality is slightly degraded as the available energy is depleted. The key to such tradeoffs lies in a content-based subsample algorithm, first proposed in this paper. As the available energy decreases, the algorithm raises the subsample rate for maximizing the battery lifetime. Differently from the existing subsample algorithms, the content-based algorithm first extracts edge pixels from a macro-block and then subsamples the remaining low-frequency part. By doing so, we can alleviate the aliasing problem and, thus, limit the quality degradation as the subsample rate increases. Given a power consumption mode, the proposed architecture first performs edge extraction to generate a turn-off mask and then uses the turn-off mask to reduce the switch activities of processing elements (PEs) in a semi-systolic array. The reduction of switch activities results in significant power consumption savings. To achieve a high degree of scalability and qualified power-awareness, we use an adaptive control mechanism to set the threshold value for edge determination and make the reduction of switch activities rather stationary. As shown by experimental results, the architecture can dynamically operate in different power consumption modes with little quality degradation according to the remaining capacity of the battery pack while the power overhead of edge extraction is kept under 0.8%	algorithm;aliasing;chroma subsampling;elegant degradation;microprocessor;microsoft edge;motion estimation;network switch;overhead (computing);pixel;programming paradigm;scalability;semiconductor industry;simulation;smoothing;stationary process;systolic array	Hsien-Wen Cheng;Lan-Rong Dung	2006	J. Inf. Sci. Eng.		data compression;embedded system;computer vision;real-time computing;adaptive control;image processing;computer science	Arch	14.589505327202964	39.82697722403105	71571
deb5e6baf2b6469f2e4e03253efea379991ed3bb	spectral theory of disjunctive decomposition for balanced boolean functions	logic design;integrated circuit design boolean functions adders multiplexing equipment vlsi logic design;boolean functions;walsh transform;walsh spectrum;boolean function;logic circuits;multiplexing equipment;spectrum;satisfiability;multiplexing;balanced boolean functions;integrated circuit design;vectors;spectral conditions spectral theory disjunctive decomposition balanced boolean functions walsh spectrum logic circuits adders parity checkers multiplexers;discrete transforms;matrix decomposition;parity checkers;adders;logic testing;boolean functions circuit synthesis logic testing multiplexing vectors matrix decomposition adders logic design discrete transforms circuit testing;vlsi;circuit testing;spectral conditions;disjunctive decomposition;multiplexers;circuit synthesis;spectral theory	A theory has been developed to identify simple disjunctive decomposition of balanced Boolean functions through the Walsh spectrum of such functions. Many common logic circuits such as adders, parity checkers and multiplexers are balanced Boolean functions. Here, various types of decomposition of the balanced functions are considered and their corresponding spectral conditions that have to be satisfied for their existence are listed.	disjunctive normal form	Bogdan J. Falkowski;Sudha Kannurao	2000		10.1109/ICVD.2000.812658	boolean circuit;and-inverter graph;combinatorics;electronic engineering;discrete mathematics;boolean expression;standard boolean model;computer science;mathematics;boolean function;algorithm;parity function	Logic	19.229655611647676	45.497109841289706	71579
d46125d818808bfc9ac388f314bcc85242542bd4	brief announcement: a decentralized algorithm for distributed trigger counting	distributed system;randomized algorithm	Consider a distributed system with n processors, in which each processor receives some triggers from an external source. The distributed trigger counting problem is to raise an alert and report to a user when the number of triggers received by the system reaches w, where w is a user-specified input. The problem has applications in monitoring, global snapshots, synchronizers and other distributed settings. The main result of the paper is a decentralized and randomized algorithm with expected message complexity O(n log n log w). Moreover, every processor in this algorithm receives no more than O(log n log w) messages with high probability.	algorithm	Venkatesan T. Chakaravarthy;Anamitra R. Choudhury;Vijay K. Garg;Yogish Sabharwal	2010		10.1007/978-3-642-15763-9_38	real-time computing;computer science;theoretical computer science;distributed computing;randomized algorithm	Logic	16.588986752614982	33.91394603206877	71785
51791d6c03b50eeb37e8410de9ae6c28c1765a09	vlsi implementation of enhanced edge preserving impulse noise removal technique	nonlinear filters;stage pipeline architecture edge preserving impulse noise removal technique enhancement image signals video signals low cost vlsi architecture line buffers register banks impulse noise detector edge oriented noise filter impulse arbiter storage space fixed size window variable window size computation complexity power consumption reduction visual quality image denoising speed improvement xilinx 9 2i;video signal processing;edge detection;impulse noise;vlsi computational complexity image denoising video signal processing;computer architecture;adaptive filters;image edge detection;computational complexity;image reconstruction;edge detection impulse noise vlsi architecture image denoising;maximum likelihood detection;vlsi;image denoising;noise adaptive filters image edge detection computer architecture image reconstruction maximum likelihood detection nonlinear filters;noise;vlsi architecture	In many applications, image and video signals are corrupted by impulse noise during acquisition or transmission. Hence there is a need for an efficient and consumer friendly impulse noise removal technique. In this paper, an efficient low cost VLSI architecture for the edge preserving impulse noise removal technique has been proposed. The architecture comprises of two line buffers, register banks, impulse noise detector, edge oriented noise filter and impulse arbiter. The storage space required for the proposed hardware is two line buffer rather than full frame memory. Moreover, proposed algorithm involves only fixed size window instead of variable window size. These two greatly reduces storage requirement as well as computation complexity. The impulse noise detector turns off the remaining circuitry if the current pixel is noise free, thus reducing power consumption. Further, the four stage pipeline architecture greatly improves the speed of operation. The implemented edge preserving algorithm results in better visual quality for denoised image. Thus the proposed architecture has less complexity, less storage requirement, low power consumption and improved speed of operation. The architecture has been implemented in Xilinx 9.2i and the results are tabulated for various images.	algorithm;arbiter (electronics);complexity;computation;electronic circuit;impulse noise (audio);noise reduction;pipeline (computing);pixel;very-large-scale integration	P. Deepa;C. Vasanthanayaki	2013	2013 26th International Conference on VLSI Design and 2013 12th International Conference on Embedded Systems	10.1109/VLSID.2013.170	iterative reconstruction;adaptive filter;computer vision;electronic engineering;real-time computing;edge detection;impulse noise;computer science;noise;very-large-scale integration;computational complexity theory	EDA	12.646885926733738	42.054338592365234	72005
49d6f1d25532d7a9c3203fff7b739c5861907f13	word level functional coverage computation	formal specification;sequential circuits circuit simulation formal specification formal verification;canonical form;linear taylor expansion diagram word level coverage computation functional coverage computation word level coverage metric sequence properties intermediate signals multiplexers linear integer equations rt level properties;sequential circuits;sequence properties;linear integer equations;functional coverage computation;taylor expansion;linear taylor expansion diagram;intermediate signals;circuit simulation;word level coverage metric;formal verification;rt level properties;software requirements and specifications;data structures boolean functions circuit simulation logic equations computational modeling taylor series microelectronics research and development multiplexing;word level coverage computation;multiplexers;sequential logic circuits	This paper proposes a word-level coverage metric to determine the completeness of a set of properties verified by a word-level method. An algorithm is presented to compute a functionality based coverage metric for a sequence property as specification. Control, intermediate and output signals are represented by a multiplexer based structure of linear integer equations, and RT level properties are directly applied to this representation. A set of integer equations are symbolically simulated based on the specified property in a predictable time. We used a canonical form of linear Taylor Expansion Diagram.	algorithm;computation;diagram;multiplexer	Bijan Alizadeh	2006	Asia and South Pacific Conference on Design Automation, 2006.	10.1145/1118299.1118304	multiplexer;canonical form;mathematical optimization;electronic engineering;discrete mathematics;formal verification;computer science;taylor series;theoretical computer science;formal specification;mathematics;sequential logic;programming language;algorithm	EDA	19.847358463551014	45.982808205419985	72200
b8895d447ecb33d8dd8ebc92096b1283679660fc	fast and scalable selection algorithms with applications to median filtering	scalable selection algorithm;parallel algorithm;scalable median filtering reconfigurable optical bus system median filtering radix spl omega representation parallel algorithm reconfigurable optical buses sorting algorithm;image processing;median filter;sorting;efficient algorithm;reconfigurable optical bus system;indexing terms;sorting parallel algorithms image processing;cost optimization;filtering algorithms optical filters sorting parallel algorithms optical arrays optical sensors algorithm design and analysis image processing military computing concurrent computing;parallel algorithms	The main contributions of this paper are in designing fast and scalable parallel algorithms for selection and median filtering. Based on the radix-! representation of data and the prune-and-search approach, we first design a fast and scalable selection algorithm on the arrays with reconfigurable optical buses (AROB). To the authors’ knowledge, this is the most time efficient algorithm yet published, especially compared to the algorithms proposed by Han et al. [8] and Pan [16]. Then, given an N N image and a W W window, based on the proposed selection algorithm, several scalable median filtering algorithms are developed on the AROB model with a various number of processors. In the sense of the product of time and the number of processors used, most of the proposed algorithms are time or cost optimal.	central processing unit;cost efficiency;han unification;median filter;parallel algorithm;prune and search;scalability;selection algorithm	Chin-Hsiung Wu;Shi-Jinn Horng	2003	IEEE Trans. Parallel Distrib. Syst.	10.1109/TPDS.2003.1239867	parallel computing;image processing;computer science;theoretical computer science;distributed computing;parallel algorithm	EDA	13.05338464889603	37.81176117189672	72225
eada1f97a925f582852870ca1e715e64200c3ccc	improved routing strategies with succinct tables	succinct table;improved routing strategy	In designing a routing scheme for a communication network it is desirable to use as short as possible paths for routing messages, while keeping the routing information stored in the processors’ local memory as succinct as possible. The efftciency of a routing scheme is measured in terms of its stretch factor-the maximum ratio between the cost of a route computed by the scheme and that of a cheapest path connecting the same pair of vertices. This paper presents several simple families of routing schemes for general networks, featuring some desirable properties. Our two main families are the hierarchical covering pi&s schemes HCP, and the hierarchical balanced schemes HB, (for k t 1). The scheme HCP, guarantees a stretch factor of 2’ 1 and requires storing a total of O(k . n’+l/k log’n) bits of routing information in the network. The new important	routing;telecommunications network	Baruch Awerbuch;Amotz Bar-Noy;Nathan Linial;David Peleg	1990	J. Algorithms	10.1016/0196-6774(90)90017-9	mathematical optimization;combinatorics;theoretical computer science;destination-sequenced distance vector routing;mathematics;algorithm	Theory	20.609682932459155	33.76652933083813	72289
b26314d4117cd37b5cca826b03cb70dc96546936	generalized fault tolerance properties of star graphs	star graph;fault tolerant	Recently an attractive interconnection topology has been proposed which compares very favorably with the well known n-cubes in terms of degree, diameter, fault tolerance and resilience. In this paper we investigate further fault tolerance properties of star graphs using the generalized measure of fault tolerance as introduced recently to study n-cubes. Star graphs are found again to compare favorably with n-cubes even using this generalized measure of fault tolerance.	fault tolerance	Pradip K. Srimani	1991		10.1007/3-540-54029-6_200	combinatorics;discrete mathematics	Networks	23.80701178730614	35.268863142278846	72796
7d0443e2d12d7f6f87fda46b32b4ca6a28d6f5c1	a design for multiple-valued logic gates based on mesfet's	logical complement;logical complement multiple valued logic circuit field effect transistor literal gate min gate max gate;multiple valued logic circuit;min gate;max gate;literal gate;field effect transistor;multiple valued logic	The majority of the work in the multiple-valued logic (MVL) area has been devoted to the study of various algebras and their properties. One such algebra was proposed by Allen and Givone. This paper is concerned with circuit realizations for this algebra. GaAs MESFET's are used as the basic circuit elements. Several five-valued circuits are proposed including a LITERAL gate, a MAX gate, and a MIN gate. The circuit analysis program SPICE is used to analyze the circuits, and the results are presented.	max;network analysis (electrical circuits);spice	Joseph G. Tront;Donald D. Givone	1979	IEEE Transactions on Computers	10.1109/TC.1979.1675265	field-effect transistor;and-or-invert;nor gate;nor logic;xor gate;toffoli gate;delay calculation;logic gate;programmable logic array;gate equivalent;theoretical computer science;pass transistor logic;quantum circuit;mathematics;and gate;nand gate;xnor gate;or gate;inverter;algorithm;resistor–transistor logic;quantum gate	EDA	19.283315408896705	45.458584582015796	72824
664bdf260fda8e9d3871a1d009b84356da019e84	analog assertion-based verification on partial state space representations using asl	formal specification;transient analysis data models analog circuits integrated circuit modeling labeling vectors charge pumps;transient analysis;formal verification analogue circuits electronic design automation formal specification;analog circuits;formal verification;vectors;integrated circuit modeling;charge pumps;eda design flow assertion based analog property verification partial state space representations asl analog specification language state space property specification formal property specification complex analog circuit properties transient simulation waveforms periodic behavior startup times complex analog behavior time signal properties time domain signals cmos charge pump analog circuit verification automation;analogue circuits;labeling;data models;electronic design automation	In this contribution a novel approach to assertion-based analog property verification by considering state space property specification is introduced. In order to apply a formal property specification of complex analog circuit properties to transient simulation waveforms using the Analog Specification Language (ASL), a partial analog state space model is developed to which the simulation waveforms are transferred. In contrast to other approaches operating directly on transient waveforms, on the partial state space model, properties such as periodic behavior, startup times and other complex analog behavior can be systematically specified and automatically verified. Due to the different perspective of state space property specification compared to approaches considering only time signal properties, critical behavior that may be overlooked in time domain signals can be detected in the state space domain. A verification methodology is introduced and a case study on complex properties of a CMOS charge pump shows the feasibility and practicability of the approach for improving the automation of analog verification.	analogue electronics;assertion (software development);cmos;charge pump;formal verification;simulation;specification language;state space;state-space representation	Sebastian Steinhorst;Lars Hedrich	2012	Proceeding of the 2012 Forum on Specification and Design Languages		data modeling;labeling theory;real-time computing;electronic design automation;formal verification;analogue electronics;computer science;theoretical computer science;formal specification;programming language	SE	21.950589856163496	45.998708343418514	72888
80369b2d3f5746fe7ae85ab979fe137742926703	an ultra-fast and low-power design of analog circuit network for dog pyramid construction of sift algorithm	transforms analogue circuits cmos analogue integrated circuits current mode circuits gaussian processes low power electronics network synthesis;sub threshold;real time;power 1 8 mw low power design analog circuit network ultra fast design dog pyramid construction sift algorithm computer vision method image feature extraction energy efficient embedded machine vision systems hardware accelerator gaussian scale space establishment current mode circuit network sub threshold region power dissipation adc standard cmos process difference of gaussian pyramid construction;navigation digital signal processing manganese;sift;analog circuit network;gaussian convolution;sub threshold sift gaussian convolution analog circuit network real time	SIFT algorithm is the most popular and widely used computer vision method for image feature extraction, because of its excellent robustness for various transformations, while the computational burden and huge hardware requirements limit the applications of SIFT algorithm, especially for the real-time and energy-efficient embedded machine vision systems. In this paper, a hardware accelerator of analog circuit network for the Gaussian scale space establishment has been proposed, which is the most computationally expensive of SIFT algorithm. The accelerator adopts a current-mode circuit network operating in sub-threshold region, so that the power dissipation can be very low. Moreover, extensive repetitive multiplication and addition operations are completed in analog domain directly, before the original data is transformed to digital domain by ADC. Thus, the calculation time is approximately equal to the settling time of circuit network, which is usually down to several nano-seconds. Simulations with standard CMOS process have demonstrated that the processing speed is about 400 ps/pixel, while the power consumption is only 1.8 mW. The overall processing speed for VGA-format (640*480) image is up to 600 fps, which is at least 27X faster than the state-of-art digital counterparts systems.	algorithm;analogue electronics;analysis of algorithms;approximation;cmos;computer simulation;computer vision;embedded system;feature (computer vision);feature extraction;gnu nano;hardware acceleration;low-power broadcasting;machine vision;pixel;real-time clock;requirement;scale space;scale-invariant feature transform;settling time;video graphics array	Zheyu Liu;Fei Qiao;Qi Wei;Xinghua Yang;Yi Li;Huazhong Yang	2016	2016 17th International Symposium on Quality Electronic Design (ISQED)	10.1109/ISQED.2016.7479233	embedded system;electronic engineering;telecommunications;computer science;electrical engineering;theoretical computer science;scale-invariant feature transform	EDA	13.869574889570096	42.17666664146798	72959
1582c6b2393cec6a9998175347a1e4ac777fc691	fault tolerance in cellular automata at low fault rates		A commonly used model for fault-tolerant computation is that of cellular automata. The essential difficulty of fault-tolerant computation is present in the special case of simply remembering a bit in the presence of faults, and that is the case we treat in this paper. The conceptually simplest mechanism for correcting errors in a cellular automaton is to determine the next state of a cell by taking a majority vote among its neighbors (including the cell itself, if necessary to break ties). We are interested in which regular two-dimensional tessellations can tolererate faults using this mechanism, when the fault rate is sufficiently low. We consider both the traditional transient fault model (where faults occur independently in time and space) and a recently introduced combined fault model which also includes manufacturing faults (which occur independently in space, but which affect cells for all time). We completely classify regular two-dimensional tessellations as to whether they can tolerate combined transient and manufacturing faults, transient faults but not manufacturing faults, or not even transient faults.	automata theory;cellular automaton;computation;directed graph;fault model;fault tolerance;graph (discrete mathematics)	Mark McCann;Nicholas Pippenger	2013	J. Comput. Syst. Sci.	10.1016/j.jcss.2013.02.001		Theory	23.085446449233118	34.991982783407	72968
ce07b40d25b2814bc89a6f5b89ba23e9e62401c9	setting ports in an anonymous network: how to reduce the level of symmetry?	port labeled network;view;level of symmetry;anonymous network	A fundamental question in the setting of anonymous graphs concerns the ability of nodes to spontaneously break symmetries, based on their local perception of the network. In contrast to previous work, which focuses on symmetry breaking under arbitrary port labelings, in this paper, we study the following design question: Given an anonymous graph G without port labels, how to assign labels to the ports of G, in interval form at each vertex, so that symmetry breaking can be achieved using a message-passing protocol requiring as few rounds of synchronous communication as possible?		Ralf Klasing;Adrian Kosowski;Dominik Pajak	2016		10.1007/978-3-319-48314-6_3	telecommunications;engineering;distributed computing;computer security	Vision	17.79237481610697	35.08402733079895	73234
4a9ff27fe9f8d465412d04350effd58f83859cff	efficient decimator and interpolator array structures		New and efficient array processor implementations of polyphase FIR and IIR decimators and interpolators, with integer compression and expansion factors, are derived using an algebraic mapping technique. The technique is based on the time-domain representation of the algorithms. It has the merit of being suitable for describing multirate algorithms. The control signals necessary to implement the polyphase structures are explicitly identified. Different array structures are derived in which the inputs are broadcast or pipelined and outputs are pipelined or added simultaneously. Upper bounds on the input/output processing rates are provided in terms of system parameters and hardware delays. The work is extended to map decimators/interpolators with fractional compression/expansion factors onto systolic hardware structures. The new structures have the advantages of being modular, regular, hierarchical, and pipelined.		Esam Abdel-Raheem;Fayez El Guibaly;Andreas Antoniou	1995	Journal of Circuits, Systems, and Computers	10.1142/S0218126695000163	electronic engineering;real-time computing;theoretical computer science;mathematics	Logic	12.926596074256798	43.64513256001986	73324
84c2ec965462d7f056ed243d08288e9e4f2cb25a	reed-muller realization of x (mod p)		This article provides a novel technique of X (mod P) realization. It is based on the Reed-Muller polynomial expansion. The advantage of the approach concludes in the capability to realize X (mod P) for an arbitrary P. The approach is competitive with the known realizations on the speed processing. Advantages and results of comparison with the known approaches for X [9:1] and P=7 is demonstrated. Keywords—modular arithmetic, residue number system, X (mod P), Reed-Muller expansion	mod database;polynomial expansion;reed–muller code;reed–muller expansion;residue number system	Danila A. Gorodecky	2015	CoRR		discrete mathematics;pure mathematics;mathematics	AI	17.04724483239131	43.55295842680329	73593
a163a192da6c1843e1f1f9aa2b29c870ede0ceb2	coloring random graphs: a short survey	computers;graph theory;random graph;constant degree chromatic number random graphs probability space erd s renyi graphs regular graphs threshold behavior;space technology moment methods scientific computing paper technology informatics chebyshev approximation physics computer simulation mathematics graph theory;chromatic number;moment methods;constant degree;random graphs;physics;threshold behavior;coloring problem random graphs;coloring problem;informatics;probability space;computer simulation;probability measure;regular graphs;erd s renyi graphs;regular graph	In this invited talk we present a short survey of some recent graphs on coloring random graphs.	graph coloring;random graph	Lefteris M. Kirousis	2009	2009 11th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing	10.1109/SYNASC.2009.8	computer simulation;random graph;combinatorics;discrete mathematics;graph theory;theoretical computer science;edge coloring;mathematics;greedy coloring	Arch	23.519786147973516	32.62618985592796	73597
3932a8facc1ce636249ea3caaeb4b8e46ce88643	on path bipancyclicity of hypercubes	longueur chemin;prueba;hypercube;graphe biparti;interconnection;procesamiento informacion;camino grafo;algorithm analysis;grafo bipartido;graph path;camino hamiltoniano;hamiltonian;contre exemple;path bipancyclicity;hamiltonien;interconnection network;contraejemplo;interconexion;ciclo;preuve;informatique theorique;68r10;information processing;interconnexion;path;chemin graphe;chemin hamiltonien;interconnection networks;analyse algorithme;traitement information;cycle;proof;bipartite graph;hamiltoniano;calcul 2 dimensions;analisis algoritmo;red interconexion;counterexample;hamiltonian path;computer theory;reseau interconnexion;informatica teorica;two dimensional calculations;hipercubo	"""Assume that P is any path in a bipartite graph G of length k with 2==3 is (2n-4)-path bipancyclicity. In this paper, counterexamples to the lemma are given, therefore, their proof fails. And we show the following result: The n-cube Q""""n with n>=3 is (2n-4)-path bipancyclicity but is not (2n-2)-path bipancyclicity, moreover, and a path P of length k with 2="""		Xie-Bin Chen	2009	Inf. Process. Lett.	10.1016/j.ipl.2009.02.009	hamiltonian path;combinatorics;hamiltonian;bipartite graph;information processing;counterexample;interconnection;proof;mathematics;path;algorithm;hypercube	DB	24.084542130110606	33.888772098507566	73778
3e02d2a3bcf4f23fe608ac79541a1ee6bc32daba	reversible logic synthesis of networks of positive/negative control gates	computers;negative control;design automation;fredkin gates;complexity theory;network synthesis;positive control gates;reversible logic function;pnc gate;probability density function;data mining;reversible network;research and development;low power;reversible logic;state transformation digraph reversible logic function pnc gate reversible network;logic gates;quantum computer;fredkin gates reversible logic synthesis positive control gates negative control gates toffoli gates;negative control gates;network synthesis logic gates;reversible logic synthesis;state transformation digraph;network synthesis signal synthesis cmos logic circuits quantum computing logic functions signal processing algorithms logic programming computer networks electronic mail computer applications;toffoli gates	The research in reversible logic has been attracted in recent years. It has applications in quantum computing, low-power CMOS design and nanotechnology. Many methods on how to cascade a reversible network have been proposed. And all these networks are composed of Toffoli/Fredkin gates. In this paper, we present a method that synthesizes a network with family of Positive/Negative Control (PNC) gates. Through introducing some examples for reversible functions, and comparing with the network which is composed of Toffoli gates, the results show that introducing the PNC gates can produce an improvement for the number of gates in the reversible network, and have an advantage in reducing the cost of network.	cmos;cascade algorithm;logic gate;logic synthesis;low-power broadcasting;quantum computing;quantum gate;reversible computing;toffoli gate	Wenying Zhu;Zhijin Guan;Yueqing Hang	2009	2009 Fifth International Conference on Natural Computation	10.1109/ICNC.2009.186	and-or-invert;electronic engineering;nor logic;toffoli gate;three-input universal logic gate;computer science;theoretical computer science;algorithm;tc0	EDA	17.190200557761674	45.094192258461085	73980
57c44ae837d204879b29457d5899a2f361e48a7e	coverings that preserve sense of direction	graph theory;distributed system;sense of direction;teoria grafo;systeme reparti;butterfly network;cayley graph;fonction decodage;reseau papillon;theorie graphe;interconnection network;connected graph;decoding function;graphe recouvrement;sistema repartido;directed graph;cycle graphe;graphe oriente;covering graph;interconnection networks;grafo cayley;grafo orientado;cycle graph;graph coverings;distributed systems;graphe connexe;red interconexion;grafo conexo;reseau interconnexion;ciclo diagrama;graphe cayley	Sense of direction is a property of labelled networks (i.e., arc-coloured graphs) that allows one to assign coherently local identifiers to other processors on the basis of the route followed by incoming messages. We prove that (weak) sense of direction is preserved by the construction of regular coverings (i.e., coverings induced by voltage assignments in a group) whose voltage assignment depends only on colours. Moreover, this construction preserves minimality.	central processing unit;color;identifier	Paolo Boldi;Sebastiano Vigna	2000	Inf. Process. Lett.	10.1016/S0020-0190(00)00094-6	covering graph;combinatorics;discrete mathematics;directed graph;topology;connectivity;graph theory;cycle graph;cayley graph;mathematics;algorithm	DB	24.277884348477112	34.122273097034466	74076
40acad5d1a3d51a571186994fd8c96083b489e0e	a routing algorithm for row-based fpgas	field programmable gate array;time complexity;maximal independent set;routing algorithm	Abstract   A row-based Field Programmable Gate Array (FPGA) consists of rows of identical processing modules separated by segmented tracks that are used for routing nets between specified module pins. The problem of finding a feasible routing of a given set of nets onto a set of segmented tracks is NP-complete. We present an enumeration algorithm for the problem that uses matching arguments to improve the time complexity while requiring polynomial space. The algorithm serves as the basis of powerful practical heuristics and its additional advantage is that it is directly parallelizable.		Dimitrios Kagaris	1997	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/S0141-9331(96)01106-4	time complexity;embedded system;mathematical optimization;parallel computing;computer science;theoretical computer science;destination-sequenced distance vector routing;maximal independent set;field-programmable gate array	EDA	18.881603058821035	39.81664910650424	74137
951f5dc8061fb84a82a0b655a40c9d301fe8102b	video processing for single-chip dvb decoder	digital video broadcasting;digital signal processing chips digital video broadcasting decoding video signal processing;0 18 micron video processing single chip dvb decoder architectures dvb decoder ic dvb decoding system demux mpeg video decoder audio decoder atapi bridge ieee1394 port conventional analog video color decoder text slicer hybrid set top box applications integrated digital tv chassis edram technology idtv;decoding;video signal processing;mpeg video;digital tv;video processing;television receivers;chip;idtv chassis video processing single chip dvb decoder dvb decoder chips architectures system demux mpeg video decoder audio decoder atapi bridge ieee1394 interface analog video color decoder text slicer analog inputs analog outputs hybrid set top box applications integrated digital tv chassis;dram chips digital video broadcasting television receivers digital signal processing chips decoding video signal processing;digital signal processing chips;digital video broadcasting decoding digital tv satellite broadcasting hdtv integrated circuit modeling tv broadcasting europe firewire application software;set top box;dram chips;digital video broadcasting decoding digital tv video recording graphics consumer electronics firewire application specific integrated circuits control systems sdram	This paper deals with aspects of video processing for DVB decoder chips. The proposed architectures were implemented in a new DVB decoder IC. This IC comprises all units required for DVB decoding like system demux, MPEG video and audio decoder. To connect external devices it offers an ATAPI bridge, an IEEE1394 port, and other interfaces. Conventional analog video is supported by an integrated color decoder and text slicer. The analog inputs and outputs allow an easy integration in hybrid set-top box applications and integrated digital TV (IDTV) chassis. The first silicon was fabricated using 0.18-/spl mu/m eDRAM technology.	digital video broadcasting;video processing	Maik Brett;Bernhard Gerstenberg;Christian Herberg;Gadi Shavit;Herbert Liondas	2001	IEEE Trans. Consumer Electronics	10.1109/30.964125	chip;embedded system;electronic engineering;real-time computing;telecommunications;computer science;video processing;digital video broadcasting	Visualization	10.873546152820685	41.09379513405746	74313
4d701eece1a2d632bb110bf12799965ab5b42670	extracting a simplified view of design functionality via vector simulation	data mining automatic test pattern generation sequential analysis semiconductor device testing system testing conferences performance analysis algorithm design and analysis reduced instruction set computing test pattern generators;vector simulation;logic design;boolean functions;automatic test pattern generation;boolean learning;logic design automatic test pattern generation boolean functions learning artificial intelligence;input output behavior;word level function learning;learning artificial intelligence;automatic test pattern generator;automatic test pattern generator simplified design functionality view extraction vector simulation input output behavior word level function learning boolean learning;simplified design functionality view extraction	This paper presents a simulation-based methodology for extracting a simplified view of a design's input-output behavior. Such a simplified design view can be used to facilitate test pattern justification from the outputs of the module to the inputs of the module. In this paper, extraction of a design simplification view is formulated as a learning problem. With a learning scheme for learning word-level functions, the core of the problem becomes developing an efficient Boolean learner. We discuss the implementation of such a Boolean learner and compare its performance with the one of best-known Boolean learning algorithms, the Fourier analysis based method. Experimental results are presented to illustrate the implementation of the simulation-based methodology and its usage for extracting a simplified functionality view from Open RISC 1200 datapath	algorithm;boolean algebra;boolean satisfiability problem;datapath;fourier analysis;level of detail;machine learning;simulation;test card	Onur Guzey;Charles H.-P. Wen;Li-C. Wang;Tao Feng;Magdy S. Abadir	2006	2006 IEEE International High Level Design Validation and Test Workshop	10.1109/HLDVT.2006.319991	boolean circuit;logic synthesis;computer science;theoretical computer science;automatic test pattern generation;machine learning;boolean function;algorithm	EDA	20.63874825521926	46.09635827236019	74792
5b0031f2c88f52ed78182dd5fa0fa0007bb5a853	energy-efficient air-indices for shortest path and distance queries on road networks		Although various air-indices, e.g., Next Region (NR), have been proposed to support shortest path and distance queries at mobile clients on a road network , how to minimize the energy cost, which is dominated by the cost for obtaining traffic data from data broadcast, is still one of the most important performance objectives. Inspired by various preprocessing schemes in traditional computation models for shortest path queries that make use of special properties of road networks, we design in this paper a new air-index called the CH Index with the purpose of minimizing the energy cost for obtaining broadcasting data to perform shortest path and distance queries at mobile clients. Experimental evaluation shows that our CH Index is more energy efficient than the state-of-the-art index NR by an order of magnitude. We also extend the CH Index to the CHBN Index which provides a tradeoff between energy efficiency and response time for obtaining traffic data via a user-tunable parameter. We show that CHBN gives shorter response time than CH while still being energy efficient. © 2017 Elsevier Ltd. All rights reserved. p t w p T m o r W r i	broadcasting (networking);computation;noise reduction;preprocessor;response time (technology);shortest path problem	Chung Keung Poon;Chun Jiang Zhu;Kam-yiu Lam	2017	Inf. Syst.	10.1016/j.is.2017.08.009	computer science;shortest job next;order of magnitude;k shortest path routing;yen's algorithm;efficient energy use;response time;shortest path problem;constrained shortest path first;distributed computing	DB	19.583357130800657	37.665255847041095	74859
8b191e7c2e5adda92f7187ebb5580d887432b9cc	spatio-temporal prediction based algorithm for parallel improvement of hevc			algorithm;high efficiency video coding	Xiantao Jiang;Tian Song;Takashi Shimamoto;Wen Shi;Lisheng Wang	2015	IEICE Transactions		parallel processing;complexity;computer science;algorithm	HPC	12.312660108991283	39.46935587284692	75092
3bafd7ed67acdd8db987ec601e09e6001b8131c8	the power of quasi-shortest paths: $\rho$ -geodesic betweenness centrality		"""Betweenness centrality metrics usually underestimate the importance of nodes that are close to shortest paths but do not exactly fall on them. In this paper, we reevaluate the importance of such nodes and propose the <italic> <inline-formula><tex-math notation=""""LaTeX"""">$\rho$</tex-math><alternatives> <inline-graphic xlink:href=""""varelademedeiros-ieq2-2708705.gif""""/></alternatives></inline-formula>-geodesic betweenness centrality</italic>, a novel metric that assigns weights to paths (and, consequently, to nodes on these paths) according to how close they are to shortest paths. The paths slightly longer than the shortest one are defined as <italic>quasi</italic>-shortest paths, and they are able to increase or to decrease the importance of a node according to how often the node falls on them. We compare the proposed metric with the traditional, distance-scaled, and random walk betweenness centralities using four network datasets with distinct characteristics. The results show that the proposed metric, besides better assessing the topological role of a node, is also able to maintain the rank position of nodes overtime compared to the other metrics; this means that network dynamics affect less our metric than others. Such property could help to avoid, for instance, the waste of resources caused when data follows only the shortest paths in dynamic networks, also reducing associated costs."""	algorithm;betweenness centrality;computer;degree distribution;experiment;graph (discrete mathematics);relevance;shortest path problem;social network;virtual machine;weighted network;xlink	Dianne S. V. Medeiros;Miguel Elias M. Campista;Nathalie Mitton;Marcelo Dias de Amorim;Guy Pujolle	2017	IEEE Transactions on Network Science and Engineering	10.1109/TNSE.2017.2708705	random walk closeness centrality;katz centrality;alpha centrality;centrality;betweenness centrality	Web+IR	23.725173028304727	38.83483583859419	75917
c2c777cdaf48d501415a6fda06d844558b93ef53	parallel approach to tracking edge segments in dynamic scenes	tratamiento paralelo;analisis imagen;modelizacion;vision ordenador;filtro kalman;traitement parallele;multiprocessor;edge detection;filtre kalman;kalman filter;intelligence artificielle;transputer;tracking movable target;computer vision;deteccion contorno;algorithme;modelisation;parallel tracking;algorithm;detection contour;computer architecture;architecture ordinateur;artificial intelligence;image analysis;vision ordinateur;arquitectura ordenador;poursuite;computer science and informatics;inteligencia artificial;computer analysis;motion flow model;multiprocesador;edge tracking;transputers;modeling;analyse image;parallel processing;persecucion y continuacion;dynamic scenes;algoritmo;multiprocesseur	Computer analysis of images imposes a significant computational burden on the processing hardware. In dynamic vision, the requirement is also to reduce the latency of the processing to allow realistic reaction times to events in the image. Flexible, massively parallel arc~~itectures hold the promise of f~~lfilling these requirements for low, medium and high level vision tasks, provided that robust algorithms can be implemented in an efficient manner. This paper describes a parallel model which is designed for use as a basis for implementation of edge tracking algorithms on parallel architectures. The model is independent of edge tracking algorithms. An implementation of the model is outlined using a tracking algorithm founded on features such as the mid-point, orientation and the length of edge segments, and using a modified form of the Kalman filter. The implementatioF~ is based on transputer.~~ and consists of three independent units, each of wretch has been staged in a studied configuration. The tracking unit is based on a tree configuration and displays MIMI) characteristics. The edge extraction unit performs the Canny operator, and two parallel programming models of data parallelism are examined to determine the most suitable and efficient topology and data routing method of this unit. The final unit is the host interface. Performance results for the implementation are ~~resented.	algorithm;canny edge detector;data parallelism;high-level programming language;kalman filter;parallel computing;requirement;routing;transputer	Majid Mirmehdi;T. J. Ellis	1993	Image Vision Comput.	10.1016/0262-8856(93)90030-K	kalman filter;parallel processing;computer vision;multiprocessing;image analysis;simulation;systems modeling;edge detection;computer science;computer graphics (images)	Robotics	11.019198693362933	35.50129588767264	76077
163377d70229ae4f70befa76f9f8c6e6f0709bbe	p systems generating hexagonal picture languages	p system;membrane computing	P systems generating rectangular arrays have been recently introduced in [1,2,3], thus bringing together the two areas of theoretical computer science, namely membrane computing and picture grammars. In this paper, hexagonal arrays on triangular grids are considered and the capabilities of the three approaches [1,2,3] that construct P systems for generation of arrays on rectangular grids, to handle arrays on triangular grids, are demonstrated.	p system	K. S. Dersanambika;Kamala Krithivasan;K. G. Subramanian	2003		10.1007/978-3-540-24619-0_12	combinatorics;discrete mathematics;theoretical computer science;mathematics	Logic	20.134549655910895	38.52230043075667	76272
29ab0f27295ec36de527a2f07220cad8e1c34797	implementation of a 2-d 8×8 idct on the reconfigurable montium core	inverse discrete cosine transform;multiprocessor systems;energy efficient;reconfigurable architectures;arm reconfigurable montium core two dimensional inverse discrete cosine transform word level reconfigurable montium processor montium tile processor asic;discrete cosine transform;chip;digital signal processing signal processing algorithms costs energy consumption silicon computer architecture decoding cmos technology discrete cosine transforms tiles;energy consumption;discrete cosine transforms;reconfigurable architectures digital signal processing chips discrete cosine transforms;digital signal processing chips;time to market;functional requirement;off the shelf	This paper describes the mapping of a two-dimensional inverse discrete cosine transform (2-D IDCT) onto a word-level reconfigurable Montiumreg processor. This shows that the IDCT is mapped onto the Montium tile processor (TP) with reasonable effort and presents performance numbers in terms of energy consumption, speed and silicon costs. The Montium results are compared with the IDCT implementation on three other architectures: TI DSP, ASIC and ARM.	arm architecture;application-specific integrated circuit;discrete cosine transform;ti-nspire series;tile processor	Lodewijk T. Smit;Gerard K. Rauwerda;Albert Molderink;Pascal T. Wolkotte;Gerard J. M. Smit	2007	2007 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2007.4380717	chip;embedded system;parallel computing;computer science;discrete cosine transform;efficient energy use;functional requirement	EDA	10.978369499993622	45.06133976537017	76755
16625ae496b86700eecbf25141f76d9f615517f2	declarative combinatorics: boolean functions, circuit synthesis and bdds in haskell	generic algorithm;decision diagram;boolean function;logic synthesis;unt;circuit synthesis	We describe Haskell implementations of interesting combinatorial generation algorithms with focus on boolean functions and logic circuit representations. First, a complete exact combinational logic circuit synthesizer is described as a combination of catamorphisms and anamorphisms. Using pairing and unpairing functions on natural number representations of truth tables, we derive an encoding for Binary Decision Diagrams (BDDs) with the unique property that its boolean evaluation faithfully mimics its structural conversion to a a natural number through recursive application of a matching pairing function. We then use this result to derive ranking and unranking functions for BDDs and reduced BDDs. Finally, a generalization of the encoding techniques to Multi-Terminal BDDs is provided. The paper is organized as a self-contained literate Haskell program, available at http://logic.csci.unt.edu/tarau/ research/2008/fBDD.zip.	algorithm;binary decision diagram;catamorphism;combinational logic;constraint logic programming;declarative programming;genetic programming;haskell;logic gate;recursion	Paul Tarau	2008	CoRR		boolean circuit;circuit minimization for boolean functions;discrete mathematics;logic synthesis;genetic algorithm;influence diagram;computer science;theoretical computer science;mathematics;boolean function;programming language;algorithm	EDA	18.78845945610447	45.613809775326295	76785
67ca396139481b58ca6f7d36d7f462ee62142a82	on the self-stabilization of mobile oblivious robots in uniform rings	local-strong multiplicity detection;previous algorithm;self-stabilizing manner;probabilistic self-stabilizing algorithm;self-stabilizing algorithm;probabilistic self-stabilizing gathering algorithm;initial configuration;atomic atom model;uniform ring;global-strong multiplicity detection;mobile oblivious robot;multiplicity point	local-strong multiplicity detection;previous algorithm;self-stabilizing manner;probabilistic self-stabilizing algorithm;self-stabilizing algorithm;probabilistic self-stabilizing gathering algorithm;initial configuration;atomic atom model;uniform ring;global-strong multiplicity detection;mobile oblivious robot;multiplicity point	robot;self-stabilization	Fukuhito Ooshita;Sébastien Tixeuil	2012		10.1007/978-3-642-33536-5_6	combinatorics;discrete mathematics;mathematics;distributed computing	Robotics	17.74102483085002	34.32748711261808	76920
00e4f4188d99ccc5c65418ad2df46fa5f6e0e960	state look ahead technique for cycle optimization of interacting finite state moore machines	finite state machines;clock cycle;combinatorial blocks;cycle optimization;hardware design;interacting finite state moore machines;logic block;state look ahead technique;timing optimization	In the area of hardware design, automata are often realcred synchronously by a clocked state register, a next state logtc block representing the state transition function, and an output logic block representing the output functzon, Assurmng that combinatorial blocks of automata are already optzmized, a further potential for timing optimization occurs, if two A400re automata interact sequentially, where the output of the first automaton is the input of the second one. Processing of an event that occurs as input of the first automaton by both atdomata needs up to two clock cycles: at most one clock cycle for producing an output of the first one, and one clock cycle to compute the jinal output by the second one. In this paper an algorithm is presented that allows to avoid one clock cycle in certain well defined situations.	algorithm;automaton;clock rate;clock signal;finite-state machine;hardware description language;interaction;logic block;logic synthesis;mathematical optimization;moore machine;newton;overhead (computing);state transition table;synchronous circuit;vhdl	Wolfgang Ecker;M. Hofmeister	1993		10.1145/259794.259858	electronic engineering;real-time computing;computer science;theoretical computer science;look-ahead;finite-state machine;algorithm	EDA	18.121230151442166	42.7312060070459	76937
6f3349e33e1b9ef71080572629ffe15e3dda9c7f	an on-line error-detectable high-speed array divider	divider;detection erreur;circuito aritmetico;circuito lsi;deteccion error;complexite calcul;complejidad calculo;implementation;diviseur;computing complexity;algorithme;algorithm;ejecucion;lsi circuit;divisor;error detection;circuit lsi;high speed;circuit arithmetique;arithmetic circuit;algoritmo	Abstract#R##N##R##N#This paper proposes an on-line error-detectable high-speed array divider. The divider is based on the high-speed division algorithm using the redundant binary representation. It can execute an n-bit division in a computation time proportional to n, has a regular cellular array structure suited to LSI implementation, with the hardware complexity proportional to n2, and a feature that any error caused by a single cell fault can be detected during the normal operation. By utilizing the locality of the computation in the divider, the on-line error detection is realized based on the check by the residue code for the dividend, divisor, quotient and the remainder, as well as some other additional checks.	online and offline	Naofumi Takagi;Shuzo Yajima	1991	Systems and Computers in Japan	10.1002/scj.4690220103	parallel computing;error detection and correction;computer science;electrical engineering;operating system;mathematics;implementation;divisor;algorithm	HPC	15.958585760960085	41.65392079674801	76991
3c598e9aa9c1f491f1496fa23fc3b382630ca3ca	embedding multi-dimensional meshes into twisted cubes	interconnection network;multi dimensional;mesh;interconnection networks;twisted cube;graph embedding;parallel processing	The twisted cube is an important variant of the most popular hypercube network for parallel processing. In this paper, we consider the problem of embedding multi-dimensional meshes into twisted cubes in a systematic way. We present a recursive method for embedding a family of disjoint multi-dimensional meshes into a twisted cube with dilation 1 and expansion 1. We also prove that a single multi-dimensional mesh can be embedded into a twisted cube with dilation 2 and expansion 1. Our work extends some previously known results.	olap cube;twisted	Qiang Dong;Xiaofan Yang;Dajin Wang	2010	Computers & Electrical Engineering	10.1016/j.compeleceng.2010.03.003	parallel processing;combinatorics;discrete mathematics;graph embedding;topology;computer science;mathematics	AI	23.50137545543787	35.88090021386418	77557
6d1bf8f270ef72c43b5dfb5bdb18486ed57822f9	hardware architecture for projective model calculation and false match refining using random sample consensus algorithm	software;computer hardware;field programmable gate arrays	The projective model is an important mapping function for the calculation of global transformation between two images. However, its hardware implementation is challenging because of a large number of coefficients with different required precisions for fixed point representation. A VLSI hardware architecture is proposed for the calculation of a global projective model between input and reference images and refining false matches using random sample consensus (RANSAC) algorithm. To make the hardware implementation feasible, it is proved that the calculation of the projective model can be divided into four submodels comprising two translations, an affine model and a simpler projective mapping. This approach makes the hardware implementation feasible and considerably reduces the required number of bits for fixed point representation of model coefficients and intermediate variables. The proposed hardware architecture for the calculation of a global projective model using the RANSAC algorithm was implemented using Verilog hardware description language and the functionality of the design was validated through several experiments. The proposed architecture was synthesized by using an application-specific integrated circuit digital design flow utilizing 180-nm CMOS technology as well as a Virtex-6 field programmable gate array. Experimental results confirm the efficiency of the proposed hardware architecture in comparison with software implementation. © 2016 SPIE and IS&T [DOI: 10.1117/1.JEI.25.6.063014]	application-specific integrated circuit;cmos;chandra–toueg consensus algorithm;coefficient;experiment;field-programmable gate array;fixed point (mathematics);hardware description language;image registration;logic synthesis;machine vision;pixel;random sample consensus;real-time clock;simulation;verilog;video processing	Ehsan Azimi;Alireza Behrad;M. B. Ghaznavi-Ghoushchi;Jamshid Shanbehzadeh	2016	J. Electronic Imaging	10.1117/1.JEI.25.6.063014	real-time computing;computer science;theoretical computer science;distributed computing;field-programmable gate array	EDA	13.542926702332718	41.87926702333219	77634
87ee36a4f0a3913cae1c54457cdb4d714676ec12	a flexible hardware jpeg 2000 decoder for digital cinema	transformation ondelette;estensibilidad;digital circuit;discrete wavelet transforms;institutional repositories;desciframiento;circuit decodeur;traitement signal;field programmable gate array;signal resolution flexible hardware jpeg 2000 decoder digital cinema image compression multimedia applications field programmable gate array;image coding;fedora;image processing;image coding cinematography data compression decoding field programmable gate arrays;motion pictures;data compression;decodage;decoding;multimedia applications;cinematography;implementation;real time;specification;procesamiento imagen;entropy coding;paralelisacion;convertisseur numerique;multimedia application;transform coding;indexing terms;red puerta programable;satisfiability;circuito desciframiento;reseau porte programmable;traitement image;vital;arithmetic code;circuit numerique;codigo aritmetico;wavelet transforms;codage image;convertidor numerico;decoding circuit;compression image;arithmetic codes;senal video;signal video;image compression;especificacion;senal numerica;signal processing;parallelisation;flexible hardware jpeg 2000 decoder;multimedia communication;digital cinema;secure system;circuito numerico;parallelization;signal numerique;signal resolution;video signal;extensibilite;scalability;code arithmetique;compresion dato;transformacion ondita;digital signal;external memory;field programmable gate arrays;hardware decoding motion pictures transform coding image coding field programmable gate arrays discrete wavelet transforms entropy coding laboratories signal resolution;digital circuits;vtls;implementacion;communication multimedia;digital converter;procesamiento senal;hardware implementation;wavelet transformation;ils;compression donnee;field programmable gate arrays fpgas;wavelet transforms arithmetic codes digital circuits field programmable gate arrays fpgas image coding;hardware;compresion imagen;resolution signal	The image compression standard JPEG 2000 proposes a large set of features that is useful for today's multimedia applications. Unfortunately, it is much more complex than older standards. Real-time applications, such as digital cinema, require a specific, secure, and scalable hardware implementation. In this paper, a decoding scheme is proposed with two main characteristics. First, the complete scheme takes place in a field-programmable gate array without accessing any external memory, allowing integration in a secured system. Second, a customizable level of parallelization allows to satisfy a broad range of constraints, depending on the signal resolution. The resulting architecture is therefore ready to meet upcoming digital cinema specifications	cinema 4d;field-programmability;field-programmable gate array;image compression;jpeg 2000;parallel computing;real-time transcription;scalability	Antonin Descampe;François-Olivier Devaux;Gaël Rouvroy;Jean-Didier Legat;Jean-Jacques Quisquater;Benoit M. Macq	2006	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2006.884573	telecommunications;image processing;computer science;electrical engineering;theoretical computer science;signal processing;digital electronics;field-programmable gate array;statistics;computer graphics (images)	EDA	11.092939332108605	41.72982865421378	78030
9ce9dc3a20d4248b6921bc096fd095a716e179f0	efficiently inverting bijections given by straight line programs	boolean field;inverting bijections;arithmetic operations;efficient algorithm;straight line programs;polynomial size arithmetic circuit;finite field;automorphism;algebraic geometry;cryptography;cryptographic considerations;circuits polynomials computer science digital arithmetic galois fields cryptography geometry size measurement;boolean field inverting bijections straight line programs arithmetic operations cryptographic considerations algebraic geometry automorphism polynomial size arithmetic circuit automorphism;arithmetic circuit	General: Computer and telecommunication networks, Internet quality of service and novel architectures, wireless networks, sensor networks, multimedia networking, peer-to-peer and distributed systems, network security. Current Focus: Internet interand intra-domain routing, wireless ad hoc routing, routing algorithms and architectures for heterogeneous networks, distributed sensor networks, resilient large-scale distributed system design, self-organizing and peer-to-peer systems, security attack detection and prevention, secure networked system design.	algorithm;distributed computing;hoc (programming language);network security;organizing (structure);peer-to-peer;quality of service;routing;self-organization;sensor;systems design	Carl Sturtivant;Zhi-Li Zhang	1990		10.1109/FSCS.1990.89551	combinatorics;discrete mathematics;automorphism;algebraic geometry;cryptography;saturation arithmetic;mathematics;arithmetic circuit complexity;finite field;algorithm;algebra	Theory	21.536095073307358	35.49472849233769	78121
84d27d2f051bca44d4f6e6db327b138f4d660456	distance monotonicity and a new characterization of hamming graphs	interval notion;hypercube;graphe intervalle;procesamiento informacion;algorithm analysis;interval graph;grafo intervalo;metric properties;propriete metrique;distance monotonicity;interconnection network;hamming graph;notion intervalle;producto grafo;metric property;informatique theorique;information processing;graphe hamming;produit cartesien;interconnection networks;hamming graphs;monotonie distance;analyse algorithme;traitement information;grafo completo;complete graph;graphe complet;graph product;analisis algoritmo;red interconexion;produit graphe;computer theory;reseau interconnexion;informatica teorica;hipercubo	Hamming graphs are simply Cartesian products of complete graphs. Several characterizations of these graphs involve of intervals in a graph and related topics. The notion of interval distance monotonicity has already been used in order a characterization of hypercubes. Here, we revisit this notion and use it to obtain a new characterization of Hamming g performing a combination of the hypercube characterization and the interval hypercube characterization.  2005 Elsevier B.V. All rights reserved.	cartesian closed category;graph (discrete mathematics);hamming distance;window function	Méziane Aïder;Mustapha Aouchiche	2005	Inf. Process. Lett.	10.1016/j.ipl.2005.08.009	1-planar graph;pathwidth;combinatorics;discrete mathematics;hamming distance;topology;information processing;hamming graph;mathematics;chordal graph;indifference graph;algorithm	Theory	24.46154322805921	33.593897081963824	78521
1393c6d1e4c987943dda634e36ba77bc55d1082e	reduction operations on a distributed memory machine with a reconfigurable interconnection network	multiprocessor interconnection networks;graph theory;maximum degree;distributed memory machine;reductionoperations;index termsdistributed memory machine;reconfigurable interconnection network;interconnection network;transputer based networks distributed memory machine reconfigurable interconnection network reduction operations interconnection graph;multiprocessor interconnection networks scattering broadcasting arithmetic concurrent computing pipelines kernel parallel architectures parallel algorithms binary trees;multiprocessor interconnection networks graph theory;transputer based networks;interconnection graph	We are interested in performing reduction operations with distributed memory machines whose interconnection networks are reconfigurable. More precisely, we focus on machines whose interconnection graph can be configured as any graph of maximum degree d. We discuss the best way of interconnecting the p processors as a function of p, d, and some problemand machine dependent-parameters that characterize the ratio communication/arithmetic for the reduction operation. Experiments on Transputer-based networks are in good accordance with the theoretical results.	central processing unit;distributed memory;graph (discrete mathematics);interconnection;machine-dependent software;transputer	Serge Miguet;Yves Robert	1992	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.149967	parallel computing;degree;computer science;graph theory;theoretical computer science;distributed computing	Arch	13.818694475315562	35.339774867853976	78850
aad84e43583fee083e71c1d5669897f828491ee1	out-of-loop rate control for video codec hardware/software co-design	rate distortion model analysis;microcontrollers;quantisation coding;rate control algorithm;microcontroller unit;soc platform;macroblock encoding loop;encoding;vlsi accelerator core;scene change detection;transform coding;system-on-chip;motion estimation;out-of-loop rate control;communication overhead reduction;software based video encoders;vlsi;video coding;hardware-software codesign;sum of absolute difference;entropy coding;rate distortion theory;asic accelerator;video codecs;system on chip;hardware;quantization	Most software-based video encoders perform rate-distortion model analysis to determine the quantizer step size after the motion estimation (ME) step. The Sum of Absolute Differences (SAD) values from the motion estimator are used as the complexity estimates for rate-control model. However, for video encoders on SoC platforms, the calculation of rate-control model is typically done on the Microcontroller Unit (MCU) core while the macroblock encoding loop (ME, transform, quantization, and entropy coding) is done by a VLSI accelerator core. In order to reduce the communication overhead between the MCU and the ASIC accelerator, a rate control algorithm that is executed outside the encoding loop solely by the MCU is very useful. In this paper, an out-of-loop rate control with scene change detection is proposed for video codec for SoC platforms. The experimental results show that this algorithm is very promising for video codec hardware/software co-design.	algorithm;application-specific integrated circuit;codec;distortion;encoder;entropy encoding;macroblock;microcontroller;motion estimation;overhead (computing);quantization (signal processing);sum of absolute differences;very-large-scale integration	Ching-Ho Chen;Chun-Jen Tsai	2004	2004 IEEE International Symposium on Circuits and Systems (IEEE Cat. No.04CH37512)		system on a chip;microcontroller;embedded system;computer vision;electronic engineering;real-time computing;rate–distortion theory;computer science;entropy encoding;motion estimation;very-large-scale integration;rate–distortion optimization;encoding;statistics	EDA	12.766498349249034	41.42851949721643	79098
e5bbe52682d9cb27b72a67c0a3d0580886300789	design of an image interpolator for low computation complexity	linear function;interpolation;cubic convolution;interpolator;image interpolation;hardware description language;image quality;computational complexity	In this paper, we propose an image interpolator for low computational complexity. The proposed image interpolator supports the image scaling using a modified cubic convolution interpolation between the input and output resolutions for a full screen display. In order to reduce the computational complexity, we use the difference in value of the adjacent pixels for selecting interpolation methods and linear function of the cubic convolution. The proposed image interpolator is compared with the conventional one for the computational complexity and image quality. The proposed image interpolator has been designed and verified by Verilog HDL(Hardware Description Language). It has been synthesized using the Xilinx VirtexE FPGA, and implemented using an FPGA-based prototype board.	computation;computational complexity theory;convolution;cubic function;field-programmable gate array;hardware description language;image quality;image scaling;input/output;interpolation;linear function;pixel;prototype;verilog	Young-Hyun Jun;Jong-Ho Yun;Jin-Sung Park;Myung-Ryul Choi	2006	JIPS		image quality;linear function;mathematical optimization;interpolation;computer science;theoretical computer science;stairstep interpolation;bicubic interpolation;hardware description language;computational complexity theory;image scaling	Vision	12.734415856395016	42.026490585076125	79850
3d2a91903d832a998f26a09e3510e196130f2a77	design of reversible sequential elements with feasibility of transistor implementation	latches cmos logic circuits sequential circuits logic devices logic design optical computing quantum computing design engineering design optimization minimization;logic design;sequential circuits;sequential circuits flip flops logic design logic gates;flip flops;flip flops reversible sequential elements modified fredkin gate modified toffoli gate reversible feynman gate;logic gates;flip flop	This paper presents the novel designs of reversible sequential circuits (latches and flip flops). The proposed reversible latches and flip flops are designed from reversible Fredkin, Feynman and Toffoli gates. Two new reversible gates called modified Fredkin gate (MFG) and modified Toffoli gate (MTG) are also proposed to design the optimized implementations. The proposed designs are better than the recently proposed ones in terms of number of reversible gates and garbage outputs. In order to reach towards the goal of transistor implementations of proposed reversible sequential circuits, transistor implementation of the existing Feynman gate, Fredkin gate, Toffoli gates as well as the proposed MTG and MFG are also proposed. The proposed transistor implementations are completely reversible in nature, i.e., suitable for both the forward and backward computation.	computation;flops;flip-flop (electronics);fredkin gate;reversible computing;sequential logic;toffoli gate;transistor	Himanshu Thapliyal;A. Prasad Vinod	2007	2007 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2007.378815	reversible computing;electronic engineering;parallel computing;logic synthesis;toffoli gate;logic gate;three-input universal logic gate;engineering;fredkin gate;pass transistor logic;quantum circuit;mathematics;sequential logic;algorithm;quantum gate	EDA	17.460327670365714	46.00438742174611	80053
0d35a44f46a282935ba07e0488b7d8102ee9fb84	optimal slack-driven block shaping algorithm in fixed-outline floorplanning	optimal solution;physical design;upper bound;block shaping;fixed outline floorplan;optimality condition	This paper presents an efficient, scalable and optimal slack-driven shaping algorithm for soft blocks in non-slicing floorplan. The proposed algorithm is called SDS. Different from all previous approaches, SDS is specifically formulated for fixed-outline floorplanning. Given a fixed upper bound on the layout width, SDS minimizes the layout height by only shaping the soft blocks in the design. Iteratively, SDS shapes some soft blocks to minimize the layout height, with the guarantee that the layout width would not exceed the given upper bound. Rather than using some simple heuristic as in previous work, the amount of change on each block is determined by systematically distributing the global total amount of available slack to individual block. During the whole shaping process, the layout height is monotonically reducing, and eventually converges to an optimal solution. We also propose two optimality conditions to check the optimality of a shaping solution. To validate the efficiency and effectiveness of SDS, comprehensive experiments are conducted on MCNC and HB benchmarks. Compared with previous work, SDS is able to achieve the best experimental result with significantly faster runtime.	algorithm;experiment;floorplan (microelectronics);heuristic;noise shaping;plover;scalability;slack variable	Jackey Z. Yan;Chris C. N. Chu	2012		10.1145/2160916.2160956	physical design;mathematical optimization;combinatorics;mathematics;upper and lower bounds	EDA	19.832360079631336	39.54714710731414	80601
e8a331ba411738be958e9a767b541aede1132b2c	efficient algorithms for constructing (1+∊,β)-spanners in the distributed and streaming models	shortest path;distributed protocol;streaming algorithm;efficient algorithm;communication complexity;time use;parallel implementation	For an unweighted undirected graph G = (V,E), and a pair of positive integers α ≥ 1, β ≥ 0, a subgraph G′ = (V,H), H ⊂eqE, is called an (α,β)-spanner of G if for every pair of vertices u,v ∊ V, distG′(u,v) ≤ α ⋅ dist G (u,v) + β. It was shown in [21] that for any ∊ > 0, κ = 1,2,…, there exists an integer β = β(∊,κ) such that for every n-vertex graph G there exists a (1+∊,β)-spanner G′ with O(n1+1/κ) edges. An efficient distributed protocol for constructing (1+∊,β)-spanners was devised in [19]. The running time and the communication complexity of that protocol are O(n1+ρ) and O(|E|n^ρ), respectively, where ρ is an additional control parameter of the protocol that affects only the additive term β. In this paper we devise a protocol with a drastically improved running time (O(n^ρ) as opposed to O(n1+ρ)) for constructing (1+∊,β)-spanners. Our protocol has the same communication complexity as the protocol of [19], and it constructs spanners with essentially the same properties as the spanners that are constructed by the protocol of [19]. The protocol can be easily extended to a parallel implementation which runs in O(log n + (|E|⋅ nρlog n)/p) time using p processors in the EREW PRAM model. In particular, when the number of processors, p, is at least |E|⋅ nρ, the running time of the algorithm is O(log n). We also show that our protocol for constructing (1+∊,β)-spanners can be adapted to the streaming model, and devise a streaming algorithm that uses a constant number of passes and O(n1+1/κ⋅ {log} n) bits of space for computing all-pairs-almost-shortest-paths of length at most by a multiplicative factor (1+∊) and an additive term of β greater than the shortest paths. Our algorithm processes each edge in time O(n^ρ), for an arbitrarily small ρ > 0. The only previously known algorithm for the problem [23] constructs paths of length κ times greater than the shortest paths, has the same space requirements as our algorithm, but requires O(n1+1/κ) time for processing each edge of the input graph. However, the algorithm of [23] uses just one pass over the input, as opposed to the constant number of passes in our algorithm. We also show that any streaming algorithm for o(n)-approximate distance computation requires Ω(n) bits of space.	approximation algorithm;central processing unit;coefficient;communication complexity;computation;distributed algorithm;edge detection;emoticon;graph (discrete mathematics);joan feigenbaum;parallel algorithm;parallel computing;parallel random-access machine;randomized algorithm;requirement;shortest path problem;sparse matrix;streaming algorithm;time complexity;utility functions on indivisible goods	Michael Elkin;Jian Zhang	2005	Distributed Computing	10.1007/s00446-005-0147-2	computer science;theoretical computer science;communication complexity;distributed computing;streaming algorithm;shortest path problem	Theory	18.23983682052718	33.0721786249646	80973
5b2b3812a6d86c7c3c33dcc6de4207802eaa9b26	parallel branch-and-bound algorithms	algoritmo paralelo;methode branch and bound;parallel algorithm;algorithmique;multiprocessor;branch and bound algorithm;algorithme parallele;probleme combinatoire;branch and bound method;problema combinatorio;calculateur mimd;metodo branch and bound;algorithmics;algoritmica;combinatory problem;multiprocesador;mimd computer;multiprocesseur	Abstract   In this paper a parallel algorithm for branch-and-bound problems is sketched. It is designed to run on MIMD machines and exploits coarse grain parallelism. Due to the irregular and unpredictable behavior of branch-and-bound algorithms, it is hard to obtain a good load-balance. Another design issue is the minimization of the communication overhead. Our algorithm overcomes these problems by a dynamic load-balancing strategy and a dynamic way to decide when communication is really useful. After first sketching a general parallel algorithm for branch-and-bound problems, we concentrate on a particular instance of a branch-and-bound problem: the so-called knapsack problem. The performance of the algorithm for this problem is measured with a simulator for a multiprocessor system.	algorithm;branch and bound	J. M. Jansen;F. W. Sijstermans	1989	Future Generation Comp. Syst.	10.1016/0167-739X(89)90003-4	parallel computing;multiprocessing;computer science;distributed computing;parallel algorithm;algorithmics;branch and bound;algorithm	Arch	10.28681129386027	32.83204958798252	81288
802f66dbc1affe62b1241e4f6e67c4ebd7377026	design of multiple-valued linear digital circuits for highly parallel k-ary operations	linear systems;multiplier;adder;many valued logics;concurrent computing;unary operations;multiplying circuits;very large scale integration;circuit design;logic circuits;input output graphs;highly parallel k ary operations;input output;parallel architectures logic circuits many valued logics;optimal code assignment;multiplicated redundant symbols multiple valued linear digital circuits highly parallel k ary operations adder multiplier optimal code assignment unary operations locally computable circuits input output graphs;design method;parallel architectures;vectors;matrix decomposition;adders;analytical method;digital systems;locally computable circuits;digital circuits combinational circuits adders very large scale integration circuit synthesis concurrent computing design methodology integrated circuit interconnections input variables digital systems;local computation;digital arithmetic;digital circuits digital systems design methodology vectors adders very large scale integration linear systems combinational circuits matrix decomposition concurrent computing;digital circuits;digital arithmetic adders multiplying circuits many valued logics;multiple valued;multiplicated redundant symbols;search procedure highly parallel multiple valued linear digital circuits unary operations representation matrix;design methodology;combinational circuits;multiple valued linear digital circuits	To design highly parallel digital circuits such as an adder and a multiplier, it is difficult to find the optimal code assignment in the nonlinear digital system. On the other hand, the use of the linear concept in digital systems seems to be very attractive because analytical methods can be utilized. For unary operations, the design method of locally computable circuits have been discussed. In this paper, we propose a new design method of highly parallel multiple-valued linear digital circuits for k-ary operations using the concept of identification of input-output graphs by the introduction of multiplicated redundant symbols. >		Masami Nakajima;Michitaka Kameyama	1994		10.1109/ISMVL.1994.302197	electronic engineering;discrete mathematics;concurrent computing;design methods;computer science;electrical engineering;theoretical computer science;mathematics;algorithm;adder;algebra	EDA	18.69128780020946	45.22030020057234	81480
14e5699840eccea7fdda22f4824ccce311d828fd	graph isomorphism in quasipolynomial time		We show that the Graph Isomorphism (GI) problem and its generalizations, the String Isomorphism (SI) and Coset Intersection (CI) problems, can be solved in quasipolynomial (exp ( (log n) ) ) time. The best previous bound for GI was exp(O( √ n log n)), where n is the number of vertices (Luks, 1983); for SI and CI, the bound was similar, exp(Õ( √ n)), where n is the size of the permutation domain (Babai, 1983). The SI problem takes as input two strings, x and y, of length n, and a permutation group G of degree n and asks if some element of G transforms x into y. Our algorithm builds on Luks’s SI framework and attacks its bottleneck, characterized by an epimorphism φ of G onto the alternating group acting on a set Γ of size k > c log n. Our goal is to break this symmetry. The crucial first step is to find a canonical t-ary relational structure on Γ, with not too much symmetry, for some t = O(log n). We say that an element x in the domain of G is affected by φ if φ maps the stabilizer of x to a proper subgroup of Ak. The affected/unaffected dichotomy provides a device to construct global symmetry from local information through the core group-theoretic “local certificates” routine. This algorithm in turn produces the required t-ary structure and thereby sets the stage for symmetry breaking via combinatorial methods of canonical partitioning. The latter lead to the emergence of the Johnson graphs as the sole obstructions to effective canonical partitioning. For a list of updates compared to the first two arXiv versions, see the Acknowledgments (Sec. 18.1). WARNING. While the present version fills significant gaps of the previous versions and improves the presentation of some components of the paper, the revision is incomplete; at the current stage, it includes notational, conceptual, and organizational inconsistencies. A fuller explanation of this disclaimer appears in the Acknowledgments (Sec. 18.1) at the end of the paper. ∗ Research supported in part by NSF Grants CCF-7443327 (2014-current), CCF-1017781 (2010-2014), and CCF-0830370 (2008–2010). Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the author and do not necessarily reflect the views of the National Science Foundation (NSF).	algorithm;bottleneck (engineering);consistency (database systems);degree (graph theory);exptime;emergence;graph isomorphism;ibm notes;quasi-polynomial;symmetry breaking;theory	László Babai	2015	CoRR		mathematical optimization;combinatorics;discrete mathematics;mathematics;graph isomorphism problem;algorithm;algebra	Theory	19.910308909383843	33.4607452982036	81590
f0aeb7de03a316911f9ac66cf5b803eab49b6e3b	programmable quantum processors	positive operator valued measure;03 67 lx;03 65 ta;completely positive;quantum program registers;positive operator valued measurements;programmable quantum processors;03 67 a;programmable quantum processor	A quantum processor is a device with a data register and a program register. The input to the program register determines the operation, which is a completely positive linear map, that will be performed on the state in the data register. We develop a mathematical description for these devices. We generalize the concept of quantum programmable processors and we propose programmable measurement devices.	microprocessor;quantum	Vladimír R. Buzek;Mark Hillery;Mário Ziman;Marián Rosko	2006	Quantum Information Processing	10.1007/s11128-006-0028-z	programmable logic array;theoretical computer science;mathematics;simple programmable logic device;algorithm	Arch	15.152814650790113	43.41067405773933	82373
153ccf6421ba540a3353192fd3590a76d8504af6	network structure and the firing squad synchronization problem	network structure	Abstract   The  firing squad synchronization problem , or  fssp , requires that a network of automata, limited to finite memory and local communications only, cooperate in a global task. Previous solutions to the fssp usually assume a certain network topology. This paper presents  embeddable  solutions which exploit the existing network topology without relying on a priori assumptions. A uniform lower bound on the firing time of embeddable solutions is derived, and optimal embeddable solutions are presented for several classes of networks, including rings, star graphs, flower graphs, and  n -dimensional rectangular arrays. In addition, we address the question, to what extent can solutions to the fssp for subnetworks contribute to the overall solution?	firing squad synchronization problem	John J. Grefenstette	1983	J. Comput. Syst. Sci.	10.1016/0022-0000(83)90025-9	combinatorics;discrete mathematics;computer science;mathematics;algorithm	Theory	18.303650701766895	35.27650668269372	82522
f0a52056db52ebf2e95aa6646acb225748ae90f0	alu control code assignment		Abstract#R##N##R##N#The number of gates in the ALU decoder depends on the control code assignment in the specification of the functions. This paper presents an assignment method which achieves the quasi-minimum number of gates in the decoder. The ALU functions are specified by the function identifiers and the relations between the data input and output terminals. The logical structure model for ALU is constructed and together with the function specifications, leads to the logical expression for the decoder. The obtained logical expression is the logical disjunction of the function identifiers. The function identifiers are replaced by the control codes, and the code assignment is made so that the number of product terms is the quasi-minimum when the simplification is made. The assignment time for the control codes of ALU with 32 function is approximately 160 s in the 2 MIPS computer.	arithmetic logic unit	Shigeru Takagi	1986	Systems and Computers in Japan	10.1002/scj.4690171004	real-time computing;computer science;theoretical computer science;algorithm	Robotics	20.073254883259494	45.387176684047944	82543
94cc3df6e5d21819f26f6396de68fc2e263afcb9	physical design: mathematical models and methods	floorplanning;very large scale integration;physical design;heuristic search;slicible floorplans;mathematical model;planar graphs;graph dualization;nonslicible floorplans	First, the goal of physical design is to obtain a good design, a design that achieves maximum performance and reliability, with minimum area and cost. Since some of the criteria are in conflict,, me need a balanced design. In mathematical programming terms, we have a multiobjective or several objective functions. One way to handle multiple objectives is to use a weighted objective function with the weighting factor proportional to the importance of the term (say, speed or area). During the optimization process, if the speed of the chip is fast enough, we can delete speed as an objective and maintain speed as a constraint. Second, we usually do not have the time to design every cell in a tailor-made fashion; it is much cheaper and less error-prone if we build the design from a cell library. The situation is similar to building a house where windows have st,andard sizes. In other words, me restrict our variables to have only limited, fixed values. In the extreme case, we want the variables z1 to be zero or one. Or, we may want all solutions to be positive integers and not arbitrary real values, In design terms, we need “modular” designs. In short, we need integer programming. Third, the ideal algorithm will first give us a design that satisfies all specifications, and then allow us to gradually improve the objective function if time permits. In mathematical programming terms, we need a primal method.	algorithm;cognitive dimensions of notations;integer programming;loss function;mathematical model;mathematical optimization;microsoft windows;optimization problem;physical design (electronics)	T. C. Hu	1997		10.1145/267665.267715	physical design;floorplan;mathematical optimization;combinatorics;discrete mathematics;heuristic;computer science;mathematical model;mathematics;very-large-scale integration;planar graph	EDA	20.129765918610907	39.886230973444775	83507
2d49a8b614fcd604b495851653be6532e6b1a105	an efficient interpolation filter vlsi architecture for hevc standard	vlsi cmos integrated circuits filtering theory image sequences interpolation motion estimation random access storage reconfigurable architectures video coding;frequency 193 mhz vlsi architecture hevc standard video coding standard high efficient video coding doubling coding efficiency h 264 avc fractional motion estimation fme clock latency area cost total encoding time computational complexity interpolation filter algorithm 8 pixel interpolation unit bd_psnr coding quality degradation reconfigurable configuration cell block reuse cmos technology half pixel interpolation quarter pixel interpolation ram processing latency real time processing video sequences size 90 nm;motion compensation hevc interpolation;interpolation computer architecture encoding filtering algorithms video coding hardware standards	The emerging video coding standard, High Efficient Video Coding (HEVC), aims at doubling coding efficiency of H.264/AVC. Fractional Motion Estimation (FME) in HEVC presents a significant challenge in clock latency and area cost as it consumes more than 40% of the total encoding time and its high computational complexity. Firstly, this paper proposes a fast interpolation filter algorithm, which is based on the 8-pixel interpolation unit. It can save 9.9% processing time on average while introducing only 0.1256% BD_PSNR coding quality degradation. Based on this, the paper designs and realizes the interpolation filter VLSI architecture with the reconfigurable configuration and the cell block reuse to reduce the implement hardware area. The final VLSI implementation only requires 64.5k gates in a standard 90nm CMOS technology at an operating frequency of 193MHz. The proposed architecture can be reused for half-pixel interpolation and quarter-pixel interpolation, which can reduce the area cost for about 131040 bits RAM. The processing latency of our proposed VLSI architecture can support the real-time processing of 4:2:0 format 3840×2160@47fps video sequences.	high efficiency video coding;interpolation;very-large-scale integration	Xiaocong Lian;Wei Zhou;Zhemin Duan;Rong Li	2014		10.1109/ChinaSIP.2014.6889269	computer vision;electronic engineering;real-time computing;quarter-pixel motion;computer science;coding tree unit;video post-processing;multiview video coding;image scaling	Graphics	12.53126759882113	40.58271392263752	83662
7c3b4d48ec564c39dd7eaf4a8cda1555c7682db5	multicolored dynamos on toroidal meshes	cluster computing;ordered set;fault tolerant;information network;activity pattern;connected graph;qa75 electronic computers computer science;computational complexity;upper and lower bounds;data structure;local time	Detecting on a graph the presence of the minimum number of nodes (target set) that will be able to “activate” a prescribed number of vertices in the graph is called the target set selection problem (TSS) proposed by Kempe, Kleinberg, and Tardos. In TSS’s settings, nodes have two possible states (active or non-active) and the threshold triggering the activation of a node is given by the number of its active neighbors. Dealing with fault tolerance in a majority based system the two possible states are used to denote faulty or non-faulty nodes, and the threshold is given by the state of the majority of neighbors. Here, the major effort was in determining the distribution of initial faults leading the entire system to a faulty behavior. Such an activation pattern, also known as dynamic monopoly (or shortly dynamo), was introduced by Peleg in 1996. In this paper we extend the TSS problem’s settings by representing nodes’ states with a ”multicolored” set. The extended version of the problem can be described as follows: let G be a simple connected graph where every node is assigned a color from a finite ordered set C = {1, . . . , k} of colors. At each local time step, each node can recolor itself, depending on the local configurations, with the color held by the majority of its neighbors. Given G, we study the initial distributions of colors leading the system to a k monochromatic configuration in toroidal meshes, focusing on the minimum number of initial k-colored nodes. We find upper and lower bounds to the size of a dynamo, and then special classes of dynamos, outlined by means of a new approach based on recoloring patterns, are characterized.	color;connectivity (graph theory);converge;fault tolerance;fixed point (mathematics);graph (discrete mathematics);graph coloring;kempe chain;malware;monochrome;monopoly;palette swap;polygon mesh;random graph;selection algorithm;sensor;social network;software propagation;toroidal graph;vertex (geometry)	Sara Brunetti;Elena Lodi;Walter Quattrociocchi	2010	CoRR		fault tolerance;parallel computing;data structure;computer cluster;computer science;connectivity;theoretical computer science;local time;distributed computing;upper and lower bounds;computational complexity theory;algorithm	Theory	19.666516317194386	34.39609443816818	83772
fe64d1c86f448ca97517018f6b73ac8246638997	pancyclicity of otis (swapped) networks based on properties of the factor graph	hamiltonian cycle;factor graph;interconnection network;pancyclicity;otis swapped networks;data structure;parallel processing	a r t i c l e i n f o a b s t r a c t The plausibility of embedding cycles of different lengths in the graphs of a network (known as the pancyclicity property) has important applications in interconnection networks, parallel processing systems, and the implementation of a number of either computational or graph problems such as those used for finding storage schemes of logical data structures, layout of circuits in VLSI, etc. In this paper, we present the sufficient condition of the pancyclicity property of OTIS networks. The OTIS network (also referred to as two-level swapped network) is composed of n clones of an n-node original network constituting its clusters. It has received much attention due to its many favorable properties such as high degree of scalability, regularity, modularity, package-ability and high degree of algorithmic efficiency. Many properties of OTIS networks have been studied in the literature. In this work, we show that the OTIS networks have the pancyclicity property when the factor graph is Hamiltonian. More precisely, using a constructive method, we prove that if the factor graph G of an OTIS network contains cycles of length {3,	algorithmic efficiency;computer cluster;data structure;factor graph;hamiltonian (quantum mechanics);interconnection;multi-core processor;parallel algorithm;parallel computing;plausibility structure;scalability;system on a chip;very-large-scale integration	Marzieh Malekimajd;M. Hoseiny Farahabady;Ali Movaghar-Rahimabadi;Hamid Sarbazi-Azad	2011	Inf. Process. Lett.	10.1016/j.ipl.2011.07.020	hamiltonian path;parallel processing;combinatorics;data structure;computer science;factor graph;mathematics;distributed computing;programming language;algorithm	Theory	23.555202310129697	35.738813259134005	83875
ea4b824212f17d6ee5fd5af2bd722915e5d6dab4	modified non-restoring division algorithm with improved delay profile and error correction	lsb nonrestoring division algorithm delay profile correction error correction radix 2 digit recurrent division algorithm nonrestoring divider multiplexer per iteration digit conversion digit converter binary number;logic circuits;multiplexing equipment;iterative methods;error correction;multiplexing equipment digital arithmetic error correction iterative methods logic circuits;digital arithmetic	This paper focuses on improving the performance of non-restoring division by reducing the delay and finding a correct quotient quickly. Although the non-restoring division algorithm is the fastest and has less complexity than other radix-2 digit recurrent division algorithms, there are still some possibilities to enhance its performance. To improve its performance, two new approaches are proposed here. For the first proposed approach, a non-restoring divider with a modified algorithm is presented. The new algorithm changes the order of the flowchart, which reduces one unit delay of the multiplexer per iteration. Secondly, a new method to find a correct quotient is presented and it removes an error that the quotient is always odd number after a digit conversion from a digit converter from the quotient with digits 1 and -1 to a conventional binary number. The new logic to generate the LSB of the quotient quickly is also explained in this paper.	binary number;division algorithm;error detection and correction;fastest;flowchart;iteration;least significant bit;multiplexer	Kihwan Jun;Earl E. Swartzlander	2012	2012 Conference Record of the Forty Sixth Asilomar Conference on Signals, Systems and Computers (ASILOMAR)	10.1109/ACSSC.2012.6489269	arithmetic;theoretical computer science;division algorithm;mathematics;check digit;algorithm	EDA	15.488951010301548	44.04784745774437	84408
6f71df3884a36fa205fe3ec4d7d71867c6d7ec69	optimization and implementation of h.264 encoder on dsp platform	optimization dsp h 264 encoder;video coding digital signal processing chips;tms320dm642;frames per second;computational complexity analysis;dsp platform;h 264 encoder;real time;video coding;computational complexity analysis dsp platform mpeg 4 h 264 standard h 264 baseline profile encoder tms320dm642;computational complexity;h 264 baseline profile encoder;mpeg 4;digital signal processing chips;optimization;digital signal processing image coding computational complexity streaming media mpeg 4 standard algorithm design and analysis videoconference digital multimedia broadcasting multimedia systems filters;data transfer;h 264 standard;dsp	Compared with MPEG-4 and other previous standards, H.264 standard has achieved great breakthrough in coding performance. In this paper, the optimization and implementation of H.264 baseline profile encoder on the TMS320DM642 has been presented. Based on the architectural features of TMS320DM642 and the computational complexity analysis of H.264 encoder, the H.264 encoder has been optimized from three aspects: algorithms, data transfer and memory/Cache use. The experimental results demonstrate that, for the video sequences with CIF format, the optimized H.264 encoder can achieve the encoding speed of more than 24 frames per second, which can meet the real-time requirements of the applications.	algorithm;analysis of algorithms;baseline (configuration management);computational complexity theory;encoder;h.264/mpeg-4 avc;mathematical optimization;real-time transcription;requirement;server message block	Li Zhuo;Qiang Wang;David Dagan Feng;Lansun Shen	2007	2007 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2007.4284629	embedded system;real-time computing;computer hardware;computer science;digital signal processing;computational complexity theory;frame rate;mpeg-4;algorithm	Robotics	11.784986323487367	40.224982420027246	84412
8c8b2331f058f8d8f2ca9c7a36cd32ab517413bd	a hardware decoder architecture for general string matching technique	decoding;very large scale integration;color;streaming media;encoding;pipeline processing;hardware	This paper proposes a low cost hardware decoder design and architecture that supports general string matching (SM) using both a primary reference buffer (PRB) and a secondary reference buffer (SRB). An SM decoding module mainly performs string copy operation. The SM decoding module can also support intra block copy and palette decoding because both are actually two special cases of general string copy. The proposed architecture has four pipeline stages. The theoretical lowest system clock frequency required to guarantee worst case string copy operation using single-port SRAM is lower than the video output pixel rate, which is the product of frame resolution and frame rate. The design uses four pieces of SRAMs with size of 24K bytes, 16.5K bytes, 4K bytes, and 3K bytes, respectively. The first one is specific to SM and the other three are shared by traditional hybrid decoding operations. The SM decoding module has 69.5K logic gate count and can achieve 60 fps real time SM decoding for 4K (4096 × 2160) video at 218 MHz working frequency.	4k resolution;best, worst and average case;byte;chroma subsampling;clock rate;cost efficiency;encoder;gate count;kilobyte;logic gate;palette (computing);pixel;primary source;racket;scheduling (computing);string searching algorithm;throughput;tip (unix utility);video decoder	Kailun Zhou;Liping Zhao;Tao Lin	2016	IEEE Journal on Emerging and Selected Topics in Circuits and Systems	10.1109/JETCAS.2016.2599876	embedded system;electronic engineering;real-time computing;computer hardware;telecommunications;computer science;theoretical computer science;operating system;very-large-scale integration;encoding	Arch	11.407201289662831	41.46520945922595	84525
751055ac70020bf5b681085828db9082c0d46df6	two-dimensional rank-order filter by using max-min sorting network	tiempo respuesta;circuit rapide;arquitectura circuito;interconnection;image processing;median filter;implementation;two dimensional filtering;circuit vlsi;procesamiento imagen;circuit architecture;response time;tecnologia mos complementario;indexing terms;traitement image;filtrage bidimensionnel;two dimensional digital filters;temps reponse;interconexion;filtrado bidimensional;ejecucion;minimax techniques;vlsi circuit;cmos digital integrated circuits;sorting network;block processing;filtro mediano;circuito rapido;interconnexion;architecture circuit;125 mhz 2d rank order filter max min sorting network vlsi architecture fast response time modular architecture regular interconnection throughput block processing time area complexity reduction median filter single poly double metal cmos process clock rate bit serial pipelining 0 8 micron;vlsi;minimax techniques two dimensional digital filters median filters vlsi cmos digital integrated circuits pipeline processing;sorting nonlinear filters throughput finite impulse response filter very large scale integration digital filters two dimensional displays filtering delay cmos process;arquitectura modular;circuito vlsi;technologie mos complementaire;modular architecture;filtre median;fast circuit;traitement bloc;architecture modulaire;complementary mos technology;median filters;pipeline processing;vlsi architecture	Based on the previously developed sorting networks, a new VLSI architecture suitable for two-dimensional (2-D) rank-order filtering is proposed. The major advantage of the proposed architecture is their fast response time, modular architecture, and simple and regular interconnection. Generally speaking, the throughput of the proposed architecture is (N-1) times faster than using a one-dimensional rank-order filter for 2-D N/spl times/N data. The concept of block processing is also incorporated into the design to reduce the time-area complexity of the proposed architecture. Roughly speaking, the complexity is reduced to 2/3 and 1/2 compared with a rank-order and median filter without using a block processing architecture, respectively. A 3/spl times/3 median filter with block processing architecture is implemented through a 0.8 /spl mu/m single-poly double-metal CMOS process. The results are correct with a clock rate up to 125 MHz.	maxima and minima;sorting network	Ching C. Lin;Chung J. Kuo	1998	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.736722	median filter;embedded system;computer vision;real-time computing;index term;image processing;sorting network;computer science;interconnection;very-large-scale integration;implementation;response time	Vision	13.509070154381657	37.92957816215687	84615
b61fbef6197b097b7f9f5ce8e1bc9daf358323d3	implementation of a multiprocessor system with distributed embedded dram on a large area integrated circuit	cmos integrated circuits;distributed memory systems;integrated circuit;multiprocessor systems;dram chips parallel architectures distributed memory systems embedded systems large scale integration cmos integrated circuits;chip;video coding;embedded systems;large scale integration;parallel architectures;multiprocessing systems random access memory video coding video compression image sensors arithmetic image storage integrated circuit yield cmos technology error correction codes;digital video;dram chips;4 mbit distributed embedded dram large area integrated circuit multiprocessor coding international standards iso mpeg 2 itu t h 263 1 9 gops video signal processor axpe 4 mbit embedded dram digital video interfaces inter processor communication single mask 0 25 spl mu m cmos circuit axpe subsystem upper metal layers connection structures chip boundaries redundancy defect processor nodes 0 25 mum 1 9 gflops;internal standard	An architecture of a multiprocessor coding system suitable for large area integration has been developed. Application field is video coding according to the international standards I S 0 MPEG-2 and ITU-T H.263 or similar methods. It is based on processor nodes, which consist of a 1.9 GOPS video signal processor AxPe, 4 MBit of embedded DRAM, and digital video interfaces for data input and output as well as for inter-processor communication. Four of these processor nodes can be fabricated with a single mask set on a 0.25pm CMOS circuit of 2x2 cm2, which is called AxPe subsystem. By overlapping manufacturing and cutting out of 2x2 of AxPe subsystems aferwards, a large area integrated circuit (LAIC) with 16 processor nodes can be realized. The upper metal layers of each AxPe subsystem contains connection structures at the chip boundaries for this purpose. Redundancy techniques ensure the functionality of the LAIC even in the case of defect processor nodes.	cmos;data compression;digital video;dynamic random-access memory;edram;embedded system;input/output;integrated circuit;mpeg-2;megabit;multiprocessing;signal processing;software bug	Klaus Herrmann;Sören Moch;Jörg Hilgenstock;Peter Pirsch	2000		10.1109/DFTVS.2000.887148	chip;embedded system;electronic engineering;real-time computing;computer science;operating system;integrated circuit;internal standard;cmos	EDA	10.785051365357042	40.95603477330349	84781
aa6b644843fb3f4d77bbbb0e70adb473baf92eb5	high performance table-based architecture for parallel crc calculation	parallel architecture crc generation parallel algorithm;generators;table lookup cyclic redundancy check codes parallel algorithms parallel architectures sram chips;polynomials;acceleration;computer architecture;algorithm design and analysis computer architecture throughput polynomials acceleration hardware generators;algorithm design and analysis;throughput;hardware;clock cycle high performance table based architecture parallel crc calculation cyclic redundancy check parallel crc algorithm crc computation lookup tables crc32 crc24 crc ccitt crc16 crc8 sram static random access memory	A high performance table-based architecture implementation for CRC (cyclic redundancy check) algorithms is proposed. The architecture is designed based on a highly parallel CRC algorithm. The algorithm first divides a given message with any length into bytes. Then it performs CRC computation using lookup tables among the divided bytes in parallel. At last, the results are XORed to obtain the CRC value of the given message. The algorithm is table-based and can accelerate different CRC algorithms. Based on the algorithm, the architecture is designed to accelerate CRC algorithms with high parallelism and flexibility. The architecture is configurable and can support CRC algorithms such as CRC32, CRC24, CRC-CCITT, CRC16, CRC8. CRC value of 128-bit input data can be generated in one cycle. Our method also allows calculation over data that is less than 128-bit wide without increasing hardware cost. With 128-bit input each clock cycle, the throughput of the proposed architecture reaches up to 100 Gbps by utilizing 16 KB SRAM (Static Random Access Memory) with about 12% area reduction compared with previous work.	128-bit;algorithm;byte;clock signal;computation;critical path method;cyclic redundancy check;data rate units;error detection and correction;exclusive or;gate count;linear-feedback shift register;lookup table;parallel computing;random access;static random-access memory;throughput	Yuanhong Huo;Xiaoyang Li;Wei Wang;Dake Liu	2015	The 21st IEEE International Workshop on Local and Metropolitan Area Networks	10.1109/LANMAN.2015.7114717	computer architecture;parallel computing;computer science;theoretical computer science;crc-based framing	Arch	10.404958470008745	44.751685770289264	84836
3010350a7ae3907173642cc9e6e7cf63164ed804	novel reversible `tsg' gate and its application for designing components of primitive reversible/quantum alu	tsg gate;reversible alu reversible logic quantum computing reversible gates tsg gate;architectural design;trees mathematics adders compressors logic gates multiplying circuits;full adder;multiplying circuits;optical computing;trees mathematics;compressor;wallace tree multiplier;low power;reversible logic;reversible gates;logic gates;compressors;quantum computer;complex system;adders;quantum alu;quantum computing cmos logic circuits optical computing cmos technology logic gates optical design very large scale integration embedded system information technology embedded computing;reversible alu;quantum computing;wallace tree multiplier tsg gate quantum alu full adder compressor	In recent years, reversible logic has emerged as a promising computing paradigm having application in low power CMOS, quantum computing, nanotechnology, and optical computing. The classical set of gates such as AND, OR, and EXOR are not reversible. This paper utilizes a new 4*4 reversible gate called TSG gate to build the components of a primitive reversible/quantum ALU. The most significant aspect of the TSG gate is that it can work singly as a reversible full adder, that is reversible full adder can now be implemented with a single gate only. A Novel reversible 4:2 compressor is also designed from the TSG gate which is later used to design a novel 8times8 reversible Wallace tree multiplier. It is proved that the adder, 4:2 compressor and multiplier architectures designed using the TSG gate are better than their counterparts available in literature, in terms of number of reversible gates and garbage outputs. This is perhaps, the first attempt to design a reversible 4:2 compressor and a reversible Wallace tree multiplier as far as existing literature and our knowledge is concerned. Thus, this paper provides an initial threshold to build more complex systems which can execute complicated operations using reversible logic	adder (electronics);arithmetic logic unit;cmos;complex systems;optical computing;programming paradigm;quantum computing;reversible computing;wallace tree	Himanshu Thapliyal;M. B. Srinivas	2005	2005 5th International Conference on Information Communications & Signal Processing	10.1109/ICICS.2005.1689293	reversible computing;complex systems;electronic engineering;toffoli gate;gas compressor;three-input universal logic gate;theoretical computer science;quantum circuit;quantum computer;algorithm;adder	EDA	16.917340246151422	45.30393982207777	85088
be483993ef506cd8cc6a0e975ec4cd9a49452c50	a new family of cayley graph interconnection networks based on wreath product and its topological properties	interconnection networks;cayley graph;routing algorithm;diameter;embedding;fault tolerance	This paper introduces a new type of Cayley graphs for building large-scale interconnection networks, namely $\mathit{WG}_{n}^{2m}$ , whose vertex degree is m+3 when n≥3 and is m+2 when n=2. A routing algorithm for the proposed graph is also presented, and the upper bound of the diameter is deduced as ⌊5n/2⌋. Moreover, the embedding properties and maximal fault tolerance are also analyzed. Finally, we compare the proposed networks with some other similar network topologies. It is found that $\mathit{WG}_{n}^{2m}$ is superior to other interconnection networks because it helps to construct large-scale networks with lower cost.	algorithm;degree (graph theory);fault tolerance;interconnection;magma;maximal set;network topology;routing	Zhen Zhang;Wenjun Xiao	2011	Cluster Computing	10.1007/s10586-011-0189-0	combinatorics;fault tolerance;distributed computing;degree (graph theory);interconnection;embedding;network topology;upper and lower bounds;wreath product;cayley graph;computer science	ML	23.66928518225515	35.2438582674757	85284
aad39fd6d1d7ff22787abbf48652c411532f752a	generalized fixed polarity helix transforms over gf(4)	transforms radio frequency abstracts logic gates equations;radio frequency;logic gates;abstracts;transforms;transforms galois fields matrix algebra;reed muller;generalized fixed polarity helix transforms quaternary reed muller transform computational costs butterfly diagrams mutual relations quaternary helix transform matrices recursive equations galois field 4 linearly independent quaternary transforms gf 4	New linearly independent quaternary transforms over Galois Field (4) called Generalized Fixed Polarity Helix transforms are introduced here. Their definitions based on recursive equations are described. Various properties of quaternary helix transform matrices, their mutual relations as well as their butterfly diagrams and computational costs versus quaternary Reed-Muller transform are also discussed.	diagram;recursion (computer science);reed–muller code	Cheng Fu;Bogdan J. Falkowski	2009	2009 17th European Signal Processing Conference		arithmetic;pure mathematics;mathematics;algebra	Vision	18.12020779474954	44.04331987164441	85743
e04d016fb48d4ec99d56de1ef92b0c813176e3a0	design of high speed vedic multiplier for decimal number system	nnd;urdhva tiryakbyham ut;vedic mathematics;high speed;bcd	"""Vedic mathematics is the ancient techniques of mathematics, based on 16 simple sutras (formulae). Decimal number system multiplication technique based on such ancient mathematics is reported in this paper. Improvement in speed was achieved through stage reduction by """"Nikhilam Navatascaramam Dasatah (NND)"""" (all from 9 and last from 10) which was adopted from Vedas, during multiplication. Binary coded decimal (BCD) methodology was incorporated with Vedic mathematics, to implement such multiplier for practical VLSI applications. The functionality of these circuits was checked and performance parameters such as propagation delay, dynamic switching power consumptions were calculated by spice spectre using 90nm CMOS technology. BCD implementation of Vedic multiplier ensures the stage reduction for decimal number, hence substantial reduction in propagation delay compared with earlier reported one, has been investigated. Implementation result offered propagation delay of the resulting (5×5) digit decimal multiplier was only ~5.798ns while the power consumption of the same was ~23.487μW. Almost ~26% improvement in speed from earlier reported decimal multiplier, e.g. parallel implementation methodology,the best architecture reported so far, has been achieved."""		Prabir Saha;Arindam Banerjee;Anup Dandapat;Partha Bhattacharyya	2012		10.1007/978-3-642-31494-0_10	binary-coded decimal;computer science;operating system;algorithm	Robotics	15.76506862768513	44.896653442392704	86043
31bdaf78fb68f767ff6f046cda1dbbe9bd17fadb	reasoning about the vhdl standard logic package signal data type	data type;formal verification	Formal veriication methods provide a way to prove that a circuit structure correctly implements its speciication. Low level gate and transistor logic circuits can be veriied using methods such as symbolic simulation. Higher level circuits can be veriied using theorem{proving methods, providing a multi-leveled approach to complex device verii-cation. Correct modeling of many VLSI circuits requires a signal value data type that includes some degree of strength indeterminacy. The VHDL Standard Logic Package includes such a signal value data type, t wlogic, that includes 46 unique values. In this paper we provide a foundation for the t wlogic values and their resolution function that provides a possible link between simulation and theorem-proving. A formalization of the low level signal values used by design tools also facilitates reasoning about CAD tools. Many VLSI design techniques involve the use of varying sized transistors for interconnected device outputs. Under certain circuit conditions the output of one device will overdrive the other connected outputs. In order to correctly simulate these circuits, a signal value model that combines three signal states, 1, 0, and X with multiple signal strengths, in such a way that there is a degree of strength indeterminacy is required 4]. The VHDL Standard Logic Package (SLP) deenes such a signal value data type, t wlogic, that includes 46 unique values 5]. In this paper, we provide a foundation for the t wlogic values and resolution function based on an extension of Bryant's lattice-theoretic approach. This foundation facilitates reasoning about the t wlogic signal value model. The imperative style signal value resolution algorithms and look-up tables (LUTs) used in many logic simulation programs are not appropriate for reasoning. The ability to manipulate these lower level signal values in a theorem{proving environment ooers a more exible interface	computer-aided design;imperative programming;indeterminacy in concurrent computation;logic gate;logic simulation;lookup table;nondeterministic algorithm;pentium overdrive;sentinel value;superword level parallelism;symbolic simulation;theory;transistor;vhdl;very-large-scale integration	J. W. Gambles;Phillip J. Windley	1993		10.1016/B978-0-444-81641-2.50014-9	computer architecture;computer science;theoretical computer science;logic simulation;algorithm	EDA	19.149798138892063	45.452740285223754	86301
7bf41d4e3027155ae919cc4b1f300c71c36094ff	performance of software-based mpeg-2 video encoder on parallel and distributed systems	parallel and distributed system;tratamiento paralelo;software;network of workstations;traitement signal;data parallel;evaluation performance;parallel processing video coding telecommunication computing motion estimation;estimation mouvement;standards;performance evaluation;traitement parallele;logiciel;evaluacion prestacion;estimacion movimiento;compresion senal;buffer management;telecommunication computing;motion estimation;cluster of workstations;indexing terms;compression signal;video coding;codificacion;senal video;signal video;signal processing;mpeg 2;signal compression;parallel and distributed systems;norma;network of workstation;coding;parallel computer;encoding application software hardware software performance software quality hypercubes computer networks concurrent computing workstations parallel processing;video signal;logicial;frame encoding rates software based mpeg 2 video encoder parallel systems distributed systems software solution performance intel paragon xp s intel ipsc 860 hypercube parallel computer data parallel approach real time applications nonreal time applications motion search window buffer management bit rate execution times speedups;procesamiento senal;norme;parallel processing;codage	Video encoding due to its high processing requirements has been traditionally done using special-purpose hardware. Software solutions have been explored but are considered to be feasible only for nonreal-time applications requiring low encoding rates. However, a software solution using a general-purpose computing system has numerous advantages: It is more available and flexible and allows experimenting with and hence improving various components of the encoder. In this paper, we present the performance of a software video encoder with MPEG-2 quality on various parallel and distributed platforms. The platforms include an Intel Paragon XP/S and an Intel iPSC/860 hypercube parallel computer as well as various networked clusters of workstations. Our encoder is portable across these platforms and uses a data-parallel approach in which parallelism is achieved by distributing each frame across the processors. The encoder is useful for both real-time and nonreal-time applications, and its performance scales according to the available number of processors. In addition, the encoder provides control over various parameters such as the size of motion search window, buffer management, and bit rate. The performance results include comparisons of execution times, speedups, and frame encoding rates on various systems.	algorithm;central processing unit;data compression;distributed computing;encoder;experiment;frame language;general-purpose modeling;h.262/mpeg-2 part 2;intel paragon;mpeg-2;parallel computing;real-time clock;real-time computing;requirement;workstation	Shahriar M. Akramullah;Ishfaq Ahmad;Ming Lei Liou	1997	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.611179	embedded system;parallel processing;computer vision;parallel computing;real-time computing;index term;computer science;signal processing;motion estimation;mpeg-2;coding	HPC	10.76711101797609	38.34031851901275	86558
0134d08388598e8440d63cb0dcc7fc758769b4ec	localized spanner construction for ad hoc networks with variable transmission range	directed graphs;distributed algorithms;metric space;ad hoc network;spanners;directed graph;unit disk graph;distributed algorithm	This article presents an algorithm for constructing a spanner for ad hoc networks whose nodes have variable transmission range. Almost all previous spanner constructions for ad hoc networks assumed that all nodes in the network have the same transmission range. This allowed a succinct representation of the network as a unit disk graph, serving as the basis for the construction. In contrast, when nodes have variable transmission range, the ad hoc network must be modeled by a general disk graph. Whereas unit disk graphs are undirected, general disk graphs are directed. This complicates the construction of a spanner for the network, since currently there are no efficient constructions of low-stretch spanners for general directed graphs. Nevertheless, in this article it is shown that the class of disk graphs enjoys (efficiently constructible) spanners of quality similar to that of unit disk graph spanners. Moreover, it is shown that the new construction can be performed in a localized fashion. Our results use only simple packing arguments, hence all algorithms work for every metric space of constant doubling dimension.	algorithm;directed graph;graph (discrete mathematics);hoc (programming language);ibm 7030 stretch;internationalization and localization;period-doubling bifurcation;set packing;unit disk graph	David Peleg;Liam Roditty	2010	TOSN	10.1145/1807048.1807054	theta graph;wireless ad hoc network;distributed algorithm;unit disk graph;combinatorics;discrete mathematics;directed graph;computer science;mathematics;distributed computing;random geometric graph;indifference graph	Theory	21.020749313081268	35.65442321696189	86699
553bbe424f9d16c0806b1df0edb8c9fa66971118	an efficient 175 mhz programmable fir digital filter	1 2 micron;coefficient values;switchable unit delay;cmos technology;multiplying circuits;switching circuits;clocks;175 mhz;finite impulse response filter;linear taps;resource management;optimal hardware resources;fir digital filter;1 2 micron programmable fir digital filter switchable unit delay optimal hardware resources filter tap linear taps die size cmos technology coefficient values digit multiplication hardware 175 mhz;digit multiplication hardware;chip;programmable fir digital filter;filter tap;finite impulse response;cmos digital integrated circuits;digital integrated circuits;multiplying circuits programmable filters fir filters digital filters delay circuits cmos digital integrated circuits;digital filters;delay circuits;programmable filters;finite impulse response filter digital filters hardware cmos technology switching circuits resource management latches clocks digital integrated circuits laboratories;latches;fir filters;high frequency;die size;hardware	An efficient 175-MHz programmable finite impulse response (FIR) digital filter is implemented. It uses a novel switchable unit-delay to allocate the optimal hardware resources to each filter tap. The authors' prototype circuit can have up to 32 linear taps with 16-bit I/O in a die size of 5.9mm by 3.4mm using 1.2 /spl mu/m CMOS technology. A simple recoding of the coefficient values results in a simplification of the digit multiplication hardware. On-chip testing circuitry permits the testing of the chip at a high frequency. >		Kei-Yong Khoo;Alan Kwentus;Alan N. Willson	1993		10.1109/ISCAS.1993.393660	electronic engineering;real-time computing;computer hardware;telecommunications;computer science;resource management;finite impulse response	HCI	13.354965349764795	44.98801182875231	86785
4ed264502b4b86217f2ac14606cc91c7db08e6a1	135-mhz 258-k gates vlsi design for all-intra h.264/avc scalable video encoder	cmos integrated circuits;variable length codes;data compression;scalable video coding svc;encoding static var compensators memory management scalability prediction algorithms very large scale integration transforms;all intra;video coding;integrated circuit design;pipelines;vlsi architecture design;vlsi;video codecs;vlsi cmos integrated circuits data compression integrated circuit design pipelines variable length codes video codecs video coding;article;vlsi architecture design all intra scalable video coding svc;size 90 nm cmos technology mb level pipeline macroblock level pipeline hardware efficient techniques context adaptive variable length coding syntax element encoding approach fast intraprediction algorithm external memory access internal memory usage memory bandwidth requirements single video encoder multiple demanded video data svc advanced video coding all intra h 264 avc scalable video encoder 258 k gates vlsi design frequency 135 mhz	To satisfy the video application diversities, an extension of H.264/advanced video coding (AVC), called scalable video coding (SVC), is designed to provide multiple demanded video data via a single video encoder. However, constructed on the fundamental of H.264/AVC, the complexity of SVC is much higher than that of H.264/AVC. In this paper, a VLSI design for all-intra scalable video encoder is proposed to aim at efficient scalable video encoding. First, the memory bandwidth requirements for several encoding methods are analyzed to find out the best encoding method which can achieve best tradeoff between internal memory usage and external memory access. Afterward, an all-intra SVC encoder combined with several advanced techniques, including fast intra prediction algorithm, efficient syntax element encoding approach in context-adaptive variable-length coding, and hardware-efficient techniques, are implemented in a macroblock (MB)-level pipeline to increase data throughput. Implementation results demonstrate that our proposed SVC encoder can process more than 594-k MBs per second, which is equivalent to the summation of 60 high-definition, 1080-p, SD 480-p, and common intermediate format frames under 135-MHz working frequency. The proposed design consumes 258-K gate counts when synthesized by 90-nm CMOS technology.	algorithm;cmos;clock rate;computer data storage;context-adaptive variable-length coding;data compression;encoder;gate count;h.264/mpeg-4 avc;hdmi;image resolution;intra-frame coding;macroblock;memory bandwidth;requirement;scalability;scalable video coding;server message block;throughput;variable-length code;very-large-scale integration	Gwo-Long Li;Tzu-Yu Chen;Meng-Wei Shen;Meng-Hsun Wen;Tian-Sheuan Chang	2013	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2012.2190536	data compression;scalable video coding;embedded system;electronic engineering;parallel computing;real-time computing;computer science;context-adaptive variable-length coding;pipeline transport;very-large-scale integration;context-adaptive binary arithmetic coding;cmos;motion compensation;statistics;multiview video coding;integrated circuit design	EDA	12.344574134662423	40.24728955267887	86961
d9d5b6e02bee9837a9d23d00681f96ec02927062	rtp payload format for 12-bit dat audio and 20- and 24-bit linear sampled audio		This document specifies a packetization scheme for encapsulating 12-bit nonlinear, 20-bit linear, and 24-bit linear audio data streams using the Real-time Transport Protocol (RTP). This document also specifies the format of a Session Description Protocol (SDP) parameter to indicate when audio data is preemphasized before sampling. The parameter may be used with other audio payload formats, in particular L16 (16-bit linear).		Katsushi Kobayashi;Akimichi Ogawa;Stephen L. Casner;Carsten Bormann	2002	RFC	10.17487/RFC3190	embedded system;real-time computing;raw audio format;computer hardware;computer science	HCI	10.21532278074464	40.51583583383695	87034
911e3987680f40f7f58bf77b0a2b30f1c288182e	a novel fir filter architecture for efficient signal boundary handling on xilinx virtex fpgas	digital signal processing;field programmable gate array;image processing;finite impulse response filter field programmable gate arrays digital filters digital signal processing programmable logic arrays signal processing filtering array signal processing image processing arithmetic;shift registers fir filters field programmable gate arrays;finite impulse response;daubechies 8 wavelet fir filter finite impulse response filters signal boundary handling xilinx virtex fpga field programmable gate array digital signal processing finite length signal processing bit parallel arithmetic shift register logic srl16;signal processing;fir filter;shift registers;fir filters;field programmable gate arrays	FIR filters are often used in digital signal processing. This paper presents a novel architecture for FIR filters on Xilinx Virtex FPGAs. The architecture is particularly useful for handling the problem of signal boundaries filtering, which occurs in finite length signal processing (e.g. image processing). Based on a bit parallel arithmetic, our architecture is fully scalable and parameterised. It cleverly exploits the Shift Register Logic (SRL16) component of the Virtex family. The implementation leads to considerable area savings compared to the conventional implementation (based on a hard router) with no speed penalty. A case study based on the implementation of the standard low filter of the Daubechies-8 wavelet on Xilinx Virtex-E FPGAs is presented.	brute-force search;digital signal processing;field-programmable gate array;finite impulse response;image processing;router (computing);scalability;shift register;virtex (fpga);wavelet	Abdsamad Benkrid;Khaled Benkrid;Danny Crookes	2003		10.1109/FPGA.2003.1227267	multidimensional signal processing;embedded system;parallel computing;cascaded integrator–comb filter;digital down converter;digital filter;computer hardware;image processing;programmable logic array;computer science;digital signal processing;finite impulse response;signal processing;linear filter;filter bank	Arch	12.209153585966362	43.5625282772124	87406
ae765ab91072af025fa6de1a0bd457108fb8c2d9	low cost boolean function generation	simulator;computer architecture;interactive animation;coherence protocol;mesi	In this work we propose a new technique for the generation of binary functions based on the use of a decoder with tri-state outputs. The hardware overhead of the proposed scheme compared to other generic schemes like the multiplexer-based and decoder-based implementation is significantly lower.	multiplexer;overhead (computing);three-state logic;triangular function	Ioannis Voyiatzis;Costas Efstathiou	2016		10.1145/3003733.3003802	computer architecture;real-time computing;computer science;theoretical computer science;operating system;mesi protocol	EDA	10.034922328784162	45.83432383887904	87412
579ff19fcabdb113f1ad8f89554a0ae3abda84d7	a 242mw 10mm2 1080p h.264/avc high-profile encoder chip	fractional motion estimation;video coding digital signal processing chips discrete cosine transforms motion estimation;video encoder;nonsampling reference memory sharing;motion estimation;h 264 avc high profile encoder chip;discrete cosine transform;chip;power 242 mw;video coding;automatic voltage control throughput energy consumption hdtv motion estimation hardware costs testing frequency circuits;integer motion estimation;discrete cosine transforms;power 242 mw h 264 avc high profile encoder chip video encoder integer motion estimation nonsampling reference memory sharing fractional motion estimation discrete cosine transform;digital signal processing chips;high definition	High-profile H.264 has been adopted as the major coding standard in popular high definition video due to its excellent coding efficiency. Several implementations have been developed ((Y. W. Huang, et al., 2005), (H.C. Chang, et al., 2007), (T.C. Chen, et al., 2007)), but, their performance is limited to baseline 720p (Y. W. Huang, et al., 2005),(H.C. Chang, et al., 2007) or SDTV (T.C. Chen, et al., 2007). The main stream 1080p high-profile application presents a series of new design challenges in throughput, cost and power because of at least a 4x higher complexity than in the 720p baseline. Thus, a 0.13mum 1080p high-profile H.264 video encoder is presented with 10mm2 core and 242 mW power. Compared to a state-of-the-art 720p baseline design (H.C. Chang, et al., 2007), this design achieves a 46.7% and 54% reduction in area and power, respectively. These savings are from parallelism enhanced throughput and a cross-stage sharing pipeline.	algorithmic efficiency;baseline (configuration management);encoder;entity–relationship model;h.264/mpeg-4 avc;parallel computing;standard-definition television;throughput	Yu-Kun Lin;De-Wei Li;Chia-Chun Lin;Tzu-Yun Kuo;Sian-Jin Wu;Wei-Cheng Tai;Wei-Cheng Chang;Tian-Sheuan Chang	2008	2008 IEEE International Solid-State Circuits Conference - Digest of Technical Papers	10.1109/ISSCC.2008.4523183	chip;embedded system;encoder;electronic engineering;real-time computing;telecommunications;computer science;discrete cosine transform;motion estimation	EDA	12.397658795066826	40.622596296023	87548
b51957bb61c7ce73f4325ffafbbfe747ab0124f0	metrics of vector logic algebra for cyber space	computers;convolution;vector matrix transformation;logical operation vector logic algebra cyber space algebraic structure vector matrix transformation discrete vector boolean space associative data;научная статья бесплатно на тему metrics of vector logic algebra for cyber space текст научной работы по автоматике и вычислительной технике из научного журнала радиоэлектроника и информатика;associative data;boolean algebra;discrete vector boolean space;convolution algebra extraterrestrial measurements computers parallel processing mathematical model;cyber space;algebra;data structures;structure determination;mathematical model;logical operation;data structures boolean algebra;algebraic structure;vector logic algebra;extraterrestrial measurements;information analysis;parallel processing	The algebraic structure determining the vector-matrix transformation in the discrete vector Boolean space for the analyzing information based on logical operations on associative data.	cyberspace;linear algebra;logical connective;stone's representation theorem for boolean algebras;transformation matrix	Vladimir Hahanov;Alexander Mishchenko;Vitaliy Varetsa	2010	2011 11th International Conference The Experience of Designing and Application of CAD Systems in Microelectronics (CADSM)	10.1109/EWDTS.2010.5742149	boolean algebra;and-inverter graph;discrete mathematics;boolean domain;theoretical computer science;mathematics;two-element boolean algebra;free boolean algebra;algebra	Robotics	20.025106213082207	42.341299437328246	87704
8a6352979666c174a2f961efdcb7bc27583fef4e	a pipelined pseudoparallel system architecture for real-time dynamic scene analysis	pseudoparallelism;computers;processing element;processing;general and miscellaneous mathematics computing and information science;problem partitioning;image processing;communications;dynamic scene analysis;distributed operating system;real time computation;continuous flow;simd and mimd algorithmic steps distributed control dynamic scene analysis interconnection networks interprocessor communication pipelining problem partitioning pseudoparallelism real time computation;data flow processing;interprocessor communication;interconnection networks;programming 990200 mathematics computers;simd and mimd;pipelining;algorithmic steps;data flow;system architecture;distributed control;parallel processing	In this paper we introduce the concept of pseudoparallelism, in which the serial algorithm is partitioned into several noninteractive independent subtasks so that parallelism can be used within each subtask level. This approach is illustrated by applying it to a real-time dynamic scene analysis. Complete details of such a pseudoparallel architecture with an emphasis to avoid interprocessor communications have been worked out. Problems encountered in the course of designing such a system with a distributed operating system (no master control) have been outlined and necessary justifications have been provided. A scheme indicating various memory modules, processing elements, and their data-path requirements is included and ways to provide continuous flow of partitioned information in the form of a synchronized pipeline are described.	dimm;distributed operating system;interactivity;master control;parallel computing;real-time clock;real-time computing;real-time transcription;requirement;sequential algorithm	Dharma P. Agrawal;Ramesh Jain	1982	IEEE Transactions on Computers	10.1109/TC.1982.1675904	embedded system;data flow diagram;parallel processing;parallel computing;image processing;computer science;processing;theoretical computer science;operating system;distributed computing;programming language;pipeline	Visualization	10.298891665743378	35.678834643920396	87870
5371274778eb16241c124e7177cc47815ca4df12	fault-free longest paths in star networks with conditional link faults	tolerancia falta;embedding;calcul tolerant les pannes;interconnection;fault tolerant;nudo;barreta lineal;linear array;distributed computing;longest path;barrette lineaire;hamiltonian;cayley graph;calculo automatico;parallel and distributed computing;hamiltonien;interconnection network;computing;hamiltonian laceability;red;calcul automatique;interconexion;68m15;fault tolerant computing;informatique theorique;reseau arrangement;fault tolerance;68r10;plongement;conditional fault model;interconnexion;palabra;grafo cayley;calculo repartido;vinculo;array;word;inmersion;noeud;random fault model;fault model;link;68q85;node;calcul reparti;hamiltoniano;tolerance faute;red interconexion;lien;mot;computer theory;star network;reseau interconnexion;graphe cayley;informatica teorica	The star network, which belongs to the class of Cayley graphs, is one of the most versatile interconnection networks for parallel and distributed computing. In this paper, adopting the conditional fault model in which each node is assumed to be incident with two ormore fault-free links, we show that an n-dimensional star network can tolerate up to 2n− 7 link faults, and be strongly (fault-free) Hamiltonian laceable, where n ≥ 4. In other words, we can embed a fault-free linear array of length n!−1 (n!−2) in an n-dimensional star network with up to 2n−7 link faults, if the two end nodes belong to different partite sets (the same partite set). The result is optimal with respect to the number of link faults tolerated. It is already known that under the random fault model, an n-dimensional star network can tolerate up to n− 3 faulty links and be strongly Hamiltonian laceable, for n ≥ 3. © 2008 Elsevier B.V. All rights reserved.	charge-coupled device;distributed computing;fault (technology);fault model;hamiltonian (quantum mechanics);interaction picture;interconnection;longest path problem;magma;star network	Ping-Ying Tsai;Jung-Sheng Fu;Gen-Huey Chen	2009	Theor. Comput. Sci.	10.1016/j.tcs.2008.11.012	fault tolerance;combinatorics;star network;computer science;mathematics;distributed computing;algorithm	Theory	23.834506437249335	34.37640352586906	88132
480420aaa78437191e8883d816ecfd0f7df76689	space efficient list merging on a multiprocessor ring	multiprocessor interconnection networks;time complexity space efficient list merging multiprocessor ring large private memory processor memories symbolic polynomials sequential algorithm odd even merging;time complexity;lattice path;multiprocessor interconnection networks data structures;data structures;merging sorting polynomials microprocessors hypercubes computer science lattices prototypes graphics topology	Algorithms are considered for a multiprocessor ring consisting of a moderate number of processors each with a large private memory. The data in the lists is distributed equally among the processor memories with each processor holding a contiguous segment of the list. The process of merging the two lists is described in terms of lattice paths. Addition of symbolic polynomials is used as the prototypical merging application. Two algorithms are described for merging. Both move the list elements so that a standard sequential algorithm can be applied locally. The first shifts both lists simultaneously so as to achieve alignment. The second uses a variant of odd-even merging. The time complexity of the alignment operation is shown to be linear in the length of the smaller list.	central processing unit;multiprocessing;polynomial;sequential algorithm;time complexity	Arthur F. Dickinson;Ratan K. Guha	1990		10.1109/PARBSE.1990.77140	parallel computing;computer science;theoretical computer science;distributed computing	Metrics	13.021207276128532	33.13122730518042	88263
40b4b174de2acb658fa3bbff28e56c60f7d6d6ce	a systematic approach for designing concurrent error-detecting systolic arrays using redundancy	detection erreur concurrente;tolerancia falta;parallelisme;concurrent error detection;systolic array;parallelism;space time mapping;paralelismo;association espace temps;fault tolerance;tolerance faute;tableau systolique	Abstract   A systematic approach for designing systolic arrays with concurrent error detection (CED) capability using time and/or space redundancy is proposed. This approach is based on a new theory which relates CED and the generalized space-time mapping. Under a restriction that there is one generated (modified) variable in the systolic arrays, a simplified CED scheme is presented. That not only significantly reduces the hardware and time overheads but also has capability of error correction. As well, the resulting systolic array can be used to compute two problem instances simultaneously to achieve double throughput without extra cost.	redundancy (information theory);sensor	Chang Nian Zhang;Hon Fung Li;Rajagopalan Jayakumar	1993	Parallel Computing	10.1016/0167-8191(93)90062-P	fault tolerance;parallel computing;real-time computing;systolic array;computer science;algorithm	HPC	16.076238614212738	41.541636281207	88714
c7ef19a52d436b430b3c6ce3b929ebc7bf4c8860	pulsedsp - a signal processing oriented programmable architecture	processing element;red sistolica;traitement signal;eficacia sistema;field programmable gate array;architecture systeme;sistema informatico;performance systeme;systolic array;computer system;red puerta programable;system performance;reseau porte programmable;fpga implementation;systolic network;signal processing;fir filter;reseau systolique;arquitectura sistema;systeme informatique;system architecture;programmable networks;procesamiento senal;parallel processing	Classical digital signal conditioning algorithms, such as FIR filtering, involve many simple independent calculations repeated in a fixed order. This makes them particularly appropriate for implementa- tion using a parallel processing approach. The PulseDSP architecture is a programmable array developed to exploit this inherent parallelism. The architecture is a systolic array of simple processing elements. Data is passed between processing elements using a programmable network of serial data channels. Each processing element performs basic fixed operations on the data before passing it the next element. Algorithms are implemented by structurally describing the calculation as a signal flow then mapping it to the array on a one-to-one, operation-to-processor ba- sis. This approach can provide a significant improvement in perform- ance over standard DSP processor and FPGA implementations, par- ticularly when large datawidths are required.	signal processing	Gareth Jones	1999		10.1007/978-3-540-48302-1_29	embedded system;parallel processing;parallel computing;systolic array;computer science;finite impulse response;signal processing;computer performance;field-programmable gate array;systems architecture	Arch	13.379079222389741	37.357726218923865	88840
04889416dcb8c90e05bbb9717c5a2a8ecd62f546	mpeg-4 aac audio decoding on a 24-bit fixed-point dual-dsp architecture	digital signal processing;mpeg 4 audio coding;mpeg 4 aac audio decoding;speech synthesis;decoding;iso standards;speech coding;code standards;huffman codes;mpeg 4 standard decoding audio coding digital signal processing iec standards iso standards digital audio broadcasting speech synthesis hdtv speech coding;fixed point;24 bit mpeg 4 aac audio decoding fixed point dual dsp architecture audio standard mpeg 4 audio coding advanced audio coding algorithm;audio coding;advanced audio coding;mpeg 4 standard;iec standards;fixed point dual dsp architecture;hdtv;digital audio broadcasting;fixed point arithmetic;cost effectiveness;24 bit;digital signal processing chips;advanced audio coding algorithm;huffman codes code standards audio coding decoding fixed point arithmetic digital signal processing chips;audio standard	MPEG-4 audio coding is a recent audio standard that integrates a wide range of different types of audio coding, and, thus, applies to almost every audio application today. A very important subpart of MPEG-4 is the Advanced Audio Coding (AAC) algorithm, which has already distinguished itself in industry due to its high audio quality. This paper describes implementation of an AAC decoder on a novel dual-DSP architecture utilizing cost-effective 24-bit fixed-point arithmetic.		V. Mesarovic;N. D. Hemkumr;M. Dokic	2000		10.1109/ISCAS.2000.856158	adaptive multi-rate audio codec;joint;real-time computing;speech recognition;cost-effectiveness analysis;mpeg-4 part 3;digital audio;bandwidth extension;advanced audio coding;computer science;digital signal processing;speech coding;fixed point;multimedia;fixed-point arithmetic;speech synthesis;mpeg-4;huffman coding	NLP	11.069309914094184	41.75394170209032	88873
079a91dc7efd35d21c8e1b8bb2c1f2bf0df71d31	hardware implementation of a stereo co-processor in a medium-scale field programmable gate array	programmable circuit;field programmable gate array;8 bit processor;vhdl language;stereo co processor;bit map;calculateur embarque;integrated circuit;procesador 8 bits;circuit programmable;implementation;hardware description languages;circuito integrado;stereo depth detection;red puerta programable;coprocessor;hardware co processor;reseau porte programmable;coprocessors;system on a chip;processeur 8 bits;algorithme;system on a programmable chip;lenguaje vhdl;public domain software;algorithm;circuito programable;sistema sobre pastilla;system on chip coprocessors field programmable gate arrays hardware description languages public domain software;system on chip;coprocesador;sum of absolute differences algorithm;carte memoire image;coprocesseur;open source vhdl library component;boarded computer;horloge;systeme sur puce;field programmable gate arrays;implementacion;clock;hardware implementation;calculador embarque;langage vhdl;reloj;circuit integre;medium scale field programmable gate array;system on a programmable chip stereo co processor medium scale field programmable gate array hardware co processor stereo depth detection sum of absolute differences algorithm open source vhdl library component;algoritmo	The design of a hardware co-processor for stereo depth detection, based on a parallel implementation of the Sum of Absolute Differences algorithm, is presented. Model-based designs are followed, and a parameterisable open source VHDL library component appropriate for integration within a system-on-aprogrammable chip is created. We target a field programmable gate array board featuring external memory and other peripheral components and implement the control path with a Nios II embedded processor clocked at 100 MHz. The hardware co-processor produces dense 8-bit disparity maps of 320 240 pixels at a rate of 25 Mpixels/s and can expand the disparity range from 32 to 64 pixels with appropriate memory techniques. Essential resources can be as low as 16 000 logic elements, whereas by migrating to more complex devices the design can easily grow to support better results.	8-bit;algorithm;binocular disparity;clock rate;coprocessor;depth perception;embedded system;field-programmable gate array;nios embedded processor;open-source software;peripheral;pixel;sum of absolute differences;vhdl	John A. Kalomiros;John N. Lygouras	2008	IET Computers & Digital Techniques	10.1049/iet-cdt:20070147	system on a chip;embedded system;electronic engineering;computer hardware;computer science;operating system;coprocessor;field-programmable gate array	EDA	14.040992270011452	40.68163885779592	89105
d2785b6e159ff297a5aaba3a2111049004d6ac40	an efficient hardware-based higher radix floating point mac design	carry lookahead adder;digital signal processing;carry save adder;high performance circuits;wallace tree multiplier;multiply accumulate circuit;floating point arithmetic;fused multiply add	This article proposes an effective way of implementing a multiply accumulate circuit (MAC) for high-speed floating point arithmetic operations. The real-world applications related to digital signal processing and the like demand high-performance computation with greater accuracy. In general, digital signals are represented as a sequence of signed/unsigned fixed/floating point numbers. The final result of a MAC operation can be computed by feeding the mantissa of the previous MAC result as one of the partial products to a Wallace tree multiplier or Braun multiplier. Thus, the separate accumulation circuit can be avoided by keeping the circuit depth still within the bounds of the Wallace tree multiplier, namely O(log2 n), or Braun multiplier, namely O(n). In this article, three kinds of floating point MACs are proposed. The experimental results show 48.54% of improvement in worst path delay achieved by the proposed floating point MAC using a radix-2 Wallace structure compared with a conventional floating point MAC without a pipeline using a 45nm technology library. The same proposed design gives 39.92% of improvement in worst path delay without a pipeline using a radix-4 Braun structure as compared with a conventional design. In this article, a radix-32 Q32.32-format-based floating point MAC is proposed using a Wallace tree/Braun multiplier. Also this article discusses the msb prediction problem and its solution in floating point arithmetic that is not available in modern fused multiply-add designs. The performance results show comparisons between the proposed floating point MAC with various floating point MAC designs for radix-2,-4,-8, and -16. The proposed design has lesser depth than a conventional floating point MAC as well as a lower area requirement than other ways of floating point MAC implementation, both with/without a pipeline.	computation;digital signal processing;mike lesser;pipeline (computing);significand;tree accumulation;wallace tree	M. Mohamed Asan Basiri;Sk. Noor Mahammad	2014	ACM Trans. Design Autom. Electr. Syst.	10.1145/2667224	minifloat;parallel computing;computer hardware;computer science;floating point;operating system;digital signal processing;multiply–accumulate operation;carry-save adder;algorithm	EDA	12.427780619373548	45.17279088319404	89129
8c38e9757bef60af55cd5c5ce8ab0d65b6fcfa1a	increment/decrement/2's complement/priority encoder circuit for varying operand lengths	codecs;digital arithmetic codecs;adders delay zinc logic gates arrays signal processing algorithms;multi precision;sub word parallelism;increment decrement;array signal processing;arrays;logic gates;adders;2 s compelement;word length 32 bit increment operation decrement operation twos complement operation priority encoder circuit multifunctional architecture word length 8 bit word length 16 bit;digital arithmetic;zinc;2 s compelement multi precision sub word parallelism reconfigurable increment decrement priority encoder;priority encoder;signal processing algorithms;logic gate;reconfigurable	Algorithms based Media applications operate on operands of varying data lengths. Although much work has been done in designing adder and multiplier architectures which operate on varying data lengths, there has been little work on implementing other operations like increment/decrement, 2's complement etc. This paper presents an architecture which can perform increment/decrement/2's complement/priority-encode operations on varying data lengths. A 32-bit implementation of the proposed multifunctional architecture is presented, which can operate on four 8-bit operands, two 16-bit operands or one 32-bit operand.	16-bit;32-bit;8-bit;adder (electronics);algorithm;encode;feature selection;increment and decrement operators;multi-function printer;operand;priority encoder;two's complement	P. Phaneendra SaiPhaneendra;Chetan Vudadha;Syed Ershad Ahmed;Sreehari Veeramachaneni;N. Moorthy Muthukrishnan;M. B. Srinivas	2011	2011 11th International Symposium on Communications & Information Technologies (ISCIT)	10.1109/ISCIT.2011.6092152	arithmetic;parallel computing;real-time computing;computer science	Arch	11.691358422093275	44.54696752475814	89444
d5d3e88f96fea2fbc6517548d04e86e2c08cdbf8	analysis and optimization of product-accumulation section for efficient implementation of fir filters	complexity theory;optimisation adders fir filters;finite impulse response filters;power delay performance finite impulse response filter fir filters product accumulation section analysis product accumulation section optimization multiple constant multiplication block critical path delay increment structural adders coefficient multiplication area delay performance;adders;finite impulse response filters adders delays optimization propagation delay complexity theory;propagation delay;optimization;retiming finite impulse response fir high speed implementation multiple constant multiplication mcm product accumulation section;delays	Most of the research on the implementation of finite impulse response (FIR) filter so far focuses on the optimization of the multiple constant multiplication (MCM) block. But it is observed that the product-accumulation section often contributes the major part of the critical path, such that the timing optimization of MCM block does not impact significantly on the overall speedup of the FIR filters. In this paper, a precise analysis and optimization of critical path for transposed direct from (TDF) FIR filters is proposed. The delay increment introduced by structural adders is estimated by comparing the delay of a tap and the corresponding delay of the coefficient multiplication only. Based on that, a novel implementation of the product-accumulation section of FIR filters is proposed by retiming the existing delays into the structural adders. It is also shown that the structural adders can be integrated with the MCM block and retimed together for further reduction of critical path. By using the proposed method, the increment of delay caused by the structural adders can be either completely eliminated or significantly reduced. Experimental results show that the critical path delay can be significantly reduced at the cost of very small area overhead. The overall area-delay performance and power-delay performance of the proposed method are superior to the existing methods.	amdahl's law;coefficient;critical path method;finite impulse response;mathematical optimization;multi-chip module;overhead (computing);retiming;speedup;tree accumulation;trusted data format	Xin Lou;Ya Jun Yu;Pramod Kumar Meher	2016	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2016.2587105	propagation delay;mathematical optimization;electronic engineering;real-time computing;computer science;mathematics;adder	EDA	13.656310730618598	46.23750181684654	89819
61a1ed4995c5d19ebbc24663f24fc7ee1e3c84ae	reliability analysis of multistate phased-mission systems with unordered and ordered states	unordered states;analytical models;ordered states;reliability;binary decision diagram reliability analysis multistate phased mission systems unordered states ordered states ms pms dynamic system configuration failure criteria state transition behavior s dependence monolithic markov models state explosion problem system structure function representation dependence behaviors multistate distributed computing system;ms pms;integrable model;boolean functions;dependence behaviors;computer model;decision diagram;s dependence;phased mission system;boolean function;dynamic system;binary decision diagram bdd;dynamic system configuration;reliability theory;structure function;system structure function representation;servers;computational modeling;markov model;binary decision diagrams;multistate system;reliability theory binary decision diagrams markov processes;distributed computing system;multistate phased mission systems;monolithic markov models;data structures;failure criteria;markov process;state transition behavior;state explosion problem;data structures boolean functions reliability analytical models computational modeling servers markov processes;reliability analysis;markov processes;state explosion;multistate distributed computing system;multistate multivalued decision diagram mmdd;data structure;unordered states binary decision diagram bdd multistate multivalued decision diagram mmdd multistate system phased mission system;state transition;analytical model;binary decision diagram	Multistate phased-mission systems (MS-PMS) are multistate systems subject to multiple, consecutive, and nonoverlapping phases of operation. The challenges in analyzing MS-PMS reside in the dynamic system configuration, failure criteria, and component state transition behavior in different phases, as well as the s-dependence across different phases and among different states of a given component. Existing methods for the reliability analysis of MS-PMS are either based on monolithic Markov models that suffer from the well-known state explosion problem, or using a hierarchical strategy that can only handle ordered component states. This paper presents integrated modeling approaches for the reliability analysis of repairable MS-PMS with both ordered and unordered component states. The proposed methods integrate efficient decision diagram models for representing the system structure function and incorporating the unordered/ordered component states at the system level, and Markov models for describing dependence and transition behaviors at the component level. The application and advantages of the proposed approaches are illustrated through a case study in which the reliability for a sequence of tasks in a multistate distributed computing system is analyzed.	algorithm;analysis of algorithms;best, worst and average case;bottom-up parsing;computation;computer programming;distributed computing;dynamical system;failure cause;graphical user interface;in-phase and quadrature components;influence diagram;ms-dos;markov chain;markov model;media foundation;programming tool;reliability engineering;state space;state transition table;system configuration;time complexity;whole earth 'lectronic link	Akhilesh Shrestha;Liudong Xing;Yuan-Shun Dai	2011	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2010.2089513	data structure;computer science;theoretical computer science;markov process;boolean function;statistics	SE	24.20183817843587	45.42275524368655	89892
32b532f84bee5ee6aae9768ff98feed590eff55e	discovery through gossip	stochastic process;resource discovery;peer to peer network;distributed networks;p2p;gossip based algorithm;information discovery;social network;random process;social networks;probabilistic analysis;directed graph;overlay network;mutual information;p2p networks;peer to peer;distributed algorithm;lower bound;dynamic networks	"""We study randomized gossip-based processes in dynamic networks that are motivated by information discovery in large-scale distributed networks such as peer-to-peer and social networks. A well-studied problem in peer-to-peer networks is resource discovery, where the goal for nodes (hosts with IP addresses) is to discover the IP addresses of all other hosts. Also, some of the recent work on self-stabilization algorithms for P2P/overlay networks proceed via discovery of the complete network. In social networks, nodes (people) discover new nodes through exchanging contacts with their neighbors (friends). In both cases the discovery of new nodes changes the underlying network --- new edges are added to the network --- and the process continues in the changed network. Rigorously analyzing such dynamic (stochastic) processes in a continuously changing topology remains a challenging problem with obvious applications.  This paper studies and analyzes two natural gossip-based discovery processes. In the push discovery or triangulation process, each node repeatedly chooses two random neighbors and connects them (i.e., """"pushes"""" their mutual information to each other). In the pull discovery process or the {\em two-hop walk}, each node repeatedly requests or """"pulls"""" a random contact from a random neighbor and connects itself to this two-hop neighbor. Both processes are lightweight in the sense that the amortized work done per node is constant per round, local, and naturally robust due to the inherent randomized nature of gossip.  Our main result is an almost-tight analysis of the time taken for these two randomized processes to converge. We show that in any undirected n-node graph both processes take O(n log2 n) rounds to connect every node to all other nodes with high probability, whereas Ω(n log n) is a lower bound. We also study the two-hop walk in directed graphs, and show that it takes O(n2 log n) time with high probability, and that the worst-case bound is tight for arbitrary directed graphs, whereas Ω(n2) is a lower bound for strongly connected directed graphs. A key technical challenge that we overcome in our work is the analysis of a randomized process that itself results in a constantly changing network leading to complicated dependencies in every round. We discuss implications of our results and their analysis to discovery problems in P2P networks as well as to evolution in social networks."""	amortized analysis;best, worst and average case;binary logarithm;converge;directed graph;gossip protocol;graph (discrete mathematics);information discovery;mutual information;overlay network;peer-to-peer;randomized algorithm;self-stabilization;social network;stochastic process;strongly connected component;with high probability	Bernhard Haeupler;Gopal Pandurangan;David Peleg;Rajmohan Rajaraman;Zhifeng Sun	2012		10.1145/2312005.2312031	stochastic process;evolving networks;computer science;artificial intelligence;machine learning;distributed computing;statistics;social network	Theory	19.184116305406892	34.83106987348613	90335
8c798a91be1965e5e06b846d2af12bd5df8da7a6	multilevel parallelization on the cell/b.e. for a motion jpeg 2000 encoding server	arithmetic coding;process capability;motion jpeg 2000;real time;cell broadband engine;performance improvement;wavelet transform;motion jpeg;cluster system;next generation;parallelization;processor element;high performance	The Cell Broadband Engine (Cell/B.E.) is a novel multi-core microprocessor designed to provide high-performance processing capabilities for a wide range of applications. In this paper, we describe the world's first JPEG 2000 and Motion JPEG 2000 encoder on Cell/B.E. Novel parallelization techniques for a Motion JPEG 2000 encoder that unleash the performance of the Cell/B.E. are proposed. Our Motion JPEG 2000 encoder consists of multiple video frame encoding servers on a cluster system for high-level parallelization. Each video frame encoding server runs on a heterogeneous multi-core Cell/B.E. processor, and utilizes its 8 Synergistic Processor Elements (SPEs) for low-level parallelization of the time consuming parts of the JPEG 2000 encoding process, such as the wavelet transform, the bit modeling, and the arithmetic coding. The effectiveness of high-level parallelization by the cluster system is also described, not only for the parallel encoding, but also for scalable performance improvement for real-time encoding and future enhancements. We developed all of the code from scratch for effective multilevel parallelization. Our results show that the Cell/B.E. is extremely efficient for this workload compared with commercially available processors, and thus we conclude that the Cell/B.E. is quite suitable for encoding next generation large pixel formats, such as 4K/2K-Digital Cinema.	arithmetic coding;cell (microprocessor);central processing unit;cinema 4d;encoder;high- and low-level;microprocessor;motion jpeg 2000;multi-core processor;parallel computing;pixel;real-time transcription;scalability;server (computing);synergy;wavelet transform	Hidemasa Muta;Munehiro Doi;Hiroki Nakano;Yumi Mori	2007		10.1145/1291233.1291442	arithmetic coding;lossless jpeg;parallel computing;real-time computing;process capability;computer hardware;computer science;operating system;jpeg 2000;world wide web;motion jpeg;wavelet transform;automatic parallelization	EDA	11.076233125961245	39.682052696372864	90516
cf0547f03ead7ce36f24a9f055f87d81533b21ac	bandwidth-efficient cache-based motion compensation architecture with dram-friendly data access control	cache storage;random access memory;cache;access control scheme;motion compensation;decoding;h 264 avc;telecommunication control;data reuse;advanced video coding;loading;code standards;access control scheme h 264 avc decoder system advanced video coding cache based motion compensation dram friendly data mapping;motion compensated;video coding;automatic voltage control;motion compensation access control bandwidth random access memory automatic voltage control delay decoding interpolation iec standards iso standards;h 264 avc cache motion compensation cache based motion compensation;data access;reference data;video coding cache storage code standards decoding dram chips motion compensation telecommunication control;bandwidth;cache based motion compensation;h 264 avc decoder system;access control;dram friendly data mapping;dram chips	For H.264/AVC decoder system, the motion compensation bandwidth comes from two parts, the reference data loading bandwidth and the equivalent bandwidth from DRAM access overhead latency. In this paper, a bandwidth-efficient cache-based MC architecture is proposed. It exploits both intra-MB and inter-MB data reuse and reduce up to 46% MC bandwidth compared to conventional scheme. To reduce the equivalent bandwidth from DRAM access overhead latency, the DRAM-friendly data mapping and access control scheme are proposed. They can reduce averagely 89.8% of equivalent DRAM access overhead bandwidth. The average MC burst length can be improved to 9.59 words/burst. The total bandwidth reduction can be up to 32∼71% compared to previous works.	access control;cpu cache;data access;dynamic random-access memory;h.264/mpeg-4 avc;motion compensation;overhead (computing)	Tzu-Der Chuang;Lo-Mei Chang;Tsai-Wei Chiu;Yi-Hau Chen;Liang-Gee Chen	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4960007	data access;embedded system;parallel computing;real-time computing;reference data;cache;computer science;access control;motion compensation;bandwidth;statistics;dynamic bandwidth allocation	Arch	12.726324419776248	39.669565942244255	90694
aeb76f4a293293f881f90f76a2d428921ee94189	on the design of a unidirectional systolic array for key enumeration	parallel calculus;red sistolica;enumeration;algorithmique;systolic arrays very large scale integration pipeline processing computational geometry councils computer science nearest neighbor searches hardware algorithm design and analysis sorting;enumeracion;geometrie algorithmique;unidirectional systolic array;computational geometry;systolic array;dependence;dependance;computational geometry design unidirectional systolic array key enumeration unidirectional data flow maximum data pipelining rate closest neighbor problems;grafico;cellular arrays;vecino mas cercano;probleme combinatoire;calculo paralelo;problema combinatorio;closest neighbor problems;systolic network;algorithmics;algoritmica;maximum data pipelining rate;graph;graphe;geometria algoritmica;unidirectional data flow;reseau systolique;computational geometry cellular arrays;plus proche voisin;design;nearest neighbour;combinatory problem;data flow;key enumeration;calcul parallele;dependencia	A systolic array for enumerating keys in n keys in 3n-1 time steps is introduced. This array has unidirectional data flow and achieves the maximum data pipelining rate. Modifications of the array for solving the closest-neighbor problems in computational geometry are presented. >		Ferng-Ching Lin;Kung Chen	1990	IEEE Trans. Computers	10.1109/12.45213	design;parallel computing;systolic array;computational geometry;computer science;theoretical computer science;mathematics;graph;enumeration;algorithmics;algorithm	EDA	13.363461176515763	34.38939056098501	90729
a1f38d03044c9e4e720830ad7e8dd652b90a653f	(t, k)-diagnosability for regular networks	topology;multiprocessor systems;program processors testing data mining sequential diagnosis topology fault diagnosis computational modeling;multiprocessing systems fault diagnosis;testing;diagnosability;data mining;matching composition;pmc model;computational modeling;t k diagnosability faulty processors multiprocessor system system level diagnosis matching composition networks;torus;k diagnosability;t;faulty processors;sequential diagnosis;multiprocessing systems;networked systems;k diagnosis;diagnosis;multiprocessor system;diagnosis algorithm;program processors;matching composition networks;network;fault diagnosis;torus diagnosability diagnosis algorithm fault diagnosis matching composition network pmc model system level diagnosis t k diagnosis;system level diagnosis	System-level diagnosis aims to identify faulty processors in a multiprocessor system by means of analyzing the test results among the processors. (t, k)-Diagnosis, which is one of the most important system-level diagnosis strategies, requires at least k faulty processors identified in each iteration provided there are at most t faulty processors, where t ≥ k. In this paper, a new (t, k)-diagnosis algorithm for regular networks is proposed. Experimental results show that the proposed algorithm has larger values of t for tori and matching composition networks, in comparison with previous (t, k)-diagnosis algorithms.	algorithm;central processing unit;iteration;multiprocessing	Guey-Yun Chang	2010	IEEE Transactions on Computers	10.1109/TC.2010.16	parallel computing;real-time computing;computer science;torus;distributed computing;software testing;computational model	EDA	23.79705923602035	44.23286926080552	90787
82ceb8151bcba879f84b72f8b354701e37508d01	computing hough transforms on hypercube multicomputers	complexite;hypercube;mimd and simd hypercube multicomputers;image processing;multiprocessor;efficient algorithm;complejidad;procesamiento imagen;complexity;calculateur simd;transformacion hough;ncube 7;traitement image;algorithme;algorithm;calculateur mimd;simd computer;pattern recognition;hough transform;transformation hough;reconnaissance forme;reconocimiento patron;multiprocesador;mimd computer;algoritmo;multiprocesseur;hipercubo	Efficient algorithms to compute the Hough transform on MIMD and SIMD hypercube multicomputer are developed. Our algorithms can compute p angles of the Hough transform of an N × N image, p ≤ N, in 0(p + log N) time on both MIMD and SIMD hypercubes. These algorithms require 0(N 2) processors. We also consider the computation of the Hough transform on MIMD hypercubes with a fixed number of processors. Experimental results on an NCUBE/7 hypercube are presented.	algorithm;central processing unit;computation;hough transform;mimd;parallel computing;simd	Sanjay Ranka;Sartaj Sahni	1990	The Journal of Supercomputing	10.1007/BF00127879	hough transform;computer architecture;parallel computing;complexity;multiprocessing;image processing;computer science;theoretical computer science;algorithm;hypercube	Theory	11.799410727409287	35.216622086608574	90864
9959d6f907dfbb2bcd9d3ba6489da6ab5d872969	high-speed multiplier design using multi-input counter and compressor circuits	multi input counter;digital signal processing;cmos integrated circuits;parallel multipliers;cmos technology;multiplying circuits;reduced instruction set computing;multiplying circuits cmos integrated circuits digital arithmetic;7 3 parallel counter;counting circuits signal processing algorithms adders encoding hardware cmos technology added delay digital signal processing throughput reduced instruction set computing;4 2 compressor multiplier design multi input counter 7 3 compressor circuits cmos technology parallel multipliers gates;counting circuits;7 3 compressor circuits;gates;4 2 compressor;adders;digital arithmetic;signal processing algorithms;encoding;high speed;added delay;multiplier design;throughput;hardware	Multiplication represents one of the major bottlenecks in most digital processing systems. Depending on the wordsize, several partial products are added to evaluate the product. The well-known shift-and-add algorithm uses minimal hardware but has unacceptable performance for most applications. Several parallel fast multiplication schemes have been suggested using several levels of blocks containing full adders. This paper presents the design of a fast multiplier implemented using either (7,3) parallel counter or (7:3) compressor circuits f o r implementation in CMOS technology, The resulting 16 by 16-bit multiplier has less delay than conventional fast multipliers, although the gate count is about 10% higher.	16-bit;adder (electronics);bottleneck (software);cmos;digital data;gate count;multiplication algorithm;whole earth 'lectronic link	Mayur Mehta;Vijay Parmar;Earl E. Swartzlander	1991		10.1109/ARITH.1991.145532	parallel computing;computer hardware;computer science;operating system;cmos	EDA	12.379738922614488	44.26174148999765	91385
954bdc0ecc4117621a1546584126265ac8db0048	some combinatorial aspects of parallel algorithm design for matrix multiplication	cylindrical configuration;red sistolica;algoritmo paralelo;note;reseau 2 dimensions;parallel algorithm;parallel algorithms combinatorial mathematics matrix algebra;producto matriz;sistema informatico;metodo combinatorio;systolic array;methode combinatoire;parallel algorithm design;computer system;cuadrado latino;two dimension network;configuracion cilindrica;matrix algebra;configuration cylindrique;cylindrical array;latin square;algorithme parallele;conception algorithme;combinatorial aspects;systolic network;two layered mesh array;reseau systolique;two dimensional arrays;2 layered mesh array;combinatorial method;matrix multiplication;systeme informatique;parallel algorithms algorithm design and analysis transmission line matrix methods systolic arrays iterative algorithms equations scheduling q measurement position measurement time measurement;produit matrice;combinatorial mathematics;transformation procedure;matrix product;carre latin;transformation procedure combinatorial aspects parallel algorithm matrix multiplication two dimensional arrays cylindrical array two layered mesh array latin square two layered mesh array;red 2 dimensiones;parallel algorithms	In this paper, some combinatorial characteristics of matrix multiplication on regular two-dimensional arrays are studied. From the studies, we are able to design many efficient varieties of the cylindrical array and the two-layered mesh array for matrix multiplication. To design a cylindrical array for matrix multiplication, a systematic design procedure is proposed. In this design procedure, Latin square (a special type of matrix) plays an important role. To design a two-layered mesh array, we find that there is a transformation procedure to transform a cylindrical array to a two-layered mesh array.	algorithm design;matrix multiplication;parallel algorithm	Jong-Chuang Tsay;Sy Yuan	1992	IEEE Trans. Computers	10.1109/12.127449	combinatorics;parallel computing;matrix multiplication;computer science;theoretical computer science;mathematics;parallel algorithm;algorithm	HPC	12.820564266541988	34.860918389923896	91893
dd4832da327d21978e945b86e72298476698c806	some remarks on distributed depth-first search	arbre graphe;distributed algorithms;graph theory;teoria grafo;tree graph;recherche arborescente;theorie graphe;analysis of algorithm;analysis of algorithms;community networks;depth first search;tree searching;arbol grafo;distributed algorithm;communication network	Depth-first search is a powerful technique that has been used in designing many efficient graph algorithms on the sequential computer model. In the parallel setting, Reif [7] showed that it is an inherently sequential technique. In the distributed setting, several algorithms have been reported, among them the best known ones are those of Cheung [2], Awerbuch [1], Cidon [3], Lakshmanan et al. [6], Sharma et al. [8,4] and Makki et al. [5]. Sharma et al. and Makki et al. achieve better bounds than the first four at the expense of longer message length and stronger network model. They use messages of length O (n), wheren is the number of nodes in the network, and require every node to be assigned a distinct identity and to know the identities of its neighboring nodes. The O (n) message length allows global information about the network, such as the list of all visited nodes, to be passed along with the depth-first search. With the availability of global information, the number of communicating messages transmitted between neighboring nodes is substantially reduced.	algorithm;computer simulation;depth-first search;graph theory;john reif;network model;numerical weather prediction	Yung H. Tsin	2002	Inf. Process. Lett.	10.1016/S0020-0190(01)00273-3	distributed algorithm;combinatorics;breadth-first search;computer science;analysis of algorithms;theoretical computer science;machine learning;mathematics;tree;telecommunications network	Crypto	17.632819477960904	33.41854111677335	91903
0ac3fd68b0e46d65dd2ba932d3207cf7c188d80a	a novel computational complexity and power reduction technique for h.264 intra prediction	hardware design languages;fpga h 264 intra prediction low power hardware implementation;complexity theory;psnr;xilinx xpower;prediction equation;video compression;prediction algorithms;tk7800 8360 electronics;fpga;bit rate;xilinx virtex ii fpga;intra prediction;video coding;standards development;low power;power reduction technique;energy consumption;computational complexity;pixel;verilog hdl;h 264;mathematical model;video coding computational complexity;h 264 intra prediction algorithm;power reduction;power consumption;field programmable gate arrays;tk7885 7895 computer engineering computer hardware;power demand;hardware implementation;xilinx xpower computational complexity power reduction technique h 264 intra prediction algorithm verilog hdl xilinx virtex ii fpga;computational complexity prediction algorithms video compression energy consumption equations psnr bit rate hardware design languages field programmable gate arrays standards development;hardware	H.264 intra prediction algorithm has a very high computational complexity. This paper proposes a novel technique for reducing the amount of computations performed by H.264 intra prediction algorithm and therefore reducing the power consumption of H.264 intra prediction hardware significantly without any PSNR and bitrate loss. The proposed technique performs a small number of comparisons among neighboring pixels of the current block before the intra prediction process. If the neighboring pixels of the current block are equal, the prediction equations of H.264 intra prediction modes simplify significantly for this block. By exploiting the equality of the neighboring pixels, the proposed technique reduces the amount of computations performed by 4times4 luminance, 16times16 luminance, and 8times8 chrominance prediction modes up to 60%, 28%, and 68% respectively with a small comparison overhead. We also implemented an efficient 4times4 intra prediction hardware including the proposed technique using Verilog HDL. We quantified the impact of the proposed technique on the power consumption of this hardware on a Xilinx Virtex II FPGA using Xilinx XPower, and it reduced the power consumption of this hardware up to 18.6%.	algorithm;computation;computational complexity theory;field-programmable gate array;h.264/mpeg-4 avc;hardware description language;intra-frame coding;overhead (computing);peak signal-to-noise ratio;pixel;verilog;virtex (fpga)	Mustafa Parlak;Yusuf Adibelli;Ilker Hamzaoglu	2008	IEEE Transactions on Consumer Electronics	10.1109/TCE.2008.4711266	electronic engineering;real-time computing;computer science;theoretical computer science;field-programmable gate array;statistics	HPC	12.81004053806164	40.94378354901593	91978
c316b2a118377596381d485280af5475b54533b0	hardware-efficient and highly reconfigurable 4- and 2-track fault-tolerant designs for mesh-connected arrays	reconfiguration;tolerancia falta;mesh systems;reconfiguracion;node covering;interconnection;deterministic fault tolerance;fault tolerant;colocacion cables;circuit vlsi;interseccion;m track designs;approche deterministe;deterministic approach;interconexion;fault tolerant system;vlsi circuit;câblage;fault tolerant multiprocessors;fault tolerance;interconnexion;vlsi wsi arrays;enfoque determinista;sistema tolerando faltas;systeme tolerant les pannes;procesador;conmutador;computer hardware;circuito vlsi;wiring;processeur;intersection;average fault tolerance;materiel informatique;material informatica;tolerance faute;processor;commutateur;selector switch	Abstract   We consider  m -track models for constructing fault-tolerant (FT) mesh systems which have one primary and  m  spare tracks per row and column, switches at the intersection of these tracks, and spare processors at the boundaries. A faulty system is reconfigured by finding for each fault  u  a  reconfiguration path  from the fault to a spare in which, starting from the fault  u , a processor is replaced or “covered” by the nearest “available” succeeding processor on the path—a processor on the path is  not available  if it is faulty or is used as a “cover” on some other reconfiguration path. In previous work, a 1-track design that can support any set of node-disjoint straight reconfiguration paths, and a more reliable 3-track design that can support any set of node-disjoint rectilinear reconfiguration paths have been proposed. In this research note, we present: (1) A fundamental result regarding the universality of simple “one-to-one switches” in  m -track 2-D mesh designs in terms of their reconfigurabilities. (2) A 4-track mesh design that can support any set of edge-disjoint (a much less restrictive criterion than node-disjointness) rectilinear reconfiguration paths, and that has 34% less switching overhead and significantly higher, actually close-to-optimal, reconfigurability compared to the previously proposed 3-track design. (3) A new 2-track design derived from the above 4-track design that we show can support the same set of reconfiguration paths as the previous 3-track design but with 33% less wiring overhead. (4) Results on the deterministic fault tolerance capabilities (the number of faults guaranteed reconfigurable) of our 4- and 2-track designs, and the previously proposed 1- and 3-track designs.	fault tolerance	Nihar R. Mahapatra;Shantanu Dutt	2001	J. Parallel Distrib. Comput.	10.1006/jpdc.2001.1702	fault tolerance;parallel computing;real-time computing;computer science;distributed computing	HPC	13.683002945911838	35.91250584103857	91987
b8d60e3630131024c5df8f6109e17154fd92ff3c	a high performance hardware based rns-to-binary converter	residue number systems;consumer electronics;adders hardware logic gates consumer electronics digital signal processing computer architecture delays;adders;pipeline arithmetic;residue number systems adders consumer electronics pipeline arithmetic;reverse converter high performance hardware rns to binary converter residue number system digital signal processing functions consumer electronic devices binary reverse converter rns based signal processing system pipelined parallel prefix based modular adders	Due to its small size and parallel arithmetic operations, Residue Number System (RNS) based data is well suitable for implementing various digital signal processing functions that are commonly used in many consumer electronic devices nowadays. This paper presents the design of the RNS to Binary reverse converter which is typically the bottleneck in an RNS based signal processing system. Specifically, it describes a pipelined parallel prefix based modular adders which is used to implement the reverse converter for the {2k-1, 2k, 2k+1} moduli set based RNS system.	digital signal processing;residue number system;responsive neurostimulation device;type system	Karuppaiah Karthik;Nicholas C. H. Vun	2014	2014 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2014.6775947	embedded system;electronic engineering;parallel computing;computer science;adder	EDA	11.804183272580417	43.36353367232096	92148
e67f72bb4e32c789364134314957db480409bb03	a family of new efficient arrays for matrix multiplication	parallel calculus;tratamiento paralelo;red sistolica;processor cells regular iterative algorithm matrix multiplication arrays iteration vector conventional arrays;multiplying circuits cellular arrays iterative methods matrix algebra;algorithmique;multiplying circuits;traitement parallele;producto matriz;systolic arrays;regular iterative algorithm;procesador panel;matrix algebra;array processor;processor cells;iterative algorithm;cellular arrays;processeur tableau;iterative methods;calculo paralelo;matrices mathematics;systolic network;algorithmics;algoritmica;iteration vector;informatique theorique;calculabilite;conventional arrays;reseau systolique;parallel processing computers;algorithms;matrix multiplication;iterative solution;equations iterative algorithms systolic arrays process design algorithm design and analysis parallel processing missiles nasa contracts design methodology;produit matrice;multiplication;matrix multiplication arrays;calcul parallele;parallel processing;matrix product;calculability;calculabilidad;computer theory;informatica teorica	The authors present a regular iterative algorithm for matrix multiplication and show that several well-known matrix multiplication arrays are directly obtained from it, differing only in the choice of iteration vector. They then present a regular iterative algorithm for matrix multiplication using the S. Winograd method (1968) and show in detail how to derive one array from this algorithmic description. Other arrays in the same family can similarly be obtained for different choices of the iteration space. The new arrays compute the product of two matrices faster than available conventional arrays and use a smaller number of processor cells. >	matrix multiplication	H. V. Jagadish;Thomas Kailath	1989	IEEE Trans. Computers	10.1109/12.8739	parallel processing;discrete mathematics;parallel computing;multiplication algorithm;matrix multiplication;matrix chain multiplication;theoretical computer science;mathematics;iterative method;algorithmics;algorithm;algebra	Vision	12.916248097106417	35.27658766110306	92181
7b453be7a77f6461f72d7bc38b756540b990c6ee	feedback from nature: simple randomised distributed algorithms for maximal independent set selection and greedy colouring	maximum degree;neighbouring node;failure probability;collision detection;node signal	We propose distributed algorithms for two well-established problems that operate efficiently under extremely harsh conditions. Our algorithms achieve state-of-the-art performance in a simple and novel way. Our algorithm for maximal independent set selection operates on a network of identical anonymous processors. The processor at each node has no prior information about the network. At each time step, each node can only broadcast a single bit to all its neighbours, or remain silent. Each node can detect whether one or more neighbours have broadcast, but cannot tell how many of its neighbours have broadcast, or which ones. We build on recent work of Afek et al. (Science 331(6014):183–185, 2011) which was inspired by studying the development of a network of cells in the fruit fly. However we incorporate for the first time another important feature of the biological system: varying the probability value used at each node based on local feedback from neighbouring nodes. Given any n-node network, our algorithm achieves with high probability the optimal time complexity of $$O(\log n)$$ O ( log n ) rounds and the optimal expected message complexity of O(1) single-bit messages broadcast by each node. We also show that the previous approach, without feedback, cannot achieve better than $$\varOmega (\log ^2 n)$$ Ω ( log 2 n ) time complexity with high probability, whatever global scheme is used to choose the probabilities. Our algorithm for distributed greedy colouring works under similar harsh conditions: each identical node has no prior information about the network, can only broadcast a single message to all neighbours at each time step representing a desired colour, and can only detect whether at least one neighbour has broadcast each colour value. We show that with high probability our algorithm has a time complexity of $$O(\Delta +\log n)$$ O ( Δ + log n ) , where $$\Delta $$ Δ is the maximum degree of the network, and also has an expected message complexity of O(1) messages broadcast by each node.	biological system;central processing unit;comparison of archive formats;distributed algorithm;greedy algorithm;independent set (graph theory);maximal independent set;time complexity;universal quantification;with high probability	Peter Jeavons;Alex D. Scott;Lei Xu	2016	Distributed Computing	10.1007/s00446-016-0269-8	computer science;theoretical computer science;machine learning;distributed computing	Theory	18.65281265175858	34.3279276973301	92366
2c7269d3f9bedfa6c9e583cd2cccb0409d6e9040	unidirectional bit/byte error control	error detection and correction;error location;error detection codes;detection erreur;deteccion error;error correction codes;computer graphics;correction erreur;error analysis;computer architecture;asymmetric error;error correction;periodic structures;data visualization;unidirectional errors;error control;borne inferieure;bit error control;asymmetric errors;correccion error;error detection;computer errors;byte error control;lower bound;bit and byte errors;error correction error correction codes computer errors data visualization computer architecture computer graphics hardware periodic structures error analysis;error detection codes error detection error correction error correction codes;cota inferior;asymmetric errors unidirectional errors byte error control bit error control error location error detection and correction;hardware	[ I ] P. Budnik and D.J. Kuck, “The organization and use of parallel memories,” IEEE Trans. Computers, vol. 20, no. 12, pp. 1566-1569, Dec. 1971. [2] 8 . Chor, C.E. Leiserson, and R.L. Rivest, “An application of number theory to the organization of raster-graphics memory,” Ann. IEEE Symp. Foundation .f Computer Science, pp. 92-99, Chicago, 1982. [3] M. Dippe and J. Swensen, “An adaptive subdivision algorithm and parallel architecture for realistic image synthesis,” Computer Graphics, vol. 18, no. 3 , pp. 149-159, July 1984. [4] D.T. Harper I11 and J.R. Jump, “Vector access performance in parallel memories using a skewed storage scheme,” IEEE Trans. Computers, vol. 36, no. 12, pp. 1440-1449, Dec. 1987. [5] A.J.W. Hilton, “On double diagonal and cross Latin square,” J. London Math Soc., vol. 11, no. 6 , pp. 679-689, 1973. [6] A. Kaufman and R. Bakalash, “Memory and processing architecture for 3D voxel-based imagery,” IEEE Computer Graphics and Applications, vol. 8, no. 6. pp. 10-23, Nov. 1988. Also in Japanese, Nikkei Computer Gruphics, vol. 3, no. 30, pp. 148-160, Mar. 1989. [7] A. Kaufman, R. Bakalash, and D. Cohen, “Viewing and rendering processor for a volume visualization system,’’ Advances in Graphics HardUnidirectional Bit/Byte Error Control	algorithm;byte;computer graphics;computer science;error detection and correction;parallel computing;rendering (computer graphics);scientific visualization;source-to-source compiler;subdivision surface;voxel	Nitin H. Vaidya	1995	IEEE Trans. Computers	10.1109/12.381959	error detection and correction;speech recognition;computer science;theoretical computer science;data visualization;algorithm;statistics	Visualization	17.109062211387208	37.93057179327525	92632
0045169c2f0cfed10c22cce84a3512a848edb8af	an exact solution to the fitting problem in the application specific state machine device		In this paper the fitting problem for a new Application Specific State Machine Device, CY7C361, from Cypress Semiconductor is formulated and the solution is proposed. This fitting problem consists of mapping a netlist obtained from high-level synthesis into the chip’s physical resources. In general, a mapping (fitting) problem can be formulated as one of the labeled graph isomorphism between the netlist graph and the subgraph of the resources graph. However, the specific architecture-related constraints of the CY7C361 device cause the fitting problem to be generalized as a graph isomorphism problem with some additional mapping constraints and node multiplication (placing some nodes of the netlist graph in more than one node of the physical graph). Such formulation is quite general for a class of Complex Programmable Logic Device (CPLD) fitting problems, and has not been found in the literature. We implemented an exact, constraint-based, tree searching algorithm with several kinds of backtracking.	curve fitting;finite-state machine	Marek A. Perkowski;Malgorzata Chrzanowska-Jeske;Edmund Pierzchala;Alan J. Coppola	1994	Journal of Circuits, Systems, and Computers	10.1142/S0218126694000119	graph power;mathematical optimization;factor-critical graph;combinatorics;discrete mathematics;graph bandwidth;null graph;graph property;graph canonization;machine learning;graph automorphism;subgraph isomorphism problem;mathematics;voltage graph;distance-hereditary graph;induced subgraph isomorphism problem;maximum common subgraph isomorphism problem;complement graph;algorithm;strength of a graph	EDA	19.079186923784956	39.75881429077011	93055
b3f8962856ee5af6367b2a053cce5197e80088bc	efficient implementation of controlled operations for multivalued quantum logic	logic arrays;complexity theory;probability density function;multivalued logic quantum computing logic arrays circuit synthesis computer science arithmetic logic circuits;logic;multivalued quantum logic gate;logic circuits;operation;circuit complexity multivalued quantum logic gate quantum array single qudit hermitian operator single qudit controlled gate;data mining;circuit complexity;mathematical operators;quantum gates;arrays;quantum gates circuit complexity logic arrays mathematical operators multivalued logic;quantum logic;controlled;logic gates;efficient implementation;quantum mechanics;quantum;multivalued;quantum array;single qudit hermitian operator;arithmetic;logic controlled operation multivalued quantum;computer science;multivalued logic;quantum computing;single qudit controlled gate;circuit synthesis	This paper presents a new quantum array that can be used to control a single-qudit hermitian operator for an odd radix r ≫ 2 by n controls using Theta(n^log_2 r + 2) single-qudit controlled gates with one control and no ancilla qudits. This quantum array is more practical than existing quantum arrays of the same complexity because it does not require the use of small roots of the operation that is being implemented. Another quantum array is also presented that implements a single-qudit operator with n controls for any radix r ≫ 2 using ceiling(log_(r - 1) n) ancilla qudits and Theta(n^(log_(r - 1) 2 + 1)) single-qudit gates with one control.	ancilla bit;like button;quantum computing;quantum logic;qubit	David J. Rosenbaum;Marek A. Perkowski	2009	2009 39th International Symposium on Multiple-Valued Logic	10.1109/ISMVL.2009.27	discrete mathematics;logic gate;theoretical computer science;mathematics;algorithm;quantum mechanics	Arch	18.15136836507092	44.62704361045367	93143
a5854dc67eba7a03f4cc3206914e44a8e92ace0f	papa - packed arithmetic on a prefix adder for multimedia applications	logic design;digital signal processing chips multimedia systems integrated circuit design integrated circuit modelling logic design adders digital arithmetic;multimedia application;multimedia systems;chip;integrated circuit design;arithmetic adders computer architecture acceleration signal processing algorithms signal generators design engineering concurrent computing logic equations;integrated circuit modelling;adders;digital signal processing chips;digital arithmetic;packed arithmetic operations logical effort analysis adder architecture delays packed arithmetic on prefix adder multimedia applications papa parallel prefix adder design packed arithmetic computations packed add subtract with saturation packed rounded average packed absolute difference prefix adder cell logic equations unused don t care state driven carry signal selection sum sub word video processors multimedia orientated processor chips	This paper introduces PAPA: Packed Arithmetic on a Prefix Adder, a new approach to parallel prefix adder design that supports a wide variety of packed arithmetic computations, including packed add and subtract with saturation, packed rounded average, and packed absolute difference. The approach consists of altering the prefix adder cell logic equations to take advantage of a previously unused “don’t care” state. Logical Effort is employed to assess the delay of the new adder architecture by establishing the extra effort needed to select and drive the appropriate carry signal to the requisite sum sub-word. This adder will find applications in video processors and other multimedia-orientated processor chips that impl ement packed arithmetic operations. 1. Motivation Multimedia processor chips (and others) make much use of “packed” arithmetic operations in order to accelerate a variety of digital signal processing algorithms for consumer applications. In such arithmetic units, long wordlength numbers are optionally treated as several independent shorter wordlength numbers – for example, a 32-bit word may be treated as 2 separate 16-bit words or as 4 8-bit words. The main motivation for this mode of operation is to support SIMD processing with its associated advantages in the context of a conventional pipelined load-store processor architecture [1]. Moreover, a common arithmetic operation used in video processing is “absolute difference”, denoted A–B, and used widely in video motion estimation and prediction algorithms. Hence, a most valuable operation is a “packed absolute difference” operation, which returns the absolute differences of a several independent pairs of 8-bit pixel values simultaneously. Ordinarily, absolute differences are computed either by performing a subtraction operation followed by a separate “absolute value” operation, which returns the magnitude of a signed number, or by performing a comparison to order the operands followed by a subtraction in which the smaller operand is subtracted from the larger [2]. Instead of such two-step implementations, absolute differences can be obtained by computing both A–B and B–A, and using the signs of the two results to select the positive result [3]. However, this is wasteful and a better technique is sought. Recently, some authors have des cribed how absolute differences can be derived using a single prefix adder [4, 5], but have not extended this insight to packed arithmetic. Previously reProceedings of the IEEE International Conference on Application-Specific Systems, Architectures, and Processors (ASAP’02) 1063-6862/02 $17.00 © 2002 IEEE ported implementations of packed arithmetic prefix adders include [6, 7], but neither of these proposals is able to support packed late increment operations, vital for computing absolute difference and rounded average instructions. A second valuable arithmetic option for media applications is saturated arithmetic, in which overflows and underflows do not cause exceptions but rather return pre-defined “saturation constants” [8]. Such constants should ideally be incorporated with little or no performance overhead since the integer adder is typically on the critical path that defines a processor’s clock rate. This paper describes how a prefix adder can be altered straightforwardly to support packed absolute difference and rounded average instructions as well as packed saturated arithmetic, and further describes how Logical Effort was employed to assess its performance potential. 2. Packed arithmetic on a parallel prefix adder 2.1 Prefix tree cell logic The parallel prefix carry-lookahead adder is a popular VLSI design technique that accelerates an n-bit addition by means of a parallel prefix tree [9]. A block diagram of a prefix adder is illustrated in Figure 1, where the adder is seen to consist of three blocks: input bit propagate, generate, and not kill cells; the prefix tree; output sum cells. The input cells derive the bit propagate, generate, and not kill signals respectively according to: p(i) = a(i) ⊕ b(i) ____ (1a) g(i) = a(i) ∧ b(i) ____ (1b) ¬k(i) = a(i) ∨ b(i) ____ (1c)	16-bit;32-bit;8-bit;adder (electronics);algorithm;block cipher mode of operation;carry-lookahead adder;central processing unit;clock rate;computation;critical path method;diagram;digital signal processing;logical effort;motion estimation;operand;overhead (computing);parsing;pixel;prefix code;simd;saturation arithmetic;trie;very-large-scale integration;video processing	Neil Burgess	2002		10.1109/ASAP.2002.1030719	chip;embedded system;computer architecture;parallel computing;logic synthesis;computer science;theoretical computer science;saturation arithmetic;serial binary adder;carry-save adder;adder;integrated circuit design	Arch	12.693181022899527	41.3549595120555	93211
429ba12e3cb36e6f9e15bf70f9948643118eea63	architecture and implementation of an associative memory using sparse clustered networks	iterative decoding;neural networks;decoding;neural nets;hardware decoding field programmable gate arrays neurons iterative decoding computer architecture neural networks;computer architecture;neural nets content addressable storage;neurons;field programmable gate arrays;content addressable storage;neural network associative memory sparse clustered network;hardware	Associative memories are alternatives to indexed memories that when implemented in hardware can benefit many applications such as data mining. The classical neural network based methodology is impractical to implement since in order to increase the size of the memory, the number of information bits stored per memory bit (efficiency) approaches zero. In addition, the length of a message to be stored and retrieved needs to be the same size as the number of nodes in the network causing the total number of messages the network is capable of storing (diversity) to be limited. Recently, a novel algorithm based on sparse clustered neural networks has been proposed that achieves nearly optimal efficiency and large diversity. In this paper, a proof-of-concept hardware implementation of these networks is presented. The limitations and possible future research areas are discussed.	algorithm;application-specific integrated circuit;artificial neural network;content-addressable memory;data mining;electrical connection;embedded system;node (computer science);sparse matrix	Hooman Jarollahi;Naoya Onizawa;Vincent Gripon;Warren J. Gross	2012	2012 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2012.6271922	parallel computing;computer science;theoretical computer science;machine learning;time delay neural network;artificial neural network;field-programmable gate array	Arch	12.171911144789327	44.13874754214272	93303
3d4e61b5c258a903eaa3b7a98d9b8f73c8c985e8	brief announcement: fast scalable byzantine agreement in the full information model with a nonadaptive adversary	information model;byzantine agreement;gossiping;communication complexity;large scale;synchrony;asychrony;message passing;leader election;byzantine faults;distributed algorithm;message complexity	We address the problem of designing distributed algorithms for large scale networks that are robust to Byzantine faults. We consider a message passing, full information model: the adversary is malicious, controls a constant fraction of processors, and can view all messages in a round before sending out its own messages for that round. Furthermore, each corrupt processor may send an unlimited number of messages. The adversary is constrained to choose its corrupt processors at the start, without knowledge of the processors' private random bits, but is otherwise adaptive. To the authors' best knowledge, there have been no subexponential protocols in the asynchronous version of this model and no protocols that compute Byzantine agreement without all-to-all communication in this model even a model in which private channels or cryptography are assumed, unless corrupt processors' messages are limited. We announce a polylogarithmic time protocol in the asynchronous model which appeared in SODA 08 and was recently improved to a resilience of n/(3 + ε). We also give a polylogarithmic time protocol for Byzantine agreement using only Õ(n3/2) total bits of pairwise communication which succeeds with high probability. These results rest on our solution to the problem of selecting a small representative sample of processors (universe reduction). This work extends the authors' work on scalable almost everywhere agreement to everywhere agreement and is an unpublished manuscript.	adversary (cryptography);block code;broadcasting (networking);byzantine fault tolerance;central processing unit;cryptography;distributed algorithm;information model;message passing;polylogarithmic function;scalability;time complexity;with high probability	Valerie King;Jared Saia	2009		10.1145/1582716.1582778	gossip;distributed algorithm;message passing;information model;computer science;quantum byzantine agreement;theoretical computer science;leader election;communication complexity;distributed computing;byzantine fault tolerance;computer security	Theory	16.474398207617277	33.93730621642765	93380
d9a47a7572c0b823c84e287a5e66c16459ce827d	low-cost hardware architectures for mersenne modulo functional units		With technology scaling leading to reliability problems and a proliferation of hardware accelerators, there is a need for cost-effective techniques to detect errors in complex datapaths. Modulo (residue) arithmetic is useful for creating a shadow datapath to check the computation of an arithmetic datapath and involves three key steps: reduction of the inputs to modulo shadow inputs, computation with those shadow values, and checking the outputs for consistency with the shadow outputs. The focus of this paper is new gate-level architectures and algorithms to reduce the cost of modulo shadow datapaths. We introduce low-cost architectures for all four key functional units in a shadow datapath: (1) a modulo reduction algorithm that generates architectures consisting entirely of full-adder standard cells; (2) minimum-area modulo adder and subtractor architectures; (3) an array-based modulo multiplier design; and (4) a modulo equality comparator that handles the residue encoding produced by the above. We compare our functional units to the previous state-of-the-art approach, observing a 12.5% reduction in area and a 47.1% reduction in delay for a 32-bit mod-3 reducer; that our reducer costs, which tend to dominate shadow datapath costs, do not increase with larger modulo bases; and that for modulo-15 and above, all of our modulo functional units have better area and delay then their previous counterparts. We also demonstrate the practicality of our approach by designing a custom shadow datapath for error detection of a multiply accumulate functional unit, which has an area overhead of only 12% for a 32-bit main datapath and 2-bit modulo-3 shadow datapath.	32-bit;adder (electronics);algorithm;color depth;comparator;computation;datapath;error detection and correction;execution unit;fixed-point arithmetic;hardware acceleration;high- and low-level;high-level synthesis;image scaling;logic synthesis;modulo operation;multiply–accumulate operation;overhead (computing);residue number system;subtractor;wraparound (video games)	Keith A. Campbell;Chen-Hsuan Lin;Deming Chen	2018	2018 23rd Asia and South Pacific Design Automation Conference (ASP-DAC)	10.1109/ASPDAC.2018.8297388	error detection and correction;computer hardware;stochastic computing;subtractor;datapath;computer science;modulo;adder;multiplier (economics);shadow	EDA	12.144273706618176	45.94029062077705	93396
ba565e6c9ef023fe0e0d42950be1d6e21352f76a	computing boolean functions on anonymous networks	random graph;hypercube;fonction booleenne;algorithm complexity;symmetric function;fonction polynomiale;complejidad algoritmo;funcion simetrica;grafo aleatorio;fonction symetrique;boolean function;graphe aleatoire;reseau;calculo automatico;computing;red;calcul automatique;complexite algorithme;funcion booliana;grafo regular;graphe regulier;funcion polinomial;polynomial function;network;regular graph;hipercubo	We study the bit-complexity of computing Boolean functions on anonymous networks. Let N be the number of nodes, δ the diameter, and d the maximal node degree of the network. For arbitrary, anonymous networks we give a general algorithm of polynomial bit complexity O(N3 · δ · d2 · log N) for computing any Boolean function which is computable on the network. This improves upon the previous best known algorithm, which was of exponential bit complexity O(dN2). For symmetric functions on arbitrary networks we give an algorithm with bit complexity O(N3· δ · d2 · log2N). This same algorithm is shown to have even lower bit complexity for a number of specific networks, for example tori, hypercubes, and random regular graphs. We also consider the class of distance regular unlabeled networks and show that on such networks symmetric functions can be computed efficiently in O(N · δ · d · log N) bits.		Evangelos Kranakis;Danny Krizanc;Jacob van den Berg	1994	Inf. Comput.	10.1006/inco.1994.1086	random graph;combinatorics;computing;discrete mathematics;regular graph;mathematics;boolean function;algorithm;hypercube;symmetric function	Logic	21.585804050469285	34.904573864881485	94155
f06bcb3b29072a7376fc5faf74e01cf379313511	connectivity in random forests and credit networks	liquidity in credit networks;uniformly random forests;edge expansion;markov chains	Recent work has highlighted credit networks as an effective mechanism for modeling trust in a network: agents issue their own currency and trust each other for a certain amount of each other’s currency, allowing two nodes to transact if there is a chain of sufficient residual trust between them. Under a natural model of repeated transactions, the probability that two agents can successfully transact in a credit network (i.e. the liquidity between these two agents) is the same as the probability that they are connected to each other in a uniformly random forest of the network. Motivated by this connection, we define the RFconnectivity between a pair of nodes in a graph G as the probability that the two nodes belong to the same connected component in a uniformly random forest of G. Our first result is that for an arbitrary subset S of nodes in G, the average RF-connectivity between pairs of nodes in S is at least 1− 2/h(GS), where h(GS) is the edge expansion of the subgraph GS induced by S. Informally, this implies that a well-connected “community” of nodes S in a credit network will have high liquidity among themselves, regardless of the structure of the remaining network. We extend this result to show that in fact every node in S has good average RFconnectivity to other nodes in S whenever S has good edge expansion. We also show that our results are nearly tight by proving an upper bound on the liquidity of regular graphs. For our motivating application, it is important that we relate the average RF-connectivity in S to the expansion inside S and not merely to expansion of G since we would like to assert that a well-connected community has high liquidity even if the graph as a whole is not well-connected. This naturally leads to a monotonicity conjecture: the RFconnectivity of two nodes can not decrease when a new edge is added to G. We show that the monotonicity conjecture is equivalent to showing negative correlation between inclusion of any two edges in a random forest, a long-standing open problem. Our result about the average RF-connectivity of nodes in S may be viewed as establishing a weak version of the monotonicity conjecture.	connected component (graph theory);radio frequency;random forest;roland gs	Ashish Goel;Sanjeev Khanna;Sharath Raghvendra;Hongyang Zhang	2015		10.1137/1.9781611973730.135	markov chain;mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	19.615357816578623	34.82681887726515	94261
1e0140be26e78d531ea1ca4498564d6b06c33f36	efficient approaches for designing reversible binary coded decimal adders	calcul optique;carry logic;circuito aritmetico;evaluation performance;adder;performance evaluation;integrated circuit;gate complexity;implementation;evaluacion prestacion;circuit binaire;circuito binario;adicionador;optical computing;circuito integrado;tecnologia mos complementario;logique retenue;carry skip bcd adder;calculo optico;binary circuit;low power;reversible logic;garbage output;low power electronics;borne inferieure;code binaire;codigo binario;temps retard;additionneur;delay time;binary coded decimal adder;implementacion;binary coded decimal;technologie mos complementaire;electronique faible puissance;tiempo retardo;binary code;lower bound;circuit arithmetique;circuit integre;complementary mos technology;arithmetic circuit;cota inferior;quantum cost	Reversible logic has become one of the most promising research areas in the past few decades and has found its applications in several technologies; such as low-power CMOS, nanocomputing and optical computing. This paper presents improved and efficient reversible logic implementations for Binary Coded Decimal (BCD) adder as well as Carry Skip BCD adder. It has been shown that the modified designs outperform the existing ones in terms of number of gates, number of garbage outputs, delay, and quantum cost. In order to show the efficiency of the proposed designs, lower bounds of the reversible BCD adders in terms of gates and garbage outputs are proposed as well. & 2008 Elsevier Ltd. All rights reserved.	adder (electronics);binary-coded decimal;cmos;carry-skip adder;computer;garbage collection (computer science);low-power broadcasting;nanocomputer;optical computing;quantum computing;reversible computing	Ashis Kumer Biswas;Md. Mahmudul Hasan;Ahsan Raja Chowdhury;Hafiz Md. Hasan Babu	2008	Microelectronics Journal	10.1016/j.mejo.2008.04.003	binary code;electronic engineering;binary-coded decimal;telecommunications;integrated circuit;optical computing;upper and lower bounds;implementation;carry-save adder;algorithm;adder;low-power electronics	EDA	16.492460448531492	45.533247239590565	94278
32918648cced1d9bdd57cef1611ac84a7c1ef3a3	a bit-level systolic array for median filter	systolic arrays filters signal processing algorithms very large scale integration filtering algorithms signal design statistics clocks streaming media speech processing;median filter;time complexity;clocks;bit level systolic array;input samples;very large scale integration;systolic arrays;signal design;speech processing;systolic arrays digital filters filtering and prediction theory;filters;systolic array;signal filtering and prediction;window size;computer applications;filtering and prediction theory;computer architecture;processor elements;filtering algorithms;streaming media;odd even transposition network;digital filters;statistics;window size bit level systolic array median filter bit strings input samples majority function odd even transposition network processor elements time complexity;processor element;bit strings;signal processing algorithms;majority function	A bit-level systolic array for median filters is presented based on the majority of the bit strings of input samples. The majority function can be implemented by a 1-b oddleven transposition network, which is regular and can be designed easily. If each input sample is represented by r bits, the systolic array contains r processor elements and requires time complexity O(N + n + r 1) for a stream of N samples with window size n in a one-dimensional median filter.	bit-level parallelism;majority function;median filter;systolic array;time complexity	Long-Wen Chang;Jin-Her Lin	1992	IEEE Trans. Signal Processing	10.1109/78.150009	time complexity;median filter;real-time computing;digital filter;majority function;systolic array;computer science;electrical engineering;theoretical computer science;speech processing;very-large-scale integration;computer applications;statistics	Theory	13.46801558846572	43.24020842709082	94360
0bbc171f3ee7f4d71409ed8e97c3f9b49411ae59	a multi-granularity fpga with hierarchical interconnects for efficient and flexible mobile computing	size 40 nm fast fourier transform asic designs coarse grained kernels interconnect area reduction mix radix hierarchical interconnect software defined radios universal dsp fft processor reconfigurable block ram medium grained dsp processors fine grained configurable logic blocks area efficiency energy efficiency dark silicon power limited soc system on a chip designs mobile computing multigranularity fpga;cmos integrated circuits;logic design;system on chip cmos integrated circuits digital signal processing chips electronic engineering computing field programmable gate arrays logic design mobile computing software radio;system on chip application specific integrated circuits digital signal processing chips fast fourier transforms field programmable gate arrays integrated circuit design integrated circuit interconnections mobile computing random access storage;software radio;size 40 nm multigranularity fpga hierarchical interconnects mobile computing mobile system on a chip soc design accelerator energy efficiency power limited soc dark silicon cmos configurable logic block look up table lut random logic basic arithmetic shift register distributed memories dsp processor mac operation simd operation reconfigurable block ram coarse grained kernel fast fourier transform fft processor 16 core universal dsp udsp software defined radio sdr asic design;system on chip;field programmable gate arrays kernel program processors energy efficiency integrated circuit interconnections system on chip digital signal processing;field programmable gate arrays routing integrated circuit interconnections hardware logic gates system on chip digital signal processing;digital signal processing chips;electronic engineering computing;field programmable gate arrays;mobile computing;low power design digital integrated circuits digital signal processors dsps fast fourier transform fft field programmable gate arrays fpgas interconnect networks	Following the rapid expansion of mobile computing, mobile system-on-a-chip (SoC) designs have off-loaded most compute-intensive tasks to dedicated accelerators to improve energy and area efficiency. An increasing number of accelerators in power-limited SoCs results in large regions of “dark silicon.” Unlike processors, dedicated hardware is inflexible, so any changes would require a chip re-design, which significantly impacts cost and timeline. To address the need for efficiency and flexibility, this work presents a multi-granularity FPGA suitable for mobile computing. Occupying 20.5 mm 2 in 40 nm CMOS, the chip incorporates fine-grained configurable logic blocks, medium-grained DSP processors and reconfigurable block RAMs, and two coarse-grained kernels: a 64- to 8192-point FFT processor and a 16-core universal DSP for software-defined radios. Using a mix-radix hierarchical interconnect, the chip achieves a 3-4× interconnect area reduction over commercial FPGAs for comparable connectivity, reducing overall area and leakage by 2-2.5×, and delivering up to 50% lower active power. With coarse-grained kernels, the energy efficiency reaches within 4-5× of ASIC designs.	application-specific integrated circuit;cmos;central processing unit;dark silicon;digital signal processor;fast fourier transform;field-programmable gate array;internet of things;mobile app;mobile computing;sensor;signal processing;speaker wire;spectral leakage;system on a chip;timeline	Fang-Li Yuan;Cheng C. Wang;Tsung-Han Yu;Dejan Markovic	2014	IEEE Journal of Solid-State Circuits	10.1109/JSSC.2014.2372034	system on a chip;embedded system;electronic engineering;parallel computing;logic synthesis;reconfigurable computing;computer science;software-defined radio;cmos;mobile computing;field-programmable gate array	EDA	11.396901096612178	46.26517501330641	94559
9f313187677c7d6fc5323af9aef043575b662a35	new design of reversible full adder/subtractor using r gate		Quantum computers require quantum processors. An important part of the processor of any computer is the arithmetic unit, which performs binary addition, subtraction, division and multiplication, however multiplication can be performed using repeated addition, while division can be performed using repeated subtraction. In this paper we present two designs using the reversible R gate to perform the quantum half adder/ subtractor and the quantum full adder/subtractor. The proposed half adder/subtractor design can be used to perform different logical operations, such as AND, XOR, NAND, XNOR, NOT and copy of basis. The proposed design is compared with the other previous designs in terms of the number of gates used, the number of constant bits, the garbage bits,	1-bit architecture;and gate;adder (electronics);arithmetic logic unit;binary number;central processing unit;computer;display device;embedded system;exclusive or;logical connective;logical unit number;multiplexer;program status word;quantum cryptography;seven-segment display;sheffer stroke;subtractor;xnor gate	Rasha Montaser;Ahmed Younes;Mahmoud Abdel-Aty	2017	CoRR	10.1007/s10773-018-3921-1	quantum mechanics;subtractor;parallel computing;physics;nand gate;serial binary adder;quantum computer;binary number;multiplication;xnor gate;adder	Arch	15.244117266520012	43.2210540383264	94785
6437f259709ad155286e667f3255daf7ba63e99f	majority adder implementation by competing patterns in life-like rule b2/s2345	logic gate;cellular automaton	We study Life-like cellular automaton rule B2/S2345. This automaton exhibits a chaotic behavior yet capable for purposeful computation. The automaton implements Boolean gates via patterns which compete for the space when propagate in channels. Values of Boolean variables are encoded into two types of patterns — symmetric (False) and asymmetric (True). We construct basic logical gates and elementary arithmetical circuits by simulating logical signals using glider reactions taking place in the channels built of non-destructible still lifes. We design a binary adder of majority gates realised in rule B2/S2345.	adder (electronics);chaos theory;computation;glider (conway's life);life-like cellular automaton;logical connective;simulation	Genaro Juárez Martínez;Kenichi Morita;Andrew Adamatzky;Maurice Margenstern	2010		10.1007/978-3-642-13523-1_12	block cellular automaton;discrete mathematics;elementary cellular automaton;theoretical computer science;mathematics;rule 184;algorithm	Logic	18.44276333819193	42.109621031572665	94913
e3518bdfe6cb4cd7a804d7760b3e34a28bd2718e	parallel regognition and parsing on the hypercube	data transmission;hypercube;64 node ncube 7 mimd hypercube machine;matrix product chain problem parallel recognition time bounds parsing hypercube parallel algorithms context free languages sequential dynamic programming algorithm nonoverlapping interprocessor data transmissions space bounds 64 node ncube 7 mimd hypercube machine polygon triangulation problem;production chain;dynamic load balancing;parallel algorithm;polygon triangulation problem;context free languages;parallel recognition;time bounds;context free language;dynamic programming algorithm;dynamic program;nonoverlapping interprocessor data transmissions;hypercubes dynamic programming parallel algorithms heuristic algorithms load management computer science data communication cost function context modeling computer languages;sequential dynamic programming algorithm;parsing;grammars;indexation;space bounds;matrix product chain problem;communication cost;load balance;parallel algorithms context free languages grammars;parallel algorithms	This correspondence describes a family of linear systolic arrays for matrix multiplication exhibiting a tradeoff between local storage and the number of PE’s. The design consists of n [n/sl processors hooked into a linear array with each processor having storage s, 1 5 s 5 n, for n x n matrix multiplication. The input matrices are fed as two speed data streams using fast and slow channels to satisfy the dependencies in the usual matrix multiplication algorithm. While a family of linear arrays have been synthesized for this problem, our technique leads to simpler designs with fewer number of processors and improved delay from input to output. All our designs use optimal number of processors for local storage in the range 1 5 s 5 n. The data flow is unidirectional which makes the designs implementable on fault tolerant wafer scale integration models.	central processing unit;dataflow;fault tolerance;matrix multiplication algorithm;parsing;thread-local storage;wafer-scale integration	Oscar H. Ibarra;Ting-Chuen Pong;Stephen M. Sohn	1991	IEEE Trans. Computers	10.1109/12.90253	parallel computing;computer science;theoretical computer science;context-free language;distributed computing;parallel algorithm;programming language;algorithm	Theory	13.334707222346884	33.71768341433713	95393
2478b5ed4e11a710be6c6f12eef9bcec8fe05c5f	a new memoryless and low-latency fft rotator architecture	fast fourier transform fft;rotator;complexity theory registers adders ieee 802 15 standards throughput memory architecture;complexity theory;cordic fast fourier transform fft rotator twiddle factor;cordic;registers;memory architecture;adders;pipelined based fft architectures low latency fft rotator architecture memoryless fft rotator architecture cascaded multiplier less cells partial twiddle factor multiplications low complexity adders multiplexers area reduction subexpression sharing coefficient selection scheme cordic based architectures memory based fft architectures;multiplexing equipment adders fast fourier transforms;proceedings paper;ieee 802 15 standards;twiddle factor;throughput	This paper presents new rotator architecture for FFT computation. The proposed architecture consists of cascaded multiplier-less cells, and each cell stage performs partial twiddle factor multiplications with low-complexity adders and multiplexers. Besides, for further area reduction, each cell is optimized with the technique of common subexpression sharing. Since those twiddle factors involved in computation are realized with multipliers generated on-the-fly by a scheme of coefficient selection, the proposed architecture doesn't require memory space to store any twiddle factors. Variable FFT lengths ranging from 64 ~ 32768 points can be supported by flexibly adding or removing some cell stages, depends on FFT length. Compared to CORDIC-based architectures, the proposed architecture has lower latency. The implementation results show that the proposed architecture is area-efficient and is suitable for either pipelined or memory based FFT architectures.	cordic;coefficient;computation;dspace;fast fourier transform;multiplexer;pipeline (computing);split-radix fft algorithm;twiddle factor	Shen-Jui Huang;Sau-Gee Chen	2014	2014 International Symposium on Integrated Circuits (ISIC)	10.1109/ISICIR.2014.7029558	electronic engineering;twiddle factor;parallel computing;split-radix fft algorithm;computer science;theoretical computer science;prime-factor fft algorithm	Arch	12.098406821323143	44.23170084369668	95607
d093a9954b80b353217e87fc088ec55063b6ca48	the bi-panconnectivity of the hypercube	graph theory;hypercube;satisfiability;interconnection network;hypercubes multiprocessor interconnection networks bipartite graph computer science fault tolerance routing broadcasting information processing hardware costs;hypercube networks graph theory;information processing;interconnection networks;algorithms;bipartite graph bi panconnectivity hypercube;bi panconnectivity;bipartite graph;hypercube networks;algorithms interconnection networks	A bipartite graph is bi-panconnected if an arbitrary pair of vertices x, y are connected by the bi-panconnected paths that include a path of each length s satisfying N-1 ges s ges dist(x, y) and s-dist(x, y) is even, where N is the number of vertices, and dist(x, y) denotes the shortest distance between x and y. Li et al. [Information Processing Letters 87 (2003) 107-110] have shown that the hypercube is bi-panconnected. However, a definite algorithm to generate such paths is still absent. In this paper, we present algorithms to generate the bi-panconnected paths joining an arbitrary pair of vertices in the hypercube.	algorithm;information processing letters;olap cube;vertex (geometry)	Jywe-Fei Fang;Chien-Hung Huang;Ko-Lin Lin;Chen-Hsiang Liao;Shr-Cheng Feng	2007	2007 International Conference on Networking, Architecture, and Storage (NAS 2007)	10.1109/NAS.2007.52	graph power;edge-transitive graph;folded cube graph;complete bipartite graph;factor-critical graph;graph bandwidth;bipartite graph;information processing;median graph;graph theory;theoretical computer science;simplex graph;hypercube graph;foster graph;path graph;voltage graph;distributed computing;induced path;biregular graph;quartic graph;complement graph;hypercube;satisfiability	Robotics	22.657619348714906	34.544539113368245	96093
df0509cd5bace0dfdaf346df8ffbd949787518bc	on the maximal connected component of a hypercube with faulty vertices iii	school of no longer in use;hypercube;electronics and computer science;fault tolerant;endnotes;interconnection network;fault tolerance;pubications;connected component;institutional repository research archive oaister;maximal connected component	Hypercube is one of the most popular topologies for connecting processors in multicomputer systems. In this paper we address the maximum order of a connected component in a faulty cube. The results established include several known conclusions as special cases. We conclude that the hypercube structure is resilient as it includes a large connected component in the presence of large number of faulty vertices.	central processing unit;connected component (graph theory);maximal set;network topology;parallel computing;vertex (geometry);vertex (graph theory)	Xiaofan Yang;David J. Evans;Graham M. Megson	2006	Int. J. Comput. Math.	10.1080/00207160500113173	fault tolerance;parallel computing;theoretical computer science;mathematics;distributed computing	Theory	23.649547361810818	35.37808897593852	96204
960d0edc4246a5e00a649c0f21899ed5c8f3d373	the implementation of an all digital speech synthesizer using a multimicroprocessor architecture	microprocessors;digital signal processing;speech synthesis;speech synthesis synthesizers linear predictive coding vocoders microprocessors time division multiplexing digital filters signal synthesis digital arithmetic digital signal processing;time division multiplex;digital filter;linear predictive coding;digital filters;vocoders;digital arithmetic;signal synthesis;time division multiplexing;synthesizers;high speed;large classes	This paper presents a unique approach for the implementation of the speech synthesis portion of a LPC vocoder. The implementation uses eight LSI—ll microprocessors operating synchronously around a time division multiplexed multiport memory. The implementation of the recursive digital filter portion of the LPC synthesis is accomplished without the use of any high speed external arithmetic elements and in a way which requires no synchronization overhead. This particular technique illustrates a general procedure which is applicable to the implementation of a large class of digital signal processing algorithms on multiprocessor	algorithm;digital filter;digital signal processing;integrated circuit;lpc;microprocessor;multiplexing;multiprocessing;overhead (computing);recursion;speech synthesis;vocoder	C. J. M. Hodges;Thomas P. Barnwell;Daniel McWhorter	1980		10.1109/ICASSP.1980.1171058	real-time computing;linear predictive coding;speech recognition;digital down converter;digital filter;digital signal;computer science;electrical engineering;digital signal processing;speech processing;speech synthesis;digital delay line	Arch	12.530978525098975	43.61056746567512	96469
909a74ab96c0351c85340e6d7ba4e09fdd1f30d2	performance of fault-tolerant diagnostics in the hypercube systems	distributed data;multiprocessor interconnection networks;computers;digital computers;distributed system;software testing;processing 990210 supercomputers 1987 1989;general and miscellaneous mathematics computing and information science;performance evaluation;fault tolerant;performance evaluation fault tolerant computing multiprocessing systems;diagnostic scheme;hypercube systems;automatic testing;performance;data processing;fault tolerant systems hypercubes fault tolerance partitioning algorithms algorithm design and analysis costs multiprocessor interconnection networks automatic testing software testing codes;complexity;mathematical logic;hadamard matrix;np hard problem;fault tolerant computing;fault tolerant systems;coding theory;codes;np hard problems;self diagnostic algorithm;fault tolerance;hypercubes;algorithms;design;multiprocessing systems;fault tolerant diagnostics;distributed systems;hadamard matrix performance evaluation fault tolerant diagnostics hypercube systems distributed systems complexity self diagnostic algorithm optimal algorithm coding theory np hard problems diagnostic scheme combinatorial structure;distributed data processing;optimal algorithm;size;hypercube computers;combinatorial structure;algorithm design and analysis;logical process;fault tolerant computers;partitioning algorithms	The concept of fault-tolerant self-diagnostics is introduced for distributed systems, and it is shown that there exists a performance tradeoff between the complexity of a self-diagnostic algorithm and the level of fault tolerance inherited by the algorithm. Hypercube systems are selected, and it is shown that designing an optimal algorithm for such systems has an equivalent coding theory formulation which belongs to the case of NP-hard problems. An efficient diagnostic scheme is proposed for these systems, and the performance tradeoff of the proposed algorithm, which is based on a combinatorial structure called the Hadamard matrix, is studied. The tradeoff between the fault tolerance and traffic complexity of the proposed diagnostic algorithm for hypercubes of small size is evaluated. An interesting compromise is exhibited for the hypercube with an arbitrary size. >		Arif Ghafoor;Patrick Solé	1989	IEEE Trans. Computers	10.1109/12.30870	fault tolerance;parallel computing;data processing;computer science;theoretical computer science;np-hard;distributed computing;algorithm	Embedded	23.533333374223172	44.11906347349968	96642
ba087a7e9c073b148d47046aef2b7cf7e9aa38f1	hybrid partitioned h.264 full high definition decoder on embedded quad-core	bandwidth;multi core;data compression;decoding;vliw;load balancing;resource allocation;embedded systems;memory management;motion compensation	In this paper, we address the problem of efficient mapping H.264 decoder on embedded quad-core platform. For this purpose, we propose new partitioning method called `Hybrid partitioning'. As a result of applying hybrid partition, we can reduce 86.0% of waiting overhead compared with functional partitioning.	embedded system;h.264/mpeg-4 avc;multi-core processor;overhead (computing);space partitioning	Minsoo Kim;Joon Ho Song;Do-Hyung Kim;Shihwa Lee	2012	2012 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2012.6161867	data compression;multi-core processor;computer architecture;parallel computing;real-time computing;resource allocation;computer science;very long instruction word;operating system;motion compensation;set partitioning in hierarchical trees;bandwidth;statistics;memory management	EDA	11.558627625414374	39.983596034697634	96710
bd6aea33c84a8e608a91c01f476d30210b3231cb	efficient partitioning of static buses for processor arrays of small size	relation n;static bus;small size;larger size n;efficient partitioning;size m;reconfigurable bus;size n;processor array;smaller size m;column bus;static row;log m	This paper shows an efficient partitioning of static row/ column buses for tightly coupled 2D mesh-connected processor arrays (mesh for short) of small size With additional $O({\frac{n}{m}\left(\frac{n}{m}+\log m\right)})$ time slowdown, it enables the mesh of size m ×m with static row/column buses to simulate the mesh of larger size n ×n with reconfigurable row/ column buses (m≤n) This means that if a problem can be solved in O(T) time by the mesh of size n ×n with reconfigurable bus, then the same problem can be solved in $O({T\cdot\frac{n}{m}\left(\frac{n}{m}+\log m\right)})$ time on the mesh of smaller size m ×m without reconfigurable function This time-cost is optimal when the relation n≥mlogm holds (e.g., m=n1−e for e>0).		Susumu Matsumae	2010		10.1007/978-3-642-13119-6_16	parallel computing;real-time computing;computer science;distributed computing	Arch	12.589813734423235	32.868459792708755	96766
0c6417423426477efafab5159553f16f20b05a57	searching for a black hole in arbitrary networks: optimal mobile agents protocols	optimal solution;sense of direction;black hole;upper bound;a priori knowledge;malicious host;network optimization;distributed search;mobile agent	Consider a networked environment, supporting mobile agents, where there is a black hole: a harmful host that disposes of visiting agents upon their arrival, leaving no observable trace of such a destruction. The black hole search problem is the one of assembling a team of asynchronous mobile agents, executing the same protocol and communicating by means of whiteboards, to successfully identify the location of the black hole; we are concerned with solutions that are generic (i.e., topology-independent). We establish tight bounds on the size of the team (i.e., the number of agents), and the cost (i.e., the number of moves) of a size-optimal solution protocol. These bounds depend on the a priori knowledge the agents have about the network, and on the consistency of the local labelings. In particular, we prove that: with topological ignorance Δ+1 agents are needed and suffice, and the cost is Θ(n 2), where Δ is the maximal degree of a node and n is the number of nodes in the network; with topological ignorance but in presence of sense of direction only two agents suffice and the cost is Θ(n 2); and with complete topological knowledge only two agents suffice and the cost is Θ(n log n). All the upper-bound proofs are constructive.	black hole;maximal set;mobile agent;observable;search problem	Stefan Dobrev;Paola Flocchini;Giuseppe Prencipe;Nicola Santoro	2006	Distributed Computing	10.1007/s00446-006-0154-y	black hole;a priori and a posteriori;computer science;theoretical computer science;mobile agent;distributed computing;upper and lower bounds	AI	17.83321698520306	34.41894562306036	96824
0b4448baaf787fc0cdea19386f27b50ae15f20a0	finding the size of a radio network with short labels		The number of nodes of a network, called its size, is one of the most important network parameters. Knowing the size (or a good upper bound on it) is a prerequisite of many distributed network algorithms, ranging from broadcasting and gossiping, through leader election, to rendezvous and exploration. A radio network is a collection of stations, called nodes, with wireless transmission and receiving capabilities. It is modeled as a simple connected undirected graph whose nodes communicate in synchronous rounds. In each round, a node can either transmit a message to all its neighbors, or stay silent and listen. At the receiving end, a node v hears a message from a neighbor w in a given round, if v listens in this round, and if w is its only neighbor that transmits in this round. If v listens in a round, and two or more neighbors of v transmit in this round, a collision occurs at v. If v transmits in a round, it does not hear anything in this round. Two scenarios are considered in the literature: if listening nodes can distinguish collision from silence (the latter occurs when no neighbor transmits), we say that the network has the collision detection capability, otherwise there is no collision detection.  We consider the task of size discovery: finding the size of an unknown radio network with collision detection. All nodes have to output the size of the network, using a deterministic algorithm. Nodes have labels which are (not necessarily distinct) binary strings. The length of a labeling scheme is the largest length of a label.  We concentrate on the following problem:  What is the shortest labeling scheme that permits size discovery in all radio networks of maximum degree Δ?  Our main result states that the minimum length of such a labeling scheme is Θ(loglogΔ). The upper bound is proven by designing a size discovery algorithm using a labeling scheme of length O (loglogΔ), for all networks of maximum degree Δ. The matching lower bound is proven by constructing a class of graphs (in fact even of trees) of maximum degree Δ, for which any size discovery algorithm must use a labeling scheme of length at least Ω(loglogΔ) on some graph of this class.	collision detection;degree (graph theory);deterministic algorithm;fastest;graph (discrete mathematics);leader election;string (computer science);time complexity	Barun Gorain;Andrzej Pelc	2018		10.1145/3154273.3154298	computer network;collision;degree (graph theory);computer science;rendezvous;leader election;collision detection;topology;upper and lower bounds;deterministic algorithm;broadcasting	Theory	18.597623978679568	33.846068079184356	97053
ddfd33fde03c54ea880b26e86f47832b99b2aa18	optimal and efficient probabilistic distributed diagnosis schemes	tolerancia falta;analytical models;distributed system;systeme reparti;probabilistic diagnosis;performance evaluation;fault tolerant;multiprocessor;diagnostic accuracy;automatic testing;distributed self diagnosis;diagnostic reparti;indexing terms;diagnostic niveau systeme;fault diagnosis algorithm design and analysis h infinity control system testing space technology automatic testing performance analysis computational modeling analytical models fault tolerant systems;diagnosis algorithms performance evaluation probabilistic distributed diagnosis schemes distributed self diagnosis multiprocessor multicomputer system interprocessor tests imperfect fault coverage intermittently faulty processors probabilistic diagnosis methods fault syndrome information diagnosis categories simulations;multicomputer system;fault tolerant computing;computational modeling;sistema repartido;interprocessor tests;fault tolerant systems;diagnosis categories;diagnosis algorithms;probabilistic analysis;intermittently faulty processors;multiprocessing systems fault tolerant computing;fault tolerance;performance analysis;probabilistic diagnosis methods;system testing;space technology;diagnostic probabiliste;multiprocessing systems;multiprocesador;h infinity control;probabilistic distributed diagnosis schemes;algorithm design and analysis;tolerance faute;fault syndrome information;fault diagnosis;distributed diagnosis;imperfect fault coverage;multiprocesseur;system level diagnosis	The distributed self-diagnosis of a multiprocessor/multicomputer system based on interprocessor tests with imperfect fault coverage (thus also permitting intermittently faulty processors) is addressed. Focusing on probabilistic diagnosis methods, we define several different categories of probabilistic diagnosis based on the type of fault syndrome information used in the diagnosis. Rigorous probabilistic analysis is then used to derive optimal diagnosis algorithms (optimal in terms of diagnostic accuracy) for the diagnosis categories introduced. Analysis and simulations are used to evaluate the performance of the diagnosis algorithms introduced. Index TermDistributed diagnosis, fault-tolerant computing, intermittent fault, multicomputer, multiprocessor, probabilistic diagnosis, system-level diagnosis.	algorithm;central processing unit;fault coverage;fault tolerance;intermittent fault;multiprocessing;parallel computing;probabilistic analysis of algorithms;simulation	Sunggu Lee;Kang G. Shin	1993	IEEE Trans. Computers	10.1109/12.237729	fault tolerance;probabilistic analysis of algorithms;real-time computing;computer science;theoretical computer science;distributed computing;algorithm	AI	23.640280711709984	44.39579810235003	97123
078aa477536861bf843e3d5aa7719bd8aa03145b	vlsi implementation of an edge-oriented image scaling processor	hardware design languages;interpolation;arquitectura circuito;tsmc 0 18 mum technology edge oriented image scaling processor edge oriented area pixel scaling processor low complexity vlsi architecture edge catching technique image edge features;performance evaluation;image processing;image resolution;integrated circuit;diminution cout;edge detection;very large scale integration;implementation;interpolacion;circuit vlsi;procesamiento imagen;pipeline architecture;circuit architecture;low complexity;circuito integrado;edge oriented area pixel scaling processor;qualite image;visual quality;traitement image;vlsi circuit;image edge features;computational complexity;very large scale integration circuits costs performance evaluation image resolution educational institutions pixel hardware design languages image processing image quality;feature extraction;edge catching technique;image quality;pixel;low complexity vlsi architecture;architecture circuit;vlsi computational complexity edge detection feature extraction image resolution microprocessor chips;vlsi;image scaling;circuits;calidad imagen;tsmc 0 18 mum technology;procesador;vlsi image scaling interpolation pipeline architecture;circuito vlsi;processeur;implementacion;reduccion costes;edge oriented image scaling processor;cost lowering;processor;quantitative evaluation;circuit integre;microprocessor chips;vlsi architecture	Image scaling is a very important technique and has been widely used in many image processing applications. In this paper, we present an edge-oriented area-pixel scaling processor. To achieve the goal of low cost, the area-pixel scaling technique is implemented with a low-complexity VLSI architecture in our design. A simple edge catching technique is adopted to preserve the image edge features effectively so as to achieve better image quality. Compared with the previous low-complexity techniques, our method performs better in terms of both quantitative evaluation and visual quality. The seven-stage VLSI architecture of our image scaling processor contains 10.4-K gate counts and yields a processing rate of about 200 MHz by using TSMC 0.18-mum technology.	clock rate;image processing;image quality;image scaling;monochrome;performance;pixel;simulation;very-large-scale integration	Pei-Yin Chen;Chih-Yuan Lien;Chi-Pin Lu	2009	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2008.2003003	embedded system;computer vision;electronic engineering;image processing;interpolation;computer science;electrical engineering;very-large-scale integration;computer graphics (images)	Visualization	13.856840342800501	40.790273358088925	97486
4ed3a2853506e9ac40d0907f6597da492995b17f	an efficient algorithm-based concurrent error detection for fft networks	tolerancia falta;detection erreur;checksum scheme;concurrent error detection;multidimensional fft network;efficient algorithm;algorithm based fault tolerance;circuit vlsi;false alarm rate;fast fourier transforms error detection fault tolerant computing;fast fourier transform;fault tolerant computing;vlsi circuits;hardware overhead algorithm based concurrent error detection fft networks checksum approach linear weight factors;fault tolerance;vlsi;fast fourier transforms;fault coverage;error detection;taux fausse alarme;transformation fourier rapide;hardware circuit faults throughput fault tolerance fast fourier transforms computer networks delay degradation multidimensional systems fault detection;porcentaje falsa alarma;tolerance faute	In this brief contribution a new algorithm-based concurrent error detection scheme employing the checksum approach is proposed for FFT networks. Our design allows high error coverage with low false alarm rate by applying the linear weight factors to the checksums. Due to the simplicity, the hardware overhead is relatively small and errors can be quickly detected. The design is also shown to be easily expanded for multidimensional FFT networks. >	algorithm;error detection and correction;fast fourier transform	Choong Gun Oh;Hee Yong Youn;Vijay K. Raj	1995	IEEE Trans. Computers	10.1109/12.464396	fast fourier transform;parallel computing;real-time computing;computer science;theoretical computer science;very-large-scale integration;algorithm	Embedded	22.826406779681122	44.49284496907628	97895
8e442ed9e80ca1f79aeba6e7729f059ea35ae621	a high throughput and low cost diamond search architecture for hdtv motion estimation	image sampling;hardware design motion estimation video compression fast algorithm;diamond search;frames per second;computer architecture algorithm design and analysis hardware hdtv throughput motion estimation software algorithms;full search;image resolution;real time;hardware description languages;video compression;motion estimation;frequency 185 7 mhz;computer architecture;frequency 185 7 mhz low cost diamond search architecture hdtv motion estimation subsampled diamond search algorithm software implementations vhdl xilinx virtex 4 fpga;motion estimation field programmable gate arrays hardware description languages high definition television image resolution image sampling;low cost diamond search architecture;fast algorithm;vhdl;hdtv;hdtv motion estimation;software algorithms;hardware design;xilinx virtex 4 fpga;software implementations;field programmable gate arrays;high throughput;algorithm design and analysis;high definition television;software implementation;subsampled diamond search algorithm;throughput;hardware	This paper presents a high throughput and low cost architecture for motion estimation using a sub-sampled diamond search algorithm (SDS). The quality of SDS was compared with full search through software implementations and the results are presented. The designed hardware considered a search area of 100times100 samples, with blocks of 16times16 pixels. The architecture was described in VHDL and mapped to a Xilinx Virtex-4 FPGA. Synthesis results indicate that SDS is able to run at 185.7 MHz, using only 3541 LUTs. This architecture can reach real time for HDTV (1920times1080 pixels) in the worst case, and it can process 120 HDTV frames per second in the average case.	best, worst and average case;field-programmable gate array;motion estimation;pixel;search algorithm;throughput;vhdl	Marcelo Schiavon Porto;Luciano Volcan Agostini;Sergio Bampi;Altamiro Amadeu Susin	2008	2008 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2008.4607614	data compression;high-throughput screening;embedded system;algorithm design;computer vision;throughput;real-time computing;image resolution;computer hardware;vhdl;computer science;motion estimation;hardware description language;frame rate;field-programmable gate array	Robotics	12.638572492909994	40.89510008144159	98215
665937aeb763530bb6b5b8ac399e08e7eb379091	hevc decoder optimization in low power configurable architecture for wireless devices	microprocessors;interpolation;wireless devices;decoding;hevc;decoding interpolation microprocessors registers streaming media optimization wireless communication;wireless communication;streaming media;registers;video decoding;wireless devices hevc video decoding interpolation filter configurable architectures;interpolation filter;optimization;configurable architectures;microprocessor architecture low power configurable architecture wireless device high efficiency video coding video compression standard power saving wireless video transmission hevc decoding optimization instruction set architecture isa quarter pixel interpolation filter implementation custom made instruction motion compensation optimization;video streaming data compression decoding filtering theory instruction sets interpolation motion compensation radio networks video coding	High Efficiency Video Coding (HEVC) is the new video compression standard, reducing bitrates nearly at half compared to H.264, offering potentially significant power savings for wireless video transmission at the network interface. This reduction in bitrate is achieved by a series of computationally expensive algorithms, thus making imperative to optimize HEVC decoding in order to provide a low-power implementation that can be used in mobile devices. Extending the Instruction Set Architecture (ISA) of a configurable microprocessor with new instructions for a target application can reduce the total effort of the application, thus reducing operating frequency and eventually power. The flexibility and relatively low design effort of such microprocessors - compared to hardwired Application-Specific-Integrated-Circuit (ASIC) designs - reduces the time space for adoption of HEVC and makes them an efficient alternative for wireless devices. We propose an efficient quarter-pixel interpolation filter implementation for HEVC using new custom-made instructions and other techniques for optimization of motion compensation, implemented on a configurable microprocessor architecture. Simulation results show a four times acceleration on average of the interpolation filter module over the reference HEVC software and an overall doubling in decoder performance.	algorithm;analysis of algorithms;application-specific integrated circuit;clock rate;codec;data compression;documentation;dyadic transformation;h.264/mpeg-4 avc;hardware acceleration;high efficiency video coding;imperative programming;interpolation;low-power broadcasting;mathematical optimization;microprocessor;mobile device;motion compensation;network interface;out of the box (feature);period-doubling bifurcation;pixel;simulation;software transactional memory;video coding format	Vasileios Magoulianitis;Ioannis Katsavounidis	2015	2015 IEEE 16th International Symposium on A World of Wireless, Mobile and Multimedia Networks (WoWMoM)	10.1109/WoWMoM.2015.7158216	embedded system;real-time computing;interpolation;computer science;operating system;processor register;wireless	Arch	12.732159771415224	41.11738961345798	98424
f24744c05c8f4b7b8c2afdf5725da0ac89d93e55	a high-performance vlsi architecture for intra prediction and mode decision in h.264/avc video encoding	cmos integrated circuits;frames per second;video encoding;cmos cell library;data reuse;vlsi cmos integrated circuits video coding;video quality;video sequences;hardware accelerator;intra prediction;avc;75 mhz;video coding;0 13 micron;h 264;vlsi;very large scale integration automatic voltage control encoding cost function software algorithms algorithm design and analysis software quality computer architecture hardware design languages circuit synthesis;0 13 micron hardware accelerator mode decision video encoding data reuse cmos cell library video sequences vlsi h 264 avc 75 mhz;high performance;mode decision;vlsi architecture	The authors propose a high-performance hardware accelerator for intra prediction and mode decision in H.264/AVC video encoding. They use two intra prediction units to increase the performance. Taking advantage of function similarity and data reuse, the authors successfully reduce the hardware cost of the intra prediction units. Based on a modified mode decision algorithm, the design can deliver almost the same video quality as the reference software. The authors implemented the proposed architecture in Verilog and synthesized it targeting towards a TSMC 0.13mum CMOS cell library. Running at 75MHz, the 36K-gate circuit is capable of realtime encoding 720p HD (1280times720) video sequences at 30 frames per second (fps)	algorithm;cmos;data compression;h.264/mpeg-4 avc;hardware acceleration;intra-frame coding;verilog;very-large-scale integration	Yu-Chien Kao;Huang-Chih Kuo;Yin-Tzu Lin;Chia-Wen Hou;Yi-Hsien Li;Hao-Tin Huang;Youn-Long Lin	2006	APCCAS 2006 - 2006 IEEE Asia Pacific Conference on Circuits and Systems	10.1109/APCCAS.2006.342532	embedded system;electronic engineering;real-time computing;hardware acceleration;computer science;video quality;machine learning;very-large-scale integration;cmos;frame rate	EDA	12.518841680720769	40.78565963228206	98560
293fb62fb672ac9dda600f71676e424a4208c240	fpga implementation of integer transform and quantizer for h.264 encoder	quantization;fpga implementation;integer transform;scalar quantization;h 264;pipelining;hardware implementation	This paper deals with the process of Transformation and Quantization that is carried out on each inter-predicted residual block in a video encoding process and their reduced complexity hardware implementation. H.264/AVC utilizes 4?×?4 integer transform, which is derived from the 4?×?4 DCT. We propose, a reduced complexity algorithm and a pipelined structure for the Core forward integer transform module. A multiplier-less architecture is realized with less number of shifts and adds compared to existing works. The corresponding inverse transform is exactly reversible. Each of the transformed coefficients is quantized by a scalar quantizer. The quantization step size can be varied from macroblock to macroblock. The proposed unified pipelined architecture outperforms many recent implementations in terms of gate count and is capable of processing a 4?×?4 residual block in 4 clock cycles.	encoder;field-programmable gate array;h.264/mpeg-4 avc;quantization (signal processing)	Reeba Korah;J. Raja Paul Perinbam	2008	Signal Processing Systems	10.1007/s11265-008-0163-0	computer vision;discrete mathematics;quantization;computer science;theoretical computer science;mathematics;pipeline;algorithm	EDA	12.395547934100335	40.95915303972581	98692
e69d808c91893ade64d42cc620b0757ac4c60f7d	implementation of generalized dft on field programmable gate array	clocks field programmable gate arrays discrete fourier transforms ofdm vectors registers arrays;logic design;clocks;gdft;hardware description languages;ofdm modulation discrete fourier transforms field programmable gate arrays hardware description languages logic design;fpga;arrays;vectors;registers;ofdm modulation;ofdm;vhdl field programmable gate array fpga discrete fourier transform dft nonlinear phase channel equalization orthogonal frequency division multiplexing communication system ofdm hardware description languages;field programmable gate arrays;ofdm gdft fpga;discrete fourier transforms	We introduce the implementation of Generalized Discrete Fourier Transform (GDFT) with nonlinear phase on a Field Programmable Gate Array (FPGA.) After briefly revisiting the GDFT framework, we apply the framework to a channel equalization problem in an Orthogonal Frequency Division Multiplexing (OFDM) communication system. The block diagram of the system is introduced and detailed explanations of the implementation for each block are given along with the necessary VHDL code snippets. The resource usage and registered performance of the design is reported and alternatives to improve the design in terms of performance and resolution are provided. To the best of our knowledge, this is the first hardware implementation of GDFT reported in the literature.	diagram;discrete fourier transform;field-programmable gate array;multiplexing;nonlinear system;resolution (logic);vhdl	Wes P. Weydig;Mustafa U. Torun;Ali N. Akansu	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288227	real-time computing;orthogonal frequency-division multiplexing;computer science;theoretical computer science;field-programmable gate array	EDA	12.716150299596803	45.066519044000444	98786
3e8bd0dbac4ce50deb0dfbe05fb2a61d1e570b0c	a new decomposition method for multilevel circuit design	boolean functions;logic circuits;logic design;boolean differential equations;decomposition method;function properties;multilevel circuit design;uncompletely specified functions	"""This Paper presents a new methodology for the decomposition of Boolean functions, which is exclusively based on the utilization of function properties. The main point in this connection is the """"groupability"""" of Boolean functions. The analysis of uncompletely specified functions is carried out by means of Boolean differential equations. Examples demonstrate the efficiency of the automatic implementation."""	boolean algebra;boolean satisfiability problem;circuit design	D. Bochmann;F. Dresig;B. Steinbach	1991			circuit complexity;boolean algebra;boolean circuit;and-inverter graph;combinatorics;circuit minimization for boolean functions;discrete mathematics;reed–muller expansion;boolean network;boolean domain;majority function;boolean expression;product term;standard boolean model;maximum satisfiability problem;theoretical computer science;mathematics;combinational logic;boolean function	EDA	19.075211318975434	45.53360748111521	99073
585840f7bb1a22e9c701b35a3c069923187f58c4	evaluating relational expressions with dense and sparse arguments	complexity theory;tree graphs;tree graphs algorithm design and analysis;transitive closure;electrical engineering;connected component;titanium;algorithm design and analysis	"""We consider expressions whose arguments are relations and whose operators are chosen from among ∪, ο, *, and -1. We further assume that operands may be designated """"sparse"""" or """"dense"""", in a manner to be made formal subsequently. Our aim is to determine whether the evaluation of such an expression is (a) as hard as general transitive closure (b) as hard as transitive closure for sparse graphs. (c) as hard as connected components of an undirected graph."""	connected component (graph theory);emoticon;graph (discrete mathematics);operand;sparse matrix;transitive closure	Thomas G. Szymanski;Jeffrey D. Ullman	1975	16th Annual Symposium on Foundations of Computer Science (sfcs 1975)	10.1109/SFCS.1975.13	transitive reduction;titanium;algorithm design;combinatorics;discrete mathematics;connected component;theoretical computer science;mathematics;transitive closure;tree	Theory	21.207885611926574	32.37648419992592	99148
e4daea867a2f977ffbe1f0366c40093e3f4edac0	time-space tradeoffs for branching programs contrasted with those for straight-line programs	finite element methods;complexity theory;probability;sorting;convolution;inspection;upper bound;finite field;indexes;computational modeling;binary decision diagrams;rna;registers;games;cognition;branching program;merging;matrix multiplication;binary decision diagrams convolution discrete fourier transforms extraterrestrial measurements sorting computer science galois fields computational modeling merging costs;computer science;decision trees;table lookup;discrete fourier transforms;extraterrestrial measurements;galois fields;mesons	This paper establishes time-space tradeoffs for some algebraic problems in the branching program model. For a finite field F, convolution of n-vectors over F requires ST = Θ(n2 log |F|), where S is space and T is time, in good agreement with corresponding results for straightline programs. Our result for n × n matrix multiplication over F, ST2 = Θ(n6 log |F|), is stronger than the previously known bound ST = Ω(n3) for straight-line and branching programs. The problem of computing PAQ, where P and Q are n × n permutation matrices and A is a particular matrix, requires Ω(n3) ≤ ST ≤ O(n3logn) for branching programs, in contrast to ST = Ω(n4) for straight-line programs.	binary decision diagram;convolution;linear algebra;matrix multiplication;paq	Karl R. Abrahamson	1986	27th Annual Symposium on Foundations of Computer Science (sfcs 1986)	10.1109/SFCS.1986.58	combinatorics;discrete mathematics;computer science;theoretical computer science;mathematics;finite field;algorithm;statistics;algebra	Theory	15.543151304436323	36.62362659993076	99498
825d2f191a8e98da60e1e1532100f731cb63fa43	a distributed real-time multi-view video encoder on consumer hardware		In this paper we describe a novel approach to create standard-compliant H.264/MVC video streams in realtime by distributing most of the computational complexity among multiple H.264/AVC encoders which can be part of the capturing cameras or separate devices in between the cameras and the transcoder. By exploiting the structural similarities between H.264/AVC and H.264/MVC the transcoder only has to decode its individual input streams at a very high level. With minimal changes to the input streams a standard-compliant multi-view video stream is created. This method is efficient enough that a high number of input streams can be transcoded on a consumer multi-core PC.	algorithmic efficiency;bitstream;computational complexity theory;encoder;free viewpoint television;h.264/mpeg-4 avc;high-level programming language;macroblock;multi-core processor;real-time clock;streaming media	Tobias Lange;Thorsten Herfet	2017	2017 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)	10.1109/BMSB.2017.7986173	computer science;video compression picture types;video capture;encoder;computer hardware;real-time computing;video processing;scalable video coding;motion compensation;video tracking;multiview video coding	Embedded	11.651774741937073	39.89448120269554	99558
2f476fe620e685b5e43028e0e14681540df29fc1	recursive odd-even sorter for vector quantizer	mismatch shaping;vector quantizer;sorting algorithm;perfect shuffle sorter;recursive sorter;odd even sorter	A recursive, odd-even transposition sorter based vector quantizer which is used in mismatch shaping algorithms is presented. Although recursive parallel sorting algorithms require less area than fully parallel sorting algorithms, they are slower than fully parallel algorithms. A widely used recursive parallel sorting algorithm is the perfect shuffle which requires multiple clock cycles to shuffle and sort the data. The proposed recursive algorithm uses fewer clock cycles than the perfect shuffle to sort less than 80 inputs. An area efficient version is also proposed to sort less than 16 inputs faster than perfect shuffle algorithm. To compare the performance of various sorting algorithms suitable for vector quantizer, they are realized and synthesized in TSMC 40nm low-power technology. Speed and area results indicate that the proposed algorithm sorts 32 inputs at a 42% faster rate by using 14% fewer components than the perfect shuffle sorter and a 80% slower rate by using 27% fewer components than the Bitonic sorter. The area efficient version sorts 32 inputs at a 21% slower rate by using 32% fewer components than the perfect shuffle sorter.	bitonic sorter;clock signal;low-power broadcasting;noise shaping;parallel algorithm;quantization (signal processing);recursion (computer science);sorting algorithm	Berkin Atila;Burak Kelleci	2017	2017 6th International Conference on Modern Circuits and Systems Technologies (MOCAST)	10.1109/MOCAST.2017.7937665	computer science;theoretical computer science;bitonic sorter;distributed computing;algorithm	EDA	13.566337423666855	43.34400404394747	99684
17da5123f982d506d17e424333e8cea75bb00803	architecture design for h.264/avc integer motion estimation with minimum memory bandwidth	distributed data;cache storage;architectural design;2d adder trees;sum of absolute difference;full search block matching hardware architecture;image matching;data traffic scheduling;2d systolic processor array;digital tv;data reuse;video coding system;vlsi cache storage computational complexity image matching motion estimation parallel processing tree searching video coding;motion estimation;minimum memory bandwidth;indexing terms;variable block size;hardware architecture;chip;single instruction multiple data;memory access;video coding;theoretical analysis;computational complexity;computation complexity;standard definition;h 264 avc integer motion estimation digital tv encoding 2d adder trees single instruction multiple data 2d systolic processor array data traffic scheduling distributed data caching off chip memory bandwidth full search block matching hardware architecture computation complexity video coding system minimum memory bandwidth;vlsi;distributed data caching;tree searching;automatic voltage control motion estimation bandwidth computer architecture two dimensional displays video coding hardware processor scheduling engines digital tv;digital tv encoding;off chip memory bandwidth;computational efficiency;full search block matching;h 264 avc integer motion estimation;memory bandwidth;parallel processing;vlsi architecture	Motion estimation (ME) is the most critical component of a video coding system, and it also dominates the major part of computation complexity and memory bandwidth. For H.264/AVC integer motion estimation (IME), this paper presents a novel memory-access and computation efficient full-search block-matching hardware architecture. With the highest level of on-chip data reuse, one-access for off-chip reference pixels is achieved, and the off-chip memory bandwidth is thus minimized. By distributed data caching and virtual connection of reference picture boundaries, the data traffic scheduling is simple, regular and efficient. The computation engine employs a two-dimensional (2-D) systolic processor array to calculate the absolute differences in single-instruction multiple-data (SIMD) manner, and 2-D adder trees to sum up the absolute differences, all with 100% utilization. The proposed architecture fully supports variable block-size matching of H.264/AVC, and can produce 41 sums of absolute differences (SADs) for one search point every cycle without bubble. The architecture is described in parameterized design, and an implementation for standard-definition digital TV encoding applications is presented. Theoretical analysis and experimental results show that, the proposed architecture can achieve the minimum off-chip memory bandwidth and the maximum computational performance.	adder (electronics);computation;computer memory;data compression;h.264/mpeg-4 avc;input method;memory bandwidth;motion estimation;pixel;processor array;requirement;simd;scheduling (computing);standard-definition television;virtual circuit	Dongxiao Li;Wei Zheng;Ming Zhang	2007	IEEE Transactions on Consumer Electronics	10.1109/TCE.2007.4341585	chip;standard-definition television;parallel processing;parallel computing;real-time computing;index term;simd;telecommunications;computer science;theoretical computer science;operating system;motion estimation;hardware architecture;very-large-scale integration;computational complexity theory;memory bandwidth	EDA	12.022633332714264	39.57409109118472	100090
2c82845fa9de5ef666aab99398e6d15908d1dce7	an embedded multimedia communication terminal based on dsp+fpga	fpga;dsp;embedded multimedia communication terminal	The main content of this paper is the embedded multimedia terminal circuit design and the research and implementation of video image correlation algorithm based on Embedded Multimedia processor. The specific research contents include the design of embedded multimedia design, driving circuit of embedded multimedia DSP and the real-time performance of the algorithm based on the DSP program design. In this paper, the design of multimedia terminal circuit embedded multimedia DSP based on FPGA structure, including the design, multimedia DSP driver design and implementation of video algorithm. The embedded multimedia terminal with embedded multimedia DSP and FPGA structure, multimedia DSP as the main control CPU, responsible for functions such as audio and video collection, video display, signal processing and network communication; FPGA is used to realize the control logic and VGA and composite video switching circuit and video format conversion function. The embedded multimedia terminal has two video input; a video VGA and composite video optional output; stereo audio input and output; Ethernet communication function.	algorithm;central processing unit;circuit design;codec;composite video;const (computer programming);control unit;data compression;debugging;digital signal processor;display device;embedded system;entry point;field-programmable gate array;inline function;input/output;mathematical optimization;multi-core processor;pipeline (computing);power supply;random-access memory;real-time clock;signal processing;software pipelining;subroutine;switching circuit theory;video graphics array;video coding format;video decoder	Li Guo	2016	Multimedia Tools and Applications	10.1007/s11042-016-3597-6	embedded system;real-time computing;computer hardware;computer science;digital signal processing;field-programmable gate array	EDA	11.035159217198856	40.90866115182163	100099
4a8968878c0458a4d59eaf86d67b7725a9318ed0	an efficient dual-mode floating-point multiply-add fused unit	size 0 13 mum efficient dual mode floating point multiply add fused unit maf units processor performance maf functionality dual mode maf architecture double precision operations single precision operations dual path approach floating point addition tsmc;multiplying circuits;low latency;lead;floating point;floating point arithmetic;multiplying circuits floating point arithmetic microprocessor chips;microprocessor chips	Multiply-Add Fused (MAF) units play a key role in the processor's performance for a variety of applications. Aiming at improving the MAF functionality this paper presents a dual-mode MAF architecture, which is able to perform either one double-precision or two single-precision operations in parallel. The design attains low latency by following a dual-path approach and by combining final addition with rounding. The organization performs a MAF instruction in three cycles, while single floating-point addition in two cycles. The design has been validated and implemented with TSMC 0.13um.	double-precision floating-point format;execution unit;multiply–accumulate operation;rounding;single-precision floating-point format	Konstantinos Manolopoulos;Dionysios I. Reisis;Vassilios A. Chouliaras	2010	2010 17th IEEE International Conference on Electronics, Circuits and Systems	10.1109/ICECS.2010.5724440	embedded system;real-time computing;computer hardware;engineering	EDA	11.032474309997392	44.914471052103984	100153
85e6e158169c5a7163a19d2b9de874d45b46b95b	on the time and the bit complexity of distributed randomised anonymous ring colouring	bit complexity;time complexity;probabilistic analysis;randomised distributed graph algorithm;ring;colouring;mellin transform	We present and analyse a very simple randomised distributed vertex colouring algorithm for ring graphs. Its time complexity is log2 n + o(log n) on average and 2log2 n + o(log n) with probability 1− o(n). Each message containing 1 bit we deduce the same values for its bit complexity. Then we compose this algorithm with another and we obtain a 3-colouring algorithm for ring graphs. Thanks to an overlapping, we obtain once more the same values for the time complexities on average and with probability 1− o(n). The same results hold for the bit complexity. These results are obtained using the Mellin transform. We establish lower bounds (on average and with probability 1− o(n)) for the distributed randomised anonymous ring colouring problem. We prove that our algorithms match these lower bounds modulo a negligible additive function (negligible with respect to log2 n). We assume that the ring is anonymous: unique identities are not available to distinguish the processes; we only assume that each vertex distinguishes between its neighbours. Furthermore we do not assume that the size (or an upper bound on the size) of the ring is known.	1-bit architecture;algorithm;binary logarithm;context of computational complexity;fast fourier transform;graph coloring;modulo operation;time complexity;utility functions on indivisible goods	Yves Métivier;John Michael Robson;Nasser Saheb-Djahromi;Akka Zemmari	2013	Theor. Comput. Sci.	10.1016/j.tcs.2012.03.028	time complexity;combinatorics;discrete mathematics;probabilistic analysis of algorithms;computer science;mathematics;algorithm;mellin transform;ring	Theory	20.82092005348172	34.835138923916816	100157
0ce841a7a4150a5f4e76ad9e527493c811e0386d	improved feedback vertex sets in kautz digraphs k (d, n)	directed graphs;acyclic subgraphs;asymptotic formula improved feedback vertex sets kautz digraphs acyclic digraph;feedback vertex set;bayes methods;computational intelligence;acyclic subgraphs feedback vertex set feedback numbers kautz digraphs cycles;kautz digraphs;hypercubes information processing computational intelligence security educational institutions bayes methods approximation methods;cycles;information processing;hypercubes;approximation methods;security;feedback numbers	This paper considers the feedback number f (d, n) of Kautz digraph K (d, n), which is the minimum number of vertices in K (d, n) whose removal results in an acyclic digraph. Xu et al. have determined the exact values of f (d, n) for 1 ≤ n ≤ 7 and obtained an asymptotic formula f (d, n)=d<sup>n</sup>/n + d<sup>n-1</sup>/n-1 + O (nd<sup>n-4</sup>) for n ≥ 8. In this paper, we construct a feedback vertex set of K (d, n) and obtain f (d, n)= d<sup>n</sup>/n +dn-1/n-1 + O (d<sup>n/2</sup>) for n ≥ 8 asymptotically.	directed acyclic graph;directed graph;feedback vertex set	Xirong Xu;Chun Yin;Sijia Zhang;Yazhen Huang	2014	2014 Tenth International Conference on Computational Intelligence and Security	10.1109/CIS.2014.79	combinatorics;discrete mathematics;directed graph;feedback vertex set;information processing;computer science;information security;computational intelligence;mathematics;computer security;algorithm;hypercube	Logic	23.448292185628887	32.711757390755594	100182
0a553a9cd61c67cd69ea8c8ba37a9c6ddabb096d	design of two-level fault-tolerant networks	k static hazards;fault tolerant;sum of prime implicants form;logic element;subcritical faults;fault masking;static hazards;upper bound;design technique;hamming distance;two level realizations;primary input faults;unate functions;fail safe logic;critical faults fail safe logic fault masking hamming distance k static hazards primary input faults static hazards subcritical faults sum of prime implicants form two level realizations unate functions;critical faults	Some new techniques for the synthesis of fault-tolerant two-level combinational networks are presented. Two classes of faults are defined, 1) critical faults and 2) subcritical faults. Critical fauls are the class of faults that cannot be tolerated by any two-level networks. Necessary conditions for synthesis of networks tolerating subcritical faults are developed. As a result it is established that the conditions required for tolerating faults in the logic elements and those required for tolerating faults in the primary inputs are significantly different. Several design techniques are presented and it is shown that if we restrict our class of faults, then certain normally assumed conditions on redundancy can be relaxed. A class of hazards is defined. It is shown that the synthesis of certain hazard-free realizations is equivalent to the fault-tolerant realization, and also an upper bound on the redundancy of the fault-tolerant realization is derived.	combinational logic;fault tolerance;fault-tolerant computer system;global serializability;hazard (computer architecture);redundancy (engineering)	Dhiraj K. Pradhan;Sudhakar M. Reddy	1974	IEEE Transactions on Computers	10.1109/T-C.1974.223775	fault tolerance;real-time computing;hamming distance;computer science;mathematics;upper and lower bounds;algorithm;statistics	Arch	23.53385098892653	45.6853301318112	100538
04ae747285d7484f6299ce5439db90f41955d9b1	synthesis of a class of data format converters with specified delays	logic arrays;discrete wavelet transform;design methodology digital to frequency converters computer architecture discrete wavelet transforms signal synthesis delay signal processing image processing throughput very large scale integration;array signal processing;data format;hardware interface module data format converters specified delays i o sequences image processing signal processing vlsi architecture discrete wavelet transform zig zag scanner area requirements;wavelet transforms;application specific integrated circuits;delay circuits;signal and image processing;vlsi;digital signal processing chips;application specific integrated circuits array signal processing wavelet transforms vlsi delay circuits logic arrays digital signal processing chips;design methodology;vlsi architecture	We propose a design methodology for synthesis of a special class of Data Format Converters (DFCs) in which the site of I/O sequences and the delays between I/O sequences are specified. The need for such DFCs arises in many signal and image processing applications. Our DFCs are based on a two-dimensional architecture. The designs using our methodology have maximum throughput rate and are area-efficient. A VLSI architecture for computing the discrete wavelet transform is given as an example to illustrate the proposed design methodology. The design of a Zig-zag Scanner is also given. For several representative problems, the area requirements of our designs are compared against those obtained by earlier design methodologies. For all the problems considered, our methodology leads to compact designs. >		Jongwoo Bae;Viktor K. Prasanna;Heonchul Park	1994		10.1109/ASAP.1994.331796	multidimensional signal processing;computer vision;real-time computing;design methods;image processing;computer science;theoretical computer science;digital signal processing;digital image processing;application-specific integrated circuit;very-large-scale integration;discrete wavelet transform;wavelet transform	DB	12.378083356390901	43.28066385958614	100559
d442571c5d32fba4e0dd834ecd76fdf00ea7a9d2	an improved column compatibility approach for partition based functional decomposition	optimisation;set theory;partitioning algorithms minimization postal services calculus logic design logic circuits encoding arthritis principal component analysis size measurement;functional decomposition;formal logic;formal logic set theory graph colouring optimisation;mis partition algorithm column compatibility approach partition based functional decomposition compatible pairs k block partition bound set incompatible pairs block partitions partition calculus based approach block compatible approach compatible block partitions compatibility graph clique partition algorithms incompatible block partitions incompatibility graph graph colouring algorithm maximum independent set;graph colouring;maximum independent set	The paper deals with the problem of generating compatible pairs of a k-block partition P(B)=(B/sub 1/, B/sub 2/, ..., B/sub k/), where B is the bound set. Compatible and incompatible pairs of block partitions of P(B) are generated by a partition calculus based approach called the Block Compatible Approach. The compatible block partitions generate a compatibility graph that can be solved using clique partition algorithms. The incompatible block partitions form an incompatibility graph, which can be solved using a graph colouring algorithm or maximum independent set (MIS) partition algorithm.		Venkatesan Muthukumar;Robert J. Bignall;Henry Selvaraj	2000		10.1109/EURMIC.2000.874617	block graph;combinatorics;discrete mathematics;partition refinement;graph partition;frequency partition of a graph;mathematics;domatic number;algorithm;strength of a graph	Logic	21.27328581146133	43.55920269583111	100678
125328cb4d7f41aa1039d14d3b6341973f21b705	a note on cycle embedding in hypercubes with faulty vertices	05xx;tolerancia falta;embedding;hypercube;calcul tolerant les pannes;combinatorics;procesamiento informacion;fault tolerant;algorithm analysis;combinatoria;combinatoire;calculo automatico;computing;calcul automatique;68m15;fault tolerant computing;informatique theorique;graph;fault tolerance;68r10;plongement;information processing;parallel computer;analyse algorithme;inmersion;traitement information;cycle;68r05;tolerance faute;analisis algoritmo;computer theory;informatica teorica;hipercubo	Let fv denote the number of faulty vertices in an n-dimensional hypercube. This note shows that a fault-free cycle of length of at least 2^n-2fv can be embedded in an n-dimensional hypercube with fv=2n-3 and nu003e=5. This result not only enhances the previously best known result, and also answers a question in [J.-S. Fu, Fault-tolerant cycle embedding in the hypercube, Parallel Computing 29 (2003) 821-832].	vertex (geometry);vertex (graph theory)	Zheng-Zhong Du;Jun-Ming Xu	2011	Inf. Process. Lett.	10.1016/j.ipl.2011.03.002	fault tolerance;combinatorics;discrete mathematics;information processing;computer science;mathematics;algorithm	AI	23.90323992838637	34.31265654920483	101116
709fd4a6c2e5a5bbfbae28765b37e85fb750fb36	fpga implementation of a novel area efficient fft scheme using mixed radix fft		In the literature, mixed radix FFT scheme has been proposed to facilitate the computation of FFT in parallel using multiple lower radix FFT modules. Alternately, the speed of the FFT can be increased using Radix-2 decimation-in-frequency (DIF) FFT algorithm with Multipath Delay Commutator (R2MDC) architecture. In this paper, a novel FFT scheme which combines the R2MDC architecture with the serial version of mixed radix FFT scheme is proposed. To study the efficacy of this approach, an 8-point FFT is implemented using R2MDC architecture. Using this, 16-point, 32-point and 64-point FFTs are realized with the serial version of mixed radix scheme and also using only R2MDC architecture on Xilinx Virtex-5 FPGA. From the implementation results, it is found that the hardware requirement for the proposed approach reduces by 25%–53% at the cost of speed compared to the other schemes reported in the literature including that using only R2MDC architecture. The proposed scheme is preferred for low sampling rate applications such as biomedical signal processing.	fast fourier transform;field-programmable gate array	R Thilagavathy;Susmitha Settivari;B. Venkataramani;M. Bhaskar	2017		10.1007/978-981-10-7470-7_9	fast fourier transform;field-programmable gate array;electronic engineering;radix;architecture;multipath propagation;sampling (signal processing);mixed radix;computer science;signal processing	EDA	12.168873200142421	44.339037824254014	101216
d97d4e0bcb7cfef6341f1c8e8cd1d2a686687ab0	n-cube network: node disjoint shortest paths for maximal distance pairs of vertices	parallel and distributed system;hypercube;shortest path;fault tolerant;time complexity;routing;efficient algorithm;interconnection network;pairwise node disjoint shortest paths;shortest path routing;n cube;fault tolerance;routing algorithm;shortest path problem	In parallel and distributed systems many communications take place concurrently, so the routing algorithm as well as the underlying interconnection network play a vital role in delivering all the messages efficiently. Fault tolerance and performance are often obtained by delivering the messages through node disjoint shortest paths. In this paper we present two efficient algorithms to construct, under certain conditions, pairwise node disjoint shortest paths for pairs of vertices in an n-cube in the presence of faulty nodes. The first algorithm has O(m) time complexity, where m is the number of input bits, and the second one takes O(m), but it solves more general problem instances. We also present an efficient algorithm for the extreme version of the edge disjoint shortest paths problem when n is odd. 2004 Elsevier B.V. All rights reserved.	algorithm;cube;distributed computing;fault tolerance;interconnection;maximal set;routing;shortest path problem;time complexity;vertex (geometry)	Teofilo F. Gonzalez;David Serena	2004	Parallel Computing	10.1016/j.parco.2004.07.006	private network-to-network interface;suurballe's algorithm;fault tolerance;combinatorics;constrained shortest path first;floyd–warshall algorithm;equal-cost multi-path routing;average path length;computer science;theoretical computer science;euclidean shortest path;yen's algorithm;mathematics;distributed computing;link-state routing protocol;shortest path problem;distance;k shortest path routing;shortest path faster algorithm;algorithm	Theory	23.247242886427067	34.85491127749091	101361
756a87261c722806e442b632b14ac0bf9eede26c	sensor network self-organization using random graphs	linear algebra;random graph;surveillance;sensor network;random graphs;percolation;self organization;security	RANDOM graph theory originated with seminal work by Erdös and Re nyi in the 1950s. Until then, graph theory analyzed either specific graph instances or deterministically defined graph classes. Erdös and Re nyi considered graph classes where the existence of edges between nodes was determined probabilistically. Their results were theoretically interesting and found applications in many practical domains [1]. Erdös and Re nyi used the same probability value to assign edges between any two nodes in the graph. As an extension to this in the 1990s, Strogatz and Watts studied ‘‘small world’’ graphs [2]. The term small world originates with Milgram’s six degrees of separation model of social networks created in the 1960s. Strogatz and Watts’ work considers networks where the probability of edges existing between nodes is not uniform. They were specifically interested in clustered graphs, where edges are more likely to exist between nodes with common neighbors. To study this phenomenon, they defined classes of pseudo-random graphs. These graphs combine a deterministic structure and a limited number of random edges. Their results have been used to analyze both social networks and technical infrastructures. An alternative approach to studying similar systems has been proposed by Barabási [1]. His group considered the probability distributions of graph node degree found in graph models of existing systems. This analysis shows that the probability of a node having degree d follows an inverse power law (i.e., is proportional to d where is a constant). They also explain how this property can emerge from positive feedback in evolving systems. These models appear to be appropriate for studying a wide range of natural and large-scale technical systems. Important results from this model include quantification of the dependability of the Internet [3], and analysis of computer virus propagation [4]. International Journal of Distributed Sensor Networks, 5: 201–208, 2009 Copyright Taylor & Francis Group, LLC ISSN: 1550-1329 print / 1550-1477 online DOI: 10.1080/15501320802498307	barabási–albert model;book;computer engineering;computer science;computer virus;dependability;emergence;francis;giant component;graph (discrete mathematics);graph theory;international standard serial number;internet;milgram experiment;network security;organizing (structure);percolation theory;percolation threshold;positive feedback;pseudorandomness;random graph;self-organization;six degrees of separation;social network;software propagation;watts humphrey	Richard R. Brooks	2009	IJDSN	10.1080/15501320802498307	network science;random graph;computer science;information security;theoretical computer science;linear algebra;distributed computing;key distribution in wireless sensor networks;complex network;computer network	Theory	20.15137359589609	36.167744632779424	101370
549bc369053cd559da50208debc8f7e821d7d546	netra: a hierarchical and partitionable architecture for computer vision systems	parallelisme;simd;algoritmo paralelo;topology;evaluation performance;index termsnetra;vision ordenador;broadcast capability;flexible architecture;parallel algorithm;memory management;performance evaluation;image processing;partitionable architecture;performance netra partitionable architecture computer vision hierarchical architecture cvs multiprocessor architecture flexible architecture topology tree type control hierarchy broadcast capability simd mimd systolic block level control data flow memory management load balancing scheduling;parallel architectures computer vision;computer architecture computer vision partitioning algorithms image processing artificial intelligence topology tree graphs broadcasting data flow computing memory management;architecture partitionnable;hierarchical architecture;evaluacion prestacion;performance;parallelarchitectures;spectrum;indexing terms;algorithme parallele;computer vision;artificial intelligent;tree graphs;computer architecture;systolic;parallelism;paralelismo;parallel architectures;mimd;architecture parallele;scheduling;tree type control hierarchy;multiprocessor architecture;memorymanagement;control flow;load balancing;graph algorithm;block level control;artificial intelligence;data flow computing;vision ordinateur;load balance;cvs;broadcasting;parallel architecture;data flow;netra;qualitative evaluation;partitioning algorithms	Computer vision is regarded as one of the most complex and computationally intensive problems. In general, a Computer Vision System (CVS) attempts to relate scene(s) in terms of model(s). A typical CVS employs algorithms from a very broad spectrum such as such as numerical, image processing, graph algorithms, symbolic processing, and artificial intelligence. This paper presents a multiprocessor architecture+ called “NETRA,” for computer vision systems. NETFLA is a highly flexible architecture. The topology of NETRA is recursively defined, and hence, is easily scalable from small to large systems. It is a hierarchical architecture with a tree-type control hierarchy. Its leaf nodes consists of a cluster of processors connected with a programmable crossbar with selective broadcast capability to provide the desired flexibility. The processors in clusters can operate in SIMD-, MIMDor Systolic-like modes. Other features of the architecture include integration of limited data-driven computation within a primarily control flow mechanism, blocklevel control and data flow, decentralization of memory management functions, and hierarchical load balancing and scheduling capabilities. This paper also presents a qualitative evaluation and preliminary performance results of a cluster of NETRA.	algorithm;artificial intelligence;central processing unit;computer vision;concurrent versions system;control flow;crossbar switch;dataflow;disk partitioning;graph theory;image processing;load balancing (computing);memory management;multiprocessing;numerical analysis;recursion;recursive definition;scalability;scheduling (computing);symbolic computation;tree (data structure)	Alok N. Choudhary;Janak H. Patel;Narendra Ahuja	1993	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.246071	computer architecture;parallel computing;image processing;computer science;load balancing;operating system;database;distributed computing	Arch	10.806443719111194	35.71628619907714	102015
1dd44f29ee696d9ab8190e351d648545a4256dca	conditional fault diagnosis of hierarchical hypercubes	fault tolerant;multiprocessor systems;05c40;diagnosability;2010 ams subject classifications;fault tolerance;comparison diagnosis;ams subject classification;connected component;05c90;fault diagnosis;hierarchical hypercubes	This article may be used for research, teaching, and private study purposes. Any substantial or systematic reproduction, redistribution, reselling, loan, sub-licensing, systematic supply, or distribution in any form to anyone is expressly forbidden. The publisher does not give any warranty express or implied or make any representation that the contents will be complete or accurate or up to date. The accuracy of any instructions, formulae, and drug doses should be independently verified with primary sources. The publisher shall not be liable for any loss, actions, claims, proceedings, demand, or costs or damages whatsoever or howsoever caused arising directly or indirectly in connection with or arising out of the use of this material. The design of large dependable multiprocessor systems requires quick and precise mechanisms for detecting the faulty nodes. The system-level fault diagnosis is the process of identifying faulty processors in a system through testing. This paper shows that the largest connected component of the survival graph contains almost all remaining vertices in the hierarchical hypercube HHC n when the number of faulty vertices is up to two or three times of the traditional connectivity. Based on this fault resiliency, we establish that the conditional diagnosability of HHC n (n = 2 m + m, m ≥ 2) under the comparison model is 3m − 2, which is about three times of the traditional diagnosability.	central processing unit;complex network;connected component (graph theory);dependability;multiprocessing;primary source;sensor;vertex (geometry)	Shuming Zhou;Limei Lin;Jun-Ming Xu	2012	Int. J. Comput. Math.	10.1080/00207160.2012.710325	fault tolerance;parallel computing;real-time computing;computer science;mathematics;distributed computing	Theory	23.861512088367768	44.403305313934055	102042
99165be94e00534f7acccbd19b9d3ba6dd1905ad	modelling, analysis and parallel implementation of an on-line video encoder	real time;multimedia application;multimedia systems;smp mpeg 4 real time encoding mapping and scheduling;mpeg 4 standard transform coding encoding parallel processing video compression job shop scheduling processor scheduling image coding high definition video mpeg standards;video coding;mapping and scheduling;mpeg 4;smp;parallel implementation;multimedia systems video coding encoding;real time encoding on line video encoder video encoding real time multimedia application mpeg 4 parallelism scheduling industrial video encoder smp;encoding;real time encoding	Video encoding is a fundamental component of a wide range of real-time multimedia applications. In this paper, we present the fine grain MPEG-4 parallelism and describe a modelling, mapping and scheduling approach that produces code for an industrial video encoder on SMP platforms.	code generation (compiler);compiler;dynamic data;encoder;experiment;heuristic;macroblock;message passing interface;online and offline;parallel computing;performance;programmer;real-time clock;scheduling (computing);symmetric multiprocessing	Ismail Assayad;Philippe Gerner;Sergio Yovine;Valérie Bertin	2005	First International Conference on Distributed Frameworks for Multimedia Applications	10.1109/DFMA.2005.37	parallel computing;real-time computing;h.263;computer science;operating system;video tracking;multimedia;video processing;symmetric multiprocessing;context-adaptive binary arithmetic coding;mpeg-4;h.261;encoding;multiview video coding	HPC	11.245747811780133	39.6459640892775	102090
b75d3a1472ad09142472c865ce26c9808a0767d4	architecture design and vlsi hardware implementation of image encryption/decryption system using re-configurable 2d von neumann cellular automata	von neumann cellular automata;reconfigurable cellular automata;very large scale integration hardware cryptography streaming media control systems pixel two dimensional displays communication system control multimedia systems communication system security;image processing equipment;encryption scheme;reconfigurable architectures;very large scale integration;random sequences;2d cellular automata;27 74 mw;integrated circuit design;tsmc microcell library;cryptography;0 18 micron;random sequence;image decryption system;vlsi;image encryption system;100 mhz;progressive cellular automata;vlsi hardware implementation;0 18 micron vlsi hardware implementation image encryption system image decryption system reconfigurable cellular automata 2d cellular automata von neumann cellular automata encryption scheme progressive cellular automata ca substitution random sequence cadence tools circuit synthesis synopsys tools tsmc microcell library 27 74 mw 100 mhz;vlsi cellular automata cryptography image processing equipment integrated circuit design random sequences reconfigurable architectures;ca substitution;cellular automata;synopsys tools;circuit synthesis;cadence tools	The first architecture design and VLSI hardware implementation of image encryption/decryption system using re-configurable two-dimensional (2D) von Neumann cellular automata (CA) is presented in this paper. Its encryption scheme is based on replacement of the pixel values using a progressive cellular automata (CA) substitution. In our scheme, we used the re-configurable 2D von Neumann CA to generate high quality random sequence as key stream. To enhance the flexibility of our system, we used 16times16 re-configurable 2D von Neumann CA which produces 1 set (256)16times16 CA, or concurrently produces 4 sets (64/set) 8times8 CA and 16 sets (16/set) 4times4 C A, respectively. We have accomplished simulations of our image encryption/decryption system by using CADENCE tools. We also have completed the circuit synthesis using the SYNOPSYS tools with the TSMC 0.18mum cell-library. The area size was 15.6816 mm2, and the maximum operation frequency was 100 MHz with 27.74 mW total dynamic power. It shows that the architecture of the proposed image encryption/decryption system is suitable for VLSI realization	automata theory;display resolution;encryption;pixel;simulation;very-large-scale integration;von neumann cellular automaton	Rong-Jian Chen;Yi-Te Lai;Jui-Lin Lai	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1692545	cellular automaton;embedded system;electronic engineering;computer science;theoretical computer science;mathematics;very-large-scale integration;algorithm;statistics	EDA	10.60880717095425	42.514693578320376	102322
ec65d7d5e59e74537a1c0a1c6d73c8dd02e3ab73	evolutionary graph generation system and its application to bit-serial arithmetic circuit synthesis	circuito aritmetico;generation graphe evolutionniste;diseno circuito;synthese circuit;appel procedure;systeme aide decision;real time;circuit design;sistema ayuda decision;multigraph;grafo;decision support system;llamada procedimiento;multigrafo;temps reel;graph;graphe;success rate;tiempo real;sintesis circuito;algorithme evolutionniste;algoritmo evolucionista;conception circuit;evolutionary algorithm;multigraphe;evolutionary optimization;procedure call;circuit synthesis;circuit arithmetique;arithmetic circuit	This paper presents an efficient graph-based evolutionary optimization technique called Evolutionary Graph Generation (EGG), and its application to the design of bit-serial arithmetic circuits, which frequently appear in real-time DSP architectures. The potential of the proposed approach is examined through experimental synthesis of bit-serial constant-coefficient multipliers. A new version of the EGG system can generate the optimal bit-serial multipliers of 8-bit coefficients with a 100% success rate in 15 minutes on an average.	arithmetic circuit complexity	Makoto Motegi;Naofumi Homma;Takafumi Aoki;Tatsuo Higuchi	2002		10.1007/3-540-45712-7_80	decision support system;computer science;artificial intelligence;multigraph;evolutionary algorithm;circuit design;graph;algorithm	EDA	17.02402686931181	39.790657248204475	102390
a8f1ffb1f7590c3fd49e01ead3fdd728612063c5	a combined decimal and binary floating-point multiplier	combined decimal floating point multiplier design;binary encoded decimal floating point;encoding decoding;multiplying circuits;binary floating point multiplier;decoding;logic design;application software;hardware reuse;rounding logic combined decimal floating point multiplier design binary floating point multiplier hardware design ieee 754 2008 floating point standard binary encoded decimal floating point right shifter exponent calculation logic;hardware encoding delay estimation algorithm design and analysis computer architecture logic design decoding digital arithmetic application software design optimization;data mining;design optimization;multiplying circuits floating point arithmetic logic design;computer architecture;computer arithmetic;commercial applications;rounding logic;ieee 754 2008;decimal floating point;critical path;ieee 754 2008 floating point standard;hardware design;digital arithmetic;floating point;right shifter;floating point arithmetic;register transfer level implementation;commercial applications ieee 754 2008 multiplication floating point computer arithmetic hardware decimal floating point hardware reuse register transfer level implementation;table lookup;multiplication;encoding;register transfer level;exponent calculation logic;algorithm design and analysis;delay estimation;hardware	In this paper, we describe the first hardware design of a combined binary and decimal floating-point multiplier, based on specifications in the IEEE 754-2008 Floating-point Standard. The multiplier design operates on either (1) 64-bit binary encoded decimal floating-point (DFP) numbers or (2) 64-bit binary floating-point (BFP) numbers. It returns properly rounded results for the rounding modes specified in IEEE 754-2008. The design shares the following hardware resources between the two floating-point datatypes: a 54-bit by 54-bit binary multiplier, portions of the operand encoding/decoding, a 54-bit right shifter, exponent calculation logic, and rounding logic. Our synthesis results show that hardware sharing is feasible and has a reasonable impact on area, latency, and delay. The combined BFP and DFP multiplier occupies only 58% of the total area that would be required by separate BFP and DFP units. Furthermore, the critical path delay of a combined multiplier has a negligible increase over a standalone DFP multiplier, without increasing the number of cycles to perform either BFP or DFP multiplication.	64-bit computing;algorithm;binary integer decimal;binary multiplier;bitwise operation;critical path method;datapath;decimal floating point;delay line memory;encoder;operand;rounding	Charles Tsen;Sonia Gonzalez-Navarro;Michael J. Schulte;Brian J. Hickmann;Katherine Compton	2009	2009 20th IEEE International Conference on Application-specific Systems, Architectures and Processors	10.1109/ASAP.2009.28	parallel computing;computer science;floating point;theoretical computer science;operating system;decimal floating point;binary integer decimal	EDA	11.708586648412442	45.66700343182291	102498
4a3ab2c341fa362d784e8e9a6a656f05e02b17c7	low power motion estimation design using adaptive pixel truncation	data compression;videotelephony;motion estimation energy consumption video compression discrete cosine transforms bit rate permission redundancy quantization hardware costs;motion estimation;video coding;low power;adaptive signal processing;motion vector;power consumption;data compression motion estimation adaptive signal processing videotelephony video coding;power consumption low power motion estimation design adaptive pixel truncation portable video applications portable videophone video encoder pixel resolution lsbs picture quality	Power consumption is very critical for portable video applications such as portable video-phone. Motion estimation in the video encoder requires huge amount of computation and hence consumes the largest portion of power. In this paper we propose a novel method of reducing power consumption of the motion estimation by adaptively changing the pixel resolution during the computation of the motion vector. The pixel resolution is changed by masking or truncating the LSBs of the pixel data which is governed by an adaptive mechanism. Experimental results show that on average more than ,j bits can be truncated without affecting the picture quality. This results in an average 70% reduction in power consumption.	computation;encoder;image quality;motion estimation;pixel;truncation	Zhong-Li He;Kai-Keung Chan;Chi-Ying Tsui;Ming Lei Liou	1997		10.1145/263272.263317	data compression;adaptive filter;quarter-pixel motion;computer science;motion estimation;block-matching algorithm;videotelephony;rate–distortion optimization;motion compensation	EDA	13.027360720683317	40.5109831809629	102518
b18c21ce470fd01d1b906b9b2efdbedee74509d3	parallel reconfigurable computing-based mapping algorithm for motion estimation in advanced video coding	graph theory;reconfigurable computing;ncku 成功大學 成大 圖書館 機構典藏;video coding;multicore;dissertations and theses journal referred papers conference papers nsc reserach report patent nckur ir ncku institutional repostiory 博碩士論文 期刊論文 國科會研究報告 專利 成大機構典藏	Computational load of motion estimation in advanced video coding (AVC) standard is significantly high and even worse for HDTV and super-resolution sequences. In this article, a video processing algorithm is dynamically mapped onto a new parallel reconfigurable computing (PRC) architecture which consists of multiple dynamic reconfigurable computing (DRC) units. First, we construct a directed acyclic graph (DAG) to represent video coding algorithms in which motion estimation is the focus. A novel parallel partition approach is then proposed to map motion estimation DAG onto the multiple DRC units in a PRC system. This partitioning algorithm is capable of design optimization of parallel processing reconfigurable systems for a given number of processing elements in different search ranges. This speeds up the video processing with minimum sacrifice.	algorithm;data compression;directed acyclic graph;h.264/mpeg-4 avc;mathematical optimization;motion estimation;multidisciplinary design optimization;parallel computing;reconfigurable computing;super-resolution imaging;video processing	Anand Paul;Yung-Chuan Jiang;Jhing-Fa Wang;Jar-Ferr Yang	2012	ACM Trans. Embedded Comput. Syst.	10.1145/2331147.2331149	multi-core processor;embedded system;parallel computing;real-time computing;reconfigurable computing;computer science;graph theory;theoretical computer science;operating system	Graphics	12.107688572386405	40.222644635922045	102627
68f782e8135f726f27ca05e65e29ab5c323f7a4b	an efficient architecture for jpeg2000 coprocessor	discrete wavelet transforms;quad code block;internal tile memory size jpeg2000 coprocessor still image compression single coding stream quad code block dwt method qcb based dwt engine;dwt;coprocessors image coding discrete wavelet transforms transform coding parallel processing computer architecture streaming media consumer electronics engines;image coding;data compression;block codes image coding data compression discrete wavelet transforms;fixed time;consumer electronics;indexing terms;memory access;image compression;ebcot;jpeg2000;compression ratio;article;block codes;code block;internal standard	JPEG2000 is a new international standard for still image compression. It provides various functions in one single coding stream and the better compression quality than the traditional JPEG, especially in the high compression ratio. However, the heavy computation and large internal memory requirement still restrict the consumer electronics applications. In this paper, we propose a QCB (quad code block)-based DWT method to achieve the higher parallelism than the traditional DWT approach of JPEG2000 coding process. Based on the QCB-based DWT engine, three code blocks can be completely generated after every fixed time slice recursively. Thus, the DWT and EBCOT processors can process simultaneously and the high computational EBCOT then has the higher parallelism of the JPEG2000 encoding system. By changing the output timing of the DWT process and parallelizing with EBCOT, the internal tile memory size can be reduced by a factor of 4. The memory access cycles between the internal tile memory and the code block memory also decrease with the smooth encoding flow.	automatic parallelization;block (programming);central processing unit;code::blocks;computation;computer data storage;coprocessor;discrete wavelet transform;elegant degradation;entropy encoding;image compression;jpeg 2000;parallel computing;preemption (computing);recursion;throughput	Bing-Fei Wu;Chung-Fu Lin	2004	IEEE Transactions on Consumer Electronics	10.1109/TCE.2004.1362517	data compression;block code;parallel computing;index term;computer hardware;image compression;computer science;theoretical computer science;compression ratio;internal standard;jpeg 2000;statistics	HPC	12.13430169062599	39.190880827530734	102682
4ae307d259a8d1dfa2fae8bf6fd101fbe4e681c9	scalable video coding deblocking filter fpga and asic implementation using high-level synthesis methodology	performance evaluation;resource allocation;hardware accelerator;fpga;video coding;embedded systems;high level synthesis;power aware computing;deblocking filter;system on chip;field programmable gate arrays decoding scalability random access memory hardware information filtering;frequency 100 mhz frame generation frame visualization tft screen df core general purpose memory controller slave core gpmc slave core powerpc440 embedded processor locallink interfaces local memory information storage virtex5 fx70t device umc cmos technology soc embedded powerpc processor clock cycles macroblock processing logic synthesis opensvc source code avc df model power utilization resource utilization cycle time latency performance measurements rtl microarchitecture high level synthesis methodology systemc functional model design flow temporal scalability spatial scalability cif video formats qcif video formats h 264 svc video decoder asic implementation fpga implementation scalable video coding deblocking filter size 65 nm;video codecs;asic;field programmable gate arrays;user interfaces;video coding dram chips embedded systems field programmable gate arrays filtering theory high level synthesis performance evaluation power aware computing resource allocation source coding system on chip user interfaces video codecs;dram chips;filtering theory;h 264 svc deblocking filter high level synthesis systemc fpga asic hardware accelerator;h 264 svc;source coding;systemc	This paper describes key concepts in the design and implementation of a deblocking filter (DF) for a H.264/SVC video decoder. The DF supports QCIF and CIF video formats with temporal and spatial scalability. The design flow starts from a SystemC functional model and has been refined using high-level synthesis methodology to RTL micro architecture. The process is guided with performance measurements (latency, cycle time, power, resource utilization) with the objective of assuring the quality of results of the final system. The functional model of the DF is created in an incremental way from the AVC DF model using OpenSVC source code as reference. The design flow continues with the logic synthesis and the implementation on the FPGA using various strategies. The FPGA implementation is capable to run at 100 MHz, and macro blocks are processed in 6, 500 clock cycles for a throughput of 130 fps for QCIF format and 37 fps for CIF format. A validation platform has been developed using the embedded PowerPC processor in the FPGA, composing a SoC that integrates the tasks for frame generation and visualization on a TFT screen. The FPGA implements both the DF core and a General Purpose Memory Controller (GPMC) slave core. Both cores are connected to the PowerPC440 embedded processor using Local Link interfaces. The FPGA also contains a local memory capable of storing information necessary to filter a complete frame and to store a decoded picture frame. The complete system is implemented in a Virtex5 FX70T device. An ASIC implementation of the deblocking filter has been done using UMC CMOS 65nm technology. The ASIC implementation is running at 181.8 MHz, occupying an area of 596, 392.4 μm2.	application-specific integrated circuit;cmos;clock signal;crystallographic information file;deblocking filter;design flow (eda);direction finding;embedded system;field-programmable gate array;function model;group policy;h.264/mpeg-4 avc;high- and low-level;high-level synthesis;logic synthesis;memory controller;picture-in-picture;powerpc;quality of results;scalability;scalable video coding;server message block;system on a chip;systemc;thin-film-transistor liquid-crystal display;throughput;umc (computer);video decoder;video file format	Pedro P. Carballo;Omar Espino;Romen Neris;Pedro Hernandez-Fernandez;Tomasz Szydzik;Antonio Núñez	2013	2013 Euromicro Conference on Digital System Design	10.1109/DSD.2013.52	embedded system;parallel computing;real-time computing;computer science;operating system;fpga prototype;field-programmable gate array	EDA	11.42961554416875	40.73401842983409	102835
288fca14dcd0acff5bd55c2f4803486fee2b89ef	audio-video terminal system-on-chip simulation	video streaming digital signal processing chips mobile communication multimedia communication system on chip video coding;abstracts biological system modeling streaming media;maxsim soc simulation environment audio video terminal system on chip simulation audio video encoding terminal multimedia mobile terminal interdevice communications video stream compression standard protocol vliw sim instruction set simulator iss very long instruction word architectures	In this paper, a system-on-chip design and simulation of an Audio-Video encoding terminal has been described. The purpose of this work was to model a multimedia mobile terminal in order to preliminarily explore system bottlenecks and inter-device communications. The system is based on two ST210[1] processors working in parallel, one dedicated to the compression of a video stream following the ITU-T H.263 [2] standard protocol, and the other one executing the ITU-T G.723 [3] speech compression. Data generated by the two processors are multiplexed together in an H.223-like [4] format. The ST210 cores are simulated using the VLIW-SIM [5][6] environment targeted for ST210 architecture. VLIW-SIM is a retargettable Instruction Set Simulator (ISS), pipeline and cycle accurate able to model state of the art VLIW (Very Long Instruction Word) architectures. The complete System has been simulated using the MaxSim SoC simulation environment[7][8]. Simulation test results for the complete system are reported.	algorithm;central processing unit;computer architecture simulator;computer terminal;data compression;design tool;g.723;instruction set simulator;mathematical optimization;mobile phone;multiplexing;multiprocessing;performance per watt;sensor;simulation;speech coding;streaming media;system on a chip;very long instruction word	Ivano Barbieri;Massimo Bariani;Marco Raggio;Alessandro Scotto	2004	2004 12th European Signal Processing Conference		embedded system;real-time computing;computer hardware;computer science	EDA	11.083762700726185	40.98962860226748	103410
dc6a27c4ae55bf16d2388cd83f9d54c51c1c90ef	a compact design of n-bit ripple carry adder circuit using qca architecture	wiring logic;full adder;network synthesis adders logic gates;logic gates dh hemts computer architecture microprocessors adders wires;n bit ripple carry adder;full adder compact design n bit ripple carry adder circuit 3 dot based qca architecture wiring logics basic gates xor gate qca 3 dot cell qca 3 dot based half adder;3 dot;n bit ripple carry adder qca 3 dot half adder full adder wiring logic;qca;half adder	In this paper, we propose a new 3-dot based QCA architecture. We also present wiring logics, basic gates and XOR gate using proposed QCA 3-dot cell. All these components are very much efficient to compute basic operations. Moreover, we design a QCA 3-dot based half adder and extend the half adder into a full adder which has least number of QCA cells till now. We also design a powerful and efficient n-bit ripple carry adder applying our new and novel scheme. Our proposed circuits perform much better than the existing ones, e.g., the proposed 32-bit ripple carry adder circuit improves 21% on QCA cell, 99% on area than the existing best known one.	32-bit;adder (electronics);brent's method;desi-iii;exclusive or;qualitative comparative analysis;ripple effect;wiring;xor gate	Tania Sultana;Rajon Bardhan;Tangina Firoz Bithee;Zinia Tabassum;Nusrat Jahan Lisa	2015	2015 IEEE/ACIS 14th International Conference on Computer and Information Science (ICIS)	10.1109/ICIS.2015.7166586	computer architecture;parallel computing;serial binary adder;carry-save adder;adder	EDA	16.1954621274635	45.40909603770203	103886
18fb6b334b2702214d76e8a08059321ca4e9af3e	fine-tuning accuracy using conditional probability of the bottom sign-bit in fixed-width modified booth multiplier		In this paper, two methods for fine-tuning the accuracy of bias-estimation with conditional probability of the bottom signed bit (BSB) for fixed-width two’s complement modified Booth multiplier are proposed. We calculate the conditional probability between the BSB and partial product elements including other signed bits to estimate the expected value of truncation part. This makes fine-tuning of accuracy instead of column-wise coarse-tuning in fixed-width multipliers possible. We propose two methods based on the BSB to estimate and compensate the fixed-width multiplier for minimization of truncation error. Two proposed methods can improve the signal-to-noise (SNR) with a small area penalty in fixed-width multiplier. Statistics on the SNR were analyzed, and the hardware performance results validate the effectiveness of the proposed methods. In real-world implementation, compared with the discrete cosine transform (DCT) core using direct-truncated (D-T) multipliers, the DCT cores using the proposed BSB_AE and BSB_HC multipliers increase average peak signal-to-noise ratio (PSNR) by 27 and 31% with only a 6 and 7% area penalty, respectively.	andrew donald booth;sign bit	Yuan-Ho Chen;Chung-Yi Li;Luhua Lai	2018	CSSP	10.1007/s00034-017-0702-7	control theory;mathematical optimization;mathematics;fine-tuning;truncation;discrete cosine transform;conditional probability;expected value;booth's multiplication algorithm;sign bit;truncation error	Vision	14.674258289126852	42.547658552717294	103907
1ede991312fd565f4fcb4afe813c203fc27b4a8f	mixed-signal cvns adder for two-operand binary addition	conversion circuits;continuous valued number system radix two operand binary addition continuous valued number system 16 bit mixed signal adder analog signal processing units digital blocks buffers analog circuits binary digits current mode circuitry conversion circuits;digital signal processing;signal generators;complexity theory;neural networks;digital arithmetic adders;analog signal processing;buffers;digital blocks;signal design;current mode;continuous valued number system;data mining;binary digits;wireless communication;analog circuits;16 bit mixed signal adder;artificial neural networks;adders signal processing arithmetic integrated circuit interconnections neural networks digital signal processing analog circuits signal design signal generators wireless communication;logic gates;adders;signal processing;integrated circuit interconnections;two operand binary addition;arithmetic;digital arithmetic;analog signal processing units;current mode circuitry;continuous valued number system radix	The Continuous Valued Number System (CVNS) has been used to implement a 16-bit mixed-signal adder for performing two-operand binary additions. The adder takes advantage of higher speed of operation of analog signal processing units, while digital blocks have been used at the output of the adder to provide better driving capability by inserting buffers. The analog circuits, process and determine the existence of carry signal on a group of binary inputs. Moreover, analog signals are used for addition over a group of binary digits, using current mode circuitry, to reduce the area and increase the speed of operations. In this paper, design of a mixed signal CVNS adder is proposed, which is used for two operand binary addition. The CVNS radix is chosen low, equal to 2, to reduce the complexity of conversion circuits between the binary and CVNS system.	16-bit;adder (electronics);analog signal processing;analogue electronics;arithmetic logic unit;binary number;bitwise operation;boolean algebra;electronic circuit;mixed-signal integrated circuit;operand	Mitra Mirhassani	2009	2009 IEEE International Conference on Electro/Information Technology	10.1109/EIT.2009.5189616	analog signal processing;computer vision;electronic engineering;parallel computing;logic gate;analogue electronics;computer science;electrical engineering;theoretical computer science;digital signal processing;machine learning;serial binary adder;carry-save adder;artificial neural network;adder;wireless;signal generator	EDA	13.554391473543806	44.91753806529857	104449
9ef39e8de69677fb532837e4aedcb0b00f8536bd	using variable-entered karnaugh maps to produce compact parametric general solutions of boolean equations	06e30;particular solutions;parametric general solutions;variable entered karnaugh maps;94c10;03g05;boolean function;boolean equations;boolean algebra;general solution;08b05;03e35;finite boolean algebras;b 6 3	The variable-entered Karnaugh map (VEKM) is shown to be the natural map for representing and manipulating general 'big' Boolean functions that are not restricted to the switching or two-valued case. The VEKM is utilized herein in producing a compact general solution of a system of Boolean equations. It serves as a powerful manual tool for function inversion, implementation of the solution procedure, handling don't-care conditions and minimization of the final expressions. The rules of using the VEKM are semi-algebraic and collective in nature, and hence are much easier to state, remember and implement than are the tabular and per-cell rules of classical maps. As a result, the maps used are significantly smaller than those required by classical methods. As an offshoot, the paper contributes some pictorial insight into the representation of 'big' Boolean algebras and functions. It also predicts the correct number of particular solutions of a Boolean equation, and produces an exhaustive list of particular solutions. Details of the method are carefully explained and further demonstrated via an illustrative example.	karnaugh map	Ali Muhammad Ali Rushdi;Motaz H. Amashah	2011	Int. J. Comput. Math.	10.1080/00207160.2011.594505	boolean algebra;boolean circuit;karnaugh map;and-inverter graph;mathematical optimization;combinatorics;circuit minimization for boolean functions;mathematical analysis;discrete mathematics;reed–muller expansion;boolean network;boolean domain;quine–mccluskey algorithm;boolean expression;product term;standard boolean model;maximum satisfiability problem;calculus;boolean algebras canonically defined;mathematics;boolean function;complete boolean algebra;algorithm;two-element boolean algebra;free boolean algebra;parity function;algebra	Theory	20.36340810513694	43.251737167064675	104502
966c303887acbb6c0a775960c5e6b0db5bd1fad7	the tightly super 3-extra connectivity and 3-extra diagnosability of crossed cubes		Many multiprocessor systems have interconnection networks as underlying topologies and an interconnection network is usually represented by a graph where nodes represent processors and links represent communication links between processors. In 2016, Zhang et al. proposed the g-extra diagnosability of G, which restrains that every component of G − S has at least (g + 1) vertices. As an important variant of the hypercube, the n-dimensional crossed cube CQn has many good properties. In this paper, we prove that CQn is tightly (4n − 9) super 3-extra connected for n ≥ 7 and the 3-extra diagnosability of CQn is 4n − 6 under the PMC model (n ≥ 5) and MM∗ model (n ≥ 7).	central processing unit;google+;graph (discrete mathematics);interconnection;marching cubes;multiprocessing;olap cube	Shiying Wang;Xiaolei Ma	2017	CoRR		discrete mathematics;cube;mathematics;network topology;vertex (geometry);interconnection;multiprocessing;hypercube;graph	Arch	23.527690503095165	35.20554884529908	104661
b191426efb25abe4655b15e8163c2b6f5147ef0c	image segmentation on a 2d array by a directed split and merge procedure	processing element;3 micron parallel processing directed split and merge procedure 2d array implementation image segmentation nearest neighbor communications processing elements arithmetic operations comparison addition double metal cmos technology;traitement signal;image segmentation merging cmos technology bandwidth parallel algorithms image processing two dimensional displays costs arithmetic communication system control;image segmentation;image processing;procesamiento imagen;segmentation;traitement image;data association;chip;algorithme;algorithm;technology scaling;parallel architectures;signal processing;linear time;nearest neighbor;communication cost;digital signal processing chips;parallel architectures digital signal processing chips image segmentation;procesamiento senal;segmentacion;algoritmo	A 2D array implementation of image segmentation by a directed split and merge procedure is proposed. Parallelism is realized on two levels: one within the split and merge operations, where more than one merge (or split) may proceed concurrently, and the second between the split and merge operations, where several splits may be performed in parallel with merges. Both the split and merge operations are based on nearest neighbor communications between the processing elements (PEs), and facilitating low communication costs. The basic arithmetic operations required to perform split and merge are comparison and addition, allowing a simple structure of the PE as well as a hardwired control. A local of 512 bytes is sufficient to hold the interim data associated with each PE. A prototype PE has been constructed using 3 mu m double-metal CMOS technology. Scaling up to 0.8 mu m, it is possible to incorporate 32 PEs on a 5 cm/sup 2/ chip. With sufficiently large PE window sizes, image segmentation can be achieved in linear time. >	image segmentation	Akhilesh Tyagi;Magdy A. Bayoumi	1992	IEEE Trans. Signal Processing	10.1109/78.165668	chip;time complexity;embedded system;computer vision;parallel computing;image processing;computer science;theoretical computer science;signal processing;merge algorithm;image segmentation;segmentation;k-nearest neighbors algorithm;polyphase merge sort	Robotics	13.407965005801925	37.87218027506017	104750
ddac37806b4361e592561e7978ee6f0c49d1fd8c	high-performance fir filter design based on sharing multiplication	0 35 micron high performance fir filter design computation sharing multiplier finite impulse response filter sharing multiplication high speed designs vector scalar products power delay product voltage scaling power consumption reduction;nonlinear filters;digital signal processing;filtering;circuito aritmetico;evaluation performance;multiplier;scalar product;performance evaluation;high performance fir filter design;multiplying circuits;integrated circuit;filtre reponse impulsion finie;implementation;evaluacion prestacion;finite impulse response filter;high speed designs;circuito integrado;indexing terms;ejecucion;filtro respuesta impulsion acabada;finite impulse response;multiplicateur;cmos digital integrated circuits;vector scalar products;energy consumption;computational complexity;fir filter;digital filters;voltage;0 35 micron;explosives;low power electronics;sharing multiplication;vlsi;computation sharing;digital arithmetic;low power electronics fir filters digital filters vlsi digital arithmetic multiplying circuits cmos digital integrated circuits high speed integrated circuits;fir filters;computation sharing multiplier;finite impulse response filter filtering digital signal processing delay voltage energy consumption frequency explosives computational complexity nonlinear filters;power delay product;power consumption;consommation energie electrique;frequency;voltage scaling;high performance;high performance finite impulse response fir filter;high speed;high speed integrated circuits;power consumption reduction;multiplicador;circuit arithmetique;circuit integre;arithmetic circuit	Finite impulse response (FIR) filtering can be expressed as multiplications of vectors by scalars. We present high-speed designs for FIR filters based on a computation sharing multiplier which specifically targets computation re-use in vector-scalar products. The performance of the proposed implementation is compared with implementations based on carry-save and Wallace tree multipliers in 0.35-/spl mu/m technology. We show that sharing multiplier scheme improves speed by approximately 52 and 33% with respect to the FIR filter implementations based on the carry-save multiplier and Wallace tree multiplier, respectively. In addition, sharing multiplier scheme has a relatively small power delay product than other multiplier schemes. Using voltage scaling, power consumption of the FIR filter based on computation sharing multiplier can be reduced to 41% of the FIR filter based on the Wallace tree multiplier for the same frequency of operation.	filter design;finite impulse response	Jongsun Park;Khurram Muhammad;Kaushik Roy	2003	IEEE Trans. VLSI Syst.	10.1109/TVLSI.2002.800529	electronic engineering;computer science;electrical engineering;finite impulse response;control theory;mathematics	EDA	13.7024580476409	45.81278944708551	104937
fbc287701a5179c2a74d1580b486591998cf68de	mpsoc architecture for macro blocks line partitioning of h.264/avc encoder	soclib;h 264 avc;data parallelism;intra prediction;high definition;mpsoc	Using multiprocessor technology is an interesting solution for reducing the processing time of complex video encoders such as H.264/Advanced Video Coding (AVC). This paper details different levels of parallelism presented in related works for H.264/AVC encoder. An efficient Macro Blocks Line level parallelism for the intra prediction encoding chain of H.264/AVC for High Definition (HD) video is proposed. It is implemented on MPSoC architecture using an open and free platform for virtual prototyping named SoCLib. Comparing to related works, the proposed partitioning meets strongly the size of required memory constraint and provides an interesting speed-up. The proposed architecture is based on three processors and ensures a reduced circuit area. Experimental results reveal a run time saving of about 59.8% in terms of processing speed, compared to a classical execution based on a single CPU, without affecting the quality of the reconstructed video. Using multiprocessor technology is an interesting solution for reducing the processing time of complex video encoders such as H.264/Advanced Video Coding (AVC). This paper details different levels of parallelism presented in related works for H.264/AVC encoder. An efficient Macro Blocks Line level parallelism for the intra prediction encoding chain of H.264/AVC for High Definition (HD) video is proposed. It is implemented on MPSoC architecture using an open and free platform for virtual prototyping named SoCLib. Comparing to related works, the proposed partitioning meets strongly the size of required memory constraint and provides an interesting speed-up. The proposed architecture is based on three processors and ensures a reduced circuit area. Experimental results reveal a run time saving of about 59.8% in terms of processing speed, compared to a classical execution based on a single CPU, without affecting the quality of the reconstructed video. MPSoC Architecture for Macro Blocks Line Partitioning of H.264/AVC Encoder		Nidhameddine Belhadj;Zied Marrakchi;Mohamed Ali Ben Ayed;Nouri Masmoudi;Habib Mehrez	2014	IJERTCS	10.4018/ijertcs.2014040104	scalable video coding;embedded system;parallel computing;real-time computing;computer science;operating system;context-adaptive variable-length coding;data parallelism	EDA	11.709548456040922	40.43324198697787	105249
941011dbe8e9a8f48be9165ccab692b82d7df6fc	super critical tree numbering and optimal tree ranking are in nc	sequential algorithm;nc;p complete;trees mathematics;erew pram super critical tree numbering optimal tree ranking nc labeling sequential algorithm p complete;computational complexity;trees mathematics computational complexity digital arithmetic parallel algorithms random access storage;time use;super critical tree numbering;optimal tree ranking;computer science labeling phase change random access memory application software very large scale integration manufacturing particle separators polynomials technological innovation algorithm design and analysis;random access storage;digital arithmetic;erew pram;labeling;parallel algorithms	This paper places the optimal tree ranking problem in NC. A ranking is 4 labeling of the nodes wi th natural numbers such tha t i f nodes U and v have the same label t hen there erists another node wi th a greater label o n the path between them. An optimal ranking is a ranking in which the largest label assigned t o any node is as smal l as possible among all rankings. An O ( n ) sequential algorithm is known. Researchers have speculated tha t the problem is P-complete. W e show that f o r a n n node tree, one can compute a n optimal ranking in O(log2 n) t i m e using n Z / l o g n EREW PRAM processors. In fac t , our ranking is super critical in that the label assigned t o each node i s absolutely as small as possible. W e achieve our results by introducing and showing that a more general problem, which we call the super critical numbering problem, i s in NC. No NC algorithm f o r the super critical tree ranking problem, approzimate or otherwise, w a s previously known; the only known NC algorithm f o r optimal b e e ranking was a n approzimate one.	central processing unit;emoticon;p-complete;parallel random-access machine;sequential algorithm	Pilar de la Torre;Raymond Greenlaw	1991		10.1109/SPDP.1991.218243	nc;labeling theory;parallel computing;computer science;p-complete;distributed computing;parallel algorithm;computational complexity theory;algorithm	Theory	14.50981539353528	33.468604564402625	106043
dcc0e5fc5f75135441f6b3e12c4da2820e7b916f	synthesis of high-speed asm controllers with moore outputs by introducing additional states		In the paper, we propose a new method for FPGA-based design of high-speed Algorithmic State Machine (ASM) controllers. The method is based on the introduction of additional states of the state machine in order to implement all transition functions in the single-level structures. In this method, such an optimization criterion as a critical path delay is applied already at the stage of converting the ASM chart to the state machine HDL description. The proposed method consists of two steps: determining the place of additional labels on the ASM chart and introducing additional states of FSM. Experimental results show that our approach achieves an average performance gain of 20.43% to 27.41% (for various FPGA devices) compared with the traditional synthesis method. The maximum performance increase achieved is 59.17%. At the same time, the method slightly increases the cost of implementation by an average of 5.13% to 5.19%, but in some cases even reduces the cost.		Valery Salauyou;Irena Bulatowa	2018		10.1007/978-3-319-99954-8_34	field-programmable gate array;algorithmic state machine;critical path method;real-time computing;machine learning;finite-state machine;artificial intelligence;computer science	EDA	12.84848669319449	46.246459309174824	106764
b07c228ab15ed47ea40a9ba95cf5a3dfc1283c11	the telephone coordination game revisited: from random to deterministic algorithms	set theory distributed algorithms game theory;telephone coordination game;delays games probabilistic logic labeling search problems joining processes educational institutions;rendezvous search game;id length telephone coordination game random algorithm deterministic algorithm telephone problem player communication lower bound worst case rendezvous delay minimisation phone pick sequences telephone labeling functions relative time difference performance bound constant factor;games;joining processes;distributed algorithm design;search problems;probabilistic logic;labeling;delays	Two players wishing to communicate are placed each in a room with N telephones connecting the two rooms. The players do not know how the telephones are interconnected. In each round, each player picks up a phone and says “hello” until when they hear each other. The problem is to devise an algorithm minimising the delay to establish communication. The above problem, called the Telephone Coordination Game, also termed as the Telephone Problem, is of fundamental importance in distributed algorithm design. In this paper, we investigate a generalised version where among N telephones, only a subset can establish communication between the two players. We are interested in devising the deterministic strategy achieving bounded rendezvous delay and minimising the worst-case rendezvous delay. Specifically, we first establish the lower-bound of worst-case rendezvous delay. We then characterise the structure of the phone pick sequences that can guarantee rendezvous without any prior coordination. Assuming each player has a globally unique ID, we further devise a deterministic strategy that (1) guarantees rendezvous between the players regardless of their telephone labeling functions and their relative time difference and (2) approaches the performance bound within a constant factor proportional to the ID length.	algorithm design;best, worst and average case;distributed algorithm;rendezvous delay	Lin Chen;Kaigui Bian	2015	IEEE Transactions on Computers	10.1109/TC.2015.2389799	games;embedded system;labeling theory;real-time computing;simulation;telecommunications;computer science;distributed computing;probabilistic logic;algorithm;computer network	Theory	17.836899624273286	34.38174885026283	106966
6048f50b44ef2988f00e402e18e4cf284744bbcb	design of novel reversible carry look-ahead bcd subtractor	ieee standards;logic gates adders floating point arithmetic ieee standards logic design;carry look ahead bcd adder;logic design;quantum computing adders circuits design engineering logic design floating point arithmetic optical computing digital arithmetic communication standards internet;reversible gate reversible carry look ahead bcd subtracter design ieee 754r floating point standard decimal format carry look ahead bcd adder reversible logic implementation;ieee 754r floating point standard;logic gates;adders;reversible carry look ahead bcd subtracter design;decimal format;reversible logic implementation;floating point arithmetic;reversible gate	IEEE 754r is the ongoing revision to the IEEE 754 floating point standard. A major enhancement to the standard is the addition of decimal format, thus the design of BCD arithmetic units is likely to get significant attention. Firstly, this paper introduces a novel carry look-ahead BCD adder and then builds a novel carry look-ahead BCD subtracter based on it. Secondly, it introduces the reversible logic implementation of the proposed carry look-ahead BCD subtracter. We have tried to design the reversible logic implementation of the BCD Subtracter optimal in terms of number of reversible gates used and garbage outputs produced. Thus, the proposed work will be of significant value as the technologies mature.	adder (electronics);arithmetic logic unit;binary-coded decimal;ieee 754 revision;reversible computing;subtractor	Himanshu Thapliyal;Sumedha K. Gupta	2006	9th International Conference on Information Technology (ICIT'06)	10.1109/ICIT.2006.44	logic synthesis;logic gate;computer science;floating point;operating system;algorithm;adder	EDA	16.09222949023807	45.09034566168947	107072
dff7d3d793a2194800f5fb593157edb350896596	reliability modeling of compensating module failures in majority voted redundancy	computers;triple modular redundancy tmr;compensating module failures fault dominance fault equivalence mission time improvement triple modular redundancy tmr;system reliability;compensating module failures;triple modular redundant;theorems;computers reliability;integrated circuit;reliability modeling;modules electronics;reliability electronics;functional equivalence;redundant components;chip;fault equivalence;n modular redundancy;mission time improvement;test generation;majority voting;fault dominance;logic devices	The classical reliability model for N-modular redundancy (NMR) assumes the network to be failed when a majority of modules which drive the same voter fail. It has long been known that this model is pessimistic since there are instances, termed compensating module failures, where a majority of the modules fail but the network is nonfailed. A different module reliability model based on lead reliability is proposed which has the classical NMR reliability model as a special case. Recent results from the area of test generation are employed to simplify the module reliability calculation under the lead reliability model. First a fault equivalent technique, based on functional equivalence of faults, is developed to determine the effect of compensating module failures on system reliability. This technique can increase the predicted mission time (the time the system is to operate at or above a given reliability) by at least 40 percent over the classical model prediction for simple networks. Since the fault equivalent technique is too complex for modeling of large circuits a second, computational simpler technique, based on fault dominance, is derived. It is then shown to yield results comparable to the fault equivalent technique. A more complex example circuit analyzed by the fault dominance model shows at least a 75 percent improvement in mission time due to modeling compensating module failures. A commercialy available 31 gate integrated circuit chip is also modeled to demonstrate the applicability of the technique to large circuits.	approximation;ccir system a;fault tolerance;integrated circuit;modular programming;reliability engineering;turing completeness	Daniel P. Siewiorek	1975	IEEE Transactions on Computers	10.1109/T-C.1975.224256	chip;embedded system;majority rule;real-time computing;theorem;telecommunications;computer science;integrated circuit	EDA	23.85139645058949	45.469404722586624	107297
6fd40073532ad52b006c0e004b7e4b358efb836c	1024-point pipeline fft processor with pointer fifos based on fpga	random access memory;cosimulation;frequency 92 6 mhz 1024 point pipeline fft processor pointer fifo fft processor fpga r2 2 sdf algorithm gray code counters virtex 4 devices spartan 6 devices spartan 3e devices smims verilink simulation matlab simulink ram flip flops twiddle factors wordlength word length 16 bit word length 32 bit;cosimulation fft pointer fifo radix2 2 sdf;radiation detectors;flip flops;fft;random access storage digital arithmetic fast fourier transforms field programmable gate arrays flip flops gray codes microprocessor chips;reflective binary codes;computer architecture;pipelines;fast fourier transforms;random access storage;digital arithmetic;field programmable gate arrays;radix2 2 sdf;pointer fifo;gray codes;microprocessor chips;throughput;computer architecture random access memory pipelines reflective binary codes radiation detectors throughput field programmable gate arrays	Design and optimized implementation of a 16-bit and 32-bit 1024-point pipeline FFT processor is presented in this paper. The architecture of the FFT is based on R22SDF algorithm with new pointer FIFO embedded with gray code counters. It is implemented in Spartan-3E, Spartan-6 and Virtex-4 devices and fully tested by method of co-simulation using SMIMS® VeriLink® as a bridge that connects software(Matlab® Simulink®) and real hardware-FPGA targets. The implementation results show that our pointer FIFO FFT processor could use lower resource, but achieve higher performance. Our 16-bit 1024-point FFT processor only costs 2580 slices, 2030 slice flip flops and just 2 block RAMs, achieving the maximum clock frequency of 92.6 MHz with the throughput per area of 0.035 Msamples/s/area. Due to the parameterized input wordlength, output wordlength, Twiddle Factors wordlength and processing stages, it is easily to implement a 16-point, 64-point, 256-point, 1024-point,4096-point or higher power of 4 points pointer FIFO FFT processor synthesized from the same code just through modifying the corresponding parameters.	16-bit;32-bit;algorithm;cordic;central processing unit;clock rate;co-simulation;embedded system;fifo (computing and electronics);flops;fast fourier transform;field-programmable gate array;flip-flop (electronics);mathematical optimization;pointer (computer programming);random-access memory;signal-to-quantization-noise ratio;simulation;simulink;throughput;twiddle factor	Guanwen Zhong;Hongbin Zheng;ZhenHua Jin;Dihu Chen;Zhiyong Pang	2011	2011 IEEE/IFIP 19th International Conference on VLSI and System-on-Chip	10.1109/VLSISoC.2011.6081654	parallel computing;real-time computing;computer hardware;computer science	EDA	11.032265200567057	44.76993275770478	107335
1850b5d510e67cf5047c04373ed15f6a785a2b37	implementation of avs jizhun decoder with hw/sw partitioning on a coarse-grained reconfigurable multimedia system		In this paper, a TPP (Task-based Parallelization and Pipelining) scheme is proposed to implement AVS (Audio Video coding Standard) video decoding algorithm on REMUS (REconfigurable MUltimedia System), which is a coarse-grained reconfigurable multimedia system. An AVS decoder has been implemented with the consideration of HW/SW optimized partitioning. Several parallel techniques, such as MB (Macro-Block)-based parallel and block-based parallel techniques, and several pipeline techniques, such as MB level pipeline and block level pipeline techniques are adopted by hardware implementation, for performance improvement of the AVS decoder. Also, most computation-intensive tasks in AVS video standards, such as MC (Motion Compensation), IP (Intra Prediction), IDCT (Inverse Discrete Cosine Transform), REC (REConstruct) and DF (Deblocking Filter), are performed in the two RPUs (Reconfigurable Processing Units), which are the major computing engines of REMUS. Owing to the proposed scheme, the decoder introduced here can support AVS JP (Jizhun Profile) 1920×1088@39fps streams when exploiting a 200 MHz working frequency.	algorithm;automatic parallelization;chroma subsampling;clock rate;computation;cryptography;deblocking filter;direction finding;discrete cosine transform;encryption;h.264/mpeg-4 avc;mpeg-2;megabyte;motion compensation;parallel computing;pipeline (computing);raid processing unit;reconfigurability;shattered world;simulation;tpp;transform, clipping, and lighting;video coding format;video decoder	Leibo Liu;Victor Y. Chen;Shouyi Yin;Li Zhou;Hang Yuan;Shaojun Wei	2013	Science China Information Sciences	10.1007/s11432-013-4979-2	embedded system;parallel computing;real-time computing;computer science	HPC	12.025399828404783	40.60616233510974	107614
f3f2fa05c9963fed0260ea76d8c02be3a708d072	a simple voronoi diagram algorithm for a reconfigurable mesh	worst case running;reconfigurable architectures;efficient algorithm;computational geometry;scheduling computational geometry reconfigurable architectures;switches australia concurrent computing partitioning algorithms computer science computer networks load management councils scholarships writing;reconfigurable mesh;communication diameter;load balancing voronoi diagram algorithm reconfigurable mesh planar points worst case running communication diameter;scheduling;planar points;load balancing;voronoi diagram algorithm;load balance;voronoi diagram	We introduce a simple and efficient algorithm for computing the Voronoi diagram for n planar points on a reconfigurable mesh of size O(n)/spl times/O(n). The algorithm has a worst case running of O(logn log logn) time, and O(logn) in practice. The algorithm exploits the O(1) communication diameter of the reconfigurable mesh model to implement efficient load balancing. >	algorithm;reconfigurable computing;voronoi diagram	Hossam A. ElGindy;Lachlan Wetherall	1995		10.1109/IPPS.1995.395948	parallel computing;real-time computing;computer science;distributed computing	EDA	14.294718005875486	33.44850995888923	107775
3059aa6e01111e7da4efe51263990519398fec11	low-complexity multiternary digit multiplier design in cntfet technology		This brief presents a multiternary digit (trit) multiplier design in carbon-nanotube field-effect transistor (CNTFET) technology using unary operators of multivalued logic. The proposed structure is based on the classical Wallace multiplier and includes a novel ternary multiplexer design requiring only a small number of CNTFETs. Two ternary full-adder configurations are also proposed based on an examination of the multiplier structure. In addition, the design includes a new single-trit multiplier which requires 67% less CNTFETs compared to a recent design. HSPICE simulations reveal low power-delay product for the proposed designs for different choices of drive strength. Furthermore, the designs are comparable to prior works with respect to noise margin.	adder (electronics);multiplexer;noise margin;power–delay product;spice 2;simulation;ternary numeral system;transistor;unary operation;wallace tree	Bodapati Srinivasu;K. Sridharan	2016	IEEE Transactions on Circuits and Systems II: Express Briefs	10.1109/TCSII.2016.2531100	electronic engineering;mathematics;carbon nanotube field-effect transistor;numerical digit;multiplier (economics)	EDA	17.968376697723254	45.046720459284394	107964
35b1649df9bee2777ecc4ade41567fdf936b7c30	single function shifting counters	single function shifting counters	Previous papers by F. H. Young [1] and G. B. Fi tzpatr ick [2] have investigated the cycle length problem for certain types of binary shifting counters. The design of binary counters for m a n y applications stresses simplicity of logic and a t ta inment of maximum number of logical states for the equipment expended. Thus it is natural to ask the following question: For any number of n flip flops in a simple shifting register, is it possible, by means of a single logical function on the initial flip flop, to generate all 2 ~ distinct states? To be specific, if the flip flop states are numbered a l , a2, -. • , an_~, a~, and	flops;flip-flop (electronics)	John S. Bailey;George Epstein	1962	J. ACM	10.1145/321127.321134	discrete mathematics;combinatorics;computer science	Theory	19.968766493802768	43.78633796267819	108954
0ff31f9ceb40adab96588d925cad937bbb7996ee	space lower bounds for graph exploration via reduced automata	modelizacion;anonymity;distributed system;topology;systeme reparti;galet;automata estado finito;cooperative robotics;cooperation;topologie;robotics;cooperacion;topologia;pebbles;anonymat;modelisation;a priori knowledge;sistema repartido;finite automata;borne inferieure;canto rodado;robotica;graph exploration;finite automaton;robotique;automate fini;mobile agent;robot;modeling;lower bound;cota inferior;anonimato	We consider the task of exploring graphs with anonymous nodes by a team of non-cooperative robots modeled as finite automata. These robots have no a priori knowledge of the topology of the graph, or of its size. Each edge has to be traversed by at least one robot. We first show that, for any set of q non-cooperative K-state robots, there exists a graph of size O(qK) that no robot of this set can explore. This improves the O(K) bound by Rollik (1980). Our main result is an application of this improvement. It concerns exploration with stop, in which one robot has to explore and stop after completing exploration. For this task, the robot is provided with a pebble, that it can use to mark nodes. We prove that exploration with stop requires Ω(log n) bits for the family of graphs with at most n nodes. On the other hand, we prove that there exists an exploration with stop algorithm using a robot with O(D log ∆) bits of memory to explore all graphs of diameter at most D and degree at most ∆.	acta informatica;algorithm;automata theory;automation;automaton;cybernetics;directed graph;finite-state machine;graph (discrete mathematics);information and computation;jenkins;lecture notes in computer science;macy conferences;pebble smartwatch;robot;robotics;shannon (unit);symposium on foundations of computer science;symposium on theory of computing	Pierre Fraigniaud;David Ilcinkas;Sergio Rajsbaum;Sébastien Tixeuil	2005		10.1007/11429647_13	robot;a priori and a posteriori;simulation;systems modeling;anonymity;computer science;artificial intelligence;machine learning;mobile agent;mathematics;distributed computing;finite-state machine;upper and lower bounds;programming language;computer security;cooperation;algorithm	Robotics	17.938060958029464	33.63861172841838	109167
1d908f4ba4ef1fb5154e379acf89fa75a64122da	implementations of low-cost hardware sharing architectures for fast 8 x 8 and 4 x 4 integer transforms in h.264/avc	tecnologia electronica telecomunicaciones;fast integer transform;h 264 avc;avc;h 264;hardware share;tecnologias;grupo a	In this paper, novel hardware sharing architectures are proposed for realizations of fast 4 × 4 and 8 × 8 forward/inverse integer transforms in H.264/AVC applications. Based on matrix factorizations, the cost-effective architectures for fast one-dimensional (1-D) 4 × 4 and 8 × 8 forward/inverse integer transforms can be derived through the Kronecker and direct sum operations. By applying the concept of hardware sharing, the proposed hardware schemes for fast integer transforms need a smaller number of shifters and adders than the direct realization architecture, where the direct architecture just implements the individual 4 × 4 and individual 8 × 8 integer transforms independently. With low hardware cost and regular modularity, the proposed hardware sharing architectures can process up to 125 MHz with the cost-effective area and are suitable for VLSI implementations to accomplish the H.264/AVC signal processing.	h.264/mpeg-4 avc	Chih-Peng Fan;Yu-Lian Lin	2007	IEICE Transactions	10.1093/ietfec/e90-a.2.511	real-time computing;theoretical computer science;mathematics	Crypto	12.224914879943453	41.731025811297094	109631
ad55f19ba7183b96782573fda208ff00050420ce	a parallel msf algorithm for planar graphs on a mesh and applications to image processing	optical clustering parallel msf algorithm planar graphs mesh image processing minimum cost spanning forest mesh connected computer k width connectivity;parallel algorithm;concurrent computing;image processing;application software;k width connectivity;computational geometry;image processing clustering algorithms application software concurrent computing parallel algorithms computational geometry labeling terminology;mesh;mesh connected computer;clustering algorithms;parallel algorithms image processing;terminology;planar graphs;optical clustering;planar graph;labeling;parallel msf algorithm;minimum cost spanning forest;parallel algorithms	We present an efficient O(n) parallel algorithm for finding a minimum-cost spanning forest (MSF) of a weighted undirected planar graph with na edges, on an n x n mesh-connected computer. We also obtain efficient MSF-based O(n) algorithms for several application problems in image processing. In particular, we show that an MSF can be used to obtain more efficient and elegant O(n) algorithms for the “k-width connectivity” problem [Hambrusch-Dehne] and the “optical clustering” problem [Dehne-MillerRau-Chaplin].	chaplin.js;cluster analysis;file spanning;graph (discrete mathematics);image processing;microsoft solutions framework;numerical aperture;parallel algorithm;planar graph;spanning tree	David Nassimi	1993		10.1109/IPPS.1993.262882	computer science;theoretical computer science;machine learning;distributed computing	Theory	15.482763753371001	33.01166321439591	109754
5d41417fa84ebc2905a1f2dc54105547a8b4663b	using low precision floating point numbers to reduce memory cost for mp3 decoding	decoding;technology;memory storage low precision floating point number mp3 decoding fixed point representation fpga prototype;teknikvetenskap;fixed point;chip;audio coding;engineering and technology;teknik och teknologier;word length;signal representation;dynamic range;floating point;costs digital audio players decoding floating point arithmetic dynamic range fixed point arithmetic libraries energy consumption digital signal processing chips embedded system;digital storage;floating point arithmetic;power consumption;signal representation floating point arithmetic digital storage decoding audio coding	The purpose of our work has been to evaluate the practicality of using a 16-bit floating point representation to store the intermediate sample values and other data in memory during the decoding of MP3 bit streams. A floating point number representation offers a better trade-off between dynamic range and precision than a fixed point representation for a given word length. Using a floating point representation means that smaller memories can be used which leads to smaller chip area and lower power consumption without reducing sound quality. We have designed and implemented a DSP processor based on 16-bit floating point intermediate storage. The DSP processor is capable of decoding all MP3 bit streams at 20 MHz and this has been demonstrated on an FPGA prototype.	16-bit;digital signal processor;dynamic range;field-programmable gate array;fixed point (mathematics);mp3;prototype;sound quality	Johan Eilert;Andreas Ehliar;Dake Liu	2004	IEEE 6th Workshop on Multimedia Signal Processing, 2004.	10.1109/MMSP.2004.1436435	minifloat;double-precision floating-point format;parallel computing;computer hardware;computer science;floating point;theoretical computer science;operating system;fixed-point arithmetic	HPC	11.929286255768405	42.189932774145355	110146
ef8122e1cc5f284cc489a2dc0dd80b58fe12101e	graphs with most number of three point induced connected subgraphs	graph node;probability;subgrafo;nudo grafo;graphe s3 maximum;fiabilite reseau;sous graphe;probabilidad;defaillance;probabilite;conexidad;network reliability;failures;connexite;subgraph;connectedness;fallo;noeud graphe	Let G be a simple graph with e perfectly reliable edges and n nodes which fail independently and with the same probability ρ. The residual connectedness reliability R(G, ) ρ of G is the probability that the graph induced by the surviving nodes is connected. If Γ(n, e) is the collection of all n node e edge simple graphs, then G n e ∈Γ( , ) is uniformly most reliable if R G R G ( , ) ( , ) ρ ρ ≥ ′ for all ′ ∈ G n e Γ( , ) and all 0 1 < < ρ . If S G 3( ) if the number of three point induced connected subgraphs of G, the G n e ∈Γ( , ) is S3-maximum if S G S G 3 3 ( ) ( ) ≥ ′ for all ′ ∈ G n e Γ( , ). It is known that if G n e ∈Γ( , ) is S3maximum and ρ is sufficiently large then R G R G ( , ) ( , ) ρ ρ > ′ for all non S3-maximum graphs G′ ∈ Γ(n, e). This paper characterizes the S 3-maximum graphs in Γ( , ) n e for the range e n n ≤ + − ( / ) ( ) / 2 4 2 3 4. * Supported by NSF Grant ECS−8600782	graph (discrete mathematics);ibm notes	Francis T. Boesch;Xiaoming Li;José Rodriguez	1995	Discrete Applied Mathematics	10.1016/0166-218X(93)E0155-R	combinatorics;discrete mathematics;topology;social connectedness;probability;mathematics;reliability	Theory	23.634490394513268	33.73811472154931	110460
0ce1627cb6230c4ab040e3c80aa17fceb331c99a	a dual symbol arithmetic coder architecture with reduced memory for jpeg2000	processing element;storage capacity 1509 bit;fpga synthesis;image coding;probability;clocks;arithmetic coder;computer architecture context transform coding registers indexes clocks field programmable gate arrays;transform coding;probability estimation table;architecture arithmetic coder image compression;reduced memory;computer architecture;indexes;image compression;registers;probability interval;jpeg2000;digital arithmetic;dual symbol arithmetic coder architecture;storage capacity 1509 bit dual symbol arithmetic coder architecture reduced memory jpeg2000 probability interval probability estimation table fpga synthesis memory size;field programmable gate arrays;architecture;context;probability computer architecture digital arithmetic field programmable gate arrays image coding;memory size	A dual-symbol arithmetic coder architecture with reduced memory is presented for JPEG2000. Eight process elements are used for the prediction of probability interval A. And the use of a dedicated Probability Estimation Table decreases the internal memory greatly. Upon FPGA synthesis results, the architecture's throughput can reach 96.60M context symbols per second with an internal memory size of 1509 bits.	arithmetic coding;computer data storage;field-programmable gate array;jpeg 2000;throughput	Kai Liu;Yunsong Li	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5651117	interleaved memory;parallel computing;transform coding;memory bank;image compression;computer science;theoretical computer science;architecture;probability;statistics	EDA	12.430361525068813	40.007265894593985	110592
531063b82981c4bf6e1cd30e2e1963416d3732d6	an energy-efficient 8×8 2-d dct vlsi architecture for battery-powered portable devices	energy efficiency;discrete cosine transforms computer architecture energy efficiency very large scale integration clocks throughput registers;clocks;energy efficient;very large scale integration;battery powered portable device;size 45 nm;size 350 nm;very large scale integrated;discrete cosine transform;frequency 4 9 mhz;size 90 nm;computer architecture;size 45 nm energy efficient 2d dct vlsi architecture battery powered portable device energy efficiency combinational 1d dct block algorithm intrinsic parallelism integer constant multiplication deep submicron technology node discrete cosine transform frequency 4 9 mhz size 350 nm power 6 08 mw size 90 nm;deep submicron technology node;total power;energy efficient 2d dct vlsi architecture;registers;discrete cosine transforms;combinational 1d dct block;vlsi discrete cosine transforms;vlsi;integer constant multiplication;algorithm intrinsic parallelism;power 6 08 mw;throughput;vlsi architecture	This paper presents an energy-efficient VLSI architecture for 8×8 2-D DCT, which relies on a fast and precise implementation of the LLM algorithm. The energy-efficiency is achieved by using a combinational 1-D DCT block that explores the algorithm's intrinsic parallelism and the integer constant multiplications. The target throughput of 19 Mpixels/s, which is required for VGA@30fps, is achieved by applying a 4.9 MHz clock, that corresponds only to 17.5% of the maximum clock. Synthesis results for a 350 nm technology estimate total power as 6.08 mW, and core area as 2.1 mm2. The proposed architecture shows to be at least 42% more energy efficient than the related work. To further investigate the efficiency on deep submicron technology nodes, synthesis for 90 nm and 45 nm were also performed.	algorithm;combinational logic;core (optical fiber);discrete cosine transform;low-pass filter;parallel computing;personal digital assistant;pixel;slack variable;throughput;very-large-scale integration	Vinicius S. Livramento;Bruno George de Moraes;Brunno Abner Machado;José Luís Almada Güntzel	2011	2011 IEEE International Symposium of Circuits and Systems (ISCAS)	10.1109/ISCAS.2011.5937633	embedded system;electronic engineering;parallel computing;real-time computing;computer science;mathematics;efficient energy use;very-large-scale integration	Arch	11.382620587037488	45.73930291234904	110595
a81af34de243c4111d8cdc7424150677902bde5a	concentrated regular data streams on grids: sorting and routing near to the bisection bound	multiprocessor interconnection networks;sorting;routing;data stream;offline routing concentrated regular data streams deterministic algorithms sorting routing bisection bound 1 1 sorting h h sorting permutation routing;permutation routing;deterministic algorithms;1 1 sorting;indexing;computational complexity;sorting routing indexing;concentrated regular data streams;sorting computational complexity multiprocessor interconnection networks;offline routing;bisection bound;h h sorting	1.1 Sorting and Routing Sorting and routing on r-dimensional n x ... x n grids of processors is studied. Deterministic algorithms are presented f o r h h problems, h 2 1, where each processor initially and finally contains h elements. W e show that the classical 1 1 sorting can be solved with ( 2 r l . 5 ) n + o ( n ) transport steps, i.e. an about 2.5n steps f o r r = 2 . The general h h sorting problem, h 3 4r 4, can be solved within a number of transport steps that asymptotically differs by a factor of at most 3 from the trivial bisection bound. Furthermore we show that the bisection bound is asymptotically tight f o r sequences of h permutation routing problems, h = 4cr, c 2 1 , and f o r so-called off-line routing.	central processing unit;deterministic algorithm;online and offline;routing;sorting	Manfred Kunde	1991		10.1109/SFCS.1991.185363	search engine indexing;routing;parallel computing;computer science;sorting;theoretical computer science;distributed computing;computational complexity theory;algorithm	Theory	13.814105732286372	32.36364155147411	110668
6ce03c6f30920dc025602cd29f85c444acc683d1	quantum division circuit based on restoring division algorithm	computers;fourier transform;quantum addition;division operation;quantum computing logic gates fourier transforms registers quantum mechanics computers;quantum division;logic circuits;quantum addition quantum division restoring division quantum fourier transform;quantum fourier transform quantum division circuit restoring division algorithm division operation n qubit quantum processors ancillaqubits;restoring division;quantum division circuit;logic gates;quantum fourier transform;quantum computer;quantum mechanics;registers;quantum circuits;ancillaqubits;fourier transforms;restoring division algorithm;quantum computing fourier transforms logic circuits;n qubit quantum processors;quantum computing;logic gate	This paper presents a new quantum circuit for implementation of division operation in n-qubit quantum processors based on restoring division algorithm. In the proposed circuit we tried to reduce the number of ancillaqubits as many as possible using quantum fourier transform so that the circuit could be easily generalized for the case of any arbitrary values of dividend and divisor.	central processing unit;division algorithm;quantum fourier transform;quantum circuit;qubit	Alireza Khosropour;Hossein Aghababa;Behjat Forouzandeh	2011	2011 Eighth International Conference on Information Technology: New Generations	10.1109/ITNG.2011.177	quantum fourier transform;logic gate;division algorithm;quantum algorithm;quantum phase estimation algorithm	EDA	14.963100458650322	43.47776707688587	111024
94c74f146134dadcc68d8480be412a7a0387d6c4	a simplified general method for static hazard detection	b ternary functions duo binary algebra hazard detection hazard in combinational circuits hazard in sequential machines multiple hazards;multiple hazards;switched system;hazard in sequential machines;hazard in combinational circuits;general methods;b ternary functions;necessary and sufficient condition;hazard detection;combinational circuit;duo binary algebra;steady state	A simplified binary technique is presented for dedecting hazards in combinational and sequential systems. First, a duo-binary representation is developed to derive the necessary and sufficient conditions for describing transient and steady-state behavior of switching systems. The results obtained by the duo-binary representation serve to prove that static hazards in switching circuits can be detected by examining only the extreme conditions of the transient signals. Thus, a 1-hazard will appear if the value zero is assigned simultaneously to the changing signals and their complements, and a 0-hazard will appear if the value one is assigned to them.	binary number;combinational logic;electronic switching system;hazard (logic);steady state	Y. Zisapel	1976	IEEE Transactions on Computers	10.1109/TC.1976.1674573	discrete mathematics;hazard;computer science;mathematics;combinational logic;steady state;algorithm	Visualization	22.989221596675115	46.207563023103376	111099
436a841d29fe096f80471683027a6120bfa113d5	feature extraction methods for handwritten character recognition based on acceleration.	feature extraction;handwritten character recognition	In the coding and decoding of Reed-Solomon codes formed of symbols larger than information symbols, redundant circuitry is eliminated, error detection and correction are performed using a simple construction, and the reliability of error detection and correction is improved by processing only data of the same size as the information symbols. Two bits of dummy data which are surplus bits in 10 bits of one symbol of information are supplied from a dummy data input circuit to 8 bit input data. At the same time, syndrome data is generated from the surplus parts of check symbols by a syndrome data correction circuit. A part of the 10 bit data is selected by a selector, and supplied to a Galois field summation circuit. The output of the Galois field summation circuit is output to a register, and the output of this register is either selected without modification or via a Galois field coefficient multiplying circuit by a selector, and supplied to the Galois field summation circuit. The output of the register is output as syndrome data by a syndrome output terminal.		Chun Yuan;Shiqi Zhang;Yan Zhang	2009			feature extraction;intelligent character recognition;intelligent word recognition;pattern recognition	AI	16.070710616873143	43.154859648044756	111179
9feb6dac5df216f9ba812a07ec56d3db766179ec	design of an area-efficient partial-sum architecture for polar decoders based on new matrix generator	matrix algebra decoding logic gates;generators;decoding;area efficient partial sum architecture and gate kronecker product xor gate d flip flops partial sum update circuit encoding matrix generator psg partial sum generator polar decoder;computer architecture;logic gates;registers;matrix decomposition;generators decoding logic gates matrix decomposition registers encoding computer architecture;encoding	This paper proposes an area-efficient partial-sum generator (PSG) architecture for polar decoder implementation. High-throughput PSG designs mainly consist of an encoding matrix generator and a partial-sum update circuit. The matrix generator conventionally is built by cascading a series of D flip-flops and XOR gates. By decomposing the target matrix into the Kronecker product of smaller matrices, this paper proposes a new multi-level matrix generator architecture built by interconnection of several matrix generators of small sizes and additional arrays of AND gates. The additional AND gates incurred by our new matrix generator can be integrated with those AND gates required in the partial-sum update circuit when forming PSG such that only two extra AND gates are required for the proposed PSG based on two-level matrix generator architecture. The required number of D flip-flops and XOR gates of two-level matrix generator is only about 50% of the conventional one. Considering the entire PSG module, the overall reduction ratio is still more than 16%. The proposed PSG design can be applied for various kinds of successive cancellation polar decoder architectures.	codec;exclusive or;flops;flip chip;flip-flop (electronics);interconnection;phrase structure grammar;programmable sound generator;the matrix;throughput	Yun-Nan Chang	2016	2016 14th IEEE International New Circuits and Systems Conference (NEWCAS)	10.1109/NEWCAS.2016.7604803	electronic engineering;parallel computing;computer science;generator matrix;theoretical computer science	Arch	12.578700132029267	45.863349228528016	111236
2eb7429b0617439d4cda99b765e703a4a4569290	odd memory systems: a new approach	geometria euclidiana;interconnection;shared memory;storage access;execution time;reseau interconnecte;routing;memoria compartida;geometrie euclidienne;calculateur simd;division;multiplaje;multiplexing;interconnection network;paralelismo masivo;permutation;chinese remainder theorem;multiplexage;simd computer;permutacion;interconnexion;acces memoire;memory systems;acceso memoria;temps execution;euclidean geometry;prime number;encaminamiento;systeme parallele;parallel system;vector processor;tiempo ejecucion;red interconectada;interconnected power system;parallelisme massif;chinese remainder network;massive parallelism;sistema paralelo;interconeccion;memoire partagee;acheminement;processeur vectoriel	To reject the use of a prime (or odd) number N of memory banks in a vector processor, it is generally advanced that address computation for such a memory system would require systematic Euclidean division by the number N. We first show that the Chinese Remainder Theorem allows one to define a very simple mapping of data onto the memory banks for which address computation does not require any Euclidean division. Massively parallel SIMD computers may have thousands of processors. When the memory on such a machine is globally shared, routing vectors from memory to the processors is a major difficulty ; the control for the interconnection network cannot be generally computed at execution time. When the number of memory banks and processors is a product of prime numbers, the family of permutations needed for routing vectors from memory to the processors through the interconnection network has very specific properties. The Chinese Remainder Network presented in the paper is able to execute all these permutations in a single path and may be easily controlled. © 1995 Academic Press, Inc.		André Seznec;Jacques Lenfant	1995	J. Parallel Distrib. Comput.	10.1006/jpdc.1995.1063	memory address;euclidean geometry;uniform memory access;distributed shared memory;shared memory;routing;interleaved memory;combinatorics;vector processor;parallel computing;distributed memory;telecommunications;memory geometry;computer science;physical address;division;theoretical computer science;operating system;interconnection;chinese remainder theorem;mathematics;distributed computing;permutation;overlay;extended memory;flat memory model;prime number;algorithm;multiplexing;computing with memory;memory map;memory management	HPC	13.273662970141826	36.001734457649576	111590
f86d89da4a160085d39ed6118342591a5aa1057e	using decision diagrams to design ulms for fpgas	field programmable gate array;synthese circuit;diagrama binaria decision;diagramme binaire decision;classification of logic functions;module logique universel;decision theory field programmable gate arrays table lookup formal logic;funcion logica;decision diagram;logic circuits;indexing terms;red puerta programable;reseau porte programmable;logical function;fonction logique;synthesis of logic functions;circuit logique;field programmable gate arrays programmable logic arrays table lookup logic functions logic arrays logic design binary decision diagrams routing pins;four input lut replacement decision diagrams ulms fpgas field programmable logic arrays lookup table logic blocks universal logic modules systematic development bdd description logic functions explicit construction three input lut replacement programming bits;decision theory;field programmable logic;fpgas;formal logic;sintesis circuito;lookup table;bdds;field programmable gate arrays;ulms;table lookup;integrated circuits;circuit synthesis;circuit integre;binary decision diagram	Many modern Field Programmable Logic Arrays (FPGAs) use lookup table (LUT) logic blocks which can be programmed to realize any function of a fixed number of inputs. It is possible to employ logic blocks that realize only a subset of all functions, while the rest can be obtained by permuting and negating the inputs. Such blocks, known as Universal Logic Modules (ULMs), have already been considered for application in FPGAs; in this paper, we propose a new class of ULMs which is more useful in the FPGA environment. Methodology for systematic development of such blocks is presented, based on the BDD description of logic functions. We give an explicit construction of a three-input LUT replacement that requires only five programming bits, which is the optimum for such ULMs. A realistic size four-input LUT replacement is obtained which uses 13 programming bits.	binary decision diagram;field-programmable gate array;logic gate;lookup table	Zeljko Zilic;Zvonko G. Vranesic	1998	IEEE Trans. Computers	10.1109/12.713316	embedded system;parallel computing;computer science;theoretical computer science;mathematics;algorithm;field-programmable gate array	EDA	18.513949107730802	45.779798010374684	111851
074e1007b1b00283d01c8c66ce3e445a22295841	a systolic architecture for labeling the connected components of multi-valued images in real time	labeling hardware computer architecture algorithm design and analysis computer science australia systolic arrays very large scale integration circuits clustering algorithms;systolic architecture;custom vlsi chip;connected components;linear systolic array;two pass system;image processing;very large scale integration;systolic arrays;real time;systolic array;chip;computer architecture;minimal support circuitry;image processing systolic arrays vlsi real time systems;video rates;vlsi;clustering algorithms;circuits;computer science;connected component;hardware implementation;algorithm design and analysis;raster scan;labeling;australia;hardware;minimal support circuitry systolic architecture labeling connected components multi valued images real time video rates linear systolic array raster scan two pass system custom vlsi chip;real time systems;multi valued images	A system capable of labeling the connected components of multi-valued images at video rates is described. The technique uses a simple linear systolic array to modify the labels of recently visited pixels in a raster scan. The array removes the need for the complex processing of label equivalence tables used in previous algorithms. A two-pass system is designed using the same custom VLSI chip for both passes with minimal support circuitry. This has an advantage over previous hardware implementations where more complex algorithms require separate hardware modules for each pass. >	connected component (graph theory)	C. J. Nicol	1993		10.1109/CVPR.1993.340998	parallel computing;real-time computing;connected component;image processing;computer science;theoretical computer science;very-large-scale integration	Robotics	13.335210726793942	43.20308274097465	111999
e23a26d78ec9238b34b602e40c7d8dd2b5352da6	on a fpga implementation of bch codes	bch codes;field programmable gate arrays;bch codes;fpga implementation;reconfigurable chip	Our paper presents the prototyping of a BCH (Bose, Chaudhuri, and Hocquenghem) encoder and decoder using a Field Programmable Gate Array (FPGA) reconfigurable chip. The solutions implemented on FPGA lead to a high calculation rate using parallelization. We implemented the BCH code in a 3s400FG456 FPGA. In this implementation we used 15 bits-size word code and the results show that the circuits work quite well.	bch code;encoder;field-programmable gate array;parallel computing	Laurentiu Ionescu;Constantin Anton;Ion Tutanescu;Alin Mazare;Gheorghe Serban	2010	2010 International Conference for Internet Technology and Secured Transactions		embedded system;electronic engineering;computer hardware;programmable logic array;computer science	EDA	10.821364595993826	44.67401954190059	112422
445f2532660d39d86647d90181d339e020169dbf	a fully parallel algorithm for residue to binary conversion	data conversion;algoritmo paralelo;nombre entier;parallel algorithm;systeme binaire;circuit vlsi;conversion donnee;algorithme parallele;integer;vlsi circuit;conversion datos;systeme residuel;entero;sistema binario;borne inferieure;circuito vlsi;lower bound;cota inferior;binary system;parallel algorithms	Abstract A new method for converting numbers from a residue system to a binary notation is proposed which is based upon an original formulation of the Chinese Remainder Theorem. To prove its effectiveness in VLSI implementations a converter is presented and evaluated following a general VLSI model of computation. The proposed structure compares favourably with preceding results presented in the literature; in particular, the time which is required to perform conversion is O(log s ), with s representing the total number of input bits.	parallel algorithm	Ferruccio Barsi;Maria Cristina Pinotti	1995	Inf. Process. Lett.	10.1016/0020-0190(95)00034-A	parallel computing;computer science;mathematics;parallel algorithm;algorithm	DB	16.720918645359923	40.317074917451585	112678
04781082d78c79f5f842f20ffd5bc3b8de78e6f2	research and implementation of a high-speed reconfigurable a5 algorithm	a5 1 algorithm;high speed dynamic reconfigurable hardware architecture;telecommunication security clocks cmos logic circuits cryptography field programmable gate arrays logic design mobile communication reconfigurable architectures;logic design;dynamic reconfiguration;reconfigurable clock controlling unit;clocks;reconfigurable architectures;cmos process;fpga;satisfiability;size 0 18 mum;hardware architecture;registers;cryptography;critical path;cmos logic circuits;mobile communication;telecommunication security;high speed reconfigurable a5;a5 algorithm;field programmable gate arrays;clocks feedback hardware polynomials mobile communication cryptography base stations conferences computational intelligence computer industry;high speed;algorithm design and analysis;size 0 18 mum high speed dynamic reconfigurable hardware architecture a5 algorithm reconfigurable clock controlling unit fpga cmos process cryptography a5 1 algorithm a5 2 algorithm mobile communication;a5 2 algorithm;reconfigurable;throughput;hardware	A high-speed and dynamic reconfigurable hardware architecture of A5 algorithm is presented, which can satisfy the different characteristic of A5/1 and A5/2 algorithm. To save the hardware cost and get shorter critical path, we proposed reconfigurable clock controlling unit and output function, which can be reconfigured to realize the critical function of two algorithms. As to the different high-speed method, the paper performs detailed comparison and analysis. The design has been realized using Altera's FPGA. Synthesis, placement and routing of reconfigurable design have accomplished on 0.18 mum CMOS process, the result proves the critical throughput rate can achieve 880 Mbps.	algorithm;apple a5;cmos;critical path method;data rate units;field-programmable gate array;place and route;reconfigurable computing;routing;throughput	Li Wei;Dai Zibin;Nan Longmei	2008	2008 IEEE Pacific-Asia Workshop on Computational Intelligence and Industrial Application	10.1109/PACIIA.2008.361	embedded system;real-time computing;computer science;hardware architecture;field-programmable gate array	EDA	10.65076537028377	45.443156604160826	112716
dee654c1a63920ae7ab9f1cb66c769861eb846d3	consistency thresholds for binary symmetric block models		We consider the problem of reconstructing symmetric block models with two blocks of n vertices each and connection probabilities pn and qn for interand intra-block edge probabilities respectively. We are interested in two different notions of consistency: strong consistency means the existence of an estimation procedure which recovers the labels of all nodes correctly with probability 1 − o(1) as n → ∞, while weak consistency only asks for the estimation procedure to correctly label 2n − o(n) nodes correctly. We provide necessary and sufficient conditions on pn and qn for both notions of consistency, and we relate these conditions to the structure of the corresponding graphs. In particular, strong consistency is attainable if and only if with high probability every node has the same label as the majority of its neighbors, while weak consistency is attainable if and only if each node has, with high probability, the same label as the majority of its neighbors. We also give efficient algorithms for consistent estimators whenever one exists. Our results hold for arbitrary sequences pn and qn. ∗University of Pennsylvania. Supported by NSF grants DMS-1106999 and CCF 1320105 and DOD ONR grant N000141110140 U.T. Austin. Supported by NSF grant DMS-1106999 and DOD ONR grant N000141110140 U.C. Berkeley and the Australian National University. Supported by an Alfred Sloan Fellowship and NSF grant DMS-1208338.	algorithm;ibm notes;microsoft customer care framework;strong consistency;the australian;weak consistency;with high probability	Elchanan Mossel;Joe Neeman;Allan Sly	2014	CoRR		statistics	Theory	22.232126804041147	33.88697576369915	112883
cad419990925a9f24a71dc0eefbba85454bfcb26	locally twisted cubes are 4-pancyclic	embedding;school of no longer in use;hypercube;electronics and computer science;matematicas aplicadas;mathematiques appliquees;hamiltonian;endnotes;hamiltonien;parallel computation;interconnection network;calculo paralelo;plongement;parallel computer;twisted cube;pubications;inmersion;applied mathematics;institutional repository research archive oaister;cube torsade;calcul parallele;hamiltoniano;red interconexion;reseau interconnexion;hipercubo	The locally twisted cube is a newly introduced interconnection network for parallel computing. Ring embedding is an important issue for evaluating the performance of an interconnection network. In this paper, we investigate the problem of embedding rings into a locally twisted cube. Our main contribution is to find that, for each integer l is an element of (4,5,...,2(n)}, a ring of length I can be embedded into an n-dimensional locally twisted cube so that both the dilation and the load factor are one. As a result, a locally twisted cube is Hamiltonian. We conclude that a locally twisted cube is superior to a hypercube in terms of ring embedding capability. (C) 2004 Elsevier Ltd. All rights reserved.	olap cube;twisted	Xiaofan Yang;Graham M. Megson;David J. Evans	2004	Appl. Math. Lett.	10.1016/j.aml.2003.10.009	combinatorics;discrete mathematics;topology;hamiltonian;applied mathematics;embedding;mathematics;geometry;quantum mechanics;hypercube	Robotics	24.126903284844136	35.47306237403883	112902
f376190365bf270eab9fa85eedffd09ec669cb47	low-power high-speed hybrid wave-pipeline architectures for binary morphological dilation	hybrid wave pipeline;binary image;morphology;asic;dilation;real time image processing;cmos	Dilation and erosion are two fundamental operations of mathematical morphology for image processing. This paper presents three hybrid wave-pipeline (HWP) architectures for real-time binary dilation operator. With minor changes to the number and/or to the type of the basic gates, they can be employed as erosion operator. In the first HWP-architecture, each single cell utilizes the wave technique along with delay units for balancing the data paths. By minimizing the number of delay units, the second HWParchitecture with reduced power consumption and hardware complexity is obtained. The third HWP-architecture employs wave technique in each three cascaded cells. This architecture improves the above performance further, at the cost of slight reduction in maximum clock frequency and clock frequency range. Simulation results, using a 0.18μmCMOS technology, indicate that the HWP architectures have higher speed, less hardware complexity, and lower power consumption compared to pipeline (P) architecture. Also, they are faster than wave-pipeline (WP) architecture, without the difficulty of balancing the delay of long signal paths. Simulation illustrates that the third HWP-architecture dilates a 1024×1024 image by a 21×21 structuring element (SE) in 214.64 μs. The maximum frequency of operation is 5 GHz for the power supply of 1.8 V. The power dissipation is 410 mW, and the chip area is 0.075 mm.	algorithm;cmos;cpu power dissipation;clock rate;dilation (morphology);erosion (morphology);frequency band;image processing;low-power broadcasting;mathematical morphology;power supply;real-time clock;simulation;structuring element;vhdl	Mahdiye Hajirahimi;Abdolreza Nabavi;Ehsanollah Kabir	2012	Signal Processing Systems	10.1007/s11265-011-0628-4	embedded system;computer vision;parallel computing;real-time computing;morphology;binary image;telecommunications;computer science;electrical engineering;theoretical computer science;application-specific integrated circuit;dilation;cmos;algorithm	Arch	13.112125838406596	42.88069099559683	113047
e3dc3a58bd62d0862cc6c82e2e59dabb5c417a37	integer sorting and routing in arrays with reconfigurable optical buses	sorting;reconfigurable architectures;permutation routing;optical computing;network routing;system buses;deterministic algorithms;parallel architectures;computational complexity deterministic algorithms sorting network routing reconfigurable architectures optical computing optical interconnections system buses parallel architectures parallel algorithms;pram simulations deterministic algorithms integer sorting on line packet routing arrays reconfigurable optical buses architectures partial permutation routing h relations algorithms;computational complexity;optical arrays sorting routing optical computing optical fiber networks information science buildings phase change random access memory data engineering councils;optical interconnections;parallel algorithms	We present deterministic algorithms for integer sorting and on-line packet routing on arrays with reconfigurable optical buses. The main objective is to identify the mechanisms specific to this type of architecture which allow building efficient integer sorting, partial permutation routing and h-relations algorithms. The consequences of these results on PRAM simulations are also investigated.	integer sorting;routing	Sandy Pavel;Selim G. Akl	1996		10.1109/ICPP.1996.537386	routing;parallel computing;computer science;sorting;theoretical computer science;distributed computing;parallel algorithm;link-state routing protocol;optical computing;computational complexity theory;algorithm	Theory	14.151597865103325	34.00779169048198	113072
196f42071705529aa35aa209e7d99b256e9d936e	diameter variability of hypercubes	hypercube;theoretical computer science;supergraph;diameter;issn 0304 3975;folded hypercube	A parallel and distributed system is usually represented by a graph. The maximum communication delay between any pair of processors in a parallel and distributed system can be determined by the diameter of its underlying graph. The diameter of a graph can be affected by the addition or deletion of edges. In this paper, we show that the diameter of an n-dimensional hypercube can be decreased by k with the addition of 2^2^k^-^1 edges for 1@?k@?@?n/2@?.	spatial variability	Ming-Yi Ju;Jeng-Jung Wang;Shu-Hao Chang	2014	Theor. Comput. Sci.	10.1016/j.tcs.2014.04.033	folded cube graph;parallel computing;computer science;theoretical computer science;diameter;hypercube graph;mathematics;distributed computing;hypercube	ECom	23.512619966931688	35.23165214976949	113118
48577cb5f74759b811736b98acd88ec80548c4ad	the long length dht design with a new hardware efficient distributed arithmetic approach and cyclic preserving partitioning	transformation hartley;discrete hartley transform;tecnologia electronica telecomunicaciones;transformacion discreta;integrated circuit;cyclic preserving partitioning;memoire morte;distributed processing;transformacion hartley;partitioning;circuito integrado;tecnologia mos complementario;algorithme;algorithm;read only memory rom;discrete transformation;distributed arithmetic;memoria muerta;hartley transformation;computation sharing;partitionnement;temps retard;subdivision;delay time;tecnologias;grupo a;technologie mos complementaire;tiempo retardo;traitement reparti;article;transformation discrete;circuit integre;complementary mos technology;tratamiento repartido;algoritmo	This paper presents a long length discrete Hartley transform (DHT) design with a new hardware efficient distributed arithmetic (DA) approach. The new DA design approach not only explores the constant property of coefficients as the conventional DA, but also exploits its cyclic property. To efficiently apply this approach to long length DHT, we first decompose the long length DHT algorithm to short ones using the prime factor algorithm (PFA), and further reformulate it by using Agarwal-Cooley algorithm such that all the partitioned short DHT still consists of the cyclic property. Besides, we also exploit the scheme of computation sharing on the content of ROM to reduce the hardware cost with the trade-off in slowing down the computing speeds. Comparing with the existing designs shows that the proposed design can reduce the area-delay product from 52% to 91% according to a 0.35 /an CMOS cell library.	distributed hash table	Hun-Chen Chen;Tian-Sheuan Chang;Jiun-In Guo;Chein-Wei Jen	2005	IEICE Transactions	10.1093/ietele/e88-c.5.1061	discrete hartley transform;parallel computing;real-time computing;telecommunications;computer science;integrated circuit;subdivision;algorithm	EDA	15.45536469541418	41.356935966369676	113261
a5d840ed270f48c0920f8824bd728a9e945a97e8	extracting a simplified view of design functionality based on vector simulation	learning algorithm;learning problems;fourier analysis	This paper presents a simulation-based methodology for extracting a simplified view of design functionality from a given module. Such a simplified design view can be used to facilitate test pattern justification from the outputs of the module to the inputs of the module. In this work, we formulate this type of design simplification as a learning problem. By developing a scheme for learning word-level functions, we point out that the core of the problem is to develop an efficient Boolean learner. We discuss the implementation of such a Boolean learner and compare its performance with the one of best-known learning algorithms, the Fourier analysis based method. Experimental results are presented to illustrate the implementation of the simulation-based methodology and its usage for extracting a simplified view of Open RISC 1200 datapath.	simulation	Onur Guzey;Charles H.-P. Wen;Li-C. Wang;Tao Feng;Hillel Miller;Magdy S. Abadir	2006		10.1007/978-3-540-70889-6_3	simulation;computer science;artificial intelligence;theoretical computer science;machine learning;fourier analysis;algorithm	EDA	20.685804557148955	46.004400410068996	113295
567d9dcf043be259dcf87dea7695f61458f8f0bd	emitter-couple logic circuit design based on the threshold-arithmetic algebraic system	mao qun yao li bin zhang emitter couple logic circuit design based on the threshold arithmetic algebraic system	Based on the threshold-arithmetic algebraic system which has been proposed for current-mode circuit design, we propose a systematic methodology for emitter-couple logic (ECL) circuit design. Compared to the traditional methodologies and the theory of differential current switches, the proposed methodology uses the HE map and the characteristics of the internal current signals of ECL circuits to determine the external voltage signals. The operations of the HE map are direct and simple, and the current signals are easy to add or subtract, which make this methodology more flexible, direct, and effective, and make it possible to design arbitrary binary and multi-valued logic functions. Two example circuits are designed and simulated by HSPICE using 0.18 μm TSMC technology. Simulation results confirm the validity of the proposed methodology.	algebraic equation;bitwise operation;circuit design;lambert's cosine law;logic gate;network switch;spice 2;simulation	Mao-qun Yao;Li-bin Zhang	2013	Journal of Zhejiang University SCIENCE C	10.1631/jzus.C1300069	logic optimization;asynchronous circuit;computer science;theoretical computer science;mathematics;register-transfer level;algorithm;emitter-coupled logic	EDA	19.56389553185013	45.778944664053675	113337
9e1d116538abc02c036abf7ad49ea5b374dc8225	effect of scaling on the area and performance of the h.264/avc full-search fractional motion estimation algorithm on field-programmable gate arrays	xilinx virtex 5 fpga;fractional motion estimation;hdtv video field programmable gate arrays fractional motion estimation h 264 avc video encoding standard high video quality full search fme algorithm xilinx virtex 5 fpga;video coding field programmable gate arrays motion estimation;hdtv video;full search fme algorithm;high video quality;h 264 avc video encoding standard;field programmable gate arrays	Fractional motion estimation (FME) is an important part of the H.264/AVC video encoding standard. The algorithm can significantly increase the compression ratio of video encoders while preserving high video quality. The full-search FME algorithm, however, is computationally expensive and can consist of over 45% of the total motion estimation process. To maximise the performance and efficiency of FME implementations on field-programmable gate arrays (FPGAs), one needs to efficiently exploit the inherent parallelism in the algorithm. The authors investigate the scalability of the full-search FME algorithm on FPGAs and also implemented six scaled versions of the algorithm on Xilinx Virtex-5 FPGAs. The authors found that scaling the algorithm vertically within a 4×4 sub-block is more efficient than scaling horizontally across several sub-blocks. It is shown that, with four reference frames, the best vertically scaled design can achieve 96 frames-per-second (fps) performance while encoding full 1920×1088 progressive HDTV video, and the design only consumes 25.5 K LUTS and 28.7 K registers.	algorithm;field-programmability;field-programmable gate array;h.264/mpeg-4 avc;image scaling;motion estimation	Jasmina Vasiljevic;Andy Gean Ye	2012	IET Computers & Digital Techniques	10.1049/iet-cdt.2010.0167	scalable video coding;embedded system;electronic engineering;real-time computing;quarter-pixel motion;computer science;block-matching algorithm;rate–distortion optimization;field-programmable gate array	EDA	12.452804259999187	40.614724544100866	113837
3503ba736bd36889d075eeac60e1a48f5285ab25	a real-time 5-views hd 1080p architecture for 3d-hevc depth modeling mode 4	video coding field programmable gate arrays logic design;3d video coding;3d hevc;dmm 4;real time 5 views hd 1080p architecture altera stratix v fpga contour computation step texture average calculator hardware architecture hardware acceleration 3d high efficiency video coding standard 2d video coding 3d hevc depth modeling mode 4;architecture design;3d video coding 3d hevc dmm 4 architecture design;computer architecture encoding high definition video adders abstracts three dimensional displays cameras	The 3D video coding increases significantly the complexity of the coding process when compared to 2D video coding. The emergent 3D-High Efficiency Video Coding (3D-HEVC) standard, which is an extension of the High Efficiency Video Coding (HEVC) standard, inserts new tools in the coding process to better deal with 3D video characteristics. This increase in complexity poses new challenges to attend real-time applications constrains, mainly in software solutions, and thus hardware acceleration is necessary to achieve the performance and energy requirements. This paper presents a hardware architecture for the Depth Modeling Mode (DMM) 4 of the 3D-HEVC emergent standard. The designed architecture is capable to encode all available block sizes in parallel by compounding results of smaller blocks into bigger ones. The architecture was divided in two steps: the Texture Average Calculator and the Contour Computation Step, working in a macro pipeline fashion. The proposed architecture was synthesized for an Altera Stratix V FPGA and is capable to process up to six HD 1080p views in real time.	computation;contour line;data compression;encode;emergence;field-programmable gate array;hardware acceleration;high efficiency video coding;real-time clock;real-time transcription;requirement;stereoscopic video coding;stratix	Gustavo Sanchez;Bruno Zatt;Marcelo Schiavon Porto;Luciano Volcan Agostini	2014	2014 27th Symposium on Integrated Circuits and Systems Design (SBCCI)	10.1145/2660540.2661002	scalable video coding;embedded system;real-time computing;h.263;computer science;theoretical computer science;coding tree unit;context-adaptive binary arithmetic coding;h.261;multiview video coding;computer graphics (images)	Graphics	12.618003662659133	40.5556813556494	114033
43d80e712349faf8ec6b28b50e8c9deb0417eddc	parallel image sequence coding on multiprocessor systems	image sequences image coding multiprocessing systems dictionaries data compression hypercubes decoding computer simulation image reconstruction digital tv;hypercube;image coding;image processing;lookup operations;data compression;decoding;time complexity;multiprocessor systems;digital tv;time complexities;single instruction multiple data;dictionary based image sequence coding;parallel imaging;image sequence data compression;computational complexity;image reconstruction;image sequence;dictionaries;lookup operations parallel image sequence coding image sequence data compression multiprocessor systems dictionary based image sequence coding rectangular mesh hypercube time complexities;hypercubes;compression ratio;rectangular mesh;parallel implementation;multiprocessing systems;parallel processing computational complexity data compression hypercube networks image coding image processing;parallel image sequence coding;computer simulation;parallel processing;hypercube networks;image sequences	The authors introduce dictionary-based image sequence coding (DISC) as a new approach to the problem of compression of image sequence data. The DISC algorithm is an adaptation of textual data compression techniques for image sequence data. The algorithm is extremely well suited for parallel implementation on standard configurations such as the rectangular mesh and the hypercube. For N*N images, the authors present SIMD (single-instruction multiple-data) algorithms of time complexities approximately theta (DN) for the mesh and theta (D log N+log/sup 2/ N) for the hypercube (D is proportional to dictionary size). The DISC approach has the additional advantage of involving essentially only simple data movement and lookup operations. Simulation results indicate that moderate to high compression ratios can be achieved along with good visual fidelity and quality of reconstruction. >	multiprocessing	Sanjeev Rampal;Dharma P. Agrawal	1992		10.1109/SPDP.1992.242759	computer simulation;parallel processing;parallel computing;image processing;computer science;theoretical computer science;distributed computing;algorithm;hypercube	Robotics	11.922559516206045	37.571044255996924	114103
b1a06348636b68a68e301645e35b33073eea3aca	a single-channel architecture for algebraic integer-based 8 $\,\times\,$8 2-d dct computation	cmos integrated circuits;video signal processing;parallel architectures;application specific integrated circuits;discrete cosine transforms;field programmable gate arrays	An area efficient row-parallel architecture is proposed for the real-time implementation of bivariate algebraic integer (AI) encoded 2-D discrete cosine transform (DCT) for image and video processing. The proposed architecture computes 8 × 8 2-D DCT transform based on the Arai DCT algorithm. An improved fast algorithm for AI-based 1-D DCT computation is proposed along with a single channel 2-D DCT architecture. The design improves on the four-channel AI DCT architecture that was published recently by reducing the number of integer channels to one and the number of eight-point 1-D DCT cores from five down to two. The architecture offers exact computation of 8 × 8 blocks of the 2-D DCT coefficients up to the FRS, which converts the coefficients from the AI representation to fixed-point format using the method of expansion factors. Prototype circuits corresponding to FRS blocks based on two expansion factors are realized, tested, and verified on FPGA-chip, using a Xilinx Virtex-6 XC6VLX240T device. Post place-and-route results show a 20% reduction in terms of area compared to the 2-D DCT architecture requiring five 1-D AI cores. The area-time and area-time2 complexity metrics are also reduced by 23% and 22% respectively for designs with eight-bit input word length. The digital realizations are simulated up to place and route for ASICs using 45 nm CMOS standard cells. The maximum estimated clock rate is 951 MHz for the CMOS realizations indicating 7.608·109 pixels/s and a 8 × 8 block rate of 118.875 MHz.	4-bit;8-bit;algorithm;application-specific integrated circuit;bivariate data;cmos;clock rate;coefficient;computation;discrete cosine transform;field-programmable gate array;level of detail;linear algebra;parallel computing;pixel;place and route;prototype;real-time clock;video processing	Amila Edirisuriya;Arjuna Madanayake;Renato J. Cintra;Vassil S. Dimitrov;Nilanka T. Rajapaksha	2013	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2013.2270397	parallel computing;trellis quantization;computer science;theoretical computer science;application-specific integrated circuit;cmos;field-programmable gate array	EDA	12.471113311175325	42.0237212748484	114121
e7d446506b2f26e75dbfe74f134d0b959879bb60	a reversible version of 4 x 4 bit array multiplier with minimum gates and garbage outputs			4-bit;bit array	Himanshu Thapliyal;M. B. Srinivas;Hamid R. Arabnia	2005			parallel computing;computer science;theoretical computer science;garbage;4-bit;multiplier (economics)	Theory	16.666470118045126	45.022999106635226	114372
edc9a7e8bda8acaa03f7faaa1b90edf405671542	pipelined architecture for a radix-2 fast walsh–hadamard–fourier transform algorithm	single path delay commutator fast walsh hadamard fourier transform fwft merged half butterfly;computer architecture discrete fourier transforms signal processing algorithms adders delays;fft wht sequence ordered complex hadamard transform radix 2 fast walsh hadamard fourier transform pipelined architecture radix 2 single path delay commutator hardware utilization buffer usage common merged half butterflies time multiplexed approach complex multipliers;computer architecture;adders;pipeline processing buffer circuits fast fourier transforms hadamard transforms hypercube networks multiplying circuits;signal processing algorithms;discrete fourier transforms;delays	This brief proposes an efficient radix-2 single-path delay commutator (SDC) pipelined architecture to implement the fast Walsh-Hadamard-Fourier transform (FWFT) algorithm. The proposed architecture includes (log2 N - 1) SDC stages, which are implemented by merged half-butterfly. The merged half-butterfly is proposed to achieve 100% hardware utilization and minimum buffer usage by sharing common merged half-butterflies in the time-multiplexed approach. Compared with the conventional pipelined radix-2 FFT+Walsh-Hadamard Transform (WHT) designs, the proposed architecture reduces the number of buffers by 50% and of adders by 25%. The required number of complex multipliers is decreased to 0.5 log2 N - 0.5, which is roughly the minimum number. Moreover, the proposed architecture can be applied to FFT/WHT/sequence-ordered complex Hadamard transform (SCHT).	algorithm;binary logarithm;central processing unit;fast fourier transform;fast walsh–hadamard transform;hadamard transform;multiplexing;pipeline (computing);smart data compression	Jinqi Liu;Qianjian Xing;Xiaobo Yin;Xiubin Mao;Feng Yu	2015	IEEE Transactions on Circuits and Systems II: Express Briefs	10.1109/TCSII.2015.2456371	fast fourier transform;electronic engineering;parallel computing;harmonic wavelet transform;hadamard transform;split-radix fft algorithm;computer science;engineering;theoretical computer science;discrete fourier transform;mathematics;discrete fourier transform;prime-factor fft algorithm;adder	Arch	12.492808449031514	43.64266969113797	114751
1f5a8c7e67391cd6235239cdea052eb154a63235	limits of using signatures for permutation independent boolean comparison	logic design;boolean functions;boolean function;logic synthesis;logic testing;equivalence classes;technology mapping;logic cad;combinational circuits	This paper addresses problems that arise while checking the equivalence of two Boolean functions under arbitrary input permutations. The permutation problem has several applications in the synthesis and verification of combinational logic: It arises in the technology mapping stage of logic synthesis and in logic verification. A popular method to solve it is to compute a signature for each variable that helps to establish a correspondence between the variables. Several researchers have suggested a wide range of signatures that have been used for this purpose. However, for each choice of signature, there remain variables that cannot be uniquely identified. Our research has shown that, for a given example, this set of problematic variables tends to be the same – regardless of the choice of signatures. The paper investigates this problem.	algorithm;antivirus software;blue (queue management algorithm);central processing unit;combinational logic;digital signature;electronic signature;logic programming;logic synthesis;turing completeness	Janett Mohnke;Paul Molitor;Sharad Malik	1995		10.1145/224818.224955	fuzzy logic;boolean algebra;boolean circuit;and-inverter graph;combinatorics;circuit minimization for boolean functions;discrete mathematics;logic synthesis;logic optimization;boolean domain;boolean expression;product term;computer science;predicate functor logic;mathematics;sequential logic;signature;combinational logic;boolean function;multimodal logic;algorithm	Logic	19.38315342910498	46.06405096210073	115308
288dce9973ea10790a25f3d791a7ddca68dace3f	an area efficient video/audio codec for portable multimedia application	audio signal processing;encoding decoding;0 5 micron area efficient video audio codec portable multimedia application single chip encoder decoder vasp video audio signal processor video signal processing block audio signal processing block mixed hardware software architecture full pixel motion estimation half pixel motion estimation discrete cosine transform quantization run length coding host interface risc type internal controller fixed point dsp fifo sram rom 3 layers metal cmos technology dct 16 bit 22 kbit 107 kbit 556 kbit;video signal processing;reduced instruction set computing;speech coding;motion estimation;multimedia application;quantization signal;multimedia systems;discrete cosine transform;fixed point;chip;input output;video coding;audio coding;software architecture;cmos digital integrated circuits;speech codecs;discrete cosine transforms;runlength codes;codecs video signal processing motion estimation cmos technology decoding application software signal processing hardware software architecture process design;video codecs;digital signal processing chips;runlength codes cmos digital integrated circuits video codecs audio coding video coding speech codecs speech coding digital signal processing chips multimedia systems motion estimation reduced instruction set computing discrete cosine transforms quantization signal	In this paper, we present an area efficient of video and audio single chip encodeddecoder for portable multimedia application. The single-chip called as VASP(Video Audio Signal Processor) consists of video signal processing block and audio signal processing block. This chip has mixed hardware/software architecture to combine performance and flexibility. The video signal processing block was designed to implement hardwired solution of pixel input/output, full pixel motion estimation, half pixel motion estimation, discrete cosine transform, quantization, run length coding, host interface, and. 16bit RISC type intemal controller. The audio signal processing block is implemented with sofiware solution using 16 bits fixed point DSP. This chip contains 142,300 gates, 22k bits FIFO, 107k bits S U M , and 556k bits ROM, and the chip size was 9.02 mm x 9.06 mm which was fabricated using 0.5 micron 3-layers metal CMOS technology.	audio signal processor;audio signal processing;cmos;codec;digital signal processor;discrete cosine transform;fifo (computing and electronics);fixed point (mathematics);input/output;motion estimation;pixel;read-only memory;run-length encoding;software architecture;sound card	Seongmo Park;Seongmin Kim;Kyeongjin Byeon;Jinjong Cha;Hanjin Cho	2000		10.1109/ISCAS.2000.857165	chip;input/output;embedded system;software architecture;reduced instruction set computing;electronic engineering;computer hardware;audio signal processing;computer science;operating system;speech coding;discrete cosine transform;motion estimation;fixed point;audio signal flow	EDA	10.877869934191429	41.07753615576571	115726
e38186a0312b7d6ddff2be7d2505b8deee833417	randomized uniform selfstabilizing mutual exclusion	mutual exclusion	 A system is self-stabilizing if when started in any arbitrary configuration it eventually reaches a legal configuration and then remains in legal configurations. A mutual exclusion is valid when in each configuration only one processor is in a privileged state while all the other are in unprivileged states and any processor get the privilege infinitely often. In this article, we present a randomized self-stabilizing mutual exclusion. It works on any (finite connected) uniform graphs, i.e.,... 	mutual exclusion;randomized algorithm	Jérôme Olivier Durand-Lose	1998			computer science;distributed computing;mutual exclusion	Robotics	17.259608632477086	34.68546463879944	115732
70bf7fca1ff2705acd0e0b682aeaaf4256f05aa6	time optimal algorithms for black hole search in rings	black hole;asymptotic optimality;time complexity;ring network;timing optimization	In a network environments supporting mobile entities (called robots or agents), a black hole is harmful site that destroys any incoming entity without leaving any visible trace. The black-hole search problem is the task of a team of k > 1 mobile entities, starting from the same safe location and executing the same algorithm, to determine within finite time the location of the black hole. In this paper we consider the black hole search problem in asynchronous ring networks of n nodes, and focus on the time complexity.#R##N##R##N#It is known that any algorithm for black-hole search in a ring requires at least 2(n-2) time in the worst case. The best algorithm achieves this bound with a team of n - 1 agents with an average time cost 2(n - 2), equal to the worst case. In this paper we first show how the same number of agents using 2 extra time units from optimal in the worst case, can solve the problem in only 7/4n- O(1) time on the average. We then prove that the optimal average case complexity 3/2n - O(1) can be achieved without increasing the worst case using 2(n- 1) agents Finally we design an algorithm that achieves asymptotically optimal both worst case and average case time complexity employing an optimal team of k = 2 agents, thus improving on the earlier results that required O(n) agents.	algorithm;black hole	Balasingham Balamohan;Paola Flocchini;Ali Miri;Nicola Santoro	2010		10.1007/978-3-642-17461-2_5	time complexity;best, worst and average case;ring network;mathematical optimization;black hole;combinatorics;computer science;mathematics;distributed computing;algorithm	Theory	17.775400120216197	34.10373399485507	115778
2a2546258a54603e08541edb16c46750ea559a12	high-speed moving picture coding using adaptively load balanced multiprocessor system	motion pictures;multiprocessor systems;motion estimation;adaptive load balancing;linear program;load balance;high performance;high speed;parallel processing	Abstract   A multiprocessor system for high-speed processing of hybrid picture coding algorithms such as H.261, MPEG or digital HDTV is presented in this study. Using a combination of a highly parallel 32-bit microprocessor, DCT and motion estimation function specific devices, a new processing module is designed for a high-performance coding system. We constructed the motion picture coder using the geometrical parallel processing technique since a single module alone cannot perform hybrid encoding algorithms at high speed, and also analyzed the processing time and communication overhead. Theoretical calculations and experimental results show that the efficiency of geometrical parallel processing system falls off as the difference of the computational amount among regions allocated to each processing module increases. An adaptive load balancing technique is proposed to resolve this performance degradation in geometrical partitioning which results from unbalanced processing time between regions. In the proposed algorithm, load estimation using DCT coefficients and load reallocation using the linear programming method are employed for optimal load balancing so that each processing module has a balanced processing time. A more balanced processing time is obtained using the adaptive load balancing technique compared to the method using only geometrical partitioning. This results in an increase of overall efficiency.	multiprocessing	S. H. Choi;K. T. Park	1996	Sig. Proc.: Image Comm.	10.1016/0923-5965(95)00040-2	parallel processing;computer vision;parallel computing;real-time computing;computer science;load balancing;theoretical computer science;motion estimation	Robotics	11.953160764487738	39.92741823694122	115861
b05d92d1721982b0ebf7d2884174706981795906	on generalized fibonacci cubes and unitary transforms	processor architecture;topological properties;fast algorithm;orthogonal transformation;hardware implementation	 We present a new interconnection topology called generalized Fibonacci topology, which unifies a wide range of connection topologies such as the Boolean cube (or hypercube), classical Fibonacci cube, etc. Some basic topological properties of generalized Fibonacci cubes are established. Finally, we developed new classes of the discrete orthogonal transforms, based on the generalized Fibonacci recursions. They can be implemented efficiently by butterfly-type networks (like the Fourier, or the Haar transforms). A generalized Fibonacci cube based processor architecture (generalizing the known SIMD architecture — hypercube processor) can be efficiently used for hardware implementation of the proposed discrete orthogonal transforms.	cubes;fibonacci cube;fibonacci heap;haar wavelet;interconnection;olap cube;recursion;simd	Karen O. Egiazarian;Jaakko Astola	1997	Applicable Algebra in Engineering, Communication and Computing	10.1007/s002000050074	combinatorics;discrete mathematics;fibonacci number;microarchitecture;theoretical computer science;fibonacci search technique;mathematics;fibonacci cube;orthogonal transformation	DB	23.256735443017792	36.51400881755738	115967
a719ee4d5a201669c768354c57f6e810a2ba46bc	a novel approach to perform reversible addition/subtraction operations using deoxyribonucleic acid	dna logic gates adders dna computing vectors computers;subtractio reversible logic dna and dna bases dna annealing reversible logic gates dna computing addition;run time complexity reversible addition subtraction operations deoxyribonucleic acid dna based logic gates reversible logic logic signal silicon computers computing systems dna based reversible adder subtractor circuit dna strands dna based system dna signals;logic gates adders computational complexity dna	Reversible logic transforms logic signal in a way that allows the original input signals to be recovered from the produced outputs, has attracted great attention because of its application in many areas. Traditional silicon computers consume much more power compared to computing systems based on Deoxyribonucleic Acid (DNA). In addition, DNA-based logic gates are stable and reusable. In this paper, we propose a new approach for designing DNA-based reversible adder/subtractor circuit; it's possible to perform addition and subtraction operations using single circuit representation. We first merge the properties of addition and subtraction operations. Then, we demonstrate reversible DNA-based addition and subtraction operations. Our proposed DNA-based reversible addition/subtraction circuit is faster than the conventional one due to parallelism and replication properties of DNA strands. It also requires less space because of compactness of DNA strands. In addition, the DNA-based adder/subtractor circuit needs low power as the formation of DNAs consumes a small amount of energy. Finally, the comparative results show that the proposed DNA-based system requires m+3.2n DNA signals, but in existing system, it requires m.2n, where m is the size of extra tags and n is the total number of bits. Besides, the run time complexity of proposed system has O(1) while the existing system has O(mln2n).	acid;adder (electronics);computation;computer;error detection and correction;fan-out;integrated circuit;logic gate;parallel computing;reversible computing;run time (program lifecycle phase);subtractor;time complexity	Ankur Sarker;Hafiz Md. Hasan Babu;Md. Saiful Islam	2014	2014 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2014.6865513	electronic engineering;theoretical computer science;mathematics;algorithm	Arch	16.82859230885533	45.02509498422861	116042
244294dbe0be25838d9e2b6191238fc04fecc143	space-efficient straggler identification in round-trip data streams via newton's identities and invertible bloom filters	symmetric polynomial;bloom filter;data stream;lower bound	We study the straggler identification problem, in which an algorithm must determine the identities of the remaining members of a set after it has had a large number of insertion and deletion operations performed on it, and now has relatively few remaining members.	bloom filter;newton	David Eppstein;Michael T. Goodrich	2007		10.1007/978-3-540-73951-7_55	combinatorics;symmetric polynomial;computer science;theoretical computer science;bloom filter;mathematics;distributed computing;upper and lower bounds;programming language;algorithm	Theory	21.06135689708281	37.226955536761416	116348
49f4e090f502056671766d0ac457540a93061622	improved precise fault diagnosis algorithm for hypercube-like graphs		The system reliability is an important issue for multiprocessor systems. The fault diagnosis has become crucial for achieving high reliability in multiprocessor systems. In the comparison-based model, it allows a processor to perform diagnosis by contrasting the responses from a pair of neighboring processors through sending the identical assignment. Recently, Ye and Hsieh devised an precise fault diagnosis algorithm to detect all faulty processors for hypercube-like networks by using the MM* model with (O(N(log _{2}N)^2)) time complexity, where N is the cardinality of processor set in multiprocessor systems. On the basis of Hamiltonian cycle properties, we improve the aforementioned results by presenting an O(N)-time precise fault diagnosis algorithm to detect all faulty processors for hypercube-like networks by using the MM* model.	medical algorithm	Tai-Ling Ye;Dun-Wei Cheng;Sun-Yuan Hsieh	2016		10.1007/978-3-319-48749-6_8	discrete mathematics;time complexity;cardinality;hypercube;multiprocessing;hamiltonian path;algorithm;graph;computer science	PL	23.861513606726607	43.73592559222031	116359
b0dcb0ae07874664a4dd7d50b249bb70127397b1	truncated squarer with minimum mean-square error	squarer;computer arithmetic;vlsi systems;truncated multiplier	Squaring is an important arithmetic operation required in a multitude of applications. In this paper we present a truncated squarer that, with an n-bit input, produces its output on a number of bits that can be defined at design time in the [n,2n] range. For each configuration, some of the partial products are unformed, to reduce area and power, and error compensation function is introduced to minimize the mean-square error. As shown by synthesis results in 90 nm CMOS, the proposed squarer has similar hardware complexity as previously proposed architectures but provides lower approximation error.#R##N##R##N#The implementation of the proposed circuit can be automated with the aid of a provided Matlab script.		Nicola Petra;Davide De Caro;Valeria Garofalo;Ettore Napoli;Antonio Giuseppe Maria Strollo	2014	Microelectronics Journal	10.1016/j.mejo.2014.02.018	electronic engineering;algorithm	Theory	13.559746767321249	44.344686275206286	116396
12e03b3e3ee0939facb7c4b32a5a50853156072c	fast linearly independent arithmetic expansions	kernel;lia logic;logic design;boolean functions;arithmetic and adding transforms;arithmetic expansions;logic circuits;fast transforms;circuit analysis;stochastic processes;algebra;linearly independent arithmetic transform;logic functions;transforms;digital arithmetic logic design transforms;arithmetic logic circuits logic functions boolean functions algebra circuit testing circuit analysis logic design kernel stochastic processes;arithmetic;linearly independent logic;digital arithmetic;circuit testing;arithmetic expansions fast linearly independent arithmetic expansions inverse fast transforms lia logic;inverse fast transforms;fast linearly independent arithmetic expansions	The concept of Linearly Independent arithmetic (LIA) transforms and expansions is introduced in this paper. The recursive ways of generating forward and inverse fast transforms for LIA are presented. The paper describes basic properties and lists those LIA transforms which have convenient fast forward algorithms and easily defined inverse transforms. In addition, those transforms which require horizontal or vertical permutations to have fast transform are also discussed. The computational advantages and usefulness of new expansions based on LIA logic in comparison to known arithmetic expansions are discussed.		Susanto Rahardja;Bogdan J. Falkowski	1999	IEEE Trans. Computers	10.1109/12.795227	arithmetic;stochastic process;discrete mathematics;kernel;logic synthesis;logic gate;network analysis;computer science;mathematics;boolean function;algebra	Vision	18.210636742102373	44.421124338397874	116455
5551a239df8c81a92967cd8a7a93567a0c68cd3c	distributed computation of sparse cuts.		Finding sparse cuts is an important tool in analyzing large-scale distributed networks such as the Internet and Peer-to-Peer networks, as well as large-scale graphs such as the web graph, online social communities, and VLSI circuits. Sparse cuts are useful in graph clustering and partitioning among numerous other applications. In distributed communication networks, they are useful for topology maintenance and for designing better search and routing algorithms. In this paper, we focus on developing fast distributed algorithms for computing sparse cuts in networks. Given an undirected n-node network G with conductance φ, the goal is to find a cut set whose conductance is close to φ. We present two distributed algorithms that find a cut set with sparsity Õ( √ φ) (Õ hides polylog n factors). Both our algorithms work in the CONGEST distributed computing model and output a cut of conductance at most Õ( √ φ) with high probability, in Õ(1b ( 1 φ + n)) rounds, where b is balance of the cut of given conductance. In particular, to find a sparse cut of constant balance, our algorithms take Õ( 1 φ + n) rounds. Our algorithms can also be used to output a local cluster, i.e., a subset of vertices near a given source node, and whose conductance is within a quadratic factor of the best possible cluster around the specified node. Both our distributed algorithm can work without knowledge of the optimal φ value and hence can be used to find approximate conductance values both globally and with respect to a given source node. We also give a lower bound on the time needed for any distributed algorithm to compute any non-trivial sparse cut — any distributed approximation algorithm (for any non-trivial approximation ratio) for computing sparsest cut will take Ω̃( √ n+D) rounds, where D is the diameter of the graph. Our algorithm can be used to find sparse cuts (and their conductance values) and to identify well-connected clusters and critical edges in distributed networks. This in turn can be helpful in the design, analysis, and maintenance of topologically-aware networks.	approximation algorithm;cluster analysis;computation;conductance (graph);cut (graph theory);distributed algorithm;distributed computing;graph (discrete mathematics);heuristic;internet;network congestion;peer-to-peer;provable prime;provable security;routing;sparse matrix;telecommunications network;time complexity;webgraph;with high probability	Atish Das Sarma;Anisur Rahaman Molla;Gopal Pandurangan	2013	CoRR		conductance;distributed computing;quadratic equation;distributed algorithm;vertex (geometry);computation;approximation algorithm;cut;computer science;upper and lower bounds	Theory	19.54761346461153	34.45506198108285	116769
9b76648cc70c2a6a5ef195da6b8aaf44f4aa2a5c	fpga implementation of an efficient multiplier over finite fields gf(2/sup m/)	binary extension fields;m clock cycles;complexity theory;finite fields;software platform;building block;arithmetic operators;serial parallel lsb first;space complexity fpga implementation finite fields error correcting codes signal processing arithmetic operators field multiplication software platforms serial multiplier binary extension fields elliptic curve cryptography serial parallel lsb first m clock cycles;finite field;fpga implementation;error correcting codes;elliptic curve cryptography;error correction code;field multiplication;computational complexity;signal processing;field programmable gate arrays computational complexity digital arithmetic;space complexity;serial multiplier;digital arithmetic;field programmable gate arrays;software platforms;hardware implementation;field programmable gate arrays galois fields arithmetic elliptic curve cryptography nist error correction codes signal processing hardware software performance delay;national institute of standards and technology	Arithmetic operations over finite fields GF(2m) are widely used in cryptography, error-correcting codes and signal processing. In particular, multiplication is especially relevant since other arithmetic operators, such as division or exponentiation, which they usually utilize multipliers as building blocks. Hardware implementation of field multiplication may provide a great speedup in procedure's performance, which easily exceeds the one observed in software platforms. In this paper we deal with an FPGA implementation of an efficient serial multiplier over the binary extension fields GF(2193) and GF(2239). Those extension fields are included among the ones recommended by NIST (National Institute of Standards and Technology) standards for Elliptic Curve Cryptography. Our multiplier is of type Serial/Parallel LSB-first and operates with a latency of m-clock cycles, where m is the length of the field word. We calculate the space complexity attending the number of slices used in the FPGA	clock signal;code;dspace;elliptic curve cryptography;error detection and correction;field-programmable gate array;forward error correction;least significant bit;signal processing;speedup	Mario Alberto Garcia Martinez;Rubén Posada-Gómez;Guillermo Morales-Luna;Francisco Rodríguez-Henríquez	2005	2005 International Conference on Reconfigurable Computing and FPGAs (ReConFig'05)	10.1109/RECONFIG.2005.18	computer science;theoretical computer science;signal processing;finite field	EDA	10.60089558963592	43.74506696627913	116922
ffa4780a687d25cf2ca45ac0763c5461d9cfff98	on accumulator-based bit-serial test response compaction schemes	design for testability;circuit faults;post compaction fault coverage drop;bit serial response compaction;automatic test pattern generation;automatic testing;post compaction fault coverage drop data paths built in self test testing structures bit serial response compaction parallel test response compaction iscas 85 benchmark circuits;built in self test;test pattern generators;compaction;registers;adders;logic testing;integrated circuit testing;fault coverage;circuit testing;parallel test response compaction;compact scheme;compaction circuit testing circuit faults built in self test test pattern generators adders automatic testing design for testability integrated circuit testing registers;iscas 85 benchmark circuits;fault diagnosis;data paths;arithmetic circuit;testing structures;automatic test pattern generation built in self test logic testing fault diagnosis design for testability integrated circuit testing	The data paths of most contemporary general and special purpose processors include registers, adders and other arithmetic circuits. If these circuits are also used for Built-In Self Test, the extra area required for embedding testing structures can be cut down eflciently. Several schemes based on accumulators, subtracters, multipliers and shgt registers have been proposed and analyzed in the past for parallel test response compaction, whereas some efforts have also been devoted in the bit-serial response compaction case. In this paper, we analyze and evaluate the bit-serial version of a recently proposed scheme for parallel test response compaction [5/. Experimental results on the ISCAS'85 benchmark circuits indicate that the post-compaction fault coverage drop attained by the new scheme is significantly lower than other already known accumulator-based compaction schemes.	accumulator (computing);adder (electronics);arithmetic logic unit;benchmark (computing);built-in self-test;central processing unit;data compaction;fault coverage;integrated circuit;processor register;pseudorandomness;serial communication;year 10,000 problem	Dimitris Bakalis;Dimitris Nikolos;Haridimos T. Vergos;Xrysovalantis Kavousianos	2001		10.1109/ISQED.2001.915255	compaction;electronic engineering;parallel computing;real-time computing;fault coverage;computer science;engineering;automatic test pattern generation;operating system;design for testing;processor register;adder	EDA	12.7433428796371	45.33766360313482	116929
916bdbe2c46f8c306f75a9dad7127bf9d8375935	a new computation of shape moments via quadtree decomposition	analisis imagen;algoritmo paralelo;parallel algorithm;algorithm performance;algorithm complexity;quad tree;quad arbol;complejidad algoritmo;algorithme parallele;complexite algorithme;cost optimization;computational complexity;resultado algoritmo;informatique theorique;performance algorithme;quad arbre;image analysis;algoritmo optimo;algorithme optimal;optimal algorithm;analyse image;computer theory;informatica teorica	The main contribution of this paper is in designing an optimal and/or optimal speed-up algorithm for computing shape moments. We introduce a new technique for computing shape moments. The new technique is based on the quadtree representation of images. We decompose the image into squares, since the moment computation of squares is easier than that of the whole image. The proposed sequential algorithm reduces the computational complexity signi cantly. By integrating the advantages of both optical transmission and electronic computation, the proposed parallel algorithm can be run in O(1) time. In the sense of the product of time and the number of processors used, the proposed parallel algorithm is time and cost optimal and achieves optimal speed-up.	central processing unit;computation;computational complexity theory;cost efficiency;parallel algorithm;quadtree;sequential algorithm	Chin-Hsiung Wu;Shi-Jinn Horng;Pei-Zong Lee;Shung-Shing Lee;Shih-Ying Lin	2000		10.1007/3-540-45591-4_15	computer vision;mathematical optimization;combinatorics;parallel computing;image analysis;computer science;quadtree;mathematics;parallel algorithm;computational complexity theory;algorithm	Theory	11.949222380981752	35.32436890437313	116950
0930372cf70f1b6c0e8a785fb1c5628ad3e653ae	on the accuracy and range of binary representations of floating point numbers				Allan G. Bromley	1976	Australian Computer Journal		data mining;computer science;floating point;theoretical computer science;binary number	Theory	21.028951789594576	41.562122988086344	117025
f0e14d22ce21876ac185c4352bca2e2b4cf3b5bb	reduced memory architecture for cordic-based fft	random access memory;generators;memory management;memory architecture digital arithmetic fast fourier transforms field programmable gate arrays;memory architecture signal processing algorithms hardware pipelines read only memory synthetic aperture radar computer architecture field programmable gate arrays logic very large scale integration;radix size fft;reduced memory architecture;fpga hardware reduced memory architecture cordic based fft memory cordic based architecture radix size fft multibank memory structure twiddle factors;registers;theoretical analysis;memory architecture;cordic based fft;data access;fast fourier transforms;digital arithmetic;memory cordic based architecture;field programmable gate arrays;multibank memory structure;twiddle factors;algorithm design and analysis;fpga hardware;conferences;hardware	In this paper, a new pipelined, reduced memory CORDIC-based architecture is presented for any radix size FFT. A multi-bank memory structure and the corresponding addressing scheme are used to realize the parallel and in-place data accesses. The proposed memory-reduced CORDIC algorithm eliminates the need for storing twiddle factors and angles, resulting in significant area savings with no negative impact on performance. As a case study, the radix-2 and radix-4 FFT algorithms have been implemented on FPGA hardware. The synthesis results match the theoretical analysis and it can be observed that more than 20% reduction can be achieved in total memory logic.	addressing scheme;cordic;fast fourier transform;field-programmable gate array;in-place algorithm;twiddle factor	Xin Xiao;Erdal Oruklu;Jafar Saniie	2010	Proceedings of 2010 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2010.5537045	data access;algorithm design;fast fourier transform;computer architecture;twiddle factor;parallel computing;computer hardware;computer science;processor register;algorithm;field-programmable gate array;memory management	Arch	11.979832668187886	44.25468923078469	117597
eb90999fed3f3c483e8ba31e9363df281a894ffa	regular realization of symmetric functions using reversible logic	logic design boolean functions;logic design;symmetric function;boolean functions;multi input multi output boolean functions regular realization symmetric functions reversible logic arbitrary symmetric function boolean function;boolean function;multi input multi output;input output;reversible logic;quantum computing circuit synthesis cmos technology cmos logic circuits signal synthesis boolean functions logic circuits logic design input variables moore s law;classical logic	Reversible logic is of growing importance to many future computer technologies. We introduce a regular structure to realize symmetric functions in binary reversible logic. This structure, called a 2 * 2 Net Structure, allows for more efficient realization of symmetric functions than the methods shown by previous authors. Our synthesis method allows to realize arbitrary symmetric function in a completely regular structure of reversible gates with smaller “garbage”. Because every Boolean function is symmetrizable by repeating input variables, our method is applicable to arbitrary multi-input, multi-output Boolean functions and realizes such arbitrary function in a circuit with a relatively small number of additional gate outputs. The method can be also used in classical logic. Its advantages in terms of numbers of gates and inputs/outputs are especially seen for symmetric or incompletely specified functions with many outputs.	reversible computing	Marek A. Perkowski;Malgorzata Chrzanowska-Jeske;Alan Mishchenko;Xiaoyu Song;Anas Al-Rabadi;Barton C. Massey;Pawel Kerntopf;Andrzej Buller;Lech Józwiak;Alan J. Coppola	2001		10.1109/DSD.2001.952289	boolean algebra;boolean circuit;and-inverter graph;and-or-invert;circuit minimization for boolean functions;logic synthesis;logic optimization;boolean domain;majority function;boolean expression;logic gate;logic family;product term;computer science;theoretical computer science;pass transistor logic;sequential logic;combinational logic;boolean function;digital electronics;algorithm;parity function	Theory	19.240217807919734	44.65683382524169	117731
0e740013fe229e09cb0c1f0707dff0f462b7c110	an improved gf(2) matrix inverter with linear time complexity	clocks hardware complexity theory inverters field programmable gate arrays matrices transforms;gj algorithm;complexity theory;time complexity;clocks;gauss jordan elimination method gf 2 matrix inverter linear time complexity boolean matrix inverter clock cycle fpga device real time system gj algorithm variable time implementation;inverters;real time systems boolean algebra computational complexity field programmable gate arrays logic gates matrix inversion;fpga;matrix inversion;boolean matrix inverter;gauss jordan;boolean algebra;gf 2 matrix inverter;matrices;logic gates;computational complexity;fpga device;vhdl;linear time;transforms;linear time complexity;real time system;matrix inverter;gauss jordan elimination method;variable time implementation;field programmable gate arrays;clock cycle;high performance;hardware implementation;boolean matrix;hardware;real time systems;vhdl boolean matrix matrix inverter gauss jordan fpga	This paper presents a new hardware implementation for boolean matrix inverters. A circuit capable of inverting a nonsingular N×N matrix in exactly N clock cycles is introduced, described, and tested in FPGA devices. This is an improvement over the fastest implementation reported to date, which computes the inverted matrix in 2N clock cycles on average or (N2+N)/2 clock cycles in the worst case. The time complexity of the proposed circuit is fixed (N clock cycles), which is important when the circuit must be used as part of a high performance or real-time system. The overall circuit operation is based on the Gauss-Jordan (GJ) elimination method, with the addition of several modifications in order to make the algorithm more hardware-oriented. The resulting hardware is still compact compared to a direct implementation of the traditional GJ algorithm, however less compact than the variable-time implementation referred to above.	algorithm;best, worst and average case;clock signal;fastest;field-programmable gate array;gauss–kronrod quadrature formula;gauss–seidel method;inverter (logic gate);power inverter;real-time clock;real-time computing;the matrix;time complexity	Ricardo P. Jasinski;Volnei A. Pedroni;Antonio Gortan;Walter Godoy	2010	2010 International Conference on Reconfigurable Computing and FPGAs	10.1109/ReConFig.2010.86	time complexity;embedded system;real-time operating system;clock skew;computer science;theoretical computer science;synchronous circuit;algorithm;field-programmable gate array	EDA	10.81476257439092	43.5101108889253	117742
e1fc7ea9c3eb77b2b95c3b0f1136a3ffc64e2b1b	an asynchronous 4-to-4 aer mapper	distributed system;field programmable gate array;look up table;random access memory;entrada salida;systeme reparti;memoria acceso directo;protocole transmission;complex network;circuit vlsi;intelligence artificielle;red puerta programable;reseau porte programmable;sistema reactivo;chip;input output;address event representation;tabla de consulta;protocolo transmision;vlsi circuit;sistema repartido;network connectivity;biomimetique;memoire acces direct;envoi message;message passing;reactive system;systeme reactif;communication protocol;artificial intelligence;lookup table;potencial accion;table conversion;inteligencia artificial;circuito vlsi;reseau neuronal;action potential;red neuronal;potentiel action;biomimetics;neural network;entree sortie;transmission protocol	In this paper, a fully functional prototype of an asynchronous 4-to-4 Address Event Representation (AER) mapper is presented. AER is an event driven communication protocol originally used in VLSI implementations of neural networks to transfer action potentials between neurons. Often, this protocol is used for direct inter-chip communication between neuromorphic chips containing assemblies of neurons. Without an active device between two such chips, the network connections between them are hard-wired in the chip design. More flexibility can be achieved by communicating through an AER mapper: The network can freely be configured and, furthermore, several AER busses can be merged and split to form a complex network structure. We present here an asynchronous AER mapper which offers an easy and versatile solution. The AER mapper receives input from four different AER busses and redirects the input AE to four output AER busses. The control circuitry is implemented on an FPGA and is fully asynchronous, and pipelining is used to maximize throughput. The mapping is performed according to preprogrammed lookup tables, which is stored on external RAM. The mapper can emulate a network of up to 2 direct connections and test results show that the mapper can handle as much as 30 × 10 events/second.	action potential;artificial neural network;asynchronous circuit;bus (computing);circuit design;communications protocol;complex network;electronic circuit;field-programmable gate array;glitch;lookup table;mapper;neuromorphic engineering;pipeline (computing);prototype;random-access memory;robustness (computer science);speedup;throughput;very-large-scale integration	Håvard Kolle Riis;Philipp Häfliger	2005		10.1007/11494669_61	embedded system;lookup table;telecommunications;computer science;artificial intelligence;operating system;programming language;artificial neural network	HPC	17.77257096101113	40.30382785884775	117755
f3b13ce36f9706291ba2103f105a08b83b6db52f	a novel architecture of local memory for programmable simd vision chip	simd;image processing;vision chip image processing latches memory architecture simd;vision chip;latches registers arrays microprocessors memory architecture image edge detection;memory architecture;latches	This paper presents a novel architecture of the local memory for the programmable SIMD vision chip. The memory architecture consists of 8 × 8 local memory cells, among which each 8 static latches in the master stage share one dynamic latch in the slave stage. The local memory performs single bit read and write in each clock cycle, and the compact area of 14.33 μm2/bit increases the integration level of the processor. A prototype chip with 64 × 64 processing units has been manufactured in 0.18 μm CMOS technology. Five types of local memory architecture have been designed, and an 8-bit input data buffer based on dedicated latch structures has been designed as the input data buffer for each processing unit. Test results show that the presented structure is suitable for real-time computer vision applications such as the edge detection at the speed of 1000 fps.	8-bit;algorithm;cmos;clock signal;computer vision;data buffer;edge detection;flops;flip-flop (electronics);machine vision;prototype;real-time clock;simd	Zhe Chen;Jie Yang;Cong Shi;Nanjian Wu	2013	2013 IEEE 10th International Conference on ASIC	10.1109/ASICON.2013.6811989	uniform memory access;shared memory;interleaved memory;computer architecture;semiconductor memory;parallel computing;memory bank;sense amplifier;memory refresh;computer hardware;computer science;computer memory;non-volatile random-access memory;conventional memory;flat memory model;registered memory;cache-only memory architecture;memory map	EDA	10.605427832571008	40.06885065404443	117905
c87a2489ed6270c97df2dbc14d7be6053370d53b	vlsi architectures for computations in finite rings and fields			computation	Andreas Curiger	1993			very-large-scale integration;theoretical computer science;computation;computer science	HPC	17.222920247105662	42.22610354062773	118140
6ef0804cb7b89a61ec51863db178b721b4f52596	symmetric network computation	fault tolerant;building block;symmetry;agents;fault tolerance;election;distributed algorithm;network computing	We introduce a simple new model of distributed computation -- finite-state symmetric graph automata (FSSGA) -- which captures the qualitative properties common to fault-tolerant distributed algorithms. Roughly speaking, the computation evolves homogeneously in the entire network, with each node acting symmetrically and with limited resources. As a building block, we demonstrate the equivalence of two automaton models for computing symmetric multi-input functions. We give FSSGA algorithms for several well-known problems.	automaton;computation;distributed algorithm;distributed computing;fault tolerance;symmetric graph;turing completeness	David Pritchard;Santosh Vempala	2006		10.1145/1148109.1148155	distributed algorithm;mathematical optimization;fault tolerance;combinatorics;discrete mathematics;parallel computing;computer science;theoretical computer science;mathematics;distributed computing;algorithm	Theory	16.732486944455825	35.02760412962613	118312
5a24320100ac68938f11d6d5f7e96e4454a9c445	a novel configurable motion estimation architecture for high-efficiency mpeg-4/h.264 encoding	high-efficiency mpeg-4;points search pattern engine;novel configurable motion estimation;motion estimation engine npspe;h.264 encoding;configurable motion estimation architecture;well-known low power fs;computing efficiency;latest efficient block-based motion;motion estimation;logic design;hardware description languages	This paper proposes a flexible, efficient and configurable motion estimation architecture. The core of this architecture is a motion estimation engine NPSPE (Nine Points Search Pattern Engine), which can support the latest efficient block-based motion estimation algorithms used by MPEG-4/H.264 encoding, such as PMVFAST and EPZS. This architecture has been designed and synthesized in SMIC 0.18um technology. The result shows it consumes only 17.5K gates, but its computing efficiency is about 15 times higher than the well-known low power FS engine including 16 PEs while its PSNR is similar to FS.	algorithm;computation (action);h.264/mpeg-4 avc;motion estimation;peak signal-to-noise ratio;whole earth 'lectronic link	Tiejun Li;Sikun Li;Cheng-Dong Shen	2005		10.1109/ASPDAC.2005.1466573	embedded system;electronic engineering;logic synthesis;real-time computing;quarter-pixel motion;computer science;theoretical computer science;motion estimation;hardware description language	Vision	12.453669028500665	40.71555342239282	118402
764a21213af550ee73c1f3f238742925e56780b0	improvement of line coding overhead targeting both run-length and dc-balance	reliability;standards;clocks;data communication;receivers;encoding;throughput	High-speed serial data communication is now very popular for connecting various resources in high-performance computing systems. In such high-speed serial links, a line coding is important to control the run length (RL) and the running disparity (RD), because a large run length causes insufficient transitions on data-links that make it difficult to perform reliable clock and data recovery (CDR), and a bounded running disparity is needed for maintaining the DC-balance of data links. These requirements for a line coding, however, cause additional bits to be transmitted, which decreases the throughput of data transmission. This paper presents a new approach to reducing the overhead of line codings, guaranteeing the bounded run length and running disparity. Our experimental results show that the proposed technique reduces the overhead by up to 98% compared to conventional line codings and by up to 48% compared to a latest low-overhead line coding.	8b/10b encoding;binocular disparity;clock recovery;correctness (computer science);data recovery;encoder;field-programmable gate array;line code;matlab;overhead (computing);requirement;ruby document format;run-length encoding;serial communication;series and parallel circuits;simulation;supercomputer;throughput	Sarat Yoowattana;Tomohiro Yoneda	2016	2016 IEEE 10th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSOC)	10.1109/MCSoC.2016.45	embedded system;real-time computing;telecommunications;engineering	Arch	13.207000108021697	39.60447804090629	118456
d42bfc10d48c7abf58a25325f3741119ceb6a534	formal systems of numerals	computers;compounds;equations compounds encoding computers;encoding	A new system of numerals is introduced for representing numbers in base 2N for N≤8. The new notation greatly simplifies arithmetical operations on numbers. For examples for, N=3(4) one obtains a notation for octal (hexadecimal) numbers in which one can perform addition and multiplication much more easily than in the standard notation. For N=8 one obtains a practical way of representing numbers to the base 256. A simplification of the decimal notation is also presented.	hexadecimal;level of detail;octal	Armen Gabrielian	1975	1975 IEEE 3rd Symposium on Computer Arithmetic (ARITH)	10.1109/ARITH.1975.6156984	arithmetic;hexadecimal;engineering notation;computer science;theoretical computer science;scientific notation;positional notation;steinhaus–moser notation;infix notation;normalized number;algorithm;encoding;pentadecimal;numeral system	Theory	16.382438313195433	43.33555350479787	118716
585d39343b1cc3a0129b381a47d9b1ad6eac6052	care bit density and test cube clusters: multi-level compression opportunities	data compression;automatic test pattern generation;boundary scan testing;statistical analysis;vector data;encoding;multilevel test stimulus data compression care bit densities scan test vectors data volume don t care bit information test cube clusters weighted random pattern methods;decoding encoding data compression test data compression automatic test pattern generation system testing logic testing circuit testing vectors broadcasting;boundary scan testing data compression automatic test pattern generation statistical analysis encoding	Most of the recently discussed and commercially introduced test stimulus data compression techniques are based on low care bit densities found in typical scan test vectors. Data volume and test times are reduced primarily by compressing the don’t-care bit information. The original care bit density, hence, dominates the theoretical compression limits. Further compression can be achieved by focusing on opportunities to compress care bit information in addition to the don’t-care bit information. This paper discusses at a conceptual level how data compression based on test cube clustering effects, as used in Weighted Random Pattern methods, could be combined with care bit oriented methods to achieve multi-level test stimulus compression. Introduction and Background Test data compression using simple on-chip decoding hardware has become a very hot topic in research as well as in commercial scan test generation tools. The recently introduced commercial scan test data compression tools take advantage of the fact that typical scan test vectors resulting from Automatic Test Pattern Generation (ATPG) contain relatively few care bits which must be at specific logic values for target fault detection. The vast majority of bit values in the test vectors are don’t-care bit values that are generated by more or less arbitrary fill algorithms. How these two scan test properties the relatively low density of care bits and the freedom to chose don’t care bit values can be utilized for test data compression was first described in [1]. The so called LFSR-Coding algorithm introduced in that paper shows how an LFSR (Linear Feedback Shift Register) seed can be calculated, such that cycling the LFSR with that seed will produce the correct care bit values while filling the don’t-care bits with pseudo-random values as a byproduct. Only the care bit values contribute to the seed calculation. It was shown that under certain statistical assumptions, a size of the seed vector needed to represent the care bit values essentially is somewhat larger than the number of care bits. In other words, the original care bit density more or less determines the achievable compression ratio. For example, if the original care bit density in a test vector is 2.5%, then a close to 40x stimulus data compression ratio can theoretically be achieved by using LFSR seeds to represent the care bits. Subsequent technical papers by different researchers have introduced various extensions and improvements to the original LFSR-Coding method and have shown empirically that the theoretical compression limit can indeed be approached [e.g., 2, 3, 4]. The seed calculation used in LFSR-Coding takes advantage of the fact that the stimulus values generated by cycling the LFSR are linear Boolean sums (XOR sums) of the seed values. The care bit values in a test vector, thus, create a set of linear equations that is solved to determine the appropriate seed vector for the test. We therefore refer to the encoding algorithm as linear Boolean encoding. The linear Boolean encoding concept can be generalized to work in conjunction with linear decoding circuits other than LFSRs. In fact, any combinational or sequential linear network could be used. The simplest form of a linear decoding network is to fan out from each scan input to multiple scan chains. This approach is known as broadcast scan, and has been shown to produce very good compression results also [5, 6, 7]. The on-chip decoding methods can be complemented by equally simple and effective tester-resident methods. One concept that is used in practice is to algorithmically generate the fill data for the don’t-care bits on the tester. The RLE (Run Length Encoding) approach described in [7, 8] utilizes the repeat features of certain testers for this purpose. The ATPG fill algorithm is changed to repeat the last care bit value rather than using the more common pseudo-random fill. This simple change creates runs of repeating scan-in vectors that can be represented by a single broadside vector and a repeat op-code rather than loading the vectors directly into buffer memory. It has been shown [7] that the tester-resident RLE method can be combined with moderate on-chip broadcast scan (e.g., 10x or 20x fan-out) for very dramatic data volume reductions (100x). All techniques described so far focus on the compressing the don’t-care bit values. However, there exists a long practical history of test data compression Proceedings of the 21st International Conference on Computer Design (ICCD’03) 1063-6404/03 $ 17.00 © 2003 IEEE using a fundamentally different approach. Weighted Random Pattern (WRP) test exploits the fact that test cubes (the care bits in a test vector) for multiple fault tests tend to form clusters with small Hamming Distance between the respective cubes in each cluster [9]. That is, the cubes in a cluster differ from each other in very few bit positions. For example, the stuck-at fault test cubes for multi-input AND gate differ from each other in at most 2 bit positions irrespective of the width of the AND gate. Each stuck-at fault test for a 20-input AND gate, for instance, has 20 care bits. There are 20+1=21 different stuck-at fault tests associated with the 20-input AND gate. The total number of care bits for all tests, hence, is 420 bits. From an information content point of view, recognizing the clustering effect, each cube could be represented as the XOR sum of one common base cube for each cluster and a difference vector for each additional unique test cube within the respective cluster. The difference vectors for the AND gate example will have at most 2 non-zero positions, meaning that the difference vectors are very amenable to data compression using sparse vector techniques. In other words, instead of storing 21 full test cubes for the AND gate, the tests would be represented by some form of data for a single base cube and by highly compressed data for the 21 difference vectors. In the WRP approach, the base cube information is implicitly expressed by so called weight value sets [9, 10] that more or less strongly bias the output values from a Pseudo Random Pattern Generator (PRPG) towards the base cube values. Each test vector bit position is associated with a 4-bit weight value, meaning that representing the base vector in this way consumes 4 test vectors’ worth of data volume. The care bit values of the vectors produced by cycling the weighted PRPG structure will, due to the biasing, be within a close Hamming distance of the base cube. Groups of “trial” vectors with different PRPG seeds are fault-simulated to determine which seeds actually produce useful test cubes. The resulting effective seeds can be interpreted as being a compact data representation for difference vectors that complement the base cube information encoded in the weight sets. Storage-efficient hardware/software methods for jumping from one effective PRPG seed to the next one are described in [11]. For large circuits with many scan cells, the data volume for the seed information is negligible compared to the weight set information. Many years of experience with WRP suggest that the combined storage for the weight value sets and effective PRPG seeds tends to be 5x to 20x less than the storage required for a full stored pattern scan test set with equivalent fault coverage. WRP, often combined with PRPG seed jumping, has been and still is in production use for some of the industry’s most complex chips [12, 13]. The concepts discussed in the following show how methods utilizing low care bit density and methods taking advantage of test cube clustering could be used in concert to achieve higher compression ratios than can be achieved with either method by itself.	4-bit;and gate;algorithm;apple multiple scan 14 display;areal density (computer storage);biasing;bit numbering;bit-oriented protocol;boolean algebra;cluster analysis;combinational logic;data (computing);data compression ratio;decoding methods;exclusive or;fan-out;fault coverage;fault detection and isolation;hamming distance;linear equation;linear-feedback shift register;olap cube;opcode;pseudorandomness;random seed;run-length encoding;seeds (cellular automaton);self-information;sparse matrix;stuck-at fault;test data;test set;test vector;wireless routing protocol;workflow resource planning	Bernd Könemann	2003		10.1109/ICCD.2003.1240913	data compression;lossy compression;data compression ratio;electronic engineering;computer hardware;image compression;computer science;theoretical computer science;automatic test pattern generation;test compression;lossless compression;encoding;statistics	ML	15.060559643361978	45.57383113385587	118723
19e3147417ab62af31f43e621083e7c7774b8974	particle computation: device fan-out and binary memory	computers;clocks;radiation detectors;logic gates computers clocks robot kinematics radiation detectors wiring;logic gates;multi robot systems collision avoidance logic gates;wiring;data storage element particle computation device fan out binary memory computational universality nano scale robots micro scale robots complex environments obstacles or logic gates and logic gates not nor nand xor xnor logic gates arbitrary digital circuits unit sized robots digital circuits;robot kinematics	We present fundamental progress on the computational universality of swarms of micro- or nano-scale robots in complex environments, controlled not by individual navigation, but by a uniform global, external force. Consider a 2D grid world, in which all obstacles and robots are unit squares, and for each actuation, robots move maximally until they collide with an obstacle or another robot. In previous work, we demonstrated components of particle computation in this world, designing obstacle configurations that implement AND and OR logic gates: by using dual-rail logic, we designed NOT, NOR, NAND, XOR, XNOR logic gates. However, we were unable to design a FAN-OUT gate, which is necessary for simulating the full range of complex interactions that are present in arbitrary digital circuits. In this work we resolve this problem by proving unit-sized robots cannot generate a FAN-OUT gate. On the positive side, we resolve the missing component with the help of 2×1 robots, which can create fan-out gates that produce multiple copies of the inputs. Using these gates we are able to establish the full range of computational universality as presented by complex digital circuits. As an example we connect our logic elements to produce a 3-bit counter. We also demonstrate how to implement a data storage element.	ap computer science a;boolean algebra;computation;computer data storage;convergence insufficiency;counter (digital);digital electronics;exclusive or;fan-out;gnu nano;interaction;logic gate;logical connective;nand logic;prototype;robot;simulation;turing completeness;universal turing machine;universality probability;xnor gate	Hamed Mohtasham Shad;Rose Morris-Wright;Erik D. Demaine;Sándor P. Fekete;Aaron Becker	2015	2015 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2015.7139951	electronic engineering;nor logic;simulation;logic level;logic gate;logic family;three-input universal logic gate;programmable logic array;computer science;artificial intelligence;theoretical computer science;pass transistor logic;control theory;sequential logic;particle detector;digital electronics;robot kinematics	Robotics	18.698155622568922	42.33384084503612	119176
52929a2ca06e9d3cddd45048e8871eeb9f71e796	fast, silent self-stabilizing distance-k independent dominating set construction	distance k independent set;distributed computing;self stabilization;fault tolerance;distance k independent dominating set;distance k dominating set	We propose a fast, silent self-stabilizing protocol building a distance-k independent dominating set, named FID. The convergence of the protocol FID is established for any computation under the unfair distributed scheduler. The protocol FID reaches a terminal (also legitimate) configuration in at most 4n+k rounds, where n is the network size; it requires (k + 1)log(n + 1) bits per node. keywords: distributed computing, fault tolerance, self-stabilization, distance-k independent dominating set, distance-k dominating set, distance-k independent set	computation;distributed computing;dominating set;fault tolerance;independent set (graph theory);scheduling (computing);self-stabilization	Colette Johnen	2014	Inf. Process. Lett.	10.1016/j.ipl.2014.04.013	self-stabilization;fault tolerance;discrete mathematics;dominating set;computer science;mathematics;distributed computing;algorithm	Theory	17.487267330236197	34.30222960217501	119226
6008c9b29db21eb28e4e1f5686e7549880a93a13	conditional connectivity of star graph networks under embedding restriction	star graphs;interconnection networks;connectivity;distributed systems	The n-dimensional star graph Sn is one of the most attractive interconnection networks of large scale multiprocessor systems. The k-embedding-restricted (edge) connectivity (gk(Sn)) fk(Sn) of Sn is defined to be the cardinality of a minimum subset of (edges) nodes, if any, whose deletion disconnects Sn and each node of the remaining components lies in an undamaged k-dimensional substar Sk. In this paper, we investigate the k-embedding-restricted (edge) connectivity of Sn and determine the values of gk(Sn) and fk(Sn) for some k’s. 2012 Elsevier Inc. All rights reserved.	adversary (cryptography);bubble sort;interconnection;k-edge-connected graph;magma;multiprocessing;recursion;subnetwork	Yuxing Yang;Shiying Wang	2012	Inf. Sci.	10.1016/j.ins.2012.02.025	combinatorics;discrete mathematics;computer science;connectivity;theoretical computer science;mathematics	AI	23.663400763959125	34.736231089954	119401
e26f6ab9fae8ff85dcc468da9e34b295806079b0	associative controlling of monolithic parallel processor architectures	tratamiento paralelo;traitement signal;processor architecture;costs very large scale integration hardware parallel architectures signal processing silicon signal processing algorithms power generation economics economic forecasting source coding;traitement parallele;monolithic integrated circuits;video signal processing;telecommunication control;circuit vlsi;video processing;calculateur simd;vlsi circuit;calculateur mimd;senal video;parallel architectures;signal video;efficient implementation;simd computer;video equipment video signal processing vlsi monolithic integrated circuits parallel architectures telecommunication control digital signal processing chips;video equipment;circuit integre monolithique;signal processing;monolithic integrated circuit;cost efficiency;prisma video signal processor monolithic parallel processor architectures associative control vlsi semiconductor technology flexible parallel processors video processing applications parallel data paths control hardware cost performance data simd mimd spmd msimd;vlsi;video signal;digital signal processing chips;procesador;circuito vlsi;circuito integrado monolitico;processeur;procesamiento senal;parallel processing;processor;mimd computer	The VLSI implementation of monolithic parallel processor architectures is supported by the ongoing progress of semiconductor technology. Nevertheless, a cost efficient realization of flexible parallel processors, suitable for a broad range of video processing applications, requires efficient schemes for controlling of the provided parallel data paths. Therefore, a novel associative controlling scheme is presented. This approach enables an efficient implementation of monolithic parallel processors at reasonable hardware cost. The paper presents performance data of associative controlling based on a simplified model in comparison to alternative controlling schemes, like SIMD, MIMD, SPMD, and MSIMD. Furthermore, an overview of the architecture of the currently developed associatively controlled PRISMA video signal processor is given. >		Winfried Gehrke;Klaus Gaedke	1995	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.473558	embedded system;parallel processing;computer architecture;parallel computing;microarchitecture;computer science;signal processing;video processing;very-large-scale integration;cost efficiency	Arch	10.746749715367184	41.39485044296241	119464
e7da0a83b82341b0f656a26a9d7e619c84891889	design of image data compression ip core based on processor local bus	discrete wavelet transforms;field programmable gate array;field programmable gate array image data compression processor local bus system on a programmable chip fpga programmable hardware user intellectual property ip core discrete wavelet transform vhdl software modelsim software edk;image coding;discrete wavelet transform;data compression;intellectual property;hardware description languages;fpga;system buses;processor local bus;system on a programmable chip;computer architecture;software edk;image compression;system on chip;vhdl;ip core;image data compression;system on chip data compression discrete wavelet transforms field programmable gate arrays hardware description languages image coding microprocessor chips system buses;user intellectual property;software modelsim;field programmable gate arrays;programmable hardware;encoding;discrete wavelet transforms computer architecture field programmable gate arrays image coding data compression encoding;microprocessor chips	The system on a programmable chip (SoPC) based on FPGA has some special characteristics, such as flexibility, customization, programmable hardware and software. The design of user intellectual property (IP) core is an important task in the design of SoPC. This paper presents the design of image data compression IP core based on the standard of processor local bus (PLB). The IP core is based on CCSDS image data compression （IDC） standard which includes discrete wavelet transform module and bit plane encoder module. Furthermore, the interface between PLB and IDC IP core is also designed. The design program of each module is using the language of VHDL, simulated by the software modelsim, and realized by the software EDK. The IP core designed in the SoPC system greatly speeds the encoding and makes the design easier and stronger in expansibility.	bit plane;data center;data compression;discrete wavelet transform;encoder;field-programmable gate array;intel core (microarchitecture);local bus;logic synthesis;microprocessor development board;semiconductor intellectual property core;system on a chip;vhdl	Xiaodong Xu;Yiqi Zhou	2010	2010 2nd International Workshop on Database Technology and Applications	10.1109/DBTA.2010.5658987	embedded system;computer hardware;computer science;operating system;field-programmable gate array	EDA	10.98313600184541	41.08276590155228	119570
43850d155cf3a5f3ca742d0b78719cf5985aed57	an auto-adaptation method for dynamically reconfigurable system-on-chip	computers;reconfigurable system;image coding;system on a chip vehicle dynamics field programmable gate arrays frequency computer architecture bandwidth clocks power system management hardware equations;data compression;partial reconfiguration;logic design;dynamic reconfiguration;clocks;energy efficient;reconfigurable architectures;adaptive dynamics;fpga;transform coding;jpeg2000 application;auto adaptation method;system on a chip;chip;reconfigurable architecture;dynamic frequency scaling auto adaptation partial reconfiguration;system on chip;system on chip data compression field programmable gate arrays image coding logic design reconfigurable architectures;adaptive method;auto adaptation;fpga auto adaptation method system on chip reconfigurable architecture dynamic frequency scaling partial reconfiguration dynamic reconfiguration decompression algorithm jpeg2000 application;field programmable gate arrays;decompression algorithm;dynamic frequency scaling;parallel processing;hardware	in this paper, we introduce an auto-adaptation method for reconfigurable system-on-chip (SoCs) architectures. The approach is based on joint dynamic frequency scaling and the partial and dynamic reconfiguration technique. The method aims the enhancement of the global auto-adaptability of SoCs in terms of energy, efficiency and scalability. The auto-adaptation method is described and tested with the decompression algorithm (IDWT) of JPEG2000 application targeting virtex-4 FPGAs of Xilinx.	algorithm;data compression;dynamic frequency scaling;field-programmable gate array;image scaling;jpeg 2000;reconfigurability;reconfigurable computing;scalability;system on a chip	Xun Zhang;Hassan Rabah;Serge Weber	2008	2008 IEEE Computer Society Annual Symposium on VLSI	10.1109/ISVLSI.2008.79	embedded system;parallel computing;real-time computing;computer science	EDA	11.571710021021135	40.364774836483576	119608
2a0a5f53986b8af38c3f8d3b4370ee524d7e4ad2	scheme for reducing the storage requirements of fft twiddle factors on fpgas	fast fourier transform;digital communication;digital circuits;digital communications	A scheme for reducing the hardware resources to implement on LUT-based FPGA devices the twiddle factors required in Fast Fourier Transform (FFT) processors is presented. The proposed scheme reduces the number of embedded block RAM for large FFTs and the number of slices for FFT lengths higher than 128 points. Results are given for Xilinx devices, but they can be generalized for other advanced LUT-based devices like ALTERA Stratix.	fast fourier transform;field-programmable gate array;twiddle factor	T. Sansaloni;Asuncion Perez-Pascual;Vicente Torres-Carot;Javier Valls-Coquillat	2007	VLSI Signal Processing	10.1007/s11265-007-0055-8	fast fourier transform;parallel computing;computer science;electrical engineering;theoretical computer science;digital electronics	EDA	11.559669895883534	44.36086842033428	119663
43ae65ebb145d88d955ce37c104aa271cc283757	esop-inspired synthesis method for ternary permutative quantum circuits	sum of products;network synthesis;logic gates quantum computing computers algorithm design and analysis algebra multivalued logic quantum mechanics;multivalued logic circuits;search algorithm;tree searching multivalued logic circuits network synthesis quantum gates ternary logic;quantum gates;quantum logic;quantum circuit design esop quantum computing quantum logic ternary logic;quantum computer;quantum circuits;ternary logic;tree searching;quantum computing;esop;quantum circuit design;ternary map esop inspired synthesis method ternary permutative quantum circuits muthukrishnan stroud gates disjoint sum of products dsop general ternary gates gtg cad tool ternary non disjoint groups minimizer tngm probabilistic search algorithm	The goal of this paper is to improve synthesis of ternary-input, ternary-output quantum per mutative circuits realized with Muthukrishnan-Stroud (M-S) gates. In [1], quantum ternary circuits are designed based on DSOP (Disjoint Sum of Products) cascades of General Ternary Gates (GTGs). Here, we develop an approach to synthesize quantum ternary circuits that uses non-disjoint groups. Our CAD tool, TNGM (Ternary Non-disjoint Groups Minimizer), uses a probabilistic search algorithm to select the largest groups in an incompletely specified Ternary Map and produces more compact circuits.	barr and stroud;computer-aided design;esop;quantum;search algorithm	Sidharth Dhawan;Marek A. Perkowski	2012	2012 IEEE 42nd International Symposium on Multiple-Valued Logic	10.1109/ISMVL.2012.25	discrete mathematics;theoretical computer science;mathematics;quantum computer;algorithm;quantum mechanics	EDA	18.574646935048676	45.55088135470633	119857
6f6252803112b9fc6bec849c75704c2a8452ef0b	fpga realization of rns to binary signed conversion architecture	digital signal processing;electronic mail;field programmable gate arrays residue number systems;residue number systems;multimedia application;multimedia systems;field programmable gate arrays arithmetic digital signal processing cathode ray tubes dynamic range electronic mail modems telecommunications multimedia systems energy consumption;n moduli set;energy consumption;fpga architecture;rns to binary signed conversion algorithm;dynamic range;arithmetic;digital arithmetic;residue number system;modems;power consumption;field programmable gate arrays;cathode ray tubes;digital arithmetic fpga architecture rns to binary signed conversion algorithm residue number system n moduli set;telecommunications	The use of the Residue Number System (RNS) in modern telecommunication and multimedia applications is becoming more and more important because it allows interesting advantages in terms of precision, power consumption and speed. Generally, the output conversion from residue to binary is the crucial point in effective realizations of application specific architectures based on residual arithmetic. This paper presents a general conversion procedure based on a N moduli set. The algorithm can process both unsigned and signed numbers. Based on this algorithm an architecture which efficiently implements the output conversion is illustrated. The architecture has been mapped on a FPGA.	algorithm;church encoding;field-programmable gate array;residue number system	Marco Re;Alberto Nannarelli;Gian-Carlo Cardarilli;Roberto Lojacono	2001		10.1109/ISCAS.2001.922245	cathode ray tube;embedded system;computer vision;dynamic range;electronic engineering;residue number system;computer science;electrical engineering;theoretical computer science;digital signal processing;mathematics;field-programmable gate array	PL	12.397140751772488	43.446807353920406	120688
cf5aad430e8b8e37022a81cb2d60de4f4dd3b77a	a novel analog-to-residue converter for biomedical dsp application	qi huang;residue number systems analogue digital conversion biomedical electronics digital signal processing chips;intermediate method analog to residue converter biomedical dsp application digital signal processors residue number system rns parallel signal processing power consumption signal processing efficiency silicon overhead area digital to residue conversion scheme analog to digital conversion scheme direct analog to residue conversion process scaling;residue number systems;isocc 2012 conference;대한전자공학회;drntu engineering electrical and electronic engineering;conference paper;yuanjin zheng;analogue digital conversion;biomedical electronics;the institute of electronics engineers of korea;digital signal processing chips;digital signal processing clocks accuracy calibration power demand delay;liter siek;a novel analog to residue converter for biomedical dsp application;zhao chuan lee;di zhu	Digital Signal Processors (DSP) based on Residue Number System (RNS) are superior to conventional DSP especially in terms of parallel signal processing and power consumption. For modern state of the art biomedical applications, signal processing efficiency and robustness are of paramount importance. Therefore, the huge silicon overhead area and high power consumption of traditional Analog-to- Digital and Digital-to-Residue conversion scheme are not feasible to be used in the field of biomedical science. Therefore, direct Analog-to-Residue conversion begins to attract immense interest from many researchers and engineers in the world today. In this paper, a new method to realize the direct Analog-to-Residue conversion is presented. This method shows a promising characteristic for low power and process scaling via utilizing time as the intermediate method in the processing.	analog-to-digital converter;digital signal processor;image scaling;overhead (computing);residue number system;signal processing	Di Zhu;Qi Huang;Zhao Chuan Lee;Yuanjin Zheng;Liter Siek	2012	2012 International SoC Design Conference (ISOCC)	10.1109/ISOCC.2012.6407118	embedded system;electronic engineering;engineering;electrical engineering	EDA	14.112680657561889	42.28952853673444	120848
e1331e8d970721d197dcf15469bf1d89dd40e07f	improved one-to-all broadcasting algorithms on faulty simd hypercubes	tolerancia falta;algoritmo paralelo;base dato multidimensional;hypercube;parallel algorithm;light occupied dimension;fault tolerant;distributed computing;radiodifusion;calculateur simd;multidimensional database;algorithme parallele;simd computer;one to all broadcasting;fault tolerance;calculo repartido;base donnee multidimensionnelle;broadcasting;calcul reparti;simd hypercube;radiodiffusion;tolerance faute;hipercubo	Consider an n-dimensional SIMD hypercube Hn with 3n/2 − 1 faulty nodes. With n + 3 log(n − 1) + 7, n + 2 log(n − 1) + 9, n + log(n − 1) + O(log log(n − 1)), n + log(n − 1) + 12, and n + 19 steps, this paper presents some one-to-all broadcasting algorithms on the faulty SIMD Hn. The sequence of dimensions used for broadcasting in each algorithm is the same regardless of which node is the source. The proposed one-to-all broadcasting algorithms can tolerate n/2 more faulty nodes than Raghavendra and Sridhar’s algorithms (J. Parallel Distrb. Comput. 35 (1996) 57) although 8 extra steps are needed. The fault-tolerance improvement of this paper is about 50%. © 2004 Published by Elsevier Inc.	algorithm	Yu-Wei Chen	2005	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2004.05.004	fault tolerance;parallel computing;computer science;theoretical computer science;distributed computing	Theory	13.173611143217856	34.53634867410049	120910
96cd29a7015919981a2884e2ccdcc7a9b89b35be	a configurable ip core for inverse quantized discrete cosine and integer transforms with arbitrary accuracy	quantization;psnr;data compression;cordic;video compression;low complexity;configurable ip core;fpga;transform coding;bit rate;quantized h 264 inverse integer transforms;video coding;add operation;computer architecture;image video transformations;configurable architecture;computational complexity;discrete cosine transforms;computational complexity configurable ip core inverse quantized discrete cosine integer transforms image video transformations quantized h 264 inverse integer transforms configurable architecture add operation shift operation cordic iterations video compression quality;h 264;integer transforms;mpeg 4;ict;inverse quantized discrete cosine;ip networks;video coding data compression discrete cosine transforms inverse transforms;discrete cosine transforms computer architecture transform coding psnr bit rate ip networks;inverse transforms;cordic iterations;h 264 dct ict quantization cordic reconfigurable fpga mpeg 4;dct;video compression quality;reconfigurable;shift operation	An extended version of low-complexity IP Core for image/video transformations based on the CORDIC architecture is presented. This IP core is able to perform quantized 8×8 IDCT and quantized 8×8/4×4 H.264-inverse integer transforms on a configurable architecture by using only shift and add operations. Furthermore, the number for CORDIC iterations and compensation steps can be adjusted, which enables a trade-off between video compression quality in PSNR and computational complexity.	cordic;computational complexity theory;data compression;discrete cosine transform;iteration;peak signal-to-noise ratio;quantization (signal processing);semiconductor intellectual property core	Chi-Chia Sun;Ce Zhang;Jürgen Götze	2010	2010 IEEE Asia Pacific Conference on Circuits and Systems	10.1109/APCCAS.2010.5774865	data compression;electronic engineering;discrete mathematics;computer science;theoretical computer science;mathematics;algorithm	EDA	12.13856228267675	41.40120546071906	120917
21dfa2c530c45b015660d0823be8fe5cf4db707c	brief announcement: deterministic graph connectivity in the broadcast congested clique	deterministic protocol;graph connectivity;spanning forest;broadcast congested clique	We present deterministic constant-round protocols for the graph connectivity problem in the model where each of the <i>n</i> nodes of a graph receives a row of the adjacency matrix, and broadcasts a single sublinear size message to all other nodes. Communication rounds are synchronous. This model is sometimes called the broadcast congested clique. Specifically, we exhibit a deterministic protocol that computes the connected components of the input graph in [1/ε] rounds, each player communicating <i>O</i>(<i>n</i><sup>ε</sup> ⋅ log <i>n</i>) bits per round, with <i>0</i> < ε ≤ 1.  We also provide a deterministic one-round protocol for connectivity, in the model when each node receives as input the graph induced by the nodes at distance at most <i>r</i>><i>0</i>, and communicates <i>O</i>(<i>n</i><sup>1/<i>r</i></sup> ⋅ log <i>n</i>) bits. This result is based on a <i>d</i>-pruning protocol, which consists in successively removing nodes of degree at most $d$ until obtaining a graph with minimum degree larger than <i>d</i>. Our technical novelty is the introduction of deterministic sparse linear sketches: a linear compression function that permits to recover sparse Boolean vectors deterministically.	adjacency matrix;clique (graph theory);connected component (graph theory);connectivity (graph theory);one-way compression function;sparse matrix	Pedro Montealegre-Barba;Ioan Todinca	2016		10.1145/2933057.2933066	algebraic connectivity;block graph;graph power;split graph;combinatorics;clique graph;discrete mathematics;graph bandwidth;null graph;degree;computer science;regular graph;distance-regular graph;connectivity;theoretical computer science;simplex graph;mathematics;voltage graph;distributed computing;windmill graph;butterfly graph;random geometric graph;complement graph;line graph;algorithm;strength of a graph;ordered graph	Theory	18.86110665252548	33.50662767566165	121140
65f838b103c02733eafaca5f0b262af41358d304	simplifying quotient determination in high-radix modular multiplication	cycle time;shift and add operation quotient determination high radix modular multiplication rewriting montgomery s algorithm;critical path;hardware clocks computer science public key councils throughput frequency conversion upper bound;digital arithmetic;modular multiplication	Until now the use of high radices to implement modular multiplication has been questioned, because it involves complex determination of quotient digits for the modulo reduction. This paper presents algorithms that are obtained through rewriting of Montgomery’s algorithm. The determination of quotients becomes trivial and the cycle time becomes independent of the choice of radix. It is discussed how the critical path in the loop can be reduced to a single shift-and-add operation. This implies that a true speed up is achieved by choosing higher radices.	bitap algorithm;critical path method;modulo operation;montgomery modular multiplication;rewriting;speedup	Holger Orup	1995		10.1109/ARITH.1995.465359	arithmetic;modular arithmetic;kochanski multiplication;cycle time variation;critical path method;mathematics;algorithm;algebra	PL	11.376231295450504	43.83925867384263	121148
bc2eec5088d435e829250dac6b9f60fce2a0187a	ring embedding in faulty honeycomb rectangular torus	distributed application;hamiltonian cycle;graphe biparti;grafo bipartido;cycle hamiltonien;ring embedding;red nido abeja;anneau;interconnection network;reseau nid abeille;ciclo hamiltoniano;toro;torus;tore;interconnection networks;grafo regular;ring;graphe regulier;bipartite graph;article;red interconexion;anillo;honeycomb torus;regular graph;reseau interconnexion;honeycomb network	Assume thatm andn are positive even integers with n 4. The honeycomb rectangular torus HReT (m,n) is recognized as another attractive alternative to existing torus interconnection networks in parallel and distributed applications. It is known that any HReT (m,n) is a 3-regular bipartite graph. We prove that any HReT (m,n) − e is hamiltonian for any edge e ∈ E(HReT(m,n)). Moreover, any HReT (m,n) − F is hamiltonian for anyF = {a, b} with a ∈ A andb ∈ B whereA andB are the bipartition of HReT (m,n), if n 6 orm = 2.  2002 Elsevier Science B.V. All rights reserved.	apollonian network;distributed computing;hamiltonian (quantum mechanics);interconnection	Hsun-Jung Cho;Li-Yen Hsu	2002	Inf. Process. Lett.	10.1016/S0020-0190(02)00310-1	hamiltonian path;combinatorics;topology;bipartite graph;regular graph;torus;mathematics;geometry;ring	Theory	24.133206421507296	34.8287066784929	121287
39bea202e22c38e1374ca63dbe7ed19226f600db	strategies for parallel unaware cleaners	visit time;competitive analysis;mobile agent;multi robot graph exploration;robot	We investigate the parallel traversal of a graph with multiple robots unaware of each other. All robots traverse the graph in parallel forever and the goal is to minimize the time needed until the last node is visited (first visit time) and the time between revisits of a node (revisit time). We also want to minimize the visit time, i.e. the maximum of the first visit time and the time between revisits of a node. We present randomized algorithms for uncoordinated robots, which can compete with the optimal coordinated traversal by a small factor, the so-called competitive ratio. For ring and path graph simple traversal strategies allow constant competitive factors even in the worst case. For grid and torus graphs with n nodes there is a O(logn)-competitive algorithm for both visit problems succeeding with high probability, i.e. with probability 1 − n−O(1). For general graphs we present an O(log n)-competitive algorithm for the first visit problem, while for the visit problem we show an O(log n)-competitive algorithm both succeeding with high probability.	best, worst and average case;competitive analysis (online algorithm);dijkstra's algorithm;randomized algorithm;robot;traverse;tree traversal;with high probability	Christian Ortolf;Christian Schindelhauer	2015	Theor. Comput. Sci.	10.1016/j.tcs.2015.09.026	robot;competitive analysis;real-time computing;simulation;computer science;graph traversal;mobile agent;distributed computing	Theory	17.941489132040147	34.10607869402133	121377
1e0a427e6167c13de36675da4884a21c26a29ca1	design of a high-speed reed-solomon decoder	libraries;3 3 v;3 3 v high speed reed solomon decoder rs decoder high speed data communication data reliability decoder architecture symbol errors error correction modified euclid algorithm chien search algorithm forney algorithm systolic arrays parallel processing architecture euclid block latency clock cycles latency reduction decoder operating frequency data transfer rate decoder supply voltage 80 mhz 640 mbit s 0 5 micron;decoder operating frequency;decoding;modified euclid algorithm;clocks;systolic arrays;reed solomon codes;systolic array;대한전자공학회;0 5 micron;data communication;decoder architecture;forney algorithm;kyung ho kim;integrated circuit design;jin yong kang;circuit simulation;data transfer rate;integrated circuit modelling;error correction;vol 1;reed solomon codes decoding delay clocks data communication error correction systolic arrays parallel processing libraries voltage;voltage;decoder supply voltage;data reliability;parallel processing architecture;reed solomon;640 mbit s;high speed data communication;the institute of electronics engineers of korea;digital signal processing chips;circuit cad;soc design conference 2001;high speed reed solomon decoder;80 mhz;integrated circuit reliability;myung hoon sunwoo;logic cad;digital signal processing chips integrated circuit design integrated circuit modelling integrated circuit reliability logic cad reed solomon codes error correction systolic arrays high speed integrated circuits circuit cad decoding circuit simulation;design of a high speed reed solomon decoder;latency reduction;high speed;clock cycles;euclid block latency;high speed integrated circuits;parallel processing;data transfer;rs decoder;symbol errors;chien search algorithm	This paper proposes a Reed-Solomon (RS) decoder for applications that require high-speed data communication and reliability. The proposed architecture can support variable n and k values (37 < n I 2 5 5 , 21 < k 5239). The RS decoder corrects up to eight symbol errors, i.e., t = 8. It employs a modified Euclid’s algorithm, the Chien search and Fomey’s algorithms using the systolic array and parallel processing architectures. The proposed Euclid block requires the latency of 2t + 1 clock cycles. This architecture reduces latency by about 72% compared with an existing architecture which requires 3t + 37 clock cycles when t = 8. The decoder operates at 80MHz and its data transfer rate is 640Mbps. The proposed decoder has been modeled using the SAMSUNG 0.5pm SOG cell library (KG80) with the supply voltage of 3.3V.	algorithm;chien search;clock signal;euclid;systolic array	Jae Hyun Baek;J. Y. Kang;Myung Hoon Sunwoo	2002		10.1109/ISCAS.2002.1010823	embedded system;parallel processing;electronic engineering;parallel computing;real-time computing;voltage;error detection and correction;soft-decision decoder;systolic array;telecommunications;computer science;reed–solomon error correction;forney algorithm;integrated circuit design	Arch	13.595863853692613	44.73701129555887	121857
c61a9342b822bf8b3d49c38da42635187f2a91a0	a time and storage optimized hardware design for context-based adaptive binary arithmetic decoding in h.264/avc	context based adaptive binary arithmetic code decoding;image storage;video streaming;adaptive decoding;decoding;h 264 avc;binary codes;design optimization hardware arithmetic decoding automatic voltage control context modeling computer architecture image storage read only memory video coding;code standards;syntax elements;design optimization;hardware architecture;video coding;computer architecture;arithmetic codes;automatic voltage control;memory architecture;bin decoding efficiency;cif video stream storage optimized hardware design context based adaptive binary arithmetic decoding h 264 avc hardware architecture context based adaptive binary arithmetic code decoding bin decoding efficiency storage efficiency syntax elements parallel working mode;context based adaptive binary arithmetic decoding;cif video stream;arithmetic;storage efficiency;digital signal processing chips;storage optimized hardware design;context modeling;video streaming adaptive decoding arithmetic codes binary codes code standards decoding digital signal processing chips memory architecture video coding;read only memory;parallel working mode;hardware	This paper proposes a hardware architecture for Context-based Adaptive Binary Arithmetic Code (CABAC) decoding in H.264/AVC. The proposed architecture takes both bin decoding efficiency and control efficiency into account. This architecture improves time and storage efficiency by taking full use of the new found characters of syntax elements (SEs). In this architecture, two controllers are designed to decode SEs. One is the main controller, and the other is the sub controller, which controls the decoding of residual block SEs. The parallel working mode of the two controller improves the time-consuming performance of the system. Experimental result shows that our design is quite rich for main profile CIF video stream at 30fps.	binary number;context-adaptive binary arithmetic coding;h.264/mpeg-4 avc;server message block;storage efficiency;streaming media	Yan Zheng;Shibao Zheng;Zhonghua Huang;Ziliang Zhao	2007	2007 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2007.4284963	storage efficiency;binary code;parallel computing;real-time computing;multidisciplinary design optimization;computer science;theoretical computer science;context-adaptive variable-length coding;hardware architecture;context model;read-only memory	Robotics	11.963002602638445	39.783700773250374	122005
c7178c8afd26aed3939b09a378d7c16d8a2d7856	a macro placer algorithm for chip design		There is a set of rectangular macros with given dimensions, and there are wires connecting some pairs (or sets) of them. We have a placement area where these macros should be placed without overlaps in order to minimize the total length of wires. We present a heuristic algorithm which utilizes a special data structure for representing two dimensional stepfunctions. This results in fast integral computation and function modification over rectangles. Our heuristics, especially our data structure for two-dimensional functions, may be useful in other applications, as well.	algorithm;computation;data structure;heuristic (computer science);hungarian algorithm;loss function;np-hardness;very-large-scale integration	Endre Csóka;Attila Deák	2015	CoRR		mathematical optimization;combinatorics;parallel computing;computer science;theoretical computer science	Theory	19.089407000473983	39.578680418683554	122408
616df6e7c0cde26c0eb6eb36e5705742cb326b73	table-based polynomials for fast hardware function evaluation	second order;digital signal processing;field programmable gate array;fast hardware function evaluation;cost function;multipartite method;linear approximation;polynomials hardware table lookup delay field programmable gate arrays digital signal processing scientific computing arithmetic cost function linear approximation;polynomials;higher order;fpga implementation;first order;field programmable gate array table based polynomials fast hardware function evaluation bipartite method multipartite method first order function approximation single multiplier second order method adders;function approximation;bipartite method;adders;elementary functions;table based polynomials;scientific computing;arithmetic;digital arithmetic;first order function approximation;field programmable gate arrays;table lookup;table lookup field programmable gate arrays digital arithmetic adders function approximation;single multiplier second order method;hardware	Many general table-based methods for the evaluation in hardware of elementary functions have been published. The bipartite and multipartite methods implement a first-order approximation of the function using only table lookups and additions. Recently, a single multiplier second order method of similar inspiration has also been published. This paper extends such methods to approximations of arbitrary order, using adders, small multipliers, and very small ad hoc powering units. We obtain implementations that are both smaller and faster than previously published approaches. This paper also deals with the FPGA implementation of such methods. Previous work have consistently shown that increasing the approximation degree lead to not only smaller but also faster designs, as the reduction of the table size meant a reduction of its lookup time, which compensated for the addition and multiplication time. The experiments in this paper suggest that this still holds when going from order 2 to order 3, but no longer when using higher order approximations, where a tradeoff appears.	24-bit;32-bit;adder (electronics);application-specific integrated circuit;carry-lookahead adder;circuit minimization for boolean functions;computer-aided design;elementary function;error analysis (mathematics);experiment;field-programmable gate array;first-order predicate;hoc (programming language);horner's method;lookup table;order of approximation;polynomial;read-only memory;server (computing);vhdl	Jérémie Detrey;Florent de Dinechin	2005	2005 IEEE International Conference on Application-Specific Systems, Architecture Processors (ASAP'05)	10.1109/ASAP.2005.61	embedded system;parallel computing;computer science;theoretical computer science;algorithm;field-programmable gate array	Robotics	13.915253271204605	44.18640705990383	123081
5662841c88789a662e1393956d934c20b2140341	an efficient algorithm for topology discovery of a blackbox communication network	topology;analisis numerico;reseau communication;matematicas aplicadas;mathematiques appliquees;topology discovery;efficient algorithm;efficiency;analyse numerique;algorithme;resolucion problema;network topology;algorithm;eficacia;numerical analysis;internet;community networks;efficacite;blackbox;applied mathematics;communication;red de comunicacion;topologie circuit;comunicacion;communication network;problem solving;resolution probleme;algoritmo	A blackbox network is a network with an unknown topology that can be used for communications. For example, we can use the Internet for communications but do not have complete information about its topology. In this paper, we model topology discovery of a blackbox network as a graph-theoretic problem and present an efficient algorithm for solving this problem. We give a formal proof of the correctness and analyze the efficiency.	algorithm;telecommunications network	Sheng Zhong	2007	Applied Mathematics and Computation	10.1016/j.amc.2006.07.122	black box;the internet;telecommunications;numerical analysis;artificial intelligence;mathematics;efficiency;network topology;algorithm;telecommunications network;logical topology	Theory	21.849615532898667	33.1323170882555	123198
776ea5e1309f096e785a76702802d458c13a7426	mobile geometric graphs, and detection and communication problems in mobile wireless networks	random geometric graph;giant component;geometric graph;two dimensions;wireless network;discrete mathematics;natural extension;mobile wireless network;mobile network	Static wireless networks are by now quite well understood mathematically through the random geometric graph model. By contrast, there are relatively few rigorous results on the practically important case of mobile networks, in which the nodes move over time; moreover, these results often make unrealistic assumptions about node mobility such as the ability to make very large jumps. In this paper we consider a realistic model for mobile wireless networks which we call mobile geometric graphs, and which is a natural extension of the random geometric graph model. We study two fundamental questions in this model: detection (the time until a given “target” point—which may be either fixed or moving—is detected by the network), and percolation (the time until a given node is able to communicate with the giant component of the network). For detection, we show that the probability that the detection time exceeds t is exp(−Θ(t/ log t)) in two dimensions, and exp(−Θ(t)) in three or more dimensions, under reasonable assumptions about the motion of the target. For percolation, we show that the probability that the percolation time exceeds t is exp(−Ω(t d d+2 )) in all dimensions d ≥ 2. We also give a sample application of this result by showing that the time required to broadcast a message through a mobile network with n nodes above the threshold density for existence of a giant component is O(log n) with high probability. ∗Computer Science Division, University of California, Berkeley CA 94720-1776, U.S.A. Email: sinclair@cs.berkeley.edu. Supported in part by NSF grant CCF-0635153 and by a UC Berkeley Chancellor’s Professorship. †Computer Science Division, University of California, Berkeley CA 94720-1776, U.S.A. Email: stauffer@cs.berkeley.edu. Supported by a Fulbright/CAPES scholarship and NSF grants CCF-0635153 and DMS0528488. ar X iv :1 00 5. 11 17 v2 [ m at h. PR ] 7 J ul 2 01 0	computer science;email;geometric graph theory;giant component;ibm notes;mobile phone;percolation;random geometric graph;random graph;uc browser;with high probability	Alistair Sinclair;Alexandre Stauffer	2010	CoRR		cellular network;combinatorics;two-dimensional space;discrete mathematics;theoretical computer science;wireless network;mathematics;random geometric graph;giant component;statistics	Theory	20.140760490151585	35.998051466790756	123510
a2dfcff9a36ee5f787b5215aff5ff006c44e310b	rapid mixing of local graph dynamics		Graph dynamics arise naturally in many contexts. For instance in peer-to-peer networks, a participating peer may replace an existing connection with one neighbour by a new connection with a neighbour's neighbour. Several such local rewiring rules have been proposed to ensure that peer-to-peer networks achieve good connectivity properties (e.g. high expansion) in equilibrium. However it has remained an open question whether there existed such rules that also led to fast convergence to equilibrium. In this work we provide an affirmative answer: We exhibit a local rewiring rule that converges to equilibrium after each participating node has undergone only a number of rewirings that is poly-logarithmic in the system size. The proof involves consideration of the whole isoperimetric profile of the graph, and may be of independent interest.	isoperimetric inequality;peer-to-peer;periodic graph (graph theory)	Laurent Massoulié;Rémi Varloot	2017	CoRR		machine learning;distributed computing;algorithm	Theory	19.581036642847234	35.06936354620116	123511
01b32a886ecba647a9d6ae4861aed11ae8101647	a quasi self-stabilizing algorithm for detecting fundamental cycles in a graph with dfs spanning tree given	computational complexity;graph theory;scheduling;trees (mathematics);dfs spanning tree;depth-first search;distributed adversarial scheduler;fundamental cycle detection;linear time quasi self-stabilizing algorithm;time complexity;undirected connected graph modeling asynchronous distributed system;self-stabilization;fault tolerance;fundamental cycles	This paper presents a linear time quasi self-stabilizing algorithm for detecting the set of fundamental cycles on an undirected connected graph modeling asynchronous distributed system. Previous known algorithm has O(n2) time complexity, whereas we prove that our stabilizes after O(n) moves. Distributed adversarial scheduler is considered. Both algorithms assume that the depth-first search (DFS) spanning tree (DFST) of the graph is given. The output is given in a distributed manner as a state of variables in the nodes.	algorithm;connectivity (graph theory);depth-first search;distributed computing;file spanning;graph (discrete mathematics);graph theory;precondition;scheduling (computing);self-stabilization;sensor;spanning tree;time complexity	Halina Bielak;Michal Panczyk	2013	2013 Federated Conference on Computer Science and Information Systems	10.2478/v10065-012-0038-7	spqr tree;graph power;kruskal's algorithm;feedback arc set;minimum degree spanning tree;spanning tree;prim's algorithm;minimum spanning tree;graph factorization;connected dominating set;trémaux tree;graph;moral graph;reverse-delete algorithm;random geometric graph;distributed minimum spanning tree;cycle basis;tree;algorithm;shortest-path tree	Theory	18.779435477864048	33.315119224758874	123524
cb57a03fb4f355636e7b855b5a6906c420e5d14d	a highly parallel sub-pel accurate motion estimator for h.264	video encoding;full search;pipelined execution;cost efficient consumer;staggered approach;search method;motion estimation;hdtv h 264 motion estimation;motion search methods;chip;video coding;computation processing scheme;video coding high definition television motion estimation;motion estimation encoding hardware hdtv decoding bit rate video compression quantization tv computer architecture;h 264 motion estimation algorithm;data dependence;hdtv encoder chip;hdtv;cost efficiency;h 264;hdtv encoder chip h 264 motion estimation algorithm video encoding sequential data dependency staggered approach motion search methods parallel execution pipelined execution computation processing scheme cost efficient consumer high definition television;high performance;parallel execution;hardware implementation;high definition television;sequential data dependency	"""This paper presents a new H.264 motion estimation algorithm and architecture for video encoding. The algorithm resolves the sequential data dependencies of H.264 motion estimation by a new method, called """"staggered approach"""". It adapts existing motion search methods (3DRS, hierarchical full search), maximizing parallel execution and allowing efficient pipelined execution while maintaining the high compression rate of H.264. Its regular and reduced-computation processing scheme makes the algorithm particularly suitable for high-performance hardware implementations, as required for realtime encoding of HDTV. The proposed motion estimator architecture demonstrates the suitability of the algorithm for cost-efficient consumer HDTV encoder chips"""	algorithm;computation;cost efficiency;data compression;data dependency;digital video recorder;encoder;h.264/mpeg-4 avc;memory bandwidth;motion estimation;pin grid array;pixel;real-time clock;scalability;stepping level	Sebastian Flügel;Heiko Klußmann;Peter Pirsch;Marco Schulz;Malik Cisse;Winfried Gehrke	2006	2006 IEEE Workshop on Multimedia Signal Processing	10.1109/MMSP.2006.285336	chip;computer vision;parallel computing;real-time computing;telecommunications;quarter-pixel motion;computer science;theoretical computer science;motion estimation;cost efficiency	Robotics	12.129285554712306	40.00772004146485	123904
e669d89ad39e5e862041ba06dff697af746e4091	matrix multiplication i/o-complexity by path routing	communication avoiding algorithms;fast matrix multiplication;i o complexity	We apply a novel technique based on path routings to obtain optimal I/O-complexity lower bounds for all Strassen-like fast matrix multiplication algorithms computed in serial or in parallel, assuming no reuse of nontrivial intermediate linear combinations. Given fast memory of size M, we prove an I/O-complexity lower bound of Ω((n/√M}ω0 • M) for any Strassen-like matrix multiplication algorithm applied to n x n matrices of arithmetic complexity Θ(nω0) with ω0<3 under this assumption. This generalizes an approach by Ballard, Demmel, Holtz, and Schwartz that provides a tight lower bound for Strassen's matrix multiplication algorithm but which does not apply to algorithms with disconnected encoding or decoding components of the underlying computation graph or algorithms with multiply copied values. We overcome these challenges via a new graph-theoretical approach for proving I/O-complexity lower bounds without the use of edge expansions.	computation;goto;graph theory;input/output;mathematical optimization;matrix multiplication algorithm;recursion;routing;strassen algorithm	Jacob Scott;Olga Holtz;Oded Schwartz	2015		10.1145/2755573.2755594	combinatorics;discrete mathematics;multiplication algorithm;matrix multiplication;theoretical computer science;mathematics	Theory	21.14787120043039	35.16960971957885	124221
abddc50b0945eda436d04afde2d19c18fe0b9661	finding the detour-critical edge of a shortest path between two nodes	camino mas corto;graph theory;shortest path;fault tolerant;most critical edge;longest detour;plus court chemin;systeme informatique tolerant panne;theorie graphe;fault tolerant computer systems;fault tolerance;transient edge failures	Abstract   Let  P   G  ( r ,  s ) denote a shortest path between two nodes  r  and  s  in an undirected graph  G  with nonnegative edge weights. A  detour  at a node  u   ϵ   P   G  ( r ,  s ) = 〈 r ,…,  u ,  v ,…, s 〉 is defined as a shortest path  P   G  −  e  ( u ,  s ) from  u  to  s  which does not make use of ( u ,  v ). In this paper we focus on the problem of finding an edge  e  = ( u ,  v )  ϵ   P   G  ( r ,  s ) whose removal produces a detour at node  u  such that the length of  P   G  −  e  ( u ,  s ) minus the length of  P   G  ( u ,  s ) is maximum. We call such an edge a  detour-critical edge . We will show that this problem can be solved in O( m  +  n   log   n ) time, where  n  and  m  denote the number of nodes and edges in the graph, respectively.	shortest path problem	Enrico Nardelli;Guido Proietti;Peter Widmayer	1998	Inf. Process. Lett.	10.1016/S0020-0190(98)00077-5	fault tolerance;combinatorics;computer science;graph theory;mathematics;distributed computing;bound graph;algorithm;shortest-path tree	DB	23.454508403361217	33.93742617705858	124309
abe4015549b95f11269fdd1ab64bfd859d280a16	fast residue-to-binary conversion using base extension and the chinese remainder theorem	reverse conversion;residue number systems;base extension;chinese remainder theorem	Residue number systems are attractive due to their basic arithmetic operations, such as addition and multiplication, are carry-free; this facilitates low-power, high-speed implementations. But converting from residue to conventional notation is generally problematic. In this paper, we propose a high-speed architecture for reverse conversion; this is based on Base Extension and the Chinese Remainder Theorem.	polynomial remainder theorem	Amos R. Omondi	2007	Journal of Circuits, Systems, and Computers	10.1142/S021812660700371X	arithmetic;residue number system;discrete mathematics;chinese remainder theorem;mathematics;algebra	Logic	15.974729072300883	43.67937677830086	124336
85e0a67166f79975c1599cc2d59374f4ae6a5985	using dynamic reconfiguration to reduce the area of a jpeg decoder on fpga	jpeg decoder;random access memory;decoding;resource utilization jpeg decoder fpga partial dynamic reconfiguration field programmable gate array virtual hardware concept zynq 7020 fpga;fpga;image coding field programmable gate arrays;transform coding;jpeg decoder dynamic reconiguration fpga;streaming media;heuristic algorithms;field programmable gate arrays;decoding random access memory transform coding field programmable gate arrays heuristic algorithms hardware streaming media;dynamic reconiguration;hardware	Partial dynamic reconfiguration of FPGAs can be used to implement complex applications using the concept of virtual hardware. In this work we have used partial dynamic reconfiguration to implement a JPEG decoder with reduced area. The image decoding process was adapted to be implemented on the FPGA fabric using this technique. The architecture was tested in a low cost ZYNQ-7020 FPGA that supports dynamic reconfiguration. The results show that the proposed solution needs only 40% of the resources utilized by a static implementation. The performance of the dynamic solution is about 9X slower than the static solution by trading-off internal resources of the FPGA. A throughput of 7 images per second is achievable with the proposed partial dynamic reconfiguration solution.	color image;field-programmable gate array;jpeg;throughput;virtual machine	Tiago Rodrigues;Mário P. Véstias	2015	2015 Euromicro Conference on Digital System Design	10.1109/DSD.2015.31	embedded system;parallel computing;real-time computing;reconfigurable computing;computer science;fpga prototype;field-programmable gate array	EDA	11.17667508809053	40.41226120626414	125084
e8a55c0c204ce034ebd101e17c16011e87ce19a9	a cost-effective d-tv system chip set	digital video broadcasting;video frame memory;error correction codes;decoding;video signal processing;digital tv;reduced instruction set computing;digital television;television receivers;hdtv receiver cost effective d tv system chip set video decoder mp hl down decode technique required video frame memory;video decoder;chip;demodulation;decoding displays streaming media digital tv costs demodulation error correction error correction codes reduced instruction set computing hdtv;streaming media;error correction;displays;television receivers digital television decoding video signal processing high definition television digital signal processing chips;hdtv;video signal processing television receivers digital video broadcasting high definition television digital signal processing chips decoding;cost effectiveness;digital signal processing chips;down decode technique;mp hl;video frame memory cost effective d tv system chip set video decoder mp hl down decode technique;cost effective d tv system chip set;costs displays decoding demodulation filters streaming media tv error correction read write memory transforms;high definition television	This paper proposes a cost-effective D-TV system chip set. The video decoder is applicable up to MP@HL, and a new down-decode technique is adopted to reduce an amount of video frame memory to a quarter.	chipset;video decoder	Yoshikazu Mihara;Katsunori Hirase;Minoru Tanaka	1999	1999 Digest of Technical Papers. International Conference on Consumer Electronics (Cat. No.99CH36277)	10.1109/30.793633	embedded system;digital television;computer hardware;telecommunications;computer science	EDA	11.137637117993332	41.57495454507216	125490
1041614faaaaa287ed82abac9ab08731c3017674	a high-throughput low-latency arithmetic encoder design for hdtv	optimisation;binary codes;video coding;arithmetic codes;video coding arithmetic codes binary codes encoding field programmable gate arrays high definition television optimisation;size 0 13 mum high throughput low latency arithmetic encoder design hdtv ae design bac architecture binary arithmetic coder architecture high definition real time applications video coding standards h 264 avc avs macroblock level pipeline mb level pipeline joint algorithm architecture optimization multibin processing technique real time encoding hybrid context memory scheme xilinx virtex 6 fpga prototype board tsmc technology frequency 200 mhz;field programmable gate arrays;encoding;throughput context video coding encoding random access memory bit rate real time systems;high definition television	In this paper, we propose a high-throughput low-latency arithmetic encoder (AE) design suitable for high definition (HD) real-time applications employing advanced video coding standards such as H.264/AVC or AVS and using a macroblock (MB) level pipeline. First, in order to derive the performance requirement on the AE, a buffer model in connected with which it is designed is thoroughly analyzed. Then, using joint algorithm-architecture optimization and multi-bin processing techniques, we introduce a novel binary arithmetic coder (BAC) architecture with throughput of 2~4 bins per cycle sufficient for real-time encoding. Furthermore, a hybrid context memory scheme is presented to meet the throughput requirement on the BAC. Simulation result shows that our design can support 1080p at 60 fps for AVS HDTV real-time coding with a bin rate up to 107K per MB line. Synthesized with the TSMC 0.13μm technology, the AE can run at 200MHz and costs 47.3K gates. By operating at 130MHz, the design is also verified in an AVS HD encoder on a Xilinx Virtex-6 FPGA prototype board for 1080p at 30 fps.	advanced visualization studio;algorithm;arithmetic coding;batman: arkham city;binary number;data compression;encoder;field-programmable gate array;h.264/mpeg-4 avc;high-throughput computing;macroblock;mathematical optimization;prototype;real-time clock;real-time transcription;simulation;throughput;video coding format	Yuan Li;Shanghang Zhang;Huizhu Jia;Xiaodong Xie;Wen Gao	2013	2013 IEEE International Symposium on Circuits and Systems (ISCAS2013)	10.1109/ISCAS.2013.6572017	embedded system;binary code;real-time computing;computer hardware;computer science;context-adaptive binary arithmetic coding;field-programmable gate array;encoding	Embedded	12.150471701901536	40.55564421415808	125654
3b8fc44ea72b889305ec7231e6ed5fd6fd91ed7d	exclusive or operation that leads to the narrowest intervals			exclusive or	Alfredo Gabaldon;Hung T. Nguyen	1998	Reliable Computing	10.1023/A:1009911831067	discrete mathematics;mathematics;exclusive or	NLP	21.310456344552712	41.14073189817434	125934
db96c9f4863536954850f86250d8d2b84faa58ae	improved table lookup algorithms for postscaled division	27 bit approximation;interpolation;concurrent computing;approximation algorithms;20 kbyte;iterative algorithm;acceleration;indium tin oxide;single precision accuracy;integer addition;postscaled division;5 kbyte;arithmetic;digital arithmetic;table lookup delay approximation algorithms computer science arithmetic acceleration indium tin oxide interpolation concurrent computing;floating point;double extended precision division;computer science;table lookup algorithms;digital arithmetic table lookup;noniterative algorithm;20 kbyte table lookup algorithms postscaled division noniterative algorithm single precision accuracy three term product multiplication integer addition 27 bit approximation double extended precision division 5 kbyte;table lookup;multiplication;three term product	Postscaled division is a non-iterative algorithm delivering a quotient of single precision accuracy by the three term product (x$)c and of double precision accuracy by the formula [(z$)c][Z (y$)c]. Here x is the dividend, $ is a low order part complemented form of the divisor y. and c is a table lookup value approximating a “reciprocal function” l/(y$) to a precision of over 27 bits. Table lookup latency is hidden by pe$orming the lookup in parallel with thejrst multiplication (xE), with the second multiplication the ”postscaling ’’ by the lookup function value. Our contribution herein is the description of two new lookup algorithms for approximating the reciprocal j h c tion 5 to high accuracy in fewer cycles than a typicaljoating point multiply latency. Our indirect bipartite lookup procedure has a latency of two successive lookups followed by a small integer addition. This first algorithm generates a 27 bit approximation of & with total table size about 5 Kbytes. Our second lookup algorithm generates a 34 bit approximation with latency determined by I 1 and I 2 bit lable lookups and a 4-1 addition. This second approximation employs some 20 Kbytes of tables to allow for a double extended precision division result in the same number of cycles as a double precision result.	algorithm;approximation;double-precision floating-point format;extended precision;iterative method;kilobyte;lookup table;matrix multiplication;single-precision floating-point format	David W. Matula	2001		10.1109/ARITH.2001.930109	acceleration;discrete mathematics;concurrent computing;indium tin oxide;interpolation;computer science;floating point;theoretical computer science;operating system;mathematics;iterative method;multiplication;approximation algorithm;algorithm	Theory	13.462196577902159	43.90783449461784	125971
07219a4372c0826f1a6e5aeff9804adf1491d6e6	complete visibility for mobile robots with lights tolerating faults		We consider the distributed setting of N autonomous mobile robots that operate in LookCompute-Move (LCM) cycles and communicate with other robots using colored lights (the robots with lights model). We study the fundamental Complete Visibility problem of repositioning N robots on a plane so that each robot is visible to all others. We assume obstructed visibility under which a robot cannot see another robot if a third robot is positioned between them on the straight line connecting them. We are interested in fault-tolerant algorithms; all existing algorithms for this problem are not fault-tolerant (except handling some special cases). We study fault-tolerance with respect to failures on the mobility of robots. Therefore, any algorithm for Complete Visibility is required to provide visibility between all non-faulty robots, independently of the behavior of the faulty ones. We model mobility failures as crash faults in which each faulty robot is allowed to stop its movement at any time and, once the faulty robot stopped moving, that robot will remain stationary indefinitely. In this paper, we present and analyze an algorithm that solves Complete Visibility tolerating one crash-faulty robot in a system of N ≥ 3 robots, starting from any arbitrary initial configuration. We also provide an impossibility result on solving Complete Visibility if a single robot is Byzantine-faulty in a system of N = 3 robots; in the Byzantine fault model, a faulty robot might behave in an unpredictable, arbitrary, and unforeseeable ways. Furthermore, we discuss how to solve Complete Visibility for some initial configurations of robots (which we call feasible initial configurations) in the crash fault model, where two robots are (crash) faulty.	algorithm;autonomous robot;crash (computing);fault model;latent class model;mobile robot;stationary process;visibility (geometry)	Aisha Aljohani;Gokarna Sharma	2018	IJNC		line (geometry);visibility;robot;fault model;mobile robot;byzantine fault tolerance;distributed computing;crash;computer science	Robotics	17.831570014516206	34.553794600476536	126469
5e8d24872e991c5cd6d3865e21f8205dd829069f	a single-chip multiprocessor for multimedia: the mvp	data stream;computer graphic equipment;chip multiprocessor;multimedia systems;chip;shared memory systems;parallel architectures;digital signal processing chips;shared memory systems computer graphic equipment digital signal processing chips multimedia systems parallel architectures;image coding video compression image storage image recognition transform coding computer graphics computer architecture cmos technology parallel processing streaming media;high performance;parallel processing;crossbar network single chip multiprocessor multimedia mvp multimedia video processor parallel processing graphics applications semiconductor chip fully programmable processors multiple data streams shared rams	The multimedia video processor (MVP) architecture, which incorporates a variety of parallel processing techniques to deliver very high performance to a wide range of imaging and graphics applications, is described. The MVP combines, on a single semiconductor chip, multiple fully programmable processors with multiple data streams connected to shared RAMs through a crossbar network. Each of the independent processors can execute many operations in parallel every cycle. The architecture is scalable and supports different numbers of processors to meet the cost and performance requirements of different markets. MVP's target environment and the development of MVP are outlined.<<ETX>>	central processing unit;crossbar switch;graphics;integrated circuit;multi-core processor;multiprocessing;parallel computing;requirement;scalability;semiconductor;video processing	Karl M. Guttag;Robert J. Gove;Jerry R. Van Aken	1992	IEEE Computer Graphics and Applications	10.1109/38.163625	chip;embedded system;parallel processing;computer architecture;parallel computing;computer science;operating system	Arch	10.581613836874519	40.783286852124675	126903
660f780cdde633eebb4303f6f74f76c5dca20b98	compressing bi-level images by block matching on a tree architecture		A work-optimal O(log M log n) time parallel implementation of lossless image compression by block matching of bi-level images is shown on a full binary tree architecture under some realistic assumptions, where n is the size of the image and M is the maximum size of the match. Decompression on this architecture is also possible with the same parallel computational complexity. Such implementations have no scalability isuues.	algorithm;binary tree;black and burst;central processing unit;code;computational complexity theory;data compression;image compression;input/output;lossless compression;monochrome;multigrid method;parsing;pixel;scalability;shared memory	Sergio De Agostino	2009			discrete mathematics;binary tree;theoretical computer science;computational complexity theory;scalability;architecture;binary logarithm;computer science;lossless compression	ML	11.987806401313446	37.47076280215959	126936
def9a907a96ba0248026305c5cdb526facc3d34d	a processor array with bounded i/o ports for computing transitive closures	processing element;memoria tampon;largeur bande;entrada salida;microprogrammation;microprogramacion;generation code;cerradura transitiva;generacion codigo;code generation;conception;procesador panel;calculo automatico;satisfiability;array processor;computing;input output;calcul automatique;processeur tableau;fermeture transitive;anchura banda;diseno;transitive closure;bandwidth;design;floyd warshall algorithm;microprogramming;algoritmo optimo;memoire tampon;algorithme optimal;optimal algorithm;entree sortie;buffer memory	Abstract   In this paper, we present the design of a processor array (PA) with bounded number of input/output (I/O) ports ( L -by- N  PEs with  O ( L ) I/O ports, where 1 ≤  L  ≤  N ) for computing transitive closures. Previous designs of PAs to solve this problem use either  N -by- N  PAs with  O ( N ) I/O ports or linear PAs with  O (1) I/O ports. These designs are restricted by their I/O bandwidth because designs using  O ( N ) I/O ports are limited by the number of PEs that can be supported by these ports, whereas designs with  O (1) ports are limited by the I/O time to load and drain data. In contrast, our proposed design uses O(L) I/O ports, where L is a technology-dependent design parameter. In our proposed architecture, we have used more powerful processing elements (PEs), each with a microprogram-driven ALU, a limited number of I/O buffers, and  O ( N / L ) memory words. Our technique for mapping transitive-closure computations consists of four steps: (1) Starting from a mapping of a transitive-closure problem on an  N -by- N  PA, we map the computations into an  L -by- N  PA. (2) We then schedule the computations in the  L -by- N PA in order to satisfy the dependencies. (3) Based on the new mapping, we derive speeds and directions of data flows and the number of buffers required in each PE. (4) Finally, we derive the microprogram in each PE for correct execution. We show that our design specializes to efficient designs in the boundary cases when  L  = 1 (linear PA) and  L  =  N  (square mesh).	memory-mapped i/o;processor array	Mokhtar Aboelaze;Benjamin W. Wah	1995	J. Parallel Distrib. Comput.	10.1006/jpdc.1995.1108	design;combinatorics;computing;telecommunications;computer science;theoretical computer science;operating system;mathematics;distributed computing;transitive closure;bandwidth;algorithm;code generation;satisfiability	HPC	13.222273925960252	35.4652349750882	127099
44cd451087390156e3e7e884e80820936e4a3562	a low-cost hardware architecture binarizer design for the h.264/avc cabac entropy coding	video coding adaptive codes arithmetic codes binary codes entropy codes;binary codes;video compression;entropy coding;adaptive codes;hardware architecture;video coding;arithmetic codes;logic gates;entropy codes;context;context based adaptive binary arithmetic coding;logic gates context;kth order exp golomb encoder low cost hardware architecture binarizer design h 264 avc cab ac entropy coding context based adaptive binary arithmetic coding h 264 avc video compression standard	This paper presents two hardware architectures design for the binarizer part of the CABAC (Context-Based Adaptive Binary Arithmetic Coding) entropy encoder as defined in the H.264/AVC video compression standard. The architectures proposed in this paper are able to reach the Level 4.2 processing requirements of the standard specification, achieving processing rates of 103,9 Mbins/s. The proposed solutions can save on average 50% hardware resources, mainly because a new technique to enable the reuse of hardware is applied, avoiding the support of the Kth order Exp-Golomb encoder.	beta encoder;binary number;context-adaptive binary arithmetic coding;entropy encoding;golomb ruler;h.264/mpeg-4 avc;requirement;video coding format	André Luís Del Mestre Martins;Vagner Santos Da Rosa;Sergio Bampi	2010	2010 17th IEEE International Conference on Electronics, Circuits and Systems	10.1109/ICECS.2010.5724535	scalable video coding;block code;arithmetic coding;parallel computing;real-time computing;shannon–fano coding;variable-length code;computer science;entropy encoding;theoretical computer science;context-adaptive variable-length coding;tunstall coding;adaptive coding;context-adaptive binary arithmetic coding;huffman coding	EDA	12.326777332151678	40.299352421739336	127177
529363b7d24c5ddd74c19e41d9c6c03e5ff76012	sorting and selection on a linear array with optical bus system	sistema optico;communication system;optical system;bus optique;sorting;barreta lineal;linear array;selection;barra colectora optica;barrette lineaire;tria;parallel computation;calculo paralelo;triage;systeme optique;randomized algorithm;parallel machines;systeme parallele;parallel system;seleccion;communication;calcul parallele;comunicacion;optical bus;sistema paralelo	In this paper we present randomized algorithms for selection and sorting on linear arrays with optical bus communication systems. We show that sorting n given numbers can be performed in O(log n) time with high probability on a linear array, of n processors, with a reconfigurable optical bus system (LinearAROB). We also show that selection can be performed in O(1) time with high probability on the same parallel machine model.	central processing unit;charge-coupled device;deterministic algorithm;parallel computing;randomized algorithm;selection algorithm;sorting;with high probability	Hossam A. ElGindy;Sanguthevar Rajasekaran	1999	Parallel Processing Letters	10.1142/S0129626499000347	embedded system;selection;telecommunications;computer science;sorting;randomized algorithm;algorithm;communications system	Theory	12.197835572385364	34.87762473374021	127278
5804fb1b5bb831991f2b383e8dfc45c74189e477	area-efficient fpga implementation of quadruple precision floating point multiplier	standards;multiplying circuits;logic design;reconfigurable computing;floating point multiplication;high performance computing;multiplying circuits digital signal processing chips field programmable gate arrays floating point arithmetic logic design;quadruple precision;karatsuba multiplication;reconfigurable computing floating point multiplication fpga quadruple precision arithmetic karatsuba multiplication high performance computing;fpga;dsp48 block area efficient virtex 4 fpga implementation quadruple precision floating point multiplier signal processing application quadruple precision arithmetic qp arithmetic logic complexity qp multiplication implementation;field programmable gate arrays hardware standards adders signal processing program processors application specific integrated circuits;application specific integrated circuits;adders;signal processing;arithmetic;digital signal processing chips;floating point arithmetic;field programmable gate arrays;program processors;hardware	Floating point multiplication is a crucial and useful arithmetic operation for many scientific and signal processing applications. High precision requirements of many applications lead to the incorporation of quadruple precision (QP) arithmetics. The logic complexity and performance overhead of quadruple precision arithmetic are quite large. This paper has focused on one of the quadruple precision arithmetic operations, multiplication. We present an efficient implementation of QP multiplication operation on a reconfigurable FPGA platform. The presented design uses much less hardware resource in terms of DSP48 blocks, and slices with a higher performance. Promising results are obtained by comparing the proposed designs with the best reported QP floating point multiplier in the literature. We have achieved more than 50% improvements in the amount of DSP48 block at a slight cost of additional slices, on a Virtex-4 FPGA.	field-programmable gate array;lagrange multiplier;overhead (computing);quadruple-precision floating-point format;requirement;signal processing	Manish Kumar Jaiswal;Ray C. C. Cheung	2012	2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum	10.1109/IPDPSW.2012.46	quadruple-precision floating-point format;computer architecture;parallel computing;computer science;operating system;signal processing;extended precision;field-programmable gate array	EDA	10.6111447022857	45.1400658040541	127395
bf4b611a9ab503d2802a7a44c568a65158b308eb	constructing arrangements of lines and hyperplanes with applications	ducts;complexity theory;time complexity;application software;color;radiation detectors;computational geometry;testing;usa councils;sleep;upper bound;shape;arrangement of hyperplanes;image color analysis;data structures;computer science;switches;fires;optimal algorithm;application software computer science testing data structures computational geometry parallel processing;parallel processing;buildings;voronoi diagram;solids	An optimal algorithm is presented for constructing an arrangement of hyperplanes in arbitrary dimensions. It relies on a combinatorial result that is of interest in its own right. The algorithm is shown to improve known worst-case time complexities for five problems: computing all order-k Voronoi diagrams, computing the λ-matrix, estimating halfspace queries, degeneracy testing, and finding the minimum volume simplex determined by a set of points.	algorithm;best, worst and average case;degeneracy (graph theory);voronoi diagram	Herbert Edelsbrunner;Joseph O'Rourke;Raimund Seidel	1983	24th Annual Symposium on Foundations of Computer Science (sfcs 1983)	10.1109/SFCS.1983.11	duct;time complexity;parallel processing;combinatorics;application software;discrete mathematics;voronoi diagram;data structure;network switch;computational geometry;shape;computer science;theoretical computer science;solid;mathematics;software testing;arrangement of hyperplanes;sleep;upper and lower bounds;particle detector;algorithm;statistics;algebra	Theory	14.544854539317798	33.37719753418547	127877
06a03b65aea7bdfe95b8703d39d04e227a85a2ee	a wavelet-tree image coding system with efficient memory utilization	focusing;image coding;memory modules;data compression;decoding;image coding engines decoding wavelet transforms encoding computer architecture focusing multimedia systems video compression mpeg 4 standard;video compression;efficient memory utilization;decoding wavelet transforms transform coding data compression image coding pipeline processing trees mathematics;transform coding;trees mathematics;coding performance;wavelet tree image coding system;multimedia systems;memory access;wavelet transforms;computer architecture;mpeg 4 standard;engines;efficient implementation;pipelined operation;spiht coding wavelet tree image coding system efficient memory utilization independent wavelet tree coding transform coding decoding engine coding engine pipelined operation system architecture coding performance memory modules memory access;spiht coding;decoding engine;system architecture;encoding;independent wavelet tree coding;pipeline processing;coding engine	This paper describes an efficient implementation of an image coding system based on the independent wavelet-tree coding concept. The system consists of a transform and a (de)coding engine that operate in a pipelined fashion. The main focus of this paper will be on the encoding part since, due to the system architecture, the decoder has identical memory utilization. Experimental results prove that the proposed system achieves comparable coding performance to the state-of-the-art, while it localizes the memory accesses to small memory modules and uses minimal computational resources.	computational resource;dimm;pipeline (computing);systems architecture;wavelet tree	Yiannis Andreopoulos;Peter Schelkens;Nikolaos D. Zervas;Thanos Stouraitis;Constantinos E. Goutis;Jan Cornelis	2001		10.1109/ICASSP.2001.941268	data compression;computer vision;interleaved memory;real-time computing;computer science;theoretical computer science;coding tree unit;mathematics;context-adaptive binary arithmetic coding;statistics;systems architecture	AI	11.760728799734736	39.948040458027734	128187
225e9ccdeabeb76f531cf587b8a0139c86928d7e	finding all simple disjunctive decompositions using irredundant sum-of-products forms	logic cad;minimisation of switching nets;compact logic networks;disjunctive decompositions;factored logic networks;factorization;functional decomposition method;input variable set;irredundant sum-of-products forms;large scale functions;simple disjunctive decompositions;single output subblock function;two-level logic factorization method	Finding disjunctive decompositions is an important technique to realize compact logic networks. Simple disjunctive decomposition is a basic and useful concept, that extracts a single output subblock function whose input variable set is disjunctive from the other part. The paper presents a method for finding simple disjunctive decompositions by generating irredundant sum-of-products forms and applying factorization. We prove that all simple disjunctive decompositions can be extracted in our method, namely all possible decompositions are included in the factored logic networks. Experimental results show that our method can efficiently extract all the simple disjunctive decompositions of the large scale functions. Our result clarifies the relationship between the functional decomposition method and the two-level logic factorization method.	algorithm;circuit complexity;circuit minimization for boolean functions;disjunctive normal form;logic optimization;mathematical optimization;semantic-oriented programming	Shin-ichi Minato;Giovanni De Micheli	1998	1998 IEEE/ACM International Conference on Computer-Aided Design. Digest of Technical Papers (IEEE Cat. No.98CB36287)	10.1109/ICCAD.1998.742859	functional decomposition;mathematical optimization;combinatorics;discrete mathematics;computer science;mathematics;logic;algorithm	EDA	18.98443823288901	45.82243630269339	128236
91b2b571071dcc3015adfee06b0db16f3ec5b53e	on the maximum edge length in vlsi layouts of complete binary trees	graph theory;concepcion asistida;computer aided design;teoria grafo;architecture systeme;integrated circuit;circuit vlsi;circuito integrado;theorie graphe;vlsi circuit;arbol binario;arbre binaire;conception assistee;arquitectura sistema;vlsi layout;circuito vlsi;system architecture;circuit integre;binary tree		recursion (computer science);very-large-scale integration	Hartmut Schmeck	1986	Inf. Process. Lett.	10.1016/0020-0190(86)90124-9	combinatorics;binary tree;computer science;graph theory;integrated circuit;mathematics;algorithm	Theory	17.974559527577984	38.63043360887661	128237
9d9845673dd309285f407ddfc4affc77e49d28d3	mapping real-time motion estimation type algorithms to memory efficient, programmable multi-processor architectures	processor architecture;largeur bande;optimisation;estimacion;movimiento;full search;algoritmo busqueda;programmation;image processing;optimizacion;algorithme recherche;real time;search algorithm;procesamiento imagen;real time processing;data exchange;motion estimation;motion;block oriented video algorithms;pulga electronica;taille memoire;traitement image;interconnection network;chip;programacion;tratamiento tiempo real;computer architecture;traitement temps reel;architecture ordinateur;senal video;signal video;estimation;mouvement;anchura banda;architectural template;video signal;bandwidth;optimization;procesador;arquitectura ordenador;memory size and transfer optimization;processeur;puce electronique;programming;programmable vsp s;processor	In this paper, an architectural template is presented, which is able to execute the full search motion estimation algorithm or other similar video or image processing algorithms in real time. The architecture is based on a set of programmable video signal processors (VSP's). It is also possible to integrate the processor cores and their local memories on a (set of) chip(s). Due to the programmability, the system is very flexible and can be used for emulation of other similar block-oriented local-neighborhood algorithms. The architecture can be easily divided into several partitions, without data-exchange between partitions. Special attention is paid to memory size and transfer optimization, which are dominant factors for both area and power cost. The trade-offs and techniques used to arrive at these solutions are explained in detail. It is shown that careful optimizations can lead to large savings in memory size (up to 66%) and bandwidth requirements (up to a factor of 4) compared to a straightforward solution.	algorithm;motion estimation;multiprocessing;real-time clock	Eddy de Greef;Francky Catthoor;Hugo De Man	1995	Microprocessing and Microprogramming	10.1016/0165-6074(95)99030-9	chip;data exchange;embedded system;programming;estimation;parallel computing;image processing;microarchitecture;computer science;motion;motion estimation;bandwidth;search algorithm;computer graphics (images)	EDA	10.845433651749635	37.28681799649876	128326
ad7ac914442093513886ef97c874b866880b4219	error analysis of certain floating-point on-line algorithms	floating point on line algorithm;concurrent computing;very large scale integration;redundant significand;redundant significand absolute error floating point on line algorithm maximum relative representation error pseudo normalized significand;error analysis;large scale integration;redundancy;maximum relative representation error;arithmetic;absolute error;bandwidth;floating point;computer science;on line algorithm;computer errors;pseudo normalized significand	The properties of redundant number system in significand (mantissa) representation are studied and the range of redundant significand is derived. From the range of the redundant significand and the absolute error of on-line operations, the MRRE (maximum relative representation error) is defined and analyzed for floating-point on-line addition and multiplication.	algorithm;approximation error;online and offline;significand	Osaaki Watanuki;Milos D. Ercegovac	1983	IEEE Transactions on Computers	10.1109/TC.1983.1676236	mathematical optimization;approximation error;concurrent computing;computer science;floating point;theoretical computer science;operating system;very-large-scale integration;redundancy;programming language;bandwidth;algorithm;statistics	Visualization	13.40924055216127	44.0952832144227	128350
fd8dd4109687b9ef6f95d2958fec4b0546319054	online graph exploration algorithms for cycles and trees by multiple searchers	graph exploration;online algorithms;competitive ratio	This paper deals with online graph exploration problems by multiple searchers. The information on the graph is given online. As the exploration proceeds, searchers gain more information on the graph. Assuming an appropriate communication model among searchers, searchers can share the information about the environment. Thus, a searcher must decide which vertex to visit next based on the partial information on the graph gained so far by searchers. We assume that all searchers initially start the exploration at the origin vertex, and the goal is that each vertex is visited by at least one searcher and all searchers finally return to the origin vertex. The objective is to minimize the time when the goal is achieved. We study the case of cycles and trees. For the former, we give an optimal online exploration algorithm in terms of competitive ratio, and for the latter, we also give an online exploration algorithm which is optimal among greedy algorithms.	algorithm	Yuya Higashikawa;Naoki Katoh;Stefan Langerman;Shin-ichi Tanigawa	2014	J. Comb. Optim.	10.1007/s10878-012-9571-y	competitive analysis;online algorithm;mathematical optimization;combinatorics;computer science;theoretical computer science;machine learning;mathematics	Theory	18.088850427489742	33.693942986287745	128400
889684cb3b8d12c8771d6e10080891408b05e443	network reliability analysis using 2-connected digraph reductions	fiabilidad;reliability;reseau;red;fiabilite;network reliability;network	Abstract#R##N##R##N#The problem of computing the SKT reliability, the probability that a source s can send communication to a specified set of terminals K ⊂ V in a probabilistic digraph D = (V,E) is considered. For general digraphs, this problem is known to be NP-hard and it is helpful to consider schemes that can decompose the problem into a number of smaller problems. A non-separable digraph is 2-connected if it contains a separation pair, a pair of nodes whose removal disconnects the digraph. Such a digraph can be partitioned into two or more segments. It is shown that at least one of these segments can be replaced by a simpler structure; this replacement results in an exact reliability preserving reduction. The proposed reduction scheme is general and is applicable to all digraphs containing a separation pair; earlier methods could only handle special cases. For a class of digraphs, called BSP digraphs, such a reduction is always admissible and the SKT reliability can be computed in time O(∣E∣). A digraph is a BSP digraph if its underlying undirected graph is series-parallel. A BSP digraph can be cyclic or acyclic. No polynomial-time algorithms were previously known for this class of digraphs.	directed graph;reliability engineering	Avinash Agrawal;A. Satyanarayana	1985	Networks	10.1002/net.3230150209	combinatorics;discrete mathematics;computer science;reliability;mathematics;reliability;algorithm;computer network	Logic	23.25432786993381	33.94707252193689	128531
cad4c78c53c912666a52d3ee61e3abc1777cd572	fault-tolerant hamiltonicity of twisted cubes	graphe non oriente;tolerancia falta;hypercube;forme torsadee;graphe biparti;non directed graph;fault tolerant;grafo bipartido;cube;cubo;twisted shape;hamiltonian;hamiltonien;interconnection network;topological properties;graph connectivity;fault tolerant system;grafo no orientado;fault tolerance;conectividad grafo;sistema tolerando faltas;twisted cube;systeme tolerant les pannes;forma torcida;bipartite graph;connectivite graphe;article;hamiltonian connected;hamiltoniano;tolerance faute;red interconexion;hamiltonian path;reseau interconnexion;hipercubo	The twisted cube TQn, is derived by changing some connection of hypercube Qn according to specific rules. Recently, many topological properties of this variation cube are studied. In this paper, we consider a faulty twisted n-cube with both edge and/or node faults. Let F be a subset of V(TQn) 5 E(TQn), we prove that TQn−F remains hamiltonian if |F| [ n−2. Moreover, we prove that there exists a hamiltonian path in TQn−F joining any two vertices u, v in V(TQn)−F if |F| [ n−3. The result is optimum in the sense that the fault-tolerant hamiltonicity (fault-tolerant hamiltonian connectivity respectively) of TQn is at most n−2 (n−3 respectively). © 2002 Elsevier Science (USA)	fault tolerance;hamiltonian (quantum mechanics);hamiltonian path;interconnection;klee–minty cube;olap cube;twisted	Wen-Tzeng Huang;Jimmy J. M. Tan;Chun-Nan Hung;Lih-Hsing Hsu	2002	J. Parallel Distrib. Comput.	10.1006/jpdc.2001.1813	fault tolerance;combinatorics;discrete mathematics;topology;mathematics	Theory	24.13394814273906	34.2848925560214	128532
d1b90a89621805aaceeb40849f5bcc5082562a95	interfacing reversible pass-transistor cmos chips with conventional restoring cmos circuits	quantum inspired circuit;technology and engineering;signal processing;adiabatic calculation;interface;pass transistor;cmos;reversible computation;transmission gate;restoring cmos	Recent progress on the prototyping of reversible digital circuits, have shown that adiabatic reversible dual-line pass-transistor logic can be used for special purpose applications in reversible computation. This, however, raises new issues regarding the compatibility between this adiabatic logic implementation and conventional CMOS logic. The greatest difficulty is brought by the difference in signal shape used by these two logic families. Whereas standard switching CMOS circuits are operated by rectangular pulses, dual-line pass-transistor reversible circuits are controlled by triangular or trapezoidal signals to ensure adiabatic switching of the transistors. This work proposes a simple technical solution that allows interfacing reversible pass-transistor logic with conventional CMOS logic, represented here by an FPGA embedded in a commercial Xilinx Spartan-3E board. All proposed solutions have successfully been tested, which enables the FPGA to perform calculations directly on a reversible chip.	cmos;computation;digital electronics;embedded system;field-programmable gate array;logic family;pass transistor logic;reversible computing	Stéphane Burignat;Michael Kirkedal Thomsen;Michal Klimczak;Mariusz Olczak;Alexis De Vos	2011		10.1007/978-3-642-29517-1_10	adiabatic circuit;logic level;logic gate;logic family;computer science;theoretical computer science;signal processing;pass transistor logic;interface;integrated injection logic;programming language;cmos	EDA	16.243451907633688	45.3080124044054	128636
6f96ce774749a3656a9ac0f6f9d5a1f5f3479b62	compression and packetization for mpeg-2/4 video and audio	audio coding;computer architecture;digital signal processing chips;speech codecs;video codecs;video coding;dsp core;mpeg-2/4 audio;mpeg-2/4 video;audio codec;flexible lsi architectural design concept;high-performed embedded digital signal processor core;single-chip architecture;video codec;digital signal processing;encoding;transform coding	We describe a flexible LSI architectural design concept suitable for various video-oriented multimedia applications. Our proposed LSI design assumes various schemes and picture formats of compression in a single-chip architecture. The hybrid architecture of an high-performed embedded Digital Signal Processor (DSP) core and dedicated hardware processing units, and its scaleable architecture are the key features. DSP core supports the processing that requires the complexity and flexibility and lower processing power. Hardware parts support the processing components that are frequently used and require processing power and lower complexity. In this architecture, evaluated performance shows that a single chip can perform MPEG MP@ML (Main Profile at Main Level) processing including video and audio codec, and media packetization. With its scaleable architecture, MPEG MP@HL (Main Profile at High Level) compression can be carried out with only 6 chips. This single LSI can also have a capability of extending to H.263 or forthcoming MPEG-4 standard algorithm by a single chip on the embedded DSP core basis.	algorithm;codec;complexity;compression artifact;digital signal processor;embedded system;moving picture experts group;network packet;software architecture	Tokumichi Murakami;Kohtaro Asai;Hideo Ohira	1998	9th European Signal Processing Conference (EUSIPCO 1998)		embedded system;codec;real-time computing;computer hardware;computer science	EDA	11.245026197560032	40.81147701110787	128950
e66cb8b2238275be4e166b6fb2b03e5403bab00c	a memory-efficient architecture for intra predictor and de-blocking filter in video coding system	decoding;video coding decoding streaming media image edge detection memory architecture memory management filtering algorithms;video coding;cmos digital integrated circuits;memory architecture;proceedings paper;video coding cmos digital integrated circuits decoding memory architecture power consumption sram chips;power consumption;cmos technology intrapredictor deblocking filter h 264 avc video coding system video resolution internal memory block data prediction next generation video application target resolution ultra hd memory efficient architecture decoder size reduction power consumption sequential interleaving memory architecture video decoding;sram chips	In the hardware architecture of the H.264/AVC video coding systems, the storage size of the intra predictor and de-blocking filter occupies a great portion of the internal memory size in the video coding. However, the higher resolution video costs huge internal memory size to store pixels to predict block data and eliminate the blocking effect, especially for the next-generation video applications which target resolution is Ultra-HD (8K×4K). In this article, a memory-efficient architecture for intra predictor and de-blocking filter has been proposed which can roughly reduce up to 19% internal memory usage to efficiently reduce the decoder size and power consumption, and the sequential-interleaving memory architecture has also been adopted in the proposed architecture to solve the memory access conflict during video decoding. A test module is designed for the proposal and operates at 200 MHz for real-time processing with 85.1 K gates and 8.4 KB SRAM in 90nm CMOS technology.	blocking (computing);cmos;computer data storage;data compression;deblocking filter;forward error correction;h.264/mpeg-4 avc;kerrison predictor;pixel;real-time clock;static random-access memory;video decoder	Chia-Lin Liu;Chang-Hung Tsai;Hsiuan-Ting Wang;Yao Li;Chen-Yi Lee	2012	2012 IEEE Asia Pacific Conference on Circuits and Systems	10.1109/APCCAS.2012.6419095	scalable video coding;uniform memory access;interleaved memory;electronic engineering;semiconductor memory;real-time computing;computer hardware;computer science;deblocking filter;decoding methods;registered memory;statistics	EDA	12.547549884988634	40.26394948069112	129076
587e64dbfef13a2845e23c95c6170e023454b499	parallel hough transform algorithm performance	algoritmo paralelo;parallel algorithm;image processing;edge detection;procesamiento imagen;transformacion hough;traitement image;algorithme parallele;deteccion contorno;detection contour;analyse performance;performance analysis;hough transformation;hough transform;transformation hough;analisis eficacia	Abstract   The Hough Transform is a computationally intensive transform useful in detecting lines in digital images. Efforts have been made to parallelize the HT, focusing on partitioning the work for a multiprocessor system. Variations on two partitioning schemes are compared, and a breakdown of the various computation overheads is presented and analyzed.	algorithm;hough transform	Matthew J. Thazhuthaveetil;Anish V. Shah	1991	Image Vision Comput.	10.1016/0262-8856(91)90017-J	hough transform;computer vision;image processing;computer science;mathematics;algorithm;computer graphics (images)	Vision	11.361880630387121	35.3750414504975	129510
717ed5386ba73c52c800989d451771693f7f42a0	comparison of the cost metrics through investigation of the relation between optimal ncv and optimal nct three-qubit reversible circuits	nct three qubit reversible circuit;information quantique;diseno circuito;synthese circuit;realisation circuit;algoritmo busqueda;ncv circuit;logic design;quantum information;algorithme recherche;electronique quantique;estudio comparativo;circuit design;search algorithm;logic circuits;controlled v gate;cnot;quantum gates;etude comparative;breadth first search method;informacion cuantica;circuit realization;realizacion circuito;search problems logic circuits logic design quantum gates;toffoli gate circuit ncv circuit nct three qubit reversible circuit breadth first search method quantum not cnot controlled v gate;comparative study;quantum electronics;sintesis circuito;conception circuit;search problems;0367;gating circuits;quantum not;circuit synthesis;toffoli gate circuit;circuit porte	A breadth-first search method for determining optimal 3-qubit circuits composed of quantum NOT, CNOT, controlled-V and controlled-V+ (NCV) gates is introduced. Results are presented for simple gate count and for technology motivated cost metrics. The optimal NCV circuits are also compared to NCV circuits derived from optimal NOT, CNOT and Toffoli (NCT) gate circuits. The work presented here provides basic results and motivation for continued study of the direct synthesis of NCV circuits, and establishes relations between function realizations in different circuit cost metrics.	breadth-first search;controlled not gate;emoticon;expect;gate count;library (computing);logic gate;qubit;toffoli gate	Dmitri Maslov;D. Michael Miller	2007	IET Computers & Digital Techniques	10.1049/iet-cdt:20060070	electronic engineering;quantum information;logic synthesis;logic gate;computer science;theoretical computer science;circuit design;comparative research;controlled not gate;algorithm;search algorithm;quantum gate	EDA	17.352902691796885	45.64217192615419	129661
c0bbfabac7237a0e733f2d20815001b7a5081ecf	impact of reconfigurable function on meshes with row/column buses		This paper studies the difference in computational power between the mesh-connected parallel computers equipped with dynamically reconfigurable bus systems and those with static ones. The mesh with separable buses (MSB) is the mesh-connected computer with dynamically reconfigurable row/column buses. The broadcasting buses of the MSB can be dynamically sectioned into smaller bus segments by program control. We examine the impact of reconfigurable capability on the computational power of the MSB model, and investigate how computing power of the MSB decreases when we deprive the MSB of its reconfigurability. We show that any single step of the MSB of size n × n can be simulated in O (log n) time by the MSB without its reconfigurable function, which means that the MSB of size n × n can work with O (log n) step slowdown even if its dynamic reconfigurable function is disabled.	computer;parallel computing;reconfigurability;simulation	Susumu Matsumae	2011	IJNC		embedded system;parallel computing;real-time computing;computer science	HPC	12.470624655904462	33.00734973748278	130152
3f84fe7981598bfeb91ce52d0b7881806321af42	contest architecture	elimination tournament;optimal architecture;prizes;incentives	A contest architecture specifies how the contestants are split among several subcontests whose winners compete against each other (while other players are eliminated). We compare the performance of such dynamic schemes to that of static winner-take-all contests from the point of view of a designer who maximizes either the expected total effort or the expected highest effort. For the case of a linear cost of effort, our main results are: 1) If the designer maximizes expected total effort, the optimal architecture is a single grand static contest. 2) If the designer maximizes the expected highest effort, and if there are sufficiently many competitors, it is optimal to split the competitors in two divisions, and to have a final among the two divisional winners. Finally, if the effort cost functions are convex, the designer may benefit by splitting the contestants into several sub-contests, or by awarding prizes to all finalists.	convex function;point of view (computer hardware company);static single assignment form	Benny Moldovanu;Aner Sela	2006	J. Economic Theory	10.1016/j.jet.2004.10.004		AI	23.246991770554416	42.14546584780471	130317
6baa0cc08dd1af10f9a73878895b41e62ec07fde	lexicographic encoding of numeric data fields	encoding digital arithmetic;compact representation;encoding computer science database systems logic application software arithmetic data engineering associative memory hardware councils;digital arithmetic;encoding;numeric comparison lexicographic encoding numeric data fields variable radix representation compact representation arbitrary numbers bitwise lexicographic comparison	A method of variable-radix representation of numeric data is presented. The method allows compact representation of arbitrary numbers. Among its properties is that bitwise lexicographic comparison (>,  >	level of measurement;lexicographical order	Naphtali Rishe	1989		10.1109/ARITH.1989.72832	discrete mathematics;theoretical computer science;mathematics;algorithm;encoding	NLP	19.916153226557068	42.37550074216311	130491
e606128e83f72919fad4e0ea3351479336b1e96b	delta-sigma waveguides for music synthesis	musica;arte;grupo a	33 Computer Music Journal, 23:4, pp. 33–47, Winter 1999 © 1999 Massachusetts Institute of Technology. Physical modeling of musical instruments has proven to be an interesting and useful technique for sound synthesis, because it provides the variability of real instruments. Natural dynamics can be captured in the model, yielding realistic emulation of traditional instruments. Model parameters can correspond to actual physical parameters so that parameter adjustment is intuitive, and these parameters can be adjusted to produce exotic new sounds. The digital waveguide (Smith 1992) is a computationally efficient method for the physical modeling of strings, drum heads, bores, and other resonant structures that appear in musical instruments. These waveguides can be implemented as a program running on a general-purpose computer which generates and stores sounds for later playback. Computers with sufficient capabilities can generate some of these sounds in real time for playing the sounds as they are produced. Alternately, special-purpose computer hardware can implement these waveguides to produce the sounds in real time. Recent work has introduced the possibility of digital hardware that can be reconfigured, and this alternative implementation may be particularly attractive for music synthesis. The reconfigurable hardware offers operating performance that approaches that of specialized, dedicated devices. However, the hardware can also be rearranged to implement different processing structures. The time required to rearrange the hardware is typically long with respect to the processing time while in any particular configuration, but this constraint appears to be consistent with typical music synthesis systems; that is, flexibility of synthesis is desired, but some delays can be tolerated to “patch” new sounds. The reconfigurable hardware does impose certain implementation constraints when compared with software emulations running on a generalpurpose computer. These constraints are similar to those associated with any hardware implementation of physical models. This article addresses the constraints and shows accommodations that support the use of reconfigurable devices to implement digital waveguides. A direct pulse-code modulated (PCM) implementation of a digital waveguide in hardware leads to underutilization of hardware resources in two significant ways. First, the hardware data paths are clocked at rates near the audio rate of 44.1 kHz, which is significantly below the maximum clock rate typical for modern digital circuits. Second, the wide PCM data paths require large coefficient multipliers in the filter. A delta-sigma approach can be used to produce a more efficient implementation, with the recirculated data reduced to a high-speed stream of single bits. This filter hardware is significantly smaller, because multiplication of a coefficient by a single bit in the stream can be implemented as a multiplexer.	algorithmic efficiency;c++;clock rate;coefficient;computer music journal;computer hardware;delta-sigma modulation;digital electronics;embedded system;emulator;field-programmable gate array;general-purpose markup language;infinite impulse response;interpolation;lagrange multiplier;level of detail;multiplexer;noise generator;quantization (signal processing);reconfigurable computing;simulation;spatial variability;speech synthesis	Eric Wallin;Ronald D. Williams;Maximo H. Salinas	1999	Computer Music Journal	10.1162/014892699559986	computer hardware;software;multiplexer;speech recognition;delta-sigma modulation;computer science;reconfigurable computing;digital electronics;emulation;computer music;clock rate	Graphics	15.055950608268224	41.632465213695774	130530
7692d2058755f78df75ed86158f47a845956fbbe	the menger number of the cartesian product of graphs	cartesian product of graphs;fault tolerant;interconnection network;cartesian product;lower bound;real time systems	Abstract   In a real-time system, the Menger number     ζ    l     (  G  )    is an important measure of the communication efficiency and fault tolerance of the system   G  . In this paper, we obtain a lower bound for the Cartesian product graph. We show that     ζ      l    1    +    l    2       (    G    1    ×    G    2    )   ≥    ζ      l    1       (    G    1    )   +    ζ      l    2       (    G    2    )    for     l    1    ≥  2   and     l    2    ≥  2  . As an application of the result, we determine the exact values     ζ    l     (  G  )    of the grid network   G  =  G   (    m    1    ,    m    2    ,  …  ,    m    n    )    for     m    i    ≥  2     (  1  ≤  i  ≤  n  )    and   l  ≥    ∑    i  =  1    n      m    i    . This example shows that the lower bound of     ζ      l    1    +    l    2       (    G    1    ×    G    2    )    obtained is tight.	menger sponge	Meijie Ma;Jun-Ming Xu;Qiang Zhu	2011	Appl. Math. Lett.	10.1016/j.aml.2010.11.026	fault tolerance;combinatorics;discrete mathematics;cartesian product;mathematics;upper and lower bounds;algorithm	Theory	24.03341952790324	34.67941631454302	130682
0d57710f419bb1d4c6fdf8a73396f0774b6a7737	new ab2 multiplier over gf(2m) using cellular automata	cellular automata		cellular automaton	Kyo-Min Ku;Kyeoung Ju Ha;Hyun-Sung Kim;Kee-Young Yoo	2003			computer science	Logic	17.210260439869334	42.847901809566146	130855
d4a81945dd5f7981c27838ffafbac6db0a633dd0	fully parameterized discrete wavelet packet transform architecture oriented to fpga	transformation ondelette;field programmable gate array;diseno circuito;transformacion discreta;filtre reponse impulsion finie;finite impulse response filter;wavelet base;circuit design;base ondita;discrete time;red puerta programable;wavelet packet;reseau porte programmable;filtro respuesta impulsion acabada;fir filter;discrete transformation;distributed arithmetic;conception circuit;discrete wavelet packet transform;transformacion ondita;tiempo discreto;temps discret;base ondelette;transformation discrete;wavelet transformation	The present paper describes a fully parameterized Discrete Wavelet Packet Transform (DWPT) architecture based on a folded Distributed Arithmetic implementation, which makes possible to design any kind of wavelet bases. The proposed parameterized architecture allows different CDF wavelet coefficient with variable bit precision (data input and output size, and coefficient length). Moreover, by combining different blocks in cascade, we can expand as many complete stages (wavelet packet levels) as we require. Our architecture need only two FIR filters to calculate various wavelet stages simultaneously, and specific VIRTEX family resources (SRL16E) have been instantiated to reduce area and increase frequency operation. Finally, a DWPT implementation for CDF(9,7) wavelet coefficients is synthesized on VIRTEX-II 3000-6 FPGA for different precisions.	field-programmable gate array;wavelet packet decomposition	Guillermo Payá Vayá;Marcos Martínez Peiró;Francisco Ballester;Francisco Mora Campos	2003		10.1007/978-3-540-45234-8_52	wavelet;embedded system;second-generation wavelet transform;continuous wavelet transform;computer science;theoretical computer science;finite impulse response;cascade algorithm;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;wavelet transform	EDA	15.072172177293313	41.17830760168521	131478
c57c87aa34610eb631f1755076a4820ccae92d32	a design of pipelined architecture for hierarchical block-matching algorithm	video coding;parallel architecture;motion estimation algorithm		block-matching algorithm	Hyung Chul Kim;Seung Ryoul Maeng;Jung Wan Cho	1995	IEICE Transactions		parallel computing;real-time computing;theoretical computer science	Vision	12.138588489004858	39.89597386355998	131944
2dd38cb1fc7efa9ddaa5e08277aa48229d836a72	a novel memory-based fft architecture for real-valued signals based on a radix-2 decimation-in-frequency algorithm	circuits and systems;memory architecture digital arithmetic fast fourier transforms;computer architecture program processors multiplexing hardware circuits and systems fast fourier transforms radiation detectors;radiation detectors;radix 2 butterflies memory based fft architecture real valued signals radix 2 decimation in frequency algorithm fast fourier transform computation processing element;multiplexing;computer architecture;memory addressing scheme memory based fast fourier transform fft real valued signals real fast fourier transform rfft;fast fourier transforms;program processors;hardware	This brief presents a novel architecture for memory-based fast Fourier transform (FFT) computation for real-valued signals based on radix-2 decimation-in-frequency algorithm. A superior strategy of stage partition for the real FFT (RFFT) is proposed to minimize the computation clock cycles and maximize the utilization of the processing element (PE). The PE employed in our RFFT architecture can process four inputs in parallel by using two radix-2 butterflies and only two multiplexers. The proposed memory-addressing scheme and control of the multiplexers can be expressed in terms of a counter according to the RFFT computation stage. Furthermore, the proposed RFFT architecture can support more PEs in two dimensions as well. Compared with prior works, the proposed RFFT processors have the advantages of fewer computation cycles and lower hardware usage. The experiment shows that the proposed processor reduces the computation cycles by a factor of 17.5% for a 32-point RFFT computation compared with a recently presented work while maintaining lower hardware usage and complexity in the PE design.	addressing scheme;algorithm;central processing unit;clock signal;computation;decimation (signal processing);digital signal processor;fast fourier transform;multiplexer	Zhen-guo Ma;Xiaobo Yin;Feng Yu	2015	IEEE Transactions on Circuits and Systems II: Express Briefs	10.1109/TCSII.2015.2435522	fast fourier transform;electronic engineering;parallel computing;split-radix fft algorithm;computer science;theoretical computer science;particle detector;prime-factor fft algorithm;multiplexing	Arch	12.03417784179945	44.53168265691545	132093
0ce0c95cf0d7e5bdaccb3fb11cb0d73ce7e8493f	lower bound for scalable byzantine agreement	distributed protocol;byzantine agreement;lower bound	We consider the problem of computing Byzantine Agreement in a synchronous network with n processors each with a private random string, where each pair of processors is connected by a private communication line. The adversary is malicious and non-adaptive, i.e., it must choose the processors to corrupt at the start of the algorithm. Byzantine Agreement is known to be computable in this model in an expected constant number of rounds.We consider a scalable model where in each round each uncorrupted processor can send to any set of log n other processors and listen to any set of log n processors. We define the loss of a computation to be the number of uncorrupted processors whose output does not agree with the output of the majority of uncorrupted processors. We show that if there are t corrupted processors, then any protocol which has probability at least 1/2 +1/log n of loss less than t 2/3<over>32fn1/3log5/3n requires at least f rounds.	adversary (cryptography);algorithm;byzantine fault tolerance;central processing unit;computable function;computation;scalability;synchronization in telecommunications	Dan Holtby;Bruce M. Kapron;Valerie King	2006	Distributed Computing	10.1007/s00446-008-0069-x	computer science;theoretical computer science;distributed computing;upper and lower bounds;algorithm	Theory	16.521654614912954	33.909755677936865	132680
88447fa7d41ec06d3a77e59ed3293d6bd048fd5e	estimating time complexity of rumor spreading in ad-hoc networks		Rumor spreading is a fundamental communication process: given a network topology modeled by a graph and a source node with a message, the goal is to disseminate the source message to all network nodes. In this work we give a new graph-based formula that is a relatively tight estimate of the time complexity of rumor spreading in ad-hoc networks by popular Push&Pull protocol. We demonstrate its accuracy by comparing it to previously considered characteristics, such as graph conductance or vertex expansion, which in some cases are even exponentially worse than our new characterization.	algorithm;average-case complexity;conductance (graph);hoc (programming language);network topology;performance;probabilistic analysis of algorithms;rumor spread in social network;spreading activation;time complexity	Dariusz R. Kowalski;Christopher Thraves	2013		10.1007/978-3-642-39247-4_21	time complexity;conductance;computer network;rumor;distributed computing;node (networking);vertex (geometry);network topology;computer science;dissemination;wireless ad hoc network	Theory	20.425684806860414	35.713718548465756	132828
536c931c3cc9a98f825e9bd204a1963c173a4e9b	live demonstration: cascades. 1: a flow-graph-based symbolic analyzer	libraries;software;symbol manipulation;symbolic network function;mathematics computing;mosfets;transfer functions;cancellation free form;flow graphs;automatic generation;analog transfer function;computational modeling;transfer functions mathematical model libraries mosfets design automation matlab sorting circuit analysis performance analysis electronic circuits;denominator generation cascades 1 flow graph based symbolic analyzer analog transfer function matlab software symbolic network function cancellation free form numerator generation;transfer function;symbol manipulation electronic engineering computing flow graphs mathematics computing;flow graph based symbolic analyzer;electronic engineering computing;cascades 1;denominator generation;numerical models;matlab software;numerator generation;circuit synthesis	A symbolic analyzer based on the use of Coates' flow-graphs is proposed. It allows full automatic generation of analog transfer functions. The program, which is implemented using MATLAB software, automatically computes symbolic transfer functions. It generates the symbolic network functions in a cancellation-free form. Besides, the numerator and denominator of the transfer function are generated separately without any sorting process.	matlab;sorting;transfer function	Mourad Fakhfakh;Mourad Loulou	2010	Proceedings of 2010 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2010.5537003	electronic engineering;computer science;engineering;theoretical computer science;control theory;transfer function;symbolic trajectory evaluation;algorithm	Embedded	21.35578477707224	46.338221628270226	133317
05d43a0a8290768cd0411929544b721c01f0708d	piecemeal graph exploration by a mobile robot (extended abstract)	mobile robot	V1’e study how a mobile robot can piecemeal learn an unknown environment. The robot’s goal is to learn a complete map of its environment, while satisfying the constraint that it must return every so often to its starting position (for refueling, say). The environment is modelled as an arbitrary, undirected graph, which is initially unknown to the robot. We assume that the robot can distinguish vertices and edges that it has already explored. JVe present a surprisingly efficient algorithm for piecemeal learning an unknown undirected graph G = (V, E) in which the robot explores every vertex and edge in the graph by traversing at most O(E + Vlt”( 1) ) edges. This nearly linear algorithm improves on the best previous algorithln, in which the robot traverses at most O(E + V-2) edges. JVe also give an application of piecemeal learning to the problem of searching a graph for a “treasure”. *We gratefully acknowledge support from NSF grant CCR-9310888, ARO grant DAAL03-86-K0171, NSF grant 9z171)41.ASC, Air Force Contract TN DGAFOSR.86.0078, ARPA/Army contract DABT63-93-C-0038, NSF contract 9114440 -CCR, DARPA contract NOOO14-J-9Z.1799, ARPA/ONR contract Nooo I4-92-J-131o, the Siemens Corporation, and a special grant from IBM. The authors can be reached at barucht!blaze. cs. jhu. ectu, margrit@lcs .mit. edu, rivest@theory. lcs .mit. edu, and monat?theory. lcs .mit . edu. t Also at Johns Hopkins University, Baltimore, MD 21218.	algorithm;graph (discrete mathematics);ibm notes;mobile robot;molecular dynamics;twisted nematic field effect;vertex (graph theory)	Baruch Awerbuch;Margrit Betke;Ronald L. Rivest;M T Singh	1995		10.1145/225298.225337	mobile robot;computer vision;simulation;computer science;social robot;mathematics;distributed computing;robot control;mobile robot navigation	Theory	19.339382372359495	33.0271209940104	133628
e413e04eee9e79f0e2e4fcc9efcddb6be14b0fd1	introduction of yield quadrant and yield capability index for vlsi manufacturing	yield		very-large-scale integration	Junichi Hirase	2014	IEICE Transactions		yield;engineering	Robotics	16.933585913184178	45.98334338670065	134195
68793f4bbe8d21290e97007c159400fc5d83947a	a novel vlsi design of dctq processor for fpga implementation	random access memory;image coding;clocks;vlsi circuit optimisation discrete cosine transforms field programmable gate arrays internet of things logic design low power electronics microprocessor chips;fpga;efficient;computer architecture;low power;discrete cosine transforms;adders;frequency 148 192 mhz;dctq;discrete cosine transforms computer architecture adders random access memory clocks image coding field programmable gate arrays;low power dctq fpga efficient;field programmable gate arrays	In this era of Internet of Things, wherein every `thing' is integrated within the existing internet architecture, it becomes quite necessary that embedded computing systems process quickly, occupy less area and consume low power. This would enable them to work quickly with real time data and have a large shelf life. As such there is a need for development of optimized algorithms and their efficient implementation in hardware. This paper presents a novel architecture for obtaining DCTQ coefficients suitable for FPGA Implementation. The design is highly parallel and pipelined so as to exploit the massive parallelism of FPGAs and occupies considerably less area (4,244/9,312) with a very high processing speed (148.192 MHz).	algorithm;application-specific integrated circuit;coefficient;computation;discrete cosine transform;embedded system;field-programmable gate array;internet of things;matrix multiplication;parallel computing;pipeline (computing)	Yogesh M. Jain;Aviraj R. Jadhav;Harish V. Dixit;Akshay S. Hindole;Jithin R. Vadakoott;Devendra Bilaye	2015	2015 19th International Symposium on VLSI Design and Test	10.1109/ISVDAT.2015.7208102	embedded system;computer architecture;electronic engineering;parallel computing;computer hardware;computer science;field-programmable gate array	Arch	11.834043944518639	43.327510057720936	134377
63af01bfedd9e5414be1cb8ea97a43404345af7c	memory management schemes for radiosity computation in complex environments	memory management;storage management;computational geometry;brightness;computational complexity;ordering strategies memory management schemes radiosity computation complex environments hierarchical radiosity computation time memory resources scene partitioning polygon subsets read operations write operations disk transfers radiosity algorithms;hierarchical radiosity;realistic images;memory management environmental management radio spectrum management layout electrical capacitance tomography lighting partitioning algorithms joining processes computational modeling solid modeling;computational geometry storage management brightness computational complexity realistic images;reading and writing	Hierarchical radiosity is a very demanding process in terms of computation time and memory ressources even for scenes of moderate complexity. To handle complex environments, not tting in memory, new solutions have to be devised. One solution is to partition the scene into subsets of polygons (3D cells or clusters) and to maintain in memory only some of them. The radiosity computation is performed only for this resident subset which changes during the resolution process. This change entails many read and write operations from or onto the disk. These disk transfers must be ordered to make the radiosity algorithms tractable. We propose in this paper diierent ordering strategies which can be seen as complementary to those devised by Teller 1]. La radiosit e hierarchique est un processus tr es co^ uteux en terme de temps de calcul et de ressources m emoire, m^ eme pour des sc enes de complexit e moyenne. Pour traiter des environ-nements complexes ne tenant pas en m emoire, de nouvelles solutions sont a envisager. L'une d'entre elle consiste a structurer la sc ene en sous-ensembles de polygones et ne conserver en m emoire qu'une partie d'entre eux. Les calculs de radiosit e sont eeectu es seulement pour ces sous-ensembles r esidant en m emoire et pouvant changer au cours du temps. Ces change-ments provoquent de nombreuses op erations de lecture et d' ecriture sur disque. Cependant, il est possible de r eduire les temps de calculs en ordonnant eecacement ces transferts de donn ees. Nous proposons dans ce rapport dii erentes strat egies d'ordonnancement pouvant ^ etre consid er ees comme compl ementaires a celles d ecrites par Teller dans 1].	algorithm;cobham's thesis;computation;entity–relationship model;les racines du mal;linear algebra;memory management;radiosity (computer graphics);sensitivity index;time complexity	Daniel Méneveaux;Kadi Bouatouch;Eric Maisel	1998		10.1109/CGI.1998.694329	parallel computing;radiosity;computational geometry;computer science;theoretical computer science;operating system;geometry;computational complexity theory;brightness;memory management;computer graphics (images)	Graphics	14.237278855315932	34.182029549234244	134483
db47fa1ace0ed28f11f555ecfcc4190e03850fce	low bandwidth decoder framework for h.264/avc scalable extension	dram access power;random access memory;scalable video coding;decoding;low bandwidth decoder framework;finite impulse response filter;online upsampling;bandwidth decoding automatic voltage control static var compensators scalability spatial resolution video coding hardware random access memory streaming media;scalable video coding decoding;hardware architecture;video coding decoding dram chips;h 264 avc scalable extension;video coding;layer interleaving decoding;svc inter layer prediction;interlayer prediction bandwidth;svc spatial scalability decoding;on the fly;bandwidth;static var compensators;decoding bandwidth;macroblock level on the fly padding;svc quality scalability;large external memory bandwidth;scalability;external memory;interlayer prediction bandwidth low bandwidth decoder framework h 264 avc scalable extension scalable video coding decoding large external memory bandwidth svc inter layer prediction macroblock level on the fly padding online upsampling svc spatial scalability decoding decoding bandwidth svc quality scalability layer interleaving decoding hardware architectures dram access power;dram chips;hardware architectures;hardware;spatial scalability	In the process of scalable video coding (SVC) decoding, large external memory bandwidth is required for SVC inter-layer prediction. In this paper, a low bandwidth decoder framework is proposed for SVC. Two main decoding schemes are developed to reduce the external memory bandwidth. Macroblock-level on-the-fly padding and on-line upsampling is proposed for SVC spatial scalability decoding. This scheme reduces 36% of decoding bandwidth and 34% of processing cycles. For SVC quality scalability, a layer-interleaving decoding scheme is proposed to eliminate all inter-layer prediction bandwidth which is 41–51% of decoding bandwidth. The corresponding hardware architectures of these two decoding schemes are also provided. This low bandwidth framework can save 33–52% of DRAM access power for SVC decoding.	data compression;dynamic random-access memory;forward error correction;h.264/mpeg-4 avc;macroblock;memory bandwidth;online and offline;scalability;scalable video coding;upsampling	Tzu-Der Chuang;Pei-Kuei Tsung;Pin-Chih Lin;Lo-Mei Chang;Tsung-Chuan Ma;Yi-Hau Chen;Liang-Gee Chen	2010	Proceedings of 2010 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2010.5538022	scalable video coding;parallel computing;real-time computing;scalability;computer science;electrical engineering;finite impulse response;hardware architecture;bandwidth;statistics;dynamic bandwidth allocation;computer network	Arch	12.708088876769498	39.793130788629014	134500
cd43fe4bf52798c999ea524c4fdda3a13514603f	a novel cache-based approach to large polygonal mesh simplification	algorithme rapide;quadric error metrics;storage access;large mesh;iterative half edge collapse;generation maille;memoria externa;independent queuing;quadrico;exploracion;cache memory;cache based polygonal mesh simplification;quadrique;antememoria;antememoire;quadrics;polygonal meshes;simplification maille polygonale;fast algorithm;acces memoire;quadric error metric;balayage;acceso memoria;memoire externe;external memory;scanning;mesh generation;algoritmo rapido;out of core algorithms	HUNG-KUANG CHEN, CHIN-SHYURNG FAHN, JEFFREY J. P. TSAI AND MING-BO LIN Department of Electronic Engineering Department of Computer Science and Information Engineering National Taiwan University of Science and Technology Taipei, 106 Taiwan Department of Information and Design Asia University Taichung, 413 Taiwan Department of Computer Science University of Illinois at Chicago Chicago, IL 60637, U.S.A.	algorithm;computer science;electronic engineering;information engineering;iterative method;level of detail;polygon mesh;text simplification	Hung-Kuang Chen;Chin-Shyurng Fahn;Jeffrey J. P. Tsai;Ming-Bo Lin	2006	J. Inf. Sci. Eng.		mesh generation;parallel computing;quadric;cpu cache;computer science;operating system;algorithm	DB	17.117864559251306	37.92233269829525	134875
5b994509dd8e18600ad69c0cbddb8951019d9cc4	radix-16 combined division and square root unit	convergence;digit selection functions;state of the art processor;digit recurrence floating point division square root;hardware multiplexing registers convergence adders computer architecture gold;digit recurrence;division;multiplexing;state of the art processor radix 16 digit recurrence algorithm division square root units digit selection functions low power methods;computer architecture;microprocessor chips floating point arithmetic low power electronics;gold;low power;registers;adders;radix 16;low power electronics;square root;floating point;division square root units;floating point arithmetic;digit recurrence algorithm;low power methods;microprocessor chips;hardware	Division and square root, based on the digit-recurrence algorithm, can be implemented in a combined unit. Several implementations of combined division/square root units have been presented mostly for radices 2 and 4. Here, we present a combined radix-16 unit obtained by overlapping two radix-4 result digit selection functions, as it is normally done for division only units. The latency of the unit is reduced by retiming and low power methods are applied as well. The proposed unit is compared to a radix-4 combined division/square root unit, and to a radix-16 unit, obtained by cascading two radix-4 stages, which is similar to the one implemented in a state-of-the-art processor.	algorithm;division by zero;retiming	Alberto Nannarelli	2011	2011 IEEE 20th Symposium on Computer Arithmetic	10.1109/ARITH.2011.30	arithmetic;parallel computing;computer science;floating point;operating system;mathematics	Arch	12.028271935335281	45.32845836061507	134908
0e8848440fbf492cda04037fb8573c06c49c9f78	fault tolerance of small-world regular and stochastic interconnection networks.		Resilience of the most important properties of stochastic and regular (deterministic) small-world interconnection networks is studied. It is shown that in the broad range of values of the fraction of faulty nodes the networks under consideration possess high fault tolerance, the deterministic networks being slightly better than the stochastic ones.	fault tolerance;interconnection	Andrey Demichev;Viatcheslav A. Ilyin;Alexander Kryukov;Stanislav Polyakov	2013	CoRR		fault tolerance;psychological resilience;computer science;interconnection;distributed computing	Theory	23.503831933043788	35.197918583638625	135715
85f68d166eacae11b9e7000c1b7da56344bbc00e	embedding even-length cycles in a hexagonal honeycomb mesh	parallel algorithm;cycle embedding;interconnection network;hexagonal honeycomb mesh	The existence and construction of cycles of various lengths in an interconnection network are important issues in efficiently executing ring-structured parallel algorithms in such a network. The hexagonal honeycomb mesh (HHM) is regarded as a promising candidate for interconnection networks. In this paper we address the problem of how to embed even-length cycles in an HHM. We prove that an HHM of order t≥3 admits a cycle of length l for each even number l such that l=6 or 10≤l≤6t 2−2. We also describe a systematic method for building these cycles.		Xiaofan Yang;Yuan Yan Tang;Jianqiu Cao;Qing Lu	2008	Int. J. Comput. Math.	10.1080/00207160701421144	combinatorics;topology;mathematics;distributed computing;parallel algorithm	Theory	23.609461626190328	35.92991360134251	135753
faa7cf5d658e1efe49a1d343dba8dba4cf122fd8	efficient stack filter implementations of rank order filters	rank order filters;feedforward;latches;bit-serial data;stack filter implementations;word size;window size;computational complexity;rank order;digital filters;feed-forward paths;pipelining;positive boolean function;nibble-serial data;computational overlap;boolean functions;bit-parallel data;nonlinear filters;pipeline processing;computer architecture;feed forward;hardware	Efficient stack filter implementations of rank order filters for bit-parallel, bit-serial, and nibble-serial data are presented. The implementations support variations in word size, window size and rank order. All the implementations are sampled at a high rate by efficient incorporation of pipelining. While the bit-parallel realizations are pipelined by adding latches in the feed-forward paths, the recursive bit-serial and nibble-serial realizations are pipelined to p levels by increasing the number of positive Boolean function (PBF) units by a factor of 2p-1. The computational complexity of the bit-parallel realizations is reduced by processing a block of outputs, thereby exploiting the computational overlap between consecutive windows		Chaitali Chakrabarti	1993			electronic engineering;parallel computing;computer science;theoretical computer science;feed forward;algorithm	ML	12.884510496494848	45.62484013586391	135941
1fd7742fe18fdc8d05a8ff0da676e8d9c9aa4f70	automatic fault detection in combinational switching networks	logic element;feedback loop;fault detection;lexicographic order;normal form;switching network	"""b i n a t i o n a l s w i t c h i n g n e t w o r k s which c o n t a i n s u f f i c i e n t r e d u n d a n c y s o t h a t t h e p r e s e n c e o f any s i n g l e f a u l t can b e d e t e c t e d. I t i s w e l l known t h a t (w i t h i n b r o a d l i m i t s) any i s o l a t e d f a u l t i n a s i n g l e-o u t p u t n e t w o r k may b e d e t e c t e d w i t h a b o u t 2 : 1 r e d u n d a n c y , t h r o u g h d u p l i c a t i o n o f t h e i r r e d u n d a n t n e t w o r k , and t h e a d d i t i o n o f a c o m p a r a t o r g a t e. t o b e a t a l l l i k e l y t o o c c u r , r a t h e r t h a n """" a l l """" f a u l t s c a t e g o r i c a l l y ; (4) w h e r e p o s s i b l e , a l l o w i n g some f a u l t s t o b e d e t e c t e d w i t h a d e l a y , r a t h e r t h a n a t t h e f i r s t o c …"""	artificial intelligence;combinational logic;fault detection and isolation	William H. Kautz	1961		10.1109/FOCS.1961.8	combinatorics;discrete mathematics;theoretical computer science;lexicographical order;feedback loop;mathematics;fault detection and isolation;algorithm;statistics;algebra	AI	19.867984371501265	45.01514339246833	135996
dba9851edbeb8caea9c454236aed4c09ae1147f9	a cmos image sensor with analog pre-processing capability suitable for smart camera applications	current 55 3 mua cmos image sensor analog pre processing capability smart camera application analog image processing operation spatial nxn pixel matrix pixel wiring row decoder column multiplexer buffer stage analog pre processing circuits averaging filter;multiplexing equipment analogue processing circuits cameras cmos image sensors decoding;decoding image sensors layout wiring multiplexing metals artificial intelligence;analog processing circuits;cmos image sensors;filtering algorithms;grk 1773 smart cameras cmos image sensors analog processing circuits filtering algorithms;smart cameras;grk 1773	This work presents a novel CMOS sensor which is suitable for analog image processing operations and describes the benefits of analog pre-processing over digital algorithms. All needed modifications to read out a spatial NxN pixel matrix simultaneously are presented regarding pixel wiring, row decoder and column multiplexer as well as descrambling the order of the outputs. The outputs are connected via a buffer stage to the analog pre-processing circuits. As an example for such an analog computation, an averaging filter is described from design constraints to the actual implementation. Simulation results show a very low current consumption of just 55.3 μA under worst-case conditions and a computation time below 200 ns. This corresponds to a power-time product ratio of 1:18.75 compared to a digital state-of-the-art implementation.	3d film;algorithm;analog computer;analog image processing;arithmetic logic unit;best, worst and average case;cmos;computation;image sensor;kalman filter;memristor;multiplexer;preprocessor;simulation;smart camera;throughput;time complexity;wiring	Christopher Soell;Lan Shi;Andreas Baenisch;Thomas Ussmüller;Robert Weigel	2015	2015 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS)	10.1109/ISPACS.2015.7432780	analog device;embedded system;electronic engineering;analog image processing;computer hardware;image processing;computer science;digital image processing;image sensor;cmos sensor;analog multiplier	EDA	14.617388381275664	39.79725974967691	136112
3063185dd8b412a537f2fc1d88c96c0a0374d2a8	efficient vlsi implementation of n/n integer division	truncated array implementation;very large scale integration;logic;nonrestoring shift subtract algorithm;integrated circuit design vlsi dividing circuits delays digital arithmetic digital signal processing chips;digital signal processing algorithms;integrated circuit design;difference equations;efficient implementation;registers;adders;propagation delay;dividing circuits;vlsi;digital signal processing chips;digital arithmetic;circuits;very large scale integration logic adders registers difference equations propagation delay circuits;vlsi implementation;combinational divider;digital signal processing algorithms integer division vlsi implementation combinational divider nonrestoring shift subtract algorithm truncated array implementation;integer division;delays	The paper presents an efficient implementation of an N-bit by N-bit combinational divider using the nonrestoring shift-subtract algorithm. A theoretical speed-up of 50% for an N/N-bit divider, compared to a merely truncated array implementation, is demonstrated. In practice, delay reduction of 25%-35% can easily be achieved with very little area overhead.	algorithm;combinational logic;overhead (computing);very-large-scale integration	Kei-Yong Khoo;Alan N. Willson	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1464677	electronic engineering;parallel computing;computer science;engineering;electrical engineering;theoretical computer science;very-large-scale integration	Arch	13.067680606988265	44.903658835280496	136224
2b6467eda33b2acbbc021f2d5ac7ec6d1881c2b6	a pipelined architecture for intra pu encoding in hevc	pipelining		high efficiency video coding;ibm systems network architecture	Yunpyo Hong;Juwon Byun;Youngjo Kim;Jaeseok Kim	2014	IEICE Transactions		real-time computing;computer science;theoretical computer science;pipeline	Arch	12.335341882842632	39.951867092469755	136240
d8375d7be19a5d52516366d0d704a146a70c6e9c	design of mos networks in single-rail input logic for incompletely specified functions	reseau logique;intelligent networks logic design logic gates lattices energy consumption mosfets logic functions algorithm design and analysis testing bipolar transistors;logic design;circuito logico;algorithme;algorithm;algorritmo;field effect integrated circuits;conception logique;circuit logique;irredundant connections minimum negative gates mos networks single rail input logic incompletely specified functions logic gate dimn;logic design field effect integrated circuits integrated logic circuits;mos transistor;integrated logic circuits;concepcion logica;logic circuit;logic gate;transistor mos;red logica;logic array	A logic gate in a logic network of MOS transistors can express a negative function, a logic function that can be expressed as the complement of a disjunctive form of only noncomplemented variables, such as x, V xzxi. So it is called a negative gate. An algorithm, called DIMN, for the design of a MOS logic network with a minimum number of negative gates and irredundant connections among negative gates for a completely specified function was published by the authors in 1985 [4]. Here, irredundant connections means that any interconiicction cannot be removed without changing the network outputs. Such networks have the least power consumption (assuming that each gate has roughly equal power consumption) and unique testability pruperties. This paper discusses an extension of Algorithm DIMN to the case of an incompletely specified function along with an example.	algorithm;boolean algebra;disjunctive normal form;logic gate;software testability;transistor	Hung Chi Lai;Saburo Muroga	1988	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.3167	and-or-invert;electronic engineering;logic optimization;logic gate;three-input universal logic gate;computer science;engineering;theoretical computer science;pass transistor logic;mathematics;sequential logic;algorithm	EDA	19.412008324656508	45.38770289137965	136299
43446cb16b3a5532012f72aadee5e2508c3e3db5	edge-disjoint spanning trees for the generalized butterfly networks and their applications	tolerancia falta;distributed system;systeme reparti;spanning trees;butterfly network;fault tolerant;point to point;arbre maximal;radiodifusion;butterfly networks;reseau papillon;broadcast;arete disjointe;interconnection network;red mariposa;fault tolerant system;sistema repartido;arbol maximo;disjoint edge;fault tolerance;sistema tolerando faltas;interconnection networks;butterfly networks spanning trees;systeme tolerant les pannes;spanning tree;broadcasting;algoritmo optimo;algorithme optimal;arista disyuntiva;optimal algorithm;radiodiffusion;tolerance faute;red interconexion;edge disjoint trees;reseau interconnexion	The generalized butterfly GBN(d, n) has recently gained some interest as a point-to-point interconnection network rather than the well known multi-stage butterfly networks. We construct edges-disjoint spanning trees (abbreviated EDSTs) for the GBN(d, n). Our construction is based on the decomposition of the GBN(d, n) into dn vertex-disjoint cycles of length n. As an application, we propose an efficient broadcasting algorithm and its fault-tolerant version for the GBN(d, n). Our fault-tolerant broadcasting algorithm is optimal in terms of fault-tolerance, because it resists 2d−1 edge failures (2d is the degree of the GBN(d, n)). We also propose an efficient scattering algorithm and its fault-tolerant version which resists 2d − 3 edge faults. © 2005 Elsevier Inc. All rights reserved.	algorithm;fault tolerance;fibre channel point-to-point;file spanning;interconnection;whole earth 'lectronic link	Abderezak Touzene;Khaled Day;Burkhard Monien	2005	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2005.05.009	fault tolerance;combinatorics;computer science;mathematics;distributed computing;algorithm	Theory	23.639908991610238	34.615495825972104	136549
2419783bfab860f35040f27f5d6033f9462c19ac	the hough transform has o(n) complexity on n x n mesh connected computers	algoritmo paralelo;parallel algorithm;transformacion radon;algorithm complexity;image processing;radon transform;multiprocessor;complejidad algoritmo;68q80;sistema informatico;radon transformation;procesamiento imagen;68q20;computer system;transformacion hough;traitement image;algorithme parallele;complexite algorithme;68u10;transformation radon;hough transformation;systeme informatique;hough transform;transformation hough;mesh array computers;68q35;multiprocesador;parallel algorithms;multiprocesseur	This paper presents algorithms for implementing an important image processing operation, the Hough transform, on a mesh connected computer (MCC). The MCC consists of an $N \times N$ array of processors, each of which holds a single pixel of the image. The MCC operates in a Single Instruction Stream, Multiple Data Stream (SIMD) mode, which is in agreement with the hardware constraints found in existing meshes. Five algorithms for computing the Hough transform are presented. These algorithms use a number of different techniques, and they have varying time complexities and architectural requirements. The most notable algorithm presented computes any P angles of the Hough transform in $O(N + P)$ time and uses only a constant amount of memory per processor. Because the Hough transform is a particular case of the discrete Radon transform, all of the algorithms will be presented for computing the Radon transform of gray-level images.	complexity;hough transform	Robert Cypher;Jorge L. C. Sanz;L. Snyder	1990	SIAM J. Comput.	10.1137/0219056	hough transform;image processing;computer science;theoretical computer science;mathematics;parallel algorithm;algorithm;computer graphics (images)	Theory	11.723623716391778	35.31610806155239	136624
233f70ca59803fed202ad608e3de650eed9fd9b8	fpga implementation of fast radix 4 division algorithm	cad;digital arithmetic;field programmable gate arrays;fpga-advantage cad tool;xilinx technology;arithmetic intensive application;fast division;field programmable gate arrays;lookup table;quotient selection;radix 4 division	The flexibility of field programmable gate arrays (FPGAs) can provide arithmetic intensive applications with the benefits of custom hardware but without the high cost of custom silicon implementations. In this paper, we present the adaptation of a fast radix 4 division algorithm (Srinivas and Parthi, 1994) for lookup table based FPGAs implementation. In this algorithm, the quotient digits are determined by observing three most-significant radix 2 digits of the partial remainder and independent of the divisor. The implementation has been done with Xilinx technology and FPGA-Advantage CAD tools.	computer-aided design;division algorithm;field-programmable gate array;lookup table;radix point	Attif A. Ibrahem;Hamed Elsimary;Aly E. Salama	2004	4th IEEE International Workshop on System-on-Chip for Real-Time Applications	10.1109/IWSOC.2004.1319852	electronic engineering;parallel computing;computer hardware;computer science	EDA	11.481981886822474	44.57146021575077	136822
c229b8a48fb31a02c745140ab8c7dee308856485	efficient implementations of a class of ± 2b parallel computations on a simd hypercube	generic algorithm;parallel algorithms computational complexity;iterations;descend algorithm;efficient implementation;computational complexity;parallel computer;concurrent computing hypercubes parallel algorithms computational intelligence society routing;batcher odd even merge algorithm pm2b descend parallel computations simd hypercube iterations descend algorithm;parallel computations;pm2b descend;simd hypercube;batcher odd even merge algorithm;parallel algorithms	The authors identify an important class of parallel computations, called +or-2/sup b/-descend, with an efficient implementation on a hypercube. Given the input A(0:N-1), a computation in this class consists of log N iterations. Iteration b, b=log N-1, . . ., 0, computes the new value of each A(i) as a function of A(i), A(i+2/sup b/) and A(i-2/sup b/). They obtain a general algorithm for implementing any computation in this class in O(log N) time on a SIMD hypercube. Their general descend algorithm results in an efficient O(log N) implementation of the Batcher odd-even merge algorithm on a hypercube. The best previously known implementation of odd-even merge on a SIMD hypercube requires O(log/sup 2/ N) time. >	simd	David Nassimi;Yuh-Dong Tsai	1991		10.1109/IPPS.1991.153749	parallel computing;computer science;theoretical computer science;distributed computing	HPC	14.554251092519614	33.0885858722396	136836
031a4be5b24da80263ecf7c92da017dc64dbbfed	an efficient systolic array for the discrete cosine transform based on prime-factor decomposition	very large scale integration;systolic arrays;speech processing;array signal processing systolic arrays discrete cosine transforms vlsi;systolic array;array signal processing;vlsi implementation systolic array discrete cosine transform prime factor decomposition index mappings;discrete cosine transform;2 dimensional;prime factor decomposition;computer architecture;index mappings;discrete transforms;systolic arrays discrete cosine transforms computer architecture signal processing algorithms very large scale integration array signal processing throughput delay discrete transforms speech processing;discrete cosine transforms;indexation;vlsi;vlsi implementation;signal processing algorithms;computational efficiency;throughput	A new design of a systolic array for computing the discrete cosine transform (DCT) based on prime-factor decomposition is presented. The basic principle of the proposed systolic array is that one-dimensional (1-D) DCT can be decomposed to a 2-dimensional (2-D) DCT by input and output index mappings and the 2-D DCT is computed efficiently on a 2-D systolic array. We modify Lee's input index mapping method in order to construct one input mapping table instead of three input index mapping tables. The proposed systolic array avoids the need for the array transposer that was required by earlier implementations for the prime-factor DCT algorithms, and thus all processing can be pipelined. The proposed design of systolic array provides a simple and regular structure, which is well suited for VLSI implementation.	discrete cosine transform;systolic array	Hyesook Lim;Earl E. Swartzlander	1995		10.1109/ICCD.1995.528936	electronic engineering;parallel computing;computer science;theoretical computer science;speech processing;very-large-scale integration	Logic	12.230548925162344	43.71494743735571	137485
bebc070e094dfd7fc237a5e5084f03af3d1dbdf8	on equivalent systolic designs of lu decomposition and its algebraic representation (short note)	red sistolica;note;concepcion circuito;fonction generatrice;circuit design;circuit vlsi;procesador panel;array processor;methode algebrique;processeur tableau;vlsi circuit;systolic network;algebraic method;funcion generatriz;reseau systolique;generating function;conception circuit;metodo algebraico;circuito vlsi	Algorithms which are to be mapped onto interconnecting processing elements in order to design a systolic array are conventionally represented by graphs or networks. This paper introduces the concept of algebraic representation and used a generationg function to represent a systolic array	lu decomposition	Young-Chang Hou;Jong-Chuang Tsay	1992	Comput. J.	10.1093/comjnl/35.6.662	generating function;discrete mathematics;computer science;circuit design;mathematics;algorithm	Theory	16.67143965908291	40.2889046308014	137879
41d15336a7169166801bf0bb08b8b1cf4d1f667f	analysis of graphs for digital preservation suitability	random graph;digital library;small world;resilience;digital preservation;robustness;power law	We investigate the use of autonomically created small-world graphs as a framework for the long term storage of digital objects on the Web in a potentially hostile environment. We attack the classic Erdos --- Renyi random, Barabási and Albert power law, Watts --- Strogatz small world and our Unsupervise. Small World (USW) graphs using different attacker strategies and report their respective robustness. Using different attacker profiles, we construct a game where the attacker is allowed to use a strategy of his choice to remove a percentage of each graph's elements. The graph is then allowed to repair some portion of its self. We report on the number of alternating attack and repair turns until either the graph is disconnected, or the game exceeds the number of permitted turns. Based on our analysis, an attack strategy that focuses on removing the vertices with the highest betweenness value is most advantageous to the attacker. Power law graphs can become disconnected with the removal of a single edge; random graphs with the removal of as few as 1% of their vertices, small-world graphs with the removal of 14% vertices, and USW with the removal of 17% vertices. Watts --- Strogatz small-world graphs are more robust and resilient than random or power law graphs. USW graphs are more robust and resilient than small world graphs. A graph of USW connected WOs filled with date could outlive the individuals and institutions that created the data in an environment where WOs are lost due to random failures or directed attacks.	barabási–albert model;betweenness;graph (discrete mathematics);random graph;vertex (geometry);watts humphrey;world wide web	Charles L. Cartledge;Michael L. Nelson	2010		10.1145/1810617.1810637	random regular graph;random graph;power law;digital library;independent set;graph product;computer science;theoretical computer science;hopcroft–karp algorithm;modular decomposition;treewidth;world wide web;chordal graph;indifference graph;robustness	Theory	23.624464580455548	38.00558815922216	137933
35635c83e1fea6c7a58732f133d98a32b230b990	a residue number system on reconfigurable mesh with applications to prefix sums and approximate string matching	prefix sums;moduli;approximate string matching;conversion techniques;reconfigurable mesh architecture;concurrent computing;linear reconfigurable mesh;reconfigurable architectures;very large scale integration;number representation;residue number systems;string matching with k mismatches;reconfigurable mesh;time delay;computer architecture;switches broadcasting concurrent computing computer architecture delay very large scale integration partitioning algorithms;residue number systems string matching reconfigurable architectures;number representations;time use;parallel computer;vlsi;prime numbers;residue number system;prime number;broadcasting;string matching;conversion techniques residue number system reconfigurable mesh prefix sums approximate string matching number representations prime numbers moduli reconfigurable mesh architecture linear reconfigurable mesh unit time delay;switches;unit time delay;partitioning algorithms	ÐSeveral new number representations based on a Residue Number System are presented which use the smallest prime numbers as moduli and are suited for parallel computations on a reconfigurable mesh architecture. The bit model of linear reconfigurable mesh with exclusive write and unit-time delay for broadcasting on a subbus is assumed. It is shown how to convert in O1 time any integer, ranging between 0 and nÿ 1, from any commonly used representation to any new representation proposed in this paper (and vice versa) using an n O log n log logn reconfigurable mesh. In particular, some of the previously known conversion techniques are improved. Moreover, as a byproduct, it is shown how to compute in O1 time the Prefix Sums of n bits by a reconfigurable mesh having the above mentioned size, thus improving previously known results. Applications to the Prefix Sums of N h-bit integers and to Approximate String Matching with mismatches are also considered. The Summation and the Prefix Sums can be computed in O1 time using O h logN  log N log logN Nh and O h2log2 N loghlogN ONh logN reconfigurable meshes, respectively. Moreover, it is shown for the first time how to find in O1 time all the occurrences of a pattern of length m in a text of length n, allowing less than mismatches, using a reconfigurable mesh of size Om log j j O n log j j  log log log , where the pattern and the text are strings over a finite alphabet and < m n. Index TermsÐNumber representation, prefix sums, reconfigurable mesh, residue number system, string matching with k mismatches,	approximate string matching;binary file;binary logarithm;broadcast delay;computation;computational problem;prefix sum;reconfigurable computing;residue number system;string searching algorithm;time complexity	Alan A. Bertossi;Alessandro Mei	2000	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.888638	parallel computing;real-time computing;concurrent computing;computer science;theoretical computer science;distributed computing;very-large-scale integration;programming language;prime number	Theory	13.066383768140017	33.40898569302243	138171
93a44575181346d6874234ae59603512f5fd484a	reference frame data compression method for h.264/avc	image processing;data compression;h 264 avc;reference frame;vlsi architecture	In this paper a novel reference frame data compression method for the H.264/AVC is proposed. Proposed method introduce a new function including the DAP (Differential of Adjacent Pixel) method and Huffman coding tool between the frame memory and the image coding engine. Simulation results show that this method could achieve not only a stable compression rate but also realize a lossless compression without additional distortion to the coding process. Furthermore, proposed method will not induce further delay during the memory access period. Proposed method is hardware-friendly and could be implemented by only about 7,000 gates.	data compression;distortion;fifo (computing and electronics);h.264/mpeg-4 avc;huffman coding;icl distributed array processor;institute of electronics, information and communication engineers;lossless compression;macroblock;pixel;random access;reference frame (video);simulation;very-large-scale integration	Tian Song;Takashi Shimamoto	2007	IEICE Electronic Express	10.1587/elex.4.121	data compression;reference frame;lossy compression;residual frame;data compression ratio;electronic engineering;real-time computing;image processing;image compression;computer science;theoretical computer science;context-adaptive variable-length coding;lossless compression;context-adaptive binary arithmetic coding	Graphics	12.648043443650497	39.85337844213521	138330
3be949bc3906c808d24ab7171b6b3a9afe7a6d13	terminating distributed construction of shapes and patterns in a fair solution of automata	distributed network construction;programmable matter;shape formation;well-mixed solution;homogeneous population;distributed protocol;interacting automata;fairness;random schedule;structure formation;self-organization;self-replication	In this work, we consider a solution of automata (or nodes) that move passively in a well-mixed solution without being capable of controlling their movement. Nodes can cooperate by interacting in pairs and every such interaction may result in an update of their local states. Additionally, the nodes may also choose to connect to each other in order to start forming some required structure. Such nodes can be thought of as small programmable pieces of matter, like tiny nanorobots or programmable molecules. The model that we introduce here is a more applied version of network constructors, imposing physical (or geometric) constraints on the connections that the nodes are allowed to form. Each node can connect to other nodes only via a very limited number of local ports. Connections are always made at unit distance and are perpendicular to connections of neighboring ports, which makes the model capable of forming 2D or 3D shapes. We provide direct constructors for some basic shape construction problems, like spanning line, spanning square, and self-replication. We then develop new techniques for determining the computational and constructive capabilities of our model. One of the main novelties of our approach is that of exploiting the assumptions that the system is well-mixed and has a unique leader, in order to give terminating protocols that are correct with high probability. This allows us to develop terminating subroutines that can be sequentially composed to form larger modular protocols. One of our main results is a terminating protocol counting the size n of the system with high probability. We then use this protocol as a subroutine in order to develop our universal constructors, establishing that it is possible for the nodes to become self-organized with high probability into arbitrarily complex shapes while still detecting termination of the construction.	automata theory;automaton;communications protocol;computation;connected component (graph theory);correctness (computer science);divergence (computer science);experiment;fastest;file spanning;interaction;interaction design pattern;java;nanorobotics;newman's lemma;numerical integration;podc;refinement (computing);robustness (computer science);scheduling (computing);self-assembly;self-organization;self-replication;sensor;subroutine;verification and validation;with high probability	Othon Michail	2017	Distributed Computing	10.1007/s00446-017-0309-z	programmable matter;parallel computing;self-organization;simulation;computer science;distributed computing;algorithm;population	Theory	17.351178661481832	34.823803891430074	138493
1b8d41a9ec69605d93cdf8c79c79f155a7ec440c	single chip mpeg2 decoder with integrated transport decoder for set-top box	standards;decoding;digital signal processing chips decoding standards asynchronous transfer mode;service utilization;single chip mpeg2 decoder;digital video disk;transport layer;atm networks;mpeg2 standard;chip;decoding satellite broadcasting cables decision support systems digital video broadcasting europe costs microprocessors operating systems electronic switching systems;digital intelligence;integrated transport decoder;operating system;digital signal processing chips;videocore ess mvd chip;digital video;highly integrated mpeg2 decoder;direct broadcast satellites;videocore ess mvd chip single chip mpeg2 decoder integrated transport decoder set top box mpeg2 standard transport layers direct broadcast satellites atm network digital video disk digital intelligence highly integrated mpeg2 decoder;set top box;atm network;asynchronous transfer mode;transport layers	In the last few years, the MPEG2 standard [ I ] has been defined to include system layer and transport layers for entertainment applications with deliveries via direct broadcast satellites (DBS), ATM network, cables, as well as storage media such as the Digital Video Disk. Several DBS services utilizing this standard have been in operation, for example, DSS from Thomson and DVB in Europe. Trials on Cable andATM networks are also on-going. The emergence of set-top boxes with digital intelligence spurs the need for a highly integrated MPEG2 decoder that not only is low cost but also isprogrammable with apower-1 32-bit microprocessor. Since set-top boxes have to operate with some kind of small operating system, the VideoCorelESS MVD chip is designed to meet the above requirements. EPROM, SRAM + Introduction The MVD chip consists of a 32-bit microprocessor, a four-unit block of microcoded datapath compute engine with SIMD architecture, avideo output display processor with programmable scaler, avariable Length decoder unit and a programmable transport unit decoder. Figure 1 shows the block diagram of the MVD chip. The system application of the MVD chip can be targeted toward a set-top box. Figure 2 shows that with a 16 Mbit DRAM, programmable EPROM, optional SRAMs, NTSCPAL video encoder DAC and an audio DAC, an MPEG2-based decoder subsystem can be built without an extemal microcontroller present. 32-bit Microprocessor 64-bit Unit Processor l l l	32-bit;64-bit computing;atm turbo;arithmetic logic unit;cable modem;datapath;diagram;digital video broadcasting;direct-broadcast satellite;dynamic random-access memory;eprom;emergence;encoder;google compute engine;mpeg-2;megabit;microcontroller;microprocessor;operating system;requirement;simd;set-top box;static random-access memory	Jan Fandrianto	1996		10.1109/CMPCON.1996.501813	embedded system;electronic engineering;telecommunications;computer science	Arch	10.77304477606062	40.98129465034874	138588
ec72fd6366fe950a50acd626224bde3f54481728	polynomial datapath synthesis and optimization based on vanishing polynomial over z2m and algebraic techniques	digital signal processing;optimisation;kernel;polynomials high level synthesis optimisation;computer graphics;algebraic techniques;embedded systems applications;polynomials;polynomials optimization kernel delay hardware digital signal processing algebra;polynomial datapath;high level synthesis;polynomial datapath optimization;polynomial computations;vanishing polynomial addition;algebra;vanishing polynomial deletion polynomial datapath synthesis polynomial datapath optimization z 2 m algebraic techniques digital signal processing dsp computer graphics embedded systems applications polynomial computations high level synthesis vanishing polynomial addition;vanishing polynomial deletion;polynomial datapath synthesis;optimization;modular optimization;finite ring algebra;z 2 m;dsp;polynomial datapath high level synthesis finite ring algebra modular optimization;hardware	The growing market for Digital Signal Processing (DSP), Computer graphics and embedded systems applications that can be modeled as polynomial computations in their datapath designs, requires improvements in high-level synthesis and optimization techniques for such systems. This paper concentrates on how to find common sub-expressions between s given polynomial functions over Z2n1 × Z2n2 × ... × Z2nd to Z2m in order to optimize the area and delay as much as possible. Our main contributions in this paper is proposing an optimization method based on adding/deleting vanishing polynomials over Z2m, i.e., those polynomials that are equivalent to zero over Z2m, to/from given polynomial functions in the hope of achieving further common sub-expressions. After applying our optimization techniques, experimental comparisons with the state-of-the-art techniques show an average improvement in the area by 36.80% with an average delay decrease of 2.41%. Regarding the comparison with our previous works, the area and delay are improved by 21.4% and 8.7% respectively.	computation;computer graphics;datapath;digital signal processing;embedded system;high- and low-level;high-level synthesis;linear algebra;mathematical optimization;polynomial	Samaneh Ghandali;Bijan Alizadeh;Zainalabedin Navabi;Masahiro Fujita	2012	Tenth ACM/IEEE International Conference on Formal Methods and Models for Codesign (MEMCODE2012)	10.1109/MEMCOD.2012.6292301	computer science;theoretical computer science;digital signal processing	EDA	13.835658550780671	44.11921121902605	138621
bbf06f94229db94865e71b657a2d2fd7ea668a54	mobile agent rendezvous in a ring using faulty tokens	fault tolerant;mobile agent	We consider the rendezvous problem which requires k mobile agents that are dispersed in a ring of size n, to gather at a single node of the network. The problem is difficult to solve when the agents are identical (i.e. indistinguishable), they execute the same deterministic algorithm, and the nodes of the ring are unlabelled (i.e. anonymous). In this case, rendezvous can be achieved by having each agent mark its starting location in the ring using a token. This paper focusses on fault tolerant solutions to the problem when tokens left by an agent may fail unexpectedly. Previous solutions to the problem had several limitations--they either assumed a completely synchronous setting or were restricted to few specific instances of the problem where the value of n is such that gcd(n, k′) = 1 ∀k′ ≤ k. We improve on these results, solving rendezvous in asynchronous rings for arbitrary values of n and k, whenever it is solvable.	mobile agent	Shantanu Das	2008		10.1007/978-3-540-77444-0_29	fault tolerance;real-time computing;computer science;mobile agent;distributed computing;algorithm	NLP	17.591908207358642	34.40185220217718	139089
4af427a48ca1728850b3fbe388cda0c9d401517a	hyper-hamiltonian laceability of balanced hypercubes	interconnection network;hamiltonian laceability;hyper hamiltonian laceability;balanced hypercube	The balanced hypercube, proposed by Wu and Huang, is a new variation of hypercube. The particular property of the balanced hypercube is that each processor has a backup processor that shares the same neighborhood. A Hamiltonian bipartite graph with bipartition $$V_{0}\cup V_{1}$$ V 0 ∪ V 1 is said to be Hamiltonian laceable if there is a Hamiltonian path between any two vertices $$x\in V_{0}$$ x ∈ V 0 and $$y\in V_{1}$$ y ∈ V 1 . A graph $$G$$ G is hyper-Hamiltonian laceable if it is Hamiltonian laceable and, for any vertex $$v\in V_{i}$$ v ∈ V i , $$i\in \{0,1\}$$ i ∈ { 0 , 1 } , there is a Hamiltonian path in G–v between any pair of vertices in $$V_{1-i}$$ V 1 - i . In this paper, we mainly prove that the balanced hypercube is hyper-Hamiltonian laceable.	backup;grid network;hamiltonian (quantum mechanics);hamiltonian path;parallel computing;vertex (geometry)	Huazhong Lü;Heping Zhang	2013	The Journal of Supercomputing	10.1007/s11227-013-1040-6	hamiltonian (quantum mechanics);computer science;vertex (geometry);distributed computing;bipartite graph;hamiltonian path;topology;graph;hypercube	ML	23.63256550574414	35.387692711518575	139356
f3b8f79a12a13489da5013a7c1b37dbc6b06de7e	low-pass filter based vlsi oriented variable block size motion estimation algorithm for h.264	image sampling;data reuse;reference frame;motion estimation;low pass filter;variable block size;chip;video coding;fast motion estimation;low pass filters very large scale integration motion estimation hardware partitioning algorithms parallel processing logic design concurrent computing computational efficiency random access memory;data dependence;computational complexity;macro block;motion vector;vlsi computational complexity image sampling low pass filters motion estimation video coding;on chip sram haar low pass filter vlsi oriented variable block size motion estimation algorithm h 264 fast motion estimation algorithm parallel vlsi hardware implementation computation complexity modified motion vector prediction macro block parallel processing integer pixel accuracy adaptive sub search window scheme reference frame data reusing memory transfer external ram;vlsi;low pass filters;hardware implementation;parallel processing	"""In this paper, a fast motion estimation algorithm, which is friendly to VLSI hardware implementation is proposed. This algorithm has such features: First, through """"Haar"""" low-pass filter based subsampling, the computation complexity at each search position is reduced to about 25% of the original algorithm; Second, one modified motion vector prediction is provided to eliminate the data dependence among sub-partitions in the same macro block (MB). Based on this approach, parallel processing for variable block size motion estimation (VBSME) with integer pixel accuracy can be realized; Third, one """"adaptive sub-search window"""" scheme is proposed to further reduce computation cost and it also can facilitate reference frame data reusing to reduce memory transfer from the external RAM to the on-chip SRAM. The proposed VBSME algorithm is very suitable for parallel VLSI implementation"""	algorithm;block size (cryptography);chroma subsampling;computation;data dependency;h.264/mpeg-4 avc;haar wavelet;low-pass filter;motion estimation;parallel computing;pixel;reference frame (video);static random-access memory;very-large-scale integration	Zhenyu Liu;Yang Song;Takeshi Ikenaga;Satoshi Goto	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1660327	parallel processing;computer vision;parallel computing;low-pass filter;computer science;theoretical computer science	Robotics	12.566184090725617	40.559946423619216	139485
1a4e91cdf61ca2ac969d1a54c52decf23767ff19	solving equations in abstract algebras: a rule-based survey of unification	rule based	A n-ary flip-flop is constructed of a series of binary cells interconnected by two rows of logic gates. One row of logic gates provides a signal to a selected cell when all cells to the left of the selected cell are reset and the other row of logic gates provides a signal to the selected cell when all cells to its right are in the reset state. The logic gate signals hold the selected cell in the on state after the original SET signal has decayed. When a different cell is placed in the set state, one row of logic gates propagates a signal to the right of that cell to reset all gates to the right while the other row concurrently propagates a signal in the other direction to reset all gates to the left. The n-ary flip-flop is modularly expandable inasmuch as cells can be readily added to both ends of the series of cells merely by connecting each added cell to the two rows of logic gates.	han unification	Jean-Pierre Jouannaud;Claude Kirchner	1991			equation solving;rule-based system;logic gate;row;algebra;binary number;mathematics;unification	Logic	20.32769240207349	43.14988200404657	139551
9cb6efa5bcf8a89d048d02ca67df61a55661cbac	toeplitz matrix approach for binary field multiplication using quadrinomials	field programmable gate array;quadrinomials;binary field;complexity theory;multiplying circuits;complexity theory polynomials logic gates delay parallel architectures galois fields;fpga;field programmable gate array fpga;polynomials;double basis;finite field;subquadratic complexity;parallel architectures;logic gates;irreducible polynomial;space complexity;field programmable gate array implementation binary field multiplication quadrinomials subquadratic space complexity multiplier irreducible trinomial irreducible polynomials modulo a quadrinomial toeplitz matrix vector product parallel multiplier two bit digit every clock cycle;digital arithmetic;matrix multiplication;polynomials digital arithmetic field programmable gate arrays matrix multiplication multiplying circuits;parallel architecture;field programmable gate arrays;toeplitz matrix;multiplication;galois field;logic gate;subquadratic complexity binary field double basis field programmable gate array fpga finite field multiplication quadrinomials;galois fields	In the recent past, subquadratic space complexity multipliers have been proposed for binary fields defined by irreducible trinomials and some specific pentanomials. For such multipliers, alternative irreducible polynomials can also be used, in particular, nearly all one polynomials (NAOPs) seem to be better than pentanomials. For improved efficiency, multiplication modulo an NAOP is performed via modulo a quadrinomial whose degree is one more than that of the original NAOP. In this paper, we present a Toeplitz matrix-vector product based approach for multiplication modulo a quadrinomial. We obtain a fully parallel multiplier with a subquadratic space complexity. The Toeplitz matrix-vector product-based approach is also interesting in the design of sequential multipliers. We present two such multipliers that process a two-bit digit every clock cycle. Field-programmable gate-array implementations of the proposed sequential as well as fully parallel multipliers for the field size of 163 are also presented.	clock signal;cryptography;dspace;exclusive or;field-programmable gate array;irreducibility;irreducible polynomial;lambda calculus;matrix multiplication;modulo operation;recursion;toeplitz hash algorithm;trinomial	M. Anwar Hasan;Ashkan Hosseinzadeh Namin;Christophe Nègre	2012	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2011.2106524	discrete mathematics;logic gate;pure mathematics;mathematics;finite field;field-programmable gate array;algebra	EDA	10.497766089954661	43.75712138224117	140272
8f4eb5b513cf7743a277cbb371341e89a3d995cf	mpeg-2 aac 5.1-channel decoder software for a low-power embedded risc microprocessor	5 1 channel audio signal embedded risc microprocessor mpeg 2 aac 5 1 channel decoder nec v830 imdct real time decoding;microprocessor;real time;low complexity;aac;audio coding;low power;decoding embedded software reduced instruction set computing microprocessors national electric code signal processing signal processing algorithms digital tv tv receivers costs;audio coding digital signal processing chips;digital signal processing chips;mpeg;decoder	Presented here is MPEG-2 AAC decoder software for a low-power embedded RISC microprocessor, NEC VS30 (300mW @133MHz). Fast processing methods for IMDCT reduce execution time by 41% and help achieve real-time decoding of a 5.1-channel audio signal, while using only 64.7% of processor capacity.	adaptive huffman coding;embedded system;low-power broadcasting;mpeg-2;microprocessor;modified discrete cosine transform;real-time clock;run time (program lifecycle phase)	Yuichiro Takamizawa;Kouhei Nadehara;Max Boegli;Masao Ikekawa;Ichiro Kuroda	2001	VLSI Signal Processing	10.1023/A:1012239731371	embedded system;real-time computing;computer hardware;audio signal processing;advanced audio coding;computer science;operating system;decoder	Embedded	11.479863603907205	41.55889673686655	140674
70998a30122c9f7c000c22255d98528d39aca633	implementing parallel counters with four-valued threshold logic	threshold logic four valved logic full adders multivalued logic parallel counters;four valved logic full adders;digital signal processing;power line;circuit design;parallel counters;threshold logic;large scale integration;carrying capacity;multivalued logic;logical form	Parallel counters are multiple-input circuits that count the number of their inputs that are in a given state. They are useful in implementing parallel multipliers, digital summers, digital correlators, and in other digital signal processing capacities. In this paper, the implementation of parallel counters with four-valued threshold logic is described and these implementations are compared to their binary full adder network counter equivalents. This logic form was selected because of the increasing importance of implementing state-of-the-art digital signal processing systems in large-scale-integrated (LSI) circuit form. LSI circuit designs are, in general, limited by the number of metal signal and power lines that must be placed upon the chip's surface, not by the number of active and passive devices used. Since each signal variable in four-valued logic may assume four logic states, twice the information carrying capacity as in binary logic, an over 50-percent savings in the total number of signal variables required to implement the parallel counter results. Also, with the circuits we describe here, approximately 50 percent fewer transistors and resistors are necessary for the implementation of four-valued logic parallel counters. These savings are attainable with a modest tradeoff in speed and power.	adder (electronics);digital signal processing;four-valued logic;integrated circuit;logic form;power-line communication;transistor	K. Wayne Current;Douglas A. Mow	1979	IEEE Transactions on Computers	10.1109/TC.1979.1675320	and-or-invert;parallel computing;logic synthesis;logic optimization;diode–transistor logic;logical form;logic level;asynchronous circuit;logic gate;logic family;three-state logic;carrying capacity;digital signal;computer science;electrical engineering;theoretical computer science;digital signal processing;circuit design;pass transistor logic;mathematics;sequential logic;pull-up resistor;digital electronics;register-transfer level;algorithm;resistor–transistor logic;emitter-coupled logic	EDA	15.419623833911151	45.25848708536901	140768
521448a1c2859b42023e59426724e1a1f8a8c4a3	on implementing erew work-optimally on mesh of trees	2 dimensional;3 dimensional	We show how to implement an `1 n log n-processor EREW PRAM workoptimally on a 2-dimensional n-sided mesh of trees, consisting of n processors, n memory modules, and O(n) nodes. Similarly, we prove that an `2 n log n-processor EREW PRAM can be implemented work-optimally on a 3-dimensional n-sided mesh of trees. By the work-optimality of implementations we mean that the expected routing time of PRAM memory requests is O(1) per simulated PRAM processor with high probability. Experiments show that on relatively small `1 and `2 the cost per simulated PRAM processor is 1:5{2:5 in the 2-dimensional case, and 2{3 in the 3-dimensional case. If at each step at most 1 3 'th of the PRAM processors make a reference to the shared memory, then the simulation cost is approximately 1. We also compare our work-optimal simulations to those proposed for coated meshes.	central processing unit;dimm;parallel random-access machine;routing;shared memory;simulation;time complexity;with high probability	Ville Leppänen	1995	J. UCS	10.3217/jucs-001-01-0023	three-dimensional space;two-dimensional space;computer science	Theory	12.628239762387727	33.027925428362266	141248
35304b3ede4cf26a23fa298e431cd3761e3ae113	a scalable dual-clock fifo for data transfers between arbitrary and haltable clock domains	estensibilidad;data transmission;cmos standard cell design;power 10 3 mw;procede de transfert;vlsi asynchronous dual clock first input first output fifo scalable;frequency synchronization;arbitrary clock domains;scalable dual clock fifo;systems on a chip;integrated circuit;dissipation energie;logic design;clocks;frequency 580 mhz;very large scale integration;fifo read and write operations;power efficiency;cmos full custom design;circuit vlsi;circuito integrado;procesador panel;energy dissipation;transfer processing;globalmente asincrono localmente sincrono;tecnologia mos complementario;data consumer;indexing terms;array processor;size 0 18 mum;fifo system;voltage 1 8 v;scalable;multiple clock cycles;processeur tableau;integrated circuit design;systeme fifo;haltable clock domains;vlsi circuit;cmos digital integrated circuits;system on chip;synchronization;clocks delay timing circuits cmos process very large scale integration synchronization robustness frequency power dissipation;procesamiento de transferencia;transmission donnee;power dissipation;globally asynchronous locally synchronous array processor;sistema fifo;vlsi;horloge;power efficient dual clock first input first out architecture;disipacion energia;extensibilite;scalability;globalement asynchrone localement synchrone;voltage 1 8 v scalable dual clock fifo data transfers arbitrary clock domains haltable clock domains power efficient dual clock first input first out architecture multiple clock cycles data consumer cmos full custom design cmos standard cell design globally asynchronous locally synchronous array processor power dissipation fifo read and write operations systems on a chip size 0 18 mum frequency 580 mhz power 10 3 mw;circuito vlsi;globally asynchronous locally synchronous;technologie mos complementaire;data transfers;clock;parallel processing;system on chip clocks cmos digital integrated circuits integrated circuit design logic design parallel processing;reloj	A robust, scalable, and power efficient dual-clock first-input first-out (FIFO) architecture which is useful for transferring data between modules operating in different clock domains is presented. The architecture supports correct operation in applications where multiple clock cycles of latency exist between the data producer, FIFO, and the data consumer; and with arbitrary clock frequency changes, halting, and restarting in either or both clock domains. The architecture is demonstrated in both a 0.18- mum CMOS full-custom design and a 0.18-mum CMOS standard cell design used in a globally asynchronous locally synchronous array processor. It achieves 580-MHz operation and 10.3-mW power dissipation while performing simultaneous FIFO read and write operations at 1.8 V.	array processing;cmos;clock rate;clock signal;embedded system;fifo (computing and electronics);full custom;globally asynchronous locally synchronous;mac os x 10.3 panther;scalability;standard cell;vector processor	Ryan W. Apperson;Zhiyi Yu;Michael J. Meeuwsen;Tinoosh Mohsenin;Bevan M. Baas	2007	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2007.903938	embedded system;parallel processing;electronic engineering;real-time computing;scalability;vector clock;telecommunications;clock domain crossing;computer science;dissipation;operating system;very-large-scale integration;synchronous circuit;clock gating;digital clock manager;data transmission;cpu multiplier	EDA	13.964629755478017	46.05861094567906	141366
5b031abb44d441e4347cf874e831ec8084f33fa8	an optimal scheduling algorithm for testing interconnect using boundary scan	optimal scheduling;necessary and sufficient condition;directed graph;upper and lower bounds;test scheduling	Systems will soon be built with ICs that conform with the IEEE 1149.1 boundary scan architecture. Due to the hierarchical nature of such systems, they may contain many boundary scan chains. These chains can be used to test the system, subsystem, and board interconnect. To reduce test time, the application of test vectors to these scan chains must be carefully scheduled. This article deals with problems related to finding an optimal schedule for testing interconnect. This problem is modeled using a directed graph. The following results are obtained: (1) upper and lower bounds on interconnect test time; (2) necessary and sufficient conditions for obtaining an optimal schedule when the graph is acyclic; (3) sufficient condition for obtaining an optimal schedule when the graph is cyclic; and (4) an algorithm for constructing an optimal schedule for any graph.	algorithm;boundary scan;scheduling (computing)	Jung-Cheun Lien;Melvin A. Breuer	1991	J. Electronic Testing	10.1007/BF00134948	mathematical optimization;real-time computing;directed graph;mathematics;distributed computing;upper and lower bounds	EDA	24.101652055067632	41.385593677054885	141487
d90ef89dc81ee19e845a3352c62169214e4cf607	embedding of rings and meshes onto faulty hypercubes using free dimensions	torus structure;tolerancia falta;computers;parallelisme;digital computers;graph theory;hypercube;rings;teoria grafo;general and miscellaneous mathematics computing and information science;concurrent computing;fault tolerant;fault tolerant computing hypercube networks redundancy;faulty hypercubes;emulation;longest path;dimension libre;mathematical logic;theorie graphe;computer architecture;enrobage;coating;envoltura;parallelism;fault tolerant computing;free dimension;paralelismo;redundancy;architecture ordinateur;mesh;free dimensions;pipelines;fault tolerance;partition;parallel;chemin plus long;subcubes rings meshes faulty hypercubes free dimensions fault tolerance redundancy linear chain ring mesh torus structure parallel pipeline free dimension partition;subcubes;hypercubes;programming 990200 mathematics computers;algorithms;design;arquitectura ordenador;hypercubes fault tolerance hardware redundancy pipelines concurrent computing costs multiprocessing systems computer science emulation;multiprocessing systems;meshes;computer science;ring;logic programs;hypercube computers;linear chain;tolerance faute;fault tolerant computers;parallel processing;pipeline;hypercube networks;hardware;hipercubo	"""Fault tolerance in hypercubes is achieved by exploiting inherent redundancy and executing tasks on faulty hypercubes. We consider tasks that require linear chain, ring, mesh, and torus structure, which are quite useful in parallel and pipeline computations. In this brief contribution, we assume the number of faults is on the order of the number of dimensions of the hypercube. Our techniques are based on a key concept called free dimension, which can be used to partition a cube into subcubes such that each subcube contains, at most, one faulty node. Subgraphs are embedded in each subcube and then merged to form the entire graph. Using this approach, we obtain the following results in an n-dimensional hypercube. A ring of length 2"""" 2 f or a chain of length 2"""" 2 f + 1 can be found, given any f 5 11 2 faulty nodes. The length of chains or rings obtained is optimal (longest) in the worst case of fault distribution. A mesh of size at least (2'""""l 2f + 1) * 2""""'L * or a torus of size at least Zf) * Z""""'2 c * Z7""""d can be found when f 5 n i l 2 where i i = 1 1 1 1 + rnz + + n i C i and 1 1 1 1 is the largest direction of mesh. A mesh or torus of size at least (2 '"""" l X 1 2 * """"""""2* where k = 111111 (f r27111/2n-f+1] ) The processor utilization of this embedding scheme is high. In particular, 1) for one-dimensional mesh or torus (linear chain or ring), the processor utilization of embedding is 100%; and 2) for 1 5 f 5 1) r r i1 + 1, a mesh or torus with the optimal size of All the embeddings can be constructed in a distributed manner with time complexity O( n )"""	best, worst and average case;computation;embedded system;fault tolerance;grid network;olap cube;time complexity;universal quantification	Pei-Ji Yang;Sing-Ban Tien;Cauligi S. Raghavendra	1994	IEEE Trans. Computers	10.1109/12.280808	parallel processing;fault tolerance;parallel computing;concurrent computing;computer science;graph theory;theoretical computer science;distributed computing;hypercube	Theory	13.821390046515024	35.40666540788476	141608
bb0b9a27db0b468fbfff7364c83a72f7e9090b59	a programmable parallel vlsi architecture for 2-d discrete wavelet transform	frames per second;dsp architecture;discrete wavelet transform;pyramid algorithm;chip;wavelet transform;jpeg 2000;real time application;high frequency;parallel processing;wavelet;vlsi architecture	Many VLSI architectures for computing the discrete wavelet transform (DWT) were presented, but the parallel input data sequence and the programmability of the 2-D DWT were rarely mentioned. In this paper, we present a parallel-processing VLSI architecture to compute the programmable 2-D DWT, including various wavelet filter lengths and various wavelet transform levels. The proposed architecture is very regular and easy for extension. To eliminate high frequency components, the pixel values outside the boundary of the image are mirror-extended as the symmetric wavelet transform (SWT) and the mirror-extension is realized via the routing network. Owing to the property of the parallel processing, we adopt the row-based recursive pyramid algorithm (RPA), similar to 1-D RPA, as the data scheduling. This design has been implemented and fabricated in a 0.35 μm 1P4M CMOS technology and the working frequency is 50 MHz. The chip size is about 5200 μm × 2500 μm. For a 256 × 256 image, the chip can perform 30 frames per second with the filter length varying from 2 to 20 and with various levels. The proposed architecture is suitable for real-time applications such as JPEG 2000.	algorithm;cmos;clock rate;discrete wavelet transform;jpeg 2000;parallel computing;pixel;real-time clock;real-time computing;recursion;routing;simd;scheduling (computing);standard widget toolkit;very-large-scale integration	Chien-Yu Chen;Zhong-Lan Yang;Tu-Chih Wang;Liang-Gee Chen	2001	VLSI Signal Processing	10.1023/A:1011180506997	chip;wavelet;embedded system;parallel processing;parallel computing;second-generation wavelet transform;continuous wavelet transform;telecommunications;computer science;high frequency;cascade algorithm;jpeg 2000;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;fast wavelet transform;lifting scheme;frame rate;wavelet transform	HPC	11.998719752519946	42.067156823970855	141738
9b341ad94900006d77d57dd8878f01c6845243ae	memory access reduced software implementation of h.264/avc sub-pixel motion estimation using differential data encoding	video streaming;motion estimation;automatic voltage control motion estimation encoding arithmetic signal processing algorithms bandwidth software algorithms digital signal processors clocks software performance;quantisation signal;memory access;video coding;digital signal processor;digital signal processing chips;digital arithmetic;software pipelining;video coding digital arithmetic digital signal processing chips encoding motion estimation quantisation signal;advanced video coding memory access reduced software implementation h 264 avc sub pixel motion estimation differential data encoding vliw simd digital signal processor tms320c6416 cpu clock cycles arithmetic operations sub word operations software pipelining memory bandwidth requirements;encoding;memory bandwidth;software implementation	We studied an efficient software implementation of H.264/AVC sub-pixel motion estimation (ME) algorithm on a VLIW-SIMD digital signal processor, TMS320C6416. The sub-pixel ME algorithm demands large memory accesses while the required arithmetic operations are fairly simple. Although the CPU clock cycles for arithmetic operations can be reduced much by employing sub-word operations and applying software pipelining techniques, the limited memory bandwidth of the architecture restricts the overall performance. Moreover, aggressive VLIW-SIMD optimization results in the degradation of the performance by causing excessive CPU stalls during memory accesses. In this paper, we relieved the memory bandwidth requirements for creating quarter-pixel images by reducing the precision of image data, from 8 bits to 4 bits. As a result, the amount of memory accesses is much reduced at the cost of some increase of the arithmetic operations, which contributes to the balance of arithmetic and memory access operations. The experimental result shows that the memory stall cycles are decreased by 80% and the speed-up of 260% is obtained. The bit rate of the encoded video stream is increased slightly, about 2% on the average, due to the effects of quantization.	algorithm;central processing unit;clock signal;digital signal processor;elegant degradation;h.264/mpeg-4 avc;mathematical optimization;memory bandwidth;motion estimation;pipeline (computing);pixel;quantization (signal processing);requirement;simd;signal processing;software pipelining;streaming media;very long instruction word	Hyojin Choi;Wonchul Lee;Wonyong Sung	2007	2007 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2007.377855	uniform memory access;software pipelining;embedded system;computer vision;digital signal processor;interleaved memory;semiconductor memory;parallel computing;real-time computing;computer hardware;computer science;motion estimation;flat memory model;context-adaptive binary arithmetic coding;registered memory;memory bandwidth;encoding	Arch	12.576021055294666	40.242257993193185	141996
4c7bf9276daf66fbb02a9c8cd7b7d30333be0ade	quantum device circuits made of trapped ions		We intend to give an insight into one kind of circuit, the rating of which is based on quantum e ects. Therefore, the role of quantum e ects is changed from undesired e ects to the basic e ect of circuit performance. The design of coherent quantum device circuits and the development of the software of quantum information processing depends on the rules of quantum mechanics. One kind of quantum information processing has become popular by the name ‘quantum computing’ (q.c.). The prospect of quantum computing consists for example in some kind of np-complete combinatorial problems. Nevertheless, for the far future we can only expect a special purpose computer instead of a general purpose machine. The goal of this paper is to provide insights into the current development of a trapped ion quantum information processor, which is a prominent candidate for a successful working quantum computer. The information is represented by the quantized state of the related quantum device circuit. Based on the time-dependent Schr odinger-equation the system’s state changes continuously in time like the state of an analog circuit. The system can be controlled by means of precise sequences of laser pulses. The laser eld is a time dependent component of the quantum system. We investigate ways of how to implement the mathematical concept of q.c. in the linear ion trap (Paul-trap). The nal state of the quantum system represents the outcome of an implemented single quantum gate or more complex quantum algorithm. In order to synthesize quantum states the designer of sequences of laser pulses needs an e cient simulation tool. We will show the implementation of a quantum gate using our trapped ion simulator (TrIoS). The leakage and the gate accuracy are optimized by parameter variations. Copyright ? 2001 John Wiley & Sons, Ltd.	analogue electronics;coherence (physics);computer cooling;controlled not gate;embedded system;german research centre for artificial intelligence;graphical user interface;information processing;information processor;john d. wiley;np-completeness;nsa product types;quantum algorithm;quantum computing;quantum decoherence;quantum error correction;quantum gate;quantum information science;quantum mechanics;quantum register;quantum state;quantum system;qubit;simulation;spectral leakage	Kai Pahlke;Wolfgang Mathis	2001	I. J. Circuit Theory and Applications	10.1002/1097-007X(200101/02)29:1%3C119::AID-CTA137%3E3.0.CO;2-S	quantum simulator;ion trapping;electronic engineering;quantum information;quantum information science;information processing;pulse sequence;telecommunications;computer science;engineering;electrical engineering;artificial intelligence;amplitude damping channel;theoretical computer science;quantum network;quantum capacity;integrated circuit;open quantum system;circuit design;control theory;quantum circuit;mathematics;quantum sensor;quantum channel;quantum computer;quantum imaging;quantum process;quantum algorithm;physics;one-way quantum computer;quantum mechanics;quantum phase estimation algorithm;quantum gate;quantum error correction	Theory	19.568403555763982	41.26210878812385	142456
a968ee92983968059848dcaa33569bf388196326	easily testable cellular realizations for the (exactly p)-out-of n and (p or more)-out-of n logic functions	symmetric function;single faults;stuck at faults;symmetric functions;elementary symmetric function;easily testable networks multiple faults single faults stuck at faults symmetric functions threshold functions;threshold functions;easily testable networks;multiple faults	Networks to realize all n-variable symmetric threshold functions and elementary symmetric functions are given. It is also shown that only 2n test inputs are necessary and sufficient to test any number of faults in these networks.	elementary	Sudhakar M. Reddy;James R. Wilson	1974	IEEE Transactions on Computers	10.1109/T-C.1974.223787	arithmetic;discrete mathematics;mathematics;algorithm;symmetric function	Theory	19.82735568402736	44.240778141240995	142526
0924081db257d4757740d5e17db0dd2edf00bd4e	matrix transpose on meshes with wormhole and xy routing	linear algebra;concurrent computing;routing;matrix transpose;complexity;wormhole;communication model;computer architecture;linear algebra computational complexity;partial differential equations;computational complexity;nearly optimal algorithms;recursive exchange algorithm;complexity matrix transpose meshes wormhole xy routing nearly optimal algorithms communication model lower bound recursive exchange algorithm;hypercubes;circuits;whales;meshes;computer science;xy routing;mesh generation;optimal algorithm;routing hypercubes computer science mesh generation circuits whales linear algebra partial differential equations computer architecture concurrent computing;lower bound	We give nearly optimal algorithms for matrix transpose on meshes with wormhole and XY routing and with a 1-port or 2-port communication model. For an N/spl times/N mesh, where N=3/spl middot/2/sup n/ and each mesh node has a submatrix of size m to be transposed, our algorithms take Nm/2 time steps for 1-port model, and about Nm/3.27 time steps for 2-port model. The lower bound is Nm/3.414. While there is no previously known algorithm for matrix transpose on meshes with wormhole and XY routing, a naive algorithm, which is naturally adapted from the well-known Recursive Exchange Algorithm, has a complexity of about Nm. That is our best algorithm improves over the naive algorithm by about a factor of 3.27, and is about a factor of 3.414/3.27 of the lower bound. >	classical xy model;routing	Kuo-Shun Ding;C. T. Howard Ho;Jyh-Jong Tsay	1994		10.1109/SPDP.1994.346111	polygon mesh;mesh generation;electronic circuit;routing;parallel computing;complexity;models of communication;concurrent computing;wormhole;computer science;theoretical computer science;linear algebra;distributed computing;upper and lower bounds;computational complexity theory;partial differential equation;algorithm;hypercube;transpose	EDA	14.327463786190998	33.57022751097985	142606
95b1f11f13a6422850c8d912121101f239407832	vector-matrix multiplication based on a ternary optical computer	signed digit;inner product;optical computing;light intensity;matrix multiplication;high efficiency;binary tree	This paper implements Optical Vector-Matrix Multiplication (OVMM) completely in parallel on a novel optical computing architecture, Ternary Optical Computer (TOC), by use of the Modified Signed-Digit (MSD) number system. For high efficiency, partial products (PPs) are generated in parallel and the vector inner products (VIPs) are produced by a binary-tree algorithm, and then the OVMM is implemented. The experimental result validates the feasibility and correctness of VMM on TOC. In this system, it is not necessary to gauge light intensities, but judge whether there is light during decoding.	algorithm;computer architecture;correctness (computer science);list of algorithms;matrix multiplication;optical computing;optical disc authoring;virtual machine manager	Xianchao Wang;Junjie Peng;Yi Jin;Mei Li;Zhangyi Shen;Shan Ouyang	2009		10.1007/978-3-642-11842-5_59	parallel computing;dot product;binary tree;matrix multiplication;theoretical computer science;optical computing;algorithm	Theory	15.014049540622821	42.90060536237608	143065
27325e294397ddae2f45fec7aa4d48a65789d6e1	realization of fir digital filters based on stochastic/binary hybrid computation	hardware complexity fir digital filters stochastic binary hybrid computation finite impulse response digital filters;image coding;complexity theory;multiplexing;finite impulse response filters;adders;fir filters;encoding;hardware;finite impulse response filters encoding adders hardware image coding complexity theory multiplexing	Recently, some attempts have been made to apply stochastic computation to realization of Finite Impulse Response (FIR) digital filters. Such new FIR filter realizations lead to significant reduction of hardware complexity over the conventional filter realizations based on binary computation. However, the stochastic FIR filters suffer from lower computational accuracy than the FIR filters based on binary computation. This paper presents a new method for realization of stochastic FIR filters to improve computational accuracy. In the proposed method, multipliers are realized using stochastic computation but adders are realized using binary computation. Evaluation results demonstrate that our method achieves a 7dB improvement in stopband attenuation.	computation;digital filter;performance evaluation	Shunsuke Koshita;Naoya Onizawa;Masahide Abe;Takahiro Hanyu;Masayuki Kawamata	2016	2016 IEEE 46th International Symposium on Multiple-Valued Logic (ISMVL)	10.1109/ISMVL.2016.40	electronic engineering;computer science;theoretical computer science;finite impulse response;linear filter;control theory;mathematics;infinite impulse response;multiplexing;adder;encoding	Robotics	12.468159856122751	45.05465426044126	143091
5d222288cf356978f8c1856ae442921609f6b211	parallel pipelined array architectures for real-time histogram computation in consumer devices	histograms arrays clocks real time systems educational institutions field programmable gate arrays;histograms;field programmable gate array;fpga parallel pipelined array architectures real time histogram computation consumer imaging products vlsi image processing engine;image processing;clocks;real time;digital imaging;fpga;endnotes;arrays;image enhancement;parallel architectures;pipelined array;parallel computer;vlsi;pubications;digital image;external memory;field programmable gate arrays;vlsi field programmable gate arrays image enhancement parallel architectures;image processing parallel histograms pipelined array fpga digital imaging;real time systems;parallel histograms	The real-time parallel computation of histograms using an array of pipelined cells is proposed and prototyped in this paper with application to consumer imaging products. The array operates in two modes: histogram computation and histogram reading. The proposed parallel computation method does not use any memory blocks. The resulting histogram bins can be stored into an external memory block in a pipelined fashion for subsequent reading or streaming of the results. The array of cells can be tuned to accommodate the required data path width in a VLSI image processing engine as present in many imaging consumer devices. Synthesis of the architectures presented in this paper in FPGA are shown to compute the real-time histogram of images streamed at over 36 megapixels at 30 frames/s by processing in parallel 1, 2 or 4 pixels per clock cycle 1.	clock signal;computation;data item;field-programmable gate array;front-side bus;image processing;image processor;image sensor;parallel computing;pipeline (computing);pixel;real-time clock;real-time operating system;speedup;streaming media	José O. Cadenas;Robert Simon Sherratt;Pablo Huerta;Wen-Chung Kao	2011	IEEE Transactions on Consumer Electronics	10.1109/TCE.2011.6131111	electronic engineering;parallel computing;image processing;computer science;theoretical computer science;field-programmable gate array	EDA	10.70882009441322	39.934580391208506	143211
9c184ab4b303c3f933c9624f39aa29a2695be2dc	an efficient vlsi architecture for full-search variable block size motion estimation in h.264/avc	langage description materiel informatique;largeur bande;vision ordenador;arquitectura circuito;estimation mouvement;compilateur;full search;storage access;multimedia;image processing;reutilizacion;real time;h 264 avc;estimacion movimiento;hardware description languages;circuit vlsi;procesamiento imagen;real time processing;circuit architecture;motion estimation;compiler;qualite image;traitement image;system on a chip;variable block size;reuse;processing time;computer vision;chip;memory access;vlsi circuit;sistema sobre pastilla;grande vitesse;energy consumption;image quality;temps reel;anchura banda;acces memoire;architecture circuit;consommation energie;vlsi;acceso memoria;tiempo real;bandwidth;temps traitement;vision ordinateur;calidad imagen;gran velocidad;vbsme;systeme sur puce;power consumption;circuito vlsi;consommation energie electrique;memory bandwidth;high speed;tiempo proceso;compilador;block matching algorithm;reutilisation;consumo energia;vlsi architecture	In this paper, an efficient VLSI architecture of full-search variable block size motion estimation (VBSME) suitable for high quality video is proposed. Memory bandwidth in high-quality video is a mainly responsible for throughput limitations and power consumption in VBSME. The proposed architecture is designed for reducing the memory bandwidth by adopting “meander”-like scan for a high overlapped data of the search area and using on-chip memory to reuse the overlapped data. We can reuse the previous candidate block of 98% for the current one and save memory access cycles about 19% in a search range of [-32, +31]. The architecture has been prototyped in Verilog HDL and synthesized by Synopsys Design Compiler with Samsung 0.18um standard cell library. Under a clock frequency of 67MHz, The simulation result shows that the architecture can achieve the real-time processing of 720x576 picture size at 30fps with the search range of [-32~+31].		Seung-Man Pyen;Kyeong-Yuk Min;Jong-Wha Chong	2007		10.1007/978-3-540-69429-8_5	image quality;chip;system on a chip;embedded system;computer vision;compiler;telecommunications;image processing;computer science;motion estimation;reuse;block-matching algorithm;very-large-scale integration;hardware description language;memory bandwidth;bandwidth	EDA	13.910917584086198	40.58701297386945	143432
d18a9a40bf6eb79efcc6470014f3bf0970fec0a6	a versatile mechanism to move data in an array processor	computers;arrays multiprocessor interconnection switches computers process control data mining algorithm design and analysis;apl language;switching network apl language array processor benes network parallel computer perfect shuffle signal processor;benes network;signal processor;array processor;data mining;arrays;perfect shuffle;process control;parallel computer;switches;switching network;algorithm design and analysis;multiprocessor interconnection	Selection of elements and alignment of operands are fundamental operations on data, just as are arithmetic operations. Whereas sophisticated algorithms have been devised for the latter, vector processors usually lack a flexible and efficient routing unit. This is especially true of SIMD computers, to which the present study is devoted. Examples of required manipulations are: transfer, shift, diffusion, compression, expansion, mesh, perfect shuffle, and bit reversal. Using a method described in a previous paper of ours [15] we present algorithms to control a Benes network and perform these manipulations on vectors whose length is equal to the number of processing elements. Then we dispense with this constraint and propose a mechanism to rearrange vectors of any size, stored according to several schemes.	algorithm;bit-reversal permutation;central processing unit;clos network;computer;data compression;operand;routing;simd;vector processor	Jacques Lenfant	1985	IEEE Transactions on Computers	10.1109/TC.1985.5009403	embedded system;algorithm design;digital signal processor;parallel computing;network switch;computer science;theoretical computer science;operating system;process control;distributed computing;algorithm	Arch	11.47509406914197	38.31443434189366	143478
2f18a84b266694deef93b48999fbc40467ead755	efficient hardware solution for low power and adaptive image-compression in wsn	cmos integrated circuits;microcontrollers;image coding;data compression;clocks;image communication;packet loss;visual communication;asic circuits efficient hardware solution adaptive image compression wsn packet loss tolerant image compression wireless sensors networks cmos circuit data packetization microcontroller image communication fpga circuits;wsn;loeffler dct;fpga;wireless sensor network;low power;image compression;performance analysis;sensor nodes;hardware design;robustness;asic;asic hardware design image compression loeffler dct wsn fpga;field programmable gate arrays;wireless sensor networks image coding field programmable gate arrays robustness hardware clocks cmos integrated circuits;hardware implementation;wireless sensor networks;wireless sensor networks cmos integrated circuits data compression image coding microcontrollers visual communication;hardware	In this paper, we present and evaluate a hardware implementation for user-driven and packet-loss tolerant image compression, especially designed to enable low-power image compression and communication over wireless sensors networks (WSNs). The proposed compression scheme, presented as a CMOS circuit, is intended to be embedded in the camera sensor. It will be considered as a co-processor for tasks related with image compression and data packetization, which unloads the main microcontroller so that it will spend less time in active mode. The interest of our solution is twofold. First, compression settings can be changed at runtime (upon reception of a request message sent by an end-user or according to the internal state of the camera sensor node). Second, the image compression chain includes a (block of) pixel interleaving scheme which significantly improves the robustness against packet loss in image communication. The main part of this paper focuses on the specification and the performances analysis of this solution when implemented on FPGA and ASIC circuits.	application-specific integrated circuit;cmos;coprocessor;embedded system;field-programmable gate array;forward error correction;image compression;image sensor;low-power broadcasting;microcontroller;network packet;performance;pixel;run time (program lifecycle phase);sensor node	Med Lassaad Kaddachi;Adel Soudani;Ibtihel Nouira;Vincent Lecuire;Kholdoun Torki	2010	2010 17th IEEE International Conference on Electronics, Circuits and Systems	10.1109/ICECS.2010.5724579	data compression;embedded system;electronic engineering;real-time computing;image compression;computer science	EDA	11.496607965102328	41.08833823642735	143626
b5326a5e1bd1edd4cf567016b8dc47af9f630ed3	digital logic with molecular reactions	digital function;logic circuits;combinational circuits;combinational components;molecular reaction;circuit feedback;bistable mechanism;shift registers;d flip-flops;molecular system;logical components;d latches;dna-based computation;flip-flops;signal value;d flip-flop;single molecular type;molecular reactions;dna;linear feedback shift register;binary adder;digital logic;dilution;sample preparation;biochip	"""This paper presents a methodology for implementing digital logic with molecular reactions based on a bistable mechanism for representing bits. The value of a bit is not determined by the concentration of a single molecular type; rather, it is the comparison of the concentrations of two complementary types that determines if the bit is """"0"""" or """"1"""". This mechanism is robust: any small perturbation or leakage in the concentrations quickly gets cleared out and the signal value is not affected. Based on this representation for bits, a constituent set of logical components are implemented. These include combinational components -- AND, OR, NOR, and XOR -- as well as sequential components -- D latches and D flip-flops. Using these components, three full-fledged design examples are given: a square-root unit, a binary adder and a linear feedback shift register. DNA-based computation via strand displacement is the target experimental chassis. The designs are validated through simulations of the chemical kinetics. The simulations show that the molecular systems compute digital functions accurately and robustly."""	and gate;adder (electronics);bistability;boolean algebra;chassis;combinational logic;computation;displacement mapping;exclusive or;flops;kinetics internet protocol;linear-feedback shift register;robustness (computer science);sentinel value;simulation;spectral leakage;strand (programming language)	Hua Jiang;Marc D. Riedel;Keshab K. Parhi	2013	2013 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)		electronic engineering;logic gate;biochip;computer science;theoretical computer science;dilution;sequential logic;shift register;combinational logic;digital electronics;register-transfer level;dna;algorithm	EDA	18.747450769648157	42.24882039630984	143775
8251f323f1c90e2428b32c397761249f78367392	bounds on the voter model in dynamic networks	consensus;004;voting distributed computing conductance dynamic graphs consensus;distributed computing;conductance;dynamic graphs;voting	In the voter model, each node of a graph has an opinion, and in every round each node chooses independently a random neighbour and adopts its opinion. We are interested in the consensus time, which is the first point in time where all nodes have the same opinion. We consider dynamic graphs in which the edges are rewired in every round (by an adversary) giving rise to the graph sequence G1, G2, . . . , where we assume that Gi has conductance at least φi. We assume that the degrees of nodes don’t change over time as one can show that the consensus time can become super-exponential otherwise. In the case of a sequence of d-regular graphs, we obtain asymptotically tight results. Even for some static graphs, such as the cycle, our results improve the state of the art. Here we show that the expected number of rounds until all nodes have the same opinion is bounded by O(m/(dmin ·φ)), for any graph with m edges, conductance φ, and degrees at least dmin. In addition, we consider a biased dynamic voter model, where each opinion i is associated with a probability Pi, and when a node chooses a neighbour with that opinion, it adopts opinion i with probability Pi (otherwise the node keeps its current opinion). We show for any regular dynamic graph, that if there is an ǫ > 0 difference between the highest and second highest opinion probabilities, and at least Ω(log n) nodes have initially the opinion with the highest probability, then all nodes adopt w.h.p. that opinion. We obtain a bound on the convergences time, which becomes O(log n/φ) for static graphs.	adversary (cryptography);conductance (graph);time complexity;voter model	Petra Berenbrink;George Giakkoupis;Anne-Marie Kermarrec;Frederik Mallmann-Trenn	2016		10.4230/LIPIcs.ICALP.2016.146	conductance;combinatorics;consensus;voting;computer science;theoretical computer science;machine learning;mathematics	Theory	19.38045245423372	34.79279532637638	143844
7cd2f0c7a90350b36fe75f04c262ed42e9ea6818	on the power of oracle \varomega ? for self-stabilizing leader election in population protocols	oracles;networks of mobile agents;population protocols;self stabilization;leader election	This paper considers the fundamental problem of self-stabilizing leader election (SSLE) in the model of population protocols. In this model an unknown number of asynchronous, anonymous and finite state mobile agents interact in pairs. SSLE has been shown to be impossible in this model without additional assumptions. This impossibility can be circumvented for instance by augmenting the system with an oracle (an external module providing supplementary information useful to solve a problem). Fischer and Jiang have proposed solutions to SSLE, for complete communication graphs and rings, using the oracle Omega?, called the eventual leader detector. In this paper, we investigate the power of Omega? on larger families of graphs. We present two important results. Our first result states that Omega? is powerful enough to allow solving SSLE over arbitrary communication graphs of bounded degree. Our second result states that, Omega? is the weakest (in the sense of Chandra, Hadzilacos and Toueg) for solving SSLE over rings. We also prove that this result does not extend to all graphs; in particular not to the family of arbitrary graphs of bounded degree.	leader election	Joffroy Beauquier;Peva Blanchard;Janna Burman;Oksana Denysyuk	2016		10.1007/978-3-319-49259-9_3	discrete mathematics;mathematics;distributed computing;algorithm	Theory	17.35013914363387	34.938507682696276	144584
de546909801d79ce4a66c0ac9a30a04d093fa957	a unified framework for off-line permutation routing in parallel networks	ring network;permutation routing;polynomial time;mesh network	In this paper we present a general strategy for finding efficient permutation routes in parallel networks. Among the popular parallel networks to which the strategy applies are mesh networks, hypercube networks, hypercube-derivative networks, ring networks, and star networks. The routes produced are generally congestion-free and take a number of routing steps that is within a small constant factor of the diameter of the network. Our basic strategy is derived from an algorithm that finds (in polynomial time) efficient permutation routes for aproduct network, G×H, given efficient permutation routes forG andH. We investigate the use of this algorithm for routingmultiple permutations and extend its applicability to a wide class of graphs, including several families ofCayley graphs. Finally, we show that our approach can be used to find efficient permutation routes among the remaining live nodes infaulty networks.	algorithm;mesh networking;network congestion;online and offline;random graph;routing;time complexity;token ring;unified framework	Marc Baumslag;Fred S. Annexstein	1991	Mathematical systems theory	10.1007/BF02090401	time complexity;ring network;combinatorics;random permutation;computer science;theoretical computer science;mesh networking;permutation graph;mathematics;distributed computing;algorithm	Theory	21.666964559234366	36.134548053946844	144693
1e54f19cce82ba1e313be64c7ee0a963a9eff668	ring embedding in hypercubes with faculty nodes	graph theory;distributed system;hypercube;teoria grafo;systeme reparti;systeme multiprocesseur memoire repartie;complexite calcul;anneau;theorie graphe;algorithme;algorithm;complejidad computacion;fault tolerant system;sistema repartido;computational complexity;sistema multiprocesador memoria distribuida;fault tolerance;sistema tolerando faltas;systeme tolerant les pannes;graph embedding;distributed memory multiprocessor system;ring;anillo;algoritmo;hipercubo	Hypercube is an attractive structure for parellel processing due to its symmetry and regularity. To increase the reliability of hypercube based systems and to allow their use in the presence of faulty nodes, efficient fault-tolerant schemes in hypercubes are necessary. In this paper, we present an algorithm for embedding rings in hypercubes based multiprocessor network in the event of node failures. The algorithm can tolerate up to θ(2n/2) faults, and guarantee that given any f < (n - 2k)2k faulty nodes, it can find a ring of size at least 2n - 2f for k = 0 and 2n - 2k f - 22k for k ≥ 1 in an n-dimensional hypercube. It improves over existing algorithms in the size of ring.		Jin-Soo Kim;Seung Ryoul Maeng;Hyunsoo Yoon	1997	Parallel Processing Letters	10.1142/S0129626497000309	fault tolerance;discrete mathematics;graph theory;mathematics;distributed computing;algorithm	ML	23.553811372507454	34.57385447130212	144753
6cf0ab0ee4da79ac22d1096e06793f18d1ca70a9	a double-path intra prediction architecture for the hardware h.265/hevc encoder	high efficiency video coding encoder double path intra prediction architecture hardware h 265 hevc encoder prediction units independent processing path fpga aria ii devices asic technology;video coding application specific integrated circuits field programmable gate arrays;standards random access memory hardware equations prediction algorithms arrays	This paper presents an innovatory approach to the design of the intra prediction architecture for the hardware H.265/HEVC (High Efficiency Video Coding) encoder. As the most of the computational complexity in the intra prediction algorithm is associated with the need to process number of 4×4 Prediction Units (PUs), an independent processing path is proposed for this specific PU size with a separate reconstruction loop. The final result from this path is then incorporated into the second path, independently checking all the remaining PUs. This approach does not entail a significant increase in utilization of hardware resources, while considerably accelerates the encoding. The proposed architecture can operate at 100 MHz for FPGA Aria II devices and at 200 MHz for the TSMC 0.13μm technology. The achieved throughput allows the processing of almost 17.5 and 35 1080p frames per second using FPGA and ASIC technology, respectively. The solution is compliant with the Main, Main 10, and Main Still Picture profiles of the H.265/HEVC standard.	algorithm;application-specific integrated circuit;aria;computation;computational complexity theory;encoder;field-programmable gate array;high efficiency video coding;ibm systems network architecture;intra-frame coding;real-time computing;reconstruction filter;throughput	Andrzej Abramowski;Grzegorz Pastuszak	2014	17th International Symposium on Design and Diagnostics of Electronic Circuits & Systems	10.1109/DDECS.2014.6868758	embedded system;real-time computing;computer science;theoretical computer science	Arch	12.466075564563955	40.79097302909378	145126
34bd65e275360f0e4e4dbf169ffee57ac040305d	manip - a parallel computer system for implementing branch and bound algorithms		In this paper, we propose and analyze the design of MANIP, a parallel machine for processing nondeterministic polynomial complete problems. The most general technique that can be used to solve a wide variety of NP-complete problems on a uniprocessor system, optimally or suboptimally, is the branch and bound algorithm. We have adapted and extended the branch and bound algorithm for parallel processing. The parallel branch and bound algorithm requires a combination of sorting and merging. A common memory to sort for a large number of processors can become a bottleneck in the system. We have proposed a system with distributed intelligence so that sorting can be carried out in a distributed fashion. A unidirectional ring network is proved to be the optimal and most cost-effective interprocessor communication network when sorting is done by a hardware priority queue in each processor. Lastly, the performance on the proposed system is evaluated using the vertex covering problem.	algorithm;bottleneck (engineering);branch and bound;central processing unit;covering problems;inter-process communication;linc;np (complexity);np-completeness;parallel computing;polynomial;priority queue;ring network;sorting;telecommunications network;uniprocessor system;vertex cover	Benjamin W. Wah;Y. W. Eva Ma	1981				Arch	12.735779695548816	33.06068892958649	145141
391f455f1464ebd64e68750cdc4c87a4ad32fc5c	a low power cordic-based hardware implementation of izhikevich neuron model		In this paper, an efficient CORDIC-based hardware implementation of the Izhikevich neuron model is introduced. The CORDIC (COordinate Rotation Digital Computer) algorithm is used to approximate the square term in Izhikevich equations that describe the neuron response. The approximation is evaluated by defining four types of errors where the CORDIC approximation shows significant improvement in error performance compared to the Piecewise Linear (PWL) model [1]. The power consumption of the CORDIC-based neuron hardware implementation ranges from 0.26 mW to 0.4 mW whereas the PWL-based neuron as well as the original Izhikevich neuron hardware implementations consume 0.3 mW and 1.06 mW, respectively. A Figure of Merit (FoM) is defined to show the tradeoff among errors, power and area. By comparing with the PWL-based neuron hardware implementation, it is found that the CORDIC-based model is preferred as an approximation method from the error, power and area perspective.		Abdelrahim Elnabawy;Hussien Abdelmohsen;M. A. Moustafa;Mostafa Elbediwy;Amr Helmy;Hassan Mostafa	2018	2018 16th IEEE International New Circuits and Systems Conference (NEWCAS)	10.1109/NEWCAS.2018.8585485	computer hardware;biological neuron model;approximation algorithm;cordic;computer science;piecewise linear function;rotation (mathematics);figure of merit	EDA	13.955587488164126	44.010085314934855	145214
bbfcbfb27a33a3380c59bf317b408e61c7ea387f	a 1280/spl times/720 pixels 30 frames/s h.264/mpeg-4 avc intra encoder	digital signal processors;dsc;quantization;video application;image coding;cost function;real time;digital video camera application;video compression;video quality;cmos process;still image encoding;chip;digital cameras;video coding;mpeg 4 standard;automatic voltage control;cmos digital integrated circuits;0 18 micron h 264 mpeg 4 avc intra encoder hdtv dsc digital video camera application still image encoding real time moving picture video application cmos process;mpeg 4 standard automatic voltage control image reconstruction hardware image coding hdtv cost function video compression quantization digital cameras;video cameras;image reconstruction;0 18 micron;proceedings paper;hdtv;video coding cmos digital integrated circuits digital signal processing chips high definition television video cameras video codecs;h 264;real time moving picture;video codecs;digital signal processing chips;digital video;functional unit;high performance;high definition television;mpeg 4 avc intra encoder;hardware	This paper presents an HDTV size H.264/MPEG-4 AVC intra encoder suitable for DSC and digital video camera applications. The chip reduces the gate count by saving the costly plane mode and enhances the video quality with the improved cost function. With careful scheduling and high performance function unit, the developed chip can easily support 29.46M pixels/s still image encoding and real-time moving picture intra coding of HDTV 720p@30fps video application when clocked at 117.28MHz under 0.18mum CMOS process	airplane mode;cmos;clock rate;digital video;encoder;gate count;h.264/mpeg-4 avc;intra-frame coding;loss function;pixel;real-time clock;scheduling (computing)	Chao-Chung Cheng;Chun-Wei Ku;Tian-Sheuan Chang	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1693838	data compression;iterative reconstruction;scalable video coding;chip;differential scanning calorimetry;computer vision;digital signal processor;electronic engineering;quantization;computer science;video quality;machine learning;computer graphics (images)	EDA	12.366616136502444	40.70311565172939	145294
682bd62e1fc5dc015d5619a4ac24a0fc5f18c608	approximation bounds for black hole search problems	search problem;distributed system;red sin hilo;black hole;systeme reparti;informatique mobile;agent mobile;temps polynomial;reseau sans fil;approximation algorithm;localization;agente movil;securite informatique;wireless network;localizacion;problema investigacion;processus stationnaire;computer security;sistema repartido;localisation;trou noir;seguridad informatica;algoritmo aproximacion;polynomial time;proceso estacionario;mobile agent;algorithme approximation;agujero negro;mobile computing;probleme recherche;stationary process;tiempo polinomial	A black hole is a highly harmful stationary process residing in a node of a network and destroying all mobile agents visiting the node without leaving any trace. The Black Hole Search is the task of locating all black holes in a network, through the exploration of its nodes by a set of mobile agents. In this paper we consider the problem of designing the fastest Black Hole Search, given the map of the network, the starting node and, possibly, a subset of nodes of the network initially known to be safe. We study the version of this problem that assumes that there is at most one black hole in the network and there are two agents, which move in synchronized steps. We prove that this problem is not polynomial-time approximable within 389 388 (unless P=NP). We give a 6-approximation algorithm, thus improving on the 9.3-approximation algorithm from [2]. We also prove APX-hardness for a restricted version of the problem, in which only the starting node is initially known to be safe.	apx;algorithm;approximation;approximation algorithm;black hole;fastest;hardness of approximation;mobile agent;np-hardness;p versus np problem;polynomial;refinement (computing);stationary process;time complexity	Ralf Klasing;Euripides Markou;Tomasz Radzik;Fabiano Sarracco	2005		10.1007/11795490_21	time complexity;stationary process;black hole;internationalization and localization;search problem;computer science;wireless network;calculus;mobile agent;mobile computing;approximation algorithm;algorithm	Theory	17.859818831486603	33.84850544756576	145441
8058d0fe2812b0768dec6f430ece5f7a74d979f5	a transfer function model for ternary switching logic circuits	vectors circuit simulation logic circuits transfer function matrices;transfer function matrices;multiple valued logic transfer function ternary logic;logic circuits;single vector matrix product calculation transfer function model ternary switching logic circuit map vector space representation logic switching model transfer matrix logic network simulation;vectors logic gates transfer functions switches multivalued logic mathematical model equations;circuit simulation;vectors;transfer function;ternary logic;multiple valued logic	Ternary switching functions are formulated as transformations over vector spaces resulting in a characterization in the form of a transfer function. Ternary logic constants are modeled as vectors, thus the transfer functions are of the form of matrices that map vectors representing logic network input values to corresponding output vectors. Techniques for determination of the transfer matrix from a logic switching model or directly from a netlist are provided. The use of transfer matrices for logic network simulation are then developed that allow for multiple output responses to be obtained through a single vector-matrix product calculation.	function model;matrix multiplication;netlist;simulation;three-valued logic;transfer function;transfer matrix	Mitchell A. Thornton	2013	2013 IEEE 43rd International Symposium on Multiple-Valued Logic	10.1109/ISMVL.2013.10	electronic engineering;discrete mathematics;logic optimization;diode–transistor logic;logic level;logic gate;mathematics;sequential logic;function block diagram;transfer function;digital electronics;algorithm	Arch	19.31745306403453	45.21237966660343	145710
b364a5e4640a86a4231dd3849e508b981c897d00	enhancing counting bloom filters through huffman-coded multilayer structures	compressed data structure;memory requirement;hierarchical structure;sram chips huffman codes;random access memory;evasion;complexity theory;memory management;multilayer structure;bloom filter;on chip sram;huffman coding;radiation detectors;counting bloom filter;network processor;network processor counting bloom filter cbf evasion hashing huffman coding multilayer structure;network processor counting bloom filter huffman coded multilayer structure randomized data structure false positive probability dynamic set memory requirement overflow probability huffman code limited memory consumption memory saving compressed data structure on chip sram;huffman codes;chip;upper bound;huffman code;memory saving;hashing;nonhomogeneous media;data structures;counting bloom filter cbf;false positive probability;randomized data structure;limited memory consumption;huffman coded multilayer structure;false positive;overflow probability;dynamic set;data structure;filters nonhomogeneous media data structures counting circuits upper bound code standards application software huffman coding data processing computer applications;sram chips	"""Bloom Filters are efficient randomized data structures for membership queries on a set with a certain known false positive probability. Counting Bloom Filters (CBFs) allow the same operation on dynamic sets that can be updated via insertions and deletions with larger memory requirements. This paper first presents a simple tight upper bound for counters overflow probability in CBFs, which is adopted in the design of more efficient CBFs. On the basis of such theoretical achievements, we introduce the idea of a hierarchical structure as well as the use of Huffman code to improve standard CBFs in terms of fast access and limited memory consumption (up to 50% of memory saving). The target could be the implementation of the compressed data structures in the small (but fast) local memory or """"on-chip SRAM"""" of devices such as network processors. As an application of our algorithms, an anti-evasion system is finally proposed."""	bloom filter;central processing unit;compressed data structure;data compression;evasion (network security);huffman coding;network processor;randomized algorithm;requirement;static random-access memory	Domenico Ficara;Andrea Di Pietro;Stefano Giordano;Gregorio Procissi;Fabio Vitucci	2010	IEEE/ACM Transactions on Networking	10.1109/TNET.2010.2055243	parallel computing;data structure;computer science;theoretical computer science;algorithm;huffman coding	EDA	10.13960661774657	37.21740957031731	145726
c8338c0499285bb985ff49754ea54a6e8913888c	boolean resubstitution with permissible functions and binary decision diagrams	multilevel logic synthesis boolean resubstitution permissible functions ordered binary decision diagrams multilevel logic optimization don t care sets data structure logic functions node boolean networks;minimization;network synthesis;boolean functions;optimization technique;boolean resubstitution;multilevel logic optimization;logic cad boolean functions;logic circuits;data structures boolean functions logic functions circuit synthesis logic testing minimization logic circuits circuit testing network synthesis artificial intelligence;ordered binary decision diagram;logic synthesis;boolean networks;permissible functions;data structures;ordered binary decision diagrams;boolean network;logic functions;logic testing;artificial intelligence;circuit testing;don t care sets;multilevel logic synthesis;logic cad;data structure;node;circuit synthesis;binary decision diagram	In this paper, we present a new Boolean resubstitution technique with permissible functions and ordered binary decision diagrams, abbreviated as OBDD[8]. Boolean resubstitution is one technique for multi-level logic optimization. Permissible functions are special don't care sets. We represent the data structure of permissible functions and logic functions at each node in Boolean networks in terms of OBDD. Therefore, logic functions can be flexibly manipulated and rapidly executed. We have previously reported a multi-level logic optimization technique called transduction methods[1] using OBDD in ICCAD'89[7]. We have improved the OBDD operation techniques, so that now OBDD operations can be executed faster than we reported before. We also applied Boolean resubstitution to our multi-level logic synthesis. We present results of experiments employing the improved OBDD operation techniques and applying Boolean resubstitution to our multi-level logic synthesis.	binary decision diagram;boolean algebra;boolean network;data structure;don't-care term;experiment;logic optimization;logic synthesis;mathematical optimization;transduction (machine learning)	Hitomi Sato;Yoshihiro Yasue;Yusuke Matsunaga;Masahiro Fujita	1990		10.1145/123186.123276	network synthesis filters;boolean circuit;discrete mathematics;logic synthesis;boolean network;data structure;boolean expression;logic gate;product term;computer science;theoretical computer science;mathematics;boolean function;node;programming language;binary decision diagram;algorithm	EDA	18.314872416709452	46.26358439627961	145867
65e8c895986a6e84b481161f5a4c4604a2544557	new power-of-2 rns scaling scheme for cell-based ic design	integrated circuit;longueur mot;residue number systems;cathode tube;correlators;cell based ic design;conception circuit integre;cell based integrated circuit cbic design;integrated circuit design;chinese remainder theorem;asic synthesis power of 2 rns scaling scheme cell based ic design residue number system positional number system cell based integrated circuit;scaling;word length;application specified integrated circuit asic synthesis;application specific integrated circuits;signal processing;residue number system rns scaling;asic synthesis;longitud palabra;dynamic range;vlsi;arithmetic;digital signal processing chips;integrated circuit synthesis;residue number system;circuit cad;cell based integrated circuit;signal synthesis;circuit cad residue number systems vlsi application specific integrated circuits integrated circuit design;positional number system;tubo catodico;residue number system rns;tube cathodique;cathode ray tubes;sensor arrays;power of 2 rns scaling scheme;arithmetic integrated circuit synthesis sensor arrays application specific integrated circuits dynamic range cathode ray tubes signal synthesis signal processing digital signal processing chips correlators	Previous scaling schemes are based on the conversion of the unpositional residue number system (RNS) digits into a positional number system via Chinese remainder theorem (CRT) or mixed-radix-conversion (MRC) and the back conversion into RNS with an associated size and speed penalty in cell-based integrated circuit (CBIC) designs. This paper presents a new scaling approach, which allows faster and more efficient schemes, because the scaling uses only RNS operations within the small word length channels.	cathode ray tube;image scaling;integrated circuit;polynomial remainder theorem;residue number system;standard cell	Uwe Meyer-Bäse;Thanos Stouraitis	2003	IEEE Trans. VLSI Syst.	10.1109/TVLSI.2003.810799	cathode ray tube;embedded system;electronic engineering;computer science;electrical engineering;signal processing;mathematics	Arch	13.702191019073723	44.72935885087632	146034
700a6d1527cad60e2f7a11ae88861a105d176861	vlsi-oriented lossy image compression approach using da-based 2d-discrete wavelet		In this paper, we introduced a Discrete Wavelet Transform (DWT) based VLSI-oriented lossy image compression approach, widely used as the core of digital image compression. Here, Distributed Arithmetic (DA) technique is applied to determine the wavelet coefficients, so that the number of arithmetic operation can be reduced substantially. As well, the compression rate is enhanced with the aid of introducing RW block that blocks some of the coefficients obtained from the high pass filter to zero. Subsequently, Differential Pulse-Code Modulation (DPCM) and huffman-encoding are applied to acquire the binary sequence of the image. The functional simulation of each module is presented as well as the performance of each module is widely analyzed with gate required, clock cycles required, power, processing rate, and processing time. From the analysis, it is found that the DCM module requires more gates to do the transformation process compared to other modules. Eventually, the proposed compression approach is compared with the existing methods in terms of processor area and power. Comparative result shows that the proposed method offers good performance in power-efficiency corresponding to 0.328 mW/chip than the prior methods.	algorithm;bitstream;clock signal;coefficient;computation;dicom;digital image;discrete wavelet transform;hardware description language;huffman coding;image compression;lenna;logic simulation;lossy compression;modulation;qualitative comparative analysis;read-write memory;verilog;very-large-scale integration	Devangkumar Shah;Chandresh Vithlani	2014	Int. Arab J. Inf. Technol.		data compression;data compression ratio;image compression;theoretical computer science;lossless compression;algorithm	EDA	12.774939754827505	42.33339990610779	146177
069188077a9a686461874181686a46dcb2f90c37	optimal simulation of meshes with dynamically separable buses by meshes with statically partitioned buses	multiprocessor interconnection networks;time complexity mesh optimal simulation statically partitioned buses meshes with separable buses meshes with multiple partitioned buses mesh connected computers broadcasting buses bus segments program control;information systems;time complexity;very large scale integration;program control;mesh optimal simulation;statically partitioned buses;meshes with separable buses;system buses;upper bound;meshes with multiple partitioned buses;computational modeling;parallel architectures;computational complexity;bus segments;broadcasting partitioning algorithms information systems computational modeling very large scale integration upper bound parallel architectures;broadcasting buses;broadcasting;multiprocessor interconnection networks system buses computational complexity;mesh connected computers;partitioning algorithms	This paper studies the simulation problem of meshes with separable buses (MSB) by meshes with multiple partitioned buses (MMPB). The MSB and the MMPB are the mesh-connected computers enhanced by the addition of broadcasting buses along every row and column. The broadcasting buses of the MSB, called separable buses, can be dynamically sectioned into smaller bus segments by program control, while those of the MMPB, called partitioned buses, are statically partitioned in advance. In the MSB model, each row/column has only one separable bus, while in the MMPB model, each row/column has L partitioned buses (L /spl ges/ 1). We present an algorithm that simulates the MSB of size n /spl times/ n in O (n/sup 1/(2L+1)/) steps using the MMPB of size n /spl times/ n, and prove that the time-complexity of the algorithm is optimal in the worst case.	algorithm;best, worst and average case;computer;simulation;time complexity	Susumu Matsumae	2004	7th International Symposium on Parallel Architectures, Algorithms and Networks, 2004. Proceedings.	10.1109/ISPAN.2004.1300524	parallel computing;real-time computing;computer science;distributed computing	Arch	14.103682433910125	33.498531227814034	146246
d4a919225433547bb97ab8a129dbd4cffa15749e	cycle embedding on the m ¨ obius cube with both faulty nodes and faulty edges	multiprocessor interconnection networks;multiprocessor interconnection networks network topology computer science hypercubes length measurement terminology;length measurement;network topology;hypercubes;terminology;computer science;hamiltonian path	A graph G = (V,E) is said to be pancyclic if it contains fault-free cycles of all lengths from 4 to |V | in G. Let Fv and Fe be the sets of faulty nodes and faulty edges of an n-dimensional M¨obius cube MQn, respectively, and let F = Fv U Fe. In this paper, we show that MQn - F contains a fault-free Hamiltonian path when |F| \le n - 1 and n \ge 1. We also show that MQn - F is pancyclic when |F| \le n - 2 and n \ge 2. Since MQn is regular of degree n, both results are optimal in the worst case.	best, worst and average case;cube;field electron emission;graph (discrete mathematics);hamiltonian path;interaction picture	Sun-Yuan Hsieh;Nai-Wen Chang	2005	11th International Conference on Parallel and Distributed Systems (ICPADS'05)	10.1109/ICPADS.2005.119	hamiltonian path;length measurement;computer science;distributed computing;terminology;network topology;hypercube;computer network	Theory	23.622960815691066	33.93382449826444	146746
e2e21d39b1e6680a22ef971dcdcbf596b92fa84a	logic synthesis of multi-output functions for pal-based cplds	graph theory;programmable logic devices;boolean functions;minimisation of switching nets;boolean function;logic synthesis;digital circuit outputs multi level synthesis logic synthesis multi output functions multi output implicants pal based cplds multi output boolean function paldec system graph nodes;circuit synthesis minimization methods logic functions feedback joining processes;technology mapping;digital circuits;logic cad;minimisation of switching nets programmable logic devices logic cad boolean functions graph theory	In this paper multi-level synthesis for PAL-based CPLDs is presented. The essence of the method is to search for multi-output implicants that can be shared by several functions. This approach presents a unique form for illustrating a minimized form of a multi-output Boolean function. The presented method, implemented within the PALDec system, is based on the analysis of graph nodes that represent states of a digital circuit outputs. The results of synthesis for benchmarks are compared to the classical technology mapping method.	logic synthesis;pal	Dariusz Kania	2002		10.1109/FPT.2002.1188727	boolean algebra;boolean circuit;and-inverter graph;circuit minimization for boolean functions;logic synthesis;logic optimization;logic level;boolean expression;logic gate;logic family;programmable logic array;computer science;graph theory;theoretical computer science;programmable logic device;sequential logic;combinational logic;boolean function;digital electronics;register-transfer level;algorithm	Logic	18.861248364420593	46.07524316114956	146882
89ec927b71ee9f015c595398e1c34fd42589a60b	distributed computing on anonymous hypercube networks	hypercube;fonction booleenne;algorithm complexity;boolean functions;efficient algorithm;complejidad algoritmo;distributed computing;boolean function;distributed computer systems;automorphism;complexite algorithme;complex i;systeme informatique reparti;automorfismo;automorphisme;hipercubo	Ž . We consider the bit-complexity i.e.a, total number of bits transmitted of computing boolean functions on an anonymous canonically labeled n-dimensional hypercube network and give a characterization of the boolean functions computable on such a network as exactly those boolean functions which are invariant under all bit-complement automorphisms of the hyercube. We provide an efficient Ž 4 . algorithm for computing all such functions with bit complexity O N ? log N . For the case of symmetric boolean functions we give an algorithm with bit complexity Ž 2 . O N ? log N . Q 1997 Academic Press	.bit;algorithm;bitwise operation;central processing unit;computability;computable function;computation;computational complexity theory;context of computational complexity;distributed computing;magma;polynomial;time complexity	Evangelos Kranakis;Danny Krizanc	1997	J. Algorithms	10.1006/jagm.1996.0817	mathematical optimization;combinatorics;discrete mathematics;boolean network;theoretical computer science;mathematics;boolean function;algorithm	Theory	21.41009972053163	35.042042954675054	146883
4008acc3d334a9c877e83076ae440bb8ddfb2a16	new systolic arrays for the longest common subsequence problem	systolic arrays;systolic array;longest common subsequence;parallel algorithms	Abstract   New systolic arrays are presented to solve the longest common subsequence (LCS) problem. The first array is based on a previous work to compute the length of an LCS. The second array improves on the first one. The second array is about   1  4   faster than the first array, and each processing element (PE) of the second array requires one less port for both input and output to transfer one less value. Either of the two arrays can be equipped with a content-addressable memory in each PE to recover an LCS. The resulting array requires fewer steps than the previously proposed one.	longest common subsequence problem	Yen-Chun Lin	1994	Parallel Computing	10.1016/0167-8191(94)90040-X	parallel computing;real-time computing;systolic array;computer science;longest common subsequence problem;mathematics;parallel algorithm;algorithm	HPC	12.690739822874624	33.282521521269445	146995
0c00fd66111db2c8c2bed064844e17cc5d0d84b1	the diagnosability of hypercubes with arbitrarily missing links	tolerancia falta;graph theory;distributed system;hypercube;teoria grafo;architecture systeme;systeme reparti;fault tolerant;multiprocessor;sistema informatico;diagnostico;computer system;diagnosability;theorie graphe;sistema repartido;liaison panne;multiprocessor networks;link failure;fault tolerance;hypercubes;arquitectura sistema;systeme informatique;multiprocesador;system architecture;diagnosis;diagnostiquabilite;tolerance faute;diagnostic;multiprocesseur;hipercubo	We study the problem of determining diagnosability for incomplete hypercubes that have arbitrarily distributed missing links, under the classic PMC diagnostic model and its variant, the BGM model. Based on the result proved in this paper, for both models, in most cases the diagnosability of an incomplete hypercube can be determined by simply checking the link degree of each node. Ó 2000 Elsevier Science B.V. All rights reserved.	algorithm;bayesian network;libor market model;olap cube;time complexity;whole earth 'lectronic link	Dajin Wang	2000	Journal of Systems Architecture	10.1016/S1383-7621(99)00015-6	fault tolerance;real-time computing;computer science;graph theory;distributed computing;algorithm;hypercube;systems architecture	AI	23.345656738409517	34.29636087772856	147080
42534f8f9046380834e0d5e836a08fd777023bcf	a new survivor memory management method in viterbi decoders: trace-delete method and its implementation	hdtv survivor memory management method viterbi decoders control logic vlsi implementation trace delete method tdm latency memory element numbers block interleaving implementation;memory element numbers;memory management;tdm;decoding;multiplication operator;video signal processing;control logic;very large scale integration;implementation;integrated memory circuits;block interleaving;logic;survivor memory management method;television receivers;memory management viterbi algorithm decoding very large scale integration logic time division multiplexing delay energy consumption hardware interleaved codes;chip;interleaved codes;last in first out;viterbi decoders;energy consumption;viterbi algorithm;trace delete method;viterbi decoder;hdtv;vlsi;storage management chips;digital signal processing chips;vlsi implementation;latency;time division multiplexing;power consumption;digital signal processing chips vlsi viterbi decoding storage management chips integrated memory circuits high definition television television receivers video signal processing;high performance;high speed;viterbi decoding;high definition television;hardware	The well known methods for survivor path storage and decoding are the register-exchange method (REM) and the trace-back method (TBM). The REM is conceptually simple, but it is not appropriate for VLSI implementation because it requires large power consumption and large chip area. The TBM is the preferred method in the VLSI implementation of Viterbi decoders (VD) having large constraint length and high performance. However, the TBM requires last-in-first-out (LIFO) buffer and has to use multiple read operations for high speed operation. This multiple operation results in complex control logic. In this paper, we propose a new survivor memory management method called trace-delete method (TDM) and realize this algorithm in hardware (H/W) for VLSI implementation and we compare the TDM with the TBM in terms of latency, the number of memory elements, and the requirements of control logic. The main advantage of the proposed method can be found as short latency and less requirements on additional control logic. Especially, if we combine the TDM with block interleaving the implementation is even simpler than the TBM. The method is studied with particular relevance to HDTV.	memory management;viterbi algorithm	Suk-Jin Jung;Myeong-Hwan Lee;Hyung-Jin Choi	1996		10.1109/ICASSP.1996.550578	parallel computing;real-time computing;computer science;very-large-scale integration;viterbi decoder;time-division multiplexing;statistics	Logic	12.761028568986635	40.10127553987276	147531
39f8b1ede8b31e2a9528131c4873678bb7b05aa0	on designing a network to defend against random attacks of radius two	graph theory;optimisation;fiabilidad;reliability;teoria grafo;architecture systeme;optimizacion;telecommunication network;conception;theorie graphe;connected graph;red telecomunicacion;fiabilite;reseau telecommunication;diseno;design;arquitectura sistema;optimization;system architecture;graphe connexe;grafico connexo	Abstract#R##N##R##N#This paper considers the following variation on the construction of a reliable communication network. Whenever a vertex is attacked, all vertices within distance 2 are also destroyed (or fail) indirectly. We are interested in designing a connected graph (undirected, all edges of length one) on p vertices such that when a random subset of the vertices are attacked the expected number of vertices that are destroyed (directly and indirectly) is minimized. It is assumed that any of the 2p subsets of vertices is equally likely to be attacked. The optimal structure is determined for all p and is shown to be one of five patterns depending on r where p = 5t + r.		Art S. Finbow;Bert Hartnell	1989	Networks	10.1002/net.3230190704	design;combinatorics;discrete mathematics;graph center;graph theory;balinski's theorem;reliability;path graph;mathematics;distance;neighbourhood;algorithm	Crypto	23.530316625920115	33.61966201808151	147552
57646f90f800c9a85886bb71d3336e7045eecb91	dsp implementation of deblocking filter for avs	digital signal processors;high resolution;digital signal processing adaptive filters kernel signal processing algorithms parallel processing digital signal processors pipelines software design assembly videos;image resolution;video signal processing;video signal processing digital signal processing chips filtering theory image resolution parallel processing pipeline processing;real time;indexing terms;deblocking filter;efficient implementation;avs;audio video standard digital signal processor platform in loop deblocking filter parallel processing high resolution video;pipelines;digital signal processor;digital signal processing chips;software pipelining;functional unit;avs pipelines digital signal processors deblocking filter;parallel processing;filtering theory;pipeline processing	The in-loop deblocking filter contains highly adaptive processing on both sample level and block edge level, which inevitably appears in the loop kernel of the algorithm. Therefore it is a challenge for parallel processing on a digital signal processor (DSP) platform. In this paper, pipelined DSP solutions to the in-loop deblocking filter in AVS1-P2 are presented. First, the whole filter process is divided into six sub-processes, so that the global filter structure can be improved to achieve regular processing flow. Then software pipelines are designed for these sub-processes, with elaborately allocating functional units and carefully choosing enhanced assembly instructions based on the DSP platform. The simulated results show that this efficient implementation can easily support real-time filter processing for high resolution videos.	algorithm;deblocking filter;digital signal processor;image resolution;parallel computing;pipeline (software);real-time clock;signal processing	Zhigang Yang;Wen Gao;Yan Liu;Debin Zhao	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379557	adaptive filter;parallel processing;computer vision;digital signal processor;parallel computing;real-time computing;image resolution;computer hardware;computer science;deblocking filter;filter;filter design	Robotics	11.778341338937818	40.409267459805875	147808
1935cbff17bc4cc5cd4a79f787011043620629a2	topology-hiding computation on all graphs		A distributed computation in which nodes are connected by a partial communication graph is called topology-hiding if it does not reveal information about the graph beyond what is revealed by the output of the function. Previous results have shown that topology-hiding computation protocols exist for graphs of constant degree and logarithmic diameter in the number of nodes [Moran-OrlovRichelson, TCC’15; Hirt et al., Crypto’16] as well as for other graph families, such as cycles, trees, and low circumference graphs [Akavia-Moran, Eurocrypt’17], but the feasibility question for general graphs was open. In this work we positively resolve the above open problem: we prove that topology-hiding computation is feasible for all graphs under either the Decisional Diffie-Hellman or QuadraticResiduosity assumption. Our techniques employ random-walks to generate paths covering the graph, upon which we apply the Akavia-Moran topology-hiding broadcast for chain-graphs (paths). To prevent topology information revealed by the random-walk, we design multiple random-walks that, together, are locally identical to receiving at each round a message from each neighbors and sending back processed messages in a randomly permuted order.	computation;decisional diffie–hellman assumption;diffie–hellman key exchange;distributed computing;randomness;thomas p. moran	Adi Akavia;Rio LaVigne;Tal Moran	2017		10.1007/978-3-319-63688-7_15	discrete mathematics;pathwidth;chordal graph;1-planar graph;computer science;indifference graph;partial k-tree;modular decomposition;dense graph;graph product	Theory	18.633135167134242	33.99106516698191	148168
9320df92b5e769c9308e6ee31b7a3336a25f74ac	a novel decision diagrams extension method	multi state multi valued decision diagram;decision diagrams extension method;binary decision diagram	Binary decision diagram (BDD) is a graph-based representation of Boolean functions. It is a directed acyclic graph (DAG) based on Shannon׳s decomposition. Multi-state multi-valued decision diagram (MMDD) is a natural extension of BDD for the symbolic representation and manipulation of the multi-valued logic functions. This paper proposes a decision diagram extension method based on original BDD/MMDD while the scale of a reliability system is extended. Following a discussion of decomposition and physical meaning of BDD and MMDD, the modeling method of BDD/MMDD based on original BDD/MMDD is introduced. Three case studies are implemented to demonstrate the presented methods. Compared with traditional BDD and MMDD generation methods, the decision diagrams extension method is more computationally efficient as shown through the running time.	decision theory;diagram;extension method	Shumin Li;Shubin Si;Hongyan Dui;Zhiqiang Cai;Shudong Sun	2014	Rel. Eng. & Sys. Safety	10.1016/j.ress.2014.01.017	discrete mathematics;theoretical computer science;mathematics;binary decision diagram;algorithm	SE	18.634866026643	45.74167375153345	148631
6089a5853a75df7b053d6ddde2c3a10f447fe65c	wire packing: a strong formulation of crosstalk-aware chip-level track/layer assignment with an efficient integer programming solution	noise minimization;chip;shielding;global routing;net ordering;vlsi design automation;integer program;integer linear program	By focusing on chip-wide slices of the global routing grid, making a few mild geometric assumptions about layer use, and suitably abstracting pin details, we derive an extremely effcient integer linear programming (ILP)formulationfor track/layer assignment. They key technical insight is to model all constraints-both geometric and crosstalk--as cliques in an appropriate conflict graph; these cliques can be extracted quickly from the interval structure of wires in a slice. We develop a “strong” linear relanation of this problem that almost always yields the integral optimum; this solution gives us directly the maximum number of wires that can pack legally without crosstalk risk. Experiments on synthetic netlists that match statistics of wire layouts from industrial 0.25um designs demonstrate that we can pack 100 1000 wires optimally, or with at worst a very few over$ows, in seconch.	clique (graph theory);crosstalk;integer programming;linear programming;linear programming relaxation;routing;serializability;set packing;synthetic data;wire wrap	Rony Kay;Rob A. Rutenbar	2000		10.1145/332357.332375	chip;mathematical optimization;theoretical computer science;electromagnetic shielding;mathematics	EDA	22.390231300546972	42.122870376881835	148985
ace6b97635cd6619df8934e792eb0d34fe244549	minimizing and-exor expressions for two-variable multiple-valued input binary output functions				Takaaki Mizuki;Hitoshi Tsubata;Takao Nishizeki	2010	Multiple-Valued Logic and Soft Computing		exor;mathematical optimization;computer science;expression (mathematics);binary number	Logic	21.20412860249658	41.37944702779572	149091
5655b7bc0742272eb8ee45a64e696450d73f6368	bundled data asynchronous multipliers with data dependent computation times	digital signal processing;rails;multiplying circuits;logic design;area time savings;asynchronous logic data dependent performance multiplier;very large scale integration;delay effects;data mining;area time savings data asynchronous multipliers asynchronous design data dependent computation time 16 spl times 16 bit multiplier;asynchronous design;physics computing;logic design multiplying circuits computational complexity asynchronous circuits;design method;data dependence;computational complexity;adders;data asynchronous multipliers;asynchronous circuits;circuits;data dependent computation time;circuits logic design delay effects adders physics computing australia data mining rails very large scale integration digital signal processing;australia;16 16 bit multiplier	A novel asynchronous design method is introduced which combines the area eflciency of bundled data with data dependent computation time. The design of a 16x16 bit multiplier using this technique is explained and evaluated. Simulation results show that area time savings of 20% compared to an equivalent synchronous design can be achieved.	asynchronous circuit;computation;simulation;synchronous circuit;time complexity	David A. Kearney;Neil W. Bergmann	1997		10.1109/ASYNC.1997.587174	electronic engineering;real-time computing;computer science;theoretical computer science	EDA	13.450399198383199	45.48542920084658	149589
5a6986e1fe8f8076f25826c889e457cf044aec8e	sff - the single-stream fpga-optimized feedforward fft hardware architecture	fast fourier transform (fft);field-programmable gate arrays (fpgas);pipeline fft;fpga optimization;single-stream fft	In this paper, a fast Fourier transform (FFT) hardware architecture optimized for field-programmable gate-arrays (FPGAs) is proposed. We refer to this as the single-stream FPGA-optimized feedforward (SFF) architecture. By using a stage that trades adders for shift registers as compared with the single-path delay feedback (SDF) architecture the efficient implementation of short shift registers in Xilinx FPGAs can be exploited. Moreover, this stage can be combined with ordinary or optimized SDF stages such that adders are only traded for shift registers when beneficial. The resulting structures are well-suited for FPGA implementation, especially when efficient implementation of short shift registers is available. This holds for at least contemporary Xilinx FPGAs. The results show that the proposed architectures improve on the current state of the art.	fast fourier transform;feed forward (control);feedforward neural network;field-programmability;field-programmable gate array;shift register;smart data compression	Carl Ingemarsson;Oscar Gustafsson	2018	Signal Processing Systems	10.1007/s11265-018-1370-y	field-programmable gate array;parallel computing;architecture;fast fourier transform;hardware architecture;feed forward;signal processing;computer science;shift register;adder	Arch	11.898482078572684	44.534982782279336	149856
93a567fa11532e05434caac5c85c11e004522be7	multilevel reverse-carry addition: single and dual adders	dual adders;vlsi design;prefix adders;computer arithmetic;most significant carry detection	The multilevel reverse-carry approach has been proposed previously for fast computation of the most-significant carry of an adder. In this paper, this approach is extended to generate several carries and is applied to the implementation of single and dual adders. Specifically, the operands are split into blocks and each block is added to produce the sum and the sum plus one. Concurrently with these additions the multilevel reverse-carry approach is used to generate the input carries of these blocks. Finally, these carries are used to select among the sum and the sum plus one.#R##N##R##N#The delay and complexity of the resulting architecture for a 64-bit adder has been estimated, considering the load introduced by long connections, resulting in a reduction of about 15p in the critical path delay and comparable complexity with respect to traditional implementations of prefix-tree based adders.		Javier D. Bruguera;Tomás Lang	2002	VLSI Signal Processing	10.1023/A:1021141818191	arithmetic;parallel computing;computer science;electrical engineering;mathematics;very-large-scale integration;carry-save adder;algorithm;adder	ML	12.986869279445346	45.914361914476025	150131
19379af3a0c5a6b9a9b433dcda72f9430f97f835	optimal algorithms for total exchange without buffering on the hypercube	hypercube	Two methods are given for constructing total exchange algorithms for hypercubic processor networks. This is done by means of bit sequences with special properties. The algorithms are optimal with respect to a given time model, need no intermediate message buuering and are local in the sense that every processor executes basically the same program.	algorithm	Kris Coolsaet;Hans De Meyer;Veerle Fack	1992	BIT		parallel computing;computer science;theoretical computer science;mathematics;distributed computing;hypercube	Theory	14.254286229857543	34.63509173309992	150290
280dd5a78d11f153cced5793136870c1a05d058d	a family of hamiltonian and hamiltonian connected graphs with fault tolerance	tolerancia falta;hypercube;estructura mobius;mobius structure;fault tolerant;cube;cubo;connected graph;hamiltonian graph;hamiltonicity;fault tolerant system;generalized hypercube;fault tolerance;sistema tolerando faltas;grafo hamiltoniano;structure mobius;systeme tolerant les pannes;graphe hamiltonien;hamiltonian connectivity;article;graphe connexe;tolerance faute;grafo conexo;hipercubo	Processor (vertex) faults and link (edge) faults may happen when a network is used, and it is meaningful to consider networks (graphs) with faulty processors and/or links. A k-regular Hamiltonian and Hamiltonian connected graph G is optimal fault-tolerant Hamiltonian and Hamiltonian connected if G remains Hamiltonian after removing at most k−2 vertices and/or edges and remains Hamiltonian connected after removing at most k−3 vertices and/or edges. In this paper, we investigate in constructing optimal fault-tolerant Hamiltonian and optimal fault-tolerant Hamiltonian connected graphs. Therefore, some of the generalized hypercubes, twisted-cubes, crossed-cubes, and Möbius cubes are optimal fault-tolerant Hamiltonian and optimal fault-tolerant Hamiltonian connected.	central processing unit;connectivity (graph theory);fault tolerance;hamiltonian (quantum mechanics);hamiltonian path;interconnection;olap cube;recursion;twisted;vertex (geometry)	Y-Chuang Chen;Yong-Zen Huang;Lih-Hsing Hsu;Jimmy J. M. Tan	2009	The Journal of Supercomputing	10.1007/s11227-009-0316-3	hamiltonian path;fault tolerance;hamiltonian path problem;indifference graph;adiabatic quantum computation	Theory	24.019973672595345	34.22626974378212	150639
5a5fa94132621bb0fbd173c3e9fbd4bdba896be2	quasi-algebraic decompositions of switching functions	benchmark circuits quasi algebraic decompositions switching functions algebraic product testability binary boolean operation canonical manner ssl testable circuit size;minimisation of switching nets switching functions logic testing state assignment;state assignment;minimisation of switching nets;ssl testable;quasi algebraic decompositions;testability;canonical manner;boolean operation;algebraic product;benchmark circuits;necessary and sufficient condition;logic testing;switching functions;binary boolean operation;circuit size;circuit testing benchmark testing switching circuits taxonomy	Brayton and others have developed a rich theory of decomposition of switching functions based on algebraic manipulations of monomials. In this theory, a product g(XJ h(X,) is algebraic if XR n X,, = 0 . There are efJicient methods for determining fafunction has an algebraic product. I f a function does not have an algebraic product, then there are good methods for obtaining a decomposition of the form f = g . h + r where g . h is an algebraic product. Algebraic decompositions have the desirable properties that they are canonical and preserve testability. In this paper we generalize the concept of an algebraic product to decompositions of the form f ( X ) = g(X,) 0 h(Xh) where 0 is any binary Boolean operation and IXg n X,l = k for some k 2 0 . We call these decompositions quasi-algebraic decompositions. We begin by showing that we may restrict ourselves to the case where 0 is + (sum), . (product) or 0 (exclusive-or). We then give necessary and sufjcient conditions for a function to have a quasi-algebraic decomposition for a given X , and X , . Ifafunction has such a decomposition, we show how to determine the functions g and h in a canonical mannel: We also show that these decompositions are fully SSL testable. Finally, using standard benchmark circuits, we show that quasi algebraic decompositions occur often and are useful in reducing circuit size.	algebraic equation;benchmark (computing);exclusive or;linear algebra;monomial;software testability	Ted Stanion;Carl Sechen	1995		10.1109/ARVLSI.1995.515632	combinatorics;discrete mathematics;mathematics;algorithm	Theory	19.358182874056034	45.54748270736352	151407
2d569f9500288b1f94b9a55bcbdca529af915840	the 1-good-neighbor conditional diagnosability of some regular graphs		The g-good-neighbor conditional diagnosability is the maximum number of faulty vertices a network can guarantee to identify, under the condition that every fault-free vertex has at least g fault-free neighbors. In this paper, we study the 1-good-neighbor conditional diagnosabilities of some general k-regular k-connected graphs G under the PMC model and the MM* model. The main result t1(G)=2k−l−1 under some conditions is obtained, where l is the maximum number of common neighbors between any two adjacent vertices in G. Moreover, the following results are derived: t1(HSn)=2n−1 for the hierarchical star networks, t1(Xn)=2n−1 for the BC networks, t1(AGn)=4n−10 for the alternating group graphs AGn.		Mei-Mei Gu;Rong-Xia Hao;Ai-Mei Yu	2017	Journal of Interconnection Networks	10.1142/S0219265917410018	alternating group;discrete mathematics;distributed computing;vertex (geometry);computer science;regular graph;star network;graph	Theory	23.490367151738422	34.1388798902529	151449
402a230eff8b4309f888c42f61d87009241d3c4d	a parallel picture processing machine	sequential machines;digital picture processing;program design;three dimensional;digital picture processing feature extraction local operations parallel processing sequential machines;feature extraction;parallel processing;local operations	A parallel picture processing machine (PPM) is presented. The proposed machine can perform local operations of both logical and arithmetical character on three by three neighborhoods of digitized pictures. It is essentially a two-dimensional machine but in a restricted sense it can be regarded as three-dimensional. The instruction repertoire is thoroughly discussed and several program examples are presented. In thelast section the implementation of the machine is described and v various tradeoffs between speed and economy are discussed. A dynamically programmable microprogram store is shown to give great freedom in program design.	3d computer graphics;image;microcode	Björn Kruse	1973	IEEE Transactions on Computers	10.1109/T-C.1973.223653	three-dimensional space;parallel processing;parallel computing;feature extraction;computer science;theoretical computer science;machine learning;program design language;programming language;algorithm	Arch	10.694145958308047	36.54918321892262	151970
80005a6253008ac55d27c002f11d4f0646a73e7d	pipelined parallel fft architecture		ABSTARCT: In this paper, an optimized efficient VLSI architecture of a pipeline Fast Fourier transform (FFT) processor capable of producing the reverse output o rder sequence is presented. Paper presents Radix-2 multipath delay architecture for FFT calculation. The implementation of FFT in hardware is very critical because for calculation of FFT number of butterfly operations i.e. number of multipliers requires due to which hardware gets increased means indirectly cost of hardware is automatically gets increased. Also multiplier operations are slow that's why it limits the speed of operation of architecture. The optimi zed VLSI implementation of FFT algorithm is presented in this paper. Here architecture is pipelined to optimize it and to increase the speed of operation. Also to increase the speed of operation 2 levels p arallel processing is used.	algorithm;fast fourier transform;multipath propagation;parallel computing;pipeline (computing);very-large-scale integration	Tanaji U. Kamble;B. G. Patil;Rakhee S. Bhojakar	2013	CoRR		architecture;parallel computing;multipath propagation;very-large-scale integration;fast fourier transform;computer science;parallel processing	Arch	12.10643510711046	44.654967581947204	152144
1f2598c7122d0409ab6a01b1ab02b2b4d4256fca	reciprocal unit based on vedic mathematics for signal processing applications	cmos integrated circuits;propagation delay power demand switches approximation methods computers delays;reciprocal;signal processing approximation theory cmos integrated circuits digital arithmetic;approximation theory;sahayaks;signal processing;vedic mathematics;iteration;size 90 nm vedic mathematics unique mathematical computation technique sutras high speed reciprocal unit ancient mathematics sahayaks auxiliary fraction practical signal processing applications vedic formulae reciprocal approximation propagation delay dynamic switching power consumption spice spectre cmos technology 5 digit reciprocal unit newton raphson based implementation;digital arithmetic;vedic mathematics high speed iteration reciprocal sahayaks;high speed	Vedic mathematics, is an ancient methodology, has a unique mathematical computation technique based on 16 sutras (formulae). High speed reciprocal unit based on such ancient mathematics is reported in this paper. Implementation methodology was adopted through sahayaks (auxiliary fraction) taken from such ancient mathematics and prototype was designed for practical signal processing applications. On account of the Vedic formulae, reciprocal approximation of a numbers is generated in fewer steps compared to Newton-Raphson's iteration based implementation with appreciable error in accuracy (~0.09%), offer high speed operation. The functionality of the algorithm was checked, and performance parameters like propagation delay, dynamic switching power consumption were calculated through spice spectre using 90nm CMOS technology. The propagation delay of the resulting 5-digit reciprocal unit was only ~3.57uS and consumes ~30.8mW power. The implementation methodology offered substantial reduction of propagation delay, and dynamic switching power consumption from earlier reported Newton-Raphson (NR) based implementation.	algorithm;approximation;cmos;computation;iteration;newton's method;noise reduction;propagation delay;prototype;signal processing;software propagation	Prabir Saha;Deepak Kumar;Partha Bhattacharyya;Anup Dandapat	2013	2013 International Symposium on Electronic System Design	10.1109/ISED.2013.15	arithmetic;electronic engineering;mathematics;algorithm	EDA	14.611569256899504	43.91743691293483	152193
15a62ac17ead8a7729230530fda3645950aede6f	the differential cordic algorithm: constant scale factor redundant implementation without correcting iterations	iterative method;langage description materiel informatique;trigonometric function;computational complexity redundant number systems parallel architectures digital arithmetic hardware description languages;arithmetique ordinateur;signed digit;calculo vectorial;systeme redondant;carry save;computational complexity differential cordic algorithm constant scale factor redundant implementation iterative method vector rotations hyperbolic functions trigonometric functions vector rotation scaling redundant number systems parallel architectures radix 2 redundant number systems logic synthesis vhdl descriptions dcordic vhdl generator latency;radix 2;iterative algorithms;hyperbolic functions;cordic;logic;redundant number systems;hardware description languages;constant scale factor redundant implementation;algorithme;redundant system;iterative methods;computer architecture;computer arithmetic;scaling;iteraccion;parallel architectures;logic synthesis;fonction trigonometrique;computer hardware description languages;computational complexity;methode iterative;vector calculus;funcion trigonometrica;differential cordic algorithm;student members;aritmetica ordenador;iteration;algorithms;trigonometric functions;digital arithmetic;calcul vectoriel;latency;vhdl descriptions;parallel architecture;sistema redundante;dcordic vhdl generator;signal processing algorithms;iteration method;vector rotations;radix 2 redundant number systems;vector rotation;redundant number system;signal processing algorithms iterative algorithms digital arithmetic student members iterative methods parallel architectures logic delay computational complexity computer architecture;vlsi architecture	"""The CORDIC algorithm is a well-known iterative method for the efficient computation of vector rotations, and trigonometric and hyperbolic functions. Basically, CORDIC performs a vector rotation which is not a perfect rotation, since the vector is also scaled by a constant factor. This scaling has to be compensated for following the CORDIC iteration. Since CORDIC implementations using conventional number systems are relatively slow, current research has focused on solutions employing redundant number systems which make a much faster implementation possible. The problem with these methods is that either the scale factor becomes variable, making additional operations necessary to compensate for the scaling, or additional iterations are necessary compared to the original algorithm. In contrast we developed transformations of the usual CORDIC algorithm which result in a constant scale factor redundant implementation without additional operations. The resulting """"Differential CORDIC Algorithm"""" (DCORDIC) makes use of on-line (most significant digit first redundant) computation. We derive parallel architectures for the radix-2 redundant number systems and present some implementation results based on logic synthesis of VHDL descriptions produced by a DCORDIC VHDL generator. We finally prove that, due to the lack of additional operations, DCORDIC compares favorably with the previously known redundant methods in terms of latency and computational complexity."""	algorithm;cordic;iteration	Herbert Dawid;Heinrich Meyr	1996	IEEE Trans. Computers	10.1109/12.485569	embedded system;parallel computing;computer science;theoretical computer science;mathematics;iterative method;algorithm;algebra	Vision	13.413593646807378	43.97838407275238	152246
a00853b06e496b50823d10b71526fc453b178fd1	a heuristic algorithm to minimize esops for multiple-output incompletely specified functions	incompletely specified functions;exact minimization;sum of products;heuristic minimization;functional decomposition;esop;heuristic algorithm;multiple valued logic	In this work, a novel heuristic algorithm for the minimization of Exclusive-Or Sum Of Products (ESOP) expressions for multiple output incompletely specified functions is presented. An initial algorithm, DCMIN, uses functional decomposition and multiple-valued logic in order to produce almost minimal expressions for these functions. Based on DCMIN, an improved algorithm, QuickDCMIN, is proposed, which outperforms existing algorithms.	algorithm;esop;exclusive or;heuristic (computer science)	Marios Kalathas;Dimitrios Voudouris;George K. Papakonstantinou	2006		10.1145/1127908.1127990	heuristic;functional decomposition;mathematical optimization;combinatorics;discrete mathematics;computer science;mathematics;canonical normal form	EDA	20.41505274263465	44.34493354906782	152319
1f04ad853cdd6f46ca28af79256ac5190b1a8964	high-speed and low-power real-time programmable video multi-processor for mpeg-2 multimedia chip on 0.6µm tlm cmos technology	real time;reduced instruction set computing;low power electronics video codecs video coding digital signal processing chips reduced instruction set computing multimedia communication multiprocessing systems real time systems pipeline processing parallel architectures cmos digital integrated circuits high speed integrated circuits;video coding;low power;parallel architectures;image compression;0 6 micron real time programmable video multi processor low power high speed mpeg 2 multimedia chip tlm cmos technology image compression image decompression programmable architecture dvd cd rom authoring tool videophone teleconferencing systems risc processor;cmos digital integrated circuits;multimedia communication;low power electronics;video codecs;digital signal processing chips;multiprocessing systems;transform coding image coding reduced instruction set computing video compression streaming media codecs signal processing algorithms arithmetic process control cmos technology;authoring tool;high speed;high speed integrated circuits;pipeline processing;real time systems	We developed a Video Multi Processor (VMP) for image compression and decompression schemes of MPEG (especially MPEG-2) in this study. The VMP would apply to programmable architecture, various flexibilities to implement real-time image compression algorithm, and other many applications such as DVD-CD ROM authoring tool and videophone/teleconferencing systems. IO architecture of the VMP is designed for the multi-processor functionality in which uses many VMPs according to required arithmetic quantities of the system. Further, the architecture of the VMP system is simplified by processing the necessary peripheral IO system operations within the processor.	cmos;low-power broadcasting;mpeg-2;multi-core processor;real-time transcription	Seung-Min Lee;Jin-Hong Chung;Mike Myung-Ok Lee	1999		10.1109/ASPDAC.1999.759995	embedded system;reduced instruction set computing;electronic engineering;real-time computing;image compression;computer science;operating system;low-power electronics	EDA	11.183983152176195	40.9971086540024	152357
bb5799a1395e2caf346c4c098e81990aabbcd690	reduced-error constant correction truncated multiplier	digital signal processing;constant correction truncated multiplier	Constant correction truncated multiplier can minimize hardware cost and power dissipation by computing only the most significant bits of partial products. However, traditional schemes introduce large truncation errors that degenerate the accuracy of the target application. In this brief, we propose an improved scheme that can significantly reduce such truncation errors. The proposed scheme estimates an accurate compensating constant by correctly counting the probability of occurrence of the input operand and the product bits with a group of probability propagation formulas. The proposed truncated multiplier is coded in Verilog RTL, implemented in 65 nm standard cell technology, and applied in image compression and color space conversion applications. Experimental results show that our scheme achieves an average peak signal-to-noise ratio (PSNR) improvement of 4.09 dB and 2.31 dB over state-of-the-art truncation scheme in the two applications, respectively. When the same truncation error range is desired, the proposed scheme can save around 5.8% of circuit area.	16-bit;8-bit;color space;computation;computational complexity theory;decibel;digital signal processing;discrete cosine transform;image compression;integrated circuit;most significant bit;operand;peak signal-to-noise ratio;round-off error;rounding;schema (genetic algorithms);standard cell;truncation error;verilog	Peng Cao;Yang Xiao	2014	IEICE Electronic Express	10.1587/elex.11.20140481	computer science;electrical engineering;digital signal processing	DB	14.577282403648162	42.64643010489677	152408
b1054ca602940f0220d70da836e4f5a27ea6ad0d	parallelism to reduce power consumption on fpga spatiotemporal image processing	real time systems field programmable gate arrays image processing portable computers power consumption;image processing;data stream;portable image processing systems;parallel processing technique;real time image processing system;video rate operation;fpga;anisotropic diffusion;sobel filter;parallelism;low power;energy consumption field programmable gate arrays spatiotemporal phenomena image processing pipelines streaming media parallel processing real time systems filters anisotropic magnetoresistance;portable computers;sobel filters;multiple processing pipelines;power consumption;field programmable gate arrays;real time image processing;parallel processing;parallel processing fpga low power sobel filter anisotropic diffusion;anisotropic diffusion parallelism power consumption fpga portable image processing systems video rate operation multiple processing pipelines parallel processing technique real time image processing system sobel filters;real time systems	Portable image processing systems require algorithms which reduce the power consumption while maintaining video rate operation. This paper describes how splitting the data stream into multiple processing pipelines can reduce power consumption in contrast to the traditional spatial (pipeline) parallel processing technique. Real-time image processing system functions (Sobel filters and anisotropic diffusion) were implemented to show the principle of the technique. Our results show that partitioning the image into multiple sections gives increasing benefits with shorter processing times, and up to 45% drop in the power consumption.	algorithm;anisotropic diffusion;field-programmable gate array;image processing;parallel computing;pipeline (software);real-time clock;sobel operator	Walid Atabany;Patrick Degenaar	2008	2008 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2008.4541708	embedded system;parallel processing;computer vision;electronic engineering;real-time computing;image processing;computer science;field-programmable gate array	EDA	10.38684473302808	39.59941952231311	152545
58575733b89dfdb2ca0c78f7bb2ee4d47825d2f9	finding r-dominating sets and p-centers of trees in parallel	dynamic programming;location theory;pram r dominating sets p centers edge weighted tree euclidean plane parallel algorithms sequential dynamic programming algorithms network location theory;pram;parallel algorithm;pram trees r hbox rm dominating sets p hbox rm centers network location theory parallel algorithms;dynamic programming algorithm;parallel algorithms phase change random access memory network servers computer society heuristic algorithms transportation land mobile radio cellular systems telephony upper bound;set theory;trees mathematics;65;network location theory;dominating set;trees;r hbox rm dominating sets;computational complexity;tree network;sets;p hbox rm centers;computational complexity parallel algorithms set theory concurrency theory trees mathematics dynamic programming;concurrency theory;parallel algorithms	Let T=(V, E) be an edge-weighted tree with |V|=n vertices embedded in the Euclidean plane. Let IE denote the set of all points on the edges of T. Let X and Y be two subsets of IE and let r be a positive real number. A subset D/spl sube/X is an X/Y/r-dominating set if every point in Y is within distance r of a point in D. The X/Y/r-dominating set problem is to find an X/Y/r-dominating set D* with minimum cardinality. Let p/spl ges/1 be an integer. The X/Y/p-center problem is to find a subset C*/spl sube/X of p points such that the maximum distance of any point in Y from C* is minimized. Let X and Y be either V or IE. In this paper, efficient parallel algorithms on the EREW PRAM are first presented for the X/Y/r-dominating set problem. The presented algorithms require O(log/sup 2/n) time for all cases of X and Y. Parallel algorithms on the EREW PRAM are then developed for the X/Y/p-center problem. The presented algorithms require O(log/sup 3/n) time for all cases of X and Y. Previously, sequential algorithms for these two problems had been extensively studied in the literature. However, parallel solutions with polylogarithmic time existed only for their special cases. The algorithms presented in this paper are obtained by using an interesting approach which we call the dependency-tree approach. Our results are examples of parallelizing sequential dynamic-programming algorithms by using the approach.	automatic parallelization;dominating set;dynamic programming;embedded system;parallel algorithm;parallel random-access machine;polylogarithmic function;regular expression;sube card;sequential algorithm;time complexity	Biing-Feng Wang	2004	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2004.36	parallel computing;location theory;computer science;theoretical computer science;dynamic programming;distributed computing;parallel algorithm;algorithm	Theory	15.470046378694889	32.45218005129088	152554
06ee4e62dac57dd9fde881cbb58106d9e3fd3c4b	nearly logarithmic time parallel algorithms for the class of \pm2^b ascend computations on a simd hypercube	parallel algorithms computational complexity hypercube networks;parallel algorithm;ascend class logarithmic time parallel algorithms simd hypercube descend class;concurrent computing;routing;parallel algorithms concurrent computing hypercubes routing computational intelligence society communication system control algorithm design and analysis genetic mutations arithmetic;descend class;computational complexity;ascend class;hypercubes;arithmetic;logarithmic time parallel algorithms;genetic mutations;communication system control;algorithm design and analysis;simd hypercube;hypercube networks;computational intelligence society;parallel algorithms	"""Recently, we have studied two important classes of algorithms requiring &2' communications: k2'descend, and f2 ' -ascend. Let N = 2"""" be the number of PES in a SIMD hypercube which restricts all communications to a single fixed dimension at a time. I n [5], we developed an efficient O(n) algorithm f o r the descend class. I n [SI, we obtained a simple O(n2/ log n) algorithm for the ascend class, requiring O(1ogn) words of local memory per PE. I n this paper, we present two new algorithms for the ascend class on a S IMD hypercube. The first algorithm runs in O ( ~ L ' . ~ ) t ime and requires O(1) space per PE. The second algorithm, which is discussed only briefly here, runs in O(nd-) tame and requires O(1ogn) space per PE."""	ascend;computation;packetized elementary stream;parallel algorithm;simd;tame;time complexity;usc interactive media & games division	David Nassimi	1992		10.1109/IPPS.1992.223060	parallel computing;computer science;theoretical computer science;distributed computing	Theory	14.763015706329957	32.9893020089793	152632
00ac47e19ccc92041187a79fe44a2dc3968a42f6	efficient exploration of faulty trees	local computation;optimal algorithm	We consider the problem of the exploration of trees, some of whose edges are faulty. A robot, situated in a starting node and unaware of the location of faults, has to explore the connected fault-free component of this node by visiting all its nodes. The cost of the exploration is the number of edge traversals. For a given tree and given starting node, the overhead of an exploration algorithm is the worst-case ratio (taken over all fault configurations) of its cost to the cost of an optimal algorithm which knows where faults are situated. An algorithm, for a given tree and given starting node, is called perfectly competitive if its overhead is the smallest among all exploration algorithms not knowing the location of faults. We design a perfectly competitive exploration algorithm for any line, and an exploration algorithm for any tree, whose overhead is at most 9/8 larger than that of a perfectly competitive algorithm. Both our algorithms are fairly natural and the total time of local computations used during exploration is linear in the size of the explored tree. Our main contribution is the analysis of the performance of these algorithms, showing that natural exploration strategies perform well in faulty trees.	algorithm;approximation algorithm;best, worst and average case;computation;overhead (computing);polynomial;robot;situated;time complexity;tree (data structure)	Euripides Markou;Andrzej Pelc	2005	Theory of Computing Systems	10.1007/s00224-005-1252-0	mathematical optimization;computer science;mathematics;distributed computing;algorithm	Theory	18.03342440650302	34.06368951513509	152738
8cb6974920890fd6a8aadcd7b2ad1c7b10451698	a novel low-power and in-place split-radix fft processor	radix 2;low power;split radix fft	Split-radix Fast Fourier Transform (SRFFT) approximates the minimum number of multiplications by theory among all the FFT algorithms. Since multiplications significantly contribute to the overall system power consumption, SRFFT is a good candidate for implementation of a low power FFT processor. In this paper we present a novel low power SRFFT processor using a modified radix-2 butterfly structure. With the proposed butterfly unit, the address generation scheme for conventional radix-2 FFT could be applied to SRFFT and therefore it can avoid the complexity of address generation and interim data registers. Simulation results show that compared with a conventional radix-2 implementation, power consumption of the new processor is reduced by an amount of 11.7% and 18.3% for 16-point and 32-point FFT respectively.	fast fourier transform;in-place algorithm;low-power broadcasting;simulation;split-radix fft algorithm	Zhuo Qian;Martin Margala	2014		10.1145/2591513.2591563	arithmetic;parallel computing;computer hardware;split-radix fft algorithm;mathematics	Arch	11.872900145976072	44.55438565117357	152947
2a559cb45f60b8b4de6d51f6a12471c19ed7f8ae	how long it takes for an ordinary node with an ordinary id to output?		In the context of distributed synchronous computing, processors perform in rounds, and the time complexity of a distributed algorithm is classically defined as the number of rounds before all computing nodes have output. Hence, this complexity measure captures the running time of the slowest node(s). In this paper, we are interested in the running time of the ordinary nodes, to be compared with the running time of the slowest nodes. The node-averaged time-complexity of a distributed algorithm on a given instance is defined as the average, taken over every node of the instance, of the number of rounds before that node output. We compare the node-averaged time-complexity with the classical one in the standard (mathsf {LOCAL}) model for distributed network computing. We show that there can be an exponential gap between the node-averaged time-complexity and the classical time-complexity, as witnessed by, e.g., leader election. Our first main result is a positive one, stating that, in fact, the two time-complexities behave the same for a large class of problems on very sparse graphs. In particular, we show that, for (mathsf {LCL}) problems on cycles, the node-averaged time complexity is of the same order of magnitude as the “slowest node” time-complexity. In addition, in the (mathsf {LOCAL}) model, the time-complexity is computed as a worst case over all possible identity assignments to the nodes of the network. In this paper, we also investigate the ID-averaged time-complexity, when the number of rounds is averaged over all possible identity assignments of size (O(log n)). Our second main result is that the ID-averaged time-complexity is essentially the same as the expected time-complexity of randomized algorithms (where the expectation is taken over all possible random bits used by the nodes, and the number of rounds is measured for the worst-case identity assignment). Finally, we study the node-averaged ID-averaged time-complexity. We show that 3-colouring the n-node ring requires (varTheta (log ^*n)) rounds if the number of rounds is averaged over the nodes, or if the number of rounds is averaged over the identity assignments. In contrast, we show that 3-colouring the ring requires only O(1) rounds if the number of rounds is averaged over the nodes, and over the identity assignments.	average-case complexity;best, worst and average case;blum axioms;central processing unit;distributed algorithm;exponential time hypothesis;graph coloring;identifier;leader election;probabilistic analysis of algorithms;randomized algorithm;sparse matrix;time complexity	Laurent Feuilloley	2017		10.1007/978-3-319-72050-0_16	order of magnitude;time complexity;distributed computing;real-time computing;distributed algorithm;leader election;computer science;binary logarithm;randomized algorithm;exponential function;graph	Theory	16.493301382153348	33.87726946058641	153292
0e36636bfa675d3649c28a53ed0d11e22408af11	finding critical regions in a network	directed graphs;geographical failure vital network infrastructure single link failure geographical region two dimensional plane critical region elementary figure circle ellipse rectangle square equilateral triangle network disruption polynomial number nontrivial position polynomial time algorithm realistic network computational geometry;network theory graphs computational complexity computational geometry geography;critical regions;computational geometry;directed graph single link failures geographical region two dimensional plane critical region polynomial number nontrivial positions polynomial time algorithm;failure analysis;measurement robustness conferences communication networks complexity theory polynomials shape;network robustness;computational complexity;geographical failures;computational geometry geographical failures critical regions network robustness;network theory graphs computational complexity computational geometry directed graphs failure analysis;network theory graphs;geography	It is important that our vital networks (e.g., infrastructures) are robust to more than single-link failures. Failures might for instance affect a part of the network that resides in a certain geographical region. In this paper, considering networks embedded in a two-dimensional plane, we study the problem of finding a critical region - that is, a part of the network that can be enclosed by a given elementary figure (a circle, ellipse, rectangle, square, or equilateral triangle) with a predetermined size - whose removal would lead to the highest network disruption. We determine that there is a polynomial number of non-trivial positions for such a figure that need to be considered and, subsequently, we propose a polynomial-time algorithm for the problem. Simulations on realistic networks illustrate that different figures with equal area result in different critical regions in a network.	algorithm;computer simulation;denial-of-service attack;embedded system;failure;polynomial;robustness (computer science);time complexity	Stojan Trajanovski;Fernando A. Kuipers;Piet Van Mieghem	2013	2013 Proceedings IEEE INFOCOM	10.1109/INFCOM.2013.6567167	failure analysis;combinatorics;discrete mathematics;directed graph;computational geometry;computer science;mathematics;geometry;computational complexity theory	Theory	23.18039755146889	38.87075443699758	153376
cac26527aeca622803b5eaffb901a27b9c435bfc	gasim: a fast galois field based simulator for functional model	design representations gasim galois field functional model logic simulation finite field applications finite field representation decision diagrams;decision diagrams;logic simulation;decision diagrams logic simulation galois fields;decision diagram;design representation;finite field;galois fields circuit simulation circuit synthesis boolean functions circuit testing data structures elliptic curve cryptography logic testing application software signal synthesis;functional model;it evaluation;galois field;simulation environment;galois fields	This paper presents a fast logic simulator based on Galois field. This is designed to act as an underlying tool for all finite field applications considering the fact that simulation plays an important role in all these applications. Three approaches for finite field representation and its evaluation are discussed. In addition an approach for normalizing the finite field based decision diagrams is also presented. The experimental results show the trade-off that can be made by using different design representations between spatial complexity and speed. Also the results show that it can be used in conventional simulation environment to achieve speedup.	decision problem;diagram;function model;grammatical framework;influence diagram;logic simulation;requirement;speedup	Dhiraj K. Pradhan;Ashutosh Kumar Singh;T. L. Rajaprabhu;Abusaleh M. Jabir	2005	Tenth IEEE International High-Level Design Validation and Test Workshop, 2005.	10.1109/HLDVT.2005.1568827	theoretical computer science;finite field;algorithm	EDA	18.440220143083348	45.8791643563633	153933
22d0b9c61b7fa576150a82490c7f74342cc25bd6	a hardware architecture for the lzw compression and decompression algorithms based on parallel dictionaries	data compression;lzw algorithm;and pdlzw algorithm;hardware architecture;lossless data decompression;lossy data decompression;lossy data compression;lossless data compression;parallel dictionary	In this paper, a parallel dictionary based LZW algorithm called PDLZW algorithm and its hardware architecture for compression and decompression processors are proposed. In this architecture, instead of using a unique fixed-word-width dictionary a hierarchical variable-word-width dictionary set containing several dictionaries of small address space and increasing word widths is used for both compression and decompression algorithms. The results show that the new architecture not only can be easily implemented in VLSI technology because of its high regularity but also has faster compression and decompression rate since it no longer needs to search the dictionary recursively as the conventional implementations do.	address space;algorithm;central processing unit;data compression;dictionary;lempel–ziv–welch;recursion;very-large-scale integration	Ming-Bo Lin	2000	VLSI Signal Processing	10.1023/A:1026559601791	data compression;lossy compression;data compression ratio;parallel computing;computer science;lz77 and lz78;theoretical computer science;hardware architecture;lossless compression;algorithm;statistics	Arch	11.879802667991402	38.02883018075549	154097
dac4ace994757a9ffb6d907df987739255e4ebca	mapping computation with no memory	program design;circuit design;graph coloring;multistage interconnection network;chip;upper bound;decomposition method;linear mapping;rearrangeability;memory optimization;modular arithmetic;mapping computation;processor optimization;boolean mapping;butterfly	We investigate the computation of mappings from a set S to itself with in situ programs, that is using no extra variables than the input, and performing modifications of one component at a time. We consider several types of mappings and obtain effective computation and decomposition methods, together with upper bounds on the program length (number of assignments). Our technique is combinatorial and algebraic (graph coloration, partition ordering, modular arithmetics). For general mappings, we build a program with maximal length 5n − 4, or 2n − 1 for bijective mappings. The length is reducible to 4n − 3 when |S| is a power of 2. This is the main combinatorial result of the paper, which can be stated equivalently in terms of multistage interconnection networks as: any mapping of {0, 1} can be performed by a routing in a double n-dimensional Beneš network. Moreover, the maximal length is 2n − 1 for linear mappings when S is any field, or a quotient of an Euclidean domain (e.g. Z/sZ). In this case the assignments are also linear, thereby particularly efficient from the algorithmic viewpoint. The in situ trait of the programs constructed here applies to optimization of program and chip design with respect to the number of variables, since no extra writing memory is used. In a non formal way, our approach is to perform an arbitrary transformation of objects by successive elementary local transformations inside these objects only with respect to their successive states.	angular momentum operator;clos network;computation;graph (discrete mathematics);graph coloring;linear algebra;mathematical optimization;maximal set;multistage interconnection networks;power of two;routing	Serge Burckel;Emeric Gioan;Emmanuel Thomé	2009		10.1007/978-3-642-03745-0_15	combinatorics;discrete mathematics;theoretical computer science;mathematics	Theory	20.856912270281747	39.25422350381602	154327
9d63a37de71f9fa86644dbb80551c5203614a933	on diagnosability of large fault sets in regular topology-based computer systems	multiprocessor interconnection networks;detection erreur;hypercube;system level diagnosis diagnosability large fault sets large multiprocessor systems t k diagnosability hypercube star graph meshes fault diagnosis;system structure;star graph;interconnection;multiprocessor;multiprocessor systems;diagnosability;computer systems;connected graph;upper bound;t k diagnosability of star graph;degree of diagnosability;t k diagnosable systems;fault tolerant computing;diagnostic panne;structure systeme;necessary and sufficient condition;fault diagnostic;interconnexion;diagnostico pana;multiprocessor interconnection networks multiprocessing systems fault diagnosis fault tolerant computing;hypercubes circuit faults fault diagnosis circuit topology circuit testing system testing integrated circuit interconnections fault tolerance logic testing;grafo estrella;graphe etoile;systeme informatique;multiprocessing systems;error detection;t k diagnosability of hypercubes;multiprocesador;t diagnosable systems;diagnostiquabilite;graphe connexe;interconeccion;diagnostico error;estructura sistema;diagnostic erreur;fault diagnosis;error diagnostic;grafo conexo;multiprocesseur;system level diagnosis;hipercubo	The classical diagnosability approach has its limitation when dealing with large fault sets in large multiprocessor systems. This is due to limited diagnosability of large multiprocessor systems connected using regular interconnection structures. We propose an alternative approach to system diagnosis by allowing a few upper bounded number of units to be diagnosed incorrectly. This measure is called t/k-diagnosability. Using this new measure, it is possible to increase the degree of diagnosability of large system considerably. The t/k-diagnosis guarantees that all the faulty units (processors) in a system are detected (provided the number of faulty units does not exceed t) while at most k units are incorrectly diagnosed. We provide necessary and sufficient conditions for t/k-diagnosability and discuss their implication. To demonstrate the power of this approach, we analyze the diagnosability of large systems connected as hypercube, star-graph, and meshes. It is shown that a substantial increase in the degree of diagnosability of these structures is achieved, compared with the degree of diagnosability achieved using the classic diagnosability approach, at the cost of a comparably small number of incorrectly diagnosed units.	hardware description language	Arun K. Somani;Ofer Peleg	1996	IEEE Trans. Computers	10.1109/12.536232	parallel computing;real-time computing;multiprocessing;error detection and correction;telecommunications;star;computer science;connectivity;interconnection;mathematics;distributed computing;upper and lower bounds;hypercube	Arch	23.741372237833275	44.464152163425915	154363
e760a63dce3645a5617076666c00e6b6da9c75f3	improving the realization of multiple-control toffoli gates using the ncvw quantum gate library	multiple control toffoli mct;reversible circuit;quantum gates;mapping structure multiple control toffoli gates ncvw quantum gate library reversible circuits quantum circuits universal quantum gates reconfigured structures;gate library quantum cost reversible circuit multiple control toffoli mct;gate library;logic gates libraries quantum computing optimization very large scale integration embedded systems;quantum cost	Multiple Control Toffoli (MCT) gates are the main constituent of reversible circuits. For quantum circuits, MCT gates are realized using quantum gate libraries such as NCV or NCVW which are composed of universal quantum gates like NOT, CNOT, V/V+, and W/W+. In order to improve the design of quantum circuits, the mapping of MCT gates to a cascade of these quantum gates has to be improved as well. In this work, we propose reconfigured structures of quantum gates realizing MCT gates. To this end, we rely on the established mapping structure, but append redundant gates which can be used afterwards for simplifications. Eventually, the proposed design is mapped to an NCVW quantum circuit. We have successfully tested our mapping technique. The obtained results have experimentally been compared to related work.	append;controlled not gate;experiment;library (computing);mos-controlled thyristor;mobile data terminal;quantum circuit;quantum gate	Laxmidhar Biswal;Chandan Bandyopadhyay;Robert Wille;Rolf Drechsler;Hafizur Rahaman	2016	2016 29th International Conference on VLSI Design and 2016 15th International Conference on Embedded Systems (VLSID)	10.1109/VLSID.2016.23	electronic engineering;nor logic;toffoli gate;three-input universal logic gate;theoretical computer science;quantum network;controlled not gate;quantum circuit;mathematics;quantum dot cellular automaton;algorithm;quantum mechanics;tc0;quantum gate	EDA	17.25435680304491	45.16778503867058	154422
bad1c1d048160b2062c06ba764c38d5baca8bf68	an area-efficient fpga realisation of a codebook-based image compression method	random access memory;image coding;data compression;field programmable gate arrays image coding hardware transform coding discrete wavelet transforms vector quantization costs wireless sensor networks sensor arrays image segmentation;quantisation signal;indexes;hamming distance;image compression;pixel;fpga architecture;field programmable gate arrays;quantisation signal data compression field programmable gate arrays image coding pipeline processing;hardware implementation;pipeline processing;pipelined implementation field programmable gate array codebook based image compression method hardware implementation multiplication free quantisation;hardware	We present a hardware implementation of an efficient image compression method optimised for small FPGAs. The compression method is based on a codebook of reference patterns to support multiplication-free quantisation of the image data. Based on specific features of a low-cost FPGA architecture, a pipelined implementation is developed and evaluated. The implemented hardware benefits from the simple structure of the compression method and is optimised for area and performance. The realised hardware as well as the underlying compression mechanism are described and the synthesis results for different model variants are compared. The results show that a high compression rate is possible at extremely low hardware costs. Also, a high frame rate can be obtained even on a low-cost FPGA.	bit-level parallelism;codebook;field-programmable gate array;image compression;mathematical optimization;norm (social);pipeline (computing);pipelining (dsp implementation);quantization (physics);random-access memory;requirement;throughput	Peter Zipf;Heiko Hinkelmann;Hui Shao;Radu Dogaru;Manfred Glesner	2008	2008 International Conference on Field-Programmable Technology	10.1109/FPT.2008.4762415	data compression;database index;embedded system;hamming distance;image compression;computer science;theoretical computer science;lossless compression;pixel;field-programmable gate array	EDA	11.459339829119283	41.07197484149442	154641
3944a677c690a4802e77a6b75292906ead975219	a method for generating prime implicants of a boolean expression	lattices;iterative algorithms;logic;minimization methods;logic minimization;i cube logic minimization prime implicant prime implicant generation quine mccluskey algorithm;quine mccluskey algorithm;prime implicant generation;computer science;i cube;prime implicant	This correspondence gives a method which generates prime implicants in a simple one-pass procedure. Starting from the lowest (or highest) points in the function lattice, it generates successive subcubes in a unique sequence. Each subcube is formed only once.	boolean expression	H. R. Hwa	1974	IEEE Transactions on Computers	10.1109/T-C.1974.224003	implicant;discrete mathematics;petrick's method;mathematics;logic;algorithm	Visualization	20.955216705868644	43.85124188850873	154775
7ba40fea7914f05d70ddd00ea6870f0bcb2c3c9a	evmdd-based analysis and diagnosis methods of multi-state systems with multi-state components	redundancy;diagrams;performance engineering;routing;computer architecture;bayes theorem;experimental design;logic circuits;optimization;systems analysis;information systems;electric power distribution;probability	A multi-state system with multi-state components is a model of systems, where performance, capacity, or reliability levels of the systems are represented as states. It usually has more than two states, and thus can be considered as a multi-valued function, called a structure function. Since many structure functions are monotone increasing, their multi-state systems can be represented compactly by edge-valued multivalued decision diagrams (EVMDDs). This paper presents an analysis method of multi-state systems with multi-state components using EVMDDs. Experimental results show that, by using EVMDDs, structure functions can be represented more compactly than existing methods using ordinary MDDs. Further, EVMDDs yield comparable computation time for system analysis. This paper also proposes a new diagnosis method using EVMDDs, and shows that the proposed method can infer the most probable causes for system failures more efficiently than conventional methods based on Bayesian networks.	bayesian network;benchmark (computing);comment (computer programming);computation;diagram;existential quantification;experiment;procedural generation;system analysis;time complexity;monotone	Shinobu Nagayama;Tsutomu Sasao;Jon T. Butler	2014	Multiple-Valued Logic and Soft Computing			AI	24.142994699382513	45.45901569134371	155002
a7492ed48df3ae6f739dec56c35384cc99f33a30	an efficient fpga design of residue-to-binary converter for the moduli set $\{2n+1,2n,2n-1\}$	digital signal processing;field programmable gate array;arithmetic coding;residue number systems;cathode ray tube;low complexity;indexing terms;digital filter;chinese remainder theorem;energy consumption;code convertors;power consumption;field programmable gate arrays	In this paper, we propose a novel reverse converter for the moduli set {2n+1,2n,2n-1}. First, we simplify the Chinese Remainder Theorem in order to obtain a reverse converter that uses mod-(2n-1) operations. Next, we present a low complexity implementation that does not require the explicit use of modulo operation in the conversion process and we prove that theoretically speaking it outperforms state of the art equivalent converters. We also implemented the proposed converter and the best equivalent state of the art converters on Xilinx Spartan 3 field-programmable gate array. The results indicate that, on average, our proposal is about 14%, 21%, and 8% better in terms of conversion time, area cost, and power consumption, respectively.	field-programmability;field-programmable gate array;modulo operation;spartan	Kazeem Alagbe Gbolagade;George Razvan Voicu;Sorin Dan Cotofana	2011	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2010.2050608	arithmetic;embedded system;electronic engineering;computer science;electrical engineering;theoretical computer science;mathematics;algorithm;field-programmable gate array	EDA	13.44654722208942	44.3311060926316	155037
64ce05fdd0d2953abf83faf39d68d44014a4bb26	parallel algorithms for hierarchical clustering	espacio n dimensiones;multidimensional space;hierarchical clustering;algoritmo paralelo;agregacion;espace n dimensions;optimisation;secuencial;parallel algorithm;butterfly network;sequential;optimizacion;metric;aggregation;algorithme parallele;multi dimensional;sequentiel;distance metric;agregation;metrico;optimization;pattern analysis;algoritmo optimo;algorithme optimal;pram algorithm;optimal algorithm;metrique	Hierarchical clustering is a common method used to determine clusters of similar data points in multidimensional spaces. O(n*) algorithms are known for this problem [3,4,11,19]. This paper reviews important results for sequential algorithms and describes previous work on parallel algorithms for hierarchical clustering. Parallel algorithms to perform hierarchical clustering using several distance metrics are then described. Optimal PRAM algorithms using n/log n processors are given for the average link, complete link, centroid, median, and minimum variance metrics. Optimal butterfly and tree algorithms using n/log n processors are given for the centroid, median, and minimum variance metrics. Optimal asymptotic speedups are achieved for the best practical algorithm to perform clustering using the single link metric on a n/log n processor PRAh4, butterfly, or tree.	central processing unit;cluster analysis;data point;hierarchical clustering;parallel algorithm	Clark F. Olson	1995	Parallel Computing	10.1016/0167-8191(95)00017-I	mathematical optimization;combinatorics;discrete mathematics;metric;computer science;mathematics	Theory	12.731471210276014	34.220871360872714	155086
5b8390cd1592401f599a220e8796d57f8fc13c09	remote-spanners: what to know beyond neighbors	distributed algorithms;graph theory;disjoint path;routing protocols;routing;unweighted input graph;vertex set;spanner;data mining;graphs distributed algorithms graph theory;graphs;connected graph;distance measurement;k connected;k connected graphs;routing algorithm;tree sub graphs;remote spanner;tree sub graphs routing algorithm remote spanner vertex set k connected graphs disjoint path distributed algorithm unweighted input graph random unit disk graphs;relays;random unit disk graphs;routing protocols distributed algorithms distributed computing tree graphs cyclic redundancy check internet network interfaces network topology ad hoc networks cost function;distributed algorithm;disjoint paths;gallium	Motivated by the fact that neighbors are generally known in practical routing algorithms, we introduce the notion of remote-spanner. Given an unweighted graph G, a sub-graph H with vertex set V (H) = V (G) is an (α, β)-remote-spanner if for each pair of points u and v the distance between u and v in Hu, the graph H augmented by all the edges between u and its neighbors in G, is at most α times the distance between u and v in G plus β. We extend this definition to k-connected graphs by considering the minimum length sum over k disjoint paths as a distance. We then say that an (α, β)-remote-spanner is k-connecting. In this paper, we give distributed algorithms for computing (1 + ε, 1 − 2ε)-remote-spanners for any ε ≫ 0, k-connecting (1, 0)-remote-spanners for any k ≥ 1 (yielding (1, 0)-remote-spanners for k = 1) and 2-connecting (2,−1)-remote-spanners. All these algorithms run in constant time for any unweighted input graph. The number of edges obtained for k-connecting (1, 0)-remote-spanner is within a logarithmic factor from optimal (compared to the best k-connecting (1, 0)-remote-spanner of the input graph). Interestingly, sparse (1, 0)-remote-spanners (i.e. preserving exact distances) with O(n4/3) edges exist in random unit disk graphs. The number of edges obtained for (1+ε, 1−2ε)-remote-spanners and 2-connecting (2,−1)-remote-spanners is linear if the input graph is the unit ball graph of a doubling metric (even if distances between nodes are unknown). Our methodology consists in characterizing remote-spanners as sub-graphs containing the union of small depth tree sub-graphs dominating nearby nodes. This leads to simple local distributed algorithms.	distributed algorithm;holographic principle;period-doubling bifurcation;routing;sparse matrix;time complexity	Philippe Jacquet;Laurent Viennot	2009	2009 IEEE International Symposium on Parallel & Distributed Processing	10.1109/IPDPS.2009.5161041	graph power;distributed algorithm;routing;multiple edges;degree;computer science;regular graph;distance-regular graph;connectivity;multigraph;hypercube graph;cycle graph;distance-hereditary graph;distributed computing;routing protocol;graph;random geometric graph;gallium;bound graph;complement graph;semi-symmetric graph;line graph;string graph;strength of a graph	Theory	22.35029256303765	34.50258901885278	155132
b2811bfe56f2ad6faacf9bb2c0c72a0d8590e2df	high-speed 8/16/32-point dct architecture using fixed-rotation adaptive cordic		In this paper, the high-speed Discrete Cosine Transform (DCT) architecture is presented using the Adaptive CORDIC (ACor) algorithm built with a fixed-rotation angle. The proposed method is implemented in six different versions corresponding to the number of DCT point, i.e., 8-point (8p), 16-point (16p), and 32-point (32p), and the number of ACor stages, i.e., 2-Stage (2S) and 3-Stage (3S). The implementations are built and verified on an Altera Stratix IV FPGA. The 2S designs of 8p-DCT, 16p-DCT, and 32p-DCT achieve the maximum operating frequencies of 179.86 MHz, 162.60 MHz, and 136.97 MHz, respectively. Moreover, the 2S-32p-DCT module is implemented in ASIC with the 65nm-SOTB CMOS technology. The synthesis shows that the core costs 47.2K gates and consumes about 0.68 mW while operating at 100 MHz clock rate. The 2S implementations of 8p-DCT, 16p-DCT, and 32p-DCT achieve four, five, and six adder-delay, mean-square-error of 1.403e-4, 2.029e-2, and 7.663e-2, and coding gain of 8.8108 dB, 9.0984 dB, and 9.2170 dB, respectively. In comparison with recent works, the proposed method achieves the best timing performances, good accuracy results, and adequate resources cost.	adder (electronics);algorithm;application-specific integrated circuit;cmos;cordic;clock rate;coding gain;discrete cosine transform;field-programmable gate array;mean squared error;nor gate;performance;stratix	Trong-Thuc Hoang;Cong-Kha Pham;Duc-Hung Le	2018	2018 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2018.8351090	stratix;electronic engineering;coding gain;field-programmable gate array;architecture;discrete cosine transform;cordic;clock rate;computer science;cmos	Arch	12.860404887985625	42.976461903641265	155464
974ef7dd24052206b523ad5a01b22bc6c9faad0a	algorithms for massively parallel image processing architectures	divide and conquer strategy;interprocessor communication speed improvement;concurrent computing;image processing;data histogramming;list manipulation;massively parallel image processing architectures;computational geometry;parallel programming;computer vision;computer architecture;parallel architectures;divide and conquer strategy computerised picture processing parallel algorithms interprocessor communication speed improvement massively parallel image processing architectures computational geometry data histogramming list manipulation computer vision;parallel image processing;computational complexity;hypercubes;computerised picture processing;artificial intelligence;parallel machines;divide and conquer;algorithm design and analysis;parallel processing;parallel architectures computational complexity computerised picture processing parallel algorithms;image processing concurrent computing hypercubes computer vision parallel algorithms algorithm design and analysis parallel processing parallel programming computer architecture artificial intelligence;parallel algorithms	The complexity of efficiently programming massively parallel machines is illustrated by presenting a number of algorithms. These algorithms deal with computational geometry, data histogramming, list manipulation, and other problems or operations that arise in computer vision tasks. All of the algorithms presented use a divide-and-conquer strategy, and they all use routines that solve a problem of size M using a machine size N, where M >	algorithm;image processing	Jorge L. C. Sanz;Robert Cypher	1988		10.1109/ICPR.1988.28256	parallel processing;algorithm design;computer vision;parallel computing;divide and conquer algorithms;concurrent computing;image processing;computational geometry;computer science;theoretical computer science;massively parallel;distributed computing;parallel algorithm;computational complexity theory;hypercube	ML	11.32178069642482	35.42113699438704	155710
34beb0bf0e60ef45b3504d73576202fc63b87afa	on the diameter of hyperbolic random graphs		Large real-world networks are typically scale-free. Recent research has shown that such graphs are described best in a geometric space. More precisely, the internet can be mapped to a hyperbolic space such that geometric greedy routing performs close to optimal (Boguná, Papadopoulos, and Krioukov. Nature Communications, 1:62, 2010). This observation pushed the interest in hyperbolic networks as a natural model for scale-free networks. Hyperbolic random graphs follow a powerlaw degree distribution with controllable exponent β and show high clustering (Gugelmann, Panagiotou, and Peter. ICALP, pp. 573–585, 2012). For understanding the structure of the resulting graphs and for analyzing the behavior of network algorithms, the next question is bounding the size of the diameter. The only known explicit bound is O((log n)) (Kiwi and Mitsche. ANALCO, pp. 26–39, 2015). We present two much simpler proofs for an improved upper bound of O((log n)) and a lower bound of Ω(logn).	bootstrap percolation;cluster analysis;degree distribution;giant component;greedy algorithm;icalp;path (graph theory);polylogarithmic function;random graph;routing;rumor spread in social network;semantic network	Tobias Friedrich;Anton Krohmer	2015		10.1007/978-3-662-47666-6_49	mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	20.193819027202796	35.61383946221116	155755
002a5cbbe6a782efa83d598eebe9dce6a39e2d80	sigma: a vlsi chip for galois field gf(2m) based multiplication and division	clocks;very large scale integration;testing;polynomials;chip;computer architecture;cryptography;very large scale integration galois fields polynomials computer architecture arithmetic signal processing algorithms clocks testing cryptography hardware;arithmetic;signal processing algorithms;galois field;galois fields;hardware	In this paper, we present a new VLSI chip for computing multiplication and division in Galois Fields GF(2m), for values of m 1. 8. The chip is based on a syslolic architecture which can produce a new result every clock cycle and the two operations can be interleaved. A prototype CMOS VLSI chip was designed, fabricated and tested. The chip was tested to be fully functional at 33.3 MHz.	very-large-scale integration	Mario Kovac;N. Ranganathan;M. Varanasi	1993		10.1109/ICVD.1993.669630	chip;electronic engineering;parallel computing;computer science;cryptography;theoretical computer science;mathematics	EDA	12.891082811211447	44.65258782815639	155962
6425e0279755b4fd2f77454cf098ebc758cfa1db	efficient parallel algorithms can be made robust	tolerancia falta;algoritmo paralelo;system reliability;pram;fiabilite systeme;parallel algorithm;shared memory;fault tolerant;multiprocessor;complexite calcul;complejidad calculo;limite inferior;lower bounds;memoria compartida;probleme write all;computing complexity;detection defaillance;parallel computation;algorithme parallele;fiabilidad sistema;performance programme;algorithme robuste;robust algorithms;computation complexity;fault tolerance;eficacia programa;program performance;multiprocesador;limite inferieure;parallelisme massif;massive parallelism;tolerance faute;lower bound;memoire partagee;multiprocesseur	The efficient parallel algorithms proposed for many fundamental problems, such as list ranking, integer sorting and computing preorder numberings on trees, are very sensitive to processor failures. The requirement of efficiency (commonly formalized usingParallel-timexProcessors as a cost measure) has led to the design of highly tuned PRAM algorithms which, given the additional constraint of simple processor failures, unfortunately become inefficient or even incorrect. We propose a new notion ofrobustness, that combines efficiency with fault tolerance. For the common case of fail-stop errors, we develop a general and easy to implement technique to make robust many efficient parallel algorithms, e.g., algorithms for all the problems listed above. More specifically,for any dynamic pattern of fail-stop errors on a CRCW PRAMwith at least one surviving processor, our method increases the original algorithm cost by at most a log2 multiplicative factor. Our technique is based on a robust solution of the problem ofWrite-All, i.e., usingP processors, write 1's in all locations of anN-sized array. In addition we show that at least a log/log log multiplicative overhead will be incurred for certain patterns of failures by any algorithm that implements robust solutions toWrite-All withP=N. However, by exploiting parallel slackness, we obtain an optimal cost algorithm when $$P \leqslant \frac{N}{{\log ^2 N - \log N\log \log N}}.$$	binary logarithm;central processing unit;coefficient;comparison of archive formats;fail-stop;fault tolerance;gödel numbering;integer sorting;list ranking;overhead (computing);parallel algorithm;parallel random-access machine;robustness (computer science)	Paris C. Kanellakis;Alexander A. Shvartsman	1989	Distributed Computing	10.1007/BF02277667	embedded system;fault tolerance;parallel computing;computer science;operating system;distributed computing;algorithm;cost efficiency	Theory	10.512192616624954	33.628103861159836	156054
9ff13b2f625d4bb43e3d1bf5f641c5189e63cf12	a diagnosis algorithm for constant degree structures and its application to vlsi circuit testing	verification;parallelisme;very large scale integration circuit testing system testing clustering algorithms fault diagnosis microelectronics circuit faults test equipment production costs;integrated circuit testing vlsi computational complexity production testing;algorithm complexity;analisis sistema;complejidad algoritmo;circuit vlsi;host system diagnosis algorithm constant degree structures vlsi circuit testing constant degree systems tori rectangular grids binomial failure distribution complexity production testing vlsi chips;aproximacion probabilista;probabilistic approach;indexing terms;probability of failure;chip;parallelism;vlsi circuit;paralelismo;complexite algorithme;diagnostic panne;computational complexity;approche probabiliste;fault diagnostic;diagnostico pana;integrated circuit testing;vlsi;system analysis;analyse systeme;vlsi testing;circuito vlsi;verificacion;production testing;diagnostico error;diagnostic erreur;error diagnostic;system level diagnosis	A simple diagnosis algorithm is presented for constant degree systems such as rectangular grids connected as tori. The algorithm determines the status of a unit according to the size of its faction, a cluster of units that call each other fault-free but outsiders faulty. Almost all units are correctly identified with this algorithm under a binomial failure distribution even when the probability of failure is rather high. The complexity of the algorithm is O(n), where n is the number of units in a constant degree system. The application of the algorithm to production testing of VLSI chips is also considered. With a test board that houses a large number of chips to be tested, all the chips can be tested in parallel in a way that they test each other and the test outcomes, not necessarily correct, are reported to a host system for analysis. The actual status of each chip is determined by using this new diagnosis algorithm. The above chip screening process can be repeated for higher accuracy. It is shown that no more than two steps are needed in most real situations. Compared with testing by test equipment that usually tests only one chip at a time, the saving of test time and the test equipment cost could be significant with our approach. >	medical algorithm;very-large-scale integration	Kaiyuan Huang;Vinod K. Agarwal;Laurence E. LaForge;Krishnaiyan Thulasiraman	1995	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.372790	chip;parallel computing;verification;index term;telecommunications;computer science;theoretical computer science;system analysis;very-large-scale integration;computational complexity theory;algorithm	EDA	23.680665240613486	44.52289671769418	156400
ff366e29176a6670a25b660f6f00707718892ca1	some results on the complexity of boolean functions for table look up architectures	sum of products;table lookup boolean functions computational complexity;boolean functions;boolean function;boolean functions upper bound programmable logic arrays circuit synthesis electronics packaging logic circuits contracts logic functions;chip;upper bound;computational complexity;tlu synthesis tool complexity boolean functions table look up architectures upper bounds;technology mapping;table lookup	We address the problem of determining the “complexity” of Boolean functions where complexity is measured as the minimum number of table look up blocks (TLUs) needed to implement a function. We present three new results. The first shows the exact value of the complexity of the class of (m + l)-input functions in terms of the TLUs with m inputs ( m 2 2). The next two derive upper bounds on the complexity, given some information about the representation of the function. One bound needs the number of literals and the number of cubes in a sum-of-products representation, and the other, the number of literals in a factored form. We compare these bounds with the results obtained by a TLU synthesis tool [ 9 ] . On average, the factored form bounds are about 20% higher than the synthesized results, and hence are reasonable predictors of the number of TLUs needed. This prediction capability can be employed to quickly estimate, without performing any technology mapping, if a circuit can fit on one chip.	complexity;olap cube	Rajeev Murgai;Robert K. Brayton;Alberto L. Sangiovanni-Vincentelli	1993		10.1109/ICCD.1993.393325	circuit complexity;boolean circuit;and-inverter graph;circuit minimization for boolean functions;boolean network;majority function;boolean expression;product term;computer science;maximum satisfiability problem;theoretical computer science;karp–lipton theorem;complexity index;boolean function;algorithm;parity function	Theory	18.559276453965	46.13278933105629	156596
01bfa62a3dcc7b0909a3ffb96df5528e4a40f303	"""comments on """"direct implementation of discrete and residue-based functions via optimal encoding: a programmable array logic approach"""""""	red logica programable;memoire adressable contenu;addition and multiplication mod m;circuito logico;positional coding;codificacion;programmable logic array;circuit logique;coding;reseau logique programmable;content addressable memory;optimal residue encoding;complexity of residue addition and multiplication;logic circuit;codage	An analytic expression for the lower bound on the complexity of residue multiplication is developed. Significant reduction of the required stored logic in a content-addressable memory is noted. Errors in Figs. 5(a), 7, and 8 of the above paper' are corrected. Index Terms -Addition-and multiplication mod M, complexity of residue addition and multiplication, content-addressable memory, optimal residue encoding, positional coding. In an interesting paper, ' Papachristou has analyzed the complexity of direct implementation of truth tables by associative logic processing. This information is valuable in evaluating a number of modem computing structures (both electronic and optical) which are based on truth-table look-up processing. The purpose of this correspondence is to present additional insight into the complexity of residue addition and multiplication and to correct some inadvertent errors that appear in the above paper. The lower bounds on the complexity of residue addition and multiplication, which correspond to the positional coding, are given as (11) in the subject paper. The expression for the multiplication should be changed to L(*M) = ITm2f1t1, since i = 1 (included in the summation in the subject paper) is used to indicate the case of having O at the output and this case does not contribute to the complexity. Another relationship for the lower bound on the complexity of residue multiplication that gives more insight and allows easy calculations can be obtained as follows. If the modulus is prime, the corresponding multiplication table is cyclic and consists of M 1 nonzero rows, each row being a permuation of numbers 0, 1, ..M 1. In positional coding, each nonzero number is represented by a 1 located at a particular bit position, hence L(*M) = (M 1)2. If the modulus is not prime, some of the entries of the multiplication table are integer products of the modulus and, therefore, are represented by zeros. Deducting these entries, the exact value for the lower bound on the multiplication complexity is	bit numbering;content-addressable memory;lookup table;matrix multiplication;modem;modulus of continuity;multidimensional digital pre-distortion;programmable array logic	M. M. Mirsalehi;Thomas K. Gaylord	1986	IEEE Trans. Computers	10.1109/TC.1986.1676842	arithmetic;embedded system;parallel computing;logic gate;programmable logic array;computer science;content-addressable memory;mathematics;coding;algorithm	Theory	16.563984585186848	42.84491546279674	156616
67348260b84680a539a4f3df84679b4b5464c702	pancyclicity of ternary n-cube networks under the conditional fault model	procesamiento informacion;fault tolerant;algorithm analysis;conditional edge faults;vertex;interconnection network;pancyclicity;informatique theorique;cycle graphe;68r10;information processing;edge graph;interconnection networks;arete graphe;analyse algorithme;ternary n cubes;vertice;cycle graph;fault model;traitement information;analisis algoritmo;red interconexion;arista grafico;computer theory;reseau interconnexion;ciclo diagrama;informatica teorica	A graph G is said to be conditional k-edge-fault pancyclic if after removing k faulty edges from G, under the assumption that each vertex is incident to at least two fault-free edges, the resulting graph contains a cycle of every length from its girth to |V(G)|. In this paper, we consider ternary n-cube networks and show that they are conditional (4n-5)-edge-fault pancyclic.	fault model	Jing Li;Shiying Wang;Di Liu	2011	Inf. Process. Lett.	10.1016/j.ipl.2011.01.009	vertex;fault tolerance;combinatorics;discrete mathematics;multiple edges;information processing;pancyclic graph;cycle graph;fault model;mathematics;algorithm	DB	23.958169841349196	34.13722629047393	157612
1421194c0ac19089e5a1a9ad55b308d6bf47634e	low cost design of a hybrid architecture of integer inverse dct for h.264, vc-1, avs, and hevc	hardware-share approach;low cost design;symmetric structure;matrix operation;integer inverse discrete cosine;maximum circuit reuse;video standard;hardware cost;advanced video;multiple modern video codecs;integer inverse dct;unified hybrid architecture	hardware-share approach;low cost design;symmetric structure;matrix operation;integer inverse discrete cosine;maximum circuit reuse;video standard;hardware cost;advanced video;multiple modern video codecs;integer inverse dct;unified hybrid architecture	discrete cosine transform;h.264/mpeg-4 avc;high efficiency video coding	Muhammad A. Martuza;Khan A. Wahid	2012	VLSI Design	10.1155/2012/242989	electronic engineering;real-time computing;computer science;theoretical computer science	EDA	11.772480905118297	41.134989751339496	157621
108e191739fa0051daff5766b33c790a783c56a9	diagnosability of regular systems	composante;use;graph theory;isoperimetric inequality;inegalite isoperimetrique;hypercube;concept;secuencial;teoria grafo;isoperimetrical inequality;sequential;maximo;desigualdad;regular system;inequality;bord;component;sistema;estructura;diagnostico;isoperimetric inequalities;inegalite;maximum;syndrome;diagnosability;maillage;sindrome;theorie graphe;utilizacion;consistencia;pmc model;borde;grid;unit;minimo;utilisation;modelo;sequentiel;celdarada;diagnostic panne;edge;rejilla;minimum;fault diagnostic;system;consistance;diagnostico pana;borne inferieure;componente;grille;grid pattern;sequential diagnosis;grafo regular;modele;regular systems;systeme;graphe regulier;diagnosabilite;connected component;modele pmc;diagnosis;structure;models;consistency;lower bound;desigualdad isoperimetrica;unite;unidad;cota inferior;regular graph;concepto;systeme regulier;diagnostic;system level diagnosis;hipercubo	A novel approach aimed at evaluating the diagnosability of regular systems under the PMC model is introduced. The diagnosability is defined as the ability to provide a correct diagnosis, although possibly incomplete. This concept is somehow intermediate between one-step diagnosability and sequential diagnosability. A lower bound to diagnosability is determined by lower bounding the minimum of a “syndrome-dependent” bound tσ over the set of all the admissible syndromes. In turn, tσ is determined by evaluating the cardinality of the smallest consistent fault set containing an aggregate of maximum cardinality. The new approach, which applies to any regular system, relies on the “edgeisoperimetric inequalities” of connected components of units declaring each other nonfaulty. This approach has been used to derive tight lower bounds to the diagnosability of toroidal grids and hypercubes, which improve the existing bounds for the same structures.  2002 Elsevier Science (USA). All rights reserved.	aggregate data;connected component (graph theory);toroidal graph	Antonio Caruso;Stefano Chessa;Piero Maestrini;Paolo Santi	2002	J. Algorithms	10.1016/S0196-6774(02)00250-X	edge;structure;combinatorics;discrete mathematics;unit;connected component;regular graph;graph theory;maxima and minima;inequality;component;system;mathematics;isoperimetric inequality;upper and lower bounds;consistency;grid;concept;algorithm;hypercube	AI	23.541870092989804	33.51841715753842	157680
593641c72c66d43d8f454c68683b0a58ddd5be60	diagnosis of systems with asymmetric invalidation	t readable sets;graph models;asymmetric invalidation;fault tolerant computing;t readable sets asymmetric invalidation diagnosable digital systems diagnosis algorithm diagnosis by part fault tolerant computing graph models;diagnosable digital systems;diagnosis by part;diagnosis algorithm	"""This paper is concerned with system diagnosis through analysis of a set of diagnostic test results. It is assumed that a faulty unit may cause one or more tests on a good unit to fail, but may not cause tests on faulty units to pass (asymmetric invalidation). The system model employed is quite general; each test may be invalidated by any one of a set of units, and each test may completely test more than one unit. Conditions for diagnosability with and without repair are determined. Exact methods, as well as simpler approximate methods, are proposed for determining diagnosability and for performing diagnosis. The theory is extended to include diagnosis """"by part"""" (where a part is a set of units). Several illustrative examples are included."""	approximation algorithm;correlation does not imply causation;runge–kutta methods	Craig S. Holt;James E. Smith	1981	IEEE Transactions on Computers	10.1109/TC.1981.1675868	computer science;theoretical computer science;distributed computing;algorithm	ML	23.843411024430658	44.94870005267139	157908
82f4c3810fc431c7593d993fba97ebdec9515df0	hardware index to permutation converter	knuth shuffle;computers;microprocessors;generators;knuth shuffle reconfigurable computer index to permutation generator random permutation generator combinatorial objects factorial number system;converters;clocks;monte carlo methods combinational circuits cryptography;reconfigurable computer;symposia;indexes;vectors;random permutation generator;cryptography;monte carlo simulations hardware index permutation converter n element permutations unique permutation hash functions parallel machines shared memory cryptographic applications nonnegative integer factorial number system representation combinational circuit src 6 reconfigurable computer microprocessor reconfigurable computer implementation knuth shuffle algorithm;combinatorial objects;permutations;indexes generators microprocessors computers clocks hardware vectors;circuits;article;monte carlo methods;index to permutation generator;factorial number system;hardware;combinational circuits	We demonstrate a circuit that generates a permutation in response to an index. Since there are n! n-element permutations, the index ranges from 0 to n! - 1. Such a circuit is needed in the hardware implementation of unique-permutation hash functions to specify how parallel machines interact through a shared memory. Such a circuit is also needed in cryptographic applications. The circuit is based on the factorial number system. Here, each non-negative integer is uniquely represented as sn-1(n - 1)! + sn-2(n - 2)! +. . . + s11!, where 1 ≤ si ≤ i. That is, the permutation is produced by generating the digits si in the factorial number system representation of the index. The circuit is combinational and is easily pipelined to produce one permutation per clock period. We give experimental results that show the efficiency of our designs. For example. we show that the rate of production of permutations on the SRC-6 reconfigurable computer is 1,820 times faster than a program on a conventional microprocessor in the case of 10-element permutations. We also show an efficient reconfigurable computer implementation that produces random permutations using the Knuth shuffle algorithm. This is useful in Monte Carlo simulations. For both circuits, the complexity is O(n2), and the delay is O(n).	algorithm;clock rate;combinational logic;cryptography;fisher–yates shuffle;hash function;microprocessor;monte carlo method;pipeline (computing);reconfigurable computing;shared memory;simulation	Jon T. Butler;Tsutomu Sasao	2012	2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum	10.1109/IPDPSW.2012.55	parallel computing;cryptography;theoretical computer science;distributed computing;factorial number system;algorithm;statistics	EDA	18.06592031737069	43.24242306629793	158094
51aa39796d730d649f61723bbe1cb9c399e24cdd	a generalized theory for system level diagnosis	system reliability;fiabilite systeme;erreur;multiprocessor;multiprocessor systems;intermittent faults;intermittent;diagnosability;fiabilidad sistema;intermitente;asymmetric invalidation model;characterization;diagnosable systems;error;caracterisation;symmetric invalidation model;multiprocesador;diagnostiquabilite;caracterizacion;characterization of diagnosable systems;system level diagnosis asymmetric invalidation model characterization of diagnosable systems diagnosable systems intermittent faults multiprocessor systems symmetric invalidation model;multiprocesseur;system level diagnosis	System-level diagnosis appears to be a viable alternative to circuit-level testing in complex multiprocessor systems. A completely new generalization of the characterization problem in the system-level diagnosis area is developed in this paper. This generalized characterization theorem provides necessary and sufficient conditions for any fault-pattern of any size to be uniquely diagnosable, under the symmetric, and asymmetric invalidation models with or without the intermittent faults. Moreover, it is also shown that the well known t-characterization theorems under these models can be derived as special cases. In addition to the generalization provided by these results, it is hoped that these results will also have a great impact on the diagnosis of faulty units in uniform structures based on the system-level diagnosis concepts and would be particularly useful in the diagnosis of WSI-oriented multiprocessor systems.	characterization test;multiprocessing;wafer-scale integration;whole earth 'lectronic link	Arun K. Somani;Vinod K. Agarwal;David Avis	1987	IEEE Transactions on Computers	10.1109/TC.1987.1676938	parallel computing;real-time computing;multiprocessing;computer science;operating system;control theory;algorithm	HPC	23.82630032967782	44.76077142451889	158142
bb879085b026a27e6f44890b03c48b46198b646d	solving systems of polynomial congruences modulo a large prime (extended abstract)	polynomials computer science approximation algorithms gaussian processes h infinity control arithmetic equations algebra robots solid modeling;algebraic set polynomial congruences randomized algorithm decision parallel complexity algebraic homotopy method;random polynomials;gaussian processes;approximation algorithms;algebraic homotopy method;randomised algorithms;mathematical logic;polynomials;homotopy method;algebra;parallel complexity;computational complexity;solid modeling;robots;randomized algorithm;decision;arithmetic;algorithms;algebraic set;computer science;h infinity control;functions;parallel processing;polynomial congruences;parallel algorithms;mathematics computers information science management law miscellaneous	We consider the following polynomial congruences problem: given a prime p, and a set of polynomials fi , . . . , fm E Pp [X 1 , . . . , xn] of total degree at most d, solve the system f1 = . . . = fm = 0 for solution(s) in q, We give a randomized algorithm for the decision version of this problem. When the system has Fp -rational solutions our algorithm finds one of them as well as an approximation of the total number of such solutions. For afixed number of variables, the algorithm runs in random polynomial time with parallel complexity poly-logarithmic in d , m and p, using a polynomial number of processors. As an essential step of the algorithm, we also formulate an algebraic homotopy method for extracting components of all dimensions of an algebraic set. The method is efficiently parallelizable.	approximation;central processing unit;decision problem;degree of a polynomial;linear algebra;modulo operation;randomized algorithm;time complexity	Ming-Deh A. Huang;Yiu-Chung Wong	1996		10.1109/SFCS.1996.548470	robot;parallel processing;combinatorics;mathematical logic;discrete mathematics;computer science;gaussian process;mathematics;parallel algorithm;solid modeling;minimal polynomial;randomized algorithm;computational complexity theory;function;approximation algorithm;algorithm;polynomial;algebra	Theory	14.839892065276846	33.32964192217797	158517
2367d7f52b5fc742a8dc5228560e1ce49e2c2e77	can we elect if we cannot compare?	total order;mobile agents;distributed computing;cayley graph;mobile environment;election;leader election;rendezvous;qualitative modeling;mobile agent;anonymous networks;cayley graphs;partial order	"""The aim of this paper is to study the computational power of the qualitative model, where entities are given distinct labels which are however mutually incomparable; this model is opposed to the quantitative model, where labels are integers. The qualitative model captures, for example,the case when the labels are written in different alphabets (e.g., Cyrillic, Latin) and there is no a priori agreement on a common encoding. We investigate the qualitative model through the problem of leader election in a distributed mobile environment. All known leader election protocols assume that the initial input values are distinct and pairwise comparable. While distinctness of the input values is clearly required, the comparability assumption is questionable. Our concern is whether it is possible to remove this comparability assumption. To focus solely on this concern, we consider theproblem in its weakest setting: anonymous highly symmetric networks (i.e.,Cayley graphs). In this way, to break the symmetry (and thus elect a leader) among the incomparable mobile agents, we can not rely on the existence of distinguished node labels nor on any topological asymmetry of the network. We describe a generic election protocol which is effective for all anonymous Cayley graphs; i.e., it solves the election problem if the problem is solvable, otherwise it determines that the problem is not solvable. For arbitrary networks, our protocol is conditionally effective; that is, it performs election of one agent among any set of agents in any network, under some weak conditions on the network and on the initial positions of the agents. Our work is a first step toward a better understanding of the inherent differences between """"quantitative computing"""" where parameters are taken from a total order, and """"qualitative computing"""" where parameters are taken from a partial order."""	computation;decision problem;entity;leader election;magma;mobile agent	Lali Barrière;Paola Flocchini;Pierre Fraigniaud;Nicola Santoro	2003		10.1145/777412.777469	mathematical optimization;combinatorics;discrete mathematics;computer science;cayley graph;leader election;mathematics;distributed computing;algorithm	AI	17.353885086343613	35.40121612225728	158534
199ea7a722cc333fd5c22b0dc10a4d28323faef0	new methods in the analysis of logic minimization data and algorithms	topology;boolean functions;minimization methods;algorithm design and analysis minimization methods logic functions permission topology robustness boolean functions synthesizers equations logic testing;simple cubic;permission;logic functions;logic testing;robustness;synthesizers;algorithm design and analysis	This paper introduces techniques from combinatorial and algebraic topology to help in explaining and measuring the performance of modern logic minimizers. The concepts of simple cubical homotopy and the Euler—Poincare characteristic of a logic cover are defined and analyzed. In particular, simple cubical homotopy is related to the minimization algorithms Espresso—EXACT and Roth's Extraction Algorithm. Experimental results on the Euler—Poincare characteristic, along with a new measure, the Euler Ratio are related to the function complexity concepts of “Cyclic constraints” in Espresso_EXACT, the “CyclicKernel” in Roth's Extraction Algorithm, and “cubical homotopy” introduced in this paper.	algorithm;circuit minimization for boolean functions;euler;linear algebra;realms of the haunting	Alan J. Coppola	1989	26th ACM/IEEE Design Automation Conference	10.1145/74382.74421	algorithm design;mathematical optimization;combinatorics;circuit minimization for boolean functions;electronic engineering;discrete mathematics;cubic crystal system;logic optimization;computer science;mathematics;boolean function;algorithm;robustness	EDA	20.20382220137066	44.8852041224638	158535
55f21826bdda49bc615360e8fae1a84204d648fc	the complexity of deciding stability under ffs in the adversarial queueing model	numerical stability;complexite;network stability;procesamiento informacion;subgrafo;temps polynomial;algorithm analysis;estabilidad numerica;queueing theory;greedy scheduling protocols;packet switched interconnection networks;complejidad;packet switched;complexity;reseau;horario;polynomial;controle;interconnection network;red;adversarial queueing theory;sous graphe;informatique theorique;queueing model;polinomio;68r10;information processing;forbidden subgraphs;polynomial time;schedule;interconnection networks;graph algorithm;vinculo;control;analyse algorithme;stabilite numerique;subgraph;traitement information;polynome;link;graph algorithms;analisis algoritmo;horaire;lien;network;computer theory;tiempo polinomial;check;informatica teorica	We address the problem of deciding whether a given network is stable in the Adversarial Queueing Model when con farthest-from-source(FFS) as the queueing policy to schedule the packets through its links. We show a characterisatio networks which are stable under FFSin terms of a family of forbidden subgraphs. We show that the set of networks stable FFS coincide with the set of universally stable networks. Sin ce universal stabilityof networks can be checked in polynom time, we obtain that stability under FFScan also be decided in polynomial time.  2004 Elsevier B.V. All rights reserved. MSC:68Q25; 68R10; 90B12; 90B15; 90B22	forbidden subgraph problem;naruto shippuden: clash of ninja revolution 3;queueing theory;time complexity	Carme Àlvarez;Maria J. Blesa;Josep Díaz;Antonio Fernández;Maria J. Serna	2004	Inf. Process. Lett.	10.1016/j.ipl.2004.02.016	check;time complexity;combinatorics;discrete mathematics;complexity;link;information processing;lien;computer science;mathematics;queueing theory;schedule;numerical stability;algorithm;scientific control;polynomial	Metrics	22.76487683184595	32.389595988231015	158659
7e787ad48bc6cfad48a6a39855202f5c7a8eb059	implementing division with field programmable gate arrays	cycle time;field programmable gate array;critical path;lookup table	This article presents a method to map digit-recurrence arithmetic algorithms to lookup-table based Field Programmable Gate Arrays (FPGAs). By reducing the number of binary inputs to combinational logic and merging algorithm steps, the strategy creates new simplified functions to decrease logic depth and area. To illustrate this method, a radix-2 digit-recurrence division algorithm is mapped to the Xilinx XC4010, a lookup-table based FPGA. The mapping develops a linear sequential array design that avoids the common problem of large fanout delay in the critical path. This approach has a cycle time independent of precision while requiring approximately the same number of logic blocks as a conventional design.	field-programmable gate array	Marianne E. Louie;Milos D. Ercegovac	1994	VLSI Signal Processing	10.1007/BF02409403	embedded system;parallel computing;real-time computing;macrocell array;lookup table;logic gate;cycle time variation;programmable logic array;computer science;theoretical computer science;critical path method;programmable logic device;complex programmable logic device;simple programmable logic device;or gate;programmable array logic;algorithm;field-programmable gate array	EDA	12.488587745137336	45.70430762755182	158707
bbce76958e566d8925eb8b6fe48edff7fe243bd5	lower bounds for convergence function based clock synchronization	fault tolerant;real time;rate constant;clock synchronization;lower bound	The second requirement says that for any time interval of length 1, the clock of a correct process can drift from rerdtime by at most p. 1+ D, where the constant D, accounting for clock adjustments, is called the rnazimum discontinuity. These requirements imply that the adjustments applicable to correct clocks is bounded by a constant, the so-called maximum correction. We sssume that each process has access to a local hardware clock and that the drift of correct hardware clocks is bounded by a given constant p <1. Since we are in this paper only interested in internal clock synchronization algorithms capable of masking arbitrary failures, when we talk about a synchronization algorithm, we mean an internal clock synchronization algorithm tolerant of arbitraxy failures. Many synchronization algorithms can be described as instances of a generic clock synchronization algorithm using the notion of a convergence function [10]. This generic algorithm can be succinctly described as follows: at the end of each synchronization round each process reads the clocks of all processes and then adjusts its clock value for the next round by applying a convergence function to the clock readings of the current round. A convergence function is a mapping that has a process p and p’s clock readings as parameters and returns a clock value. A synchronization algorithm which can be described as an instance of the above generic algorithm using any convergence function will be termed a convergence junction based algorithm. Convergence function based synchronization algorithms can use the following assumptions to provide internally synchronized clocks:	algorithm;clock signal;clock synchronization;generic programming;reflections of signals on conducting lines;requirement	Christof Fetzer;Flaviu Cristian	1995		10.1145/224964.224980	clock synchronization;fault tolerance;real-time computing;computer science;control theory;distributed computing;upper and lower bounds;reaction rate constant	Theory	17.5507730540623	36.14967143744907	159295
16e86ec155ff417d899cc5e9956bc2d185351fe9	fault testing for reversible circuits	digital signal processing;cell fault model vlsi circuits reversible circuits quantum logic minimal test sets integer linear program binary variables ilp method circuit decomposition approach;information loss;circuit faults;generic algorithm;circuit testing circuit faults quantum computing energy dissipation energy consumption very large scale integration electrical fault detection logic circuits government logic gates;very large scale integration;government;logic circuits;logic testing vlsi integrated circuit testing integer programming linear programming fault diagnosis quantum gates integrated logic circuits;energy dissipation;quantum gates;quantum logic;low power;logic gates;integer programming;quantum computer;energy consumption;logic testing;integrated circuit testing;linear programming;vlsi;circuit testing;integrated logic circuits;technology adoption;fault model;quantum computing;integer linear program;electrical fault detection;fault diagnosis	Applications of reversible circuits can be found in the fields of low-power computation, cryptography, communications, digital signal processing, and the emerging field of quantum computation. Furthermore, prototype circuits for low-power applications are already being fabricated in CMOS. Regardless of the eventual technology adopted, testing is sure to be an important component in any robust implementation. We consider the test-set generation problem. Reversibility affects the testing problem in fundamental ways, making it significantly simpler than for the irreversible case. For example, we show that any test set that detects all single stuck-at faults in a reversible circuit also detects all multiple stuck-at faults. We present efficient test-set constructions for the standard stuck-at fault model, as well as the usually intractable cell-fault model. We also give a practical test-set generation algorithm, based on an integer linear programming formulation, that yields test sets approximately half the size of those produced by conventional automatic test pattern generation.	algorithm;cmos;cell (microprocessor);computation;cryptography;digital signal processing;fault detection and isolation;fault model;heuristic;integer programming;library (computing);linear programming formulation;low-power broadcasting;prototype;quantum circuit;quantum computing;reversible computing;robustness (computer science);simulation;stuck-at fault;test card;test set	Ketan N. Patel;John P. Hayes;Igor L. Markov	2003	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/VTEST.2003.1197682	integer programming;logic gate;computer science;linear programming;theoretical computer science;mathematics;very-large-scale integration;quantum computer;physics;algorithm;quantum mechanics	EDA	17.060007993190393	45.78021966485558	159486
8947cd03d0f55116a80a241987943ce9dd69832a	high-speed multiplication and multiple summand addition	full adder delays high speed multiplication multiple summand addition summand generation summand summation 2s complement 32 bit floating point multiplication ecl lsi packages mxm bit multipliers column summing counters nonpropagating carry bits;handheld computer;radiation detectors;radiation detector;radiation detectors adders delay large scale integration arrays handheld computers;higher order;arrays;floating point arithmetic adders;large scale integration;adders;floating point;floating point arithmetic;high speed;handheld computers	The problem of high-speed multiplication is considered from the viewpoint of summand generation and summand summation. The goal is to obtain at least a 2's-complement, 32-bit floating-point (sign plus 24-bit fraction) multiplication in 10 to 20 ns using ECL LSI packages. Summand generation is implemented by mxm-bit multipliers. The optimum values for m are 9, 13, 17, or 21. Summand summation is implemented by a row of (p, 2) column-summing counters. The (3, 2), (5, 2), and (7, 2) counters are optimum choices. These counters compress p inputs into two outputs plus nonpropagating carry bits, where these bits are added to the next higher-order stage with at most two full adder delays.	24-bit;32-bit;adder (electronics);two's complement	Raymond S. Lim	1978	1978 IEEE 4th Symposium onomputer Arithmetic (ARITH)	10.1109/ARITH.1978.6155788	arithmetic;computer hardware;computer science;floating point;operating system;mathematics;particle detector	Theory	14.738788078162209	44.218611730318244	159790
080628a45f335969dc49483598bbdfada102ca9e	a review, classification, and comparative evaluation of approximate arithmetic circuits		Often as the most important arithmetic modules in a processor, adders, multipliers, and dividers determine the performance and energy efficiency of many computing tasks. The demand of higher speed and power efficiency, as well as the feature of error resilience in many applications (e.g., multimedia, recognition, and data analytics), have driven the development of approximate arithmetic design. In this article, a review and classification are presented for the current designs of approximate arithmetic circuits including adders, multipliers, and dividers. A comprehensive and comparative evaluation of their error and circuit characteristics is performed for understanding the features of various designs. By using approximate multipliers and adders, the circuit for an image processing application consumes as little as 47% of the power and 36% of the power-delay product of an accurate design while achieving similar image processing quality. Improvements in delay, power, and area are obtained for the detection of differences in images by using approximate dividers.	adder (electronics);approximation algorithm;booth's multiplication algorithm;cpu power dissipation;curve fitting;erdős–rényi model;image processing;iterated conditional modes;library oriented architecture;logic simulation;lookup table;most significant bit;operand;overhead (computing);performance per watt;power–delay product;programmed data processor;rounding;speculative execution;truncated differential cryptanalysis;truncation;usb on-the-go;unsharp masking	Honglan Jiang;Cong Liu;Leibo Liu;Fabrizio Lombardi;Jie Han	2017	JETC	10.1145/3094124	image processing;computer science;theoretical computer science;electronic engineering;electronic circuit;electrical efficiency;arithmetic;approximate computing;data analysis;adder	EDA	13.636719217713969	42.43416554879099	159898
a5df7f1d7bc1928129dc4b82fda9cf886be497e7	mean-field framework for performance evaluation of push-pull gossip protocols	stochastic process;performance evaluation;network analysis;mean field;gossip protocol;gossip protocols;mean field approximation;quantitative evaluation	Gossip protocols are designed to operate in very large, decentralised networks. A node in such a network bases its decision to interact (gossip) with another node on its partial view of the global system. Because of the size of these networks, analysis of gossip protocols is mostly done using simulations, but even they tend to be expensive in computation time and memory consumption. We employ mean-field approximation for an analytical evaluation of gossip protocols. Nodes in the network are represented by small identical stochastic processes. Joining all nodes would result in an enormous stochastic process. If the number of nodes goes to infinity, however, mean-field analysis allows us to replace this intractably large stochastic process by a small deterministic process. This process approximates the behaviour of very large gossip networks, and can be evaluated using simple matrix-vector multiplications.	apollonian network;approximation;computation;computational complexity theory;gossip protocol;performance evaluation;simulation;stochastic process;time complexity	Rena Bakhshi;Lucia Cloth;Wan Fokkink;Boudewijn R. Haverkort	2011	Perform. Eval.	10.1016/j.peva.2010.08.025	stochastic process;gossip protocol;real-time computing;computer science;theoretical computer science;mean field theory;mathematics;distributed computing;statistics	Metrics	16.196717011061097	35.509286317167415	160478
d2a2945ce36d6ed9b44355d92b1a032bfbd577cf	hardware-efficient architecture of photo core transform in jpeg xr for low-cost applications	two dimensional displays;transform coding;multiplexing;computer architecture;adders;transforms;hardware	This paper proposes a novel two-input, two-output architecture of photo core transform (PCT) in JPEG XR. First, the lifting operations of PCT are optimized such that some operations can be reused. Then the time multiplexing technique is used by combining lifting steps to design a hardware-efficient architecture. Experimental results based on field programmable gate array (FPGA) demonstrate that our architecture has good performance in reducing hardware resources and power consumption, which could be an efficient alternative in low-cost applications.	computation;field-programmable gate array;jpeg xr;lambda lifting;lifting scheme;multiplexer;multiplexing;very-large-scale integration	Shuiping Zhang;Huabing Zhou;Haihui Wang;Xia Hua	2016	2016 15th International Symposium on Parallel and Distributed Computing (ISPDC)	10.1109/ISPDC.2016.56	embedded system;parallel computing;transform coding;computer hardware;computer science;multiplexing;adder;computer graphics (images)	Arch	11.730369073680677	41.128044992215344	160773
0375e4067077e7300971a045356bd76c18a2ad3b	on running windowed image computations on a pipeline	image processing;computational modeling;tiles pipelines image processing bandwidth parallel processing computational modeling cameras;windowed computation;pipelines;bandwidth;pipeline processing image sequences;image algorithms;windowed computation image algorithms pipeline;tiles;parallel processing;cameras;pipeline;pipeline processing;overhead windowed image computation sequence image processing image pixel size windowed operation size z stage pipelined computational model input output bandwidth;image sequences	Many image processing operations manipulate an individual pixel using the values of other pixels in the given pixel's neighborhood. Such operations are called windowed operations. The size of the windowed operation is a measure of the size of the given pixel's neighborhood. A windowed computation applies a windowed operation on all pixels of the image. An image processing application is typically a sequence of windowed computations. While windowed computations admit high parallelism, the cost of inputting and outputting the image often restricts the computation to a few computational units. In this paper we analytically study the running of a sequence of z windowed computations, each of size w, on a z-stage pipelined computational model. For an N × N image and n × n input/output bandwidth per stage, we show that the sequence of windowed computations can be run in N2/n2 (1 + δ) steps, where δ = (n/N + 3n2/wN + zw/N). This produces a speed-up of z/1+δ over a single stage. Generally, N ≫ n >; z, w; so the overhead, δ, is dominated by the term which is typically small. This also indicates the time to be relatively independent of the number of stages z.	computation;computational model;image processing;input/output;overhead (computing);parallel computing;pixel;window function	Ramachandran Vaidyanathan;Phaneendra Vinukonda	2012	2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum	10.1109/IPDPSW.2012.100	parallel processing;parallel computing;computer hardware;image processing;computer science;theoretical computer science;operating system;pipeline transport;computational model;pipeline;bandwidth	Arch	11.582118194140914	38.82703852054054	160775
a84ae1eb3b6b52c0bd479c014f9272bf662d45b7	path resolution with link deletion	connected graph;graphical representation;normal form	We introduce a graphical representation of quantifier-free predicate calculus formulas and a new rule of inference which employs this representation. The new rule is an amalgamation of resolution and Prawitz analysis which we call path resolution. Path resolution allows Prawitz analysis of an arbitrary subgraph of the graph representing a formula. If such a subgraph is not large enough to demonstrate a contradiction, a path resolvent of the subgraph may be generated with respect to the entire graph. This generalizes the notions of large inference present iu hyperresolution, clash-resolution, NC-resolution, and PLresolution. Two forms of path resolution are described for which deletion of the links resolved upon preserves the spanning property.	file spanning;first-order logic;induced subgraph;quantifier (logic);resolution (logic)	Neil V. Murray;Erik Rosenthal	1985			combinatorics;discrete mathematics;topology;computer science;connectivity;subgraph isomorphism problem;graph factorization;mathematics;distance-hereditary graph;induced subgraph isomorphism problem;induced path	AI	21.580335594042324	32.60499365316079	161213
f8be00b422d29098db0af4dd775af5ba7b7c3f02	effective multi-standard macroblock prediction vlsi design for reconfigurable multimedia systems	chinese avs jizhun profile multistandard macroblock prediction vlsi design reconfigurable multimedia systems reconfigurable computing arrays computation intensive algorithms multimedia processing control intensive algorithms memory efficient macroblock prediction boundary strength calculation engine motion vector prediction real time decoding h 264 avc high profile;motion vector prediction;decoding;reconfigurable computing;reconfigurable architectures;very large scale integration;real time;vlsi decoding multimedia computing real time systems reconfigurable architectures video coding;prediction algorithms;boundary strength calculation engine;multimedia processing;very large scale integrated;vlsi design;multimedia systems;multimedia computing;video coding;computer architecture;memory efficient macroblock prediction;multistandard macroblock prediction vlsi design;registers;motion vector;registers decoding computer architecture very large scale integration prediction algorithms pixel multimedia systems;pixel;reconfigurable computing arrays;vlsi;reconfigurable multimedia systems;chinese avs jizhun profile;real time decoding;high performance;control intensive algorithms;computation intensive algorithms;h 264 avc high profile;real time systems	Reconfigurable computing arrays facilitate the flexibility with high performance for regular and computation-intensive algorithms in multimedia processing. However, the efficiency of the irregular and control-intensive algorithms becomes the performance bottleneck of reconfigurable multimedia systems. In this paper, we propose the design and VLSI implementation of a novel memory efficient macroblock prediction and boundary strength (Bs) calculation engine. The control-intensive algorithms, including intra mode prediction, motion vector prediction, and Bs calculation, are implemented with 4x4 block level pipeline to achieve real-time decoding for H.264/AVC high profile and Chinese AVS Jizhun profile. Compared with existing designs, our design achieves 60% registers reduction for neighboring block load and update. Implementation results indicate that the proposed architecture can support 1920×1088@30fps of H.264 and AVS decoding at 86 MHz.	advanced visualization studio;algorithm;computation;h.264/mpeg-4 avc;macroblock;megabyte;processor register;real-time clock;reconfigurable computing;throughput;very-large-scale integration;video decoder	Yuliang Tao;Guanghui He;Weifeng He;Qin Wang;Jun Ma;Zhigang Mao	2011	2011 IEEE International Symposium of Circuits and Systems (ISCAS)	10.1109/ISCAS.2011.5937856	embedded system;parallel computing;real-time computing;computer science;electrical engineering;theoretical computer science;very-large-scale integration	Arch	12.414610920308256	40.226489715805194	161263
083efa1fa0efb653c93d9a9b31078e82d4718ab6	practical algorithms for image component labeling on simd mesh connected computers	connected component;parallel processing;parallel algorithm;image processing;binary image;mathematical logic;compression;time complexity;algorithms;processing;two dimensional	Abstract : Two new parallel algorithms are presented for the problem of labeling the connected components of a binary image, which is also known as the connected ones problem. The machine model is an SIMD two-dimensional mesh connected computer consisting of an N x N array of processing elements, each containing a single pixel of an N x N image. Both new algorithms use a shrinking operation defined by Levialdi and have time complexities of O(N log N) bit operations, which makes them the fastest local algorithms for the problem. Compared with other approaches having similar or better time complexities, this local approach dramatically simplifies the algorithms and reduces the constants of proportionality by nearly two orders of magnitude, thus making them the first practical algorithms for the problem. The two algorithms differ in the amount of memory required per processing element; the first uses O(N) bits while the second employs a novel compression scheme to reduce the requirement to O(log N) bits.	algorithm;simd	Robert Cypher;Jorge L. C. Sanz;L. Snyder	1987			time complexity;local algorithm;distributed computing;binary image;parallel computing;binary logarithm;image processing;parallel algorithm;theoretical computer science;algorithm;simd;computer science;connected component	Vision	12.24978754208899	36.32179739624795	161581
0d2693c2af407b7382604a89e06b6842a8f46182	the word-level models for efficient computation of multiple-valued functions. part 2: lwl based model	linear arithmetical expression;feedforward neural network;decision diagrams;computational modeling semiconductor device modeling computer networks delay integrated circuit interconnections planarization boolean functions computer science information systems logic;information systems;logic circuit design word level models multiple valued functions lar based model multilevel combinational multiple valued logic circuit neuron like gates mvl circuit neural like network feedforward neural networks linear arithmetic expression linear word level decision diagram experiments circuit simulation memory ternary gates threshold gate based network;boolean functions;word level models;word level decision diagrams;decision diagram;logic;multivalued logic circuits;threshold gates;boolean functions multivalued logic multivalued logic circuits neural chips feedforward neural nets logic gates decision diagrams;linear weighted logic expressions;linear decision diagrams;computer networks;neuron like gates;circuit simulation;neural chips;computational modeling;logic gates;multiple valued function computation;neural like networks;semiconductor device modeling;computational modeling logic functions logic gates neural networks circuit simulation logic design very large scale integration logic circuits computer science combinational circuits;integrated circuit interconnections;feedforward neural nets;computer science;planarization;logic gates multivalued logic circuits multivalued logic feedforward neural nets neural chips decision diagrams circuit simulation;multiple valued logic circuits;multiple valued;multivalued logic;boolean functions word level models multiple valued function computation neural like networks multiple valued logic feedforward neural network threshold gates neuron like gates linear arithmetical expression linear weighted logic expressions linear decision diagrams experimental study memory;memory;neural network;multiple valued logic	A new model of a multi-level combinational multiple-valued logic (MVL) circuit with no feedback and no learning is introduced. This model includes neuron-like gates (NLGs), each represents a level of the MVL circuit, so that the number of NLGs in the corresponding neural-like network (NLN) is equal to the number of levels in the circuit. The formal description of an NLG is a linear arithmetic expression (LAR) that is directly mapped to the linear word-level decision diagram (LDD) planar by its nature. Thus, an l-level MVL circuit is described by a set of l LDDs. The experiments on simulation of large MVL circuits show that the LDD format of an MVL circuit consumes 5-20 times less memory than EDIF and ISCAS formats. The proposed technique allows to simulate an arbitrary MVL circuit by an NLN and corresponding set of LDDs. In particular, we successfully simulated an NLN with about 250 NLGs corresponding to an MVL circuit with more than 8000 ternary gates that has been impossible by any recently reported threshold gate-based network.	computation;ladies who lunch	Anna Lewandowska;Svetlana N. Yanushkevich;Vlad P. Shmerko	2002		10.1109/ISMVL.2002.1011091	feedforward neural network;electronic engineering;semiconductor device modeling;computer science;theoretical computer science;machine learning;mathematics;memory;logic;artificial neural network;algorithm	Crypto	19.63663628155208	45.10795307049934	161709
a03ec981d4a1e7103c0e48a1e36ba0851fb9bd0c	an implemented graph algorithm for winning shannon switching games	graph theory;computer program;graph processing;game theory;shannon switching games;demonstration programs;spinning trees;graph algorithm;game playing;graph algorithms;positional games	In this tutorial paper a computer program which wins Shannon Switching Games is described. Since these games are played on graphs, the program is a good example of the implementation of graph algorithms. The two players in a Shannon Switching Game, CONNECT and CUT, have nonsimilar goals. Either CONNECT, CUT, or the player moving first is guaranteed the existence of a winning strategy. The simple strategy explained in this paper is valid in all three cases. In fact, the major routines never need to know whether the computer is CONNECT or CUT.	algorithm;computer program;list of algorithms;need to know;shannon (unit)	Stephen M. Chase	1972	Commun. ACM	10.1145/361284.361293	combinatorial game theory;game theory;combinatorics;signal-flow graph;computer science;graph theory;theoretical computer science;mathematics;distributed computing;graph;moral graph;shannon capacity of a graph;algorithm	Theory	20.109807659066174	32.64740450063868	161723
f0dcc888c18cdc4285cc1b30f43ac7debfd2dd0e	memory-efficient high-speed convolution-based generic structure for multilevel 2-d dwt	discrete wavelet transforms;multiplying circuits;multiplying circuits adders convolution discrete wavelet transforms filtering theory memory architecture;very large scale integration vlsi 2 d discrete wavelet transform dwt dwt lifting systolic array;convolution;discrete wavelet transforms complexity theory memory management system on a chip random access memory hardware;memory architecture;adders;filtering theory;parallel lifting based structure memory efficient high speed convolution based generic structure multilevel 2d dwt design strategy memory efficient architecture convolution based generic architecture biorthogonal filters frame buffer line buffers daubechies wavelet filter biorthogonal wavelet filter image height data flow hardware utilization efficiency daub 4 filter multipliers adders arithmetic components lifting based structures substantial reduction memory size clock period asic synthesis area delay product adp energy per image epi convolution based structure	In this paper, we have proposed a design strategy for the derivation of memory-efficient architecture for multilevel 2-D DWT. Using the proposed design scheme, we have derived a convolution-based generic architecture for the computation of three-level 2-D DWT based on Daubechies (Daub) as well as biorthogonal filters. The proposed structure does not involve frame-buffer. It involves line-buffers of size 3(K-2)M/4 which is independent of throughput-rate, where K is the order of Daubechies/biorthogonal wavelet filter and M is the image height. This is a major advantage when the structure is implemented for higher throughput. The structure has regular data-flow, small cycle period TM and 100% hardware utilization efficiency. As per theoretical estimate, for image size 512 × 512, the proposed structure for Daub-4 filter requires 152 more multipliers and 114 more adders, but involves 82 412 less memory words and takes 10.5 times less time to compute three-level 2-D DWT than the best of the existing convolution-based folded structures. Similarly, compared with the best of the existing lifting-based folded structures, proposed structure for 9/7-filter involves 93 more multipliers and 166 more adders, but uses 85 317 less memory words and requires 2.625 times less computation time for the same image size. It involves 90 (nearly 47.6%) more multipliers and 118 (nearly 40.1%) more adders, but requires 2723 less memory words than the recently proposed parallel structure and performs the computation in nearly half the time of the other. Inspite of having more arithmetic components than the lifting-based structures, the proposed structure offers significant saving of area and power over the other due to substantial reduction in memory size and smaller clock-period. ASIC synthesis result shows that, the proposed structure for Daub-4 involves 1.7 times less area-delay-product (ADP) and consumes 1.21 times less energy per image (EPI) than the corresponding best available convolution-based structure. It involves 2.6 times less ADP and consumes 1.48 times less EPI than the parallel lifting-based structure.	algorithmic efficiency;application-specific integrated circuit;biorthogonal wavelet;computation;convolution;dataflow;discrete wavelet transform;framebuffer;image resolution;lambda lifting;throughput;time complexity;usb on-the-go;very-large-scale integration	Basant K. Mohanty;Pramod Kumar Meher	2013	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2012.2203745	discrete mathematics;second-generation wavelet transform;theoretical computer science;mathematics;convolution;adder	Mobile	12.47766069874312	43.17887718414327	161955
f6de0532a2a76be941861e187913ec048c2df661	the complexity of a pipelined algorithm for remainder computing in a given modulo		A pipelined algorithm for computing the remainder when dividing an arbitrary binary number of a given bit capacity (numerator) by a certain constant value (a constant) is proposed. The algorithm is based on the same types of operations of comparisons and addition–subtraction of partial remainders upon division by this constant. Depending on whether an intermediate result during computation of the remainder is positive or negative, addition with the value of the intermediate result or subtraction from it of the remainder upon division of a given power of two occur. The number of algorithm stages compared with the model is known in advance and depends on the bit capacities of both the dividend and constants. The estimates of the time complexity of the proposed pipelined algorithm are determined by the maximum delay time of operation of the pipeline stage. The estimates of the hardware complexity of the proposed algorithm, as well as the model of the device that implements the algorithm, are determined at the abstract and structural levels.	algorithm;binary number;computation;modulo operation;power of two;time complexity	Vyacheslav M. Zakharov;Valerii A. Pesoshin;Sergey V. Shalagin	2018	Automatic Control and Computer Sciences	10.3103/S0146411618040120	mathematical optimization;fraction (mathematics);time complexity;power of two;subtraction;computation;remainder;algorithm;computer science;binary number;division (mathematics)	Theory	16.40340425418288	41.60097926801834	162039
436e18f6ae921b70e04efc3905c35a703f7ecf22	on-line adaptive parallel prefix computation	parallelisme;distributed system;algoritmo paralelo;metodo adaptativo;vol criminel;work stealing;codigo prefijo;systeme reparti;robo;parallel algorithm;algoritmo adaptativo;distributed computing;methode adaptative;prefix code;criminal theft;parallel computation;algorithme parallele;adaptive algorithm;parallelism;calculo paralelo;sistema repartido;algorithme adaptatif;paralelismo;scheduling;adaptive method;borne inferieure;calculo repartido;algoritmo optimo;algorithme optimal;optimal algorithm;calcul parallele;calcul reparti;ordonnancement;lower bound;reglamento;cota inferior;code prefixe	We consider parallel prefix computation on processors of different and possibly changing speeds. Extending previous works on identical processors, we provide a lower bound for this problem. We introduce a new adaptive algorithm which is based on the on-line recursive coupling of an optimal sequential algorithm and a parallel one, non-optimal but recursive and fine-grain. The coupling relies on a work-stealing scheduling. Its theoretical performance is analysed on p processors of different and changing speeds. It is close to the lower bound both on identical processors and close to the lower bound for processors of changing speeds. Experiments performed on an eight-processor machine confirms this theoretical result.	adaptive algorithm;ambiguous name resolution;asymptotically optimal algorithm;central processing unit;computation;degree of parallelism;experiment;linear system;online and offline;parallel algorithm;parallel computing;recursion;resolution (logic);scheduling (computing);sequential algorithm;symmetric multiprocessing;work stealing	Jean-Louis Roch;Daouda Traoré;Julien Bernard	2006		10.1007/11823285_88	prefix code;parallel computing;computer science;distributed computing;parallel algorithm;upper and lower bounds;scheduling;algorithm	HPC	10.541130325473222	32.346940931652625	162097
c7d592b51478a8751c60e96ec9713adff48e1fdb	high-speed hardware implementation of fixed and runtime variable window length 1-d median filters	circuits and systems;clocks;radiation detectors;computer architecture;registers;runtime configuration median filters hardware implementation;chlorine;benchmark results 1d median filters high speed hardware implementation fixed variable window length runtime variable window length nonlinear digital filters maximum working clock frequency hardware complexity;computer architecture clocks registers hardware radiation detectors chlorine circuits and systems;median filters clocks;hardware	Nonlinear digital filters play an important role in digital signal processing applications. In this brief, a novel architecture is proposed for the hardware implementation of fixed and runtime variable window length one-dimensional median filters. In the proposed architecture, the maximum working clock frequency is almost independent of the median filter window length, whereas the hardware complexity is proportional to the number of samples in the window. This feature enables the construction of filters with relatively large window lengths with negligible reduction in the maximum clock frequency, whereas in previous architectures, the maximum clock frequency significantly drops as the window length is increased. The benchmark results show the efficiency of the proposed architecture in comparison with state-of-the-art techniques.	benchmark (computing);clock rate;code;combinational logic;critical path method;digital filter;digital signal processing;field-programmability;field-programmable gate array;median filter;throughput	Eesa Nikahd;Payman Behnam;Reza Sameni	2016	IEEE Transactions on Circuits and Systems II: Express Briefs	10.1109/TCSII.2015.2504945	chlorine;embedded system;electronic engineering;real-time computing;computer science;processor register;particle detector	EDA	12.121197033912459	44.89935834051426	162247
e4339b5a7810459074d88927a070586dad0b5d62	a hardware/software co-design of high efficiency aac audio decoder	audio compression;he-aac;codec;low power;ip;vlsi;sopc	This paper presents an implementation of hardware/software co-design for high efficiency advanced-audio-coding (HE-AAC) audio decoder. The decoder system is partitioned into software and hardware part throughout the computation analysis. In our design strategy, the bitstream parser and lower complexity part are performed by software solution, and the higher complexity part is computed by hardware solution. As in the dedicated hardware, four units are developed to cope with IMDCT, analysis quadrature mirror filterbank (AQMF), HF generator and envelope adjuster and synthesis quadrature mirror filterbank (SQMF). To support the various types of transformation-based functions in HE-AAC decoding, we manipulate it based on the decomposition of common radix-2 FFT method. The hardware part is designed as an intellectual property (IP) by TSMC 90 nm library. As a cost-effective design, it consumes about 150 K gates and executes at a very low operation frequency with 1.75 MHz. The power consumption is only 7.69 mW with some low power design considerations. Moreover we construct the overall system including the wrapper design and embedded platform as a system on a programmable chip (SOPC) platform. With this design approach, over 91.26 % processor-based loading can be saved and substituted by this hardware IP.	advanced audio coding	Tsung-Han Tsai;De-Ming Chen	2017	Signal Processing Systems	10.1007/s11265-016-1165-y	chip;parallel computing;computer hardware;real-time computing;very-large-scale integration;dynamic range compression;codec;software;bitstream;advanced audio coding;computer science;design strategy	ML	11.616252061194176	42.964957146319776	162471
c3d77af42a7fbfcb5daf53dbd09ab5b2446334e8	improved complexity of quantum oracles for ternary grover algorithm for graph coloring	computers;graph theory;map;toffoli quantum ternary algorithm grover s oracle complexity cost map coloring;mirrors;quantum computers quantum oracles ternary grover algorithm graph coloring ternary quantum circuits colored graphs;logic gates color mirrors algorithm design and analysis quantum computing complexity theory computers;complexity theory;quantum optics computational complexity graph theory;color;quantum oracles;quantum optics;coloring;toffoli;graph coloring;complexity;logic circuits design and construction;colored graphs;algorithm;grover s;logic synthesis;logic gates;quantum computer;quantum circuits;computational complexity;quantum;ternary;computer algorithims;ternary grover algorithm;quantum computing;ternary quantum circuits;cost;algorithm design and analysis;oracle;quantum computers	The paper presents a generalization of the well-known Grover Algorithm to operate on ternary quantum circuits. We compare complexity of oracles and some of their commonly used components for binary and ternary cases and various sizes and densities of colored graphs. We show that ternary encoding leads to quantum circuits that have significantly less qud its and lower quantum costs. In case of serial realization of quantum computers, our ternary algorithms and circuits are also faster.	bitwise operation;computer;graph coloring;grover's algorithm;oracle machine;quantum circuit;quantum computing;whole earth 'lectronic link	Yushi Wang;Marek A. Perkowski	2011	2011 41st IEEE International Symposium on Multiple-Valued Logic	10.1109/ISMVL.2011.42	combinatorics;quantum complexity theory;computer science;graph theory;theoretical computer science;quantum optics;mathematics;quantum computer;quantum algorithm;algorithm;quantum mechanics	Arch	21.462369397329994	42.7260816006243	162964
4c4e52592126212a18a97de309d264ea4eace48b	an efficient external-memory implementation of region query with application to area routing	memoire;diseno circuito;area routing;degradation;external memory implementation;region query data structure;application software;geometrie algorithmique;routing;very large scale integration;implementation;circuit design;computational geometry;circuit vlsi;routage;spatial locality external memory implementation region query area routing tile cached kd tree two dimensional region query in memory algorithms paging behavior region query data structure;circuit layout cad tree data structures vlsi circuit complexity;routing tiles spatial databases data structures application software very large scale integration algorithm design and analysis degradation computational geometry multidimensional systems;tree data structures;circuit complexity;vlsi circuit;memoria;data structures;paging behavior;spatial databases;region query;in memory algorithms;kd tree;vlsi;two dimensional region query;circuit layout cad;geometria computacional;conception circuit;tiles;spatial locality;external memory;circuito vlsi;external memory algorithms;implementacion;tile cached kd tree;data structure;algorithm design and analysis;multidimensional systems;memory;enrutamiento	"""The ICCD encompasses a wide range of topics in the research, design, and implementation of computer systems and their components. The ICCD's unique multidisciplinary emphasis provides an ideal environment for developers and researchers to discuss practical and theoretical work covering system and processor architecture, logic and circuit design, verification and test methods, along with tools and methodologies. A primary goal of ICCD was to present a technical program of the highest quality to its participants and bring together top researchers and developers from academic institutions, research laboratories, and high-technology companies from around the world. The 173 paper submissions came from 19 different countries. Of the 173 papers originally submitted to the conference, the program committee worked diligently to select 47 regular papers for presentation. The program committee invited the authors of the most well-received papers at the conference to submit extended versions for consideration in this Special Section. Upon careful review, the following five papers were selected, which appropriately reflect the broad multidisciplinary nature of the conference. The first two papers consider hardware for encryption and compression. The paper by Nikara et al., """" Multiple-symbol parallel decoding for variable length codes """" presents a multiple-symbol parallel variable-length decoding scheme, including an efficient hardware implementation. When applied to MPEG-2 standard benchmark scenes, an average of 4.8 codewords are decoded per cycle, resulting in the throughput of 106 million symbols per second. The paper by Morioka and Satoh, """" A 10-Gbps full-AES crypto design with a twisted-BDD S-Box architecture """" describes the design of a high-speed core for advanced en-cryption standard (AES) which runs at 780 MHz in a 0.13-m CMOS process and achieves a 10-Gbps throughput. A critical hardware component that defies pipelining is the S-box and the authors propose a new architecture that reduces latency by a factor-of-two over previous implementations. The ICCD always has a good representation of papers on new design methodologies and tools flows. The third paper """" Designing an asynchronous microcontroller using Pipefitter """" by Blunno and Lavagno, describes a design methodology and synthesis flow for asynchronous circuits that uses Verilog both as a specification language and as an intermediate format. This allows """" standard """" EDA tools to be used for most parts of the design process, including synthesis, simulation, and layout. The fourth paper by Ishaq et al., """" Fitted Elmore delay: A simple and accurate interconnect delay model """" describes an enhanced Elmore …"""	benchmark (computing);cmos;charge-coupled device;circuit design;code word;elmore delay;encryption;mpeg-2;microcontroller;norm (social);pipeline (computing);routing;s-box;simulation;specification language;speech synthesis;throughput;twisted;variable-length code;verilog	Stan Y. Liao;Narendra V. Shenoy;William Nicholls	2002		10.1109/ICCD.2002.1106744	embedded system;electronic engineering;parallel computing;data structure;computer science;theoretical computer science;operating system;machine learning;database;very-large-scale integration;programming language;algorithm	EDA	12.677464198809878	36.31305234218712	163596
56d34b8562bdb86c871016d3fd3be2c7af2e284a	the comparison approach to multiprocessor fault diagnosis	bernoulli trial;assignment;tolerancia falta;computers;digital computers;system reliability;syndrome comparison assignment diagnosis fault tolerance multiprocessor systems self diagnosable systems;reliability;fiabilite systeme;general and miscellaneous mathematics computing and information science;systems analysis 990210 supercomputers 1987 1989;probability;fault tolerant;multiprocessor;multiprocessor systems;test comparacion;efficiency;prueba bernoulli;syndrome;failure mode;diagnosability;mathematical logic;sindrome;self diagnosable systems;fiabilidad sistema;algorithme;failure analysis;algorithm;comparison assignment;failure mode analysis;algorritmo;diagnostic panne;fault diagnostic;array processors;fault tolerance;comparative evaluations;diagnostico pana;system analysis;afectacion;algorithms;planning;affectation;system failure analysis;comparison test;multiprocesador;diagnosis;tirage bernoulli;diagnostiquabilite;tolerance faute;fault tolerant computers;test comparaison;fault diagnosis;multiprocesseur	In this correspondence a system-level, comparison-based strategy for identifying faulty processors in a multiprocessor system is described. Unlike other strategies which have been proposed in the literature, the comparison approach is more efficient and relies on more realistic assumptions about the system under consideration. The new strategy is shown to correctly identify the set of faulty processors with a remarkably high probability, making it an attractive and viable addition or alternative to present fault diagnosis techniques.	central processing unit;multiprocessing	Anton T. Dahbura;Krishan K. Sabnani;Linda L. King	1987	IEEE Transactions on Computers	10.1109/TC.1987.1676912	embedded system;fault tolerance;parallel computing;real-time computing;computer science;failure mode and effects analysis;algorithm;statistics	DB	23.712232151577666	44.51375723866838	163605
bd9f1ce8ec2cff067add1f8bafbfc2ffcd05e5fb	an internal partial dynamic reconfiguration implementation of the jpeg encoder for low-cost fpgasb	internal partial dynamic reconfiguration;quantization;image coding;hardware software codesign;huffman coding;reconfigurable computing;dynamic reconfiguration;image converters;reconfigurable architectures;compression algorithms;hw sw architecture;reconfigurable architectures field programmable gate arrays hardware software codesign image coding;jpeg encoder;fpga;transform coding;computer architecture;real world application;reconfigurable computing internal partial dynamic reconfiguration jpeg encoder fpga hw sw architecture hw cores;discrete cosine transforms;field programmable gate arrays;hardware field programmable gate arrays computer architecture image converters quantization reconfigurable architectures transform coding compression algorithms huffman coding discrete cosine transforms;hw cores;hardware	This paper presents the design of a JPEG encoder which exploits this feature. We propose a mixed HW/SW architecture, where most compute-intensive components of the application are mapped to application-specific HW cores. These cores can be alternated on the FPGA, by means of internal dynamic reconfiguration. Our purpose is to describe a real-world application of reconfigurable computing, illustrating how this approach allows to save area with negligible performance overhead.	encoder;field-programmable gate array;jpeg;overhead (computing);reconfigurable computing	Antonino Tumeo;Matteo Monchiero;Gianluca Palermo;Fabrizio Ferrandi;Donatella Sciuto	2007	IEEE Computer Society Annual Symposium on VLSI (ISVLSI '07)	10.1109/ISVLSI.2007.25	embedded system;parallel computing;real-time computing;computer science	EDA	11.102284439603489	40.97422329518864	164046
0c96676859d865d445644aa2dfc8dee906f7d108	short length menger's theorem and reliable optical routing	tolerancia falta;condicion existencia;longueur chemin;multicommodity flow;optical network;maximum degree;randomized rounding;calcul tolerant les pannes;approximate algorithm;fault tolerant;condition necessaire suffisante;approximation algorithm;routing in optical networks;estimacion promedio;fault tolerant routing;path coloring;red fibra optica;routage reseau;network routing;flujo red;fault tolerant computing;coloration chemin;connexion;menger s theorem;multiflot;informatique theorique;conexion;necessary and sufficient condition;menger theorem;theoreme menger;fault tolerance;reseau fibre optique;raccordement;condition existence;algoritmo aproximacion;randomized algorithm;optical fiber network;mean estimation;network flow;algorithme approximation;capacity constraint;estimation moyenne;existence condition;communication;multipath routing;comunicacion;connection;tolerance faute;flot reseau;condicion necesaria suficiente;disjoint paths;computer theory;informatica teorica	In the minimun path coloring problem, we are given a graph and a set of pairs of vertices of the graph and we are asked to connect the pairs by colored paths in such a way that paths of the same color are edge disjoint. In this paper we deal with a generalization of this problem where we are asked to connect each pail by k edge disjoint paths of the same color. The objective is to minimize the number of colors. The reason for multiple paths between the same pair of vertices is to ensure fault tolerance of the connections. We propose an O(k2F) = O(k2Δα-1 log n) approximation algorithm for this problem where F is the flow number of the graph, Δ is the maximum degree and α is the expansion. This is an improvement even for the special case k = 1 where, to our knowledge, the best previously known bound is weaker by a factor of log n.The underlying problem is that of finding several disjoint paths between a given pair of vertices. Menger's theorem provides a necessary and sufficient condition for the existence of k such paths. However, it does not say anything about the length of the paths although in communication problems the number of links used is an issue. We show that any two k-connected vertices are connected by k edge disjoint paths of average length O(kF) which improves an earlier result of Galil and Yu (Proceedings of the 27th Annual ACM Symposium on Theory of Computing, 1995) for several classes of graphs. In fact, this is only a corollary of a stronger result for multicommodity flow on networks with unit edge capacities: any multicommodity flow with k units for each commodity can be rerouted such that the flow for each commodity is shipped through k-tuples of edge disjoint paths of average length O(kF) without exceeding the edge capacities significantly.	menger sponge;menger's theorem;routing	Amitabha Bagchi;Amitabh Chaudhary;Petr Kolman	2005	Theor. Comput. Sci.	10.1016/j.tcs.2005.03.009	fault tolerance;combinatorics;discrete mathematics;floyd–warshall algorithm;computer science;menger's theorem;mathematics;k-vertex-connected graph;approximation algorithm;algorithm	Theory	23.292998049612816	33.719828740632764	164415
72e9ed6a341aac4fdb1d661adee4ba50543777a8	a local algorithm for the sparse spanning graph problem		Constructing a sparse spanning subgraph is a fundamental primitive in graph theory. In this paper, we study this problem in the Centralized Local model, where the goal is to decide whether an edge is part of the spanning subgraph by examining only a small part of the input; yet, answers must be globally consistent and independent of prior queries. Unfortunately, maximally sparse spanning subgraphs, i.e., spanning trees, cannot be constructed efficiently in this model. Therefore, we settle for a spanning subgraph containing at most (1+ǫ)n edges (where n is the number of vertices and ǫ is a given approximation/sparsity parameter). We achieve query complexity of Õ(poly(∆/ε)n), where ∆ is the maximum degree of the input graph. Our algorithm is the first to do so on arbitrary graphs. Moreover, we achieve the additional property that our algorithm outputs a spanner, i.e., distances are approximately preserved. With high probability, for each deleted edge there is a path of O(poly(∆/ε) log n) hops in the output that connects its endpoints. MPI for informatics, Saarbrücken 66123, Germany. Email: clenzen@mpi-inf.mpg.de. MPI for informatics, Saarbrücken 66123, Germany. Email: rlevi@mpi-inf.mpg.de. Õ-notation hides polylogarithmic factors in n.	approximation;centralized computing;decision tree model;email;file spanning;graph theory;informatics;local algorithm;message passing interface;polylogarithmic function;spanning tree;sparse matrix	Christoph Lenzen;Reut Levi	2017	CoRR		mathematical optimization;combinatorics;discrete mathematics;minimum degree spanning tree;mathematics	Theory	19.490910785427825	33.794841542590426	164456
9d3e9096636c268a4820e085059331473e606875	gdft types mapping algorithms and structured regular fpga implementation	field programmable gate array;twiddle factors fft algorithms gdft types mapping algorithms structured regular fpga implementation nonmultiplicative mapping algorithms generalized discrete fourier transform hardware realization xilinx xc4000 vertix series;structured regular fpga implementation;digital arithmetic discrete fourier transforms field programmable gate arrays;flow graphs;hardware realization;gdft types mapping algorithms;fpga implementation;generalized discrete fourier transform;discrete transforms;xilinx xc4000;twiddle factors fft algorithms;discrete fourier transform;vertix series;field programmable gate arrays discrete fourier transforms sparse matrices arithmetic discrete transforms flow graphs cities and towns hardware costs algorithm design and analysis;arithmetic;nonmultiplicative mapping algorithms;cities and towns;digital arithmetic;field programmable gate arrays;discrete fourier transforms;sparse matrices;algorithm design and analysis;hardware	In this paper, regular and non-multiplicative mapping algorithms between different types of Generalized Discrete Fourier Transform (GDFT) are proposed. The proposed mapping algorithms are used to build regular and real twiddle factors FFT algorithms. It presents a more regular FFT than that recently presented GDFT type-I, which in addition requires log (N/2) stages of permutation [3]. Hardware realization of 16-point FFT, based on the proposed mapping algorithms, with real twiddle factors butterfly rather than complex twiddle factors in traditional FFT algorithms, is implemented in Xilinx XC4000 and Vertix series Field Programmable Gate Array (FPGA). Cost comparisons with alternative approaches are presented. Our proposed algorithms achieve a significant improvement in the FPGA-based designs.	algorithm;discrete fourier transform;fast fourier transform;field-programmable gate array;twiddle factor	H. I. Saleh;M. A. Ashour;Aly E. Salama	2003		10.1109/ISCAS.2003.1205790	embedded system;electronic engineering;discrete mathematics;computer science;theoretical computer science;field-programmable gate array	EDA	11.325126868999504	44.034404802225176	164487
0ee3c55cc06b5e73845042f53518425c9fad06b7	quantum circuit synthesis using classes of gf(3) reversible fast spectral transforms	multiplying circuits;reversible davio expansions;logic design;reversible butterfly circuit plane;very large scale integration;reversibility gf 3 reversible fast spectral transforms quantum circuit synthesis reversible davio expansions reversible butterfly circuit plane quantum gates plane addition circuits multiplication circuits power consumption reduction;circuit design;moore s law;quantum circuit synthesis;quantum gates;multiplication circuits;quantum gate;quantum gates plane;reversibility;energy consumption;quantum circuits;adders;low power electronics;low power electronics logic design quantum gates multiplying circuits adders galois fields;circuit testing;addition circuits;signal synthesis;power consumption;circuit synthesis quantum computing energy consumption switches very large scale integration signal synthesis galois fields circuit testing moore s law buildings;gf 3 reversible fast spectral transforms;quantum computing;switches;power consumption reduction;buildings;circuit synthesis;galois fields	Novel quantum circuit synthesis, using reversible Davio expansions, is introduced. The new method uses two planes to synthesize the quantum circuits: (1) a reversible butterfly circuit plane; and (2) a plane of quantum gates to perform additions and multiplications. Since the reduction of power consumption is a major requirement for circuit design of future technologies, such as in quantum circuits, the main features of several future technologies must include reversibility, and thus the new synthesis method, using reversible butterfly circuits, can play an important role in the synthesis of circuits that consume minimal power.	circuit design;integrated circuit;quantum circuit;quantum gate	Anas Al-Rabadi	2004	Proceedings. 34th International Symposium on Multiple-Valued Logic	10.1109/ISMVL.2004.1319925	electronic engineering;computer science;pure mathematics;quantum circuit;mathematics;algorithm;quantum mechanics;quantum gate	EDA	17.5984527998126	44.9881476841557	164581
033b514c9255532b724dce62544784a76daf770d	fast consensus for voting on general expander graphs		Distributed voting is a fundamental topic in distributed computing. In the standard model of pull voting, at each step every vertex chooses a neighbour uniformly at random and adopts its opinion. The voting is completed when all vertices hold the same opinion. In the simplest case, each vertex initially holds one of two different opinions. This partitions the vertices into arbitrary sets A and B. For many graphs, including regular graphs and irrespective of their expansion properties, if both A and B are sufficiently large sets, then pull voting requires Ω(n) expected steps, where n is the number of vertices of the graph. In this paper we consider a related class of voting processes based on sampling two opinions. In the simplest case, every vertex v chooses two random neighbours at each step. If both these neighbours have the same opinion, then v adopts this opinion. Otherwise, v keeps its own opinion. Let G be a connected graph with n vertices and m edges. Let P be the transition matrix of a simple random walk on G with second largest eigenvalue λ < 1/ √ 2. We show that if the initial imbalance in degree between the two opinions satisfies |d(A) − d(B)|/2m ≥ 2λ, then with high probability voting completes in O(logn) steps, and the opinion with the larger initial degree wins. The condition that λ < 1/ √ 2 includes many classes of expanders, for example random d-regular graphs where d ≥ 10. If however 1/ √ 2 ≤ λ(P ) ≤ 1 − for a constant > 0, or only a bound on the conductance of the graph is known, the sampling process can be modified so that voting still provably completes in O(logn) steps with high probability. The modification uses two sampling based on probing to a fixed depth O(1/ ) from any vertex. In its most general form our voting process allows vertices to bias their sampling of opinions among their neighbours to achieve a desired outcome. This is done by allocating weights to edges. ? This work was supported in part by EPSRC grant EP/M005038/1, “Randomized algorithms for computer networks”. N. Rivera was supported by funding from Becas CHILE.		Colin S Cooper;Robert Elsässer;Tomasz Radzik;Nicolas Rivera;Takeharu Shiraga	2015		10.1007/978-3-662-48653-5_17	combinatorics;discrete mathematics;machine learning;mathematics;distributed computing;neighbourhood;algorithm;statistics	Theory	19.44215139656286	34.76930298640481	164975
1127a46a9b286f6dc01e218c926ed5ee49be9081	vector bank based multimedia codec system-on-a-chip (soc) design	memory resources vector bank based multimedia codec system on chip design video encoder system h 264 encoder transmitting bandwidth motion estimation video codec gaussian operator processing load;moving object;laplacian of gaussian;processing load;data compression;system on a chip multimedia systems codecs motion estimation video compression object detection decoding algorithm design and analysis design engineering computer networks;sensors;h 264 encoder;edge detection;gaussian operator;motion estimation h 264 edge detection;transmitting bandwidth;reference frame;motion estimation;system on a chip;video codec;video encoder system;video coding;laplace equations;vector bank based multimedia codec;image edge detection;streaming media;system on chip;image color analysis;h 264;video codecs;vector data;memory resources;system on chip design;motion detection;video coding data compression motion estimation system on chip video codecs;object detection	In this paper, we present a design architecture of implementing a ”Vector Bank” into video encoder system, namely, an H.264 encoder, in order to detect and analyze the moving objects within the specific area. Also, we believe that the transmitting bandwidth could be saved with the implementation of Vector Bank design. Motion Estimation is a common technology for today’s video codec. By abstracting the vector data from the Motion Estimation block using the motion detection method and with the application of Laplacian of Gaussian operator, we could obtain the object motion data generated by up to 16 reference frames. Thus, it could save the bandwidth, processing load, and memory resources dramatically.	bandwidth (signal processing);blob detection;closed-circuit television;codec;data compression;encoder;extended memory;gaussian blur;h.264/mpeg-4 avc;make;motion estimation;out there;sensor;streaming media;system on a chip;transmitter	Ruei-Xi Chen;Wei Zhao;Jeffrey Fan;Asad Davari	2009	2009 10th International Symposium on Pervasive Systems, Algorithms, and Networks	10.1109/I-SPAN.2009.74	embedded system;real-time computing;computer hardware;quarter-pixel motion;computer science;motion estimation	EDA	11.650873822972505	39.87894791801589	165081
71744b38fe9aeddc2cc3502408ff485a654eaeac	average whenever you meet: opportunistic protocols for community detection		Consider the following asynchronous, opportunistic communication model over a graph G: in each round, one edge is activated uniformly and independently at random and (only) its two endpoints can exchange messages and perform local computations. Under this model, we study the following random process: The first time a vertex is an endpoint of an active edge, it chooses a random number, say ±1 with probability 1/2; then, in each round, the two endpoints of the currently active edge update their values to their average. We provide a rigorous analysis of the above process showing that, if G exhibits a twocommunity structure (for example, two expanders connected by a sparse cut), the values held by the nodes will collectively reflect the underlying community structure over a suitable phase of the above process. Our analysis requires new concentration bounds on the product of certain random matrices that are technically challenging and possibly of independent interest. We then exploit our analysis to design the first opportunistic protocols that approximately recover community structure using only logarithmic (or polylogarithmic, depending on the sparsity of the cut) work per node. 2012 ACM Subject Classification Theory of computation → Distributed algorithms	communication endpoint;distributed algorithm;polylogarithmic function;random access;random number generation;sparse matrix;stochastic process;theory of computation	Luca Becchetti;Andrea E. F. Clementi;Pasin Manurangsi;Emanuele Natale;Francesco Pasquale;Prasad Raghavendra;Luca Trevisan	2018		10.4230/LIPIcs.ESA.2018.7	discrete mathematics;random matrix;computation;vertex (geometry);models of communication;computer science;logarithm;stochastic process;exploit;asynchronous communication	Theory	19.023057706257084	34.57563098963368	165168
f79c8fd4caa0904503bb01d1657a33dc42ace781	hamilton-connectivity and cycle-embedding of the möbius cubes	embedding;hypercube;reseau communication;hamilton connectivity;sistema hamiltoniano;cube mobius;connexite hamilton;dilatacion;interconnection network;systeme hamiltonien;mobius cube;ciclo;hamiltonian system;dilatation;plongement;interconnection networks;conexidad;inmersion;connexite;hamiltonian connectivity;dilation;cycle;connectedness;red de comunicacion;communication network;red interconexion;reseau interconnexion;hipercubo	The recently introduced interconnection network, the Möbius cube, is an important variant of the hypercube. This network has several attractive properties compared with the hypercube. In this paper, we show that the n-dimensional Möbius cube Mn is Hamilton-connected when 3. Then, by using the Hamilton-connectivity of Mn, we also show that any cycle of length l (4 l 2n) can be embedded into Mn with dilation 1 (n 2). It is a fact that then-dimensional hypercube Qn does not possess these two properties.  2002 Elsevier Science B.V. All rights reserved.	dilation (morphology);embedded system;grid network;interconnection;olap cube;simulation	Jianxi Fan	2002	Inf. Process. Lett.	10.1016/S0020-0190(01)00256-3	hamiltonian system;combinatorics;topology;social connectedness;embedding;mathematics;geometry;dilation;telecommunications network;hypercube	AI	24.27721308520737	35.129072771149055	165170
fb45b0fd97dd7f28255460b2c6ae4c0ac0cb430b	mediated population protocols	resource limitation;liverpool;population size;repository;directed graph;protocol specification;transitive closure;university	We extend here the Population Protocol (PP) model of Angluin et al. [2004,2006] in order to model more powerful networks of resource-limited agents that are possibly mobile. The main feature of our extended model, called the Mediated Population Protocol (MPP) model, is to allow the edges of the interaction graph to have states that belong to a constant-size set. We then allow the protocol rules for pairwise interactions to modify the corresponding edge state. The descriptions of our protocols preserve both the uniformity and anonymity properties of PPs, that is, they do not depend on the size of the population and do not use unique identifiers. We focus on the computational power of the MPP model on complete interaction graphs and initially identical edges. We provide the following exact characterization of the class MPS of stably computable predicates: A predicate is in MPS iff it is symmetric and is in NSPACE(n).	circuit complexity;communication endpoint;commutation theorem;computable function;computation;distributed computing;fault tolerance;file spanning;goodyear mpp;interaction;non-deterministic turing machine;population protocol;precondition;real life;scheduling (computing);simulation;state space;stavros fasoulas;time complexity;tree structure;unique identifier	Ioannis Chatzigiannakis;Othon Michail;Paul G. Spirakis	2009		10.1007/978-3-642-02930-1_30	mathematical optimization;combinatorics;discrete mathematics;population size;directed graph;computer science;artificial intelligence;mathematics;transitive closure;algorithm	Theory	16.73664751969575	35.09694724916116	165325
8b1662aa6cff73b9741eaf400e6fd334c81921c2	a new systolic array algorithm for a high throughput low cost vlsi implementation of dct	read only storage;convolutional codes;local interconnection topology;systolic arrays;systolic array;discrete cosine transform;network topology;arrays;systolic arrays throughput costs very large scale integration discrete cosine transforms signal processing algorithms hardware convolution convolutional codes algorithm design and analysis;local interconnection topology systolic array algorithm vlsi discrete cosine transform multiport rom;discrete cosine transforms;systolic array algorithm;vlsi;multiport rom;vlsi discrete cosine transforms network topology read only storage systolic arrays;high throughput;signal processing algorithms;algorithm design and analysis;throughput;hardware	This paper presents a new design approach for the VLSI implementation of a prime-length discrete cosine transform DCT based on a new hardware algorithm for DCT that can be implemented using a multi-port ROM-based systolic array. The proposed algorithm is based on the idea of reformulating prime-length DCT into several cycle convolutions having the same length and similar structures. Using the proposed approach we can efficiently exploit the inherent parallelism thus doubling the throughput without to double the hardware and I/O cost but only slightly increasing them. Moreover, the proposed VLSI implementation preserves all the other advantages of the VLSI algorithms based on circular correlations or cycle convolutions such as modular and regular structures with local interconnection topology..	algorithm;circular convolution;control flow;discrete cosine transform;input/output;interconnection;parallel computing;period-doubling bifurcation;read-only memory;ringing artifacts;systolic array;throughput;very-large-scale integration	Doru-Florin Chiper;M. N. Shanmukha Swamy;M. Omair Ahmad	2008	2008 15th IEEE International Conference on Electronics, Circuits and Systems	10.1109/ICECS.2008.4674897	electronic engineering;parallel computing;computer science;theoretical computer science	EDA	12.938007054382368	43.48886204702704	165513
30c274d3ff41e7a96e85b1171ea7ebeea459b48d	stability measures of some static interconnection networks	stability measure;graph theory;reseau communication;mesure stabilite;teoria grafo;long period;network stability;star graph;integrite;integridad;theorie graphe;graphs;interconnection network;stability;stabilite reseau;integrity;community networks;interconnection networks;grafo estrella;conexidad;graphe etoile;c 2 0;connexite;stabilite;g2 2;connectedness;human error;red de comunicacion;communication network;red interconexion;estabilidad;communication service;reseau interconnexion	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;interconnection;primary source	Pinar Dündar	2001	Int. J. Comput. Math.	10.1080/00207160108805039	human error;stability;telecommunications;star;social connectedness;computer science;graph theory;mathematics;distributed computing;graph;algorithm;telecommunications network	Robotics	23.16931524873722	33.35091105451074	165636
2d057ce18db0569572938585dca2f3c6589ebd29	a new algorithm-based fault tolerance technique for computing matrix operations	encoding decoding;fault tolerant;algorithm based fault tolerance;systolic arrays;matrix algebra;fault tolerant computing;matrix operations;fault tolerance matrix decomposition decoding signal processing algorithms hardware encoding error correction digital signal processing roundoff errors logic arrays;linear processor arrays;rectangular processor arrays;systolic arrays fault tolerant computing matrix algebra;pe faults;pe faults linear processor arrays algorithm based fault tolerance matrix operations rectangular processor arrays	The paper proposes a new algorithm-based fault tolerance (ABFT) technique for computing matrix operations. The scheme provides fault tolerant capability to linear or rectangular processor arrays so that all single PE faults can be tolerated. It also shows that the effect of implementation problems (overflow, round-off errors, and hardware overhead which includes encoding/decoding logic) on the proposed scheme is significantly less than that on other existing schemes. >	algorithm;fault tolerance	D. L. Tao;Eugene Foo;Carlos R. P. Hartmann	1991		10.1109/SPDP.1991.218206	fault tolerance;parallel computing;real-time computing;matrix multiplication;computer science;theoretical computer science	HPC	13.837025751539615	44.87583345382292	165722
c599d668e965baf23bc1a04b27ae92d85873499b	average probe complexity in quorum systems	quorum system;efficiency;failure detectors;accuracy;randomized algorithm;upper and lower bounds;scalability;distributed systems	This paper discusses the probe complexity of randomized algorithms and the deterministic average case probe complexity for some classes of non-dominated coteries, including majority, crumbling walls, tree, wheel and hierarchical quorum systems, and presents upper and lower bounds for the probe complexity of quorum systems in these classes.	best, worst and average case;randomized algorithm	Yehuda Hassin;David Peleg	2001		10.1145/383962.384014	mathematical optimization;scalability;computer science;theoretical computer science;mathematics;distributed computing;accuracy and precision;efficiency;upper and lower bounds;randomized algorithm;algorithm	Theory	15.609512319473655	34.96075236049622	165976
01dc05e28fdc2b247d3297d41b0f95a56b72311a	guest editorial: special issue on soc for multimedia networking (sips 2007)	multimedia networking	This special issue focuses on “SoC for Multimedia Networking,” the theme of the 2007 IEEE Workshop on Signal Processing Systems (SiPS 2007) held in Shanghai, China in October 2007. This issue consists of highest quality papers selected from SiPS 2007 presentations with three major emphases centered on multimedia, networking, and architecture/SoC. For SiPS 2007, we were very pleased to receive a record number of 299 paper submissions from 36 countries. After a rigorous review and selection process, the final technical program consisted of 130 papers (43.48% acceptance rate) representing 25 different countries. These papers were arranged into nine lecture and 10 poster sessions (one special session), and the threeday technical program also included a student paper contest. The highlights of the workshop were the three keynote presentations by Magdy Bayoumi, Liang-Gee Chen, and Lajos Hanzo, each of them addressing state-ofthe-art and future direction in these emerging areas. To select high quality papers for publication in this special issue, authors of the papers that received high review scores were invited to submit expanded version of their papers to include more in-depth research work. Each of the papers was again carefully reviewed by at least three reviewers and the final decision for each was reached after careful discussion among the four guest editors. Finally, only seven papers passed through this rigorous process and were eventually accepted. They address some contemporary design and implementation issues in “SoC for Multimedia Networking” applications. In multimedia communication, it is always challenging to achieve high received video quality under coding and communication constraints. Guo et al. presents a fast multihypothesis motion compensated filter for video denoising. The authors utilize a number of hypotheses (temporal predictions) to estimate the current pixel which is corrupted with noise. Song et al. proposes an adaptive pixel interpolation technique for spatial error concealment in block-based coding system, where a missing pixel in a corrupted block can be derived from four neighborhoods of the block through interpolation using a multiple prediction strategy. The design works effectively in consecutive block loss situation which is common in real-time video. Pantoja et al. addresses the use of super-resolution algorithm based on irregular sampling for video transcoding with resolution conversion. This is because in transcoding, quantization and other techniques could result in lower video output quality. The proposed method was applied to VC-1 to H.264 transcoding to show video quality improvement. The paper also includes a hardware feasibility study. In system-on-chip designs, recent research have migrated from fairly simple single processor and memory designs to relatively complicated systems with multiple processors, onchip memories, standard peripherals, and other functional blocks. Communication between these blocks becomes the dominant critical system path and performance bottleneck of system-on-chip designs. Network-on-chip architectures emerged as solutions for future system-on-chip communication architecture designs. Wang et al. presents a novel network-onchip architecture, pipelining multi-channel central caching, to address the cost and communication latency/throughput of existing architectures. By embedding a central cache into every switch of the network, blocked head packets can be removed from the input buffers and stored in the caches temporally, thus alleviating the effect of head-of-line and X. Yang :W. Zhang Shanghai Jiao Tong University, Shanghai, China		Xiaokang Yang;Nam Ling;Wenjun Zhang;Chang Wen Chen	2010	Signal Processing Systems	10.1007/s11265-009-0420-x	computer science;multimedia	HPC	14.617736806757454	38.75952003095284	166334
94b8bd26e05eb077f88e7c587065fd75e401ca5a	sign detection in residue arithmetic units	sign detection;eficacia sistema;circuito aritmetico;architecture systeme;signe;integrated circuit;complexite calcul;residue number systems;performance systeme;signo;circuito integrado;residue arithmetic units;system performance;algorithme;algorithm;base extension;hardware algorithms;complejidad computacion;computational complexity;calcul numerique;numerical computation;calculo numerico;arquitectura sistema;residue number system;point of view;system architecture;circuit arithmetique;circuit integre;arithmetic circuit;algoritmo	The parallelism of computation, that characterizes some operations in residue number systems (RNS), is heavily reduced in operations as division, magnitude and sign detection, since numbers must be converted to the weighted system thus reducing efficiency, in spite of the efforts to speed up the conversion. In this work the problem of detecting the sign of numbers represented in RNS is considered and a procedure is devised, which keeps numbers in residue notation, and requires a redundant modulus  m   p +1 ⩾2. A sign detecting circuit is also designed that, merely to speed up the operation, exploits a further redundant modulus  m  r ⩾ p  in the signed number representation. Circuit response time is evaluated, both from the complexity point of view and in a finite case, where 50 gate delays are estimated for a range   [−2 64 ,2 64 −1]   .	residue number system	Giuseppe Alia;Enrico Martinelli	1998	Journal of Systems Architecture	10.1016/S1383-7621(97)00085-4	residue number system;computer science;integrated circuit;sign;computer performance;computational complexity theory;algorithm;systems architecture	Arch	15.989740221169548	41.632615496112365	166521
1d3f41e69878272669bbc2d2900314fb97f3b930	pairing functions, boolean evaluation and binary decision diagrams	boolean function;logic programs;unt;binary decision diagram	A “pairing function” J associates a unique natural number z to any two natural numbers x,y such that for two “unpairing functions” K and L, the equalities K(J(x,y))=x, L(J(x,y))=y and J(K(z),L(z))=z hold. Using pairing functions on natural number representations of truth tables, we derive an encoding for Binary Decision Diagrams with the unique property that its boolean evaluation faithfully mimics its structural conversion to a a natural number through recursive application of a matching pairing function. We then use this result to derive ranking and unranking functions for BDDs and reduced BDDs. The paper is organized as a self-contained literate Prolog program, available at http://logic.csci.unt.edu/tarau/research/2008/pBDD.zip .	binary decision diagram;combinational logic;constraint logic programming;genetic programming;logic gate;prolog;recursion	Paul Tarau	2008	CoRR		boolean algebra;boolean circuit;and-inverter graph;combinatorics;circuit minimization for boolean functions;discrete mathematics;reed–muller expansion;boolean domain;boolean expression;standard boolean model;theoretical computer science;mathematics;boolean function;binary decision diagram;two-element boolean algebra	Logic	18.803092472050682	45.46892197736754	166558
f893ab4aab7a9c1b733b545e0d7c856f79ae3e03	computing a maximal independent set using beeps	cluster computing;communication model;upper bound;collision detection;maximal independent set;data structure;lower bound	We consider the problem of finding a maximal independent set (MIS) in the discrete beeping model introduced in DISC 2010. At each time, a node in the network can either beep (i.e., emit a signal) or be silent. Silent nodes can only differentiate between no neighbor beeping, or at least one neighbor beeping. This basic communication model relies only on carrier-sensing. Furthermore, we assume nothing about the underlying communication graph and allow nodes to wake up (and crash) arbitrarily. We show that if a polynomial upper bound on the size of the network n is known, then with high probability every node becomes stable inO(log n) time after it is woken up. To contrast this, we establish a polynomial lower bound when no a priori upper bound on the network size is known. This holds even in the much stronger model of local message broadcast with collision detection. Finally, if we assume nodes have access to synchronized clocks or we consider a somewhat restricted wake up, we can solve the MIS problem in O(log n) time without requiring an upper bound on the size of the network, thereby achieving the same bit complexity as Luby’s MIS algorithm. ∗acornejo@mit.edu, Massachusetts Institute of Technology (MIT) †haeupler@mit.edu, Massachusetts Institute of Technology (MIT) ‡fabian.kuhn@usi.ch, University of Lugano (USI), Switzerland 1 ar X iv :1 10 8. 19 26 v1 [ cs .D C ] 9 A ug 2 01 1	algorithm;beep;collision detection;context of computational complexity;crash (computing);independent set (graph theory);maximal independent set;maximal set;michael luby;polynomial;switzerland;with high probability	Alejandro Cornejo;Bernhard Haeupler;Fabian Kuhn	2011	CoRR		data structure;computer science;distributed computing;upper and lower bounds;algorithm	Theory	17.836416479339892	33.35283262197993	166584
8cd707bc9ecb626b0a1ab2a45ccd76c6b01ee2ef	fourier transforms in vlsi	shuffle exchange network;parallel algorithm;fourier transform;complexite calcul;circuit vlsi;fft;computing complexity;algorithme parallele;algorithms implemented in hardware;vlsi circuit;fourier transformation;computational complexity;area time complexity;system design;transformation fourier;vlsi;vlsi algorithms implemented in hardware area time complexity computational complexity fft fourier transform mesh connected computers parallel algorithms shuffle exchange network;mesh connected computers;conception systeme;parallel algorithms	This paper surveys nine designs for VLSI circuits that compute N-element Fourier transforms. The largest of the designs requires O(N2 log N) units of silicon area; it can start a new Fourier transform every O(log N) time units. The smallest designs have about 1/Nth of this throughput, but they require only 1/Nth as much area.	very-large-scale integration	Clark D. Thomborson	1983	IEEE Trans. Computers	10.1109/TC.1983.1676155	fourier transform;parallel computing;computer science;theoretical computer science;parallel algorithm;algorithm	Vision	12.436016355512804	35.38724650705426	166655
3b338f7968960d2b455ce0d65c17fac6f37d8bf0	dsp-based down-sampling process using lanczos filter bank	digital signal processing;ieee transactions;random access memory;interpolation;image resolution;dsptms320c6472 re sampling down sampling poly phase filter;multicore processing;multicore processing digital signal processing filter banks interpolation random access memory image resolution ieee transactions;filter banks	Re-sampling process is the technique used to create a new version of the image with different width and/or height in terms of pixels count. It may be either increasing the size of image so called upsampling or reducing its size so called down-sampling. In this paper we interest on down-sampling process. Hence, a DSP TMS320C6472 implementation of the down-sampling module is performed. The down-sampling method is based on poly-phase filter bank. A software design for this poly-phase filter interpolation on two levels is proposed in order to satisfy a significant time saving. Results show the efficiency of the down sampler filter especially for removing the aliasing artifacts. A good speed performance is performed for the two implementation design. For execution time results, we might attaint a gain of 33 % for multi-core design execution compared with the one core design.	aliasing;digital signal processor;filter bank;interpolation;lanczos resampling;multi-core processor;pixel;polyphase matrix;run time (program lifecycle phase);sampling (signal processing);software design;upsampling	Amina Kessentini;Nejmeddine Bahri;Nouri Masmoudi;Amine Samet;Mohamed Ali Ben Ayed	2014	2014 1st International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)	10.1109/ATSIP.2014.6834595	filter;adaptive filter;electronic engineering;real-time computing;kernel adaptive filter;computer hardware;anti-aliasing filter;computer science;root-raised-cosine filter;reconstruction filter;filter;filter bank;filter design;half-band filter	EDA	12.901748419718102	41.744789274730124	166715
b6b7914d506c153f9de9bab1e2c5515f53a21eb2	optimal adaptive diagnosis with spares	microprocessors;spare processors fault diagnosis multiprocessor systems optimal adaptive diagnosis algorithm;multiprocessing;multiprocessor systems;optimal adaptive diagnosis algorithm;computer fault tolerance;fault tolerant computing;multiprocessing systems fault diagnosis fault tolerant computing microprocessor chips;multiprocessing systems;fault diagnosis system testing electronic equipment testing multiprocessing systems sequential analysis mathematics informatics heuristic algorithms fault tolerant systems upper bound;complete graph;spare processors;fault diagnosis;microprocessor chips	This paper considers fault diagnosis of multiprocessor systems. A diagnosis algorithm is said to be adaptive if the algorithm determines tests dynamically depending on previous test results. In this paper, we present an optimal adaptive diagnosis algorithm for a multiprocessor system modeled by G using spare processors if G is a complete graph, path, or cycle	central processing unit;medical algorithm;multiprocessing	Takeshi Yamada;A. Koh	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1693624	computer architecture;parallel computing;real-time computing;multiprocessing;computer science;mathematics;complete graph	Arch	23.646567337585743	44.24099990932731	166840
3f5d057044846b438ac9aa88aa2eca420fd0bb7b	non-uniform wordlength delay lines for fir filters	ffc algorithm;algorithm;delays abstracts adders accuracy indexes;accuracy;indexes;fir design non uniform wordlength delay lines fir filters floating point arithmetic asic fixed point arithmetic quantization effects multipliers adders delay register wordlengths delay signal wordlengths multiplier wordlengths;adders;abstracts;dsp system;fir filters;floating point to fixed point conversion;ffc;multiplying circuits adders application specific integrated circuits delay lines fir filters fixed point arithmetic floating point arithmetic logic design;delays;electrical engineering electronics nuclear engineering	When FIR filters are designed floating point arithmetic is generally used. However when implemented on hardware such as ASICs, fixed point arithmetic must be used to minimise cost and power requirements. Research to minimise hardware costs has mainly focused on the quantization effects of fixed point wordlengths for the coefficients, multipliers and adders of FIR filters, but with the actual data delays assigned a uniform wordlength and essentially not optimised. This paper proposes that the wordlengths of the delay line can be non-uniform with a minimal increase in quantization noise for parallel implementation of FIR filters where there are differences in the magnitudes of the coefficients. A non-uniform delay line allows hardware savings in terms of delay register wordlengths, delay signal wordlengths and multiplier wordlengths. Results for an FIR design are presented which demonstrate the hardware savings when using a non-uniform wordlength delay line.	algorithm;analog delay line;application-specific integrated circuit;coefficient;delay line memory;finite impulse response;fixed point (mathematics);fixed-point arithmetic;low-pass filter;numerical analysis;quantization (signal processing);requirement	Gregour Bolton;Robert W. Stewart	2009	2009 17th European Signal Processing Conference		arithmetic;electronic engineering;real-time computing;mathematics	EDA	13.178668781324957	44.55216568075407	166889
6c2075b76b2b17eceab8ae423cd6b3e4f42e91a2	biased selection for building small-world networks	informatica;t technology general;telecomunicaciones;q science general;qa75 electronic computers computer science;relative error;probability distribution;long range;small world network;distributed algorithm;tk electrical engineering electronics nuclear engineering	Small-world networks are currently present in many distributed applications and can be built augmenting a base network with long-range links using a probability distribution. Currently available distributed algorithms to select these long-range neighbors are designed ad hoc for speciflc probability distributions. In this paper we propose a new algorithm called Biased Selection (BS) that, using a uniform sampling service (that could be implemented with, for instance, a gossip-based protocol), allows to select long-range neighbors with any arbitrary distribution in a distributed way. This algorithm is of iterative nature and has a parameter r that gives its number of iterations. We prove that the obtained sampling distribution converges to the desired distribution as r grows. Additionally, we obtain analytical bounds on the máximum relative error for a given valué of this parameter r. Although the BS algorithm is proposed in this paper as a tool to sample nodes in a network, it can be used in any context in which sampling with an arbitrary distribution is required, and only uniform sampling is available. The BS algorithm has been used to choose long-range neighbors in complete and incomplete tori, in order to build Kleinberg's small-world networks. We observe that using a very small number of iterations (1) BS has similar error as a simulation of the Kleinberg's harmonic distribution and (2) the average number of hops with greedy routing is no larger with BS than in a Kleinberg network. Furthermore, we have observed that before converging to the performance of a Kleinberg network, the average number of hops with BS is signiflcantly smaller (up to 14% smaller in a 1000 x 1000 network).	approximation error;converge;distributed algorithm;distributed computing;greedy algorithm;hoc (programming language);iteration;local algorithm;markov chain;routing;sampling (signal processing);simulation	Andrés Sevilla;Alberto Mozo;M. Araceli Lorenzo;José Luis López-Presa;Pilar Manzano-Hernandez;Antonio Fernández	2010		10.1007/978-3-642-17653-1_3	probability distribution;distributed algorithm;approximation error;computer science;theoretical computer science;machine learning;distributed computing;small-world network	Metrics	19.26074151470716	35.54194040092059	167131
526c6759d9c0273eb559443c9c8e369dbb131646	sierpinski gaskets for logic functions representation	minimisation;logic function representation;minimization;decision diagrams;fractals;logic design;boolean functions;logic expression;decision diagram;discrete mathematics;gaskets logic functions fractals circuit synthesis binary decision diagrams computer science graphics logic design digital circuits boolean functions;graphic representations;sierpinski gaskets;minimisation formal logic logic design functions fractals;recursiveness;binary decision diagrams;gaskets;graphical representation;function representation;sierpinski gasket;logic functions;logic function minimization;formal logic;fractal;logic in computer science;computer science;digital circuits;multiple valued;boolean logic;esop;functions;logic function;multiple valued logic sierpinski gaskets logic function representation logic expression recursiveness fractals graphic representations decision diagrams logic design logic function minimization boolean logic;graphics;circuit synthesis;multiple valued logic	This paper introduces a new approach to represent logic functions in the form of Sierpinski Gaskets . The structure of the gasket allows to manipulate with the corresponding logic expression using recursive essence of fractals. Thus, the Sierpinski gasket’s pattern has myriad useful properties which can enhance practical features of other graphic representations like decision diagrams. We have covered possible applications of Sierpinski gaskets in logic desig n and justified our assumptions in logic function minimization (both Boolean and multiple-valued cases). The experimental results on benchmarks with advances in the novel structure are considered as well.	benchmark (computing);boolean algebra;diagram;fractal;recursion;sierpinski triangle	Denis V. Popel;Anita Dani	2002		10.1109/ISMVL.2002.1011068	combinatorics;discrete mathematics;fractal;computer science;mathematics;algorithm	EDA	18.764776802327443	45.7761408593276	167258
97b99fa6e2b54ecb71fc490db82b278a7d8be2e1	design of optimized radix-2 and radix-4 butterflies from fft with decimation in time	decimation 16 bit width radix 2 dit butterflies 16 bit width radix 4 dit butterflies fft computation power consumption reduction arithmetic operator number minimization;microprocessors;computer architecture adders optimization power demand delays microprocessors;computer architecture;adders;optimization;power consumtpion reduction fft radix 2 butterfly radix 4 butterfly;power demand;delays;logic design digital arithmetic fast fourier transforms	In the FFT computation, the butterflies play a central role, since they allow calculation of complex terms. In this calculation, involving multiplications of input data with appropriate coefficients, the optimization of the butterfly can contribute for the reduction of power consumption of FFT architectures. In this paper different and dedicated structures for the 16 bit-width radix-2 and radix-4 DIT butterflies are implemented, where the main goal is to minimize the number of arithmetic operators in order to produce power-efficient structures. Firstly, we improve a radix-2 butterfly previously presented in literature, reducing one adder and one subtractor in the structure. After that, part of this optimized radix-2 butterfly is used to reduce the number of real multipliers in the radix-4 butterfly. The main results show that the optimization guarantees reduced power consumption for radix-2 butterfly, when compared with previous works from the literature. Moreover, the use of part of the optimized radix-2 into the radix-4 structure leads to the reduction of power consumption for this structure.	16-bit;adder (electronics);bbn butterfly;butterfly diagram;coefficient;computation;decimation (signal processing);directory information tree;fast fourier transform;mathematical optimization;subtractor	Renato Neuenfeld;Mateus Fonseca;Eduardo A. C. da Costa	2016	2016 IEEE 7th Latin American Symposium on Circuits & Systems (LASCAS)	10.1109/LASCAS.2016.7451037	arithmetic;electronic engineering;theoretical computer science;mathematics	Embedded	12.893939927912049	44.97682941401938	167267
1099d69258a455227552c3f4221eb314b0dd6e1f	smaller two-qubit circuits for quantum communication and computation	smaller two-qubit circuits;cnot gate;quantum gate library;quantum circuit identity;small circuit;basic gate;arbitrary two-qubit unitary operation;elementary gate;quantum computation;generic circuit;quantum communication;quantum algorithm;quantum gates;routing;communication protocol;bandwidth;logic synthesis;upper bound;lower bound;quantum computer;bound states;cores;quantum gate	We show how to implement an arbitrary two-qubit unitary operation using any of several quantum gate libraries with small a priori upper bounds on gate counts. In analogy to library-less logic synthesis, we consider circuits and gates in terms of the underlying model of quantum computation, and do not assume any particular technology. As increasing the number of qubits can be prohibitively expensive, we assume throughout that no extra qubits are available for temporary storage.Using quantum circuit identities, we improve an earlier lower bound of 17 elementary gates by Bullock and Markov to 18, and their upper bound of 23 elementary gates to 18. We also improve upon the generic circuit with six CNOT gates by Zhang et al. (our circuit uses three), and that by Vidal and Dawson with 11 basic gates (we use 10).We study the performance of our synthesis procedures on two-qubit operators that are useful in quantum algorithms and communication protocols. With additional work, we find small circuits and improve upon previously known circuits in some cases.	computation;controlled not gate;library (computing);logic synthesis;markov chain;quantum algorithm;quantum channel;quantum circuit;quantum computing;quantum gate;quantum information science;qubit	Vivek V. Shende;Igor L. Markov;Stephen S. Bullock	2003	Proceedings Design, Automation and Test in Europe Conference and Exhibition		electronic engineering;nor logic;three-input universal logic gate;computer science;theoretical computer science;quantum network;controlled not gate;quantum circuit;upper and lower bounds;quantum algorithm;tc0;quantum gate;quantum error correction	EDA	20.316083082739045	43.91614964791082	167517
ac993bc90ee89764464db2ba537080b06d0fb1b1	energy-efficient motion estimation with approximate arithmetic		Energy efficiency has become a primary concern in the design of multimedia digital systems, particularly when targeting mobile devices. Approximate computing is a highly promising approach to address this challenge. This paper presents an architectural exploration in a variable block size motion estimation (VBSME) architecture using imprecise Lower-Part-OR Adders (LOA). These adders were applied to Sum of Absolute Differences units (SAD) in order to reduce the energy consumption while introducing a minimum impact on the coding efficiency. Three VBSME architectures with LOA operators were developed by considering different imprecision levels. The conducted evaluations, performed using the High-Efficiency Video Coding standard (HEVC) reference software, showed that this technique introduces a negligible impact on the coding efficiency (between 0.6% and 2.5% increase of the BD-Rate). Nevertheless, when the designed architectures were synthesized for a 45nm standard cells technology, significant power savings were observed (between 7% and 11.5%, depending on the used LOA version), demonstrating the viability and significant gains of the proposed approach.	algorithmic efficiency;approximate computing;approximation algorithm;block size (cryptography);blu-ray;digital electronics;high efficiency video coding;library oriented architecture;mobile device;motion estimation;sum of absolute differences	Roger Endrigo Carvalho Porto;Luciano Volcan Agostini;Bruno Zatt;Marcelo Schiavon Porto;Nuno Roma;Leonel Sousa	2017	2017 IEEE 19th International Workshop on Multimedia Signal Processing (MMSP)	10.1109/MMSP.2017.8122248	computer science;architecture;artificial intelligence;operator (computer programming);real-time computing;pattern recognition;algorithmic efficiency;block size;energy consumption;motion estimation;adder;sum of absolute differences	EDA	13.1434652510057	41.40868999730686	167519
3af9e14ec732bd937dbb8a9380060658d2e02884	base-layer motion estimation with limited enhancement-layer search window for hardware h.264/svc encoder	system on a chip memory management static var compensators bandwidth encoding hardware video coding;memory management;motion estimation scalable video codec svc inter layer prediction;motion estimation;system on a chip;video coding;inter layer prediction;scalable video codec svc;bandwidth;static var compensators;encoding;hardware	In the hardware design of ME, memory bandwidth and on-chip memory size are major constraints to be considered, especially for mobile video encoder, so as to reduce power consumption and area. In a spatial scalable video encoder, with limited memory bandwidth and on-chip memory size to fetch and store reference frames, the search window for enhancement layer (EL) becomes relatively smaller than that of base layer (BL). However, the smaller search window in EL causes quality loss because scaled base-layer MVs may not be inside it and therefore inter-layer prediction cannot be fully exploited. To address this issue in the hardware H.264/SVC encoder, the base-layer ME with a new MV cost is proposed in this work. The proposed method can improve the coding efficiency of EL up to around 17 % at the cost of marginal BL quality and trade off the qualities of BL and EL.	algorithmic efficiency;bl (logic);encoder;marginal model;memory bandwidth;motion estimation;online and offline;scalability	Do-Kyoung Kwon;Hyung J. Kim	2012	2012 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2012.6271547	system on a chip;embedded system;computer vision;real-time computing;computer hardware;computer science;operating system;motion estimation;bandwidth;encoding;memory management	EDA	12.65315864939667	40.12778474622417	168244
7f3a7e9e8151e7d7ead76ec9b5f06c2d437b7132	fast and feasible periodic sorting networks of constant depth	mathematics;concurrent computing;compare exchange operations;layout area;comparator network;periodic sorting networks;d dimensional mesh;sorting;very large scale integration;sorting computer science runtime concurrent computing registers web sites mathematics communication switching very large scale integration circuits;runtime;general methods;registers;computational complexity;sorting network;batcher s networks;web sites;aks network;parallel algorithms sorting computational complexity;circuits;computer science;communication switching;d dimensional mesh periodic sorting networks constant depth comparator network compare exchange operations layout area aks network batcher s networks;constant depth;parallel algorithms	A periodic comparator network has depth (or period) k, if for every t > k, the compare-exchange operations performed at step t are executed between exactly the same registers as at step t k. We introduce a general method that converts an arbitrary comparator network that sorts n items in time T(n) and that has layout area A into a periodic sorting network of depth 5 that sorts O(n .T(n)) items in time O(T(n) . logn) and has layout area O ( A 1 T(n)). This scheme applied to the AKS network yields a depth 5 periodic comparator network that sorts in time 0 log2 n . More practical networks with runtime O(1og n) can be obtained from Batcher’s networks. Developing the techniques for the main result, we improve some previous results: Let us fix a d E N. Then we can construct a network of depth 3 based on a d-dimensional mesh sorting n items in time ~ ( n ’ l d . logo(d) n). i )	binary logarithm;comparator;sorting network	Miroslaw Kutylowski;Krzysztof Lorys;Brigitte Oesterdiekhoff;Rolf Wanka	1994		10.1109/SFCS.1994.365679	electronic circuit;concurrent computing;sorting network;computer science;sorting;theoretical computer science;distributed computing;parallel algorithm;very-large-scale integration;processor register;computational complexity theory;algorithm	Theory	14.110624504467367	33.36629701845605	168367
25720ad1953207d3f59d3efb8f832af985b5db8a	improvement of the optimal hamilton circuit for undirected complete graph	minimum path length variation;graph theory;nearest neighbor searches;optimisation;gradually convergent algorithm;convergence;complexity theory;algorithm design and analysis nearest neighbor searches accuracy approximation algorithms circuit stability stability analysis complexity theory;approximation algorithms;optimisation convergence graph theory;circuit stability;optimal hamilton circuit;cross wire cutting algorithm;accuracy;nearest neighbor algorithm;undirected complete graph;nearest neighbor;stability analysis;cross wire cutting algorithm undirected complete graph optimal hamilton circuit nearest neighbor algorithm gradually convergent algorithm;complete graph;minimum path length variation optimal hamilton circuit undirected complete graph nearest neighbor algorithm gradually convergent algorithm cross wire cutting algorithm;algorithm design and analysis	Based on the Nearest Neighbor Algorithm, this paper introduces the Algorithm of Gradually Convergent and the Cross-wire Cutting, which according to gradually convergence of the path-changing and the principle of minimum path length variation respectively. Both of them optimize the algorithm of Optimal Hamilton Circuit in undirected complete graphs, comparing three aspects of accuracy, stability and running speed. Finally, we can know the preponderance and inferior of these algorithms through synthesizing.	graph (discrete mathematics);hamiltonian path;instruction path length;k-nearest neighbors algorithm	Hua Liu;Huilai Zou;Lili Zhong;Chaonan Wang;Youtian Qu	2010	2010 10th IEEE International Conference on Computer and Information Technology	10.1109/CIT.2010.363	mathematical optimization;computer science;graph theory;machine learning;k-nearest neighbors algorithm	Vision	22.31164400172056	40.00698857706564	168387
fcdd73212163c4bb179358b6cb856d99542ee873	quaternary reed-muller expansions of mixed radix arguments in cryptographic circuits	signal encoding cryptographic circuit logic synthesis optimisation technique quaternary fixed polarity reed muller expansion galois field arithmetic mixed radix argument binary arithmetic mathematical representation gate level model;optimisation;galois field arithmetic;binary arithmetic;fixed polarity reed muller;probability density function;reed muller codes binary codes cryptography digital arithmetic galois fields logic circuits optimisation;binary codes;mathematical representation;wires;logic circuits;ashur rafiev;cryptographic circuit;mixed radix;data mining;signal encoding;eprints newcastle university;quaternary fixed polarity reed muller expansion;logic synthesis;logic gates;energy consumption;reed muller expansions mixed radix;cryptography;displays;open access;mixed radix argument;gate level model;mathematical model;arithmetic;professor alex yakovlev;digital arithmetic;signal synthesis;cryptography galois fields arithmetic security displays energy consumption mathematical model circuit synthesis signal synthesis encoding;power consumption;optimisation technique;dr julian murphy;switches;reed muller expansions;reed muller;security;galois field;encoding;high efficiency;reed muller codes;circuit synthesis;direct method;galois fields	Circuits built using multi-valued fixed polarity Reed-Muller expansions based on Galois field arithmetic, in particular quaternary expansions over GF(4), normally display high efficiency in terms of power consumption, area, etc. However, security application specific gate level mapping shows inefficient results for uniform radix expansions. The idea of the research here is to consolidate binary and quaternary Galois field arithmetic within a single circuit in such a way that the mathematical representations can benefit down to the gate level model. A direct method to compute quaternary fixed polarity Reed-Muller expansions of mixed radix arguments is proposed and implemented in a synthesis tool. The results for the various types of power-balanced signal encoding catered for the security application are compared and analysed.	algorithm;authorization;benchmark (computing);bitwise operation;computation;cryptography;direct method in the calculus of variations;display resolution;ieee xplore;integrated circuit;mathematical optimization;radix point;radix sort;reed–muller code;software propagation	Ashur Rafiev;Julian P. Murphy;Alexandre Yakovlev	2009	2009 39th International Symposium on Multiple-Valued Logic	10.1109/ISMVL.2009.21	arithmetic;logic gate;computer science;information security;theoretical computer science;mathematics;algorithm;statistics;algebra	EDA	18.186988019646044	44.83853980831198	168597
ecf86316a2ac8c47d0847d98b8dbdef5054a4af3	matrix operations using arrays with reconfigurable optical buses*	time complexity;parallel computer	This paper examines the possibility of implementing matrix operations on an array with reconfigurable optical buses (AROB). The AROB combines some of the advantages and characteristics of reconfigurable meshes and meshes with optical pipelined buses. This model is extremely flexible, as demonstrated by its ability to efficiently simulate CREW PRAMs and reconfigurable networks. A number of applications arc investigated and it is shown that many matrix operations can be implemented efficiently, reducing the time complexity and/or the cost of existing algorithms which are given for other models of parallel computation.		Sandy Pavel;Selim G. Akl	1996	Parallel Algorithms Appl.	10.1080/10637199608915554	time complexity;parallel computing;real-time computing;computer science;distributed computing	Robotics	12.625312417204457	33.24684576701866	168634
b2869ac884fd2734e5bb6d101b682355f389c451	intermediate signed-digit stage to perform residue to binary transformations based on crt	carry logic;signed digit;multiplication operator;convertors;residue number systems;cathode ray tubes dynamic range equations electronic mail hardware algorithm design and analysis arithmetic signal processing algorithms digital signal processors fault tolerant systems;logic circuits;cadence intermediate signed digit stage residue to binary transformations chinese remainder theorem residue to binary converter architecture residue to signed digit signed digit to binary end around carry adder subtracter residue number systems shift left operations redundant adder subtracter blocks;chinese remainder theorem;logic circuits residue number systems carry logic convertors;residue number system	A residue to binary converter architecture based on the Chinese Remainder Theorem (CRT) is presented. The conversion from residue to binary is performed in three levels: Residue to signed-digit 0 Signed-digit to binary 0 End-around cany adder/subtractor By choosing the residue number systems based on (2m-l, 2m+1} or ( ~ 1 , 2m+1, 22m+1} moduli sets, the necessary multiplication operations embedded within the CRT can be replaced by simple shift left operations. The CRT equation is realized by using redundant addedsubtractor blocks in the first level and the carry propagation is totally eliminated. The second level of operation converts the redundant form of CRT to the binary representation. The proposed architecture is free of modulo-M adders. This is achieved by using an end-around carry propagate adder/subtractor in the third level. The two moduli converter was designed and simulated on Cadence.	adder (electronics);binary number;cathode ray tube;embedded system;logical shift;modulo operation;signed number representations;software propagation;subtractor	F. Pourbigharaz;H. M. Yassine	1994		10.1109/ISCAS.1994.408977	arithmetic;multiplication operator;electronic engineering;residue number system;logic gate;serial binary adder;chinese remainder theorem;mathematics;carry-save adder;algorithm	Embedded	15.30846792265408	44.29396693605288	168958
9d25dc8d8f0b398a96fb8ee60dccec290be32339	dynamically adaptable architecture for real-time video processing	dynamically adaptable architecture;field programmable gate array;video signal processing;computer architecture pipelines field programmable gate arrays concurrent computing image sensors image resolution delay video equipment embedded computing image fusion;real time;adaptable ring based architecture;parallel structure dynamically adaptable architecture real time video processing adaptable ring based architecture fpga field programmable gate array pipelined structure;video processing;fpga;computer architecture;pipelined structure;servers;parallel architectures;streaming media;pipelines;pixel;process control;real time video processing;field programmable gate arrays;dynamic adaptation;video signal processing field programmable gate arrays parallel architectures;parallel structure	In this paper, we present a new adaptable ring-based architecture for video processing applications. The proposed architecture allows handling pipelined and parallel organization of computation for multiple video flows. A simplified version with four nodes has been implemented on an FPGA for a video application to show the adaptation mechanism between a pipelined and parallel structure.	computation;field-programmable gate array;parallel computing;pipeline (computing);real-time clock;scalability;video processing	Nicolas Ngan;Eva Dokladalova;Mohamed Akil;François Contou-Carrère	2010	Proceedings of 2010 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2010.5537617	embedded system;parallel computing;real-time computing;computer science;applications architecture;process control;field-programmable gate array	Arch	10.90950204110457	40.07750950530875	169142
e651555bb79014b90d6c064f63f2d99795c64f40	accelerating the dynamic time warping distance measure using logarithmetic arithmetic	logarithmic arithmetic;instruction set extension ise;application specific processor;euclidean distance ed;time series;logarithmic arithmetic time series similarity search application specific processor instruction set extension ise euclidean distance ed dynamic time warping dtw floating point arithmetic;time series analysis optimization software time measurement hardware energy consumption euclidean distance;ise dynamic time warping distance measure logarithmetic arithmetic application specific embedded processor instruction set extensions dtw distance measure time series similarity search;floating point arithmetic;time series digital arithmetic embedded systems instruction sets;similarity search;dynamic time warping dtw	This paper describes an application-specific embedded processor with instruction set extensions (ISEs) for the Dynamic Time Warping (DTW) distance measure, which is widely used in time series similarity search. The ISEs in this paper are implemented using a form of logarithmic arithmetic that offers significant performance and power/energy advantages compared to more traditional floating-point operations.	algorithm;central processing unit;dynamic time warping;embedded system;field-programmable gate array;hardware acceleration;mathematical optimization;similarity search;time series	Joseph Tarango;Eamonn J. Keogh;Philip Brisk	2014	2014 48th Asilomar Conference on Signals, Systems and Computers	10.1109/ACSSC.2014.7094472	arithmetic;discrete mathematics;theoretical computer science;dynamic time warping;mathematics	EDA	11.387798720775203	42.84039250348438	169183
3244bd28d4cfa2dc51502dff5d802efe8a973326	efficient parallel binary search on sorted arrays, with applications	search problems computational complexity parallel algorithms sorting computational geometry;algoritmo paralelo;visibilite;pram;visibilidad;arbre recherche binaire;parallel algorithm;routing sorting concurrent computing physics computing very large scale integration application software gallium arsenide foundries process design parallel algorithms;time complexity;sorting;geometrie algorithmique;computational geometry;algorithme parallele;complexite temps;visibility;arbol investigacion binaria;binary search tree;computational complexity;complexity parallel binary search sorted arrays parallel algorithm erew pram processors;merging;time use;binary search;geometria computacional;search problems;read conflict;complejidad tiempo;parallel algorithms	Let A be a sorted array of n numbers and B a sorted array of m numbers, both in nondecreasing order, with n/spl les/m. We consider the problem of determining, for each element A(j), j=1, 2, ..., n, the unique element B(i), 0/spl les/i/spl les/m, such that B(i)/spl les/A(j) >	binary search algorithm	Danny Ziyi Chen	1995	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.372799	parallel computing;computational geometry;computer science;theoretical computer science;distributed computing;parallel algorithm;algorithm	HPC	14.050495226345918	33.86808017733494	169209
d74007e6d7f76ee6c1376ee3773d64648d7ad940	on the exploration of time-varying networks	macquarie university institutional repository;researchonline;digital repository;carrier networks;mobile networks;mobile agents;evolving graphs;macquarie university;traversal;delay tolerant networks;time varying graphs;exploration;dynamic networks	We study the computability and complexity of the exploration problem in a class of highly dynamic networks: carrier graphs, where the edges between sites exist only at some (unknown) times defined by the periodic movements of mobile carriers among the sites. These graphs naturally model highly dynamic infrastructure-less networks such as public transportswith fixed timetables, lowearth orbiting (LEO) satellite systems, security guards’ tours, etc.We focus on the opportunistic exploration of these graphs, that is by an agent that exploits the movements of the carriers to move in the network. We establish necessary conditions for the problem to be solved. We also derive lower bounds on the amount of time required in general, as well as for the carrier graphs defined by restricted classes of carrier movements. We then prove that the limitations on computability and complexity we have established are indeed tight. In fact we prove that all necessary conditions are also sufficient and all lower bounds on costs are tight. We do so constructively by presenting two optimal solution algorithms, one for anonymous systems, and one for those with distinct node IDs. Crown Copyright© 2012 Published by Elsevier B.V. All rights reserved.	algorithm;complexity;computability;crown group;dynamic infrastructure;exploration problem;schedule;time-varying network	Paola Flocchini;Bernard Mans;Nicola Santoro	2013	Theor. Comput. Sci.	10.1016/j.tcs.2012.10.029	combinatorics;digital library;simulation;exploration;telecommunications;computer science;distributed computing;tree traversal;complex network;algorithm	Theory	18.181922678809695	34.32841664155519	169355
e8589600651026301524bf5f5915ecab18b07084	control complexity of schulze voting	graph representation;high degree;wikimedia foundation;pirate party;condorcet voting system;prospective user;schulze voting;control complexity;schulze voting;electoral control case;control resistance;known voting system	Schulze voting is a recently introduced voting system enjoying unusual popularity and a high degree of real-world use, with users including the Wikimedia foundation, several branches of the Pirate Party, and MTV. It is a Condorcet voting system that determines the winners of an election using information about paths in a graph representation of the election. We resolve the complexity of many electoral control cases for Schulze voting. We find that it falls short of the best known voting systems in terms of control resistance, demonstrating vulnerabilities of concern to some prospective users of the system.	algorithm;graph (abstract data type);ibm notes;pirate party;prospective search;time complexity;vertex separator	Curtis Menton;Preetjot Singh	2013			bullet voting;first-past-the-post voting;voting;single-member district;schulze method;approval voting;calculus of voting;cardinal voting systems;positional voting system;preferential block voting;weighted voting;computer security;anti-plurality voting;condorcet method;disapproval voting	AI	21.682650094879378	37.7559701911991	169666
c18b5583335c7a6870960d39911595daf03a47c7	data reuse exploration for low power motion estimation architecture design in h.264 encoder	architectural design;data locality;nested loops;data reuse;motion estimation;memory access;video coding;low power;signal processing;data access;h 264;low power design;parallel architecture;parallel processing;design methodology	Data access usually leads to more than 50% of the power cost in a modern signal processing system. To realize a low-power design, how to reduce the memory access power is a critical issue. Data reuse (DR) is a technique that recycles the data read from memory and can be used to reduce memory access power. In this paper, a systematic method of DR exploration for low-power architecture design is presented. For a start, the signal processing algorithms should be formulated as the nested loops structures, and data locality is explored by use of loop analysis. Then, corresponding DR techniques are applied to reduce memory access power. The proposed design methodology is applied to the motion estimation (ME) algorithms of H.264 video coding standard. After analyzing the ME algorithms, suitable parallel architectures and processing flows of the integer ME (IME) and fractional ME (FME) are proposed to achieve efficient DR. The amount of memory access is respectively reduced to 0.91 and 4.37% in the proposed IME and FME designs, and thus lots of memory access power is saved. Finally, the design methodology is also beneficial for other signal processing systems with a low-power consideration.	algorithm;computation;data access;data compression;encoder;foreach loop;formal methods europe;h.264/mpeg-4 avc;input method;locality of reference;low-power broadcasting;memory hierarchy;mesh analysis;motion estimation;pixel;power architecture;signal processing;video coding format;windows me	Yu-Han Chen;Tung-Chien Chen;Chuan-Yung Tsai;Sung-Fang Tsai;Liang-Gee Chen	2008	Signal Processing Systems	10.1007/s11265-007-0112-3	uniform memory access;data access;embedded system;parallel processing;computer vision;parallel computing;real-time computing;nested loop join;design methods;computer science;operating system;signal processing;motion estimation	EDA	12.449681124960474	40.13754754047734	169985
15294fc4deb8b523fc180d981e4cbfe3defb8679	how do evolved digital logic circuits generalise successfully?	digital circuit;diseno circuito;logic design;software maintenance;circuit design;periodic structure;digital logic;intelligence artificielle;circuito logico;probabilistic approach;estructura periodica;arithmetique;boolean algebra;circuit numerique;maintenance logiciel;vie artificielle;conception logique;aritmetica;circuit logique;arithmetics;enfoque probabilista;approche probabiliste;circuito numerico;structure periodique;artificial intelligence;conception circuit;inteligencia artificial;artificial evolution;concepcion logica;logic circuit;artificial life	Contrary to indications made by prior researchers, digital logic circuits designed by artificial evolution to perform binary arithmetic tasks can generalise on inputs which were not seen during evolution. This phenomenon is demonstrated experimentally and speculatively explained in terms of the regular structure of binary arithmetic tasks and the nonoptimality of random circuits. This explanation rests on an assumption that evolution is relatively unbiased in its exploration of circuit space. Further experimental data is provided to support the proposed explanation.	adder (electronics);binary number;combinational logic;digital electronics;evolutionary algorithm;experiment;logic gate	Simon McGregor	2005		10.1007/11553090_37	boolean algebra;computer science;artificial intelligence;evolutionary algorithm;mathematics;algorithm	EDA	16.814144707158935	45.65533985929186	170173
13f7bb098dcab30294661aaf1265e031d36845a5	vlsi design of on-line add/multiply algorithms	signed digit;vlsi design;fixed point;computer architecture;computer architecture digital arithmetic vlsi;polynomials on line add multiply algorithms vlsi structures online arithmetic embedded algorithms radix two fixed point signed digit system online add multiply architectures zero delay operator online computation real functions;vlsi;digital arithmetic;very large scale integration algorithm design and analysis arithmetic polynomials delay laboratories computer architecture circuits encoding logic	IINTRODUCTION On-line arithmetic principles were introduced by Ercegovac and Trivedi in 1977 [1][2]. In this arithmetic, operands are represented in a redundant number system such as Avizienis' signed digit systems [3]. They are serially introduced starting from the most significant digit (MSD). Consequently, the result MSDs are first obtained, and can thus be exploited while computation is still in progress. This allows dynamically pushing the computation precision to any extent [4][5][6]. To obtain the first digit of the output of an operator, a small number, called the operator on-line delay, of digits of the input operands are needed. From then on, a new result digit is generated for each new operand digit. Another important feature of on-line architectures is the operator latency which expresses the transition time between the operator inputs and outputs. The question is to know what compromise to make between these two features. In fact, one can optimise one of them at the expense of the other. Designers have to choose which feature should be preferred according to the operator utilisation. In the following, we assume that the numbers are represented in a radix-two signed-digit number system, with digits -1, 0 and 1. Each Signed Binary Digit (SBD) c is represented by two bits c+ and csuch that c = c+ c. This encoding, called borrow-save, is quite convenient for operator design. We first expose the basic VLSI tools used to implement on-line arithmetic algorithms. Then we give some examples of synthesis concerning on-line addition and multiplication. As a straightforward application we will discuss, in section IV, the realisation of a VLSI circuit for on-line polynomial computing. IIVLSI TOOLS II-1 The PPM cell Given three inputs a, b and c, the PPM cell (for Plus Plus Minus) generates two outputs e and f such that 2e f = a + b c. Fig. 1 represents the PPM symbol, logic equations and schematic. e = Maj (a, b, c) f = a ⊕ b ⊕ c a b c e f + + +	a. b. and c.;algorithm;binary-coded decimal;computation;increment and decrement operators;online and offline;operand;polynomial;rise time;schematic;significant figures;smart battery;very-large-scale integration	Ali Skaf;Alain Guyot	1993		10.1109/ICCD.1993.393369	parallel computing;computer science;theoretical computer science;very-large-scale integration	Theory	15.131863733246014	43.92674854461337	170401
1a605fda0701cad0fb73329891f0e2eebec9c8b3	multivalued integrated injection logic	quaternary rom;post logic;quaternary logic;multilevel i2l multivalued logic post logic quaternary logic quaternary rom quaternary flip flops radix 4 arithmetic threshold i2l;radix 4 arithmetic;multivalued logic;quaternary flip flops;flip flop;threshold i2l;multilevel i2l	A family of circuits for multivalued, in particular quaternary, integrated injection logic is described. The basic elements are the I2L current mirror and I2L threshold gates.	current mirror;integrated injection logic	Tich T. Dao;Edward J. McCluskey;Lewis K. Russel	1977	IEEE Transactions on Computers	10.1109/TC.1977.1674784	arithmetic;and-or-invert;logic gate;mathematics;algorithm	Visualization	18.383094800948182	44.88794173385654	170668
c24cedeb3c0fa90fd20660919c945c7c44e9c677	practical summation via gossip	sums;simulation;gossip;peer to peer	Distributed summation is computed in asymptotically minimal rounds by Kempe et alu0027s Push-Sum algorithm. Unfortunately it has minor problems in practise, resolved here.	algorithm;kempe chain	Wesley W. Terpstra;Christof Leng;Alejandro P. Buchmann	2007		10.1145/1281100.1281188	gossip;combinatorics;discrete mathematics;computer science;mathematics;distributed computing	Theory	21.115350106393112	37.151082995363204	170709
f324ca859d4aeca21972f94743b59684eb57cb3b	a need of quantum computing: reversible logic synthesis of parallel binary adder-subtractor	quantum computer		adder (electronics);logic synthesis;quantum computing;reversible computing;subtractor	Himanshu Thapliyal;M. B. Srinivas;Hamid R. Arabnia	2005			quantum gate;discrete mathematics;logic synthesis;computer science;three-input universal logic gate;theoretical computer science;quantum circuit;toffoli gate;quantum computer;adder;reversible computing	EDA	17.237228897772546	45.20816470757956	170840
d3e3c84fb05c3b6e921c63cb78a0eb586d4685da	minimization of boolean functions	boolean function;tabular method;minimization method tabular method selective prime implicants;minimization method;selective prime implicants	The Quine–McCluskey method of minimizing a Boolean function gives all the prime implicants, from which the essential terms are selected by one or more cover tables known as the prime implicant tables. This note describes a tabular method where the essential prime implicants are selected during the process of forming the combination tables, and other essential terms are selected from what have been described in the note as chains of selective prime implicants. Consequently, the need for successive prime implicant tables is eliminated.	quine (computing);table (database);table (information)	Nripendra N. Biswas	1971	IEEE Transactions on Computers	10.1109/T-C.1971.223373	combinatorics;discrete mathematics;computer science;mathematics;boolean function;algorithm	Visualization	21.012451794947797	43.81452345558703	171122
1cfc842bb63fa7b1ee195cce8484b3bd6ec5d745	algorithm/architecture co-design of the generalized sampling theorem based de-interlacer [video signal processing]	image sampling;algorithm architecture co design;processor architecture;hardware software codesign;motion compensation;video signal processing;processor scheduling hardware software codesign image sampling motion estimation motion compensation video signal processing high definition television;signal sampling;processor scheduling;multilevel scratchpad memories;local pixel storage;motion estimation;de interlacer;10 gbyte s;motion compensated;heterogeneous multiprocessor architecture;computer architecture;generalized sampling theorem;signal processing algorithms signal sampling video signal processing bandwidth image sampling motion estimation computer architecture image quality displays motion compensation;deinterlacing;processor computational load balancing;displays;image quality;bandwidth;communication bandwidth;signal processing algorithms;memory bandwidth;10 gbyte s algorithm architecture co design generalized sampling theorem de interlacer deinterlacing image quality motion estimation motion compensation hdtv interlaced input material communication bandwidth local pixel storage heterogeneous multiprocessor architecture processor computational load balancing multilevel scratchpad memories;hdtv interlaced input material;high definition television;sampling theorem	De-interlacing is a major determinant of image quality in a modern display processing chain. The de-interlacing method, based on the generalized sampling theorem (GST) applied to motion estimation and motion compensation, provides the best de-interlacing results. With HDTV interlaced input material (1920*1080i), this method requires about 1000 GOPs and a communication bandwidth around 10 Gbytes/sec. We analyze and simplify the algorithm and propose a processing architecture. As a result, the operation count of the motion estimator decreases by a factor of 5.5 and the bandwidth to local pixel storage by a factor of 3.3 with only mild and acceptable quality loss. We present a task breakup and a suitable heterogeneous multi-processor architecture. The task break-up is such that the computational load of the processors is balanced and the flexibility of the architecture is preserved within the application domain. To cope with the large memory bandwidth requirements, we exploit locality of reference with multi-level scratchpad memories.	algorithm;application domain;application-specific instruction set processor;central processing unit;computation;digital light processing;elegant degradation;gigabyte;heterogeneous computing;image quality;interlaced video;interlacing (bitmaps);iteration;level of detail;locality of reference;loss function;memory bandwidth;memory hierarchy;motion compensation;motion estimation;multiprocessing;nyquist–shannon sampling theorem;pixel;requirement;sampling (signal processing);scratchpad memory;signal processing	Aleksandar Beric;Gerard de Haan;Jef L. van Meerbergen;Ramanathan Sethuraman	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1465244	image quality;nyquist–shannon sampling theorem;embedded system;computer vision;electronic engineering;parallel computing;real-time computing;microarchitecture;computer science;deinterlacing;motion estimation;motion compensation;memory bandwidth;bandwidth	Arch	11.781794392707633	40.06763094629021	171524
da5b11a2f777430fae5302aa0d9c2e96b0e77eae	parallel memories in video encoding	storage allocation;encoding video compression motion estimation discrete cosine transforms parallel processing postal services video signal processing laboratories memory architecture interpolation;interpolation;real time video compression;idct;data compression;video signal processing;vertical coordination;real time;module assignment function;transform coding video coding parallel memories motion estimation data compression interpolation storage allocation discrete cosine transforms;video compression;motion estimation;transform coding;2d coordinate system;low bit rate video compression;address function parallel memories hybrid video coding motion estimation idct real time video compression low bit rate video compression itu t h 263 standard parallel processing elements zigzag scanning interpolation module assignment function 2d coordinate system memory addresses;video coding;postal services;memory addresses;memory architecture;discrete cosine transforms;itu t h 263 standard;zigzag scanning;parallel processing elements;encoding;address function;memory bandwidth;parallel processing;parallel memories;coordinate system;hybrid video coding	A novel architecture with parallel memories [1] suitable for hybrid video coding is presented. It efficiently relieves the memory bandwidth bottleneck in motion estimation, DCT, and IDCT involved in the real-time low-bit rate ITU-T H.263 video compression standard. There are four parallel processing elements and eight parallel memory blocks in the system. The address space is divided into three areas. Coordinate areas 0 and 1 can be accessed simultaneously for row or column formats, needed in the motion estimation, DCT, and IDCT. Alternatively, the area 2 can be accessed for a more complex formats. Such formats are needed, for example, in zigzag scanning and interpolation. The module assignment function S(i,j), expresses how data is stored to the memory modules. We can describe the memory space as an 2D coordinate system with horizontal and vertical coordinates ( i,j). The coordinate values are restricted to positive values, and (0,0) is fixed to the uppermost left corner of the coordinate area. The function S(i,j) simply describes the memory block, where the value of coordinate point ( i,j) is stored. Memory addresses are described by the address function a(i,j). The coordinate area 0 deals with the memory blocks 0...3, the area 1 with the blocks 4...7 and the area 2 with the blocks 0...7. In Fig.1. constants a0max and a1max are the maximum addresses of the coordinate areas 0 and 1, respectively. The width of the coordinate area is given by Li. The processing power increases linearly with the number of parallel processing elements. Using more parallel memory blocks enables using more access formats.	address space;dimm;dspace;data compression;discrete cosine transform;interpolation;left corner;memory bandwidth;motion estimation;parallel computing;random-access memory;real-time clock;video coding format	Jarno K. Tanskanen;Jarkko Niittylahti	1999		10.1109/DCC.1999.785709	data compression;parallel processing;computer hardware;computer science;theoretical computer science;mathematics;statistics;computer graphics (images)	ML	11.937781924224872	39.561102253669084	171745
52fc882d3b2be1ecd2a0cc034c21b14bd8b93097	efficient analysis of fault trees with voting gates	fault tree;complexity theory;decision tree;boolean functions;evaluation method;boolean function;binary decision tree based algorithms fault tree analysis voting gates k out of n gate standard logic gate fault trees modelling fault tolerant systems and gate or gate combinatorial explosion problem minimal cut sets reduction rules minimal cut vote;fault tolerant system;fault tolerant computing;logic gates;data structures;efficiency analysis;decision trees;logic gate;data structure;algorithm design;algorithm design and analysis;logic gates fault trees boolean functions data structures algorithm design and analysis equations complexity theory;fault trees;logic gates decision trees fault tolerant computing fault trees	The voting gate, or k-out-of-n (k/n) gate, is a standard logic gate used in fault trees modelling fault-tolerant systems. It is traditionally expanded into a combination of AND and OR gates, and this expansion may result in combinatorial explosion problem in the calculation of minimal cut sets (MCSs) of the fault tree for even a not very big n, especially when the voting gate inputs are intermediate rather than basic events. In this paper we propose a set of reduction rules to simplify the voting gates without direct expanding, and also propose a concept of minimal cut vote (MCV) denoting a k/n gate whose inputs are all basic events and whose k-combinations are all MCSs of the fault tree. With the proposed reduction rules and MCV concept, the MCSs of fault trees can be evaluated and weeded more efficiently and the result can be represented in a more compact form. The results of experiments on practical fault trees with voting gates show that our method not only outperforms conventional MCS evaluation methods by several orders of magnitude but also provides performance comparably to that provided by binary decision tree (BDD) based algorithms.	algorithm;byzantine fault tolerance;decision tree;experiment;fault tree analysis;logic gate;max-flow min-cut theorem;mobile television	Jianwen Xiang;Kazuo Yanoo;Yoshiharu Maeno;Kumiko Tadano;Fumio Machida;Atsushi Kobayashi;Takao Osaki	2011	2011 IEEE 22nd International Symposium on Software Reliability Engineering	10.1109/ISSRE.2011.23	algorithm design;discrete mathematics;fault tree analysis;data structure;logic gate;computer science;theoretical computer science;decision tree;mathematics;boolean function;algorithm	Logic	21.682882999573277	42.64184540252431	172026
8ea00c54e2fc1f55b4ddd8c520f8b347856792cf	self-stabilizing rendezvous of synchronous mobile agents in graphs		We investigate self-stabilizing rendezvous algorithms for two synchronous mobile agents. The rendezvous algorithms make two mobile agents meet at a single node, starting from arbitrary initial locations and arbitrary initial states. We study deterministic algorithms for two synchronous mobile agents with different labels but without using any whiteboard in the graph. First, we show the existence of a self-stabilizing rendezvous algorithm for arbitrary graphs by providing a scheme to transform a non-stabilizing algorithm to a self-stabilizing one. However, the time complexity of the resultant algorithm is not bounded by any function of the graph size and labels. This raises the question whether there exist polynomial-time self-stabilizing rendezvous algorithms. We give partial answers to this question. We give polynomial-time self-stabilizing rendezvous algorithms for trees and rings.	approximation algorithm;existential quantification;graph coloring;mobile agent;polynomial;resultant;self-stabilization;time complexity	Fukuhito Ooshita;Ajoy K. Datta;Toshimitsu Masuzawa	2017		10.1007/978-3-319-69084-1_2	time complexity;self-stabilization;rendezvous;bounded function;mathematics;distributed computing;graph	Theory	17.95962974246381	33.96595634525073	172141
01695a054a46257e166c02e23262ceba53c83d31	an algorithm for minimum space quantum boolean circuits construction	boolean circuits;computer circuit design;quantum circuits	Implementing a quantum computer at the circuit level has emerged as an important field of research recently. An important topic of building a general-purpose quantum computer is to implement classical Boolean logic using quantum gates and devices. Since the Toffoli gate is universal in classical Boolean logic, any classical combinational circuit can be implemented by the straightforward replacement algorithm with auxiliary qubits as intermediate storage. However, this inefficient implementation causes a large number of auxiliary qubits to be used. In this paper, a systematic procedure is proposed to derive a minimum space quantum circuit for a given classical combinational logic. We first formulate the problem of transforming an m-to-n bit classical Boolean logic into a t-bit unitary quantum operation. The eligible solution set is then constructed such that a solution can be found simply by selecting any member from this set. Finally, we show that the algorithm is optimal in terms of the space consumption.	boolean algebra;boolean circuit;boolean expression;circuit design;combinational logic;computer hardware;general-purpose modeling;logic gate;logical connective;page replacement algorithm;quantum circuit;quantum computing;quantum gate;quantum mechanics;quantum operation;qubit;search algorithm;speedup;toffoli gate	I-Ming Tsai;Sy-Yen Kuo	2006	Journal of Circuits, Systems, and Computers	10.1142/S0218126606003349	boolean algebra;boolean circuit;and-inverter graph;circuit minimization for boolean functions;discrete mathematics;quantum information;boolean domain;toffoli gate;boolean expression;product term;standard boolean model;computer science;theoretical computer science;quantum network;quantum circuit;mathematics;combinational logic;quantum computer;digital electronics;quantum algorithm;algorithm;quantum phase estimation algorithm;quantum sort;quantum gate	EDA	17.32059531410868	45.33957471713362	172220
4a2deabdae17c817fcf58d0466214bd905403272	a short note on the 1, 2-good-neighbor diagnosability of balanced hypercubes	good neighbor diagnosability;interconnection network;pmc model;mm model;balanced hypercube	Let tc(G) and tg(G) be the conditional diagnosability and g-good-neighbor diagnosability, respectively, of a graph G. The notion of the g-good-neighbor conditional diagnosability is less restrictive as compared with that of the conditional diagnosability in general. Particularly, the conditional faulty set notion requires that, any vertex, faulty or not, have at least one non-faulty neighbor; while the 1-good-neighbor faulty only requires that a non-faulty vertex have at least one non-faulty neighbor. Compared with conditional diagnosability, g-good-neighbor diagnosability is interesting since it characterizes a stronger tolerance capability. In this paper, we investigate the equal relation between t1(BHn) and tc(BHn) for the balanced hypercubes BHn. That is t1(BHn) = tc(BHn) = 4n - 3 for n ≥ 2 under the PMC model and t1(BHn) = tc(BHn) = 4n - 4 for n ≥ 2 under the MM model; Furthermore, the 2-good-neighbor diagnosability t2(BHn) = 4n − 1 for n ≥ 2 under the PMC model and the MM model is obtained.		Mei-Mei Gu;Rong-Xia Hao;Dond-Xue Yang	2016	Journal of Interconnection Networks	10.1142/S0219265916500018	discrete mathematics;parallel computing;mathematics;distributed computing	Theory	23.34285566363681	34.44353102142922	172501
aa0037e6225e6213ff5f33084113e6de2e0c91fb	entropy coding on a programmable processor array for multimedia soc	vliw processor;3d information;image reconstruction field programmable gate arrays computer architecture sensor phenomena and characterization instruments embedded system packaging wireless sensor networks real time systems machine vision;data compression;decoding;instruction set extensions;vliw processors;data processing;entropy coding;system on chip multimedia systems parallel processing;real time embedded system;hardware accelerator;transform coding;image sensors;multimedia systems;real time embedded systems;image sensors computer vision digital signal processing chips image reconstruction;embedded system;computer vision;cyclope;programming model;vliw;video coding;instruction set extension;power engineering computing;programmable processor array;system on chip;data dependence;image reconstruction;power dissipation;entropy coding encoding hardware decoding multimedia systems costs data compression power engineering computing vliw transform coding;interconnected system;digital processing architecture;digital signal processing chips;power dissipation digital processing architecture 3d reconstruction 3d vision real time embedded systems cyclope 3d information;multi spectral;parallel architecture;3d vision;entropy encoding;encoding;hardware implementation;parallel processing;3d reconstruction;instruction set extensions programmable processor array entropy encoding entropy decoding multimedia system vliw processors;software implementation;multimedia system;entropy decoding;hardware	Entropy encoding and decoding is a crucial part of any multimedia system that can be highly demanding in terms of computing power. Hardware implementation of typical compression and decompression algorithms is cumbersome, while conventional software implementations are slow due to bit-level operations, data dependencies and conditional branching. Several solutions have been proposed along the years, ranging from hardware accelerators for high-end systems to careful implementations in VLIW processors and instruction-set extensions, both hardwired and reconfigurable. Multimedia systems must often implement several encoders and decoders for different formats. Hence, a programmable solution is mandatory. However, programmable processors may be challenged by highly-complex algorithms. In this work, a highly efficient and low cost alternative is presented based on an array processor. The dataflow of several entropy coding algorithms has been studied, leading to the choice of an efficient programming model, processor layout and interconnection system. Results are presented for JPEG and H.264 image and video coding standards.	algorithm;array processing;binary decoder;bit-level parallelism;branch (computer science);central processing unit;data compression;data dependency;dataflow;encoder;entropy encoding;h.264/mpeg-4 avc;hardware acceleration;interconnection;jpeg;processor array;programming model;vector processor;very long instruction word;video coding format	Roberto R. Osorio;Javier D. Bruguera	2007	2007 IEEE International Conf. on Application-specific Systems, Architectures and Processors (ASAP)	10.1109/ASAP.2007.4429984	embedded system;parallel processing;computer architecture;parallel computing;real-time computing;data processing;computer science;entropy encoding;theoretical computer science;operating system;statistics	Arch	11.390148006431875	40.792024564439046	172683
e0fa6883e95ba5f8ac16b75753fd5233f2667b0c	efficient algorithms for computing two nearest-neighbor problems on a rap	algoritmo paralelo;nombre entier;parallel algorithm;image processing;efficient algorithm;procesamiento imagen;reconfigurable bus system;conception;traitement image;algorithme parallele;integer;entero;nearest neighbor;diseno;design;maximum number	Abstract   This paper makes an improvement of computing two nearest-neighbor problems of images on a reconfigurable array of processors (RAP) by increasing the bus width between processors. Based on a base- n  system, a constant time algorithm is first presented for computing the maximum/minimum of  N  log  N -bit unsigned integers on a RAP using  N  processors each with  N  1/ c  -bit bus width, where  c  is a constant and  c  ≥ 1. Then, two basic operations such as image component labeling and border following are also derived from it. Finally, these algorithms are used to design two constant time algorithms for the nearest neighbor black pixel and the nearest neighbor component problems on an   N      1  2     × N      1  2     image using   N      1  2     × N      1  2     processors each with  N  1/ c  -bit bus width, where  c  is a constant and  c  ≥ 1. Another contribution of this paper is that the execution time of the proposed algorithms is tunable by the bus width.	algorithm	Tzong-Wann Kao;Shi-Jinn Horng	1994	Pattern Recognition	10.1016/0031-3203(94)90088-4	integer;computer vision;design;parallel computing;image processing;computer science;theoretical computer science;machine learning;mathematics;parallel algorithm;algorithm	Vision	11.997588282348058	35.426015385720476	172910
f29697212f2c7fb05547367bb206b4a205b865f2	vlsi implementation of high-throughput parallel h.264/avc baseline intra-predictor	intracoding;parallel vlsi architecture;data dependency;macroblock coding;coding performance improvement;frequency 105 mhz mb coding macroblock coding yuv component processing configurable computation core high speed architecture coding performance improvement intracoding intraprediction data dependency parallel vlsi architecture high throughput parallel h 264 avc baseline intra predictor;high throughput parallel h 264 avc baseline intra predictor;intraprediction;frequency 105 mhz;high speed architecture;yuv component processing;vlsi prediction theory video coding;mb coding;configurable computation core	This study presents a parallel very large scale integrated circuits architecture for an intra-predictor based on a fast 4 × 4 algorithm. For real-time scheduling, the proposed algorithm overcomes the data dependency between intra-prediction and intracoding, thereby improving coding performance and reducing the number of coding cycles. The high-speed architecture for intraprediction includes configurable computation cores to process YUV components using 10 pixel parallelism. Prediction for one macro-block (MB) coding (luminance: 4 × 4 and 16 × 16 block modes; chrominance: 8 × 8 block modes) can all be completed within 256 cycles. The proposed architecture achieves throughput of 410 kMB/s, suitable for 1920 × 1080/35 Hz 4:2:0 HDTV encoder at a working frequency of 105 MHz.	algorithm;baseline (configuration management);chroma subsampling;computation;data dependency;encoder;h.264/mpeg-4 avc;high-throughput computing;integrated circuit;kerrison predictor;parallel computing;pixel;real-time clock;real-time operating system;scheduling (computing);throughput;very-large-scale integration	Shih-Chang Hsia;Ying-Chao Chou	2014	IET Circuits, Devices & Systems	10.1049/iet-cds.2013.0097	electronic engineering;parallel computing;real-time computing;computer science;context-adaptive variable-length coding;coding tree unit;context-adaptive binary arithmetic coding	EDA	12.406828128136887	40.37381737994242	173266
a1ca4dda916bd2d581626682e588f21379eba025	computations of uniform recurrence equations using minimal memory size	minimisation;digital circuit;68m07;minimization;diseno circuito;max;circuito y;sistema;circuit design;synchronous;max plus linear system;equation recurrence;task graph;minimizacion;minimisation registre;synchrone;linear system;registre;parallel computation;94c15;capacidad memoria;recurrence;circuit numerique;grafo;en parallele;calculo paralelo;capacite memoire;memory capacity;en paralelo;sincronico;recurrencia;uniformite;system;graph;uniformidad;graphe;circuito numerico;parallel;recurrence equation;uniformity;and circuit;93c65;conception circuit;procesador;systeme;ure;processeur;uniform recurrence equation;circuit et;calcul parallele;register minimization;registro;processor;register;ecuacion recurrencia	We consider a system of uniform recurrence equations (URE) of dimension one. We show how its computation can be carried out using minimal memory size with several synchronous processors. This result is then applied to register minimization for digital circuits and parallel computation of task graphs.	central processing unit;computation;digital electronics;parallel computing;recurrence relation	Bruno Gaujal;Alain Jean-Marie;Jean Mairesse	2000	SIAM J. Comput.	10.1137/S0097539795290350	minimisation;combinatorics;discrete mathematics;circuit design;parallel;system;mathematics;linear system;graph;digital electronics;synchronous learning;algorithm	Theory	12.653241403292853	35.144195161278994	173924
0e63012e6540778037eaba0aa1ed9cedc2fdcf12	on the polynomial residue number system [digital signal processing]	digital signal processing;polynomials computerised signal processing digital arithmetic number theory parallel algorithms;polynomials;polynomials digital signal processing convolution signal processing algorithms parallel processing arithmetic autocorrelation parallel architectures throughput digital filters;number theory;multiplication intensive algorithms prns operations rules modular ring signal processing pairwise multiplication polynomial residue number system prns totally parallel polynomial multiplication mapping parallel scheme mapped polynomial coefficients digital signal processing dsp;digital arithmetic;residue number system;computerised signal processing;parallel algorithms	The theory of the polynomial residue number system (PRNS), a system in which totally parallel polynomial multiplication can be achieved provided that the arithmetic takes place in some carefully chosen ring, is examined. Such a system is defined by a mapping which maps the problem of multiplication of two polynomials onto a completely parallel scheme where the mapped polynomial coefficients are multiplied pairwise. The properties of the mapping and the rules of operations in the PRNS are proven. Choices of the rings for which the PRNS is defined are also studied. It is concluded that the PRNS can offer significant advantages in those digital signal processing (DSP) applications that involve multiplication-intensive algorithms like convolutions and one-dimensional or multidimensional correlation. >	digital signal processing;polynomial;residue number system	Alexander Skavantzos;Fred J. Taylor	1991	IEEE Trans. Signal Processing	10.1109/78.80821	multidimensional signal processing;residue number system;number theory;discrete mathematics;theoretical computer science;digital signal processing;mathematics;parallel algorithm;polynomial;algebra	Embedded	11.19894796051337	43.516806358313126	173996
ee6b16043a2aefbe0933037cbf9beeee3cc56665	a low-power oriented architecture for h.264 variable block size motion estimation based on a resource sharing scheme	motion estimation me;sad;fsvbsme;motion vector mv	In the Advanced Video Coding (AVC) standard, motion estimation (ME) adopts many new features to increase the coding performances such as block matching algorithm (BMA), motion vector prediction (MVP) and variable block size motion estimation (VBSME). However, VBSME is utilized in the MPEG4-AVC/H.264 standard which leads to high computational complexity and data dependency that make the hardware implementation very complex. This paper proposes a flexible VLSI architecture for full-search VBSME (FSVBSME), allowing the partitioning of the source frames into sixteen 4x4 sub-blocks and using a MVP scheme. A clock gating technique based on a distributed control unit is used for power saving. The proposed architecture was designed by Synopsys Design Compiler with 0.13@mm CMOS standard cell library. Under a clock frequency of 500MHz, it allows a power consumption of about 131mW. Our VLSI architecture, compared with contemporary ones, can offer higher processing speed, lower power consumption, lower latency and lower gate count complexity.	block size (cryptography);h.264/mpeg-4 avc;low-power broadcasting;motion estimation	Majdi Elhaji;Abdelkrim Zitouni;Samy Meftali;Jean-Luc Dekeyser;Rached Tourki	2013	Integration	10.1016/j.vlsi.2012.09.001	embedded system;electronic engineering;real-time computing;quarter-pixel motion;computer science;operating system;selected area diffraction	Robotics	12.577107860092099	40.89157382395992	174074
e3c037ba46e35bf4f4362e4d78c3bb850ae36a68	a reconfigurable heterogeneous multimedia processor for ic-stacking on si-interposer	processing element;cache storage;cmos integrated circuits;filtering;high resolution;static random access memory;image processing;integrated circuit;intellectual property;energy efficient;sram chips cache storage cmos integrated circuits computer graphics data communication device drivers embedded systems graphics processing units image processing integrated circuits mobile computing multimedia computing multiprocessing systems pipeline processing reconfigurable architectures;computer graphics;reconfigurable architectures;prototypes;data communication;three dimensional;chip;multimedia computing;memory access;media;embedded systems;mobile environment;media processor;graphics three dimensional displays media filtering hardware pipelines prototypes;si interposer 3 d stacked ic augmented reality ar multimedia processor;media processing;device drivers;three dimensional displays;multimedia processor;graphics processing units;pipelines;si interposer;handheld device;floating point;multiprocessing systems;memory hierarchy;external memory;augmented reality ar;augmented reality;mobile computing;memory bandwidth;high speed;integrated circuits;3 d stacked ic;graphics;pipeline processing;level 1;hardware;sram chips	This paper presents a heterogeneous multimedia processor for embedded media applications such as image processing, vision, 3-D graphics and augmented reality (AR), assuming integrated circuit (IC)-stacking on Si-interposer. This processor embeds reconfigurable output drivers for external memory interface to increase memory bandwidth even in a mobile environment. The implemented output driver reconfigures its driving strength according to channel loss between the implemented processor and the memory, so it enables highspeed data communication while achieving 8× higher memory bandwidth compared to previous embedded media processors. The implemented processor includes three main programmable intellectual properties, mode-configurable vector processing units (MCVPUs), a unified filtering unit (UFU), and a unified shader. MCVPUs have 32 integer (16 bit) cores in order to support dual-mode operations between image-level processing and graphics processing. This mode-configuration enables a frame-level pipelining in AR application, so the proposed processor achieves 1.7× higher frame rate compared to the sequential AR processing. UFU supports 16 types of filtering operations only with a single instruction. Most image-level processing consists of various types of filtering operations, so UFU can improve media processing performance and energy-efficiency. UFU also supports texture filtering which is performance bottleneck of common graphics pipeline. A memory-access-efficient (off-chip memory) texturing algorithm named as an adaptive block selection is proposed to enhance texturing performance in 3-D graphics pipeline. UFU has two-level on-chip memory hierarchies, a 512B level-0 (L0) data buffer, and an 8kB level-1 (L1) static random-access memory (SRAM) cache. The small-sized L0 data buffer limits direct references to the large-sized L1 SRAM cache to reduce energy consumed in on-chip memories. Unified shader consists of four homogeneous scalar processing elements (SPEs) for geometry operations in 3-D graphics. Each SPE has single-precision floating-point data-paths, since precision of geometry operations in 3-D graphics is important in today's handheld devices (high resolution). The proposed media processor is fabricated in 0.13 μm CMOS technology with 4 mm × 4 mm chip size, and dissipates 275 mW for full AR operation.	16-bit;algorithm;augmented reality;cmos;central processing unit;clock rate;computer graphics;computer memory;data buffer;data parallelism;embedded system;external memory interface;flops;gigabyte;graphics pipeline;image processing;image resolution;integrated circuit;interposer;logic gate;media processor;memory bandwidth;memory hierarchy;mobile device;parallel computing;pipeline (computing);pixel;power supply;random access;report definition language;simd;shading;single-precision floating-point format;stacking;static random-access memory;texture filtering;usb on-the-go;unified shader model;vector processor;vertex (graph theory)	Hyo-Eun Kim;Jae Sung Yoon;Kyu-Dong Hwang;Young-Jun Kim;Jun-Seok Park;Lee-Sup Kim	2012	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2011.2171209	embedded system;augmented reality;parallel computing;computer hardware;image processing;computer science;integrated circuit;texture mapping unit;mobile computing	Arch	10.428380458704629	40.534049443578766	174190
9a85ff1a2d7c66b8f139a7ff53e4f82ae06f0269	utilization of a fluidic infrastructure for the realization of enzyme-based boolean logic operations	parallel computing;biocomputing;enzyme;biomolecular computing;reversible logic;flow systems;unconventional computing;logic gate	AbstractThe paper overviews the origin and motivation of the unconventional molecular computing, specially emphasizing the advantages of biomolecular systems for designing highly sophisticated logic circuits. Technological solutions based on flow design of enzyme-based logic gates are discussed and illustrated with examples. Integration of the enzyme-based logic gates and their concatenated assemblies with signal-responsive materials and electronic interfaces is suggested as a platform for biomedical sensors/actuators digitally responding in the Yes/No format to various combinations of biomolecular signals, particularly represented by biomarkers important in biomedical applications. The complexity of the designed enzyme-based logic systems allowed for logically reversible computing, thus being a step forward in the development of biomolecular computing devices. The features of reversible biocomputing systems are discussed, particularly emphasizing their logic reversibility, but not physical reversibility....	logical connective	Brian E. Fratto;Evgeny Katz	2017	IJPEDS	10.1080/17445760.2016.1213250	embedded system;enzyme;reversible computing;parallel computing;logic synthesis;logic gate;computer science;theoretical computer science;distributed computing;algorithm;unconventional computing	EDA	18.31111056067463	41.88336416041174	174432
5f89a9b334651aac84cc13f6c6509d2f1a70c520	the random bit complexity of mobile robots scattering	scattering;mobile robots;randomized algorithm;random bits complexity	We consider the problem of scattering n robots in a two dimensional continuous space. As this problem is impossible to solve in a deterministic manner [6], all solutions must be probabilistic. We investigate the amount of randomness (that is, the number of random bits used by the robots) that is required to achieve scattering. We first prove that n logn random bits are necessary to scatter n robots in any setting. Also, we give a sufficient condition for a scattering algorithm to be random bit optimal. As it turns out that previous solutions for scattering satisfy our condition, they are hence proved random bit optimal for the scattering problem. Then, we investigate the time complexity of scattering when strong multiplicity detection is not available. We prove that such algorithms cannot converge in constant time in the general case and in o(log logn) rounds for random bits optimal scattering algorithms. However, we present a family of scattering algorithms that converge as fast as needed without using multiplicity detection. Also, we put forward a specific protocol of this family that is random bit optimal (n logn random bits are used) and time optimal (log logn rounds are used). This improves the time complexity of previous results in the same setting by a logn factor. Aside from characterizing the random bit complexity of mobile robot scattering, our study also closes its time complexity gap with and without strong multiplicity detection (that is, O(1) time complexity is only achievable when strong multiplicity detection is available, and it is possible to approach it as needed otherwise). keyword Mobile robots, Scattering, Probabilistic algorithms, Complexity	algorithm;context of computational complexity;converge;mobile robot;probabilistic turing machine;randomness;time complexity	Quentin Bramas;Sébastien Tixeuil	2017	Int. J. Found. Comput. Sci.	10.1142/S0129054117500083	mobile robot;mathematical optimization;combinatorics;computer science;theoretical computer science;mathematics;bit field;scattering;randomized algorithm;algorithm	Theory	17.150197780766874	33.16453285093831	174762
478f5b20de293b2545b7c29101088d5d1f839207	measuring and reducing the performance gap between embedded and soft multipliers on fpgas	multiplying circuits adders application specific integrated circuits digital arithmetic field programmable gate arrays;field programmable gate arrays adders delay digital signal processing pipeline processing table lookup space exploration;soft multiplier fpga dsp blosck embedded multiplier;multiplying circuits;soft multiplier;fpga;design space;application specific integrated circuits;adders;digital arithmetic;digital circuit embedded multipliers soft multipliers fpga asic arithmetic dominated circuits dsp blocks logic blocks architecture arithmetic circuits adder tree parallel multiplier;field programmable gate arrays;digital circuits;embedded multiplier;arithmetic circuit;dsp blosck	To bridge the gap between FPGAs and ASICs for arithmetic dominated circuits, one key step is to improve multipliers on FPGAs. This is a key feature that FPGA vendors have tried to improve in recent years by embedding ASIC like multipliers in the DSP blocks. However, due to the limited number of DSP blocks in an FPGA, their fixed location and bit-width limitation, efficient soft logic implementation of multipliers is fundamental. This is the reason that FPGA vendors have enhanced the logic blocks architecture to improve certain arithmetic circuits such as adder tree, which is the basic part of a parallel multiplier. This paper has two main contributions: (1) The performance gap between embedded and soft multipliers is measured and the design space is explored and (2) the current performance gap is reduced by employing a number of target specific mapping and arithmetic transformation techniques. For this purpose, a multiplier generator tool is developed and two conventional multiplication techniques are implemented in this tool. We compare our multipliers with the ones that are generated by Altera core generator tool considering a wide range of bit-widths. Therefore, this paper can be used as a reference for the digital circuit designers to choose the right way of implementing multipliers on FPGAs based on their design constraints.	adder (electronics);application-specific integrated circuit;digital electronics;embedded system;field-programmable gate array	Hadi Parandeh-Afshar;Paolo Ienne	2011	2011 21st International Conference on Field Programmable Logic and Applications	10.1109/FPL.2011.48	embedded system;parallel computing;computer hardware;computer science;field-programmable gate array	EDA	11.622334099878081	46.109031575259266	174833
4c91eedf07957f94590593e309b87a40b19e748e	adaptive approximation in arithmetic circuits: a low-power unsigned divider design		Many approximate arithmetic circuits have been proposed for high-performance and low-power applications. However, most designs are either hardware-efficient with a low accuracy or very accurate with a limited hardware saving, mostly due to the use of a static approximation. In this paper, an adaptive approximation approach is proposed for the design of a divider. In this design, division is computed by using a reduced-width divider and a shifter by adaptively pruning the input bits. Specifically, for a 2n/n division 2k/k bits are selected starting from the most significant ‘1’ in the dividend/divisor. At the same time, redundant least significant bits (LSBs) are truncated or if the number of remaining LSBs is smaller than 2k for the dividend or k for the divisor, ‘0’s are appended to the LSBs of the input. To avoid overflow, a 2(k + 1)/(k + 1) divider is used to compute the division of the 2k-bit dividend and the k-bit divisor, both with the most significant bits being ‘0’. Thus, k < n is a key variable that determines the size of the divider and the accuracy of the approximate design. Finally, an error correction circuit is proposed to recover the error caused by the shifter by using OR gates. The synthesis results in an industrial 28nm CMOS process show that the proposed 16/8 approximate divider using an 8/4 accurate divider is 2.5χ as fast and consumes 34.42% of the power of the accurate 16/8 design. Compared with the other approximate dividers, the proposed design is significantly more accurate at a similar power-delay product. Moreover, simulation results show that the proposed approximate divider outperforms the other designs in two image processing applications.	approximation algorithm;arithmetic circuit complexity;cmos;error detection and correction;frequency divider;image processing;integrated circuit;least significant bit;low-power broadcasting;most significant bit;power–delay product;simulation	Honglan Jiang;Leibo Liu;Fabrizio Lombardi;Jie Han	2018	2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)	10.23919/DATE.2018.8342233	computer science;divisor;error detection and correction;image processing;image restoration;electronic circuit;or gate;arithmetic;logic gate;cmos	EDA	14.457058621410066	42.666292841754405	174975
1411ae914ffc465e0ee2564e8aa01363720737e2	a new serial/parallel two's complement multiplier for vlsi digital signal processing	traitement signal;circuit theory;digital signal processing;teoria circuito;theorie circuit;implementation;circuit vlsi;algorithme;algorithm;ejecucion;circuit serie parallele;vlsi circuit;senal numerica;signal processing;signal numerique;digital signal;multiplicacion;circuito vlsi;multiplication;procesamiento senal;algoritmo		digital signal processing;two's complement;very-large-scale integration	George Alexiou;Nick Kanopoulos	1992	I. J. Circuit Theory and Applications	10.1002/cta.4490200207	computer vision;electronic engineering;parallel computing;network analysis;digital signal;computer science;digital signal processing;signal processing;implementation;multiplication;algorithm	EDA	16.45484552710327	40.46128404152955	175070
065b42537e221b46c31f385bac6ce7585b159dd7	process cooperation in multiple message broadcast	process cooperation;empirical study;communication scheduling;communication schedule;collective communication;broadcast;communication model;one port fully connected system;efficient implementation;space complexity;mpi collective communication;optimal algorithm	We present an optimal algorithm for broadcasting m messages from one process to n-1 other processes in a one-port fully connected communication model, where m>=1,n>1. In this algorithm, the processes are organized into 2^@?^l^o^g^n^@? cooperation units, each consisting of one or two processes. Messages are broadcast among the units following a basic schedule. Processes in each two-process unit cooperate to carry out the basic schedule. At any communication round, either process has at most one message that the other has not received. This algorithm completes the broadcast operation in m+@?logn@?-1 communication rounds, which is theoretically optimal. We consider practical issues for efficient implementation of the algorithm and develop a schedule construction that has both time and space complexity of O(logn). Empirical study shows that this algorithm outperforms other widely used algorithms significantly when the data to broadcast is large.		Bin Jia	2009	Parallel Computing	10.1016/j.parco.2009.09.002	parallel computing;real-time computing;atomic broadcast;broadcast communication network;models of communication;computer science;theoretical computer science;distributed computing;dspace;empirical research;algorithm	HPC	11.80002216551954	34.09154878042631	175136
7c70a0ca5dc60b313de2aa142ed93ef9218009fe	self-simulation for the passive optical star	optical network;reseau communication;interconnection;shared memory;pos;selfsimulation problem;passive optical star;technology;probleme autosimulation;portability;parallel computation;interconexion;calculo paralelo;portabilite;technologie;interconnexion;parallel computer;parallel machines;upper and lower bounds;red de comunicacion;calcul parallele;communication network;algorithm design;communication pattern;portabilidad;tecnologia	Optical technology ooers simple interconnection schemes with straightforward layouts that support complex logical interconnection patterns. The Passive Optical Star (pos) is often suggested as a platform for implementing the optical network: Logically it ooers an all-to-all broadcast capability. We investigate the use of pos optical technology as the communication medium for parallel computing. In particular, a feature of parallel models which is extremely important for the simplicity of algorithm design and program portability is the scalability property or self-simulation capability. It states that when a computation achieves a certain speedup on a large machine with many processors, then it achieves a similar speedup on any smaller machine (relative to the number of processors). We show that the pos is indeed scalable, namely, we present a randomized algorithm for an n-processor pos that does not assume global knowledge and that simulates a kn-processor pos with a slowdown of O(k + log n). We also analyze direct algorithms, namely, algorithms that send messages directly from the original sender to the nal destination. We prove that k 2 is the exact complexity if we allow the use of the original set of frequencies, and k 3 is an upper bound if the set of frequencies is restricted too. On the way to our self-simulation result we show that the pos computational model is equivalent to the well known crcw collision pram with shared memory of linear size, hence the same results hold for the latter model as well.	algorithm design;central processing unit;computation;computational model;interconnection;parallel computing;parallel random-access machine;passive optical network;randomized algorithm;scalability;shared memory;simulation;speedup;whole earth 'lectronic link	Pascal Berthomé;Torben Hagerup;Ilan Newman;Assaf Schuster	2000	J. Algorithms	10.1006/jagm.1999.1036	shared memory;algorithm design;mathematical optimization;combinatorics;computer science;theoretical computer science;interconnection;distributed computing;upper and lower bounds;algorithm;technology	Theory	10.965015685947483	33.130426008437844	175315
67ed8da362f0b2d003698f0a8ef3521abf1a7ec3	stateless near optimal flow control with poly-logarithmic convergence	multicommodity flow;flow control	We design completely local, stateless, and self-stabilizing flow control mechanism to be executed by “greedy” agents associated with individual flow paths. Our mechanism is very natural and can be described in a single line: If a path has many “congested” edges, decrease the flow on the path by a small multiplicative factor, otherwise increase its flow by a small multiplicative factor. The mechanism does not require any initialization or coordination between the agents. We show that starting from an arbitrary feasible flow, the mechanism always maintains feasibility and reaches, after poly-logarithmic number of rounds, a 1 + approximation of the maximum throughput multicommodity flow. Moreover, the total number of rounds in which the solution is not 1 + approximate is also poly-logarithmic. Previous distributed solutions in our model either required a state since they used a primal-dual approach or had very slow (polynomial) convergence.	approximation algorithm;coefficient;flow control (data);greedy algorithm;maximum flow problem;maximum throughput scheduling;polynomial;self-stabilization;stateless protocol	Baruch Awerbuch;Rohit Khandekar	2008		10.1007/978-3-540-78773-0_50	multi-commodity flow problem;computer science;flow control;mathematics;computer network	Theory	18.258067460721474	36.149094417339775	175374
8e4305e2d370e998e7a8d46f887f0e5c6afd73c1	a hardware-based predictive motion estimation algorithm	low power hardware solutions;cmos technology;full search;hardware software codesign;hardware complexity;clocks;prediction algorithms;motion estimation prediction algorithms hardware energy consumption partitioning algorithms memory architecture algorithm design and analysis cmos technology streaming media clocks;motion estimation;predictors;variable block size;integrated circuit design motion estimation memory architecture power consumption prediction theory hardware software codesign video coding search problems cmos digital integrated circuits;video coding;integrated circuit design;low power;prediction theory;cmos digital integrated circuits;hardware based predictive motion estimation algorithm;streaming media;energy consumption;memory architecture;0 18 micron;search patterns;h 264;7 61 mhz;search problems;power consumption;7 61 mhz hardware based predictive motion estimation algorithm low power hardware solutions h 264 search patterns predictors memory architecture power consumption hardware complexity cmos technology 0 18 micron;algorithm design and analysis;is research;partitioning algorithms;hardware	For low-power motion estimation hardware solutions, advanced hardware aimed algorithms and architectures are a necessity. A hardware solution for the modern emerging standard H.264 is investigated. Design trade-offs, including search patterns, predictors, and memory architecture, have been made with power consumption and hardware complexity in mind. The memory architecture has been optimised for the variable block sizes used in H.264 motion estimation. All predictors are refined with the small search pattern. An average PSNR of -0.0438 dB compared to the full-search algorithm was simulated. A 2.793 mm/sup 2/ layout, using 0.18 /spl mu/m CMOS technology, of the design running at 7.61 MHz is presented.	best, worst and average case;block size (cryptography);cmos;clock rate;h.264/mpeg-4 avc;image quality;low-power broadcasting;memory bandwidth;motion estimation;peak signal-to-noise ratio;search algorithm	Saku Hamalainen;Lauri Koskinen;Kari Halonen	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1466035	embedded system;algorithm design;computer vision;electronic engineering;real-time computing;prediction;computer science;theoretical computer science;motion estimation;cmos;integrated circuit design	Arch	12.720669152397624	40.88964293105414	175460
f5e8298aabaab3805a72b8a9c7ab9ab090ad8ceb	a design of ternary logic circuits using m-nand and not gates		Abstract#R##N##R##N#We have proposed the canonical form for the p-valued logical function (p is a natural number, p > 2), using M-AND, M-OR and NOT operations. However, the relations among those operations have been discussed only briefly. This paper discusses in detail the properties of M-NAND, M-OR and NOT operations for the ternary case (p = 3), deriving another new canonical form called M-disjunctive canonical form. Then M-NAND operation is defined, and a realization of the operation by electronic circuit is presented. The proposed circuit is the circuit by MOSFET, and can realize, in addition to M-NAND operation, two other logical functions. By using these three kinds of logical functions, the logical circuit can be simplified. The circuit is called M-NAND gate in this paper. It is possible to construct any ternary logical circuit using only M-NAND gates. However, the total number of elements can be reduced by combining the proposed circuit with NOT gates by Mouftah and Jordan. Based on these properties and the canonical form, the construction of the logical circuit using M-NAND and NOT gates is discussed. The circuit has properties which are similar to the well-known operations such as AND, OR, NOT and NAND, and the construction will be easy for those familiar with such operations.	logic gate;three-valued logic	Akio Odaka;Kunio Satoh	1985	Systems and Computers in Japan	10.1002/scj.4690160608	equivalent circuit;discrete mathematics;nor logic;theoretical computer science;truth table;mathematics;circuit extraction;register-transfer level;algorithm;tc0	EDA	19.16876654120184	45.19495427913314	175831
2737c04f3ebcfdfbdf90e64b2133ed9fac368155	dynamic memory model based optimization of scalar and vector quantizer for fast image encoding	optimisation;image coding;data compression;heterogeneous computing;data compression image coding optimisation vector quantisation;fast imaging;vector quantizer encoder dynamic memory model based optimization optimal computation memory tradeoff scalar quantizer encoder fast image encoding heterogeneous computing environment signal processing algorithms encoding time minimisation formal machine dependent optimizations;vector quantizer;vector quantisation;image coding signal processing algorithms pervasive computing encoding quantization costs computational modeling high level languages algorithm design and analysis information retrieval;memory model	The rapid progress of computers and today’s heterogeneous computing environment means computation-intensive signal processing algorithms must be optimized for performance in a machine dependent fashion. In this paper, we present formal machinedependent optimizations of scalar and vector quantizer encoders. Using a dynamic memory model, the optimal computation-memory tradeoff is exploited to minimize the encoding time. Experiments show marked improvements over existing techniques.	algorithm;computation;computer;encoder;heterogeneous computing;machine-dependent software;memory management;quantization (signal processing);scalar processor;signal processing	Gene Cheung;Steven McCanne	2000		10.1109/ICIP.2000.899306	data compression;memory model;parallel computing;computer science;theoretical computer science;machine learning;symmetric multiprocessor system	HPC	10.813041289571027	39.21047311801801	176146
aa0fc9b006e2964e0bdd72f61b0d5566aa28010b	frame buffer-less stream processor for accurate real-time interest point detection	hw accelerator;image elaboration;interest point detection;field programmable gate arrays	A high performance HW accelerator is proposed to extract and refine the Interest Points from images, by accurately calculating the Difference-of-Gaussian and using refinement algorithms from the SIFT method. Unique features of the accelerator consist in an accuracy comparable to the CDVS Test Model, reference software; in the capability to process the incoming pixel in streaming order to minimize the amount of embedded memory and avoid external frame buffers; in the possibility to configure the processor with different area/speed ratios. FPGA synthesis on a Xilinx XC7V2000T returns a maximum operation frequency up of 309 MHz at the fastest corner. Standard cell synthesis with the STMICROELECTRONICS FDSOI 28 nm technology, de-congestioned by the use of DPREG memories in place of SRAM, gives a maximum frequency of 1.2 GHz and a power dissipation of about 1 W at the typical conditions. & 2016 Elsevier B.V. All rights reserved.	algorithm;cpu power dissipation;difference of gaussians;embedded system;fastest;field-programmable gate array;framebuffer;handheld game console;interest point detection;maxima and minima;mobile device;moving picture experts group;performance;pixel;real-time clock;refinement (computing);scale space;scale-invariant feature transform;standard cell;standard-definition television;static random-access memory;stream processing	Gian Domenico Licciardo;Thomas Boesch;Danilo Pau;Luigi Di Benedetto	2016	Integration	10.1016/j.vlsi.2015.12.010	embedded system;real-time computing;computer hardware;telecommunications;computer science;electrical engineering;operating system;interest point detection;field-programmable gate array	EDA	13.814721268972757	40.741194915599195	176235
976ee7c050e66d5685d769ac9e63af6027f993f4	a novel fpga logic block for improved arithmetic performance	6 2 compressor;carry chain;fpga;global routing;arithmetic circuits;design;circuits;compressor tree;multiplication;multi operand addition;arithmetic circuit	To improve FPGA performance for arithmetic circuits, this paper proposes a new architecture for FPGA logic cells that includes a 6:2 compressor. The new cell features additional fast carry-chains that concatenate adjacent compressors and can be routed locally without the global routing network. Unlike previous carry-chains for binary and ternary addition, the carry chain used by the new cell only spans 2 logic blocks, which significantly improves the delay of multi-input addition operations mapped onto the FPGA. The delay and area overhead that arises from augmenting a traditional FPGA logic cell with the new compressor structure is minimal. Using this new cell, we observed an average speedup in combinational delay of 1.41x compared to adder trees synthesized using ternary adders	adder (electronics);bitwise operation;combinational logic;concatenation;field-programmable gate array;logic block;overhead (computing);routing;speedup	Hadi Parandeh-Afshar;Philip Brisk;Paolo Ienne	2008		10.1145/1344671.1344698	embedded system;design;parallel computing;computer science;theoretical computer science;multiplication	EDA	12.56854161546708	46.19174749012152	176608
70bf571ca3116725cadc127d4fd43dd2bc6f9911	diagnosability and diagnosis of algorithm-based fault-tolerant systems	tolerancia falta;parallelisme;detection erreur;deteccion error;fiabilidad;reliability;concurrent error detection;multiprocessor systems;algorithm based fault tolerance;fault diagnosis fault tolerance signal processing algorithms computer architecture parallel processing computer applications concurrent computing throughput fault tolerant systems fault detection;diagnostico;diagnosability;parallelism;fault tolerant computing;paralelismo;signal processing;fiabilite;fault tolerance;parallel processing fault tolerant computing;algorithme base tolerance faute;faulty processors;error detection;algorithm based fault tolerant systems;high throughput;algorithme based fault tolerance;diagnosis;fault location scheme;high performance;system level diagnosis diagnosability parallel processing architectures diagnosis algorithm based fault tolerant systems signal processing faulty processors concurrent error detection fault location scheme multiprocessor systems;real time application;tolerance faute;parallel processing;fault diagnosis;parallel processing architectures;diagnostic;fault location;system level diagnosis	Parallel processing architectures are commonly used for signal processing and other computationally intensive applications. These applications are characterized by high throughput and long processing periods. Such characteristics decrease the reliability of high-performance architectures. The erroneous data produced by faulty processors could have damaging consequences, particularly in critical real-time applications. It is therefore desirable that any erroneous data produced by the system be detected and located as quickly as possible. Algorithm-based fault tolerance (ABFT) is a low-cost system-level concurrent error detection and fault location scheme. Methods used in the analysis of multiprocessor systems using system-level diagnosis are applied to the analysis of ABFT systems. A new algorithm for analyzing an ABFT system for its fault diagnosability is developed using these methods. Based on this work, a fault diagnosis algorithm is developed for ABFT systems. >	algorithm	Bapiraju Vinnakota;Niraj K. Jha	1993	IEEE Trans. Computers	10.1109/12.238483	high-throughput screening;parallel processing;fault tolerance;parallel computing;real-time computing;error detection and correction;computer science;signal processing;reliability;distributed computing;statistics	Embedded	23.05976476645909	44.43874535773801	176770
e5811971e3d15f99c57ea7b8f305daaa430c5719	the cost of data dependence in motion vector estimation for reconfigurable platforms	reconfigurable platforms;video applications data dependence cost motion vector estimation reconfigurable platforms temporal redundancy;full search;data reuse;motion estimation;video applications;estimation algorithm;temporal redundancy;memory access;data dependence;motion vector;data dependence cost;motion estimation field programmable gate arrays;field programmable gate arrays;costs motion estimation hardware field programmable gate arrays educational institutions algorithm design and analysis embedded system data engineering communication standards quality of service;motion vector estimation	Motion vector estimation is frequently performed as a prelude to the exploitation of temporal redundancies in video applications. As a result, a large volume of work has been done to develop techniques to avoid the heavy memory access requirements of full search motion vector estimation. Often, these approaches introduce data dependence to the algorithm, leading to memory accesses which cannot be determined at design time. Consequently, this complicates the exploitation of data reuse in hardware. In this work, the cost of data dependence is quantified. Experiments indicate that a data dependent fast motion vector estimation approach is faster than full search by up to 47% in the absence of data re-use optimisation. However, full search is approximately 16 times faster than the `fast' motion vector estimation algorithm when a static line buffering scheme and a parallel caching scheme are used respectively to exploit data re-use. Therefore, it is established that data dependence in motion vector estimation is very expensive in terms of hardware performance	algorithm;clock rate;data dependency;elegant degradation;mathematical optimization;memory management;requirement	Su-Shin Ang;George A. Constantinides;Wayne Luk;Peter Y. K. Cheung	2006	2006 IEEE International Conference on Field Programmable Technology	10.1109/FPT.2006.270341	embedded system;real-time computing;quarter-pixel motion;computer science;theoretical computer science;operating system;motion estimation;field-programmable gate array	Visualization	12.494188141490486	39.980874375100825	176822
02692827979ffa7bed93c5fba00c1adbba1492eb	a radix-4 redundant cordic algorithm with fast on-line variable scale factor compensation	digital signal processing;art;laboratories hardware very large scale integration digital signal processing costs art;very large scale integration;transforms digital arithmetic;scale factor decomposition algorithm;rotation number;scale factor compensations;decomposition algorithm;rotation iterations;scale factor compensations radix 4 redundant cordic algorithm fast online variable scale factor compensation scale factor decomposition algorithm shift and add operations rotation number rotation iterations;transforms;shift and add operations;digital arithmetic;high speed;radix 4 redundant cordic algorithm;fast online variable scale factor compensation;hardware	In this work, a fast radix-4 redundant CORDIC algorithm with variable scale factor is proposed. The algorithm includes an on-line scale factor decomposition algorithm that transforms the complicated variable scale factor into a sequence of simple shift-and-add operations and does the variable scale factor compensation in the same fashion. On the other hand, the on-line decomposition algorithm itself can be realized with a simple and fast hardware. The new CORDIC algorithm has the smallest number of 0.8n iterations among all the CORDIC algorithms, which requires only about two-third rotation number that of the existing best (hybrid radix-2 and radix-4) redundant algorithms. Therefore, the new algorithm achieves fast rotation iterations, high-speed and low-overhead scale factor compensations, which are hard to attain simultaneously for the existing algorithms. The on-line scale factor compensation can be also applied to the existing on-line CORDIC algorithms.	bitap algorithm;cordic;iteration;online and offline;overhead (computing);rotation number	Chieh-Chih Li;Sau-Gee Chen	1997		10.1109/ICASSP.1997.599849	parallel computing;rotation number;computer science;theoretical computer science;digital signal processing;mathematics;very-large-scale integration	Vision	12.861693635668129	44.256370607249124	177239
7b23c1ba508b91b0880580e657dc082277653af8	runtime reconfigurable dsp unit using one's complement and minimum signed digit	multiplying circuits;digital signal processing field programmable gate arrays adders table lookup multiplexing runtime hardware;deterministic algorithms;word length 32 bit runtime reconfigurable dsp unit one complement runtime reconfigurable digital signal processing unit minimum signed digit multiplier msd multiplier shift and add subtract operations carry save architecture propagation delay carry out bits deterministic dsp algorithm xilinx field programmable gate arrays look up tables lut fpga;table lookup deterministic algorithms digital signal processing chips field programmable gate arrays multiplying circuits;digital signal processing chips;field programmable gate arrays;table lookup	A runtime reconfigurable Digital Signal Processing (DSP) unit using one's complement data and a Minimum Signed Digit (MSD) multiplier is described. The MSD multiplier changes the partial product shift-and-add operations to shift-and-add/subtract operations and reduces the number of partial product terms by half, decreasing the size and increasing the speed of the multiplier. Using a carry-save architecture, the propagation delay of an adder to add the carry-out bits is eliminated. Taking advantage of the deterministic nature of the desired DSP algorithm, the runtime reconfiguration of the multiplier can be pipelined to eliminate an added set-up state. The described DSP unit compared to a DSP unit using a conventional multiplier is shown to be 20% faster and 30% smaller for a 32-bit coefficient and using Xilinx Field Programmable Gate Arrays (FPGAs) with 6-input Look-Up Tables (LUTs).	32-bit;adder (electronics);bitap algorithm;coefficient;digital signal processing;field-programmable gate array;ones' complement;propagation delay;software propagation	Travis Manderson;Laurence Turner	2012	22nd International Conference on Field Programmable Logic and Applications (FPL)	10.1109/FPL.2012.6339160	embedded system;parallel computing;computer hardware;computer science;field-programmable gate array	EDA	12.095049023636767	44.858944979080526	177375
2fb015fd165a1a47549f95d7ca6904cee7387be9	deterministic communication in radio networks with large labels	radio networks;reseau communication;temps lineaire;telecommunication network;ad hoc network;algorithme deterministe;diffusion telecommunications;tiempo lineal;approche deterministe;deterministic approach;aleatorizacion;deterministic algorithms;reseau radio;red telecomunicacion;linear time;diffusion donnee;enfoque determinista;reseau telecommunication;difusion dato;randomisation;radio communication;radiocommunication;data broadcast;broadcasting;randomization;red de comunicacion;communication network;radiocomunicacion	We study deterministic gossiping in ad-hoc radio networks, where labels of the nodes are large, i.e., they are polynomially large in the size n of the network. A label-free model was introduced in the context of randomized broadcasting in ad-hoc radio networks, see [2]. Most of the work on deterministic communication in ad-hoc networks was done for the model with labels of size O(n), with few exceptions; Peleg [19] raised the problem of deterministic communication in ad-hoc radio networks with large labels and proposed the first deterministic O(n2 log n)- time broadcasting algorithm. In [11] Chrobak et al. proved that deterministic radio broadcasting can be performed in time O(n log2 n); their result holds for large labels.Here we propose two new deterministic gossiping algorithms for ad-hoc radio networks with large labels. In particular: - a communication procedure giving an O(n5/3 log3 n)-time deterministic gossiping algorithm for directed networks and an O(n4/3 log3 n)- time algorithm for undirected networks; - a gossiping procedure designed particularly for undirected networks resulting in an almost linear O(n log3 n)-time algorithm.		Leszek Gasieniec;Aris Pagourtzis;Igor Potapov	2002		10.1007/3-540-45749-6_46	telecommunications;computer science;deterministic algorithm;distributed computing;algorithm;telecommunications network	DB	18.353891687682125	32.677950244359344	177472
73718dad0af8d37651d0222d10f6e53621b2df01	pipelined architectures for transform domain lms adaptive filtering	delayed adaptation;convergence analysis;adaptive filtering;pipelined implementation;transform domain lms algorithm;adaptive filter	In this paper, efficient pipelined architectures for the implementation of the Transform Domain LMS (TD-LMS) adaptive filter are considered. Pipelining of the TD-LMS algorithm is achieved by introducing an amount of time delay into the original adaptive scheme. The resulting algorithm, called thereafter the delayed TD-LMS, performs adaptive filtering with delayed coefficients adaptation. A statistical performance analysis of the proposed delayed TD-LMS adaptive algorithm is presented, both for the mean error and the mean squared error of the filter coefficients. A closed form expression is derived for the estimation of the steady state excess mean squared error, in terms of the adaptation delay and the input signal characteristics. The adaptation delay introduced to the delayed TD-LMS algorithm is subsequently utilized for the development of pipelined architectures. By retiming the delays existing in the error signal feedback loop, efficient pipelined implementations of the delayed TD-LMS algorithm are developed. The proposed architectures are suitable for parallel implementation on a general-purpose parallel machine, or on dedicated hardware, integrated on ASIC or ASIP VLSI processors.	adaptive filter	George-Othon Glentis	2005	Journal of Circuits, Systems, and Computers	10.1142/S0218126605002519	adaptive filter;computer vision;electronic engineering;real-time computing;kernel adaptive filter;computer science;theoretical computer science	Arch	12.165545159814588	45.45258324342257	177647
25cf3d5b6341451d046bce21fa092e89da704640	disaster management in scale-free networks: recovery from and protection against intentional attacks	ad hoc networks;power law;com- pensation;preferential deletion;scale-free;peer-to-peerp2p;attacks;growing networks;disaster management;statistical mechanics;data analysis;scale free;maximum degree;data structure;degree distribution;p2p;connected component;scale free network;ad hoc network	Susceptibility of scale free Power Law (PL) networks to attacks has been traditionally studied in the context of what may be termed as instantaneous attacks, where a randomly selected set of nodes and edges are deleted while the network is kept static. In this paper, we shift the focus to the study of progressive and instantaneous attacks on reactive grown and random PL networks, which can respond to attacks and take remedial steps. In the process, we present several techniques that managed networks can adopt to minimize the damages during attacks, and also to efficiently recover from the aftermath of successful attacks. For example, we present (i) compensatory dynamics that minimize the damages inflicted by targeted progressive attacks, such as linear-preferential deletions of nodes in grown PL networks; the resulting dynamic naturally leads to the emergence of networks with PL degree distributions with exponential cutoffs; (ii) distributed healing algorithms that can scale the maximum degree of nodes in a PL network using only local decisions, and (iii) efficient means of creating giant connected components in a PL network that has been fragmented by attacks on a large number of high-degree nodes. Such targeted attacks are considered to be a major vulnerability of PL networks; however, our results show that the introduction of only a small number of random edges, through a reverse percolation process, can restore connectivity, which in turn allows restoration of other topological properties of the original network. Thus, the scale-free nature of the networks can itself be effectively utilized for protection and recovery purposes.	algorithm;apollonian network;circuit restoration;connected component (graph theory);emergence;half-life 2: episode one;percolation;randomness;time complexity	Behnam Attaran Rezaei;Nima Sarshar;P. Oscar Boykin;Vwani P. Roychowdhury	2005	CoRR			ECom	23.561549580911368	38.20461010957336	178017
e338102ba704c04d40bc2afadb27abb11f2277b2	pipeline architectures for dynamic programming algorithms	parallel calculus;dynamic programming;optimisation;programacion dinamica;probleme sac a dos;optimizacion;probleme np complet;dynamic programming algorithm;problema mochila;operations research;computer architecture;probleme combinatoire;calculo paralelo;problema combinatorio;architecture ordinateur;programmation dynamique;procesador oleoducto;problema np completo;optimization;arquitectura ordenador;combinatory problem;knapsach problem;processeur pipeline;calcul parallele;pipeline processor;np complete problem;pipeline processing;parallel algorithms	Abstract   Dynamic programming is one of the most powerful approaches to many combinatorial optimization problems. In this paper, we present pipeline architectures for the dynamic programming algorithms for the knapsack problems. They enable us to achieve an optimal speedup using processor arrays, queues, and memory modules. The processor arrays can be regarded as pipelines where the dynamic programming algorithms are implemented through pipelining.	algorithm;dynamic programming	Gen-Huey Chen;Maw-Sheng Chern;Jin Hwang Jang	1990	Parallel Computing	10.1016/0167-8191(90)90124-R	parallel computing;reactive programming;functional reactive programming;computer science;theoretical computer science;dynamic programming;inductive programming;algorithm	HPC	11.689523945652988	34.265815900758675	178318
3d175b6919a8546867ee6a9d6b16b04a5faab913	regularization of hierarchical vhdl-ams models using bipartite graphs	graph theory;complex continuous systems;hierarchical vhdl ams models;mathematics;vhdl ams;daes;bipartite graphs;differential algebraic equations;hardware description languages;analog design;continuous system;structural solvability;packaging;regularization;integrated circuit design;circuit simulation;discrete algorithm;permission;error correction;vlsi;mixed analogue digital integrated circuits;differential algebraic equation;differential equations;vlsi hardware description languages circuit simulation differential equations integrated circuit design graph theory mixed analogue digital integrated circuits;bipartite graph algorithm design and analysis numerical simulation permission differential equations power system modeling differential algebraic equations error correction mathematics packaging;power system modeling;bipartite graph;algorithm design and analysis;unsolvable dae systems;numerical simulation;unsolvable dae systems regularization hierarchical vhdl ams models bipartite graphs complex continuous systems analog design differential algebraic equations numerical simulation discrete algorithm	The powerful capability of VHDL-AMS to describe complex continuous systems in form of differential algebraic equations (DAEs) often leads to problems during numerical simulation. This paper presents a discrete algorithm to analyze unsolvable DAE systems and to correct the underlying hierarchical VHDL-AMS description automatically in interaction with the designer, avoiding timeconsuming manual error correction.	algorithm;differential algebraic equation;error detection and correction;numerical analysis;numerical weather prediction;simulation;vhdl;vhdl-ams	Jochen Mades;Manfred Glesner	2002		10.1145/513918.514056	mathematical optimization;combinatorics;electronic engineering;discrete mathematics;bipartite graph;differential algebraic equation;computer science;graph theory;theoretical computer science;mathematics;algorithm	Logic	21.14405256454836	46.08750085186871	178600
608ef8673c8bf0ebfd236eb3f6e66a0f21ccd428	a gpu implementation of bulk execution of the dynamic programming for the optimal polygon triangulation		The optimal polygon triangulation problem for a convex polygon is an optimization problem to find a triangulation with minimum total weight. It is known that this problem can be solved using the dynamic programming technique in (O(n^3)) time. The main contribution of this paper is to present an efficient parallel implementation of this (O(n^3))-time algorithm for a lot of instances on the GPU (Graphics Processing Unit). In our proposed GPU implementation, we focused on the computation for a lot of instances and considered programming issues of the GPU architecture such as coalesced access of the global memory, warp divergence. Our implementation solves the optimal polygon triangulation problem for 1024 convex 1024-gons in 4.77 s on the NVIDIA TITAN X, while a conventional CPU implementation runs in 241.53 s. Thus, our GPU implementation attains a speedup factor of 50.6.	dynamic programming;graphics processing unit;polygon triangulation	Kohei Yamashita;Yasuaki Ito;Koji Nakano	2017		10.1007/978-3-319-78024-5_28	parallel computing;architecture;regular polygon;dynamic programming;speedup;general-purpose computing on graphics processing units;polygon triangulation;triangulation (social science);computer science;convex polygon	PL	10.117657180067459	32.86111638376845	178746
b75a5683c5e8bdd7fd6fe2491b4087dad644ae78	a cost-effective 2-d discrete cosine transform processor with reconfigurable datapath	reconfigurable datapath processor;cmos technology;logic design;reconfigurable architectures;very large scale integration;2d discrete cosine transform processor;transform coding;multiplexing;discrete cosine transform;integrated circuit design;0 35 micron cost effective dct processor 2d discrete cosine transform processor reconfigurable datapath processor multiplexers computational complexity reduction processor operating block size vlsi implementation cmos 100 mhz;cost effective dct processor;matrix decomposition;computational complexity;discrete cosine transforms;cmos logic circuits;fourier transforms;0 35 micron;vlsi;cost effectiveness;100 mhz;vlsi implementation;processor operating block size;discrete fourier transforms;computational complexity reduction;multiplexers;discrete cosine transforms multiplexing discrete fourier transforms hardware matrix decomposition very large scale integration costs cmos technology transform coding fourier transforms;cmos;direct method;hardware;cmos logic circuits discrete cosine transforms logic design integrated circuit design reconfigurable architectures vlsi	In this paper, a cost-effective 2D discrete cosine transform processor using a reconfigurable datapath is described. The proposed architecture uses some multiplexers to reduce computational complexity. This processor operates on 8/spl times/8 blocks. Unlike other direct methods, the proposed architecture is regular for VLSI implementation. The proposed 2D DCT processor costs 38598 transistors, with an operating frequency of 100 MHz, using 0.35 /spl mu/m CMOS technology.	datapath;discrete cosine transform	Yeong-Kang Lai;Han-Jen Hsu	2003		10.1109/ISCAS.2003.1206018	computer architecture;electronic engineering;parallel computing;computer science;electrical engineering;very-large-scale integration;cmos;algorithm	EDA	12.413919995267216	43.947406188500544	178766
3dea94da1fcfefc10972bf4b54e72061d90b9c6f	error-tolerant fir filters based on low-cost residue codes	8 bit arithmetic codes cmos error tolerant fir filters low cost residue codes error tolerant finite impulse response filter low power fir filters modulo bitplane structure single error correction multiple tap programmable fir filter 6 bit;digital signal processing;error correction codes;finite impulse response filter;6 bit;error tolerant finite impulse response filter;8 bit;finite impulse response filter error correction codes arithmetic digital signal processing computer errors throughput fault tolerance redundancy delay systems engineering and theory;programmable filters arithmetic codes error correction codes fir filters residue codes;systems engineering and theory;finite impulse response;arithmetic codes;low power;low cost residue codes;redundancy;error tolerant fir filters;error correction;fir filter;fault tolerance;multiple tap programmable fir filter;programmable filters;arithmetic;low power fir filters;fir filters;residue codes;single error correction;modulo bitplane structure;high performance;cmos;computer errors;throughput	A new error-tolerant finite impulse response (FIR) filter architecture for low-power high-performance applications is presented. The combination of low-cost residue coding and a novel structure, called modulo bitplane, proves to be very efficient Results for single-error correction of a typical 12-tap programmable FIR filter with 6-bit sample data and 8-bit coefficients show that it exhibits just 85% area overhead.	8-bit;code;coefficient;error detection and correction;error-tolerant design;finite impulse response;low-power broadcasting;modulo operation;overhead (computing)	José L. Rodríguez-Navarro;Michael Gansen;Tobias G. Noll	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1465809	electronic engineering;real-time computing;computer science;theoretical computer science;finite impulse response	Arch	13.770897956234297	44.860867423290436	178768
4f6070c37017723cd7ce4e134ae66718ece23ae3	fast vlsi algorithms for division and square root	digital signal processing;system of equations;high performance;vlsi architecture	Real time digital signal processing demands high performance implementations of division and square root. This can only be achieved by the design of fast and efficient arithmetic algorithms which address practical VLSI architectural design issues. In this paper, new algorithms for division and square root are described. The new schemes are based on pre-scaling the operands and modifying the classical SRT method such that the result digits and the remainders are computed concurrently and the computations in adjacent rows are overlapped. Consequently, their performance exceeds that of the SRT methods. The hardware cost for higher radices is considerably more than that of the SRT methods but for many applications, this is not prohibitive. A system of equations is presented which enables both an analysis of the method for any radix and the parameters of implementations to be easily determined. This is illustrated for the case of radix 2 and radix 4. In addition, a highly regular array architecture combining the division and square root method is described.	algorithm;very-large-scale integration	Stephen E. McQuillan;John V. McCanny	1994	VLSI Signal Processing	10.1007/BF02109382	arithmetic;system of linear equations;parallel computing;computer science;digital signal processing	Theory	11.829208654690104	44.202408318484935	179046
bd7312d9ec4a728c2287ddc32ff80d181df4caea	parallel split-level relaxation	topology;hydrogen;parallel algorithms computerised pattern recognition computerised picture processing;computerised pattern recognition;layout;labeling layout parallel processing hydrogen computer science cities and towns image analysis topology;computerised picture processing;cities and towns;image analysis;computer science;multiple queues parallel algorithms computerised picture processing computerised pattern recognition split level relaxation technique parallel processing;parallel processing;labeling;multiple queues;split level relaxation technique;parallel algorithms	The goal of the scene labeling problem is t o identify a set of regions in a given image. There are several approaches t o solve this problem, including backtracking, graph matching, etc. A new method called split-level relaxation based on discrete relaxation was proposed in [3]. I t takes care of multiple semantic constraints, by considering each of them independently. The problem is NP-complete, so it takes a long time to solve this problem. With the advent of multiprocessors, i t is now imperative t o see if the problem can be solved faster in the average case. We give a framework for solving the problem in a parallel processing environment, using split-level relaxation. Experiments done on a multiprocessor show that indeed i t is advantageous to use them to solve this problem. The results are also presented.	backtracking;best, worst and average case;care-of address;imperative programming;linear programming relaxation;matching (graph theory);multiprocessing;np-completeness;parallel computing	Thomas C. Henderson;Ashok Samal	1988		10.1109/ICPR.1988.28210	layout;parallel processing;computer vision;labeling theory;parallel computing;hydrogen;image analysis;computer science;theoretical computer science;distributed computing;parallel algorithm	Vision	14.338168292043592	34.12063568651256	179061
9596a52f6bd90e0ffb61b6c4c73df0bb28f99792	cabac accelerator architectures for video compression in future multimedia: a survey	comparative analysis;real time;h 264 avc;accelerators;video compression;hardware accelerator;cabac;multimedia processing;multimedia systems;hardware architecture;video coding;high performance computer;uhdtv;context based adaptive binary arithmetic coding;rc hardware architectures	The demands for high quality, real-time performance and multi-format video support in consumer multimedia products are ever increasing. In particular, the future multimedia systems require efficient video coding algorithms and corresponding adaptive high-performance computational platforms. The H.264/AVC video coding algorithms provide high enough compression efficiency to be utilized in these systems, and multimedia processors are able to provide the required adaptability, but the algorithms complexity demands for more efficient computing platforms. Heterogeneous (re-)configurable systems composed of multimedia processors and hardware accelerators constitute the main part of such platforms. In this paper, we survey the hardware accelerator architectures for Context-based Adaptive Binary Arithmetic Coding (CABAC) of Main and High profiles of H.264/AVC. The purpose of the survey is to deliver a critical insight in the proposed solutions, and this way facilitate further research on accelerator architectures, architecture development methods and supporting EDA tools. The architectures are analyzed, classified and compared based on the core hardware acceleration concepts, algorithmic characteristics, video resolution support and performance parameters, and some promising design directions are discussed. The comparative analysis shows that the parallel pipeline accelerator architecture seems to be the most promising.	algorithm;binary number;central processing unit;computation;context-adaptive binary arithmetic coding;data compression;electronic design automation;h.264/mpeg-4 avc;hardware acceleration;image resolution;performance;qualitative comparative analysis;real-time clock;real-time computing;real-time transcription;requirement;throughput;video processing	Yahya Jan;Lech Józwiak	2009		10.1007/978-3-642-03138-0_4	data compression;qualitative comparative analysis;embedded system;parallel computing;real-time computing;hardware acceleration;computer science;theoretical computer science;hardware architecture;context-adaptive binary arithmetic coding	Arch	11.597870700869017	40.32069874800752	179121
8890486679c76032bc1158a3e2e92ffa16365283	the edge product of networks	myrinet networks;algorithm design and analysis ethernet networks clustering algorithms concurrent computing size measurement computer networks central processing unit message passing distributed computing application software;cpu algorithm selection collective communication mpich message passing interface ethernet myrinet networks;collective communication;ethernet;algorithm selection;message passing application program interfaces local area networks;message passing interface;application program interfaces;message passing;mpich;cpu;local area networks	In this paper, a new graph product, called Edge Graph Product (EGP) is proposed by replacing each edge in the multiplicand graph by a copy of the multiplier graph via two candidate nodes. The edge product, unlike other products already proposed, results in a graph whose number of edges is numerical product of the number of the edges in the multiplicand and multiplier graphs, and the number of vertices is not equal to the numerical product of the number of vertices in the multiplicand and multiplier graphs. After formal definition of the new product, some basic properties of the product operator are studied. We then address Hamiltonian, Eulerian and routing properties of the new product, and we show that some of the recently proposed topologies fall within the family of edge product graphs.	file spanning;graph product;hamiltonian (quantum mechanics);numerical analysis;routing;spanning tree;vertex (geometry);very-large-scale integration	Ali Jalali;Hamid Sarbazi-Azad	2007	Eighth International Conference on Parallel and Distributed Computing, Applications and Technologies (PDCAT 2007)	10.1109/PDCAT.2007.20	local area network;parallel computing;message passing;real-time computing;computer science;message passing interface;operating system;central processing unit;distributed computing;ethernet;computer network	HPC	22.278069725517756	36.327143180950024	179189
e818a76a15fb30965ef0113fe70cf3488f71f322	a reconfigurable hevc sub-pixel interpolation hardware	interpolation;tk7800 8360 electronics;video coding;interpolation hardware finite impulse response filters power demand decoding standards video coding;64 quad full hd video frames reconfigurable hevc subpixel interpolation hardware high efficiency video coding video encoder video decoder half pixel interpolation hardware quarter pixel interpolation hardware power consumption;video codecs;video coding interpolation video codecs;tk7885 7895 computer engineering computer hardware;fpga hevc sub pixel interpolation hardware implementation	Sub-pixel interpolation is one of the most computationally intensive parts of High Efficiency Video Coding (HEVC) video encoder and decoder. Therefore, in this paper, a reconfigurable HEVC sub-pixel (half-pixel and quarter-pixel) interpolation hardware for all prediction unit sizes is proposed. The proposed reconfigurability reduces the area and power consumption of HEVC sub-pixel interpolation hardware more than 30%. The proposed hardware, in the worst case, can process 64 quad full HD (2560×1600) video frames per second.	best, worst and average case;encoder;frame (video);graphics display resolution;high efficiency video coding;interpolation;pixel;reconfigurability	Ercan Kalali;Yusuf Adibelli;Ilker Hamzaoglu	2013	2013 IEEE Third International Conference on Consumer Electronics ¿ Berlin (ICCE-Berlin)	10.1109/ICCE-Berlin.2013.6698023	electronic engineering;real-time computing;computer science;video post-processing;h.261;multiview video coding;image scaling;computer graphics (images)	Vision	12.317018979486544	40.871851707668604	179414
b3d71b6146dbc4e41ada9d0ba6442dbb5af8536a	design methods for binary to decimal converters using arithmetic decompositions	digital signal processing;design technique;design method;weighted sums;high speed	In digital signal processing, radixes other than two are often used for high-speed computation. In the computation for finance, decimal numbers are used instead of binary numbers. In such cases, radix converters are necessary. This paper considers design methods for binary to q-nary converters. It introduces a new design technique based on weighted-sum (WS) functions. The method computes a WS function for each digit by an LUT cascade and a binary adder, then adds adjacent digits with q-nary adders. A 16-bit binary to decimal converter is designed to show the method.	16-bit;adder (electronics);analog-to-digital converter;binary number;computation;digital signal processing;radix point	Yukihiro Iguchi;Tsutomu Sasao;Munehiro Matsuura	2007	Multiple-Valued Logic and Soft Computing		arithmetic;digit sum;design methods;computer science;theoretical computer science;digital signal processing;mathematics;decimal floating point;binary integer decimal	EDA	16.061268315992503	44.2092174931563	179760
65d716d913523c8adccadf3edc351d8c1b9d782f	an efficient cgm-based parallel algorithm solving the matrix chain ordering problem	dynamic programming;pram;bulk synchronous parallel;coarse grain multicomputer;parallel processing	This study focuses on the parallel resolution of the matrix chain ordering problem and the optimal convex polygon triangulation problem on the Coarse grain multicomputer model (CGM for short). There has been intensive work on the parallelization of these dynamic programming problems in PRAM, including the use of systolic arrays, but a BSP/CGM solution is necessary for ease of implementation and portability. Our CGM algorithm is based on Yao’s sequential solution running in O(n2) time and O(n2) space. This CGM algorithm uses p processors, each with O(n/p) local memory. It requires at most O(S/p×n2) running time with S communication rounds and with S/p<1. Our algorithm performs better than the algorithm proposed in 2012 by Dilson and Marco when S is less than n/p. We offer several ways of partitioning the problem to solve and study the impact of each partitioning algorithm performance. A CGM solution exists based on Yao’s algorithm, but the subdivision of tasks is defined according to the BSP cost model. In this paper, we propose a solution based only on the CGM model specifications. Note that S is the number of super-steps of the CGM algorithm. An Efficient CGM-Based Parallel Algorithm Solving the Matrix Chain Ordering Problem	analysis of algorithms;central processing unit;dynamic programming;matrix chain multiplication;parallel algorithm;parallel computing;polygon triangulation;software portability;subdivision surface;systolic array;the matrix;time complexity;yao graph	Jean Frédéric Myoupo;Vianney Kengne Tchendji	2014	IJGHPC	10.4018/ijghpc.2014040105	parallel processing;parallel computing;computer science;operating system;dynamic programming;distributed computing;algorithm;bulk synchronous parallel	HPC	12.30069229709239	32.626270893441365	179771
2cc24bda8b1a0f4675bb50c1d7120406531c4d0e	a memory-based architecture for mpeg2 system protocol lsis	outil logiciel;mpeg2 systems;evaluation performance;circuito lsi;array processing;mpeg2 system protocol lsi;protocols;concepcion circuito;demultiplexor;software tool;multiplexor demultiplexor;arquitectura circuito;core central processing unit;hardware software codesign;codecs;performance evaluation;protocol processing;memory based architecture;data compression;integrated circuit;application software;protocols data compression video coding code standards multiplexing equipment demultiplexing equipment large scale integration cmos digital integrated circuits digital signal processing chips hardware software codesign real time systems multimedia communication memory architecture vlsi codecs audio coding;mpeg2 encoded streams;implementation;evaluacion prestacion;demultiplexing;circuit design;circuit vlsi;circuit architecture;circuito integrado;0 5 micron;multiplexing equipment;code standards;tecnologia mos complementario;indexing terms;hardware software codesign techniques;hw sw codesign;design optimization;multiplexing;memory architecture protocols hardware large scale integration computer architecture demultiplexing central processing unit design optimization real time systems application software;embedded system;0 5 micron memory based architecture mpeg2 system protocol lsi multiplexing demultiplexing mpeg2 encoded streams core central processing unit memories dedicated application specific hardware hardware software codesign techniques real time application mpeg2 codec systems multimedia communication cmos embedded gate array process technology;demultiplexeur;mpeg2;demultiplexer;ejecucion;video coding;computer architecture;audio coding;vlsi circuit;lsi circuit;large scale integration;cmos digital integrated circuits;codesign;memory architecture;herramienta controlada por logicial;mpeg2 codec systems;multimedia communication;demultiplexing equipment;architecture circuit;vlsi;multiplexeur;dedicated application specific hardware;codec;multiplexer	This paper proposes a memory-based architecture implementing the MPEG2 system protocol large scale integrations (LSIs), and demonstrates its flexibility and performance. The memory-based architecture implements the full functionality of the MPEG2 system protocol for both multiplexing and demultiplexing MPEG2-encoded streams. It consists of a core central processing unit, memories, and dedicated application-specific hardware. It is designed and optimized by hardware/software codesign techniques. The LSI's provide sufficient performance and flexibility for real-time application of the MPEG2 system protocol. They were fabricated with 0.5 /spl mu/m CMOS embedded gate array process technology. They are now in use on MPEG2 codec systems for several multimedia communication and storage services.	mpeg-2	Minoru Inamori;Jiro Naganuma;Makoto Endo	1999	IEEE Trans. VLSI Syst.	10.1109/92.784095	multiplexer;embedded system;electronic engineering;codec;real-time computing;computer science;operating system;multiplexing	Embedded	10.900594840188711	41.42181468258782	179935
e9f0c15a4db1d6e3eac31e2ca8cc7a03cdb6df87	high-level synthesis implementation of hevc 2-d dct/dst on fpga		This paper presents the first known high-level synthesis (HLS) implementation of integer discrete cosine transform (DCT) and discrete sine transform (DST) for High Efficiency Video Coding (HEVC). The proposed approach implements these 2-D transforms by two successive 1-D transforms using a well-known row-column and Even-Odd decomposition techniques. Altogether, the proposed architecture is composed of a 4-point DCT/DST unit for the smallest transform blocks (TBs), an 8/16/32-point DCT unit for the other TBs, and a transpose memory for intermediate results. On Arria II FPGA, the low-cost variant of the proposed architecture is able to support encoding of 1080p format at 60 fps and at the cost of 10.0 kALUTs and 216 DSP blocks. The respective figures for the proposed high-speed variant are 2160p at 30 fps with 13.9 kALUTs and 344 DSP blocks. These cost-performance characteristics outperform respective non-HLS approaches on FPGA.	4k resolution;discrete cosine transform;discrete sine transform;field-programmable gate array;high efficiency video coding;high- and low-level;high-level synthesis	Panu Sjovall;Vili Viitamaki;Jarno Vanne;Timo Hämäläinen	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952416	field-programmable gate array;parallel computing;lapped transform;discrete cosine transform;architecture;discrete sine transform;transform coding;macroblock;high-level synthesis;computer science	EDA	12.266143517644094	41.410290049149786	180008
2658cf5103b1e8ae43c38df9293ed1bd04df985a	sequential multiplier with sub-linear gate complexity	sequential multiplier;binary polynomial multiplication;horner scheme;sub linear gate complexity	In this article, we present a new sequential multiplier for extended binary finite fields. Like its existing counterparts, the proposed multiplier has a linear complexity in flip-flop or temporary storage requirements, but a sub-linear complexity in gate counts. For the underlying polynomial multiplication, the proposed field multiplier relies on the Horner scheme.	flops;flip-flop (electronics);horner's method;polynomial ring;requirement	M. Anwar Hasan;Christophe Nègre	2012	Journal of Cryptographic Engineering	10.1007/s13389-012-0035-1	discrete mathematics;mathematics;algorithm;algebra	Crypto	10.352540817904107	43.476151799714486	180160
6b2dab252b75fcacc4b86dfcbf7d327fb7467b3f	a mux-based high-performance single-cycle cmos comparator	static priority;digital signal processing;signal generators;test chip measurement;two stage comparator;mux;clocks;post layout simulation;logic;process design;chip;single cycle cmos comparator;comparators circuits;comparator;word length 64 bit;adders;cmos logic circuits;high fan in cmos comparator;digital arithmetic;circuits;static priority encoder;signal processing algorithms;parallel msb checking;high performance;high speed integrated circuits;pipeline processing	In this brief, a new architecture for high-fan-in CMOS comparator is proposed. The architecture is based on a hierarchical two-stage comparator structure and a dynamic MUX is used instead of a comparator in the second stage of the structure. By doing so, the fast dynamic MUX significantly improves the overall delay of the high-fan-in comparators. At the same time, a novel high-performance static priority encoder is proposed to generate the control signal for the MUX. A 64-bit MUX-based comparator has been built and compared with the existing fastest single-cycle design in the study by Lam and Tsui (2006). From both the post-layout simulation and test-chip measurement results, it is shown that the performance is improved by around 28%.	64-bit computing;cmos;comparator;fan-in;fastest;lam/mpi;multiplexer;priority encoder;simulation	Hing Mo Lam;Chi-Ying Tsui	2007	IEEE Transactions on Circuits and Systems II: Express Briefs	10.1109/TCSII.2007.899856	chip;process design;embedded system;electronic engineering;real-time computing;telecommunications;computer science;engineering;electrical engineering;digital signal processing;comparator;logic;comparator applications	EDA	13.154549360135777	46.18483216837284	180203
0937ccb334f86f9fa90a179ce12038855774ac34	algebraic decision trees and euler characteristics	semi algebraic set;algebraic computation tree;decision tree;decision trees integrated circuit modeling testing computational complexity computer science computational geometry marine vehicles computational modeling;membership question;euler characteristics;computational geometry;testing;trees mathematics;closed polyhedron;computational modeling;marine vehicles;algebraic decision trees;computational complexity;decision theory;integrated circuit modeling;euler characteristic;computer science;decision trees;trees mathematics computational geometry decision theory;closed polyhedron algebraic decision trees euler characteristics algebraic computation tree membership question	For any set S contained in R/sup n/, let chi (S) denote its Euler characteristic. The author shows that any algebraic computation tree or fixed-degree algebraic decision tree must have height Omega (log mod chi (S) mod )for deciding the membership question of a compact semi-algebraic set S. This extends a result by A. Bjorner, L. Lovasz and A. Yao where it was shown that any linear decision tree for deciding the membership question of a closed polyhedron S must have height greater than or equal to log/sub 3/ mod chi (S) mod . >	decision tree;euler	Andrew Chi-Chih Yao	1992		10.1109/SFCS.1992.267765	combinatorics;discrete mathematics;computational geometry;computer science;decision tree;mathematics;algorithm;algebra	Theory	20.610504837851824	32.82368045463405	180371
d1424217c4903befd6451f2756c40b4301561d1c	hardware-efficient true motion estimator based on markov random field motion vector correction	random access memory;video coding hardware description languages image resolution markov processes motion estimation program compilers;image resolution;hardware description languages;motion estimation;markov random field;chip;vectors motion estimation random access memory bandwidth computer architecture algorithm design and analysis hardware;video coding;computer architecture;vectors;motion vector;bandwidth;markov processes;program compilers;algorithm design;motion vector grouping techniques hardware efficient true motion estimator markov random field motion vector correction true motion estimation true object motion trajectory computer vision display systems target frame size large on chip sram size requirements video systems full hd resolution psnr evaluation verilog hdl synopsis design compiler umc cell library bandwidth reduction cycle reduction on chip sram reduction ping pong two way scheduling;hardware implementation;algorithm design and analysis;hardware	True motion estimation is a well-known technique to find the true object motion trajectory in a video, and it has a lot of applications in computer vision and display systems. However, if the target frame size becomes large, many new design challenges are introduced, such as huge computation, large bandwidth and large on-chip SRAM size requirements. Within the consideration of both algorithm and architecture, we develop a true motion estimator with ±128x±128 search range for video systems with Full-HD (1920×1080) resolution. The PSNR evaluation shows that our algorithm is better than other three existing algorithms. For hardware implementation, we use Verilog-HDL and synthesize it by SYNOPSIS Design Compiler with UMC 90nm cell library. The implementation works at 300MHz frequency, and it shows that there are total 76% bandwidth reduction, 66% cycle reduction and 88% on-chip SRAM reduction with the proposed ping-pong two-way scheduling and motion vector grouping techniques.	algorithm;compiler;computation;computer vision;hardware description language;markov chain;markov random field;motion estimation;peak signal-to-noise ratio;requirement;scheduling (computing);static random-access memory;verilog	Fu-Chen Chen;Yung-Lin Huang;Shao-Yi Chien	2012	Proceedings of Technical Program of 2012 VLSI Design, Automation and Test	10.1109/VLSI-DAT.2012.6212598	parallel computing;real-time computing;quarter-pixel motion;computer science;theoretical computer science;motion estimation	EDA	12.843677332247927	40.781499725679495	180443
0fffc38fc686d2a384ca2951a1a772f24b3fafd6	new approach to the reduction of sign-extension overhead for efficient implementation of multiple constant multiplications	adders finite impulse response filters partitioning algorithms complexity theory delays signal processing algorithms classification algorithms;complexity theory;finite impulse response filters;pdp sign extension overhead reduction shift add network multiple constant multiplication mcm block most significant bit msb fir filter area delay product adp power delay product;adders;classification algorithms;sign extension critical path high speed low complexity multiple constant multiplication mcm;multiplying circuits digital signal processing chips fir filters;signal processing algorithms;delays;partitioning algorithms	Sign-extension of operands in the shift-add network of multiple constant multiplication (MCM) results in a significant overhead in terms of hardware complexity as well as computation time. This paper presents an efficient approach to minimize that overhead. In the proposed method, the shift-add network of an MCM block is partitioned into three types of sub-networks based on the types of fundamentals and interconnections they involve. For each type of sub-network, a scheme which takes the best advantage of the redundancy in the computation of sign-extension part is proposed to minimize the overhead. Moreover, we also propose a technique to avoid the additions pertaining to the most significant bits (MSBs) of the fundamentals. Experimental results show that the proposed method always leads to implementations of MCM blocks with the lowest critical path delay. The existing methods for the minimization of sign-extension overhead are designed particularly for single multiplication or MCM blocks of FIR filter, but the proposed method can be used to reduce the overhead of sign-extension for MCM blocks of any application. In the case of FIR filters, the proposed method outperforms other competing methods in terms of critical path delay, area-delay product (ADP), and power-delay product (PDP), as well.	benchmark (computing);computation;critical path method;finite impulse response;most significant bit;multi-chip module;operand;overhead (computing);power–delay product;sign extension;subnetwork;time complexity;usb on-the-go	Xin Lou;Ya Jun Yu;Pramod Kumar Meher	2015	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2015.2476319	statistical classification;electronic engineering;parallel computing;real-time computing;telecommunications;computer science;mathematics;adder	EDA	12.311169431407412	44.49375672522759	180676
3495e0f02e44e7315d8e4734a5b99aa92166fed7	optimal design of checks for error detection and location in fault-tolerant multiprocessor systems	detection erreur concurrente;error location;tolerancia falta;concurrent error location;detection erreur;deteccion error;diagnostic majoritaire;fault tolerant;concurrent error detection;multiprocessor systems;optimal design of checks;fault tolerant multiprocessor systems;unifgen;algorithm based fault tolerance;finite impulse response filter;arbitrary data check;contracts;uniform checks error location optimal design of checks error detection fault tolerant multiprocessor systems randgen arbitrary data check algorithm based fault tolerance majority diagnosability unifgen;indexing terms;satisfiability;controle;dc generators;tolerance faute base algorithm;majority diagnosability;fault tolerant computing;fault tolerant systems;matrix decomposition;multiprocessing systems error detection fault tolerant computing;uniform check;algorithme aleatoire;randgen;fault detection;fault tolerance;uniform checks;randomized algorithm;optimal design;control;localisation erreur concurrente;fault detection fault tolerant systems multiprocessing systems fault tolerance signal processing algorithms contracts matrix decomposition algorithm design and analysis finite impulse response filter dc generators;multiprocessing systems;error detection;signal processing algorithms;controle uniforme;algorithm design and analysis;tolerance faute;check;majority diagnosis	Designing checks to detect or locate errors in the data plays an important role in the design of fault tolerant systems. Recently, the problem of synthesizing the data-check (DC) relationship has received a lot of attention in the context of a natural paradigm for concurrent error detection/location known as algorithm-based fault tolerance (ABET). Banerjee and Abraham have shown that an ABET scheme can be modeled as a tripartite graph consisting of processors (P), data (D), and checks (C). Any technique for designing ABET systems requires a procedure for synthesizing a DC relationship, which not only has a low overhead but also has all the properties required by the designer. The main contribution of this work is to propose a simple and novel algorithm called RANDGEN to generate DC graphs. This synthesis approach itself is very fast and can be fully parallelized. By simply varying its parameters, the same algorithm RANDGEN can produce DC graphs with a wide spectrum of properties, many of which have been considered very important in recent ABET designs. RANDGEN produces s-error-detectable DC graphs with asymptotically the least number of checks for the first time. RANDGEN can also produce s-error-locatable DC graphs using only a small number of checks. This is the first general procedure for producing error-locatable graphs for any value of s. Another important outstanding problem in DC graph design is providing fast and practical methods for actually locating the errors in the data from the output pattern at the checks. We show that RANDGEN can be used to design DC graphs, which permit easy diagnosis, again with a small number of checks. It has been pointed out previously that “uniform” checks may simplify the design of the ABET system. We show how RANDGEN can be modified very simply to produce uniform s-error-detectable/locatable DC graphs. Finally, we show how one can generalize these results to synthesize strictly s-errordetectable/locatable DC graphs which can detect/locate up to s data errors even when s or fewer check computations are erroneous.	algorithm;central processing unit;computation;error detection and correction;fault tolerance;multiprocessing;optimal design;overhead (computing);p (complexity);parallel computing;programming paradigm	Ramesh K. Sitaraman;Niraj K. Jha	1993	IEEE Trans. Computers	10.1109/12.237719	fault tolerance;parallel computing;real-time computing;computer science;distributed computing;algorithm;statistics	PL	14.092262068075396	35.795012792057314	180849
595ee820afa52343070d22fc10cd03af822d2dea	fpga implementation of a tool breakage detection algorithm in cnc milling machines	transformation ondelette;field programmable gate array;correlacion;evaluation performance;diseno circuito;performance evaluation;asymmetry;edge detection;reconfigurable architectures;evaluacion prestacion;circuit design;asymetrie;red puerta programable;fresadora;system performance;reseau porte programmable;deteccion contorno;computer digital control;detection contour;herramienta corte;fpga implementation;outil coupe;wavelet transform;milling machine;detection algorithm;asimetria;procesador oleoducto;fraiseuse;conception circuit;transformacion ondita;correlation;cutting tool;processeur pipeline;control numerico computador;architecture reconfigurable;wavelet transformation;pipeline processor;commande numerique calculateur	In this paper, an on-line tool breakage detection algorithm in CNC milling machines is implemented into a single 32,000-gate FPGA from Actel. The tool breakage detection algorithm is based on three pipelined processing units: a two-channel modulo operator, a one dimension wavelet transform and an asymmetry correlator. The two-channel modulo operator performs two twelve-bit square operations and a 25-bit square root. The one dimension wavelet transform performs a 256x8 point matrix per a 256 point vector multiplication over the incoming data from the modulo operator. The third processor performs an asymmetry correlation over the wavelet data to give a single value which contains the estimation of the tool condition. The overall processing unit cost is kept low by using optimized numeric digital structures, suited to fit into a 32,000-gate FPGA while allowing the system to give on-line tool condition estimation. Results are presented in order to show overall system performance.	algorithm;field-programmable gate array	René de Jesús Romero-Troncoso;Gilberto Herrera Ruiz	2004		10.1007/978-3-540-30117-2_150	embedded system;edge detection;computer science;circuit design;computer performance;correlation;asymmetry;field-programmable gate array;wavelet transform	Logic	15.442843845780386	40.97721620111115	180970
2284d5318a255e781b52326c373c6c72b7788a7d	collaboration in distributed systems: robots, ants, and matchings		From robots playing soccer together, through ants ensuring the survival of their colony, to nodes in a peer-to-peer network, collaboration is a corner-stone of the success of many different kind of distributed systems. The goal of this thesis is to shed more light onto various kinds of collaboration, and the advantages that individuals get from working together with others. In the first part of the thesis, we consider a version of the Gale-Shapley stable matching setting, where each pair of n nodes is associated with a (symmetric) matching cost and the preferences are determined with respect to these costs. This stable matching version is analyzed through the Price of Anarchy (PoA) and Price of Stability (PoS) lens with the objective of minimizing the total cost of matched nodes. A simple example demonstrates that in the general case, the situation is hopeless, hence we restrict our attention to metric costs. Our first result is a tight bound of Θ(nlog(3/2)) on the PoA in such metric graphs. We then use the notion of α-stability, where a pair of unmatched nodes defect only if both can thereby reduce their costs by a factor greater than α ≥ 1. Our main result is an asymptotically tight trade-off, showing that with respect to α-stable matchings, the PoS is Θ ( nlog(1+ 1 2α ) ) . The proof is constructive: we present a simple algorithm that outputs an α-stable matching satisfying this bound. In the second part of the dissertation, we examine various aspects of systems of mobile robots (or agents) with restricted capabilities. We analyze an existing gathering algorithm for n robots that cannot communicate with each other and have only limited visibility, and show that the algorithm has polynomial runtime of Θ(n2). We then turn our view towards n agents controlled by finite automata that explore a two-dimensional integer grid while being able to communicate with each other in a very restricted manner. Their goal is to locate a treasure which is located in some cell at a distance D from the common starting point of all agents. Despite the restriction to constant-size memory, we show that their collaborative performance is sufficient to locate the treasure in an optimal time of O(D + D2/n) even in an asynchronous setting. We also look at small, i.e., constant numbers of agents and give upper and lower bounds on the minimal number of ants sufficient to locate the treasure for various modifications of the aforementioned model. In the last part of the thesis, we focus on the power of teamwork in systems that actively try to prevent collaboration. We investigate the feasibility of a Sybil attack, where many fake identities are created in order to get an unfair advantage, against online poker platforms. For this purpose, we implemented a large-scale attack on a poker platform in which automated players (bots) collaborate to increase their chances of winning. Due to ethical considerations, our bots were only deployed at play money tables, where we found that there is a linear rise in the average gain when increasing the number of bots. We conjecture that the essence of our findings can be generalized to real money tables and conclude that it is indeed possible to benefit from such an attack and that poker platforms are in dire need of stronger countermeasures. On the whole, the quintessence of this dissertation is that collaboration, when implemented properly, is a versatile and effective instrument to improve the performance of all kinds of distributed systems in various ways.	algorithm;anarchy;automata theory;distributed computing;finite-state machine;matching (graph theory);mobile robot;peer-to-peer;play money;polynomial;price of stability;software bug;stable marriage problem;sybil attack;time complexity	Tobias Langner	2015			simulation;computer science;artificial intelligence;communication	ECom	17.58883910639128	34.441481883683956	181037
3bac56d06e41432475ddda1735b10202b39e3501	neighborhood communications in networks	and forward;2 dimensional;cost model	Abstract   Abstract  Broadcasting (resp. gossiping) refers to the task whereby a node (resp. every node) in a network, knowing a piece of information, must transmit it to all the other nodes. We consider here a generalized form of broadcasting and gossiping, that we call respectively  k -neighborhood broadcasting and  k -neighborhood gossiping ( k -NB and  k -NG, for short). It consists in the following : a node  u  (resp. every node  u ) in the network has to send its information to all the nodes which are at distance less than or equal to  k  from  u.  We study fe-NB and fe-NG in paths, trees, cycles, 2-dimensional grids and 2-dimensional tori under the store-and-forward, 1-port, unit cost model. For most of these families, we give the optimal  k -NB (resp. fe-NG) time; if not, the optimal  k -NB (resp.  k -NG) time is given within an additive (resp. multiplicative) constant never exceeding 2.		Guillaume Fertin;André Raspaud	2001	Electronic Notes in Discrete Mathematics	10.1016/S1571-0653(04)00371-3	two-dimensional space;real-time computing;telecommunications;mathematics;distributed computing	Theory	20.807861141031058	34.42489363440956	181066
1f40fc19a48d623d38189d1f738fb11f060e8853	on the impact of identifiers on local decision	symmetry breaking;decision problems	The issue of identifiers is crucial in distributed computing. Informally, identities are used for tackling two of the fundamental difficulties that are inherent to deterministic distributed computing, namely: (1) symmetry breaking, and (2) topological information gathering. In the context of local computation, i.e., when nodes can gather information only from nodes at bounded distances, some insight regarding the role of identities has been established. For instance, it was shown that, for large classes of construction problems, the role of the identities can be rather small. However, for the identities to play no role, some other kinds of mechanisms for breaking symmetry must be employed, such as edgelabeling or sense of direction. When it comes to local distributed decision problems, the specification of the decision task does not seem to involve symmetry breaking. Therefore, it is expected that, assuming nodes can gather sufficient information about their neighborhood, one could get rid of the identities, without employing extra mechanisms for breaking symmetry. We tackle this question in the framework of the LOCAL model. Let LD be the class of all problems that can be decided in a constant number of rounds in the LOCAL model. Similarly, let LD∗ be the class of all problems that can be decided at constant cost in the anonymous variant of the LOCAL model, in which nodes have no identities, but each node can get access to the (anonymous) ball of radius t around it, for any t, at a cost of t. It is clear that LD∗ ⊆ LD. We conjecture that LD∗ = LD. In this paper, we give several evidences supporting this conjecture. In particular, we show that it holds for hereditary problems, as well as when the nodes know an arbitrary upper bound on the total number of nodes. Moreover, we prove that the conjecture holds in the context of non-deterministic local decision, where nodes are given certificates (independent of the identities, if they exist), and the decision consists in verifying these certificates. In short, we prove that NLD∗ = NLD.	complete (complexity);computation;decision problem;distributed computing;identifier;karp's 21 np-complete problems;one-to-many (data model);public key certificate;symmetry breaking;verification and validation	Pierre Fraigniaud;Magnús M. Halldórsson;Amos Korman	2012		10.1007/978-3-642-35476-2_16	distributed computing;algorithm	Theory	17.43826888758351	35.24742300452611	181243
2a3fc7c51e3360330cc86ec2ee20bcb29ce51df3	routing strategies for high speed parallel data compression.	data compression;high speed		data compression;routing	Mark Milward;Jose Luis Nunez;David J. Mulvaney	2003			data compression;data compression ratio;computer science	HPC	12.674955565854575	38.82249513024444	181416
64efd0e29c956f3b821123c7b62d2b1278130ce1	decimal addition and subtraction units using the p-valued decimal signed-digit number representation	subtraction;signed digit representation;signed digit;many valued logics adders digital arithmetic logic circuits;many valued logics;circuit noise;number representation;logic circuits;sd10r signed digit number representation addition subtraction signed digit representation;adders;addition;equations circuit noise adders;digital arithmetic;sd10r	Parallel addition and subtraction of two numbers with signed-digit number representation can be performed in constant time. A signed-digit representation for radix r=10 (SD10R) is presented, and the usefulness of asymmetrical SD10R is considered. The authors propose p(qk. . .q1)-valued SD10Rs in which p is represented as a combination of qk to q1. They also describe a p(q2q1)-valued SD10R addition and subtraction unit in which p is large. Simulation data for typical SD10Rs are compared. >		Noriaki Muranaka;Shigeru Imanishi;D. Michael Miller	1993		10.1109/ISMVL.1993.289555	arithmetic;discrete mathematics;subtraction;logic gate;pure mathematics;mathematics;addition;adder;decimal representation;algebra	NLP	18.188318472278727	44.26317577545662	181540
4e5b9678919185e7cb4c06e8c12d66b4644f58d9	symbolic evaluation of performance models for tradeoff visualization	hardware design languages;integer linear programming;page description languages;computer languages;design process;mathematical model hardware design languages equations permission process design data visualization computer languages page description languages propagation delay;process design;permission;visual representation;propagation delay;data visualization;performance model;mathematical model;covering problems;hardware description language	Often during the design process, it is necessary to analyzethe effects of tradeoff among various performance attributesof the design.Visual representations in the formof plots, graphs, and tables are typically used.These visualizationscan be generated using different applications suchas MatLab®, Mathematica®, Excel®, and so forth.Inorder to use these tools, performance equations for the designmust be rendered in an equational form with only afew variables, usually 2-3.However, performance modelsusually consist of a large number of attributes and evaluzationprocedures, typically written using the full power of aprogramming or hardware description language.We introducea symbolic evaluation procedure to simplify performancemodels.Given partial data, such evaluation yields a residualperformance model which is simpler than the original model.Symbolic evaluation can be effectively used to obtain residualperformance equations in terms of the variables whosetradeoff visualization is of interest to the user.	hardware description language;matlab;partial evaluation;symbolic execution;tree traversal;wolfram mathematica	Jeffrey Walrath;Ranga Vemuri	1997		10.1145/266021.266168	process design;propagation delay;mathematical optimization;design process;covering problems;computer science;theoretical computer science;operating system;mathematical model;mathematics;hardware description language;programming language;data visualization;algorithm	HPC	21.30566686154396	46.23118277587864	181550
a25a5fcfc02672b805032b21ec4b5ce6f4c102a4	efficient embeddings of trees in hypercubes	hypercube;descomposicion grafo;sistema informatico;computer system;binary trees;boolean hypercube;enrobage;coating;envoltura;grado diagrama;arbol binario;68r10;arbre binaire;congestion;parallel computer;68q10;tree decomposition;systeme informatique;graph embedding;degre graphe;systeme parallele;parallel system;dilation;68r05;68m10;sistema paralelo;graph degree;graph decomposition;decomposition graphe;expansion;binary tree;hipercubo	The boolean hypercube is a particularly versatile network for parallel computing. It is well known that multidimensional grid machines can be simulated on a hypercube with no communications overhead. In this paper it is shown that every bounded-degree tree can be simulated on the hypercube with constant communications overhead. In fact, the proof shows that every bounded-degree graph with an O(1)-separator can be embedded in a hypercube of the same size with dilation and congestion both O(1). It is also proved that not all bounded-degree graphs can be efficiently embedded within the hypercube. 1. Introduction. The binary hypercube is emerging as one of the most popular network architectures for parallel machines. This is due partly to the facts that the hypercube has a simple recursive structure and that there are simple algorithms for message routing on the hypercube that work well in practice. Another important consideration in the choice of network architecture is its ability to accommodate different algorithms efficiently. The problem of efficiently implementing various algorithms on parallel architectures has traditionally been studied as the	algorithm;dilation (morphology);embedded system;grid network;network architecture;network congestion;overhead (computing);parallel computing;recursion;routing	Sandeep N. Bhatt;Fan Chung Graham;Frank Thomson Leighton;Arnold L. Rosenberg	1992	SIAM J. Comput.	10.1137/0221012	folded cube graph;combinatorics;grid network;discrete mathematics;binary tree;theoretical computer science;hypercube graph;mathematics;algorithm	Theory	23.548705104061437	35.42635268203403	181644
8cdcf99fe63fca62ffee4def5d385f1721551bb8	parallel distance transforms on a linear array architecture	tratamiento paralelo;processing element;algoritmo paralelo;partition method;parallel algorithm;traitement parallele;distance transformation;image matching;real time;barreta lineal;linear array;shape analysis;transformation distance;barrette lineaire;algorithme parallele;methode partition;architecture parallele;vlsi;metodo particion;parallel architecture;distance transform;article;parallel processing	Distance transformation (DT) has been widely used for image matching and shape analysis. In this paper, a parallel algorithm for computing distance transformation is presented. First, it is shown that the algorithm has an execution time of 6 N − 4 cycles, for anN ×N image using a parallel architecture that requires N/2 parallel processors. By doing so, the real time requirement is fulfilled and its execution time is independent of the image contents. In addition, a partition method is developed to process an image when the parallel architecture has a fixed number of processing elements (PEs); say two or more. The total execution time for anN × N image by employing a fixed number of PEs is 2[ N2/M + 2(M − 1)], whenM is the fixed number of PEs.  2002 Elsevier Science B.V. All rights reserved.	binary image;central processing unit;dataflow;distance transform;image registration;image resolution;microprocessor;parallel algorithm;parallel computing;partition problem;run time (program lifecycle phase);shape analysis (digital geometry);very-large-scale integration	Tsorng-Lin Chia;Kuang-Bor Wang;Zen Chen;Der-Chyuan Lou	2002	Inf. Process. Lett.	10.1016/S0020-0190(01)00250-2	parallel processing;combinatorics;computer science;theoretical computer science;shape analysis;mathematics;parallel algorithm;very-large-scale integration;distance transform;algorithm	HPC	11.8246799177648	35.3789833727505	181851
a3284d4cf8c04ed6161466f272a4f8afd2164586	ulm implicants for minimization of univers logic module circuits	integrated circuit;logic design;logic circuits;higher order;boolean algebra;universal functions;tree structure;optimization;prime implicants;multiplexers;integrated circuits	In this paper a method is developed for circuit minimization using the universal logic modules (ULM's) of Yau and Tang. This objective is obtained by using an extension of prime implicants termed ULM implicants. Each ULM implicant implies one possible saving of a module in the tree structure implementing a function. The method is developed for the ULM(l) and an extension to the higher order ULM(p) is discussed.	circuit minimization for boolean functions;tree structure	Raymond P. Voith	1977	IEEE Transactions on Computers	10.1109/TC.1977.1674858	arithmetic;embedded system;computer science;integrated circuit;mathematics;algorithm	Vision	18.68244208342598	45.37164071562893	181968
202b6769eedb3f4701ff4b88d0bf964e5e476357	high-speed fft processors based on redundant number systems	fft co processors;computer architecture adders program processors delays encoding registers fast fourier transforms;decimal co processors;redundant number systems;three operand addition fft butterfly unit redundant number system complex number system;redundant number systems adders coprocessors fast fourier transforms;communication systems redundant number systems fast fourier transform function fused dot product add unit binary signed digit representation parallel bsd constant multiplier fdpa unit carry limited bsd adder three operand adder booth encoding power consumption fft coprocessors butterfly units complex numbers	Fast Fourier Transform (FFT) processors, having a significant impact on the performance of communication systems, have been a hot topic of research for many years. FFT function consists of consecutive multiply-add operations over complex numbers, dubbed as butterfly units. Use of redundant number systems is a way of increasing the speed of FFT coprocessors. It eliminates carry-propagation and hence permits latency reduction of each stage of the pipelined FFT architecture. This paper proposes a high-speed FFT processor using the devised fused-dot-product-add (FDPA) unit, to compute AB ± CD ± E, based on Binary-Signed-Digit (BSD) representation. Three-operand BSD adder and BSD constant multiplier are the constituents of the proposed FDPA unit. A carry-limited BSD adder is proposed and used in the three-operand adder and in the parallel BSD multiplier, so as to improve the speed of the FDPA unit. Moreover, modified-booth encoding is used to accelerate the BSD multiplier. Synthesis results show that the proposed design is about two times faster than the best previous work; but at cost of more area/power consumption.	adder (electronics);bsd;booth's multiplication algorithm;central processing unit;coprocessor;fast fourier transform;multiply–accumulate operation;operand;pipeline (computing);software propagation	Amir Kaivani;Seok-Bum Ko	2014	2014 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2014.6865615	arithmetic;parallel computing;computer hardware;computer science	Arch	11.517197674612149	44.43756024688964	181988
a86654f95bca12a01580c8242ae8b70c4c70adaa	an efficient method for transposing large matrices and its application to separable processing of two-dimensional signals	random access memory;image processing;matrix algebra;signal processing image processing matrix algebra;matrix transpose signal processing image processing large matrices separable processing two dimensional signals arbitrary matrix transposed matrix read write passes;signal processing;read write memory image reconstruction filter bank image coding signal processing algorithms image processing multidimensional signal processing video signal processing hdtv	An attempt is made to transpose an arbitrary matrix when the total number of matrix elements is too large to store them all in random-access memory. This problem is often a computational bottleneck in large computed-imaging problems. A simple algorithm for obtaining the transposed matrix using only two read/write passes over the data is derived. This algorithm is efficient for a wide range of practical problems. The first step of the algorithm reorders the data in a form that permits efficient access to the data either by row or by column. Thus, if the only reason for constructing the transpose is to provide efficient access to the data for processing along the slow dimension of a two-dimensional data set, the matrix transpose can be eliminated simply by storing the data in this intermediate form. Furthermore, this reordering can be performed in place with a single read/write pass over the data.	algorithm;intermediate representation;license;random access;random-access memory;the matrix	Michael R. Portnoff	1993	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.210874	multidimensional signal processing;computer vision;parallel computing;computer hardware;image processing;computer science;theoretical computer science;signal processing;digital image processing	DB	11.614352880642375	38.46141866261335	182122
84aba7eb6329a16881475c0a010cb567733d9085	parallel algorithms for contour extraction and coding on an erew pram computer	deteccion borde;algoritmo paralelo;parallel algorithm;algorithm complexity;image processing;time complexity;binary image;edge detection;complejidad algoritmo;procesamiento imagen;traitement image;algorithme parallele;detection contour;codificacion;complexite temps;complexite algorithme;coding;image binaire;pattern recognition;imagen binaria;reconnaissance forme;reconocimiento patron;complejidad tiempo;detection bord;codage	A parallel approach to contour extraction and coding on an Exclusive Read Exclusive Write (EREW) Parallel Random Access Machine (PRAM) is presented and analyzed. The algorithm is intended for binary images. The labeled contours can be represented by lists of coordinates, and/or chain codes, and/or any other user designed codes. Using O(n2/log n) processors, the algorithm runs in O(log n) time, where n by n is the size of the processed binary image.	binary image;central processing unit;code;exclusive or;parallel algorithm;parallel random-access machine	Its'hak Dinstein;Gad M. Landau	1990	Pattern Recognition Letters	10.1016/0167-8655(90)90118-L	time complexity;computer vision;edge detection;binary image;image processing;computer science;artificial intelligence;theoretical computer science;parallel algorithm;coding;algorithm	Theory	11.76893256377938	35.326854997662835	182422
8c949c85600a1ac33e06404d235d2bfb53f843b1	divisionless method of integer conversion	integer conversion;divisionless method	"""The following simple method for converting base b~ integers to base b2 integers using base b, arithmetic has the advantages: 1. I t does not require a division operation. 2. The procedure terminates automatically when the last base b2 digit has been found. Thus it is not necessary for a program to keep a """"digit counter,"""" and """"leading zeros"""" are never produced. 3. The method is completely general; that is, it is applicable for any b~, b2, and computer word length. Although the pr imary application of the method would be the conversion of binary integers to binary coded decimal in a computer which lacked a division instruction, the same procedure could be used, for example, to convert a time expressed in seconds to the form of seconds, minutes, hours, etc."""	binary-coded decimal;leading zero	William K. Clarkson;Benjamin M. Prince	1961	Commun. ACM	10.1145/366622.366628	theoretical computer science;computer science;integer	Graphics	15.749346439485386	42.745627846490905	182767
9f95032e46b604874f1c70216471d3938521f9b4	triggering cascades on strongly connected directed graphs	perfect target set;bootstrap percolation;irreversible dynamic monopoly;watts model	Consider the following process of activation on a directed graph G(V,E). In round zero, a set of vertices, called the seeds, are active. Thereafter, a vertex is activated in a round if at least a ρ ∈ (0,1] fraction of its in-neighbors are active in the previous round. Once a vertex is activated, it remains active. Assuming the strong connectivity of G, this paper proves the existence of O([ρ|V|]) seeds that will activate all vertices after a finite number of rounds.	asymptotically optimal algorithm;connectivity (graph theory);directed graph;graph (discrete mathematics);maxima and minima;seeds (cellular automaton);strongly connected component;vertex (geometry)	Ching-Lueh Chang;Yuh-Dauh Lyuu	2012	2012 Fifth International Symposium on Parallel Architectures, Algorithms and Programming	10.1016/j.tcs.2015.05.043	combinatorics;discrete mathematics;mathematics	Theory	21.364132193810455	34.06243839993121	183086
6af0c0c4dc09558025c983867193adcca4cd2f3e	digit-serial systolic architectures for inversions over gf(2m)	systolic arrays;extended euclidean algorithm;satisfiability;systolic arrays galois fields;throughput computer architecture hardware delay signal processing algorithms galois fields digital arithmetic cost function algorithm design and analysis cryptography;critical path;finite field arithmetic;finite field arithmetic operation;euclidean algorithm;gf;extended euclidean algorithm digit serial systolic architectures galois field gf finite field arithmetic operation;galois field;galois fields;digit serial systolic architectures	Digit-serial architectures for finite field arithmetic operations are of interest since hardware costs can be optimized by minimizing the digit size to satisfy throughput requirements. Based on a reformulation of the extended Euclidean algorithm, we design digit-serial systolic architectures with digit size L for inversions over GF(2m) using a systematic approach. Compared with previously proposed digit-serial inversion architectures, our new architectures require significantly less hardware and reduce critical path delays by 50% while achieving the same throughput L/m and latency. Unlike previously proposed digit-serial inversion architectures, for which L has to be a divisor of m, our systematic approach is applicable even when L is not a divisor of m	critical path method;extended euclidean algorithm;inversion (discrete mathematics);requirement;throughput	Zhiyuan Yan	2006	2006 IEEE Workshop on Signal Processing Systems Design and Implementation	10.1109/SIPS.2006.352559	galois theory;euclidean algorithm;finite field arithmetic;discrete mathematics;theoretical computer science;critical path method;mathematics;extended euclidean algorithm;gf(2);finite field;algebra;satisfiability	Arch	10.305602455753132	44.63439994244184	183322
2e0cc219f78ba0b6178b38a304bc530aba626dea	friend or foe? population protocols can perform community detection		We present a simple distributed algorithm that, given a regular graph consisting of two communities (or clusters), each inducing a good expander and such that the cut between them has sparsity 1/polylog (n), recovers the two communities. More precisely, upon running the protocol, every node assigns itself a binary label of m = Θ(logn) bits, so that with high probability, for all but a small number of outliers, nodes within the same community are assigned labels with Hamming distance o(m), while nodes belonging to different communities receive labels with Hamming distance at least m/2− o(m). We refer to such an outcome as a community sensitive labeling of the graph. Our algorithm uses Θ(log n) local memory and computes the community sensitive labeling after each node performs Θ(log n) steps of local work. Our algorithm and its analysis work in the (random) population protocol model, in which anonymous nodes do not share any global clock (the model is asynchronous) and communication occurs over one single (random) edge per round. We believe, this is the first provably-effective protocol for community detection that works in this model.	distributed algorithm;hamming distance;population protocol;sparse matrix;window function;with high probability	Luca Becchetti;Andrea E. F. Clementi;Emanuele Natale;Francesco Pasquale;Prasad Raghavendra;Luca Trevisan	2017	CoRR		combinatorics;discrete mathematics;telecommunications;mathematics;distributed computing;algorithm	Theory	19.063956744313998	34.68027453761735	183663
3a9b19dd4ca805ff682a97c8d41152cac7c3b733	pipelining data compression algorithms	tratamiento datos;data transmission;data compression;estudio comparativo;reseau ordinateur;data processing;traitement donnee;computer network;algorithme;etude comparative;algorithm;codificacion;transmission donnee;coding;comparative study;red ordenador;compresion dato;transmision datos;compression donnee;codage;algoritmo			R. L. Bailey;Ravi Mukkamala	1990	Comput. J.	10.1093/comjnl/33.4.308	data compression;software pipelining;data processing;telecommunications;computer science;comparative research;coding;algorithm;statistics;data transmission	Theory	13.848884654525953	37.97028428854041	183735
6f6acb433dd1932ff1fcd726fb1c59d6c2d7e6a1	fault tolerant reversible logic synthesis: carry look-ahead and carry-skip adders	digital signal processing;signal processing adders boolean functions cmos logic circuits cooling fault tolerance logic circuits nanotechnology;complexity theory;fault tolerant;logic design;boolean functions;building block;circuit design;boolean function;logic circuits;arbitrary boolean functions;nanotechnology;heat dissipation;heating;cmos process;dh hemts;carry skip adders;process design;hardware architecture;low power;reversible logic;logic gates;cmos digital integrated circuits;fault tolerant systems;adders;irreversible logic circuits;signal processing;cmos logic circuits;fault tolerance;carry look ahead;carry look ahead adders;fault tolerance adders circuit synthesis logic circuits logic design cmos logic circuits heating cmos digital integrated circuits cmos process process design;high speed;low power cmos design;reversible adder;fault tolerant reversible logic synthesis;circuit synthesis;cooling;hardware;arbitrary boolean functions fault tolerant reversible logic synthesis carry look ahead adders carry skip adders irreversible logic circuits heat dissipation low power cmos design digital signal processing nanotechnology reversible adder	Irreversible logic circuits dissipate heat for every bit of information that is lost. Information is lost when the input vector cannot be recovered from its corresponding output vector. Reversible logic circuit naturally takes care of heating because it implements only the functions that have one-to-one mapping between its input and output vectors. Therefore reversible logic design becomes one of the promising research directions in low power dissipating circuit design in the past few years and has found its application in low power CMOS design, digital signal processing and nanotechnology. This paper presents the efficient approaches for designing reversible fast adders that implement carry look-ahead and carry-skip logic. The proposed 16-bit high speed reversible adder will include IG gates for the realization of its basic building block. The IG gate is universal in the sense that it can be used to synthesize any arbitrary Boolean-functions. The IG gate is parity preserving, that is, the parity of the inputs matches the parity of the outputs. It allows any fault that affects no more than a single signal readily detectable at the circuit's primary outputs. Therefore, the proposed high speed adders will have the inherent opportunity of detecting errors in its output side. It has also been demonstrated that the proposed design offers less hardware complexity and is efficient in terms of gate count, garbage outputs and constant inputs than the existing counterparts.	16-bit;adder (electronics);cmos;care-of address;carry-skip adder;circuit design;digital signal processing;fault tolerance;gate count;input/output;logic gate;logic synthesis;one-to-one (data model);reversible computing;sensor	Md. Saiful Islam;Muhammad Mahbubur Rahman;Zerina Begum;Mohd. Zulfiquar Hafiz	2009	2009 International Conference on Advances in Computational Tools for Engineering Applications	10.1109/ACTEA.2009.5227871	fault tolerance;electronic engineering;real-time computing;xor gate;toffoli gate;logic gate;three-input universal logic gate;computer science;electrical engineering;theoretical computer science;signal processing;pass transistor logic;hardware architecture;sequential logic;and gate;boolean function	EDA	16.88756838755637	45.210878887126576	183867
52216d281e8e83085be7287a68fd933a8eaee5cf	design and implementation of high-speed and energy-efficient variable-latency speculating booth multiplier (vlsbm)	energy conservation;adaptive carry estimation;cmos integrated circuits;carry logic;probability;multiplying circuits;speculating multiplier;self reliant bit most significant part high speed variable latency speculating booth multiplier energy efficient variable latency speculating booth multiplier data hazards pipeline performance degradation data intensive computing processes pipeline efficiency high speed vlsbm energy efficient vlsbm correcting phase critical path vlsbm partial products bit least significant part lsp size 90 nm two stage pipelined booth multiplier cycle count ratio speed up ratio cmos conventional pipelined booth multiplier well pipelined modified booth multiplier probability dsp algorithm multiplication pipelined datapath data dependence carry prediction partial product accumulation estimation function msp;error compensation;digital signal processing chips;probability carry logic cmos integrated circuits digital signal processing chips energy conservation multiplying circuits pipeline arithmetic;article;pipeline arithmetic;speculating multiplier adaptive carry estimation error compensation;estimation cmos integrated circuits adders hazards error compensation logic gates silicon	Data hazards cause severe pipeline performance degradation for data-intensive computing processes. To improve the performance under a pessimistic assumption on the pipeline efficiency, a high-speed and energy-efficient VLSBM is proposed that successively performs a speculating and correcting phase. To reduce the critical path, the VLSBM partial products are partitioned into the (n-z)-bit least significant part (LSP) and the self-reliant (n+z)-bit most significant part (MSP), and an estimation function stochastically predicts the carry to the MSP, thereby allowing independent calculation of the partial-product accumulation of parts. When a carry prediction is accurate, the data dependence is hidden and the correcting phase is bypassed, thereby ensuring the potential speed-up of the pipelined datapath. If a prediction is inaccurate, the speculation is flushed and the correcting phase is executed to obtain the exact multiplication. The simulation results verify the effectiveness of the proposed VLSBM. When applied to a DSP algorithm with a data hazard (or dependence) probability PD, 0 ≤ PD ≤ 1, the results show that the proposed VLSBM outperforms the original Booth multiplier and the fastest conventional well-pipelined modified Booth multiplier when PD > 0.32. For the case of high PD with PD ≈ 1, the proposed VLSBM improves approximately 1.47 times speedup against the fastest conventional pipelined Booth multiplier (@UMC 90 nm CMOS) and, furthermore, approximately 25.4% of energy per multiplication and 7% of area are saved. By examining multiplications during three multimedia application processes (i.e., JPEG compression, object detection, and H.264/AVC decoding), the proposed VLSBM improves the speed-up ratio by approximately 1.0 to 1.4 times, and reduces the cycle count ratio by approximately 1.3 to 1.8 times in comparison to the fastest conventional two-stage pipelined Booth multiplier.	adder (electronics);andrew donald booth;assembly language;booth's multiplication algorithm;branch predictor;cmos;central processing unit;compiler;critical path method;cycle count;data dependency;data-intensive computing;datapath;elegant degradation;fastest;graphics pipeline;h.264/mpeg-4 avc;hazard (computer architecture);interrupt latency;jpeg;kerrison predictor;max;microprocessor;object detection;overhead (computing);pareto efficiency;pipeline (computing);simulation;software propagation;speedup;tree accumulation;very-large-scale integration	Shin-Kai Chen;Chih-Wei Liu;Tsung-Yi Wu;An-Chi Tsai	2013	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2013.2248851	electronic engineering;parallel computing;real-time computing;energy conservation;computer science;probability;mathematics;cmos;statistics	Arch	13.96560311866165	41.80624197749631	184115
0c6456937a3424ce8574b55ca454b52417182293	fast computation by population protocols with a leader	agent interaction;population model;turing machine;parallel models;fast algorithm	Fast algorithms are presented for performing computations in a probabilistic population model. This is a variant of the standard population protocol model, in which finite-state agents interact in pairs under the control of an adversary scheduler, where all pairs are equally likely to be chosen for each interaction. It is shown that when a unique leader agent is provided in the initial population, the population can simulate a virtual register machine with high probability in which standard arithmetic operations like comparison, addition, subtraction, and multiplication and division by constants can be simulated in O(n log5 n) interactions using a simple register representation or in O(n log2 n) interactions using a more sophisticated representation that requires an extra O(n log O(1) n)-interaction initialization step. The central method is the extensive use of epidemics to propagate information from and to the leader, combined with an epidemic-based phase clock used to detect when these epidemics are likely to be complete. Applications include a reduction of the cost of computing a semilinear predicate to O(n log5 n) interactions from the previously best-known bound of O(n 2 log n) interactions and simulation of a LOGSPACE Turing machine using O(n log2 n) interactions per step after an initial O(n log O(1) n)-interaction startup phase. These bounds on interactions translate into polylogarithmic time per step in a natural parallel model in which each agent participates in an expected Θ(1) interactions per time unit. Open problems are discussed, together with simulation results that suggest the possibility of removing the initial-leader assumption.	adversary (cryptography);algorithm;binary logarithm;commutation theorem;computation;interaction;l (complexity);polylogarithmic function;population model;population protocol;predicate (mathematical logic);register machine;scheduling (computing);simulation;time complexity;turing machine;with high probability	Dana Angluin;James Aspnes;David D Eisenstat	2008	Distributed Computing	10.1007/s00446-008-0067-z	embedded system;population model;computer science;turing machine;theoretical computer science;distributed computing;algorithm;statistics	Theory	16.37335310017598	35.04440035993802	184175
fa5041884f17d9842ffb490dcbed9632f02dd38e	o(log log n) time algorithms for hamiltonian-suffix and min-max-pair heap operations on hypercube multicomputers	deletemax;gray code;deletemin;optimal mapping;magnetic heads;hypercube multicomputers;hypercubes binary trees data structures embedded computing reflective binary codes tree data structures algorithm design and analysis phase change random access memory costs magnetic heads;pipelined suffix minima computation;o log log n time algorithms;parallel algorithms gray codes data structures hypercube networks;phase change random access memory;tree data structures;reflective binary codes;min max pair heap;binary trees;binary reflected gray codes;data structures;heap data structure o log log n time algorithms min max pair heap hypercube multicomputers mapping deletemin deletemax optimal mapping pipelined suffix minima computation binary reflected gray codes binary tree;hypercubes;mapping;heap data structure;data structure;algorithm design and analysis;embedded computing;hypercube networks;gray codes;hamiltonian path;parallel algorithms;binary tree	We present an efficient mapping of a min-max-pair heap of size N on a hypercube multicomputer of p processors in such a way the load on each processor’s local memory is balanced and no additional communication overhead is incurred for implementation of the single insertion, deletemin and deletemax operations. Our novel approach is based on an optimal mapping of the paths of a binary heap into a hypercube such that in O( logN p + logp) time we can compute the Hamiltonian-Suffix, which is defined as a pipelined suffix-minima computation on an O(logN )length heap path embedded into the Hamiltonian path of the hypercube according to the binary reflected Gray codes. However, the binary tree underlying the heap data structure is not altered by the mapping process.	binary heap;binary tree;central processing unit;code;computation;data structure;embedded system;hamiltonian (quantum mechanics);hamiltonian path;heap (data structure);maxima and minima;overhead (computing);parallel computing	Sajal K. Das;Maria Cristina Pinotti	1997		10.1109/IPPS.1997.580947	heapsort;binomial heap;fibonacci heap;parallel computing;heap;min-max heap;skew heap;double-ended priority queue;computer science;theoretical computer science;binary heap;d-ary heap;distributed computing	Theory	13.88506578522278	33.41258310787978	184186
bdae77991bec58a7b9123a674231ac66a3fe0338	a high performance vlsi architecture for integer motion estimation in hevc	arrays very large scale integration video coding clocks motion estimation encoding	A high performance VLSI architecture for integer motion estimation (IME) in High Efficiency Video Coding (HEVC) is presented in this paper. It supports coding tree block (CTB) structure with the asymmetric motion partition (AMP) mode. The architecture contains two parallel sub-architectures to meet 1080p@30fps real-time video coding. The size L×L of CTB in the architecture is set to L=32 pixels by default, and it can be extended to L=64 and L=16 pixels. A serial mode decision module to find optimal partition mode for the architecture has also been implemented.	coding tree unit;data compression;high efficiency video coding;huffman coding;input method;motion estimation;pixel;real-time clock;very-large-scale integration	Yuan Xu;Jinsong Liu;Liwei Gong;Zhi Zhang;Robert K. F. Teng	2013	2013 IEEE 10th International Conference on ASIC	10.1109/ASICON.2013.6811845	electronic engineering;real-time computing;quarter-pixel motion;computer science;theoretical computer science;coding tree unit	Robotics	12.356632742984651	40.21799512361036	184842
cad1c4556b7d0aa2d2fa759986e942451ac2ea15	reliability analysis of fault-tolerant systems with common-cause failures	fault tree;system reliability;failure analysis fault tolerant systems power system modeling power system reliability fault trees redundancy computational complexity combinatorial mathematics concrete computer network reliability;reliability modeling;failure analysis;reliability assessment;fault tolerant system;redundancy;fault tolerant systems;computational complexity;reliability analysis;fault coverage;power system reliability;power system modeling;networked systems;combinatorial mathematics;concrete;fault trees;computer network reliability	This paper proposes two separable approaches for analyzing reliability of fault-tolerant systems that are possibly subject to both common-cause failures (CCF) and imperfect fault coverage (IPC). Almost all the existing reliability assessment methods either fail to consider CCF or fail to consider IPC. Both cases result in exaggerated system reliability. Our methods provide accurate reliability by incorporating both CCF and IPC into the reliability modeling for an arbitrary system. Further, the two methods have low computational complexity in that they separate the consideration of both IPC and CCF from the combinatorics of the solution. The methods are illustrated with a concrete analysis of a sample network system. A general illustration of those approaches in the context of two important classes of reliability problems: networks and fault trees, is also provided.	reliability engineering	Liudong Xing	2003		10.1109/DSN.2003.1209984	reliability engineering;real-time computing;fault tree analysis	Logic	23.91734331992549	44.99034948685808	184886
1487126e4befb0f8273dd9064e846b6c79bf3e7f	efficiency of randomized parallel backtrack search	distributed system;hypercube;systeme reparti;algoritmo busqueda;shared memory;systeme multiprocesseur memoire repartie;time complexity;routing;algorithme recherche;reseau ordinateur;search algorithm;distributed computing;transmission message;backtrack search;message transmission;parallel computation;computer network;complexite temps;calculo paralelo;sistema repartido;sistema multiprocesador memoria distribuida;red ordenador;backtracking;parallel computer;randomized algorithm;message passing;communication cost;calculo repartido;encaminamiento;distributed memory multiprocessor system;complejidad tiempo;calcul parallele;calcul reparti;acheminement;transmision mensaje;shared memory multiprocessor;hipercubo	This paper presents an improved analysis of a randomized parallel backtrack search algorithm (RPBS). Our analysis uses the single-node-donation model that each donation contains a single tree node. It is shown that with high probability the total number of messages generated by RPBS is O(phd) where p is the number of processors, and h and d are the height and degree of the backtrack search tree. Under the assumption of unit-time message delivery, it is shown that with high probability the execution time of RPBS is n/p + O(hd) where n is the number of nodes of the backtrack search tree and the leading term n/p has no constant factor. As the result of limited communication requirement, RPBS can be efficiently implemented in message-passing or shared-memory multiprocessor systems. A general analysis of network implementation of RPBS is presented. The concept of total routing time, the sum of routing times of all messages, is introduced as a measure of communication cost. It is shown that the overall effect of message delay to the execution time of RPBS is small if the total routing time is small. Some experimental data on a shared-memory machine are reported.	backtrack;backtracking;central processing unit;message passing;multiprocessing;randomized algorithm;routing;run time (program lifecycle phase);search algorithm;search tree;shared memory;with high probability	Yanjun Zhang;A. Ortynski	1999	Algorithmica	10.1007/PL00009269	time complexity;shared memory;routing;parallel computing;message passing;computer science;distributed computing;randomized algorithm;algorithm;hypercube;backtracking;search algorithm	Theory	10.86334988955543	33.00942980381263	185200
6193c1ac912ec062887316677baa39b84dc590e5	parallel simulated annealing for shape detection	algoritmo paralelo;parallel simulated annealing;detection forme;parallel algorithm;cost function;multiprocessor;implementation;simulated annealing algorithm;simulated annealing;shape detection;funcion coste;algorithme parallele;paralelismo masivo;algorithme;algorithm;ejecucion;aleatorizacion;deteccion forma;recuit simule;calculateur mimd;randomisation;fonction cout;recocido simulado;parallel implementation;model of computation;multiprocesador;randomization;parallelisme massif;massive parallelism;mimd computer;algoritmo;multiprocesseur	Abstract   In this paper, we describe two parallel implementations of the simulated annealing method applied to the shape detection problem. The first is a massively parallel implementation on an SIMD mesh-connected architecture; the second uses an MIMD model of computation. The main focus of the paper is on restructuring the basic simulated annealing algorithm to execute on a multiprocessor. We show how to select appropriate sets of perturbations to be attempted at different temperatures to obtain good speed-ups. We give experimental results for the serial version of the algorithm applied to the detection of ellipses and parallelograms; we also present results obtained on an MIMD computer, the ENCORE MULTIMAX.	simulated annealing	Giancarlo Bongiovanni;Pierluigi Crescenzi;Concettina Guerra	1995	Computer Vision and Image Understanding	10.1006/cviu.1995.1005	parallel computing;simulated annealing;computer science;theoretical computer science;adaptive simulated annealing;algorithm	Vision	11.437792843603068	34.76239763576659	185794
2696331cb2c0dab0d373212340d850455a8a9b33	algebraic properties of functions affecting optimum fault-tolerant realizations	directed graphs;algebraic structure combinational networks directed graphs fault detection fault tolerance large fault classes large scale integration mathematical models multiple faults;fault tolerant;combinational networks;large scale integration;mathematical models;directed graph;fault detection;fault tolerance;mathematical model;algebraic structure;large fault classes;multiple faults	When a specific type of network is required, the function to be realized limits the amount of fault tolerance that can be achieved. Parameters of functions that affect the maximum obtainable fault tolerance and the maximum obtainable diagnosability are investigated for several types of combinational memoryless) networks.	combinational logic;fault tolerance	F. Gail Gray;John F. Meyer	1976	IEEE Transactions on Computers	10.1109/TC.1976.1674558	fault tolerance;discrete mathematics;directed graph;stuck-at fault;theoretical computer science;mathematical model;mathematics;distributed computing;statistics	Visualization	23.91703776624887	45.29344605792719	185846
e1b6edba37678fd9d73de843b53d5c477ad29622	parallel algorithm and architecture for public-key cryptosystem	public key cryptography;algoritmo paralelo;cryptographie cle publique;architecture systeme;parallel algorithm;time complexity;simultaneidad informatica;public key cryptosystem;algorithme parallele;complexite temps;concurrency;automate cellulaire;arquitectura sistema;complejidad tiempo;system architecture;cellular automata;simultaneite informatique;cellular automaton;modular multiplication;automata celular	This paper proposes a new parallel algorithm and architecture for two modular multiplications over GF(2m). The algorithm uses the property of irreducible all one polynomial as a modulus and computes two modular multiplications in parallel. The architecture is based on cellular automata and has smaller area and time complexity than previous architectures. Since the proposed architecture has regularity, modularity and concurrency, it is suitable for VLSI implementation. The proposed architecture can be used as a basic architecture for the public-key cryptosystems.	cryptosystem;parallel algorithm	Hyun-Sung Kim;Kee-Young Yoo	2002		10.1007/3-540-36087-5_17	cellular automaton;time complexity;modular arithmetic;space-based architecture;parallel computing;concurrency;computer science;cellular architecture;theoretical computer science;parallel algorithm;public-key cryptography;algorithm	Crypto	13.120404411612972	36.67417022847559	186272
0cf5a26d79e4fcba2425fc2bf3a49ff7baaad314	a low-power reconfigurable mixed-radix fft/ifft processor	block floating point approach reconfigurable mixed radix ifft processor reconfigurable mixed radix fft processor mixed radix algorithm scalable power consumption pipeline based architectures signal to noise ratio;reconfigurable mixed radix ifft processor;pipeline processing fast fourier transforms floating point arithmetic microprocessor chips;computer architecture hardware signal to noise ratio computational complexity discrete fourier transforms fast fourier transforms equations microelectronics information systems costs;low power;pipeline based architectures;block floating point;proceedings paper;fast fourier transforms;scalable power consumption;reconfigurable mixed radix fft processor;floating point arithmetic;power consumption;signal to noise ratio;block floating point approach;mixed radix algorithm;pipeline processing;microprocessor chips	In this paper, we present a novel FFT/IFFT processor, called reconfigurable mixed-radix (RMR) FFT. It can be easily reconfigured as from 16-point to 4096-point FFT/IFFT with proper mixed-radix algorithm assigned for each mode. The proposed processor is characterized with scalable power-consumption for different FFT/IFFT sizes. Unlike the general pipeline-based architectures which use a larger internal wordlength to achieve a high signal to noise ratio (SNR), our processor keeps the internal wordlength the same as the wordlength of the input data while adopting the block-floating point (BFP) approach to maintain the SNR	algorithm;fast fourier transform;scalability;signal-to-noise ratio	Chi-Chen Lai;Wei Hwang	2006	APCCAS 2006 - 2006 IEEE Asia Pacific Conference on Circuits and Systems	10.1109/APCCAS.2006.342238	embedded system;computer vision;fast fourier transform;electronic engineering;parallel computing;computer science;floating point;operating system;signal-to-noise ratio	EDA	11.775161616324839	44.50098142052458	186296
0c4bac3dbf849c10d087bffb4b865c627ebf9e1d	brief announcement: rapid asynchronous plurality consensus		We consider distributed plurality consensus on a complete graph of size n with k initial opinions in the following asynchronous communication model. Each node is equipped with a random Poisson clock with parameter lambda=1. Whenever a node's clock ticks, it samples some neighbors uniformly at random and adjusts its opinion according to the sample.  Distributed plurality consensus has been deeply studied in the synchronous communication model. A prominent example is the so-called Two-Choices protocol, where in each round, every node chooses two neighbors uniformly at random, and if the two sampled opinions coincide, then that opinion is adopted. This protocol is very efficient when k=2. If k=O(nε) for some small epsilon, we show that it converges to the initial plurality opinion within O(k log n) rounds, w.h.p., as long as the initial difference between the largest and second largest opinion is Omega(sqrt(n log n)). On the negative side, we show that there are cases in which Omega(k) rounds are needed, w.h.p.  To beat this lower bound, we combine the Two-Choices protocol with push-pull broadcasting. We divide the process into several phases, where each phase consists of a two-choices round followed by several broadcasting rounds. Our main contribution is a non-trivial adaptation of this approach to the above asynchronous model. If the support of the most frequent opinion is at least (1+ε) times that of the second-most frequent one and k=O(Exp(log n / log log n)), then our protocol achieves the best possible run time of O(log n), w.h.p. Key to our adaptation is that we relax full synchronicity by allowing o(n) nodes to be poorly synchronized, and the well synchronized nodes are only required to be within a certain time difference from one another. We enforce this sufficient synchronicity by introducing a novel gadget into the protocol. Other parts of the adaptation are made to work using arguments and techniques based on a Pólya urn model.		Robert Elsässer;Tom Friedetzky;Dominik Kaaser;Frederik Mallmann-Trenn;Horst Trinker	2017		10.1145/3087801.3087860	time complexity;theoretical computer science;pólya urn model;asynchronous communication;distributed computing;synchronicity;log-log plot;poisson distribution;upper and lower bounds;plurality opinion;mathematics	Theory	18.155889747615618	35.30390556925418	186397
36d3a7a521f338101330e5eb9b5e9afe3ca8dfcf	exploring the implementation of jpeg compression on fpga	clock cycles jpeg compression field programmable gate array data streaming camera logic resources pipeline architecture designed architectures handel c jpeg module matlab jpeg compressor image readout discrete cosine transform;huffman coding image compression jpeg fpga handel c dct zigzag quantization;pipeline processing c language cameras discrete cosine transforms field programmable gate arrays image coding	This paper presents the implementation of the JPEG compression on a field programmable gate array as the data are streamed from the camera. The goal was to minimise the logic resources of the FPGA and the latency at each stage of compression. The modules of these architectures are fully pipelined to enable continuous operation on streamed data. The designed architectures are detailed in this paper and they were described in Handel-C. The compliance of each JPEG module was validated using MATLAB. The resulting JPEG compressor has a latency of 8 rows of image readout plus 154 clock cycles.	algorithm;clock signal;continuous operation;field-programmable gate array;grayscale;handel;image quality;jpeg;matlab;pipeline (computing);streaming media	Ann Malsha De Silva;Donald G. Bailey;Amal Punchihewa	2012	2012 6th International Conference on Signal Processing and Communication Systems	10.1109/ICSPCS.2012.6508008	embedded system;lossless jpeg;computer hardware;computer science;theoretical computer science;jpeg;jpeg 2000;quantization	EDA	11.434161924597582	40.712037558599334	186504
5abb63aa4e9158108284cd5288888eb45ddfc837	18.6 a 0.5nj/pixel 4k h.265/hevc codec lsi for multi-format smartphone applications	random access memory;codecs;standards;video coding optimisation smart phones sram chips;decoding;decoding encoding bandwidth codecs random access memory standards large scale integration;large scale integration;dbt techniques h 265 hevc codec lsi multiformat smartphone applications hevc video codec chip cmos process lsi chip dual standard single chip logic gates internal sram rate distortion optimization rdo process external bandwidth line store sram pool lssp data bus translation;bandwidth;encoding	A 4K×2K H.265/HEVC video codec chip is fabricated in a 28nm CMOS process with a core area of 2.16mm2. This LSI chip integrates a dual-standard (H.265 and H.264) video codec and a series of prevalent (VC-1, WMV-7/8/9, VP-6/8, AVS, RM-8/9/10, MPEG-2/4) decoders into a single chip. It contains 3,558K logic gates and 308KB of internal SRAM. Moreover, it simplifies intra/inter-rate-distortion optimization (RDO) processes and reduces external bandwidth via line-store SRAM pool (LSSP) and data-bus translation (DBT) techniques. For smartphone applications, it completes real-time HEVC encoding and decoding with 4096×2160 resolution and 30fps, and consumes 126.73mW (0.5nJ/pixel) of core power dissipation at 0.9V, at 494MHz (encoding) and 350MHz (decoding). 1080HD and 720HD resolutions are reported as well. The chip features are summarized in Fig. 18.6.1.	4k resolution;cmos;codec;core (optical fiber);distortion;h.264/mpeg-4 avc;high efficiency video coding;logic gate;mathematical optimization;mobile app;pixel;raster document object;rate–distortion optimization;real-time clock;smartphone;static random-access memory	Chi-Cheng Ju;Tsu-Ming Liu;Kun-Bin Lee;Yung-Chang Chang;Han-Liang Chou;Chin-Ming Wang;Tung-Hsing Wu;Hue-Min Lin;Yi-Hsin Huang;Chia-Yun Cheng;Ting-An Lin;Chun-Chia Chen;Yu-Kun Lin;Min-Hao Chiu;Wei-Cing Li;Sheng-Jen Wang;Yen-Chieh Lai;Ping Chao;Chih-Da Chien;Meng-Jye Hu;Peng-Hao Wang;Fu-Chun Yeh	2015	2015 IEEE International Solid-State Circuits Conference - (ISSCC) Digest of Technical Papers	10.1109/ISSCC.2015.7063063	embedded system;electronic engineering;codec;real-time computing;computer science;bandwidth;encoding	EDA	11.87816166760602	40.576574732899864	186749
c0481e06079b13261d8bf4428326d3188164fd72	on top-down design of mpeg-2 audio encoder	heterogeneous multi processor system;mpeg 2 audio encoder;partitioning;top down approach	This paper presents a top-down approach to implement an MPEG-2 audio encoder in VLSI. As the algorithm of an MPEG-2 audio encoder is heavy-weighted and heterogeneous(to be mixture of several strategies), the encoder design process is undertaken carefully from the algorithmic level to the architectural level. Firstly, the encoding algorithm is analyzed and divided into sub-algorithms, called tasks, and the tasks are partitioned in the way of reusing the same designs. Secondly, the partitioned tasks are scheduled and synthesized to make the most efficient use of time and space. In the end, a real-time 5 channel MPEG-2 audio encoder is designed which is a heterogeneous multiprocessor system; two hardwired logic blocks and one specialized DSP processor.	encoder;mpeg-2	Sung Wook Park	2008	Int. J. Fuzzy Logic and Intelligent Systems	10.5391/IJFIS.2008.8.1.075	embedded system;encoder;real-time computing;computer hardware;computer science	AI	11.052266243900055	40.656952458633974	186960
898eed29299c4602a487c213d64006e1f0ed0ce6	node-pair reliability of network systems with small distances between adjacent nodes	graph node;graphe non oriente;modelizacion;random graph;valor absoluto;system reliability;k terminal reliability;fiabilite systeme;one step method;non directed graph;nudo grafo;directed undirected graph;absolute value;numerical method;metodo un paso;sequence ordering;relacion orden;grafo aleatorio;algoritmo recursivo;ordering;graphe aleatoire;probabilistic approach;windows;computer network;fiabilidad sistema;modelisation;methode noeud;graph connectivity;relation ordre;valeur absolue;algorithme recursif;metodo numerico;sliding window technique;grafo no orientado;enfoque probabilista;approche probabiliste;fenetre;directed graph;metodo nudo;graphe oriente;rupture;conectividad grafo;defaillance;methode un pas;node method;ventana;grafo orientado;recursive algorithm;network reliability;failures;two terminal reliability;networked systems;modeling;connectivite graphe;fallo;ruptura;methode numerique;sliding window;noeud graphe	A new method for computing the node-pair reliability of network systems modeled by random graphs with nodes arranged in sequence is presented. It is based on a recursive algorithm using the ‘‘sliding window’’ technique, the window being composed of several consecutive nodes. In a single step, the connectivity probabilities for all nodes included in the window are found. Subsequently, the window is moved one node forward. This process is repeated until, in the last step, the window reaches the terminal node. The connectivity probabilities found at that point are used to compute the node-pair reliability of the network system considered. The algorithm is designed especially for graphs with small distances between adjacent nodes, where the distance between two nodes is defined as the absolute value of the difference between the nodes’ numbers. The maximal distance between any two adjacent nodes is denoted by G(G), where G symbolizes a random graph. If GðGÞ 1⁄4 2 then the method can be applied for directed as well as undirected graphs whose nodes and edges are subject to failure. This is important in view of the fact that many algorithms computing network reliability are designed for graphs with failure-prone edges and reliable nodes. If GðGÞ 1⁄4 3 then the method’s applicability is limited to undirected graphs with reliable nodes. The main asset of the presented algorithms is their low numerical complexity—OðnÞ, where n denotes the number of nodes. r 2006 Elsevier Ltd. All rights reserved.	algorithm;graph (discrete mathematics);maximal set;numerical analysis;random graph;recursion (computer science)	Jacek Malinowski	2007	Rel. Eng. & Sys. Safety	10.1016/j.ress.2005.12.012	sliding window protocol;random graph;combinatorics;discrete mathematics;systems modeling;directed graph;absolute value;numerical analysis;order theory;connectivity;mathematics;reliability;algorithm;recursion	Theory	24.046160256362626	33.30066487784663	187186
2d9c0b0110b93a49852ada48e8415f6357d8375e	optimal layout of edge-weighted forests	graph theory;concepcion circuito;teoria grafo;foret graphe;layout problem;circuit design;circuit vlsi;probleme agencement;problema np duro;optimization method;theorie graphe;metodo optimizacion;algorithme;algorithm;np hard problem;very large scale integrated circuit;vlsi circuit;probleme np difficile;arbol binario;estructura datos;arbre binaire;methode optimisation;problema disposicion;bosque grafo;structure donnee;conception circuit;circuito vlsi;forest graph;data structure;algoritmo;binary tree	The layout problem for trees with weighted edges is motivated by the design of very large scale integrated circuits. Some of the nodes are xed and the object is to position the remainder so that the total weighted edge cost is minimized. The cost of each edge is the product of its weight and its length under some appropriate norm. Optimization for planar layouts is shown to be NP-hard. If crossings are permitted, then optimal layouts under the L 1 norm can be eeciently computed. Suitable algorithms and data structures are presented, and explicit exact cost functions are given for two classes of weighted complete binary trees.	algorithm;binary tree;data structure;exa;gri;integrated circuit;left-right planarity test;limbo;mathematical optimization;nan;planar graph;taxicab geometry;time complexity	Michael J. Fischer;Mike Paterson	1999	Discrete Applied Mathematics	10.1016/S0166-218X(98)00088-2	combinatorics;discrete mathematics;data structure;binary tree;graph theory;circuit design;np-hard;mathematics;algorithm	Theory	18.131239377689194	38.5796205395836	187355
5edaa39a5a53d6cdc2d1dd388a88aa8c43b6b300	the hypercube as a dynamically reconfigurable processor mesh	graph theory;algoritmo paralelo;hypercube;gray code;teoria grafo;parallel algorithm;algorithm performance;systeme multiprocesseur memoire repartie;geometrie algorithmique;volume rendering;computational geometry;maillage;theorie graphe;algorithme parallele;celdarada;resultado algoritmo;mappage;sistema multiprocesador memoria distribuida;performance algorithme;code gray;grid pattern;geometria computacional;mapping;distributed memory multiprocessor system;codigo gray;dynamically reconfigurable processor;hipercubo	We describe a technique for the efficient processing of large, multidimensional arrays on a MIMD hypercube. The technique allows the hypercube to be used as a processor mesh whose relative dimension sizes may be changeddynamically, while always keeping adjacent array elements on the same node or on physically adjacent nodes. The technique is based on a mapping scheme, calledpermuted Gray code mapping, which is a generalization of the binary reflected Gray code mapping. We also extend the technique to allowinterleavingof the array data over the nodes of the hypercube. This technique can be used to efficiently parallelize scan-line algorithms, including operations such as volume rotation and volume rendering.	reconfigurable computing	Joseph M. Joy;R. Daniel Bergeron	1998	J. Parallel Distrib. Comput.	10.1006/jpdc.1997.1402	gray code;parallel computing;mimd;computational geometry;computer science;graph theory;theoretical computer science;parallel algorithm;volume rendering;algorithm;hypercube	HPC	11.897088563554291	35.12504002821035	187370
fc96778a4b1685ffd5330e882b1873312c9dac22	multi-gigahertz arbitrary timing generator and data pattern serializer/formatter	generators;clocks;delay lines;timing on the fly;prototypes;picosecond;multiplexing;ate;picosecond ate high speed digital test high speed i o timing on the fly frequency switching jitter multiplexing;frequency switching;high speed digital test;function generators;field programmable gate arrays delay generators clocks delay lines prototypes;signal waveforms multi gigahertz arbitrary timing generator data pattern serializer formatter atg design hardware prototype unlimited timing flexibility software simulation tools bit by bit basis real time algorithmic calculation pipelined fpga controller;electronic engineering computing;timing electronic engineering computing field programmable gate arrays function generators real time systems;field programmable gate arrays;jitter;high speed i o;real time systems;timing	A multi-GHz arbitrary timing generator (ATG) design is described and demonstrated in a hardware prototype. The objective of the ATG is to realize ATE hardware that nearly matches the unlimited timing flexibility of software simulation tools. The ATG allows timing edges to be programmed at almost any desired point within the test, with minimal constraints. The delay of every edge can be changed on a cycle-to-cycle basis. The period (frequency) can be changed on a bit-by-bit basis. Real-time algorithmic calculation of timing values is accomplished using a pipelined FPGA controller so that highly complex timing sequences can be synthesized. The ATG generates timing edges according to the FPGA calculations, and combines these with serialized digital “pattern” data to create the desired signal waveforms. A prototype supports ~10ps resolution and achieves approximately +/-20ps accuracy (including 6σ random jitter). Its maximum sustainable data rate is 3.2Gbps (non-multiplexed) and 6.4Gbps (multiplexed). Bursts patterns up to 10.0Gbps are also demonstrated. Minimum pulse-width is ~70ps.	clock generator;computer simulation;cycle basis;field-programmable gate array;multiplexing;pipeline (computing);prototype;real-time cmix;uncompressed video	David C. Keezer;Te-Hui Chen;Carl Edward Gray;Hyun Woo Choi;Sungyeol Kim;Seongkwan Lee;Hosun Yoo	2012	2012 IEEE International Test Conference	10.1109/TEST.2012.6401544	picosecond;embedded system;electronic engineering;real-time computing;jitter;computer hardware;telecommunications;computer science;operating system;prototype;multiplexing;field-programmable gate array	EDA	11.467779602088896	45.88844447847987	187371
a56cc73f47f1739139c2a84fcc264e7a1785ea86	a low power mpeg i/ii layer 3 audio decoder	field programmable gate array;layer 3;low power electronics audio coding decoding;decoding;audio coding;low power;low power electronics;decoding iso standards iec standards energy consumption field programmable gate arrays clocks equalizers frequency low voltage virtual prototyping;power reduction;14 mhz low power mpeg i ii audio decoder genie iso iec 11172 3 layer 3 standard task based clock management huffman decoding equalizer fpga prototyping iso iec 11172 4 compliance vector mp3 music bit stream 1 8 v 18 mw;power consumption;low power consumption	This paper describes a low power MPEG audio decoder called GENIE, which implements the full ISO/IEC 11172-3 layer 3 standard. To reduce the power consumption, we have applied tight task-based clock supervision and developed novel design schemes for Huffman decoding and equalizer. Using these approaches, GENIE can consume at most 18 mW at 14 MHz operating frequency and 1.8 V supply voltage, and this low power consumption is acceptable to the portable application. These schemes lead to a power reduction of 55% compared to other MPEG layer 3 audio decoders. GENIE was verified by simulation using the ISO/IEC 11172-4 compliance vectors and by FPGA (Field Programmable Gate Array) prototyping using real MP3 music bit streams.	mpeg-1;moving picture experts group	S. J. Nam;B. H. Kim;C. D. Im;J. B. Kim;Seung Jun Lee;S. S. Jeong;J. K. Kim;S. J. Park	2001		10.1109/ISCAS.2001.921080	embedded system;electronic engineering;real-time computing;telecommunications;computer science;operating system;network layer;field-programmable gate array;low-power electronics	EDA	11.435336977144166	41.49807706948729	187463
038d8ec38134028017133634e6981a310f1e9350	cayley graphs as models of deterministic small-world networks	characteristic;adaptability;adaptabilite;reseau communication;world;red local;procesamiento informacion;distance moyenne internodale;algorithm analysis;probabilistic method;average internode distance;modelo determinista;cayley graph;modele deterministe;caracteristica;recherche;interconnection network;adaptabilidad;moyenne;graphe simple;local network;coefficient clustering;clustering coefficient;informatique theorique;promedio;information processing;distancia;grafo cayley;caracteristique;average;low diameter network;analyse algorithme;monde;mundo;traitement information;small world network;deterministic model;reseau petit diametre;reseau local;communication;investigacion;red de comunicacion;comunicacion;communication network;analisis algoritmo;red interconexion;distance;computer theory;reseau interconnexion;graphe cayley;informatica teorica	Many real networks, including those in social, technological, and biological realms, are small-world networks. The two guishing characteristics of small-world networks are high local clustering and small average internode distance. A grea previous research on small-world networks has been based on probabilistic methods, with a rather small number of re advocating deterministic models. In this paper, we further the study of deterministic small-world networks and show tha graphs may be good models for such networks. Small-world networks based on Cayley graphs possess simple structures icant adaptability. The Cayley-graph model has pedagogical value and can also be used for designing and analyzing comm and the other real networks.  2005 Elsevier B.V. All rights reserved.	cluster analysis;clustering coefficient;magma;realms	Wenjun Xiao;Behrooz Parhami	2006	Inf. Process. Lett.	10.1016/j.ipl.2005.10.001	local area network;combinatorics;adaptability;evolving networks;information processing;artificial intelligence;network motif;probabilistic method;deterministic system;cayley graph;clustering coefficient;mathematics;small-world network;characteristic;distance;complex network;algorithm;telecommunications network;series-parallel networks problem	Theory	24.366269135347693	35.04929577126308	187643
1960cb9d144da129fe388eb764d6c2385894bbbe	parallel implementation of vision algorithms on distributed systems	distributed system;object recognition;divide and conquer policy;concurrent computing;real time environments;application software;processor scheduling;image matching;hierarchical object recognition system parallel implementation vision algorithms distributed systems vision computing real time environments distributed computer system divide and conquer policy matrix operation image matching pvm parallel virtual machine;real time;hierarchical object recognition system;virtual machining;distributed computing;matrix algebra;vision computing;divide and conquer methods;computer vision;distributed computer system;virtual machines;distributed computing system;pvm;machine vision;computer vision parallel processing machine vision application software concurrent computing distributed computing processor scheduling image matching virtual machining object recognition;parallel implementation;distributed systems;parallel virtual machine;matrix operation;divide and conquer;parallel processing;virtual machines computer vision parallel processing divide and conquer methods matrix algebra image matching;structured data;vision algorithms	Vision computing involves the execution of a large number of operations on large sets of structured data. The need to implement vision tasks in parallel arises from the speed requirements of real-time environments in various application domains. In this paper we propose that a distributed computer system can be utilised to replace the specialised machine for the parallel implementation of vision tasks. We introduce some techniques used in distributed systems and adopt a divide-and-conquer policy to schedule the complex vision tasks for parallelism. Two traditional vision algorithms for matrix operation and image matching are implemented using PVM (parallel virtual machine). Furthermore, a hierarchical object recognition system is described as an example of parallelism on distributed systems. Finally we conclude that some vision tasks can be realised on a general distributed system to achieve the speedup at a low cost.	algorithm;distributed computing	Jane You;Suresh Hungenahally	1998		10.1109/KES.1998.725994	parallel computing;computer science;theoretical computer science;distributed computing	EDA	10.789811397459442	35.612033117545515	187706
77c73a50c80d27e3f2ae11ec85a5bfa8976bca73	minimum multiple originator broadcast graphs	multiple originator;broadcast;minimum broadcast graph	Broadcasting frommultiple originators is a variant of broadcasting in which any k vertices may be the originators of amessage in a network of n vertices. Aminimumbroadcast graph has the fewest possible edgeswhile still allowingminimum time broadcasting from any set of k originators. We provide a census of all known such graphs. © 2016 Elsevier B.V. All rights reserved.	power of two;vertex (geometry)	Arthur L. Liestman;Dana S. Richards	2017	Discrete Applied Mathematics	10.1016/j.dam.2016.06.015	combinatorics;discrete mathematics;mathematics;distributed computing;broadcasting	Theory	20.654879872841605	33.578094754082144	187825
c88118b4da504e6ba896cc4353772264d413254d	diagnosability of crossed cubes under the comparison diagnosis model	parallel and distributed system;multiprocessor interconnection networks;hypercube;multiprocessor systems;cycles diagnosability crossed cubes comparison diagnosis model multiprocessor system parallel processing hypercube variant wide diameter fault diameter trees;diagnosability;crossed cube;hypercubes fault diagnosis multiprocessing systems polynomials power system modeling parallel processing maintenance system testing topology;hypercubes multiprocessing systems fault diagnosis polynomials power system modeling parallel processing maintenance system testing topology;fault trees hypercube networks fault diagnosis;faulty nodes diagnosability crossed cube comparison diagnosis model trees cycles polynomial algorithm;comparison diagnosis model;polynomial algorithm;fault diagnosis multiprocessor interconnection networks;diagnosis;multiprocessor system;parallel processing;hypercube networks;fault diagnosis;fault trees	Diagnosability of a multiprocessor system is one important study topic in the parallel processing area. As a hypercube variant, the crossed cube has many attractive properties. The diameter, wide diameter and fault diameter of it are all approximately half of those of the hypercube. The power that the crossed cube simulates trees and cycles is stronger than the hypercube. Because of these advantages of the crossed cube, it has attracted much attention from researchers. We show that the n-dimensional crossed cube is n-diagnosable under a major diagnosis model-the comparison diagnosis model proposed by Malek (1980) and Maeng and Malek (1981) if n/spl ges/4. According to this, the polynomial algorithm presented by Sengupta and Dahbura (1992) may be used to diagnose the n-dimensional crossed cube, provided that the number of the faulty nodes in the n-dimensional crossed cube does not exceed n. The conclusion of this paper also indicates that the diagnosability of the n-dimensional crossed cube is the same as that of the n-dimensional hypercube when n>5 and better than that of the n-dimensional hypercube when n=4.	marching cubes	Jianxi Fan	2002	IEEE Trans. Parallel Distrib. Syst.	10.1109/TPDS.2002.1019858	parallel processing;parallel computing;fault tree analysis;computer science;theoretical computer science;distributed computing;hypercube	Arch	23.81895033646217	44.10073552148414	188391
768be0edd50d6238b7300948d705267dd95f37e5	an engineering consideration of spectral transforms for ternary logic synthesis	logic synthesis		logic synthesis;three-valued logic	S. L. Hurst	1979	Comput. J.	10.1093/comjnl/22.2.173	logic synthesis;computer science;theoretical computer science	Logic	18.85424206678209	45.64681132794926	188441
36488045379ded339c9b4873e37f307b11cb67a3	a linear delay algorithm for building concept lattices	galois lattice;compact representation;concept lattice;binary relation;delay time	Concept lattices (also called Galois lattices) have been applied in numerous areas, and several algorithms have been proposed to construct them. Generally, the input for lattice construction algorithms is a binary matrix with size |G||M | representing binary relation I ⊆ G×M . In this paper, we consider polynomial delay algorithms for building concept lattices. Although the concept lattice may be of exponential size, there exist polynomial delay algorithms for building them. The current best delay-time complexity is O(|G||M |). In this paper, we introduce the notion of irregular concepts, the combinatorial structure of which allows us to develop a linear delay lattice construction algorithm, that is, we give an algorithm with delay time of O(|G||M |). Our algorithm avoids the union operation for the attribute set and does not require checking if new concepts are already generated. In addition, we propose a compact representation for concept lattices and a corresponding construction algorithm. Although we are not guaranteed to achieve optimal compression, the compact representation can save significant storage space compared to the full representation normally used for concept lattices.	algorithm;existential quantification;formal concept analysis;polynomial delay;time complexity	Martin Farach-Colton;Yang Huang	2008		10.1007/978-3-540-69068-9_20	combinatorics;discrete mathematics;pure mathematics;binary relation;mathematics;map of lattices;lattice miner;algorithm	Theory	20.222085120862474	41.9194342918443	188835
9733a55101489f96a02c487bfc2566f004ec0760	number theoretic transform based on ternary arithmetic	convolution;polynomials;power engineering and energy;arithmetic convolution power engineering and energy councils polynomials;modular arithmetic;councils;arithmetic;hardware implementation	A number theoretic transform (NTT) is proposed, which can efficiently be computed by using ternary modular arithmetic. The new NTT relaxes the restriction imposed on the convolution length and, as in other efficient NTT's, its computation can be performed by means of data shifts and additions. A hardware implementation of the new NTT is then described and is compared with the implementation of conventional NTT's based on binary modular arithmetic.	ternary numeral system;theory	Prabhakara C. Balla;Andreas Antoniou	1983		10.1109/ICASSP.1983.1172156	modular arithmetic;discrete mathematics;arbitrary-precision arithmetic;saturation arithmetic;pure mathematics;mathematics;convolution;polynomial;algebra	Crypto	15.103521554901608	43.7409340540687	189226
4a7e332800d9b121ca8829236b4dbc40322a4fa2	parallel, memory access schemes for h.263 encoder	encoding bandwidth digital signal processing chips parallel processing video compression concurrent computing video coding discrete cosine transforms video signal processing laboratories;dsp chips h 263 encoder parallel memory access schemes h 263 video encoding onchip parallel processors flexible memory access schemes scalable processor architecture module assignment functions memory bandwidth;memory access;video coding;parallel architectures;digital signal processing chips;digital signal processing chips video coding parallel memories parallel architectures;parallel memories	In this paper, we present new parallel memory access schemes for efficient use in H.263 video encoding with on-chip parallel processors. Since video processing has variable demand for computational power depending on optional coding modes, picture resolution, desired frame rate, compression ratio, and quality etc., we propose new flexible parallel memory access schemes in a scalable processor architecture with N=2/sup n/, (n=4,5,6) parallel processors. The necessary module assignment functions are described in detail. These parallel memory access schemes provide a solution to the increased need of memory bandwidth when the number of processors is increased.	encoder	Jarno K. Tanskanen;Tero Sihvo;Jarkko Niittylahti;Jarmo Takala;Reiner Creutzburg	2000		10.1109/ISCAS.2000.857189	uniform memory access;interleaved memory;parallel computing;computer hardware;computer science;theoretical computer science	EDA	11.942955442347769	40.093267238721786	189361
e07eff03aa65d3aa1e5342b106c8b7210314a4e6	disjoint paths of bounded length in large generalized cycles	iterated line digraphs;generalized cycles;large generalized cycle;fault-tolerant networks;fault-diameter;bounded length;disjoint path;lower bound;fault tolerant	"""A generalized p-cycle is a digraph whose set of vertices is partitioned in p parts that can bc ordered in such a way that a vertex is adjacent only to vertices in the next part. The families of B G C ( p , d , d ' ) and KGC(p ,d ,d """"~ + d ' ) are the largest known p-cycles for their degree and diameter. [n this paper we present a lower bound for the fault-diameter of a generalized cycle. Then we calculate the wide-diameter and the fault-diameter of the families mentioned above, by constructing disjoint paths between any pair of vertices. We conclude that the values of these parameters for B G C ( p , d , d ' ) and KGC(p ,d ,d """"+~ + d ~') exceed the lower bound at most in one unit. @ 1999 Elsevier Science B.V. All rights reserved Keywordw Fault-tolerant networks; Fault-diameter; Generalized cycles; Iterated line digraphs"""	directed graph;iterated function;line graph;vertex (geometry)	Daniela Ferrero;Carles Padró	1999	Discrete Mathematics	10.1016/S0012-365X(99)90075-4	fault tolerance;combinatorics;discrete mathematics;mathematics;upper and lower bounds	Theory	24.437848321287014	34.054864751977455	189537
c5ba9bce0f6e33bcd207bc1073349730d42ccddf	a parallel algorithm for join operation on cube connected computer	parallel algorithm	This paper presents a parallel join algorithm on cube connected computer(CCC), where one tuple corresponds to one processing element(PE). First, we shall show that our algorithm takes O(log 2 N) + O(R max log N) using initial sort and eecient broadcasting procedures , where N and R max represent respectively the number of tuples and the maximal subset size which determines the number of iteration for joining all the tuples. Next, we shall show that the performance of the parallel algorithm can be sharply improved using the preprocess-ing step where R max is reduced by removing the tuples which do not participate in the join operation. Also, we implement and evaluate our parallel join algorithm on transputer based parallel computer.	cube;iteration;join (sql);maximal set;parallel algorithm;parallel computing;preprocessor;transputer	Yong-Seok Cheon;Chang-Sung Jeong	1995			parallel computing;distributed computing;cube;parallel algorithm;computer science;klee–minty cube	DB	13.41662536025409	32.62110644211129	189562
c1a1fac20518c3f3c084786fa4e22fd823dd76b7	area/performance evaluation of digit-digit gf(2k) multipliers on fpgas	multiplying circuits;logic design;bit serial implementation digit digit multipliers fpga hardware architectures parametric designs elliptic curve cryptography bit serial multiplier digit serial implementation;iterative methods;multiplying circuits field programmable gate arrays iterative methods logic design;field programmable gate arrays;field programmable gate arrays polynomials timing hardware computer architecture throughput	This work describes novel hardware architectures for GF(2k) multipliers using a digit-digit approach. Contrary to the bit-serial and digit-serial approaches previously addressed in the literature, we consider the partition of the multiplier, multiplicand and modulus in several digits and execute a field multiplication in an iterative way, like in a software implementation but exploiting the parallelism in the operations. We focused on parametric designs that allow to study area-performance trade offs when the multipliers are implemented in FPGAs. This study would guide a designer to select the most appropriate configuration based on the digits sizes in order to meet system requirements such as available resources, throughput, and efficiency. Although the proposed multiplier can be implemented for any finite field of order k, we provide implementation results for GF(2163) and GF(2233), two recommended finite fields for elliptic curve cryptography. For specific digit sizes, our proposed digit-digit multiplier uses considerably less area than a bit-serial multiplier with a penalization in the timing. Compared to a digit-serial implementation, area resources can be saved with still an improvement in the timing respect to a bit-serial implementation.	dadda multiplier;elliptic curve cryptography;field-programmable gate array;iterative method;modulus of continuity;parallel computing;penalty method;performance evaluation;requirement;serial communication;system requirements;throughput	Miguel Morales-Sandoval;Arturo Diaz-Perez	2013	2013 23rd International Conference on Field programmable Logic and Applications	10.1109/FPL.2013.6645546	embedded system;parallel computing;logic synthesis;computer science;theoretical computer science;iterative method;field-programmable gate array	EDA	10.22092211837362	44.90867923945792	189763
f7997fcb8fa9a7ddaf5ff4a7cd79665a4bc7135d	a new frame recompression algorithm integrated with h.264 video compression	golomb rice coding;degradation;memory recompression algorithms;psnr;data compression;64 bit;h 264 intra prediction results;h 264 encoder;frame recompression algorithm;video compression;differential pulse code modulation;intra prediction;video coding;h 264 video compression;modulation coding;frame memory;bandwidth;video codecs;64 bit frame recompression algorithm h 264 video compression frame memory memory recompression algorithms h 264 intra prediction results differential pulse code modulation golomb rice coding h 264 encoder;video compression hardware bandwidth pulse modulation costs degradation modulation coding psnr delay encoding;encoding;video coding data compression differential pulse code modulation video codecs;pulse modulation;hardware	To reduce the size and bandwidth requirement of a frame memory for video compression, a number of memory recompression algorithms have been proposed. These previous algorithms are performed independently of a video compression standard and therefore do not take advantage of the information obtained during the processing of the compression standard. This paper proposes a new recompression algorithm that makes use of the information from H.264 intra prediction results. The proposed algorithm decomposes a frame into 4times4 blocks which are then compressed into 64-bit segments. The result of 4times4 intra prediction is used to select the scan order of the 4times4 block and DPCM (differential pulse code modulation) is performed along this scan order. Then, the DPCM results are further compressed by Golomb-Rice coding. The proposed recompression algorithm is implemented in hardware and integrated with an H.264 encoder. The proposed algorithm improves the average PSNR by 2.9dB compared to the previous work in (Lee, 2003). The hardware cost for the implementation of the recompression algorithm is 28 K gates and the additional latency to read the compressed frame memory is 162 cycles per a macroblock	64-bit computing;algorithm;data compression;encoder;golomb coding;golomb ruler;h.264/mpeg-4 avc;intra-frame coding;macroblock;modulation;peak signal-to-noise ratio;rice's theorem;video coding format	Yongje Lee;Chae-Eun Rhee;Hyuk-Jae Lee	2007	2007 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2007.378829	video compression picture types;data compression;electronic engineering;real-time computing;computer science;theoretical computer science;mathematics;statistics	EDA	12.567246856881676	39.87490097736011	190060
867ea38e35561525d16d1623b1ff868a505d935d	a design on the vector processor of 2048point mdct/imdct for mpeg-2 aac	sound quality;langage description materiel informatique;data transmission;field programmable gate array;calidad sonora;diseno circuito;optimisation;naming;microarchitecture;haute performance;qualite sonore;reproduccion sonido;data compression;optimizacion;langage c;reproduction son;efficient algorithm;transformation cosinus discrete;circuit design;hardware description languages;distributed computing;lapped transform;sound reproduction;red puerta programable;reseau porte programmable;circuit a la demande;hardware architecture;custom circuit;c language;imaging system;circuito integrato personalizado;moving picture expert group;discrete cosine transforms;transmission donnee;cancellation;denomination;alto rendimiento;microarquitectura;denominacion;calculo repartido;time domain;optimization;conception circuit;mpeg;modified discrete cosine transform;compresion dato;vector processor;annulation;optimal algorithm;high performance;moving pictures expert group;calcul reparti;audio acoustics;transmision datos;compression donnee;lenguaje c;acoustique audio;processeur vectoriel	High Quality CD, and DAT audio is very data intensive. Currently, the multi-channel technique is the preferred method of audio transmission. The MPEG(Moving Picture Experts Group) provides data compression technology for sound and image systems. The MPEG-2 AAC standard provides multi-channel 5.1 sound, using the same audio algorithm as MPEG-1, thus MPEG-2 audio both forward and backward compatible. The MDCT(Modified Discrete Cosine Transform)is a linear orthogonal lapped transform based on the concept of TDAC(Time Domain Aliasing Cancellation). In this paper, we propose an efficient algorithm for the optimization of the core in the audio part of the data transmission based on the MDCT/IMDCT(Inverse MDCT). This algorithm reduced the operating coefficient by overlapped area to bind. In the comparison of the original algorithm with the optimized algorithm that cosine coefficient reduced 0.5%, multiplies operating 0.098% and adds operating 0.58%. The proposed Algorithm was implemented using the C language then we designed hardware architecture of micro-programmed method it's applied to optimized algorithm. This processor was designed with the VHDL language and was synthesized using the design analyzer of SYNOPSYS, with rule checking by SADAS. This processor operates at a clock frequency of 20MHz and a voltage of 5V. Thus, the designed system can be used for systems based on other FPGA and ASIC.	advanced audio coding;mpeg-2;modified discrete cosine transform;vector processor	Dae-Sung Ku;Jung-Hyun Yun;Jong-Bin Kim	2005		10.1007/11539902_132	data compression;embedded system;vector processor;speech recognition;lapped transform;modified discrete cosine transform;microarchitecture;time domain;computer science;artificial intelligence;operating system;circuit design;sound quality;hardware architecture;hardware description language;algorithm;annulation;field-programmable gate array;statistics;data transmission	EDA	14.06932171454496	40.68412620403233	190572
7665bcc843f924950fb86bb3422219b4c998b39a	spatial gossip and resource location protocols	performance measure;location problem;distributed computing;gossip;decentralized algorithm;satisfiability;gossip protocol;randomized algorithm;euclidean space;resource location;dynamic behavior	"""The dynamic behavior of a network in which information is changing continuously over time requires robust and efficient mechanisms for keeping nodes updated about new information.  Gossip protocols  are mechanisms for this task in which nodes communicate with one another according to some underlying deterministic or randomized algorithm, exchanging information in each communication step. In a variety of contexts, the use of randomization to propagate information has been found to provide better reliability and scalability than more regimented deterministic approaches.In many settings, such as a cluster of distributed computing hosts, new information is generated at individual nodes, and is most """"interesting"""" to nodes that are nearby. Thus, we propose  distance-based propagation bounds  as a performance measure for gossip mechanisms: a node at distance  d  from the origin of a new piece of information should be able to learn about this information with a delay that grows slowly with  d , and is  independent  of the size of the network.For nodes arranged with uniform density in Euclidean space, we present natural gossip mechanisms, called  spatial gossip , that satisfy such a guarantee: new information is spread to nodes at distance  d , with high probability, in  O (log 1 + e   d ) time steps. Such a bound combines the desirable qualitative features of  uniform gossip , in which information is spread with a delay that is logarithmic in the full network size, and  deterministic flooding , in which information is spread with a delay that is linear in the distance and independent of the network size. Our mechanisms and their analysis resolve a conjecture of Demers et al. [1987].We further show an application of our gossip mechanisms to a basic  resource location problem , in which nodes seek to rapidly learn the location of the nearest copy of a  resource  in a network. This problem, which is of considerable practical importance, can be solved by a very simple protocol using Spatial Gossip, whereas we can show that no protocol built on top of uniform gossip can inform nodes of their approximately nearest resource within poly-logarithmic time. The analysis relies on an additional useful property of spatial gossip, namely that information travels from its source to sinks along short paths not visiting points of the network far from the two nodes."""		David Kempe;Jon M. Kleinberg;Alan J. Demers	2004	J. ACM	10.1145/1039488.1039491	gossip;mathematical optimization;gossip protocol;real-time computing;computer science;euclidean space;theoretical computer science;mathematics;distributed computing;randomized algorithm;algorithm;satisfiability	Theory	18.61263477340316	34.6932061955166	190634
b12fc6c76374dd43ad1cde18f3ab21048ae79e52	tight bounds for parallel randomized load balancing: extended abstract	asymptotic optimality;time complexity;connected graph;load balancing;load balance;randomized allocation;asymptotic bounds;lower bound	We explore the fundamental limits of distributed balls-into-bins algorithms, i.e., algorithms where balls act in parallel, as separate agents. This problem was introduced by Adler et al., who showed that non-adaptive and symmetric algorithms cannot reliably perform better than a maximum bin load of Theta(log log n / log log log n) within the same number of rounds. We present an adaptive symmetric algorithm that achieves a bin load of two in log* n+O(1) communication rounds using O(n) messages in total. Moreover, larger bin loads can be traded in for smaller time complexities. We prove a matching lower bound of (1-o(1))log* n on the time complexity of symmetric algorithms that guarantee small bin loads at an asymptotically optimal message complexity of O(n). The essential preconditions of the proof are (i) a limit of O(n) on the total number of messages sent by the algorithm and (ii) anonymity of bins, i.e., the port numberings of balls are not globally consistent. In order to show that our technique yields indeed tight bounds, we provide for each assumption an algorithm violating it, in turn achieving a constant maximum bin load in constant time.  As an application, we consider the following problem. Given a fully connected graph of n nodes, where each node needs to send and receive up to n messages, and in each round each node may send one message over each link, deliver all messages as quickly as possible to their destinations. We give a simple and robust algorithm of time complexity O(log* n) for this task and provide a generalization to the case where all nodes initially hold arbitrary sets of messages. Completing the picture, we give a less practical, but asymptotically optimal algorithm terminating within O(1) rounds. All these bounds hold with high probability.		Christoph Lenzen;Roger Wattenhofer	2011		10.1145/1993636.1993639	mathematical optimization;combinatorics;discrete mathematics;computer science;load balancing;mathematics;algorithm	Theory	17.661378357770097	34.09415034531519	190726
546be750e6d4665ed205516a88de6448c91a8e16	quantum advantage for the local model in distributed computing		There are two central models considered in (fault-free synchronous) distributed computing: the CONGEST model, in which communication channels have limited bandwidth, and the LOCAL model, in which communication channels have unlimited bandwidth. Very recently, Le Gall and Magniez (PODC 2018) showed the superiority of quantum distributed computing over classical distributed computing in the CONGEST model. In this work we show the superiority of quantum distributed computing in the LOCAL model: we exhibit a computational task that can be solved in a constant number of rounds in the quantum setting but requires Ωpnq rounds in the classical setting, where n denotes the size of the network. 2012 ACM Subject Classification Theory of computation Ñ Quantum computation theory	distributed computing;podc;quantum;theory of computation	François Le Gall;Harumichi Nishimura;Ansis Rosmanis	2018	CoRR		quantum;computer science;distributed computing;bandwidth (signal processing);communication channel	Theory	16.46731437059954	33.818597069237	191047
75b138950c3c3ebd856f223d2788631cbbc62961	axiomatizations of floating point arithmetics	computers;distance measure;approximation method;computer model;organic computing;distance measurement;computational modeling;floating point arithmetic organizations computers joining processes distance measurement approximation methods computational modeling;joining processes;floating point;approximation methods;organizations;floating point arithmetic	We present a universal scheme for axiomatizing floating point ariththmetic. The schema can be used to axiomatize any floating point arithmetic. It consists of a labeled graph with vertices describing some arithmetical properties and edges containing appropriate axioms. The language of floating point arithmetic is developed gradually in this scheme. The scheme can provide a vehicle for studying and implementing various versions of floating point arithmetic.	axiomatic system;graph labeling;vertex (graph theory)	Wlodzimierz Zadrozny	1985	1985 IEEE 7th Symposium on Computer Arithmetic (ARITH)	10.1109/ARITH.1985.6158980	computer simulation;minifloat;discrete mathematics;arbitrary-precision arithmetic;binary scaling;computer science;floating point;theoretical computer science;operating system;mathematics;machine epsilon;algorithm;algebra	Theory	22.686195363998312	37.798656070548624	191241
c3cda500cf23696ecbc3144e7b0efcc72e318293	fpga implementation of multiplierless 5/3 legall discrete wavelet transform using lifting approach	shifters;field programmable gate array;dwt;design process;discrete wavelet transform;image processing;convolution;behavior modeling;real time;symmetric periodic extension;hardware architecture;5 3 legall wavelet filter;fpga implementation;wavelet transform;xilinx;adders;hardware implementation;multiplier less;real time systems;lifting	For the hardware implementation architecture for discrete wavelet transform (DWT), one current focus is how to efficiently decrease hardware complexity and reduce hardware overhead while the need of real-time system is met. The conventional DWT makes use of convolution, so it needs a lot of computation and hardware resources. This case is hard to imagine for the hardware architecture which has high requirement of real time and small hardware overhead.  On the problem that the hardware overhead of hardware implementation architecture for discrete wavelet transform wastes a lot, on the basis of convolution method which is replaced here with a multiplierless design. This can be achieved with lifting approach with shifters and adders/subtractors replacing multipliers.  The paper presents the architecture and implementation of lifting-based wavelet transform for 5/3 LeGall Wavelet filter of JPEG2000 standard. A VHDL model is designed, synthesized by ISE 6.3i and implemented in Xilinx-field-programmable gate array XC3S200. The proposed 2-D DWT architecture consists of Control module, RWTU module and memory module for forward and inverse transform.  As a result of implementing DWT hardware, multipliers have been replaced by Shifters. Thus giving less number of Computations and makes control complexity very simple. The synthesis report shows that for both forward and inverse discrete wavelet transform implemented by lifting theorem using LeGall 5/3 Wavelet Transform have the same calculation complexity since the total number of logic devices required are to be same. Therefore it reduces the number of operations involved in computing a DWT to almost one-half of those needed with a Convolution approach.  The performance achieved as 67.604 MHz frequency, 4.217ns delay, 2 no. of FSM's, 2 no. of Shifters and a few no. of adders. This design is implemented for 256x256 pixel sized images. This approach of forward and inverse wavelet transform can be applied for an image to be decomposed at different levels. As the level of decomposition of image increases, more and more approximate and detailed information is available. Thus provides efficient mutiresolution analysis at different frequencies.  The Synthesis process is carried out which produces RTL Schematics successfully. It shows that chosen algorithm has met the requirement of design process. Thus developed a behavioral model in VHDL which can be used for discrete wavelet transform for Image processing. Thus the design can meet real time requirements.	adder (electronics);american and british english spelling differences;approximation algorithm;behavioral modeling;computation;computational complexity theory;convolution;discrete wavelet transform;field-programmability;field-programmable gate array;image processing;jpeg 2000;lifting scheme;memory module;overhead (computing);pixel;real-time computing;real-time transcription;requirement;vhdl	A. Naregalkar;B. Harish;S. Dhanorkar;B. L. Raju	2011		10.1145/1980022.1980254	electronic engineering;real-time computing;second-generation wavelet transform;computer science;theoretical computer science;cascade algorithm;wavelet packet decomposition;stationary wavelet transform;lifting scheme	Graphics	12.084759180056121	42.19068751176005	191242
bbc253d48f3089e93fc25d38d1a0f9b2a14d001d	a look-up-based universal real-time transformer for image coding	moving image;algorithme rapide;systeme temps reel;look up table;circuit codeur;random access memory;kernel;image coding;coding circuit;image processing;image communication;helium;real time;procesamiento imagen;tecnica bidimensional;technique bidimensionnelle;transform coding;imagen movil;traitement image;image mobile;codificacion;vectors;image transmission;computational complexity;discrete cosine transforms;fast algorithm;circuito codificacion;transformation lineaire;coding;transform coding image coding;two dimensional technique;linear transformation;real time system;sistema tiempo real;read write memory;table lookup;transmission image;table exploration;algoritmo rapido;transformacion lineal;image coding table lookup vectors discrete cosine transforms computational complexity kernel read write memory random access memory image communication;codage;transmision imagen	A universal transformer for two-dimensional sepqrable linear transforms in image coding is proposed. It is not restricted to fast transforms, and switching between transforms only requires writing the appropriate constants into an internal RAM. Look-up tables are used to reduce computational complexity. The linear transform properties are exploited to reduce the memory requirements by decomposing the input vector into smaller arrays and into bit planes. The input is pipelined into parallel data paths. An implementation example on a single gate array with existing technology achieves real-time transformation of 8 X 8 blocks, with area requirements of around 38K gates.	computational complexity theory;gate array;internal ram;pipeline (computing);random-access memory;real-time clock;real-time transcription;requirement;transformer	Arie Heiman;Kenneth M. Rose	1987	IEEE Journal on Selected Areas in Communications	10.1109/JSAC.1987.1146624	kernel;transform coding;lookup table;telecommunications;image processing;computer science;theoretical computer science;linear map;coding;helium;computational complexity theory;algorithm;computer graphics (images)	Embedded	11.86876732403465	38.474484591308766	191496
0fdd9eea7850d74fb4e44b6c0b1f527f5c6d9b57	a parallel approach to direct analog-to-residue conversion	a d converter;tratamiento paralelo;look up table;traitement parallele;convertisseur an;ad converter;residue number systems;aproximacion sucesiva;parallel computation;chip;calculo paralelo;digital to analog converter;modulo adder;systeme nombre residu;successive approximation;residue number system;rns;analog to residue converter;calcul parallele;parallel processing;approximation successive;convertidor an	A novel design of a direct analog-to-residue converter is presented in this paper. The design makes use of two successive approximation analog-to-digital (A/D) converters, a few modulo adders and a small look-up table. One of the digital-to-analog converters is modified to generate outputs which are weighted by a constant factor, and one of the comparators is replaced by a difference amplifier. The look-up table needed is a very small percentage of the entire chip area and is shown to be only 840 bytes for a 36-bit residue number system converter.  1999 Elsevier Science B.V. All rights reserved.	36-bit;analog-to-digital converter;approximation;byte;comparator;differential amplifier;digital-to-analog converter;lookup table;modulo operation;residue number system	Damu Radhakrishnan;Adimathara P. Preethy	1999	Inf. Process. Lett.	10.1016/S0020-0190(99)00012-5	chip;arithmetic;parallel processing;residue number system;shaping;lookup table;computer science;mathematics;algorithm	AI	16.119354453578033	41.59768655261679	192014
cdaaaca88e261eb89ad1b36819765246b8c214e5	realization of reversible logic in dna computing	dna;computers;dna bases;biocomputing;addition etc reversible logic dna dna bases dna annealing tofolli gate;base composition;annealing;logic gates dna vectors annealing computers dna computing bonding;dna annealing;logic circuits;bonding;low power;reversible logic;vectors;logic gates;adders;comparative study;tofolli gate;dna computing;reversible half adder circuit dna based toffoli gate dna characteristics dna computing ex or gates not gates and gates replication properties dna strand parallelism properties reversible dna based composite logic;logic gate;biomolecular electronics;logic gates adders biocomputing biomolecular electronics dna logic circuits;addition etc	In this paper, DNA-based Toffoli gate has been modeled using the formation of DNA characteristics. After that, the logics of the basic (AND, OR, NOT) gates and EX-OR gates have been realized using our DNA-based Tofffoli gate. The proposed DNA-based Tofolli gate is faster due to parallelism and replication properties of DNA strands. The proposed gate also requires less space because of the compactness of DNA strands. Moreover, the DNA-based Tofolli gate requires low power as the formation of DNA consumes a small amount of energy. Reversible DNA-based composite logic has also been accomplished by a reversible Half-Adder circuit. Finally, a comparative study between the existing Toffoli gate and our proposed DNA-based Toffili gate has been done.	dna computing;parallel computing;reversible computing;toffoli gate	Ankur Sarker;Tanvir Ahmed;S. M. Mahbubur Rashid;Shahed Anwar;Lafifa Jamal;Nazma Tara;Md. Masbaul Alam;Hafiz Md. Hasan Babu	2011	2011 IEEE 11th International Conference on Bioinformatics and Bioengineering	10.1109/BIBE.2011.46	toffoli gate;logic gate;three-input universal logic gate;theoretical computer science;mathematics;genetics;algorithm	EDA	16.86553225301821	44.94717896755039	192079
3f8caf9b7ae51ecaa01b5f096c072fb1ab9e3c52	a variant of a radix-10 combinational multiplier	cmos technology;multiplying circuits adders cmos logic circuits combinational circuits digital arithmetic;multiplying circuits;binary form;multi operand decimal addition;time 2 51 ns;partial product array;carry save adder;size 90 nm;time 2 65 ns;adders;cmos logic circuits;combinational decimal multiplier;radix 10 combinational multiplier;digital arithmetic;decimal carry save adders;time 2 65 ns radix 10 combinational multiplier combinational decimal multiplier decimal carry save adders multi operand decimal addition partial product array binary form cmos technology size 90 nm time 2 51 ns;combinational circuits	We consider the problem of adding the partial products in the combinational decimal multiplier presented by Lang and Nannarelli. In the original paper this addition is done with a tree of decimal carry-save adders. In this paper, we treat the problem using the multi-operand decimal addition previously published by Dadda, where the sum of each column of the partial product array is obtained first in binary form and then converted to decimal. The multiplication, using a 90 nm CMOS technology, in this modified scheme takes 2.51 ns, while in the original scheme it takes 2.65 ns. The area of the two schemes is roughly the same.	cmos;combinational logic;operand;reduction (complexity)	Luigi Dadda;Alberto Nannarelli	2008	2008 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2008.4542181	arithmetic;binary form;electronic engineering;computer science;mathematics;combinational logic;cmos;carry-save adder;algorithm;adder;binary integer decimal	Arch	15.810615105160876	44.10691550044978	192245
a53493eae80a8619b763bcbb7ab5b4b695be21e1	on construction by worm-like agents on a self-timed cellular automaton		This paper presents a novel scheme to construct circuits on the cell space of an asynchronous cellular automaton that has partitioned cells. The construction process in this scheme is conducted by worm configurations that contain instructions on how to operate. Circuits are constructed by multiple worms, which move around in parallel on the cell space, resulting in an efficient process, as is shown.		Daichi Takata;Teijiro Isokawa;Jia Lee;Ferdinand Peper;Nobuyuki Matsui	2012		10.1007/978-3-642-33350-7_59	block cellular automaton;discrete mathematics;theoretical computer science;asynchronous cellular automaton;continuous automaton;mathematics;mobile automaton;algorithm	ECom	18.33578951340105	41.14982349516285	192884
28b064029e59bd84999fe53ce0e3ad983d39b3bd	parallel squared error clustering on hypercube arrays	computers;algoritmo paralelo;hypercube;cluster algorithm;computer program;evaluation performance;errors;general and miscellaneous mathematics computing and information science;parallel algorithm;algorithmique;performance evaluation;image processing;three dimensions;evaluacion prestacion;procesamiento imagen;calculateur simd;traitement image;algorithme parallele;computer architecture;architecture ordinateur;algorithmics;algorithme psec;simd computer;algoritmica;computer calculations;pattern recognition;programming 990200 mathematics computers;arquitectura ordenador;reconnaissance forme;classification automatique;reconocimiento patron;automatic classification;hypercube computers;clasificacion automatica;parallel processing;hipercubo	Though new parallel algorithms are continually appearing in the literature, it is still quite rare for them to be capable of dealing with problems of sizes that do not conveniently fit the machine for which they are designed. This article presents the algorithm PSEC, a parallel squared error clustering algorithm for hypercube SIMD computers of arbitrary cube dimension with local memory. PSEC owes its flexibility to the association of each of the three dimensions of the problem (numbers of data points, features, and clusters) with a distinct subset of the dimensions of the hypercube.		Francisco F. Rivera;M. A. Ismail;Emilio L. Zapata	1990	J. Parallel Distrib. Comput.	10.1016/0743-7315(90)90105-X	three-dimensional space;parallel processing;parallel computing;image processing;computer science;theoretical computer science;operating system;distributed computing;parallel algorithm;algorithmics;algorithm;hypercube	HPC	12.430525778492102	34.73243322663821	193191
0dfe511ecd0f95fde7d3bcdad13f2852097bc85f	symbolic time-domain behavioral and performance modeling of linear analog circuits using an efficient symbolic newton-iteration algorithm for pole extraction	graph theory;modeling technique;behavior modeling;performance estimation;newton iteration;inverse laplace transform;time domain analysis;time domain analysis analog circuits circuit synthesis measurement character generation performance analysis integrated circuit synthesis laplace equations graph theory numerical simulation;performance metric;input output;poles and zeros;analog circuits;linear network analysis;operational amplifier;graph theory symbolic time domain behavioral modeling linear analog circuits newton iteration algorithm pole extraction inverse laplace transform filters operational amplifiers ac behavior symbolic time domain analyzer;integrated circuit modelling;laplace transforms;time domain analysis integrated circuit modelling analogue circuits linear network analysis poles and zeros newton method laplace transforms;performance model;time domain;analogue circuits;newton method;symbolic analysis;analog circuit synthesis;numerical simulation	Symbolic modeling techniques employ behavioral models to capture the input-output relationships of linear(ized) analog circuits in conjunction with performance models to measure performance metrics. The formulation of symbolic design equations depicting AC behavior of analog circuits has been fully automated in most symbolic analysis programs. Symbolic time-domain modeling till date relies extensively on manual abstraction. In this paper, we propose an automated technique for generation of symbolic behavioral and performance models that analyze time-domain characteristics of linear(ized) analog circuits. A fully symbolic time-domain analyzer has been integrated into the analog circuit synthesis flow. The calculation of time-domain response using inverse Laplace transform is facilitated by a fully symbolic pole extraction methodology. The mainstay for this pole extraction technique is an efficient symbolic Newton-iteration algorithm based on graph theory. Significant speedup is achieved using pre-compiled symbolic models inside the synthesis loop, thereby alleviating the computational inefficiencies introduced by full numerical simulation and performance estimation. The classes of analog circuits that we have synthesized using symbolic modeling techniques with effective performance closure include filters and operational amplifiers.	algorithm;amdahl's law;analogue electronics;compiler;computer program;computer simulation;explanatory combinatorial dictionary;graph theory;iteration;iterative method;newton;newton's method;nonlinear system;numerical analysis;operational amplifier;performance prediction;spice;speedup	Ritochit Chakraborty;Mukesh Ranjan;Ranga Vemuri	2006	19th International Conference on VLSI Design held jointly with 5th International Conference on Embedded Systems Design (VLSID'06)	10.1109/VLSID.2006.153	mathematical optimization;electronic engineering;computer science;graph theory;theoretical computer science;newton's method;symbolic trajectory evaluation	EDA	21.44811533685198	46.257832935952536	193276
ff769ed7bb8f4e2a7ee4d074ae34699acaebd035	optimized cubic chebyshev interpolator for elementary function hardware implementations	function approximation chebyshev approximation delays hardware optimization methods;series mathematics chebyshev approximation cmos integrated circuits digital arithmetic interpolation matrix algebra;size 65 nm optimized cubic chebyshev interpolator elementary function hardware functions truncated matrix arithmetic units coefficient bit optimized number chebyshev series approximation maximum absolute error minimization interpolator output cmos technology reduced lookup table sizes word length 16 bit word length 24 bit word length 32 bit	This paper presents a cubic interpolator for computing elementary functions using truncated-matrix arithmetic units and an optimized number of coefficients bits. The proposed method optimizes the initial coefficient values found using a Chebyshev series approximation, minimizing the maximum absolute error of the interpolator output. The resulting designs can be utilized for approximating any function up to 53-bits of precision (IEEE double precision significant). Area, delay and power estimates are given for 16, 24 and 32-bit cubic interpolators that compute the reciprocal function, targeting a 65nm CMOS technology from IBM. Results indicate the proposed method uses smaller arithmetic units and has reduced lookup table sizes than previously proposed methods.	32-bit;approximation error;cmos;chebyshev polynomials;coefficient;cubic function;double-precision floating-point format;elementary function;lookup table	Masoud Sadeghian;James E. Stine;E. George Walters	2014	2014 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2014.6865440	mathematical optimization;discrete mathematics;theoretical computer science;mathematics;algorithm	EDA	13.936192454250042	44.15255789367247	193389
ec6cad663514af8182ea916eb4d723961bfe038d	a suggestion for a new rns-based multiplier for a family of moduli			residue number system	Ahmad A. Hiasat	2004	I. J. Comput. Appl.		parallel computing;theoretical computer science;moduli;computer science;multiplier (economics)	Theory	16.89349662431427	43.3435452960183	193472
82475769e0ea0fac427e6fd5ebe5debcf7b1ca01	linked decompositions of networks and the power of choice in polya urns	dimension;fixed parameter tractability;complexity;preferential attachment;clustering;resource sharing;load balance;internet topology;failure recovery;lower bound	"""A linked decomposition of a graph with n nodes is a set of subgraphs covering the n nodes such that all pairs of subgraphs intersect; we seek linked decompositions such that all subgraphs have about √n vertices, logarithmic diameter, and each vertex of the graph belongs to either one or two subgraphs. A linked decomposition enables many control and management functions to be implemented locally, such as resource sharing, maintenance of distributed directory structures, deadlock-free routing, failure recovery and load balancing, without requiring any node to maintain information about the state of the network outside the subgraphs to which it belongs. Linked decompositions also enable efficient routing, schemes with small routing tables, which we describe in Section 5. Our main contribution is to show that """"Internet-like graphs"""" (e.g. the preferential attachment model proposed by Barabasi et al. [10] and other similar models) have linked decompositions with the parameters described above with high probability; moreover, our experiments show that the Internet topology itself can be so decomposed. Our proof proceeds by analyzing a novel process, which we call Polya urns with the power of choice, which may be of great independent interest. In this new process, we start with n nonempty bins containing O(n) balls total, and each arriving ball is placed in the least loaded of m bins, drawn independently at random with probability proportional to load. Our analysis shows that in our new process, with high probability the bin loads become roughly balanced some time before O(n2+ε) further balls have arrived and stay roughly balanced, regardless of how the initial O(n) balls were distributed, where ε > 0 can be arbitrarily small, provided m is large enough."""	attachments;deadlock;directory (computing);experiment;internet topology;load balancing (computing);routing table;with high probability	Henry C. Lin;Christos Amanatidis;Martha Sideri;Richard M. Karp;Christos H. Papadimitriou	2008			shared resource;mathematical optimization;combinatorics;discrete mathematics;complexity;internet topology;computer science;load balancing;mathematics;geometry;cluster analysis;upper and lower bounds;dimension;algorithm;statistics	Theory	21.852169266899402	33.61000894113565	193501
93313c7d983cec792558a7670a77376dddd19024	vlsi-oriented architecture for two's complement serial-parallel multiplication without speed penalty	edge collapsing based simplification algorithm;model simplification algorithms;solid modeling clustering algorithms iterative algorithms topology minimization methods energy resolution computer applications application software computer science visualization;mesh quality;visual quality;visualization purposes;data visualisation;model simplification;quality evaluation;cad models;geometrical errors;cad models simplification algorithm model simplification algorithms visualization purposes quality evaluation geometrical errors edge collapsing based simplification algorithm;simplification algorithm	A serial-parallel multiplier computes a product by multiplying a parallel input and a serial (or online) input. Serial-parallel multipliers are frequently used in digital communication systems, digital signal processing, on-line computing applications, and embedded computing and communication systems. In this paper, a VLSI-oriented, size-efficient two's complement serial-parallel multiplication architecture is proposed. In addition to its smaller size, it is also suitable for VLSI implementation because it consists of modularized logic cells and locally interconnected signals. According to the analysis results for 2- to 32- bit multiplication, the proposed architecture requires up to 30 percent smaller size without speed penalty compared to the previous architecture.	32-bit;digital signal processing;embedded system;online and offline;parallel algorithm;two's complement;very-large-scale integration	Sangman Moh	2007	2007 International Conference on Computational Science and its Applications (ICCSA 2007)	10.1109/ICCSA.2007.56	simulation;computer science;theoretical computer science;machine learning;data visualization;statistics	HPC	14.243474961837832	43.969808148487665	193556
767836bc1e52c0cb103471a1bbd74a2ba1c7da94	domain theory, testing and simulation for labelled markov processes	domain theory;bisimilarite;05c05;bisimulacion;bisimulation;32xx;testing;test;satisfiability;teoria medida;ensayo;essai;processus markov marque;state space method;methode espace etat;mesure probabilite;informatique theorique;theorie domaine;state space;markov process;labelled graph;labelled markov process;theorie mesure;28xx;grafo marcado;probability measure;measure theory;medida probabilidad;metodo espacio estado;computer theory;graphe marque;informatica teorica	This paper presents a fundamental study of similarity and bisimilarity for labelled Markov processes. The main results characterize similarity as a testing preorder and bisimilarity as a testing equivalence. In general, labelled Markov processes are not required to satisfy a finite-branching condition—indeed the state space may be a continuum, with the transitions given by arbitrary probability measures. Nevertheless we show that to characterize bisimilarity it suffices to use finitely-branching labelled trees as tests. Our results involve an interaction between domain theory and measure theory. One of the main technical contributions is to show that a final object in a suitable category of labelled Markov processes can be constructed by solving a domain equation D ∼= V(D), where V is the probabilistic powerdomain. Given a labelled Markov process whose state space is an analytic space, bisimilarity arises as the kernel of the unique map to the final labelled Markov process. We also show that the metric for approximate bisimilarity introduced by Desharnais, Gupta, Jagadeesan and Panangaden generates the Lawson topology on the domain D. 1 The support of the US Office of Naval Research is gratefully acknowledged. 2 Supported by ONR contract N00014-95-1-0520, Defense Advanced Research Project Agency and the Army Research Office under contract DAAD19-01-1-0485. 3 The support of the National Science Foundation is gratefully acknowledged. 4 Supported by the Natural Sciences and Engineering Research Council of Canada. Preprint submitted to Elsevier Science 29 April 2005	approximation algorithm;bisimulation;domain theory;lawson topology;markov chain;markov property;power domains;simulation;software testing;state space;triune continuum paradigm;turing completeness	Franck van Breugel;Michael W. Mislove;Joël Ouaknine;James Worrell	2005	Theor. Comput. Sci.	10.1016/j.tcs.2004.10.021	discrete mathematics;computer science;calculus;mathematics;software testing;statistics	Logic	21.712678209321563	33.0969090168327	193673
b828525cc4a2aa7ea7b533b16a09762537fd0623	an in-place architecture for the deblocking filter in h.264/avc	73 73 mhz;cmos technology;logic design;very large scale integration;real time;h 264 avc;filters;transform coding;0 25 micron deblocking filter h 264 avc video coding standard intermediate data storage real time deblocking cmos technology vlsi architecture design 30 hz 73 73 mhz 100 mhz;video coding;computer architecture;data storage;deblocking filter;automatic voltage control;cmos digital integrated circuits;filters automatic voltage control computer architecture data flow computing bandwidth video coding very large scale integration memory cmos technology transform coding;cmos digital integrated circuits video coding digital filters logic design;vlsi architecture design deblocking filter h 264 avc;digital filters;30 hz;vlsi architecture design;real time deblocking;bandwidth;data flow computing;100 mhz;intermediate data storage;video coding standard;article;0 25 micron;memory;vlsi architecture	This brief presents an in-place computing design for the deblocking filter used in H.264/AVC video coding standard. The proposed in-placed computing flow reuses intermediate data as soon as data is available. Thus, the intermediate data storage is reduced to only the four 4 /spl times/ 4 blocks instead of whole 16 /spl times/ 16 macroblock. The resulting design can achieve 100 MHz with only 13.41K gate count and support real-time deblocking operation of 2K /spl times/ 1K@30 Hz video application when clocked at 73.73 MHz by using 0.25-/spl mu/m CMOS technology.	cmos;clock rate;codec;computer data storage;data compression;dataflow;deblocking filter;gate count;h.264/mpeg-4 avc;image resolution;in-place algorithm;macroblock;real-time clock;very-large-scale integration;video coding format	Chao-Chung Cheng;Tian-Sheuan Chang;Kun-Bin Lee	2006	IEEE Transactions on Circuits and Systems II: Express Briefs	10.1109/TCSII.2006.875323	embedded system;electronic engineering;logic synthesis;real-time computing;transform coding;digital filter;computer science;electrical engineering;deblocking filter;computer data storage;very-large-scale integration;memory;cmos;bandwidth	Visualization	12.579332079768983	40.330316228958345	193905
12ee29cf1ed57e56d9002e01ba5a513ea484cfc4	highly fault-tolerant routings and diameter vulnerability for generalized hypercube graphs	fault tolerant;fault tolerant routing;community networks;directed graph	Consider a communication network G in which a limited number of link and/or node faults F might occur. A routing  for the network(a fixed path between each pair of nodes) must be chosen without knowing which components might become faulty. The diameter of the surviving route graph R(G, )/F, where the surviving route graph R(G, )/F is a directed graph consisting of all nonfaulty nodes in G with a directed edge from x to y iff there are no faults on the route from x to y, could be one of the fault-tolerant measures for the routing . In this paper, we show that we can construct efficient and highly fault-tolerant routings on a k-dimensional generalized d-hypercube C(d, k) such that the diameter of the surviving route graph is bounded by constant for the case that the number of faults exceeds the connectivity of C(d, k).	diameter (protocol);fault-tolerant computer system	Koichi Wada;Takaharu Ikeo;Kimio Kawaguchi;Wei Chen	1995		10.1007/3-540-60618-1_76	fault tolerance;combinatorics;parallel computing;directed graph;mathematics;distributed computing;computer network	Theory	23.481571330032015	34.52724073865048	194004
9692877950141f4bf289701e776015c717850764	feedback from nature: an optimal distributed algorithm for maximal independent set selection	expected complexity;intercellular signalling;randomised algorithms;feedback;beeping model;mis;message complexity	Maximal Independent Set selection is a fundamental problem in distributed computing. A novel probabilistic algorithm for this problem has recently been proposed by Afek et al, inspired by the study of the way that developing cells in the fly become specialised. The algorithm they propose is simple and robust, but not as efficient as previous approaches: the expected time complexity is O(log2 n). Here we first show that the approach of Afek et al cannot achieve better efficiency than this across all networks, no matter how the global probability values are chosen.  However, we then propose a new algorithm that incorporates another important feature of the biological system: the probability value at each node is adapted using local feedback from neighbouring nodes. Our new algorithm retains all the advantages of simplicity and robustness, but also achieves the optimal efficiency of O(log n) expected time. The new algorithm also has only a constant message complexity per node.	average-case complexity;biological system;distributed algorithm;distributed computing;independent set (graph theory);maximal independent set;probabilistic analysis of algorithms;randomized algorithm;time complexity	Alex D. Scott;Peter Jeavons;Lei Xu	2013		10.1145/2484239.2484247	average-case complexity;theoretical computer science;machine learning;management information systems;feedback;distributed computing	ML	18.749839096447364	34.48374749427834	194069
ee5b39482fcd6e298e38461f9ae906daf7d624de	basis sets for synthesis of switching functions	funcion conmutacion;basis set size;dimension base;logical operator;switching functions logic design;logic design;funcion logica;sistema informatico;switching function;computer system;function synthesis;reed muller expansion;logic network synthesis sufficient conditions boolean algebra boolean functions;indexing terms;logical function;synthesis;fonction logique;operateur logique;boolean algebra;generating set;expansion reed muller;ensemble generant;fonction commutation;sintesis funcion;necessary and sufficient condition;algebre boole;logic primitives;basis set;systeme informatique;switching functions;operador logico;completeness;synthese fonction;reed muller;algebra boole;reed muller expansion synthesis switching function completeness basis set logic primitives	The synthesis of switching function f(x/sub 1/, x/sub 2/, . . ., x/sub n/) from a given family of functions g/sub i/(x/sub 1/, x/sub 2/, . . ., x/sub n/), 1 u003e	basis set (chemistry);network switch	Snehamay Kundu	1992	IEEE Trans. Computers	10.1109/12.135561	boolean algebra;discrete mathematics;logic synthesis;index term;completeness;computer science;generating set of a group;mathematics;logical connective;basis set;algorithm;algebra	EDA	19.905334143208112	44.57240660001094	194099
d4a2d792fdfafe739b37b1e7883a552965ff2058	using adder and subtractor compressors to sum of absolute transformed differences architecture for low-power video encoding		The Sum of Absolute Transformed Differences (SATD) calculation is one of the most time-consuming operations of the state-of-the-art video encoders. This work proposes a sequential SATD architecture based on 8×8 Hadamard Transform employing efficient adder and subtractor compressors. Synthesis results for 45 nm technology show that our SATD architecture when using adder and subtractor compressors saves power dissipation by 14.4% (on average) when compared with the baseline SATD architecture implemented using the adder from the synthesis tool. Furthermore, our architecture using 3-2 and 42 adder and subtractor compressors is 10% more energy-efficient than state-of-the-art SATD architectures.	adder (electronics);baseline (configuration management);data compression;encoder;hadamard transform;low-power broadcasting;subtractor;sum of absolute transformed differences	Bianca Silveira;Brunno Abreu;Guilherme Paim;Mateus Grellert;Rafael Sales Medina Ferreira;Cláudio Machado Diniz;Eduardo Costa;Sergio Bampi	2017	2017 24th IEEE International Conference on Electronics, Circuits and Systems (ICECS)	10.1109/ICECS.2017.8292076	electronic engineering;encoder;subtractor;computer science;sum of absolute transformed differences;hardware architecture;architecture;hadamard transform;adder	EDA	12.683352231016787	41.82839197585482	194302
57d5260c8c2b475974a91eff69a9a326fabc49b5	implementation of multiplier block with reduced adder cost	reduced adder cost;graph theory;graph based algorithm;multiplying circuits;hardware complexity;digital filter;fixed point;multiple constant multiplications multiplier block reduced adder cost hardware complexity digital filters dsp modules fixed point coefficient multiplication fir filter graph based algorithm computational complexity;multiplying circuits adders computational complexity digital filters graph theory;multiple constant multiplication;multiplier block;computational complexity;adders;fixed point coefficient multiplication;fir filter;digital filters;dsp modules;costs adders finite impulse response filter hardware digital filters digital signal processing energy consumption arithmetic computational complexity computational modeling;power consumption;multiple constant multiplications	"""The hardware complexity of digital filters (and many other DSP modules) is mainly dominated by the coefficient multipliers. Implementing fixed-point coefficient multiplication as a network of adders, subtractors and shifters, yields lower power consumption. In such filters the number of adders (and subtractors) determines the implementation cost. The reason is that shifts are implemented as hard-wired inter-block connections and are considered """"free"""". In transposed implementation of an FIR filter, each input is multiplied by several coefficients. Considering all coefficients as a multiplier block and omitting the redundancies by sharing the common fundamentals among different coefficients, yields great reduction in the number of arithmetic operations. This paper presents a graph based algorithm to reduce the computational complexity of multiple constant multiplications. Simulation results show that using the proposed method results good improvement in adder cost of multiplier blocks."""	adder (electronics);algorithm;coefficient;computational complexity theory;digital filter;digital signal processor;finite impulse response;lagrange multiplier;simulation	Somayyeh Rahimian Omam;S. M. Mortazavi Zanjani;Sied Mehdi Fakhraie;Omid Shoaei	2006	2006 13th IEEE International Conference on Electronics, Circuits and Systems	10.1109/ICECS.2006.379920	electronic engineering;discrete mathematics;theoretical computer science;mathematics	EDA	13.751430730493347	45.70413101482566	194604
7d74be319dd317f786ccfb7e48fe3231e527869d	an fpga implementation of a simple lossless data compression coprocessor	rice coder;field programmable gate array;pins;image coding;data compression;decoding;radiation detectors;lossless image compression;field programmable gate arrays coprocessors data compression decoding encoding;lossless compression;radiation detector;coprocessors;fpga implementation;synchronization;fpga implementation rice coder lossless compression;lossless data compression;frequency 10 mhz field programmable gate array simple lossless data compression coprocessor rice compression method rice code encoder decoder xilinx xc4005 fpga lossless image compression frequency 11 6 mhz frequency 19 4 mhz;field programmable gate arrays;encoding;coprocessors decoding field programmable gate arrays synchronization image coding radiation detectors pins	The paper describes a Field Programmable Gate Array (FPGA)-based lossless data compression coprocessor using implementing a compression method developed by Rice. We have implemented the Rice code (both encoder and decoder) for 8 bit/sample data on an FPGA Xilinx XC4005. The code has been designed to be optimal on 1.5 &#60; H &#60; 7.5 bits/sample, that is usually required in lossless image compression. The encoder and decoder can achieve 11.6 MHz and 19.4 MHz clock, respectively, where a 10 MHz clock corresponds to a 1.5 Mbits/s throughput. The XC4005 contains combinatorial logic units (CLU) and I/O pins. The Rice encoder uses 30% CLB F&G, 15% CLB H, 16% CLB FF, and 34% I/O pins. The Rice decoder uses 31% CLB F&G, 19% CLB H, 16% CLB FF, and 34% I/O pin. Hence, an X4005 is sufficient to implement both encoder and decoder.	clu;codec;combinational logic;coprocessor;data compression;encoder;field-programmable gate array;golomb coding;image compression;input/output;megabit;rice's theorem;throughput	Armein Z. R. Langi	2011	Proceedings of the 2011 International Conference on Electrical Engineering and Informatics	10.1109/ICEEI.2011.6021669	embedded system;electronic engineering;computer hardware;computer science;lossless compression;statistics	EDA	11.244183421150973	44.50776615801687	194702
25183220cb5c0edcdf51d2ac54bf7a36c4a89492	a lower bound on the complexity of arbitrary switching function realizers	switching;network synthesis;boolean functions;bound;switching circuits;logic;boolean function;isobaric;testing;complexity;function;bound complexity function realization switching;electrons;physics;realization;switches;lower bound;circuit synthesis	In this note, arbitrary switching function realizers and their complexity are defined. Then, a lower bound on the complexity is derived in terms of the number of such switching function realizers required to realize all Boolean functions of n variables.		William E. Hansalik	1972	IEEE Transactions on Computers	10.1109/T-C.1972.223552	combinatorics;discrete mathematics;computer science;theoretical computer science;mathematics;boolean function;algorithm	Theory	19.772949673884096	44.823486446787626	195908
b23f19fee686f891cbbb21b16ca70706cf95d36d	(t, k) - diagnosis for matching composition networks under the mm* model	multiprocessor interconnection networks;matching composition network;mobius cubes matching composition network mm model sequential diagnosis generalization faulty processors interconnection networks hypercubes crossed cubes twisted cubes;multiprocessor systems;diagnosability;mobius cubes;multiprocessor interconnection networks fault diagnosis fault tolerant computing;interconnection network;k hbox rm diagnosis diagnosability matching composition network mm model multiprocessor system precise diagnosis strategy t;precise diagnosis strategy;fault tolerant computing;twisted cubes;t;k hbox rm diagnosis;interconnection networks;hypercubes;faulty processors;sequential diagnosis generalization;multiprocessor system;mm model;crossed cubes;fault diagnosis	(t, k)-diagnosis, which is a generalization of sequential diagnosis, requires at least k faulty processors identified and repaired in each iteration provided there are at most t faulty processors, where tgesk. In this paper, a (t, k)-diagnosis algorithm under the MM* model is proposed for matching composition networks, which include many well-known interconnection networks, such as hypercubes, crossed cubes, twisted cubes, and Mobius cubes. It is shown that a matching composition network of n dimensions is (Omega((2n*log n)/n), n)-diagnosable	algorithm;central processing unit;interconnection;iteration;marching cubes;olap cube;twisted	Guey-Yun Chang;Gen-Huey Chen;Gerard J. Chang	2007	IEEE Transactions on Computers	10.1109/TC.2007.1	teaspoon;parallel computing;computer science;theoretical computer science;distributed computing;hypercube	Robotics	23.808516732639344	44.10804711993857	195977
f5844ce75a9b2869eed4ea4043983e94f33b20a3	a transform approach to logic design	logic arrays;fourier transform;logic design;input variables;prototypes;combinational logic;polynomials;affine group;large scale integration;integrated circuit technology;affine group combinational logic equivalence classes fourier transform large scale integration switching theory;logic functions;switching theory;equivalence classes;encoding;combinational circuits	"""This paper describes a new approach to the design of combinational logic using large-scale integrated (LSI) circuit technology. A simple """"prototype"""" logic function of n binary variables is imbedded within an array of at most (n+1) rows and columns. The cells of this array contain two-input EXCLUSIVE-OR gates, and its rows are fed by the input variables and logical """"1."""" Its column outputs are first-degree polynomial functions of the input variables. These functions supply inputs to, and modify the output of, the prototype in order to realize the desired function. These transformations form a group; specifically, the largest subgroup of the (n+1)-dimensional affine group such that input variable encodings are not affected by feedback from the function's output."""	logic synthesis	Robert J. Lechner	1970	IEEE Trans. Computers	10.1109/T-C.1970.222995	combinatorics;discrete mathematics;mathematics;sequential logic;function block diagram;combinational logic;algorithm	EDA	19.021552780468916	45.13297580067376	196235
e00745f8d21592b06d8e8b61f6ea0f0fdceb09b4	expanding window random linear codes for data partitioned h.264 video transmission over dvb-h network	video coding data compression digital video broadcasting linear codes random codes;digital video broadcasting;dvb h network;data transmission;h 264 video transmission;expanding window fountain codes;data compression;decoding;advanced video coding;linear codes;video quality;data partitioned;digital video broadcasting streaming media ip networks decoding multimedia communication forward error correction encoding;digital video broadcast;data partitioning;video coding;forward error correction;dvb expanding window fountain codes random linear codes data partitioning;streaming media;linear code;unequal error protection;multimedia communication;video transmission;channel conditions expanding window random linear codes data partitioned h 264 video transmission dvb h network al fec application layer forward error correction heterogeneous users ew rlc multimedia broadcast;on the fly;random codes;ip networks;random linear codes;dvb;encoding;electrical engineering electronics nuclear engineering	Rateless codes can be advantageously used to provide Application Layer Forward Error Correction (AL-FEC) with a distinct advantage that an infinite number of packets can be generated on the fly from the source packets. For heterogeneous users and/or variable channel conditions, significant adaptation features and improvement in the video quality can be achieved by partitioning the video data into different priority classes. Such partitioned data can be unequally protected using appropriate FEC scheme based on its contribution to video reconstruction. Expanding Window Random Linear Codes (EW RLC) are a simple unequal error protection fountain coding scheme which can adapt to the prioritized data transmission. In this paper, EW RLC are proposed for broadcasting the H.264/Advanced video coding partitioned with the data partitioning feature. The results show viability of the EW RLC for multimedia broadcast applications to suit different data rates and channel conditions.	code;dvb-h;data compression;forward error correction;h.264/mpeg-4 avc;on the fly;rlc circuit	Sajid Nazir;Vladimir Stankovic;Dejan Vukobratovic	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6116073	telecommunications;computer science;theoretical computer science;digital video broadcasting;computer network	DB	15.438734095363499	38.20048726917823	196416
0cb993315d965bd618f1cfa9825d24f3202dd8d7	design of reversible counter	reversible logic;garbage output;counter;thesai;ijacsa volume 5 issue 1;flip flop;quantum cost	This article presents a research work on the design and synthesis of sequential circuits and flip-flops that are available in digital arena; and describes a new synthesis design of reversible counter that is optimized in terms of quantum cost, delay and garbage outputs compared to the existing designs. We proposed a new model of reversible T flip-flop in designing reversible counter. Keywords—Flip-flop; Counter; Garbage Output; Reversible Logic; Quantum Cost	algorithm;asynchronous circuit;flops;flip-flop (electronics);quantum;reversible computing;ring counter;sequential logic	Md. Selim Al Mamun;B. K. Karmaker	2014	CoRR	10.14569/IJACSA.2014.050117	parallel computing;real-time computing;computer science;algorithm	EDA	16.5042772899686	45.19117388391878	196453
ecd410d3e91cc1f273e7f7cfc7cbaf81fc8d7f3e	stable sets of threshold-based cascades on the erdős-rényi random graphs	active vertex;neighboring vertex;size o;following reversible cascade;threshold-based cascade;round zero;stable set;nyi random graph;non-isolated vertex;irreversible cascade	Consider the following reversible cascade on the Erdős-Rényi random graph G(n, p). In round zero, a set of vertices, called the seeds, are active. For a given ρ ∈ ( 0, 1 ], a non-isolated vertex is activated (resp., deactivated) in round t ∈ Z if the fraction f of its neighboring vertices that were active in round t − 1 satisfies f ≥ ρ (resp., f < ρ). An irreversible cascade is defined similarly except that active vertices cannot be deactivated. A set of vertices, S, is said to be stable if no vertex will ever change its state, from active to inactive or vice versa, once the set of active vertices equals S. For both the reversible and the irreversible cascades, we show that for any constant > 0, all p ∈ [ (1 + ) (ln (e/ρ))/n, 1 ] and with probability 1−n−Ω(1), every stable set of G(n, p) has size O( ρn ) or n − O( ρn ).	active galactic nucleus;erdős number;erdős–rényi model;random graph;seeds (cellular automaton);vertex (geometry);vertex (graph theory)	Ching-Lueh Chang;Yuh-Dauh Lyuu	2011		10.1007/978-3-642-25011-8_8	combinatorics;discrete mathematics;topology;mathematics	Theory	21.43823848083347	34.079909485382764	196567
b2a9395213903467c7ccc6ed07abb9a95e4a4fcf	the use of digital signal processors in computer graphics	computer graphics;computer graphic;digital signal processor;procesador;processeur;grafico computadora;infographie;processor	The geometric processing stage in a computer graphic system is generally realised in VLSI or dedicated hardware, this paper describes the use of a Digital Signal Processor (DSP). By using a programmable DSP the hardware design effort is displaced into software. Using a single DSP in a co-processor configuration the speed of the vertex processing stage within the geometric processor is increased by a factor of five compared to a high performance microprocessor. The use of DSPs in a multi-processor system is also discussed.	computer graphics;coprocessor;digital signal processor;microprocessor;multiprocessing;very-large-scale integration	D. Bland;T. Bonnet	1988	Comput. Graph. Forum	10.1111/j.1467-8659.1988.tb00594.x	digital signal processor;computer architecture;parallel computing;media processor;computer hardware;computer science;computer graphics;computer graphics (images)	Arch	10.867181040949935	36.748182403658156	196876
18ea1a20a6bbd2aa30ea3cd53b3c38ea2b53f5d0	a parallel processing approach to image object labeling problems (abstract only)	shortest path;ada;time complexity;object oriented design;real time embedded system;interconnection network;artificial intelligent;artificial intelligence;simulation study;frame;inference engine;parallel inference engine;forward backward chaining;parallel processing;object detection;blackboard driven	The image object labeling problem plays an important role in the computer vision and artificial intelligence fields [5]. Several parallel processing approaches have been proposed to obtain solutions in various stages involved in this problem [4]. This paper describes a new parallel object labeling algorithm, based on multi-processor systems with a hypercube interconnection network. The system could be either SIMD or MIMD. The computation time complexity is discussed in this paper and compared with sequential methods and other parallel methods. The labeling problem can be briefly described as following. Let A = {<italic>a<subscrpt>1</subscrpt>, a<subscrpt>2</subscrpt>, …, a<subscrpt>n</subscrpt></italic>} be a set of objects detected from an image, and L = {<italic>l<subscrpt>1</subscrpt>, l<subscrpt>2</subscrpt>, …, l<subscrpt>m</subscrpt></italic>} be a set of labels, i.e., names or identifications of objects. L<subscrpt>i</subscrpt> is a subset of L that contains of all possible labels for the object a<subscrpt>i</subscrpt>. Consider an element <italic>L</italic> of L<subscrpt>1</subscrpt> X L<subscrpt>2</subscrpt> X … X L<subscrpt>n</subscrpt>, in which the labelings of all neighboring objects are compatible. Such <italic>L</italic> is called an “unambiguous labeling”, which is a logical explanation of the image. The objective of the labeling problem is to find all such <italic>L</italic>'s. A “consistent labeling” is a collection of subsets, L'<subscrpt>i</subscrpt> where L'<subscrpt>i</subscrpt> is a subset of L<subscrpt>i</subscrpt> for 1≤i≤n, in which, for i≠j, for each label l<subscrpt>i</subscrpt> in L'<subscrpt>i</subscrpt> there exists a label l<subscrpt>j</subscrpt> in L'<subscrpt>j</subscrpt> that is compatible with l<subscrpt>i</subscrpt>. Finding the greatest consistent labeling is a necessary step towards deriving unambiguous labelings. Computation time to detect an unambiguous labeling will be determined by the size of the consistent labeling in the first step. To find consistent labelings, we first define a compatibility matrix C = {<italic>C<subscrpt>pq</subscrpt></italic>}, where each row and each column correspond to a possible label for an object. The size of C is &Sgr;|L<subscrpt>i</subscrpt>|X&Sgr;|L<subscrpt>i</subscrpt>|. C<subscrpt>pq</subscrpt> = 1 if this pair of labelings is compatible, C<subscrpt>pq</subscrpt> = 0 if not. In regular relaxation approaches, the elements of <italic>C</italic> corresponding to objects which are not neighbors of each other are given value “1” or “unconstrained”. The submatrices corresponding to neighboring objects are examined and used to reduce the size of matrix “C”. In our approach, we need several steps to find the consistent labeling with the smallest size. In the first step, the submatrices corresponding to neighboring objects are examined. If some rows or columns are not compatible, we cross out these rows or columns. All pairs of objects are classified according to the length of the shortest path connecting them in the neighboring graph. The immediate neighbor pairs have path length 1. The second step will process the object pairs of path length 2. A labeling of an object pair of path length 2 is considered to be “compatible” if all labelings along all paths of length 2 are compatible. With this definition, the submatrices of C corresponding to object pairs with path length 2 can be used to reduce the size of matrix C significantly in the same way as in regular relaxation approaches for the submatrices corresponding to neighboring objects. The further processing of C can greatly simplified after the second step. Since each step in our approach involves a high number of matrix operations, a multi-processor machine with a hypercube interconnection network can effectively execute the computation and communication for our algorithm. A Gray sequence will be used in the hypercube network to eliminate unnecessary date movements. The similar technique has been used for image correlation computation [1,2,3]. A simulation study of the above labeling algorithm will be reported in a separate paper.	algorithm;artificial intelligence;column (database);computation;computer vision;interconnection;linear programming relaxation;mimd;multiprocessing;parallel computing;simd;semantic role labeling;shortest path problem;simulation;time complexity;unambiguous finite automaton	Zhixi Fang;Xiaobo Li	1987		10.1145/322917.323084	time complexity;frame;parallel processing;ada;edge-graceful labeling;computer science;artificial intelligence;theoretical computer science;object-oriented design;database;shortest path problem;inference engine;algorithm	AI	10.258941495223343	34.7328224219995	196962
de037e245b122035ee1481d345cced2014d1ff51	a very fast multiplication algorithm for vlsi implementation	o(log n ) time;recursive multiplication;regular layout	We present a simple recursive algorithm for multiplying two binary N-bit numbers in parallel O(log N) time. The simplicity of the design allows for a regular layout. The area requirement of this algorithm is comparable with that of much slower designs classically used in monolithic multipliers and in signal processing chips, hence the construction has definite practical impact.	multiplication algorithm;very-large-scale integration	Jean Vuillemin	1983	Integration	10.1016/0167-9260(83)90005-6	parallel computing;theoretical computer science;mathematics;algorithm	EDA	14.695189586760652	43.66747241948502	197121
2233ee602d4eb582d7a685142f9ebf4792cd397b	ping pong in dangerous graphs: optimal black hole search with pure tokens	black hole;mobile agents;distributed computing;autonomous robots;graph exploration;mobile agent;autonomous robot;dangerous graphs	We prove that, for the black hole search problem, the pure token model is computationally as powerful as the whiteboard model; furthermore the complexity is exactly the same. More precisely, we prove that a team of two asynchronous agents, each endowed with a single identical pebble (that can be placed only on nodes, and with no more than one pebble per node) can locate the black hole in an arbitrary network of known topology; this can be done with Θ(n logn) moves, where n is the number of nodes, even when the links are not FIFO.	black hole;fifo (computing and electronics);search problem	Paola Flocchini;David Ilcinkas;Nicola Santoro	2008		10.1007/978-3-540-87779-0_16	black hole;simulation;computer science;theoretical computer science;mobile agent;mathematics;distributed computing	Theory	17.83101510935065	34.2640809922421	197154
1a0e88f9632c566f67c107ae88af1131e2419b10	when can multi-agent rendezvous be executed in time linear in the diameter of a plane configuration?	random geometric graph;publikationer;konferensbidrag;robots;artiklar;rapporter;randomized algorithm;random bits;multi agent rendezvous	In multi-agent rendezvous it is naturally assumed that agents have a maximum speed of movement. In the absence of any distributed control issues, this imposes a lower bound on the time to rendezvous, for idealised point agents, proportional to the diameter of a configuration. Assuming bounded visibility, we consider Ω(n2 log n) points distributed independently and uniformly at random in a disc of radius n, so that the visibility graph is asymptotically almost surely (a.a.s.) connected. We allow three types of possible interaction between neighbors, which we term signalling, sweeping and tracking. Assuming any such interaction can be executed without significant delay, and assuming each point can generate random bits and has unlimited memory, we describe a randomized algorithm which a.a.s. runs in time O(n), hence in time proportional to the diameter, provided the number of points is o(n3). Several questions are posed for future work.	distributed control system;multi-agent system;randomized algorithm;visibility graph	Peter Hegarty;Anders Martinsson;Dmitry Zhelezov	2016		10.1145/2833312.2833457	robot;combinatorics;simulation;computer science;distributed computing;randomized algorithm;random geometric graph;algorithm	Theory	17.852529966408234	34.46285466739243	197257
bbaf0d455731cad5fffbcea49150741b8226619d	a 116 fps/74 mw heterogeneous 3d-media processor for 3-d display applications	algorithme de division;three dimensional displays energy consumption graphics decoding delay energy management environmental management turning runtime environment image processing;circuit decodeur;image tridimensionnelle;metodo adaptativo;evaluation performance;modulo calculus;material heterogeneo;representation graphique;interpolation;performance evaluation;image processing;integrated circuit;execution time;algoritmo de division;display equipment;3d display image processing heterogeneous 3d media processor 3d display applications 3d graphics ip stereo video decoder division free algorithm;real time;evaluacion prestacion;interpolacion;view interpolation;turn off;circuito integrado;affichage 3 dimensions;stereoscopy;methode adaptative;circuito desciframiento;function block;chip;division algorithm;video coding stereo image processing three dimensional displays;video coding;decoding circuit;mobile environment;consumo electricidad;media processor;performance improvement;parallel architectures;architecture parallele;three dimensional displays;adaptive method;stereo image processing;electric power consumption;power management;calcolo modulo;equipement affichage;grafo curva;stereoscopie;tridimensional image;temps execution;floating point;procesador;materiau heterogene;coma flotante;estereoscopia;parallel architecture;power consumption;equipo visualizacion;processeur;tiempo ejecucion;article;memory bandwidth;real time 3 d display;calcul de modulo;3d display;consommation electricite;pixel shader;3d graphics;processor;graphics;circuit integre;imagen tridimensional;heterogeneous material;virgule flottante;3d graphics pixel shader real time 3 d display view interpolation 3d display	In this paper, a heterogeneous 3D-media processor is presented, which supports all 3-D display applications by combining a 3-D display IP with a 3-D graphics IP and a stereo video decoder. For mobile environments, adaptive power management scheme is proposed, which saves power consumption up to 186 mW by turning off idle functional blocks based on a target application, a target performance, and the run-time ratio between different IPs. As a result, the minimum power consumption of the processor is only 15 mW, while the overall power consumption is 201 mW. As well as the reduction of power consumption, this work shows impressive performance improvement. The proposed fast modulo operators and adopted division-free algorithm reduces the critical latencies of 3-D display image processing. The proposed fast datapath with parallel architecture increase synthesis rate up to 116 fps which is 17 times faster than a previous work. In addition, reordered operation sequence fixes memory bandwidth regardless of the number of images to be produced. In the 3-D graphics IP and the decoding IP, redundant datapath are merged using an IEEE 754 compliant floating-point vector unit to save both chip area and power consumption, which even reduces the critical latency by 30%.	3d computer graphics;algorithm;cmos;clock gating;computation;datapath;image processing;media processor;memory bandwidth;modulo operation;parallel computing;power management;server message block;stereo display;synergy;video decoder;visual effects	Seok-Hoon Kim;Hong-Yun Kim;Young-Jun Kim;Kyusik Chung;Donghyun Kim;Lee-Sup Kim	2010	IEEE Journal of Solid-State Circuits	10.1109/JSSC.2009.2039532	embedded system;real-time computing;telecommunications;image processing;interpolation;computer science;operating system;computer graphics (images)	EDA	13.689803948674212	40.456110122075096	197740
d8dfc90a55d7f12f9e60df86ea98b4604f77fe91	diagnosis and repair in multiprocessor systems	hypercube;fiabilidad;reliability;rings;probability;multiprocessor;multiprocessor systems;hypercubes multiprocessor systems faulty processors sequential diagnosis diagnosis and repair probabilistic model rings grids meshes tori;performance;indexing terms;test;tori;ensayo;algorithme;probabilistic model;algorithm;essai;fault tolerant system;fault tolerant computing;diagnostic panne;fiabilite;fault diagnostic;diagnostico pana;diagnosis and repair;sistema tolerando faltas;modele probabiliste;probability fault tolerant computing multiprocessing systems;hypercubes;faulty processors;sequential diagnosis;systeme tolerant les pannes;multiprocessing systems;meshes;multiprocessing systems fault diagnosis sequential diagnosis system testing sequential analysis hypercubes performance evaluation topology councils;rendimiento;multiprocesador;grids;large classes;fault diagnosis;modelo probabilista;algoritmo;multiprocesseur;hipercubo	Diagnosis of multiprocessor systems in which faulty processors can be replaced by spares or repaired is known as sequential diagnosis. A generalization is considered of classical sequential diagnosis, referred to as diagnosis and repair, under a probabilistic model for the faults and test outcomes in a system. It is shown that correct diagnosis and repair of all faulty processors can be achieved with high probability in a large class of systems including, for example, rings, grids, meshes, tori, and hypercubes. These results show, without restrictive assumptions on the behavior of faulty processors, that correct diagnosis can be achieved in these widely used, low-degree systems when a fixed percentage of the processors in the system are faulty. >	multiprocessing	Douglas M. Blough;Andrzej Pelc	1993	IEEE Trans. Computers	10.1109/12.204793	parallel computing;real-time computing;computer science;distributed computing;hypercube;statistics	Embedded	23.692541126944082	44.392554773236895	197852
1fe84adf48eb0f9034a230c667580c0b48b9a56d	exploration of full hd media decoding on a software defined radio baseband processor	simd;deblocking filter full hd media decoding software defined radio baseband processor wireless communication applications sdr baseband processors parallel computation elements signal processing applications h 264 avc media decoding sdr baseband processor wireless baseband applications architecture cooptimizations motion compensation inverse integer transform;baseband processor;journal;video coding high definition video inverse transforms multimedia communication signal processing software radio;software radio;video coding;full hd;signal processing;期刊论文;multimedia communication;high definition video;inverse transforms;simd baseband processor h 264 avc decoder full hd;h 264 avc decoder	Recently, various specialized Software Defined Radio (SDR) baseband processors have been proposed for meeting the high performance and programmability requirements for emerging wireless communication applications. In order to support high throughput wireless communication, the SDR baseband processors typically employ many parallel computation elements, which may also be used for performing other signal processing applications. This paper explores the feasibility of performing complex media processing on such SDR baseband processors. Specifically, the full HD 1080p state-of-the-art H.264/AVC media decoding has been implemented onto a recent version of the ADRES based SDR baseband processor. Since the processor was originally designed exclusively for wireless baseband applications, algorithm and architecture co-optimizations are required to make the goal feasible. Following algorithm analysis, the computationally dominant tasks of the H.264/AVC, including the motion compensation, the intra prediction, the inverse integer transform, and the deblocking filter, have been selected to be mapped onto the ADRES. The experimental results show that, with limited architectural extensions, the ADRES based baseband processor achieves competitive performance efficiency, even compared with several processors that are specifically optimized for the media decoding application.	algorithm;analysis of algorithms;application domain;baseband processor;central processing unit;computation;deblocking filter;etsi satellite digital radio;h.264/mpeg-4 avc;intra-frame coding;motion compensation;parallel computing;requirement;signal processing;throughput	Chen Mei;Min Li;Peng Cao;Amir Amin;Chunshu Li;Jun Yang;Antoine Dejonghe;Liesbet Van der Perre;Longxing Shi;Sofie Pollin	2013	IEEE Transactions on Signal Processing	10.1109/TSP.2013.2270465	embedded system;computer vision;parallel computing;real-time computing;simd;computer science;signal processing;software-defined radio	Arch	11.574806640257215	40.268235500994074	198348
b8691ba6d0318365150fd61bbef1459e279260d6	current-source-sharing differential-pair circuits for a low-power fine-grain reconfigurable vlsi architecture	bit serial reconfigurable vlsi;current mode d latch;binary controlled series gating circuit bit serial reconfigurable vlsi current source sharing technique;low power fine grain reconfigurable vlsi architecture;programmable series gating differential pair circuit;current source sharing differential pair circuits;logic circuits;current mode;bit serial cell;multiple valued switch blocks;binary controlled current steering technique;binary controlled series gating circuit;low power;current source sharing technique;programmable circuits;multiple valued signaling;current mode circuits;vlsi;compact switch block;bit serial cell current source sharing differential pair circuits low power fine grain reconfigurable vlsi architecture multiple valued switch blocks binary logic modules bit serial reconfigurable vlsi multiple valued signaling compact switch block binary controlled current steering technique programmable series gating differential pair circuit low power arithmetic logic operations current source current mode d latch power consumption;low power arithmetic logic operations;current source;delay time;power consumption;multiple valued;vlsi current mode circuits logic circuits programmable circuits;high performance;binary logic modules;vlsi architecture	A bit-serial reconfigurable VLSI using multiple-valued switch blocks and binary logic modules is proposed. In a cell, multiple-valued signaling is utilized to implement a compact switch block. Binary-controlled current steering technique is introduced utilizing a programmable series-gating differential-pair circuit to implement high-performance low-power arithmetic logic operations such as an arbitrary two-variable binary logic operation and a full-adder sum. Moreover, current-source sharing between a series-gating differential-pair circuit and a current-mode D-latch is proposed to reduce the current source count to reduce power consumption. As a result, the power consumption and the delay time of the proposed bit-serial cell are reduced to 63% and 72%, respectively, in comparison with those of a previous multiple-valued bit-serial cell.	adder (electronics);boolean algebra;current source;differential signaling;flip-flop (electronics);low-power broadcasting;nx bit;serial communication;very-large-scale integration	Xu Bai;Michitaka Kameyama	2012	2012 IEEE 42nd International Symposium on Multiple-Valued Logic	10.1109/ISMVL.2012.13	electronic engineering;parallel computing;real-time computing;logic gate;logic family;computer science;programmable logic device;very-large-scale integration	Arch	12.989990416175086	46.33751275142009	198574
6f77bedf75a9bf6a68f46b1415eec33f260766ec	a mapping and memory chip hardware which provides symmetric reading/writing of horizontal and vertical lines	chip	This paper describes a mapping and memory chip hardware for enhancing the performance of an APA display. The approach describes a modification to the primary port of a quasi*twoported memory. This modification allows several contiguous horizontal or vertical bits to be read or written in one cycle. The number of bits that can be stored is given by the number of memory chips. The hardware modifications can be on or off chip, and if on chip, the chip can still be used as a conventional memory chip. Simple modifications to the hardware will support different screen sizes.		Daniel L. Ostapko	1984	IBM Journal of Research and Development	10.1147/rd.284.0393	chip;embedded system;electronic engineering;computer hardware;computer science	Arch	14.948791688363505	43.042174308047905	198680
988e7dc1d4ab3034948c34ee2e428c8dd858dde6	cycles embedding of twisted cubes	parallel computing;computers;graph theory;cycle embedding;systematics;hypercubes computer science systematics computers parallel processing;twisted cubes;hypercube networks graph theory;embedding cycles twisted cube tq n hypercube network host graph;interconnection networks;hypercubes;cycle embedding interconnection networks parallel computing twisted cubes;computer science;parallel processing;hypercube networks	The twisted cube TQn is an alternative to the popular hypercube network and some interesting properties of TQn were investigated recently. The problem of how to embed cycles into a host graph has attracted a great attention in recent years. However, there are few systematic methods proposed to generate the desired cycles in TQn. In this paper, we provide two kinds of systematic methods of embedding cycles into TQn.	cubes;cycle (graph theory);twisted	Pao-Lien Lai;Kao-Lin Hu;Hong-Chun Hsu	2013	2013 42nd International Conference on Parallel Processing	10.1109/ICPP.2013.128	parallel processing;folded cube graph;parallel computing;computer science;graph theory;theoretical computer science;distributed computing;systematics;hypercube	DB	23.53916326898804	35.90421917343992	198700
3f8180d580c365f63f9940e9c6529ec184440ca3	generating maximal domino patterns by cellular automata agents		Considered is a 2D cellular automaton with moving agents. The objective is to find agents controlled by a Finite State Program (FSP) that can form domino patterns. The quality of a formed pattern is measured by the degree of order computed by counting matching 3 × 3 patterns (templates). The class of domino patterns is defined by four templates. An agent reacts on its own color, the color in front, and whether it is blocked or not. It can change the color, move or not, and turn into any direction. Four FSP were evolved for multi-agent systems with 1, 2, 4 agents initially placed in the corners of the field. For a 12×12 training field the aimed pattern could be formed with a 100% degree of order. The performance was also high with other field sizes. Livelocks are avoided by using three different variants of the evolved FSP. The degree of order usually fluctuates after reaching a certain threshold, but it can also be stable, and the agents may show the termination by running in a cycle, or by stopping their activity.	cellular automaton;color;cycle (graph theory);fixed point (mathematics);identifier;maximal set;multi-agent system	Rolf Hoffmann;Dominique Désérable	2017		10.1007/978-3-319-62932-2_2	discrete mathematics;domino;template;cellular automaton;mathematics;multi-agent system;pattern formation	AI	21.554767350572128	34.11768146753205	198902
fea90e2fe720df0666b6cdf668256836026d3435	construction of check sets for algorithm-based fault tolerance	on line check;tolerancia falta;detection erreur;deteccion error;check set;a priori bound;multiprocessor systems;probleme np complet;algorithm based fault tolerance;bounded check size assumption;abft;indexing terms;algorithme;upper bound;algorithm;fault tolerant computing;detection defaut;minimum cardinality;computational complexity;fault detection;fault tolerance;borne inferieure;check sets;problema np completo;upper and lower bounds;multiprocessing systems;design problem;error detection;fault tolerance fault detection fault tolerant systems upper bound computer science hardware arithmetic polynomials parallel algorithms nasa;np hard;borne superieure;bounded check size model;deteccion imperfeccion;np hard check sets algorithm based fault tolerance error detection multiprocessor systems abft check set minimum cardinality bounded check size assumption bounded check size model fault detection design problem;fault tolerant computing multiprocessing systems computational complexity error detection;tolerance faute;lower bound;np complete problem;cota superior;defect detection;cota inferior;algoritmo	Algorithm-based fault tolerance (ABFT) is a popular approach to achieve fault and error detection in multiprocessor systems. The design problem for ABFT is concerned with the construction of a check set of minimum cardinality that detects a specified number of errors or faults. Previous work on this problem has assumed an a priori bound on the size of a check. We motivate and carry out an investigation of the problem without the bounded check size assumption. We establish upper and lower bounds on the number of checks needed to detect a given number of errors. The upper bounds are obtained through new schemes which are easy to implement, and the lower bounds are established using new types of arguments. These bounds are sharply different from those previously established under the bounded check size model. We also show that unlike error detection, the design problem for fault detection is NP-hard even for detecting only one fault. >	algorithm;fault tolerance	Dechang Gu;Daniel J. Rosenkrantz;S. S. Ravi	1994	IEEE Trans. Computers	10.1109/12.286298	parallel computing;computer science;theoretical computer science;mathematics;distributed computing;upper and lower bounds;algorithm	Visualization	14.55198209127018	35.68191689522835	199011
658dde608230a4c704728fb6b69463da0c132271	critical area computation for missing material defects in vlsi circuits	yield prediction;time complexity;electromagnetic field solvers;very large scale integrated;indexing terms;upper bound;size distribution;skin effect current distribution;cladding material;vlsi layout;medial axis;copper interconnect;voronoi diagram	We address the problem of computing critical area for missing material defects in a circuit layout. The extraction of critical area is the main computational problem in VLSI yield prediction. Missing material defects cause open circuits and are classi ed into breaks and via-blocks. Our approach is based on the L1 medial axis of polygons and the weighted L1 Voronoi diagram of segments. The critical area problem for both breaks and via-blocks is reduced to a weighted L1 Voronoi diagram of segments. This reduction results in a plane sweep algorithm to compute critical area in one pass. The time complexity is O(n log n) in the case of breaks and O(n log n +K) in the case of via-blocks, where n is the size of the input and K is bounded by the number of interacting vias (in practice K is small). The critical area computation assumes square defects and re ects all possible defect sizes following the D(r) = r 0=r 3 defect size distribution. The method is presented for rectilinear layouts.	apache axis;circuit diagram;computation;computational problem;interaction;medial graph;regular grid;software bug;sweep line algorithm;time complexity;very-large-scale integration;via (electronics);voronoi diagram	Evanthia Papadopoulou	2000		10.1145/332357.332390	time complexity;combinatorics;index term;voronoi diagram;medial axis;theoretical computer science;copper interconnect;mathematics;upper and lower bounds	Theory	22.153203612369094	39.083643653279296	199091
e4cac22392c7ff993ab48df3c77b0c8bbc5cb5f2	a cyclic algebra for the synthesis of ternary digital systems	electronic circuits;cyclic algebra cyclo decomposition cyclo power operation implementation synthesis;boolean functions;canonical form;implementation;sequential circuits;minimization methods;cyclo decomposition;synthesis;algebra;adders;digital systems;cyclic algebra;circuit synthesis;cyclo power operation;design methodology;combinational circuits	The original multivalued algebra defined by Post in 1921 [1] included the cyclization operation in its generating set. Up to now the algebra has not been used for the synthesis of ternary digital systems because of two shortcomings: no simple canonical forms are possible, and the cyclization is difficult to implement.	digital electronics	Jorge A. Santos;Héctor Arango;Manuel Pascual;G. Roing	1970	IEEE Transactions on Computers	10.1109/T-C.1970.222999	canonical form;electronic circuit;discrete mathematics;design methods;computer science;mathematics;sequential logic;combinational logic;boolean function;implementation;algorithm;adder;algebra	Visualization	18.99831807058278	45.41787873493956	199187
22a576af0dc792838ad163ba29e8b38e4e8db4cc	using carry-save adders in low-power multiplier blocks	block design;school of no longer in use;electronics and computer science;power saving;multiplying circuits;design algorithm carry save adder low power multiplier block fir filter power consumption direct form transposed direct form tree structure linear structure carry ripple adder;carry save adder;multiplying circuits adders low power electronics fir filters;low power;adders;fir filter;tree structure;low power electronics;adders algorithm design and analysis chromium energy consumption cmos logic circuits costs finite impulse response filter tree data structures integrated circuit interconnections iir filters;fir filters;power consumption	For a simple multiplier block FIR filter design, we compare the effects on power consumption of using direct versus transposed direct forms, tree versus linear structures and carry-save (CS) versus carry-ripple (CR) adders (for which multiplier block algorithms have been designed). We find that tree structures offer power savings, as expected, as does transposition in general but not always. Selective use of CS adders is shown to offer power savings provided that care is taken with their deployment. Our best result is with a direct form CWCS hybrid. The need for new multiplier-block design algorithms is identified.	algorithm;cs-cipher;filter design;finite impulse response;low-power broadcasting;ripple effect;software deployment;tree structure	V. A. Bartlett;Andrew G. Dempster	2001		10.1109/ISCAS.2001.922212	electronic engineering;computer hardware;computer science;engineering;electrical engineering;operating system;finite impulse response;mathematics	HCI	13.65262826130441	46.15169284490827	199449
cb785f971567a3b8c150afffe0d39edfce9675f1	the red-blue algorithm for dynamic programming on linear arrays	dynamic programming;parallel algorithms dynamic programming;concurrent computing;linear arrays;very large scale integration;linear array;dynamic program;ear;vlsi implementation red blue algorithm dynamic programming linear arrays redundant data technique shift registers;logic programming;control structure;heuristic algorithms;shift registers;linear programming;communication cost;redundant data technique;vlsi implementation;red blue algorithm;heuristic algorithms dynamic programming costs shift registers very large scale integration logic programming delay linear programming concurrent computing ear;parallel algorithms	A new algorithm for dynamic programming on linear arrays is presented which uses a single data path and runs twice as fast using less than half the memory locations of the best previously known algorithm. The algorithm employs a redundant data technique in which a permuted copy of the dynamic programming table is maintained to reduce communication costs. The simplified control structure and exclusive use of shift registers for storage (i.e. no RAM is required) make the algorithm particularly suitable for VLSI implementation. >	algorithm;dynamic programming	Kevin J. Rappoport	1994		10.1109/SPDP.1994.346142	algorithm design;parallel computing;real-time computing;concurrent computing;computer science;linear programming;theoretical computer science;dynamic programming;shift register;parallel algorithm;very-large-scale integration;programming language;control flow;logic programming	Vision	12.223938763739143	33.3038111031362	199453
367f130d7d5efaf5a3edf43653226fd61ed68dcc	search methods for tile sets in patterned dna self-assembly	reliable self assembly;pattern assembly;dna self assembly;tile assembly model;tilings;tile set synthesis;dna self assembly pattern assembly reliable self assembly tile assembly model tile set synthesis tilings	The Pattern self-Assembly Tile set Synthesis (PATS) problem, which arises in the theory of structured DNA self-assembly, is to determine a set of coloured tiles that, starting from a bordering seed structure, self-assembles to a given rectangular colour pattern. The task of finding minimum-size tile sets is known to be NP-hard. We explore several complete and incomplete search techniques for finding minimal, or at least small, tile sets and also assess the reliability of the solutions obtained according to Winfree’s kinetic Tile Assembly Model.	adder (electronics);answer set programming;approximation algorithm;boolean satisfiability problem;branch and bound;cmos;carbon cycle;circuit design;computation;computer cluster;computer simulation;dna computing;dna microarray;emoticon;experiment;fault tolerance;feedback;field effect (semiconductor);global serializability;heuristic (computer science);high- and low-level;integrated circuit layout design protection;interaction;inverter (logic gate);logic programming;nand gate;np-hardness;numerical analysis;polynomial;power inverter;procedural generation;search tree;self-assembly;semiconductor;solver;stable model semantics;sticky bit;tile-based video game;tiling window manager;time complexity;transistor	Mika Göös;Tuomo Lempiäinen;Eugen Czeizler;Pekka Orponen	2014	J. Comput. Syst. Sci.	10.1016/j.jcss.2013.08.003	combinatorics	Theory	18.797150559737887	43.66211303428071	199460
2984638090457cf02d82715d9834314448efa878	standards for graph algorithm primitives	linear algebra;graph theory;mathematics computing;mathematics computing application program interfaces graph theory linear algebra;application program interfaces;api graph algorithm primitives linear algebraic operations primitive building blocks;software standards graphs algorithms linear algebra;sparse matrices standards laboratories educational institutions vectors software algorithms;article	It is our view that the state of the art in constructing a large collection of graph algorithms in terms of linear algebraic operations is mature enough to support the emergence of a standard set of primitive building blocks. This paper is a position paper defining the problem and announcing our intention to launch an open effort to define this standard.	algorithm;emergence;graph theory;list of algorithms	Timothy G. Mattson;David A. Bader;Jonathan W. Berry;Aydin Buluç;Jack J. Dongarra;Christos Faloutsos;John Feo;John R. Gilbert;Joseph Gonzalez;Bruce Hendrickson;Jeremy Kepner;Charles E. Leiserson;Andrew Lumsdaine;David A. Padua;Stephen W. Poole;Steven P. Reinhardt;Michael Stonebraker;Steve Wallach	2013	2013 IEEE High Performance Extreme Computing Conference (HPEC)	10.1109/HPEC.2013.6670338	spqr tree;combinatorics;discrete mathematics;graph product;computer science;clique-width;graph theory;theoretical computer science;linear algebra;algebraic graph theory;mathematics;voltage graph;graph;graph algebra;programming language;algorithm;algebra;graph rewriting	DB	22.083642599133086	38.274201126212205	199919
