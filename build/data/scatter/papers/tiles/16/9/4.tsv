id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
d553bafa1434938c3e42eb2c46f06f6e6e08c454	multi-task vector field learning	biological patents;biomedical journals;text mining;europe pubmed central;citation search;citation networks;research articles;abstracts;open access;life sciences;clinical guidelines;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously and identifying the shared information among tasks. Most of existing MTL methods focus on learning linear models under the supervised setting. We propose a novel semi-supervised and nonlinear approach for MTL using vector fields. A vector field is a smooth mapping from the manifold to the tangent spaces which can be viewed as a directional derivative of functions on the manifold. We argue that vector fields provide a natural way to exploit the geometric structure of data as well as the shared differential structure of tasks, both of which are crucial for semi-supervised multi-task learning. In this paper, we develop multi-task vector field learning (MTVFL) which learns the predictor functions and the vector fields simultaneously. MTVFL has the following key properties. (1) The vector fields MTVFL learns are close to the gradient fields of the predictor functions. (2) Within each task, the vector field is required to be as parallel as possible which is expected to span a low dimensional subspace. (3) The vector fields from all tasks share a low dimensional subspace. We formalize our idea in a regularization framework and also provide a convex relaxation method to solve the original non-convex problem. The experimental results on synthetic and real data demonstrate the effectiveness of our proposed approach.	cluster hypothesis;computer multitasking;convex optimization;directional derivative;generalization (psychology);gradient;kerrison predictor;linear model;linear programming relaxation;matrix regularization;multi-task learning;nonlinear system;numerous;relaxation (approximation);relaxation (iterative method);semi-supervised learning;semiconductor industry;synthetic intelligence;manifold	Binbin Lin;Sen Yang;Chiyuan Zhang;Jieping Ye;Xiaofei He	2012	Advances in neural information processing systems		multi-task learning;text mining;medical research;computer science;data science;machine learning;data mining;mathematics;statistics	ML	23.652935751163096	-43.20055633855432	46207
63af72ed45ac6b6220706d824895809b7ad1434d	effect of linear biases in latent factor models on high-dimensional and sparse matrices from recommender systems		Latent factor (LF)-based models have been proven to be efficient in implementing recommender systems, owing to their ability to well represent high-dimensional and sparse matrices. While prior works focus on boosting both the prediction accuracy and computation efficiency of original LF model by adding linear biases to it, the individual and combinational effects by linear biases in such performance gain remains unclear. To address this issue, this work thoroughly investigates the effect of prior linear biases and training linear biases. We have investigated the parameter update rules and training processes of an LF model with different combinations of linear biases. Empirical validations are conducted on a high dimensional and sparse matrix from industrial systems currently in use. The results show that each linear bias does have positive/negative effects in the performance of an LF model. Such effects are partially data dependent; however, some linear biases like the global average can bring stable performance gain into an LF model. The theoretical and empirical results along with analysis provide guidance in designing the bias scheme in an LF model for recommender systems.	combinational logic;computation;linear logic;loss function;negativity (quantum mechanics);recommender system;sparse matrix;stochastic gradient descent;translation lookaside buffer	Ye Yuan;Xin Luo;Ming-Sheng Shang;Xin-Yi Cai	2017	2017 IEEE 14th International Conference on Networking, Sensing and Control (ICNSC)	10.1109/ICNSC.2017.8000141	recommender system;boosting (machine learning);sparse matrix;machine learning;data modeling;computation;computer science;matrix decomposition;factor analysis;artificial intelligence	ML	21.65781894847211	-44.096978004402686	46486
8889c9f190589e7eb612fb32d69ae7ece3b4a57b	comparison based learning from weak oracles		There is increasing interest in learning algorithms that involve interaction between human and machine. Comparison-based queries are among the most natural ways to get feedback from humans. A challenge in designing comparison-based interactive learning algorithms is coping with noisy answers. The most common fix is to submit a query several times, but this is not applicable in many situations due to its prohibitive cost and due to the unrealistic assumption of independent noise in different repetitions of the same query. In this paper, we introduce a new weak oracle model, where a non-malicious user responds to a pairwise comparison query only when she is quite sure about the answer. This model is able to mimic the behavior of a human in noise-prone regions. We also consider the application of this weak oracle model to the problem of content search (a variant of the nearest neighbor search problem) through comparisons. More specifically, we aim at devising efficient algorithms to locate a target object in a database equipped with a dissimilarity metric via invocation of the weak comparison oracle. We propose two algorithms termed WorcsI and Worcs-II (Weak-Oracle Comparisonbased Search), which provably locate the target object in a number of comparisons close to the entropy of the target distribution. While Worcs-I provides better theoretical guarantees, Worcs-II is applicable to more technically challenging scenarios where the algorithm has limited access to the ranking dissimilarity between objects. A series of experiments validate the performance of our proposed algorithms. Proceedings of the 21 International Conference on Artificial Intelligence and Statistics (AISTATS) 2018, Lanzarote, Spain. PMLR: Volume 84. Copyright 2018 by the author(s).	algorithm;artificial intelligence;baseline (configuration management);experiment;information retrieval;machine learning;malware;nearest neighbor search;oracle database;retransmission (data networks);search problem;security hacker	Ehsan Kazemi;Lin Chen;Sanjoy Dasgupta;Amin Karbasi	2018			machine learning;oracle;artificial intelligence;computer science;pairwise comparison;nearest neighbor search;interactive learning;ranking	DB	18.01985578586967	-50.88024463297723	46565
18d28245f10d8032e9e3d02ef9793c627a3a96fe	identifying agile waveforms with neural networks		Ahstract- With the advent of widespread digital technology, modern radar and communication systems have grown more complex and agile, rendering them difficult to adequately document and identify. The traditional solution of comparing incoming signals to a library of known waveforms is therefore becoming unworkable. The authors present two solutions to the problem of modern radio frequency (RF) waveform identification: a deep neural network and a recurrent neural network using GRUs. Both networks are designed to fuse together an arbitrary number of agile RF pulses and identify the emitter that produced them. Compared to a naïve DNN approach that simply averaged together pulses before classification, our solution adds a pre-projection step, which preserves information about sequential agility, even after averaging across pulses. After being trained against a set of 15 highly ambiguous emitters, the naïve DNN identified 52.2 % of test waveforms, our DNN with projection identified 72.3 %, and our RNN solution identified 84.8%.	agile software development;artificial neural network;deep learning;digital electronics;lambert's cosine law;naivety;radar;radio frequency;random neural network;recurrent neural network;waveform	Samuel A. Shapero;Austin B. Dill;Babafemi O. Odelowo	2018	2018 21st International Conference on Information Fusion (FUSION)	10.23919/ICIF.2018.8455370	artificial intelligence;computer science;rendering (computer graphics);machine learning;communications system;radar;artificial neural network;radio frequency;common emitter;recurrent neural network;waveform	EDA	21.576692916162994	-50.95010987179661	46628
ff9b522a788beadd4ab002d786372ffecde04061	a supervised fuzzy adaptive resonance theory with distributed weight update	unsupervised clustering;supervised learning;fuzzy art;clustering;classification accuracy;winner take all;adaptive resonance theory;scale dependence	The Fuzzy Adaptive Resonance Theory is an unsupervised clustering algorithm that solves stability plasticity dilemma The existing winner-take-all approach to updating weights in Fuzzy ART has two flaws: (i) it only updates one cluster while an input might belong to more than one cluster and (ii) the winner-take-all approach is costly in training time since it compares one weight to the input at a time We propose an algorithm that compares all weights to the input simultaneously and allows updating multiple matching clusters that pass the vigilance test To mitigate the effects of possibly updating clusters belonging to the wrong class we introduced weight scaling depending on the “closeness” of the weight to the input In addition, we introduced supervision to penalize the weight update for weights that have the wrong class The results show that our algorithm outperformed original Fuzzy ART in both classification accuracy and time consumption.	adaptive resonance theory	Aisha Yousuf;Yi Lu Murphey	2010		10.1007/978-3-642-13278-0_55	winner-take-all;fuzzy clustering;computer science;artificial intelligence;adaptive resonance theory;machine learning;data mining;mathematics;cluster analysis;supervised learning	Theory	13.61376109810085	-39.70520007311218	47238
01fdebac943edcfe75809eeace6a498cfd2a4b71	slowness as a proxy for temporal predictability: an empirical comparison		The computational principles of slowness and predictability have been proposed to describe aspects of information processing in the visual system. From the perspective of slowness being a limited special case of predictability we investigate the relationship between these two principles empirically. On a collection of real-world data sets we compare the features extracted by slow feature analysis (SFA) to the features of three recently proposed methods for predictable feature extraction: forecastable component analysis, predictable feature analysis, and graph-based predictable feature analysis. Our experiments show that the predictability of the learned features is highly correlated, and, thus, SFA appears to effectively implement a method for extracting predictable features according to different measures of predictability.	computation;experiment;feature extraction;information processing;simple features	Björn Weghenkel;Laurenz Wiskott	2018	Neural Computation	10.1162/neco_a_01070	artificial intelligence;machine learning;slowness;mathematics;pattern recognition (psychology);component analysis;feature extraction;predictability;information processing;special case;data set	ML	17.33343674688353	-48.94666779665216	47378
13823e5b16d709e49caed12c061fa26fe31067f2	deepgauge: multi-granularity testing criteria for deep learning systems		Deep learning (DL) defines a new data-driven programming paradigm that constructs the internal system logic of a crafted neuron network through a set of training data. We have seen wide adoption of DL in many safety-critical scenarios. However, a plethora of studies have shown that the state-of-the-art DL systems suffer from various vulnerabilities which can lead to severe consequences when applied to real-world applications. Currently, the testing adequacy of a DL system is usually measured by the accuracy of test data. Considering the limitation of accessible high quality test data, good accuracy performance on test data can hardly provide confidence to the testing adequacy and generality of DL systems. Unlike traditional software systems that have clear and controllable logic and functionality, the lack of interpretability in a DL system makes system analysis and defect detection difficult, which could potentially hinder its real-world deployment. In this paper, we propose DeepGauge, a set of multi-granularity testing criteria for DL systems, which aims at rendering a multi-faceted portrayal of the testbed. The in-depth evaluation of our proposed testing criteria is demonstrated on two well-known datasets, five DL systems, and with four state-of-the-art adversarial attack techniques against DL. The potential usefulness of DeepGauge sheds light on the construction of more generic and robust DL systems.	data-driven programming;deep learning;display resolution;experiment;faceted classification;neuron;programming paradigm;software bug;software deployment;software system;system analysis;test automation;test data;testbed;vulnerability (computing)	Miaozhang Zhu;Felix Juefei-Xu;Fuyuan Zhang;Jiyuan Sun;Minhui Xue;Bo Li;Chunyang Chen;Ting Su;Li Li;Yang Liu;Jianjun Zhao;Yadong Wang	2018		10.1145/3238147.3238202	rendering (computer graphics);software deployment;computer science;theoretical computer science;software system;programming paradigm;machine learning;deep learning;testbed;test data;interpretability;artificial intelligence	SE	18.59074220843705	-51.6063682537517	47471
ef05d5e3dde34e9d833b02b0c78846645c46d315	improving the efficiency of low-level decision making in robosoccer using boosted svm	adaptive boosting;agent modeling;test data generation;support vector machine;neural network	Decision making in a multi-agent environment has continued to be one of the most formidable challenges for AI researchers.  Increasing the efficiency of predictors is an essential task, especially in a substrate like robosoccer where a misclassification  can cost dearly. It is also necessary for the agent to perform well, irrespective of the nature of testing data, generated  in a markovian fashion. For this reason, we apply AdaBoost (Adaptive Boosting) algorithm onto support vector machines which  helps us achieve a better generalization performance than normal Support Vector Machines (SVM) and a better efficiency when  compared to other adaboosted neural networks. To illustrate the concept, we propose a highly efficient decision predictor  for low-level behavior in robosoccer using Adaboosted SVM (AdSVM). Through experiments, we have proved that the proposed agent  model has outwitted existing neural networks and SVM in classifying two-class data of any nature in a multi-agent environment  like robosoccer.  	gradient boosting	Pravin Chandrasekaran;R. Muthucumaraswamy	2008		10.1007/978-3-540-79355-7_6	machine learning;pattern recognition;data mining	ML	12.113884903398375	-40.625320165263595	47522
055fafc9a29e4a64be40ab63dd198290442c4b1c	robust sparse rank learning for non-smooth ranking measures	performance guarantee;learning model;information retrieval;truncated gradient;gradient descent;loss function;cumulant;learn to rank;learning to rank;rsrank	Recently increasing attention has been focused on directly optimizing ranking measures and inducing sparsity in learning models. However, few attempts have been made to relate them together in approaching the problem of learning to rank. In this paper, we consider the sparse algorithms to directly optimize the Normalized Discounted Cumulative Gain (NDCG) which is a widely-used ranking measure. We begin by establishing a reduction framework under which we reduce ranking, as measured by NDCG, to the importance weighted pairwise classification. Furthermore, we provide a sound theoretical guarantee for this reduction, bounding the realized NDCG regret in terms of a properly weighted pairwise classification regret, which implies that good performance can be robustly transferred from pairwise classification to ranking. Based on the converted pairwise loss function, it is conceivable to take into account sparsity in ranking models and to come up with a gradient possessing certain performance guarantee. For the sake of achieving sparsity, a novel algorithm named RSRank has also been devised, which performs L1 regularization using truncated gradient descent. Finally, experimental results on benchmark collection confirm the significant advantage of RSRank in comparison with several baseline methods.	algorithm;baseline (configuration management);benchmark (computing);database normalization;gradient descent;learning to rank;loss function;matrix regularization;regret (decision theory);sparse matrix	Zhengya Sun;Tao Qin;Qing Tao;Jue Wang	2009		10.1145/1571941.1571987	gradient descent;mathematical optimization;computer science;machine learning;pattern recognition;ranking svm;learning to rank;statistics;cumulant;loss function	ML	22.998068177252353	-41.98606402134037	47742
766456836c7271bbcb5565f98c585caff56011a7	scalable training with approximate incremental laplacian eigenmaps and pca	measurement;performance;algorithms;experimentation	The paper describes the approach, the experimental settings, and the results obtained by the proposed methodology at the ACM Yahoo! Multimedia Grand Challenge. Its main contribution is the use of fast and efficient features with a highly scalable semi-supervised learning approach, the Approximate Laplacian Eigenmaps (ALEs), and its extension, by computing the test set incrementally for learning concepts in time linear to the number of images (both labelled and unlabelled). A combination of two local visual features combined with the VLAD feature aggregation method and PCA is used to improve the efficiency and time complexity. Our methodology achieves somewhat better accuracy compared to the baseline (linear SVM) in small training sets, but improves the performance as the training data increase. Performing ALE fusion on a training set of 50K/concept resulted in a MiAP score of 0.4223, which was among the highest scores of the proposed approach.	approximation algorithm;baseline (configuration management);laplacian matrix;nonlinear dimensionality reduction;principal component analysis;scalability;semi-supervised learning;semiconductor industry;supervised learning;test set;time complexity	Eleni Mantziou;Symeon Papadopoulos;Yiannis Kompatsiaris	2013		10.1145/2502081.2508124	performance;computer science;machine learning;pattern recognition;data mining;measurement	Vision	17.852911318671726	-46.603893379386044	47959
e79731819ceb4bc0ae51b8e09533e0739e9d9d27	transfer learning for survival analysis via efficient l2,1-norm regularized cox regression	analytical models;standards;prediction algorithms;hazards;regularization;learning systems;regression;high dimensional data;survival analysis;transfer learning;predictive models;data models	"""In survival analysis, the primary goal is to monitor several entities and model the occurrence of a particular event of interest. In such applications, it is quite often the case that the event of interest may not always be observed during the study period and this gives rise to the problem of censoring which cannot be easily handled in the standard regression approaches. In addition, obtaining sufficient labeled training instances for learning a robust prediction model is a very time consuming process and can be extremely difficult in practice. In this paper, we propose a transfer learning based Cox method, called Transfer-Cox, which uses auxiliary data to augment learning when there are insufficient amount of training examples. The proposed method aims to extract """"useful"""" knowledge from the source domain and transfer it to the target domain, thus potentially improving the prediction performance in such time-to-event data. The proposed method uses the l_2,1-norm penalty to encourage multiple predictors to share similar sparsity patterns, thus learns a shared representation across source and target domains, potentially improving the model performance on the target task. To speedup the computation, we apply the screening approach and extend the strong rule to sparse survival analysis models in multiple high-dimensional censored datasets. We demonstrate the performance of the proposed transfer learning method using several synthetic and high-dimensional microarray gene expression benchmark datasets and compare with other related competing state-of-the-art methods. Our results show that the proposed screening approach significantly improves the computational efficiency of the proposed algorithm without compromising the prediction performance. We also demonstrate the scalability of the proposed approach and show that the time taken to obtain the results is linear with respect to both the number of instances and features."""	algorithm;algorithmic efficiency;benchmark (computing);censoring (statistics);computation;dhrystone;entity;karush–kuhn–tucker conditions;microarray;neural coding;overfitting;proportional hazards model;scalability;sparse matrix;speedup	Yan Li;Lu Wang;Jie Jin Wang;Jieping Ye;Chandan K. Reddy	2016	2016 IEEE 16th International Conference on Data Mining (ICDM)	10.1109/ICDM.2016.0034	semi-supervised learning;data modeling;multi-task learning;regularization;econometrics;regression;prediction;hazard;transfer of learning;computer science;machine learning;data mining;predictive modelling;survival analysis;statistics;clustering high-dimensional data	ML	18.718436217037798	-43.561779886346564	47969
20e9b2df78abc9cae5a4f453311b196e9a204b6c	pca-based algorithm for feature score measures ensemble construction.		Feature filtering algorithms are commonly used in feature selection for high-dimensional datasets due to their simplicity and efficacy. Each of these algorithms has its own strengths and weaknesses. Ensemble of different ranking methods is a way to provide a stable and efficacious ranking algorithm. We propose a PCA-based algorithm for filter ranking algorithms ensemble. We compared this algorithm with four other rank aggregation algorithms on five different datasets used in the NIPS-2003 feature selection challenge. We evaluated the stability of the resulting rankings and the AUC score for four classifiers learnt on resulting feature sets. The proposed method has shown better stability and above-average efficacy.	algorithm;feature selection;feature vector	Andrey Filchenkov;Vladislav Dolganov;Ivan Smetannikov	2015				AI	11.950495379937326	-44.793998448292115	48138
855ccae643893a510ffee595fe4a9fe8e8e040eb	hyperkernel construction for support vector machines	kernel;support vector machines;training;construction industry;kernel function;support vector machines combinatorial mathematics polynomials;hyperkernel;polynomials;tuning;machine learning;polynomial combination;combinatorial construction;support vector machine;combinatorial mathematics;support vector machines kernel polynomials machine learning spline neural networks computer science application software information geometry data mining;polynomial combination support vector machines hyperkernel;polynomial combination hyperkernel construction support vector machines combinatorial construction;hyperkernel construction	Construction of kernel functions is crucial for research and application of support vector machines (SVM). In this paper, we propose a combinatorial construction of hyperkernel functions for SVM. We first analyze the under and over-learning phenomena of common kernel functions. Then, we construct hyperkernel function with a polynomial combination of common kernels, and prove the Mercer condition of the hyperkernel. Finally, we experiment both on simulation and benchmark data to demonstrate the performance of hyperkernel for SVM. The theoretical proofs and experimental results illuminate the validity and feasibility of hyperkernel.	benchmark (computing);database tuning;kernel (operating system);model selection;polynomial;simulation;support vector machine	Lei Jia;Shizhong Liao	2008	2008 Fourth International Conference on Natural Computation	10.1109/ICNC.2008.156	least squares support vector machine;mathematical optimization;machine learning;pattern recognition;mathematics;polynomial kernel	Robotics	22.97972445857655	-40.021651385927086	48227
8d66836f6cc5a8d8f618fb4e1ae5fcaeb7f99d4a	ensemble learning and hierarchical data representation for microarray classification	biology computing;classifiers hierarchical data representation microarray data classification active research field omics data ensemble learning algorithm classification framework ensemble vote ensemble thinning algorithm ensemble cohort definition nonexpert notion thinning process public datasets microarray quality control maqc;quality control biology computing classification data handling lab on a chip learning artificial intelligence;prediction algorithms accuracy gene expression training quality control protocols merging;classification;conference report;lab on a chip;data handling;learning artificial intelligence;quality control	The microarray data classification is an open and active research field. The development of more accurate algorithms is of great interest and many of the developed techniques can be straightforwardly applied in analyzing different kinds of omics data. In this work, an ensemble learning algorithm is applied within a classification framework that already got good predictive results. Ensemble techniques take individual experts, (i.e. classifiers), to combine them to improve the individual expert results with a voting scheme. In this case, a thinning algorithm is proposed which starts by using all the available experts and removes them one by one focusing on improving the ensemble vote. Two versions of a state of the art ensemble thinning algorithm have been tested and three key elements have been introduced to work with microarray data: the ensemble cohort definition, the nonexpert notion, which defines a set of excluded expert from the thinning process, and a rule to break ties in the thinning process. Experiments have been done on seven public datasets from the Microarray Quality Control study, MAQC. The proposed key elements have shown to be useful for the prediction performance and the studied ensemble technique shown to improve the state of the art results by producing classifiers with better predictions.	algorithm;data (computing);ensemble learning;hierarchical database model;microarray;microelectronics and computer technology corporation;omics;performance;thinning	Mattia Bosio;Pau Bellot;Philippe Salembier;Albert Oliveras-Vergés	2013	13th IEEE International Conference on BioInformatics and BioEngineering	10.1109/BIBE.2013.6701647	quality control;lab-on-a-chip;biological classification;computer science;bioinformatics;data science;machine learning;group method of data handling;data mining;ensemble learning	Robotics	13.26142428693408	-41.55498736761349	48269
7a9225eb7ae4843dc2e6b329b4875461adebe54b	new results on combination methods for boosting ensembles	ensemble method;adaptive boosting;neural network	The design of an ensemble of neural networks is a procedure that can be decomposed into two steps. The first one consists in generating the ens emble, i.e., training the networks with significant differences. The second one c sists in combining properly the information provided by the networks. Adaptive Boosting, one of the best performing ensemble methods, has been studied and im proved by some authors including us. Moreover, Adaboostand itsvariantsuse a specific combiner based on the error of the networks. Unfortunately, any deep study on combining this kind of ensembles has not been done yet. In this paper, we study the performance of some important ensemble combiners on ensembles previously trained withAdaboostandAveboost . The results show that an extra increase of performance can be provided by applying the appropriate combiner.	adaptive grammar;artificial neural network;diplexer;ensemble kalman filter;ensemble learning;power dividers and directional couplers	Joaquín Torres-Sospedra;Carlos Hernández-Espinosa;Mercedes Fernández-Redondo	2008		10.1007/978-3-540-87536-9_30	computer science;machine learning;pattern recognition;data mining;ensemble learning;artificial neural network	ML	12.809630065792811	-40.937048721984375	48529
75b987f86af2bc7f68edc45be240dd30e1ef2699	sampling algorithms to handle nuisances in large-scale recognition		Author(s): Karianakis, Nikolaos | Advisor(s): Soatto, Stefano | Abstract: Convolutional neural networks (CNNs) have risen to be the de-facto paragon for detecting the presence of objects in a scene, as portrayed by an image. CNNs are described as being approximately to nuisance transformations such as planar translation, both by virtue of their convolutional architecture and by virtue of their approximation properties that, given sufficient parameters and training data, could in principle yield discriminants that are insensitive to nuisance transformations of the data. The fact that contemporary deep convolutional architectures appear very effective in classifying images as containing a given object regardless of its position, scale, and aspect ratio in large-scale benchmarks suggests that the network can effectively manage such nuisance variability. We conduct an empirical study and show that, contrary to popular belief, at the current level of complexity of convolutional architectures and scale of the data sets used to train them, CNNs are not very effective at marginalizing nuisance variability.This discovery leaves researchers the choice of investing more effort in the design of models that are less sensitive to nuisances or designing better region proposal algorithms in an effort to predict where the objects of interest lie and center the model around these regions. In this thesis steps towards both directions are made. First, we introduce DSP-CNN, which deploys domain-size pooling in order to transform the neural networks to be scale invariant in the convolutional operator level. Second, motivated by our empirical analysis, we propose novel sampling and pruning techniques for region proposal schemes that improve the end-to-end performance in large-scale classification, detection and wide-baseline correspondence to state-of-the-art levels. Additionally,since a proposal algorithm involves the design of a classifier, whose results are to be fed to another classifier (a Category CNN), it seems natural to leverage on the latter to design the former. Thus, we introduce a method that leverages on filters learned in the lower layers of CNNs to design a binary boosting classifier for generating class-agnostic proposals. Finally, we extend sampling over time by designing a temporal, hard-attention layer which is trained with reinforcement learning, with application in video sequences for person re-identification.	algorithm	Nikolaos Karianakis	2017			boosting (machine learning);deep learning;artificial neural network;convolutional neural network;machine learning;classifier (linguistics);reinforcement learning;invariant (mathematics);artificial intelligence;gibbs sampling;computer science	Vision	24.419945964602764	-50.39081580899466	48611
9c1b26626f46a6a43e699e3367efa60e78717129	analysis of temporal high-dimensional gene expression data for identifying informative biomarker candidates	margin;multivariate time series data high dimensional temporal data feature selection margin;temporal data;genetics;uncertainty gene expression optimization time series analysis time measurement vectors linear programming;medical computing;fixed point gradient descent method temporal high dimensional gene expression data analysis informative biomarker candidate identification individual health status prediction clinical application temporal microarray data margin based feature selection filter machine learning data distribution temporal margin based feature selection nearest neighbor iteration;data analysis;high dimensional;pattern recognition;gradient methods;feature selection;pattern recognition data analysis genetics gradient methods medical computing;multivariate time series data	Identifying informative biomarkers from a large pool of candidates is the key step for accurate prediction of an individual's health status. In clinical applications traditional static feature selection methods that flatten the temporal data cannot be directly applied since the patient's observed clinical condition is a temporal multivariate time series where different variables can capture various stages of temporal change in the patient's health status. In this study, in order to identify informative genes in temporal microarray data, a margin based feature selection filter is proposed. The proposed method is based on well-established machine learning techniques without any assumptions about the data distribution. The objective function of temporal margin-based feature selection is defined to maximize each subject's temporal margin in its own relevant subspace. In the objective function, the uncertainty in calculating nearest neighbors is taken into account by considering the change in feature weights in each iteration. A fixed-point gradient descent method is proposed to solve the formulated objective function. The experimental results on both synthetic and real data provide evidence that the proposed method can identify more informative features than the alternatives that flatten the temporal data in advance.	feature selection;gradient descent;information;iteration;loss function;machine learning;microarray;optimization problem;synthetic intelligence;time series	Qiang Lou;Zoran Obradovic	2012	2012 IEEE 12th International Conference on Data Mining	10.1109/ICDM.2012.92	margin;computer science;machine learning;pattern recognition;data mining;temporal database;data analysis;feature selection;statistics	ML	14.414602387931248	-50.03270882621405	48649
21940188396e1ab4f47db30cf6fd289eee2a930a	revisiting batch normalization for practical domain adaptation		Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study (Tommasi et al. 2015) shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without finetuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives stateof-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.	archive;artificial neural network;computer vision;deep learning;domain adaptation;object detection	Yanghao Li;Naiyan Wang;Jianping Shi;Jiaying Liu;Xiaodi Hou	2016	CoRR		speech recognition;computer science;artificial intelligence;machine learning;pattern recognition;statistics	ML	21.959611278507513	-50.547821455673336	48737
828707be918f9c1fd08549eb06b8d02324581a57	novel multi-class feature selection methods using sensitivity analysis of posterior probabilities	probability;multiclass support vector machine learning;support vector machines;support vector machines feature extraction filtering theory learning artificial intelligence pattern classification probability sensitivity analysis;training;multi class classification feature selection feature ranking support vector machines;posterior probability;data mining;feature space;multiclass feature selection method;error analysis;sensitivity;multi class classification;sensitivity analysis;feature extraction;multiclass feature based sensitivity;pattern classification;standard filtering method;feature ranking criteria;sensitivity analysis support vector machines support vector machine classification benchmark testing mechanical engineering nervous system aggregates filtering diversity reception biological neural networks;posterior probabilities;feature selection;approximation methods;recursive feature elimination;support vector machine;probabilistic logic;feature ranking;learning artificial intelligence;filtering theory;standard filtering method multiclass feature selection method sensitivity analysis posterior probabilities multiclass support vector machine learning feature ranking criteria multiclass feature based sensitivity recursive feature elimination method;recursive feature elimination method	Novel feature-selection methods are proposed for multi-class support-vector-machine (SVM) learning. They are based on two new feature-ranking criteria. Both criteria, collectively termed multi-class feature-based sensitivity of posterior probabilities (MFSPP), evaluate the importance of a feature by computing the aggregate value, over the feature space, of the absolute difference of the probabilistic outputs of the multi-class SVM with and without the feature. In their original form, the criteria are computationally expensive and three approximations, MFSPP1-MFSPP3, are then proposed. In a carefully controlled experimental study, all these three approximations are tested on various artificial and benchmark datasets. Results show that they outperform the multi-class versions of support-vector-machine recursive feature-elimination method (SVM-RFE) and other standard filtering methods, with one of the three proposed approximations having a slight edge over the other two. Based on the experiments, the advantage of the proposed methods is particularly significant when training dataset is sparse.	aggregate data;analysis of algorithms;approximation;benchmark (computing);experiment;feature selection;feature vector;multiclass classification;recursion;sparse matrix;support vector machine	Kai Quan Shen;Chong Jin Ong;Xiao Ping Li;Einar P. V. Wilder-Smith	2008	2008 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2008.4811431	support vector machine;computer science;machine learning;pattern recognition;data mining;posterior probability;feature selection	Vision	18.719411349076196	-39.203649857230175	48807
f620ec7bc0632be5518718cb81e2bfb57c81e950	hubs in space: popular nearest neighbors in high-dimensional data	high dimensional data;nearest neighbor	Different aspects of the curse of dimensionality are known t o present serious challenges to various machine-learning methods and tasks. This paper explores a n w spect of the dimensionality curse, referred to ashubness , that affects the distribution of k-occurrences: the number of times a point appears among the k nearest neighbors of other points in a data set. Through theo retical and empirical analysis involving synthetic and real data sets we show t at under commonly used assumptions this distribution becomes considerably skewed as dimensio nality increases, causing the emergence of hubs, that is, points with very highk-occurrences which effectively represent “popular” neare st neighbors. We examine the origins of this phenomenon, showi ng that it is an inherent property of data distributions in high-dimensional vector space, di scuss its interaction with dimensionality reduction, and explore its influence on a wide range of machin e-learning tasks directly or indirectly based on measuring distances, belonging to supervis ed, semi-supervised, and unsupervised learning families.	curse of dimensionality;dimensionality reduction;emergence;k-nearest neighbors algorithm;machin-like formula;machine learning;semi-supervised learning;semiconductor industry;synthetic intelligence;unsupervised learning	Milos Radovanovic;Alexandros Nanopoulos;Mirjana Ivanovic	2010	Journal of Machine Learning Research		curse of dimensionality;computer science;machine learning;pattern recognition;data mining;mathematics;k-nearest neighbors algorithm;clustering high-dimensional data	ML	22.37288476353296	-42.10986993768852	48957
2d2211ec1a659c570204e089ac6eb0c950adad91	multi-view representative and informative induced active learning		Most existing active learning methods often manually label samples and train models with labeled data in an iterative way. Unfortunately, at the early stage of the experiment, few labeled data are available, hence, selecting the most valuable data points to label is necessary and important. To this end, we propose a novel method, called Multi-view Representative and Informative-induced Active Learning (MRI-AL), which selects samples of both representativeness and informativeness with the help of complementarity of multiple views. Specifically, subspace reconstruction with structure sparsity technique is employed to ensure the selected samples to be representative, while the global similarity constraint guarantees the informativeness of the selected samples. The proposed method is solved efficiently by alternating direction method of multipliers (ADMM). We empirically show that our method outperforms existing early experimental design approaches.	information	Huaxi Huang;Changqing Zhang;Qinghua Hu;Pengfei Zhu	2016		10.1007/978-3-319-42911-3_12	machine learning;active learning;computer science;artificial intelligence;pattern recognition;labeled data;representativeness heuristic;data point;complementarity (molecular biology);subspace topology	ML	20.381683272786596	-43.219326323591176	49097
30fd1363fa14965e3ab48a7d6235e4b3516c1da1	a deep semi-nmf model for learning hidden representations		Semi-NMF is a matrix factorization technique that learns a low-dimensional representation of a dataset that lends itself to a clustering interpretation. It is possible that the mapping between this new representation and our original features contains rather complex hierarchical information with implicit lower-level hidden attributes, that classical one level clustering methodologies can not interpret. In this work we propose a novel model, Deep Semi-NMF, that is able to learn such hidden representations that allow themselves to an interpretation of clustering according to different, unknown attributes of a given dataset. We show that by doing so, our model is able to learn low-dimensional representations that are better suited for clustering, outperforming Semi-NMF, but also other NMF variants.	algorithm;approximation;cluster analysis;expectation propagation;experiment;j. alan george;multi-source;non-negative matrix factorization;semiconductor industry;signal processing;spatial variability;speech recognition	George Trigeorgis;Konstantinos Bousmalis;Stefanos P. Zafeiriou;Björn W. Schuller	2014			fuzzy clustering;machine learning;pattern recognition;data mining;mathematics;cluster analysis;brown clustering	ML	22.097806869401474	-44.27654864188545	49224
6cdb7d2fa7d6a9deff8cf4240407c2a6174bab87	reducing the dimensionality of dissimilarity space embedding graph kernels	classification algorithm;edit distance;embedded graph;data representation;feature vector;dimensionality reduction;graph based representation;pattern classification;dimensional reduction;graph kernel;dissimilarity space embedding	Graphs are a convenient representation formalism for structured objects, but they suffer from the fact that only a few algorithms for graph classification and clustering exist. In this paper a new approach to graph classification by dissimilarity space embedding is proposed. This approach, which is in fact a new graph kernel, allows us to apply advanced classification tools while retaining the high representational power of graphs. The basic idea of the proposed graph kernel is to view the edit distances of a given graph g to a set of training graphs as a vectorial description of g. Once a graph has been transformed into a vector, different dimensionality reduction algorithms are applied such that redundancies are eliminated. To this reduced vectorial data representation any pattern classification algorithms available for feature vectors can be applied. Through various experiments it is shown that the proposed dissimilarity space embedding graph kernel outperforms conventional classification algorithms applied in the original graph domain.		Kaspar Riesen;Horst Bunke	2009	Eng. Appl. of AI	10.1016/j.engappai.2008.04.006	lattice graph;graph power;graph embedding;edit distance;feature vector;graph bandwidth;null graph;graph property;computer science;clique-width;machine learning;pattern recognition;graph kernel;voltage graph;distance-hereditary graph;external data representation;graph;complement graph;graph operations;strength of a graph;dimensionality reduction	AI	20.924472914011695	-44.2255075502269	49312
6214a8d06f4aae008c644da2194131623a62c4b9	minimum deviation distribution machine for large scale regression		In this paper, by introducing the statistics of training data into support vector regression (SVR), we propose a minimum deviation distribution regression (MDR). Rather than just minimizing the structural risk, MDR also minimizes both the regression deviation mean and the regression deviation variance, which is able to deal with the different distribution of boundary data and noises. The formulation of minimizing the first and second order statistics in MDR leads to a strongly convex quadratic programming problem (QPP). An efficient dual coordinate descend algorithm is adopted for small sample problem, and an average stochastic gradient algorithm for large scale one. Both theoretical analysis and experimental results illustrate the efficiency and effectiveness of the proposed method.	algorithm;gradient;memory data register;quadratic programming;support vector machine	Ming-Zeng Liu;Yuan-Hai Shao;Zhen Wang;Chun-Na Li;Wei-Jie Chen	2018	Knowl.-Based Syst.	10.1016/j.knosys.2018.02.002	support vector machine;machine learning;artificial intelligence;quadratic programming;minimum deviation;statistics;convex function;computer science;order statistic;training set	ML	21.19230690709332	-39.74671318299926	49364
69092affc3461a38eb05cf7982f104eb30b0492c	mitigating evasion attacks to deep neural networks via region-based classification		Deep neural networks (DNNs) have transformed several artificial intelligence research areas including computer vision, speech recognition, and natural language processing. However, recent studies demonstrated that DNNs are vulnerable to adversarial manipulations at testing time. Specifically, suppose we have a testing example, whose label can be correctly predicted by a DNN classifier. An attacker can add a small carefully crafted noise to the testing example such that the DNN classifier predicts an incorrect label, where the crafted testing example is called adversarial example. Such attacks are called evasion attacks. Evasion attacks are one of the biggest challenges for deploying DNNs in safety and security critical applications such as self-driving cars.  In this work, we develop new DNNs that are robust to state-of-the-art evasion attacks. Our key observation is that adversarial examples are close to the classification boundary. Therefore, we propose region-based classification to be robust to adversarial examples. Specifically, for a benign/adversarial testing example, we ensemble information in a hypercube centered at the example to predict its label. In contrast, traditional classifiers are point-based classification, i.e., given a testing example, the classifier predicts its label based on the testing example alone. Our evaluation results on MNIST and CIFAR-10 datasets demonstrate that our region-based classification can significantly mitigate evasion attacks without sacrificing classification accuracy on benign examples. Specifically, our region-based classification achieves the same classification accuracy on testing benign examples as point-based classification, but our region-based classification is significantly more robust than point-based classification to state-of-the-art evasion attacks.	adversary (cryptography);artificial intelligence;artificial neural network;autonomous car;computer vision;deep learning;evasion (network security);mnist database;naive bayes classifier;natural language processing;neural networks;speech recognition	Xiaoyu Cao;Neil Zhenqiang Gong	2017		10.1145/3134600.3134606	computer science;computer security;artificial neural network;machine learning;adversarial machine learning;mnist database;artificial intelligence	AI	19.1863120359291	-51.25055271074178	49373
7311ea4261669d517f28dbaed82d7afcb77d6cab	preliminary studies on a large face database		We perform preliminary studies on a large longitudinal face database MORPH-II, which is a benchmark dataset in the field of computer vision and pattern recognition. First, we summarize the inconsistencies in the dataset and introduce the steps and strategy taken for cleaning. The potential implications of these inconsistencies on prior research are introduced. Next, we propose a new automatic subsetting scheme for evaluation protocol. It is intended to overcome the unbalanced racial and gender distributions of MORPH-II, while ensuring independence between training and testing sets. Finally, we contribute a novel global framework for age estimation that utilizes posterior probabilities from the race classification step to compute a racecomposite age estimate. Preliminary experimental results on MORPH-II are presented.	benchmark (computing);catherine browman;computer vision;data validation;design of experiments;dimensionality reduction;feature extraction;pattern recognition;plasma cleaning;procedural generation;random seed;statistical classification;unbalanced circuit	Benjamin Yip;Garrett Bingham;Katherine Kempfert;Jonathan Fabish;Troy P. Kling;Cuixian Chen;Qizhou Wang	2018	CoRR		machine learning;data mining;database;artificial intelligence;large face;computer science;posterior probability	Vision	15.609107826327515	-41.927058358021185	49463
71abc462fcaaa30e8c3f76832adb1a78d023ed38	making the torch lighter: areinforced active sampling framework for image classification		In this paper, we aim to construct a more reasonable and effective active sampling model, named as reinforcement uncertainty sampling with bag-of-visual-words (RUSB). Compared with traditional active sampling strategy based on uncertainty, both certainty metric and sample post-processing are introduced for better performance. The certainty metric is measured by the bag-of-visual-words (BoVW) classification model in order to entirely evaluate samples, and the post-processing module is driven by the Q-learning method to construct a compact and efficient training set for the BoVW module. The performance of BoVW is used to initialize and determine the status of the post-processing module during the process of iteration. Meanwhile, the weight of the measurement is associated with each iteration instead of being set manually. Experimental results on real world datasets show the effectiveness of the proposed framework.	bag-of-words model in computer vision;iteration;q-learning;sampling (signal processing);test set;torch;video post-processing	Peng Liu;Zhipeng Ye;Xianglong Tang;Wei Zhao	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296400	visualization;artificial intelligence;sampling (statistics);pattern recognition;contextual image classification;computer science;training set	Vision	16.825923814780136	-43.978663785648806	49563
2c258eec8e4da9e65018f116b237f7e2e0b2ad17	deep quantization: encoding convolutional activations with deep generative model		Deep convolutional neural networks (CNNs) have proven highly effective for visual recognition, where learning a universal representation from activations of convolutional layer plays a fundamental problem. In this paper, we present Fisher Vector encoding with Variational Auto-Encoder (FV-VAE), a novel deep architecture that quantizes the local activations of convolutional layer in a deep generative model, by training them in an end-to-end manner. To incorporate FV encoding strategy into deep generative models, we introduce Variational Auto-Encoder model, which steers a variational inference and learning in a neural network which can be straightforwardly optimized using standard stochastic gradient method. Different from the FV characterized by conventional generative models (e.g., Gaussian Mixture Model) which parsimoniously fit a discrete mixture model to data distribution, the proposed FV-VAE is more flexible to represent the natural property of data for better generalization. Extensive experiments are conducted on three public datasets, i.e., UCF101, ActivityNet, and CUB-200-2011 in the context of video action recognition and fine-grained image classification, respectively. Superior results are reported when compared to state-of-the-art representations. Most remarkably, our proposed FV-VAE achieves to-date the best published accuracy of 94.2% on UCF101.	artificial neural network;autoencoder;computation;computer vision;convolutional neural network;encoder;end-to-end principle;experiment;farmville;feature learning;generative adversarial networks;generative model;gradient method;machine learning;mixture model;occam's razor;quantization (signal processing);variational principle	Zhaofan Qiu;Ting Yao;Tao Mei	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.435	computer vision;mixture model;machine learning;architecture;convolutional neural network;deep learning;artificial neural network;computer science;pattern recognition;generative model;deep belief network;artificial intelligence;contextual image classification	Vision	23.375547505743448	-48.03729551164572	49758
625c596950c57d9212499bbd57185d1d51cb93ad	hybrid improved gravitional search algorithm and kernel based extreme learning machine method for classification problems	machine learning algorithms;accuracy decision support systems testing classification algorithms algorithm design and analysis machine learning algorithms training;improved gravitational search algorithm uci machine learning datasets feature subset selection kelm parameters binary igsa continuous value igsa convergence rate particle swarm optimization generalization performance igsa kelm hybrid system kernel based extreme learning machine method;training;testing;accuracy;hybrid system gravitational search algorithm kernel based extreme learning machine feature selection parameter optimization;decision support systems;classification algorithms;pattern classification data mining feature selection generalisation artificial intelligence learning artificial intelligence particle swarm optimisation;algorithm design and analysis	In this paper, we hybridize the improved gravitational search algorithm (IGSA) with kernel based extreme learning machine (KELM) method. Based on this, a novel hybrid system IGSA-KELM is proposed to improve the generalization performance for classification problems. In this system, IGSA is designed by combining the search strategy of particle swarm optimization and GSA to effectively reduce the problem of slow convergence rate, moreover, the continuous-value IGSA and binary IGSA are integrated in one algorithm in order to optimize the KELM parameters and feature subset selection simultaneously. This proposed hybrid algorithm is evaluated on several well-known UCI machine learning datasets. The results indicate that the superiority of the proposed model in terms of classification accuracy. Our hybrid method not only can select the most relevant feature subset, but also achieves a high classification accuracy over other similar state-of-the-art classifier systems.	feature selection;global storage architecture;hybrid algorithm;hybrid system;kernel (operating system);machine learning;mathematical optimization;particle swarm optimization;rate of convergence;search algorithm	Chao Ma;Jihong OuYang;Jian Guan	2014	Proceedings 2014 IEEE International Conference on Security, Pattern Analysis, and Cybernetics (SPAC)	10.1109/SPAC.2014.6982703	computer science;machine learning;pattern recognition;data mining	AI	10.21626573737925	-41.5780672135115	49773
adde56ae2edaf73fbfb31ed82ce7f044b6a45019	locally connected deep learning framework for industrial-scale recommender systems	wide deep;locally connected dnn;dnn	In this work, we propose a locally connected deep learning framework for recommender systems, which reduces the complexity of deep neural network (DNN) by two to three orders of magnitude. We further extend the framework using the idea of recently proposed Wide&Deep model. Experiments on industrial-scale datasets show that our methods could achieve good results with much shorter runtime.	artificial neural network;deep learning;ising model;recommender system	Cen Chen;Peilin Zhao;Longfei Li;Jun Zhou;Xiaolong Li;Minghui Qiu	2017		10.1145/3041021.3054227	speech recognition;machine learning;pattern recognition	AI	20.839149098890413	-48.52057915882487	49780
208224091208b7c4e15c0a76f263493218babef1	relaxation-free deep hashing via policy gradient		In this paper, we propose a simple yet effective relaxationfree method to learn more effective binary codes via policy gradient for scalable image search. While a variety of deep hashing methods have been proposed in recent years, most of them are confronted by the dilemma to obtain optimal binary codes in a truly end-to-end manner with nonsmooth sign activations. Unlike existing methods which usually employ a general relaxation framework to adapt to the gradient-based algorithms, our approach formulates the non-smooth part of the hashing network as sampling with a stochastic policy, so that the retrieval performance degradation caused by the relaxation can be avoided. Specifically, our method directly generates the binary codes and maximizes the expectation of rewards for similarity preservation, where the network can be trained directly via policy gradient. Hence, the differentiation challenge for discrete optimization can be naturally addressed, which leads to effective gradients and binary codes. Extensive experimental results on three benchmark datasets validate the effectiveness of the proposed method.	algorithm;backpropagation;benchmark (computing);binary code;discrete optimization;elegant degradation;end-to-end principle;experiment;gradient;hash function;image retrieval;linear programming relaxation;mathematical optimization;sampling (signal processing);scalability;software propagation	Xin Yuan;Liangliang Ren;Jiwen Lu;Jie Zhou	2018		10.1007/978-3-030-01225-0_9	machine learning;binary code;artificial intelligence;hash function;sampling (statistics);scalability;computer science;discrete optimization	AI	22.587811043980548	-46.69258576789626	50149
5155bd6e5494c8278b9cfc795295ce1451afd4a3	estimating the class posterior probabilities in protein secondary structure prediction	class membership probabilities;protein secondary structure prediction;multi class support vector machines	Support vector machines, let them be bi-class or multi-class, have proved efficient for protein secondary structure prediction. They can be used either as sequence-to-structure classifier, structure-to-structure classifier, or both. Compared to the classifier most commonly found in the main prediction methods, the multi-layer perceptron, they exhibit one single drawback: their outputs are not class posterior probability estimates. This paper addresses the problem of post-processing the outputs of multi-class support vector machines used as sequence-to-structure classifiers with a structure-to-structure classifier estimating the class posterior probabilities. The aim of this comparative study is to obtain improved performance with respect to both criteria: prediction accuracy and quality of the estimates.	bioinformatics;bradley–terry model;discriminant;experiment;master of business informatics;multilayer perceptron;overfitting;protein structure prediction;statistical classification;support vector machine;touchstone file;video post-processing	Yann Guermeur;Fabienne Thomarat	2011		10.1007/978-3-642-24855-9_23	margin classifier;machine learning;pattern recognition;data mining;mathematics	ML	13.314804899553993	-41.909111775580776	50412
54a30e9ed24ac68f03f4a62f18d167e5aeaa5a4a	an adaptive weighted majority vote rule for combining multiple classifiers	adaptive signal processing genetic algorithms handwritten character recognition knowledge verification learning artificial intelligence pattern classification;handwritten digit recognition;voting pattern recognition genetic algorithms handwriting recognition nist spatial databases topology;reliability adaptive weighted majority vote rule multiple classifier system global optimization technique genetic algorithm expert decision handwritten digit recognition problem training nist database;multiple classifiers;adaptive signal processing;pattern classification;genetic algorithms;global optimization;learning artificial intelligence;majority voting;multiple classifier system;handwritten character recognition;knowledge verification	We introduce a novel multiple classifier system that incorporates a global optimization technique based on a genetic algorithm for configuring the system. The system adopts the weighted majority vote approach to combine the decision of the experts, and obtains the weights by maximizing the performance of the whole set of experts, rather than that of each of them separately. The system has been tested on a handwritten digit recognition problem, and its performance compared with those exhibited by a system using the weights obtained during the training of each expert separately. The results of a set of experiments conducted on 30,000 digits extracted from the NIST database have shown that the proposed system exhibits better performance than those of the alternative one, and that such an improvement is due to a better estimate of the reliability of the participating classifiers.		Claudio De Stefano;Antonio Della Cioppa;Angelo Marcelli	2002		10.1109/ICPR.2002.1048270	adaptive filter;majority rule;speech recognition;genetic algorithm;computer science;machine learning;pattern recognition;global optimization	NLP	10.382212904114056	-40.861641190551005	50498
b876eb4ac85ed5ea791d0ad91ed972723c620393	ensemble based reactivated regularization extreme learning machine for classification		Ensemble trick has been widely used in extreme learning machine (ELM), and most paradigms concern about the training phase with expectation of improving their generalization ability. Unlike traditional strategies, this paper pays more attention to the prediction phase and proposes a discriminatory approach called ensemble based reactivated regularization ELM (ER2-ELM). In short, the novel literature consists of two interrelated steps where the probability density estimation is first conducted to show the degree of difficulty of identifying an instance, and then a random factor is adopted to determine whether the ELM base learner is sequentially reactivated. As such, instances easily to identify cost less computing burden, while the vague ones are taken carefully consideration. Compared with other ensemble methods, the prediction computation overhead decreases. In the end, a number of examples, including UCI benchmark datasets, handwritten digits, object detection, etc., are employed so as to illustrate its state-of-the-art performance.	matrix regularization	Boyang Zhang;Zhao Ma;Yingyi Liu;Haiwen Yuan;Lingjie Sun	2018	Neurocomputing	10.1016/j.neucom.2017.07.018	artificial intelligence;probability density function;machine learning;pattern recognition;object detection;extreme learning machine;computation;ensemble learning;regularization (mathematics);computer science	ML	15.957518493839213	-39.447686715582556	50785
9f7679311fb5a51b05713d5f84e961236bae29bf	mipal: multiple-instance passive aggressive learning for identification of attention deficit hyperactive disorder from fmri		This paper proposes a new algorithm for the multiple instance learning problem (MIL) and investigates its application for detecting Attention Deficit Hyperactive Disorder (ADHD) from resting-state functional Magnetic Resonance Imaging data. The core component of many kernel-based MIL algorithms is usually an SVM-like batch optimization framework, hence scaling to large datasets like fMRI is often difficult. On the other hand, a family of on-line kernel classification algorithms widely known as “perceptron-like” kernel classifiers demonstrate efficient and accurate solutions. This paper presents MiPAL — Multiple-instance Passive Aggressive Learning algorithm, based on such a perceptron-like kernel classifier. First, MiPAL builds a labeller by inputting negative bags into PA algorithm. Second, this labeller helps to train a separate PA classifier to predict binary class labels such that least-negative instances are regarded as positive. Due to the on-line PA algorithm's fast adaptation, the impact of invalid positive support-vectors could be attenuated by the new, accurate support-set over time. Our experimental results reveal performance gains in several MIL datasets including state-of-the-art performance in Muskl, Fox, and comparable accuracy in the preprocessed ADHD-200 dataset.	algorithm;binary classification;experiment;image scaling;kernel (operating system);mathematical optimization;multiple instance learning;online and offline;pa-risc;perceptron;resonance;sensor	Prabhash Kumarasinghe;Sundaram Suresh;Vigneshwaran Subbaraju	2017	2017 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2017.7966432	support vector machine;kernel (linear algebra);machine learning;scaling;artificial intelligence;algorithm design;pattern recognition;computer science;statistical classification;linear programming	ML	20.260802228358703	-39.46646992933368	50939
e59e35453aaafdeb317ab3c239bdcbbe76d8242d	agent-based approach to distributed ensemble learning of fuzzy artmap classifiers	agent platform;fuzzy artmap;ensemble learning;agent based;noisy data;neural network ensemble;network performance;large scale;neural network	This paper presents a parallel and distributed approach to ensemble learning of Fuzzy ARTMAP classifiers based on the multi-agent platform. Neural networks have been used successfully in a broad range of non-linear problems that are difficult to solve using traditional techniques. Training a neural network for practical applications is often time consuming thus extensive research work is being carried out to accelerate this process. Fuzzy ARTMAP (FAM) is one of the fastest neural network architectures given its ability to produce neurons on demand to represent new classification categories. FAM can adapt to the input data without having to specify an arbitrary structure. However, FAM is vulnerable to noisy data which can rapidly degrade network performance. Due to its fast learning features, FAM is sensitive to the sequence of input sample presentations. In this paper we propose a parallel and distributed approach to ensemble learning for FAM networks as a means to improve the over-all performance of the classifier and increase its resilience to noisy data. We use the multi-agent platform to distribute the computational load of the ensemble to several hosts. The multi-agent platform is a robust environment that can support large-scale neural network ensembles. Our approach also demonstrates the feasibility of large-scale ensembles. The experimental results show that ensemble learning substantially improved the performance of fuzzy ARTMAP classifiers.	ensemble learning	Louie F. Cervantes;Jung-sik Lee;Jaewan Lee	2007		10.1007/978-3-540-72830-6_84	computer science;artificial intelligence;machine learning;pattern recognition;data mining;ensemble learning;network performance	ML	12.163414497611015	-40.551558475666084	51000
fa6742ea993629551b2ac44af8d08002ebe6bab0	a memetic algorithm to select training data for support vector machines	support vector machines;training data selection;self adaptation;memetic algorithm	In this paper we propose a new memetic algorithm (MASVM) for fast and efficient selection of a valuable training set for support vector machines (SVMs). This is a crucial step especially in case of large and noisy data sets, since the SVM training has high time and memory complexity. The majority of state-of-the-art methods exploit the data geometry analysis, both in the input and kernel space. Although evolutionary algorithms have been proven to be very efficient for this purpose, they have not been extensively studied so far. Here, we propose a new method employing an adaptive genetic algorithm enhanced by some refinement techniques. The refinements are based on utilizing a pool of the support vectors identified so far at various steps of the algorithm. Extensive experimental study performed on the well-known benchmark, real-world and artificial data sets clearly confirms the efficacy, robustness and convergence capabilities of the proposed approach, and shows that it is competitive compared with other state-of-the-art techniques.	algorithmic efficiency;benchmark (computing);evolutionary algorithm;experiment;genetic algorithm;memetic algorithm;refinement (computing);robustness (computer science);signal-to-noise ratio;support vector machine;test set;user space;vergence;whole earth 'lectronic link	Jakub Nalepa;Michal Kawulok	2014		10.1145/2576768.2598370	support vector machine;computer science;artificial intelligence;machine learning;pattern recognition;data mining;memetic algorithm	AI	10.711559798857657	-42.97733301051893	51205
cc9d5f2e9a7d818ec123b2e21d45be13b1bc6a8d	cracks in krx: when more distant points are less anomalous		We examine the Mahalanobis-distance based kernel-RX (KRX) algorithm for anomaly detection, and find that it can exhibit an unfortunate phenomenon: the anomalousness, for points far from the training data, can decrease with increasing distance. We demonstrate this directly for a few special cases, and provide a more general argument that applies in the large bandwidth regime.	algorithm;anomaly detection	James Theiler;Guen Grosklos	2016	2016 8th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)	10.1109/WHISPERS.2016.8071717	anomaly detection;mahalanobis distance;kernel density estimation;machine learning;artificial intelligence;training set;phenomenon;mathematics;pattern recognition;bandwidth (signal processing)	ML	20.031176886948625	-49.61514567766127	51300
91a3cea657569626fe86ec436e0962ae85ed67b2	semi-supervised deep generative modelling of incomplete multi-modality emotional data		There are threefold challenges in emotion recognition. First, it is difficult to recognize human's emotional states only considering a single modality. Second, it is expensive to manually annotate the emotional data. Third, emotional data often suffers from missing modalities due to unforeseeable sensor malfunction or configuration issues. In this paper, we address all these problems under a novel multi-view deep generative framework. Specifically, we propose to model the statistical relationships of multi-modality emotional data using multiple modality-specific generative networks with a shared latent space. By imposing a Gaussian mixture assumption on the posterior approximation of the shared latent variables, our framework can learn the joint deep representation from multiple modalities and evaluate the importance of each modality simultaneously. To solve the labeled-data-scarcity problem, we extend our multi-view model to semi-supervised learning scenario by casting the semi-supervised classification problem as a specialized missing data imputation task. To address the missing-modality problem, we further extend our semi-supervised multi-view model to deal with incomplete data, where a missing view is treated as a latent variable and integrated out during inference. This way, the proposed overall framework can utilize all available (both labeled and unlabeled, as well as both complete and incomplete) data to improve its generalization ability. The experiments conducted on two real multi-modal emotion datasets demonstrated the superiority of our framework.	approximation;emotion recognition;experiment;geo-imputation;latent variable;missing data;modal logic;modality (human–computer interaction);semi-supervised learning;semiconductor industry;supervised learning;view model	Changde Du;Changying Du;Hao Wang;Jinpeng Li;Wei-Long Zheng;Bao-Liang Lu;Huiguang He	2018		10.1145/3240508.3240528	modalities;imputation (statistics);generative grammar;computer vision;missing data;gaussian;inference;emotion recognition;pattern recognition;computer science;artificial intelligence;latent variable	AI	23.65681507205856	-45.61625440766169	51352
ee2e43a16afa5aa884a6f07e0b3b6996911d84b3	semi-supervised co-selection: features and instances by a weighting approach	pattern clustering feature selection;associated partitioning semisupervised co selection feature selection instance selection semisupervised clustering weighting constrained clustering objective function;clustering algorithms optimization linear programming proposals data mining genetic algorithms neural networks	Feature selection, instance selection and semi-supervised clustering are different challenges for machine learning and data mining communities. While other works have addressed each of these problems separately, in this paper we show how they can be addressed together, simultaneously. We propose an unified framework for semi-supervised co-selection of features and instances, based on weighting constrained clustering. In particular, we define a novel objective function by weighting both instances and features; and constraining the associated partitioning. Experiments are carried out on some known datasets, and results are promising, showing that our proposal outperforms other state-of-the-art algorithms.	algorithm;cluster analysis;constrained clustering;data mining;experiment;feature selection;loss function;machine learning;optimization problem;semi-supervised learning;semiconductor industry;unified framework	Raywat Makkhongkaew;Khalid Benabdeslem;Haytham Elghazel	2016	2016 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2016.7727645	correlation clustering;constrained clustering;fuzzy clustering;flame clustering;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;biclustering;clustering high-dimensional data	ML	20.557242325002786	-41.84303480993906	51436
6fbe046da44d8cb49e8294f8c2d491c24966a784	improved uniformity enforcement in stochastic discrimination	stochastic discrimination;feature space;machine learning;random forest	There are a variety of methods for inducing predictive systems from observed data. Many of these methods fall into the field of study of machine learning. Some of the most effective algorithms in this domain succeed by combining a number of distinct predictive elements to form what can be described as a type of committee. Well known examples of such algorithms are AdaBoost, bagging and random forests. Stochastic discrimination is a committee-forming algorithm that attempts to combine a large number of relatively simple predictive elements in an effort to achieve a high degree of accuracy. A key element of the success of this technique is that its coverage of the observed feature space should be uniform in nature. We introduce a new uniformity enforcement method, which on benchmark datasets, leads to greater predictive efficiency than the currently published method.	adaboost;algorithm;benchmark (computing);bootstrap aggregating;circuit complexity;experiment;feature vector;machine learning;random forest;retry;statistical classification;test set	Matthew Prior;Terry Windeatt	2009		10.1007/978-3-642-02326-2_34	computer science;machine learning;data mining;statistics	AI	15.138935643436458	-41.41504203487485	51455
b2ebaaf853187c58f16652d3c8491b73e9475c87	splinenets: continuous neural decision graphs		We present SplineNets, a practical and novel approach for using conditioning in convolutional neural networks (CNNs). SplineNets are continuous generalizations of neural decision graphs, and they can dramatically reduce runtime complexity and computation costs of CNNs, while maintaining or even increasing accuracy. Functions of SplineNets are both dynamic (i.e., conditioned on the input) and hierarchical (i.e., conditioned on the computational path). SplineNets employ a unified loss function with a desired level of smoothness over both the network and decision parameters, while allowing for sparse activation of a subset of nodes for individual samples. In particular, we embed infinitely many function weights (e.g. filters) on smooth, low dimensional manifolds parameterized by compact B-splines, which are indexed by a position parameter. Instead of sampling from a categorical distribution to pick a branch, samples choose a continuous position to pick a function weight. We further show that by maximizing the mutual information between spline positions and class labels, the network can be optimally utilized and specialized for classification tasks. Experiments show that our approach can significantly increase the accuracy of ResNets with negligible cost in speed, matching the precision of a 110 level ResNet with a 32 level SplineNet.	artificial neural network;b-spline;computation;convolutional neural network;decision tree;embedded system;experiment;loss function;mutual information;sampling (signal processing);sparse matrix;spline (mathematics)	Cem Keskin;Shahram Izadi	2018			manifold;convolutional neural network;computer science;artificial intelligence;machine learning;conditioning;computation;smoothness;parameterized complexity;mutual information;contextual image classification	ML	22.272780063108883	-48.28931640325869	51508
cf90f4d719742c9f8974f1603e5329d8b0c65db5	bridging supervised learning and test-based co-optimization		This paper takes a close look at the important commonalities and subtle differences between the well-established field of supervised learning and the much younger one of cooptimization. It explains the relationships between the problems, algorithms and views on cost and performance of the two fields, all throughout providing a two-way dictionary for the respective terminologies used to describe these concepts. The intent is to facilitate advancement of both fields through transfer and cross-pollination of ideas, techniques and results. As a proof of concept, a theoretical study is presented on the connection between existence / lack of free lunch in the two fields, showcasing a few ideas for improving computational complexity of certain supervised learning approaches.	algorithm;bridging (networking);circuit complexity;computational complexity theory;design of experiments;dictionary;experiment;heuristic (computer science);machine learning;mathematical optimization;reinforcement learning;supervised learning;time complexity;whole earth 'lectronic link	Elena Popovici	2017	Journal of Machine Learning Research		active learning;artificial intelligence;machine learning;proof of concept;supervised learning;semi-supervised learning;bridging (networking);unsupervised learning;computational complexity theory;computer science	AI	20.237781661269544	-45.291622462469086	51550
94eb7d9d7dede866339fd61574c69e38e0b96b92	pattern-based subspace classification model	classification patterns feature selection;bayesian network;pattern clustering;decision tree;support vector machines;classification;machine learning;pattern clustering pattern classification predictive models pattern extraction;feature extraction;pattern clustering feature extraction pattern classification;subspace clustering;pattern classification;feature selection;patterns;prediction model;support vector machine;classification accuracy	The use of patterns in predictive models has received a lot of attention in recent years. This paper presents a pattern-based classification model which extracts the patterns that have similarity among all objects in a specific class. This introduced model handles the problem of the dependence on a user-defined threshold that appears in the pattern-based subspace clustering. The experimental results obtained, show that the overall pattern-based classification accuracy is high compared with other machine learning techniques including Support vector machine, Bayesian Network, multi-layer perception and decision trees.	algorithm;bayesian network;cluster analysis;clustering high-dimensional data;decision tree;geo-imputation;information;layer (electronics);machine learning;missing data;predictive modelling;requirement;rule induction;statistical classification;support vector machine	Mostafa A. Salama;Aboul Ella Hassanien;Aly A. Fahmy	2010	2010 Second World Congress on Nature and Biologically Inspired Computing (NaBIC)	10.1109/NABIC.2010.5716318	support vector machine;computer science;machine learning;pattern recognition;data mining;feature selection	AI	12.247778808587908	-46.130015486571175	51571
3e86ba3e36d9259806dec2d8a44edc5318423093	learning kernel expansions for image classification	mouth;kernel;kernel expansions learning;support vector machines;visual classification;mouth detection;image classification;data nonlinear representation;matrix algebra;gradient descent based method;kernel machine;dimensionality reduction;machine learning;gradient descent;kernel image classification machine learning support vector machines support vector machine classification linear discriminant analysis clustering algorithms robots robustness mouth;robots;pedestrian detection;clustering algorithms;gradient methods;support vector machine classification;mouth detection kernel expansions learning image classification kernel machines support vector machine kernel linear discriminant analysis visual classification gradient descent based method data nonlinear representation dimensionality reduction matrix formulation pedestrian detection;robustness;kernel machines;support vector machine;linear discriminant analysis;dimensional reduction;matrix formulation;support vector machines gradient methods image classification matrix algebra;kernel linear discriminant analysis	Kernel machines (e.g. SVM, KLDA) have shown state-of-the-art performance in several visual classification tasks. The classification performance of kernel machines greatly depends on the choice of kernels and its parameters. In this paper, we propose a method to search over a space of parameterized kernels using a gradient-descent based method. Our method effectively learns a non-linear representation of the data useful for classification and simultaneously performs dimensionality reduction. In addition, we suggest a new matrix formulation that simplifies and unifies previous approaches. The effectiveness and robustness of the proposed algorithm is demonstrated in both synthetic and real examples of pedestrian and mouth detection in images.	algorithm;algorithmic efficiency;computation;cross-validation (statistics);dimensionality reduction;experiment;gradient descent;kernel (operating system);kernel method;mathematical optimization;matrix mechanics;maxima and minima;nonlinear system;polynomial;radial basis function;simon;supervised learning;synthetic intelligence;unsupervised learning	Fernando De la Torre;Oriol Vinyals	2007	2007 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2007.383151	support vector machine;mathematical optimization;radial basis function kernel;computer science;machine learning;pattern recognition;mathematics;linear discriminant analysis;tree kernel;variable kernel density estimation	Vision	24.281638753737294	-40.57269368893926	51700
8185be0689442db83813b49e215bf30870017459	feature learning for image classification via multiobjective genetic programming	g400 computer science;feature learning techniques;jochen triesch static hand posture ii datasets;mogp;cmu pie dataset;mit urban dataset;image classification;hand crafted features;g700 artificial intelligence;genetic programming;data mining;learning systems;multiobjective optimization feature extraction genetic programming gp image classification;computer architecture;image classification methods;tree complexity;genetic programming gp;feature extraction;期刊论文;feature extraction data mining genetic programming sociology statistics learning systems computer architecture;domain adaptive global feature descriptor generation;statistics;classification error;nature scene dataset;multiobjective genetic programming;multiobjective optimization;genetic algorithms;feature descriptors;learning artificial intelligence;objective fitness criteria;primitive 2d operators;caltech 101 dataset;evolutionary learning methodology;sociology	Feature extraction is the first and most critical step in image classification. Most existing image classification methods use hand-crafted features, which are not adaptive for different image domains. In this paper, we develop an evolutionary learning methodology to automatically generate domain-adaptive global feature descriptors for image classification using multiobjective genetic programming (MOGP). In our architecture, a set of primitive 2-D operators are randomly combined to construct feature descriptors through the MOGP evolving and then evaluated by two objective fitness criteria, i.e., the classification error and the tree complexity. After the entire evolution procedure finishes, the best-so-far solution selected by the MOGP is regarded as the (near-)optimal feature descriptor obtained. To evaluate its performance, the proposed approach is systematically tested on the Caltech-101, the MIT urban and nature scene, the CMU PIE, and Jochen Triesch Static Hand Posture II data sets, respectively. Experimental results verify that our method significantly outperforms many state-of-the-art hand-designed features and two feature learning techniques in terms of classification accuracy.	caltech 101;color image;computer vision;feature (computer vision);feature extraction;feature learning;genetic programming;grayscale;lexicography;maximum parsimony (phylogenetics);occam's razor;poor posture;randomness;tournament selection;visual descriptor	Ling Shao;Li Liu;Xuelong Li	2014	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2013.2293418	genetic programming;contextual image classification;genetic algorithm;feature extraction;computer science;artificial intelligence;multi-objective optimization;machine learning;pattern recognition;data mining;feature	Vision	10.253747790721974	-42.38418830658235	51744
7232ad5f1220f6d784056c171abccc7eb9d50ec3	extreme learning machines for credit scoring: an empirical evaluation		Abstract Classification algorithms are used in many domains to extract information from data, predict the entry probability of events of interest, and, eventually, support decision making. This paper explores the potential of extreme learning machines (ELM), a recently proposed type of artificial neural network, for consumer credit risk management. ELM possess some interesting properties, which might enable them to improve the quality of model-based decision support. To test this, we empirically compare ELM to established scoring techniques according to three performance criteria: ease of use, resource consumption, and predictive accuracy. The mathematical roots of ELM suggest that they are especially suitable as a base model within ensemble classifiers. Therefore, to obtain a holistic picture of their potential, we assess ELM in isolation and in conjunction with different ensemble frameworks. The empirical results confirm the conceptual advantages of ELM and indicate that they are a valuable alternative to other credit risk modelling methods.		Artem Bequé;Stefan Lessmann	2017	Expert Syst. Appl.	10.1016/j.eswa.2017.05.050	resource consumption;machine learning;artificial intelligence;data mining;usability;decision support system;credit risk;artificial neural network;computer science;statistical classification	AI	10.525976853898575	-38.17256186017146	51791
6fc7edd755a263fa1da0170741430b18b5f1c7b3	effects of large constituent size in variable neural ensemble classifier for breast mass classification		This paper proposes a novel ensemble technique for mass classification in digital mammograms by varying the number of hidden units to create diverse candidates. The effects of adding more networks to the ensemble are evaluated on a mammographic database and the results are presented. A classification accuracy of ninety nine percent is achieved.	ensemble learning;neural ensemble	Peter McLeod;Brijesh Verma	2013		10.1007/978-3-642-42051-1_65	machine learning;pattern recognition;data mining	ML	13.893346560330063	-45.372180985265466	52016
30ece28ea92a545d1e88d7f629e734f6a26d2a38	bayesian regularized neural network decision tree ensemble model for genomic data classification		ABSTRACTMachine learning techniques have been widely applied to solve the classification problem of highly dimensional and complex data in the field of bioinformatics. Among them, Bayesian regularized neural network (BRNN) became one of the popular choices due to its robustness and ability to avoid over fitting. On the other hand, Bayesian approach applied to neural network training offers computational burden and increases its time complexity. This restricts the use of BRNN in an on-line machine learning system. In this article, a Bayesian regularized neural network decision Tree (BrNdT) ensemble model, is proposed to combat high computational time complexity of a classifier model. The key idea behind the proposed ensemble methodology is to weigh and combine several individual classifiers and apply majority voting decision scheme to obtain an efficient classifier which outperforms each one of them. The simulation results show that the proposed method achieves a significant reduction in time complexity an...	artificial neural network;decision tree	Deepika Garg;Amit Mishra	2018	Applied Artificial Intelligence	10.1080/08839514.2018.1483115	time complexity;robustness (computer science);overfitting;artificial neural network;artificial intelligence;data classification;ensemble forecasting;machine learning;computer science;decision tree;bayesian probability	AI	17.67842534887389	-38.57577695148862	52097
8c1dafe24d154fcf96be7e83d59113ce7df106b7	use of structured pattern representations for combining classifiers	combination of classifiers;edit distance;info eu repo semantics article;combining classifier;string representation;string data representation;tree data representation;k nn rule;pattern recognition;tree representation;combining classifiers;structured data	In Pattern Recognition, there are problems where distinct representations can be obtained for the same pattern, and depending on the type of classifiers (statistical or structural) one type of representation is preferred versus the others. In the last years, different approaches to combining classifiers have been proposed to improve the performance of individual classifiers. However, few works investigated the use of structured pattern representations. In this paper combination of classifiers has been applied using tree pattern representation in combination with strings and vectors for a handwritten character classification task. In order to save computational cost, some proposals based on the use of both embedding structured data and refine and filter framework are provided.		Raisa Socorro;Luisa Micó	2008		10.1007/978-3-540-89689-0_85	random subspace method;machine learning;pattern recognition;data mining;mathematics	ML	12.449772648714271	-45.57282188560347	52275
ad31cd6a626b975369bc9f3c739a23161dd791e2	deep hashing with multilevel similarity learning for multimedia similarity search		In this work, we propose a novel deep multimodal hashing method, termed as Deep Hashing with Multilevel Similarity Learning (DHMSL), which learns discriminative hash functions with deep neural networks by exploiting multilevel semantic similarity correlations of multimedia data. Firstly, we construct multilevel similarity correlation by jointly exploiting the local structure and semantic label information. Then, the unified binary codes are learned by preserving the multilevel similarity correlations as well as incorporating the bit balance and quantization error properties. Besides that, two deep neural networks are jointly trained to learn two sets of nonlinear hash functions by minimizing the errors of unified binary codes and outputs of the networks. We conduct experiments on two widely-used multimodal datasets, and the proposed DHMSL method can achieve the state-of-the-art performance compared with the baselines for both image-query-text and text-query-image tasks.	artificial neural network;binary code;data retrieval;deep learning;discriminative model;experiment;hash function;locality-sensitive hashing;multimodal interaction;nonlinear system;quantization (signal processing);semantic similarity;similarity learning;similarity search	Lu Jin;Qiuli Liu;Zechao Li	2018		10.1145/3240876.3240898	similarity learning;binary code;semantic similarity;discriminative model;quantization (signal processing);multimedia;hash function;artificial neural network;computer science;artificial intelligence;pattern recognition;nearest neighbor search	AI	23.774070521081722	-45.488361148620115	52583
9f6638fb891aa5ceb4d2786a648e267f3b56f6e6	an unsupervised feature selection algorithm based on ant colony optimization	multivariate technique;ant colony optimization;univariate technique;dimensionality reduction;filter approach;feature selection	Feature selection is a combinatorial optimization problem that selects most relevant features from an original feature set to increase the performance of classification or clustering algorithms. Most feature selection methods are supervised methods and use the class labels as a guide. On the other hand, unsupervised feature selection is a more difficult problem due to the unavailability of class labels. In this paper, we present an unsupervised feature selection method based on ant colony optimization, called UFSACO. The method seeks to find the optimal feature subset through several iterations without using any learning algorithms. Moreover, the feature relevance will be computed based on the similarity between features, which leads to the minimization of the redundancy. Therefore, it can be classified as a filter-based multivariate method. The proposed method has a low computational complexity, thus it can be applied for high dimensional datasets. We compare the performance of UFSACO to 11 well-known univariate and multivariate feature selection methods using different classifiers (support vector machine, decision tree, and naive Bayes). The experimental results on several frequently used datasets show the efficiency and effectiveness of the UFSACO method as well as improvements over previous related methods.		Sina Tabakhi;Parham Moradi;Fardin Akhlaghian	2014	Eng. Appl. of AI	10.1016/j.engappai.2014.03.007	ant colony optimization algorithms;minimum redundancy feature selection;computer science;machine learning;pattern recognition;data mining;feature selection;feature;dimensionality reduction	AI	10.976063810557083	-43.47246758340775	52685
97c939c7a9de57956c28e4c355f22bcffe173b36	a novel hybrid genetic algorithm with granular information for feature selection and optimization		Abstract Feature selection has been a significant task for data mining and pattern recognition. It aims to choose the optimal feature subset with the minimum redundancy and the maximum discriminating ability. This paper analyzes the feature selection method from two aspects of data and algorithm. In order to deal with the redundant features and irrelevant features in high-dimensional u0026 low-sample data and low-dimensional u0026 high-sample data, the feature selection algorithm model based on the granular information is presented in this paper. Thus, our research examines experimentally how granularity level affects both the classification accuracy and the size of feature subset for feature selection. First of all, the improved binary genetic algorithm with feature granulation (IBGAFG) is used to select the significant features. Then, the improved neighborhood rough set with sample granulation (INRSG) is proposed under different granular radius, which further improves the quality of the feature subset. Finally, in order to find out the optimal granular radius, granularity λ optimization based on genetic algorithm (ROGA) is presented. The optimal granularity parameters are found adaptively according to the feedback of classification accuracy. The performance of the proposed algorithms is tested upon eleven publicly available data sets and is compared with other supervisory methods or evolutionary algorithms. Additionally, the ROGA algorithm is applied to the enterprise financial dataset, which can select the features that affect the financial status. Experiment results demonstrate that the approaches are efficient and can provide higher classification accuracy using granular information.	feature selection;genetic algorithm;mathematical optimization;memetic algorithm	Hongbin Dong;Tao Li;Rui Ding;Jing Sun	2018	Appl. Soft Comput.	10.1016/j.asoc.2017.12.048	machine learning;redundancy (engineering);mathematics;genetic algorithm;artificial intelligence;granularity;rough set;evolutionary algorithm;data set;feature selection	AI	10.060595770451634	-42.610751111344484	52729
7cf171fb628f41e7ade8779fec61e7c8389c341f	online ensemble using adaptive windowing for data streams with concept drift		Data streams, which can be considered as one of the primary sources of what is called big data, arrive continuously with high speed. The biggest challenge in data streams mining is to deal with concept drifts, during which ensemble methods are widely employed. The ensembles for handling concept drift can be categorized into two different approaches: online and block-based approaches. The primary disadvantage of the block-based ensembles lies in the difficulty of tuning the block size to provide a tradeoff between fast reactions to drifts. Motivated by this challenge, we put forward an online ensemble paradigm, which aims to combine the best elements of block-based weighting and online processing. The algorithm uses the adaptive windowing as a change detector. Once a change is detected, a new classifier is built replacing the worst one in the ensemble. By experimental evaluations on both synthetic and real-world datasets, our method performs significantly better than other ensemble approaches.	concept drift	Yange Sun;Zhihai Wang;Haiyang Liu;Chao Du;Jidong Yuan	2016	IJDSN	10.1155/2016/4218973	computer science;machine learning;pattern recognition;data mining	AI	13.74751270658298	-38.39451409039828	52736
b4c6dfc158fa864ae0a999ce87094fb04d9c2574	an efficient many-core implementation for semi-supervised support vector machines	semi supervised support vector machines;non convex optimization;big data analytics;graphics processing units;part of book or chapter of book	The concept of semi-supervised support vector machines extends classical support vector machines to learning scenarios, where both labeled and unlabeled patterns are given. In recent years, such semi-supervised extensions have gained considerable attention due to their huge potential for real-world applications with only small amounts of labeled data. While being appealing from a practical point of view, semi-supervised support vector machines lead to a combinatorial optimization problem that is difficult to address. Many optimization approaches have been proposed that aim at tackling this task. However, the computational requirements can still be very high, especially in case large data sets are considered and many model parameters need to be tuned. A recent trend in the field of big data analytics is to make use of graphics processing units to speed up computationally intensive tasks. In this work, such a massively-parallel implementation is developed for semi-supervised support vector machines. The experimental evaluation, conducted on commodity hardware, shows that valuable speed-ups of upi¾?to two orders of magnitude can be achieved over a standard single-core CPUi¾?execution.	semiconductor industry;support vector machine	Fabian Gieseke	2015		10.1007/978-3-319-27926-8_13	computer science;data science;machine learning;data mining	HCI	18.76677439110853	-38.01595592933123	52899
815a6f2f2e94ae774459eab376685dbdb243be47	an evolutionary algorithm for classifier and combination rule selection in multiple classifier systems	genetic algorithms pattern classification;aggregation strategies;design process;evolutionary computation;classifier selection;ga;multiple classifier systems;bagging;classifier system;testing;euclidean distance;boosting;voting;system design;multiexpert configurations;pattern classification;performance gain;pattern recognition;tk7800 electronics see also telecommunications;evolutionary computation genetic algorithms bagging pattern recognition voting performance gain testing euclidean distance;genetic algorithm;genetic algorithms;evolutionary algorithm;multiple classifier system;character recognition;combination rule selection;boosting evolutionary algorithm classifier selection combination rule selection multiple classifier systems genetic algorithm ga multiexpert configurations character recognition aggregation strategies bagging	We introduce a multiple classifier system which incorporates a genetic algorithm in order to simultaneously and dynamically select not only the participating classifiers but also the combination rule to be used. In this paper we focus on exploring the efficiency of such an evolutionary algorithm with respect to the behaviour of the resulting multiexpert configurations. To this end we initially test the proposed system on an artificially generated dataset, and then on a problem drawn from the character recognition domain. Subsequently we proceed to investigate the performance of our system not only in comparison to that of its constituent classifiers, but also in comparison to a number of alternative aggregation strategies ranging from a simple random selection scheme to the well-known “bagging” and “boosting” algorithms. Our results indicate that significant gains can be obtained by integrating an evolutionary algorithm into the multi-classifier systems design process.	evolutionary algorithm;evolutionary computation;genetic algorithm;learning classifier system;optical character recognition;systems design;whole earth 'lectronic link	Konstantinos Sirlantzis;Michael C. Fairhurst;Richard M. Guest	2002		10.1109/ICPR.2002.1048416	genetic algorithm;computer science;machine learning;evolutionary algorithm;pattern recognition;data mining;mathematics;evolutionary computation	AI	10.42792759364454	-40.88896370478039	53053
84b68516c42cffb5c88a284816a860a9bbb470a9	time series classification using macd-histogram-based sax and its performance evaluation	histograms;cybernetics;approximation algorithms;data mining;time series analysis;aggregates;conferences	Time series classification is one of the most well-known grand challenges in many different application domains. Time series classification is the task of assigning a discrete class label to an unclassified time series. Three important key points should be considered in the design of time series classifiers: the feature expression for the time series, the definition of the distance function, and the classification strategy. Many researchers of time series have been focusing on Symbolic Aggregate approXimation (SAX), which is a state-of-the-art feature expression for time series. SAX is a high-level symbolic representation for time series that allows for dimensionality reduction. SAX allows symbol-based approaches, which have been studied in depth to be applied in time series classifiers. In this paper, we propose a novel method for time series classification using a SAX-based symbolic representation. The proposed method includes: Moving average convergence divergence (MACD)-Histogram-based SAX and Nearest Neighbor (1-NN) utilizing the extended Levenshtein distance. To evaluate the performance of the proposed method, we implemented it and conducted experiments using the UCR time series classification archive. The experimental results showed that the proposed method outperforms not only other distance-based 1-NNs, but also other state-of-the-art methods.	aggregate function;archive;dimensionality reduction;experiment;grand challenges;high- and low-level;levenshtein distance;performance evaluation;time series	Keiichi Tamura;Tatsuhiro Sakai;Takumi Ichimura	2016	2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2016.7844601	cybernetics;computer science;artificial intelligence;machine learning;time series;data mining;histogram;statistics	DB	17.54088035074125	-45.10753966729416	53166
bcb9e81f8e1a6b942adc18758e3cb032836203d4	entropy-based iterative face classification	cluster algorithm;face classification;discriminant analysis;entropy	This paper presents a novel methodology whose task is to deal with the face classification problem. This algorithm uses discriminant analysis to project the face classes and a clustering algorithm to partition the projected face data, thus forming a set of discriminant clusters. Then, an iterative process creates subsets, whose cardinality is defined by an entropybased measure, that contain the most useful clusters. The best match to the test face is found when one final face class is retained. The standard UMIST and XM2VTS databases have been utilized to evaluate the performance of the proposed algorithm. Results show that it provides a good solution to the face classification problem.	algorithm;cluster analysis;database;iteration;iterative method;linear discriminant analysis	Marios Kyperountas;Anastasios Tefas;Ioannis Pitas	2011		10.1007/978-3-642-19530-3_13	machine learning;pattern recognition;data mining;mathematics	AI	11.346557307088807	-43.724973620253834	53320
348e63021a0d055ce6b79c3cca8696b4ff1907e3	adaptive metric nearest neighbor classification	nearest neighbor searches;q measurement;kernel;classification performance;flexible metric;dimensionality;conditional probabilities;image classification;curse of dimensionality;euclidean distance;local adaptation;error analysis;class conditional probabilities;nearest neighbor;computer science;nearest neighbor searches computer science error analysis q measurement euclidean distance kernel;high dimension;conditional probability;class conditional probabilities nearest neighbor classification conditional probabilities dimensionality flexible metric classification performance;nearest neighbor classification	Nearest neighbor classification assumes locally constant class conditional probabilities. This assumption becomes invalid in high dimensions with finite samples due to the curse of dimensionality. Severe bias can be introduced under these conditions when using the nearest neighbor rule. We propose a locally adaptive nearest neighbor classification method to try to minimize bias. We use a Chisquared distance analysis to compute a flexible metric for producing neighborhoods that are highly adaptive to query locations. Neighborhoods are elongated along less relevant feature dimensions and constricted along most influential ones. As a result, the class conditional probabilities tend to be smoother in the modified neighborhoods, whereby better classification performance can be achieved. The efficacy of our method is validated and compared against other techniques using a variety of simulated and real world data.	curse of dimensionality;decision tree learning;feature vector;k-nearest neighbors algorithm;nearest neighbor search;nearest neighbour algorithm;recursion;simulation;statistical classification	Carlotta Domeniconi;Dimitrios Gunopulos;Jing Peng	2000		10.1109/CVPR.2000.855863	large margin nearest neighbor;nearest neighbor graph;curse of dimensionality;conditional probability;computer science;machine learning;pattern recognition;mathematics;cover tree;nearest neighbor search;statistics	Vision	15.500002320539974	-38.94128539407289	53360
d20d8e9fe031e7e9c9756592c3d4af5af7ef5fcb	image compression with stochastic winner-take-all auto-encoder	orthogonal matching pursuit;image compression;sparse representations;auto encoders	This paper addresses the problem of image compression using sparse representations. We propose a variant of autoencoder called Stochastic Winner-Take-All Auto-Encoder (SWTA AE). “Winner-Take-All” means that image patches compete with one another when computing their sparse representation and “Stochastic” indicates that a stochastic hyperparameter rules this competition during training. Unlike auto-encoders, SWTA AE performs variable rate image compression for images of any size after a single training, which is fundamental for compression. For comparison, we also propose a variant of Orthogonal Matching Pursuit (OMP) called Winner-Take-All Orthogonal Matching Pursuit (WTA OMP). In terms of rate-distortion trade-off, SWTA AE outperforms auto-encoders but it is worse than WTA OMP. Besides, SWTA AE can compete with JPEG in terms of rate-distortion.	autoencoder;distortion;encoder;image compression;jpeg;matching pursuit;online advertising;openmp;sparse approximation;sparse matrix;weapon target assignment problem	Thierry Dumas;Aline Roumy;Christine Guillemot	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952409	computer vision;image compression;computer science;machine learning;pattern recognition;mathematics;matching pursuit	Vision	23.183353578704438	-47.919454066901224	53424
4482c2b95f76eb566c3aa230d6fa98a3abec57c7	controllable semantic image inpainting		We develop a method for user-controllable semantic image inpainting: Given an arbitrary set of observed pixels, the unobserved pixels can be imputed in a user-controllable range of possibilities, each of which is semantically coherent and locally consistent with the observed pixels. We achieve this using a deep generative model bringing together: an encoder which can encode an arbitrary set of observed pixels, latent variables which are trained to represent disentangled factors of variations, and a bidirectional PixelCNN model. We experimentally demonstrate that our method can generate plausible inpainting results matching the user-specified semantics, but is still coherent with observed pixels. We justify our choices of architecture and training regime through more experiments.	coherence (physics);encode;encoder;experiment;generative model;inpainting;latent variable;pixel	Jin Xu;Yee Whye Teh	2018	CoRR		machine learning;artificial intelligence;encoder;mathematics;inpainting;pixel;generative model;architecture;semantics;latent variable	ML	23.983735458233863	-49.28231129187146	53518
ddcce5ec1e78563cd2616e909d324f8316f17955	improved random projection with $k$-means clustering for hyperspectral image classification		Random projection based dimensionality reduction methods are particularly attractive options for hyperspectral data analysis, due to their data independent representation, reduction in computation time and storage costs, while preserving data separability and important information at lower dimensions. In this work, we combine the benefits of dimensionality reduction using random projections with feature selection using $k$-means clustering in low dimensions to achieve a two-fold dimensionality reduction. Supervised classification using support vector machine (SVM) was done to study the classification performance. It is experimentally demonstrated that our proposed random projection based $k$-means feature selection methods offers superior classification performance at far fewer dimensions than original data without dimensionality reduction.	cluster analysis;computation;dimensionality reduction;experiment;feature selection;linear separability;machine learning;random projection;support vector machine;time complexity	Vineetha Menon;Qian Du;S. Christopher	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518450	support vector machine;computer vision;feature extraction;dimensionality reduction;feature selection;computer science;cluster analysis;artificial intelligence;k-means clustering;contextual image classification;pattern recognition;random projection	ML	14.013298151378711	-43.26260210203549	53563
692bc624976b791efdee000957b4275c416a6f81	a learning algorithm based on primary school teaching wisdom	training sample selection;sample selection;learning algorithm;primary school;empirical analysis;learning curve;optimal method;dierence boosting neural network;teaching and learning;data mining;feature space;artificial intelligent;model evaluation;incremental learning;machine learning;high school;science learning;neural network;optimization methods	A learning algorithm based on primary school teaching and learning is presented. The methodology is to continuously evaluate the performance of the network and to train it on the examples for which they repeatedly fail, until all the examples are correctly classified. Empirical analysis on UCI data show that the algorithm produces good training data and improves the generalization ability of the network on unseen data. The algorithm has interesting applications in data mining, model evaluations and rare objects discovery.	algorithm;artificial neural network;data mining;selection algorithm	Ninan Sajeeth Philip	2010	Paladyn	10.2478/s13230-011-0002-z	feature vector;wake-sleep algorithm;computer science;artificial intelligence;data science;machine learning;supervised learning;stability;learning curve;artificial neural network;generalization error	ML	14.720326632494654	-38.74070648586261	53598
24503262ca3d87e61db023776d5c4ef5176be202	deep double sparsity encoder: learning to sparsify not only features but also parameters		This paper emphasizes the significance to jointly exploit the problem structure and the parameter structure, in the context of deep modeling. As a specific and interesting example, we describe the deep double sparsity encoder (DDSE), which is inspired by the double sparsity model for dictionary learning. DDSE simultaneously sparsifies the output features and the learned model parameters, under one unified framework. In addition to its intuitive model interpretation, DDSE also possesses compact model size and low complexity. Extensive simulations compare DDSE with several carefully-designed baselines, and verify the consistently superior performance of DDSE.	dictionary;encoder;machine learning;simulation;sparse matrix;unified framework	Zhangyang Wang;Liang Zhang;Yingzhen Yang;Jiayu Zhou;Georgios B. Giannakis;Thomas S. Huang	2016	CoRR		simulation;computer science;artificial intelligence;machine learning	AI	21.876232574742758	-47.64664685783959	53884
9a4da6dbce3835f1467c85598174391b0deb7c79	a new ensemble-based cascaded framework for multiclass training with simple weak learners	computer vision;learning methods	We present a novel approach to multiclass learning using an ensemblebased cascaded learning framework. By implementing a multiclass cascaded classifier with AdaBoost, we show how detection runtimes are accelerated since only a subset of the ensemble is executed, thus making the classifiers suitable for computer vision applications. We also propose a new multiclass weak learner and demonstrate the framework's ability to achieve arbitrarily low training errors in conjunction with it. We tested our algorithm against AdaBoost.OC, ECC and M2 multiclass learning methods, on seven benchmark UCI datasets. In our experiments, we found that our framework achieves higher accuracy on five out of seven datasets and displays faster runtime efficiency in all cases.		Teo Susnjak;Andre L. C. Barczak;Napoleon H. Reyes;Kenneth A. Hawick	2011		10.1007/978-3-642-23672-3_68	computer vision;computer science;machine learning;multiclass classification;pattern recognition;data mining	Vision	14.406448572208367	-40.242263974369465	54074
6e688a8215b42d9bc881ef815655933d17607944	capsule networks for low-data transfer learning		We propose a capsule network-based architecture for generalizing learning to new data with few examples. Using both generative and non-generative capsule networks with intermediate routing, we are able to generalize to new information over 25 times faster than a similar convolutional neural network. We train the networks on the multiMNIST dataset lacking one digit. After the networks reach their maximum accuracy, we inject 1-100 examples of the missing digit into the training set, and measure the number of batches needed to return to a comparable level of accuracy. We then discuss the improvement in low-data transfer learning that capsule networks bring, and propose future directions for capsule research.		Andrew Gritsevskiy;Maksym Korablyov	2018	CoRR		fold (higher-order function);transfer of learning;pattern recognition;convolutional neural network;machine learning;architecture;computer science;data transmission;artificial intelligence;training set;generalization	ML	22.17334394280899	-50.60705347491173	54222
930eaf03e21ae59e2309722a310fc67d0e541560	transductive learning on adaptive graphs	transductive learning;classification;semi supervised learning	Graph-based semi-supervised learning methods are based on some smoothness assumption about the data. As a discrete approximation of the data manifold, the graph plays a crucial role in the success of such graphbased methods. In most existing methods, graph construction makes use of a predefined weighting function without utilizing label information even when it is available. In this work, by incorporating label information, we seek to enhance the performance of graph-based semi-supervised learning by learning the graph and label inference simultaneously. In particular, we consider a particular setting of semi-supervised learning called transductive learning. Using the LogDet divergence to define the objective function, we propose an iterative algorithm to solve the optimization problem which has closed-form solution in each step. We perform experiments on both synthetic and real data to demonstrate improvement in the graph and in terms of classification accuracy.	algorithm;approximation;data point;experiment;iterative method;laplacian matrix;mathematical optimization;optimization problem;semi-supervised learning;semiconductor industry;semidefinite programming;supervised learning;synthetic intelligence;transduction (machine learning);weight function	Yan-Ming Zhang;Yu Zhang;Dit-Yan Yeung;Cheng-Lin Liu;Xinwen Hou	2010			semi-supervised learning;biological classification;transduction;computer science;machine learning;pattern recognition;data mining;graph;generalization error	AI	24.468903965094402	-38.19942866907172	54254
883982ff94333a472aa3422deeb7ae1098966e0c	learning translation invariant kernels for classification	translation invariant;support vector machines;translation invariant kernels;capacity control;classification;kernel learning;semi infinite programming	Appropriate selection of the kernel function, which implic it y defines the feature space of an algorithm, has a crucial role in the success of kernel methods. In this paper, we consider the problem of optimizing a kernel function over the class of translatio n invariant kernels for the task of binary classification. The learning capacity of this class is invar iant with respect to rotation and scaling of the features and it encompasses the set of radial kernels. We show that how translation invariant kernel functions can be embedded in a nested set of sub-class es nd consider the kernel learning problem over one of these sub-classes. This allows the choic e of an appropriate sub-class based on the problem at hand. We use the criterion proposed by Lanck riet et al. (2004) to obtain a functional formulation for the problem. It will be proven that th e optimal kernel is a finite mixture of cosine functions. The kernel learning problem is then formu lated as a semi-infinite programming (SIP) problem which is solved by a sequence of quadratically onstrained quadratic programming (QCQP) sub-problems. Using the fact that the cosine kernel i s of rank two, we propose a formulation of a QCQP sub-problem which does not require the kernel m atrices to be loaded into memory, making the method applicable to large-scale problems. We al so ddress the issue of including other classes of kernels, such as individual kernels and iso tropic Gaussian kernels, in the learning process. Another interesting feature of the proposed metho d is that the optimal classifier has an expansion in terms of the number of cosine kernels, instead o f support vectors, leading to a remarkable speedup at run-time. As a by-product, we also generaliz the kernel trick to complex-valued kernel functions. Our experiments on artificial and real-wo rld benchmark data sets, including the USPS and the MNIST digit recognition data sets, show the usef uln ss of the proposed method.	algorithm;benchmark (computing);binary classification;discrete cosine transform;embedded system;experiment;feature vector;image scaling;kernel (operating system);kernel method;mnist database;quadratic programming;quadratically constrained quadratic program;radial (radio);radial basis function;semi-infinite programming;semiconductor industry;speedup	Sayed Kamaledin Ghiasi Shirazi;Reza Safabakhsh;Mostafa Shamsi	2010	Journal of Machine Learning Research		kernel density estimation;support vector machine;kernel method;mathematical optimization;string kernel;kernel embedding of distributions;radial basis function kernel;biological classification;kernel principal component analysis;computer science;machine learning;pattern recognition;reproducing kernel hilbert space;graph kernel;mathematics;tree kernel;variable kernel density estimation;polynomial kernel;kernel smoother	ML	23.696007149095056	-40.50414485433542	54323
bbcb926f5ba3eac8be59d5d4dbdc745f40aa6809	an effective intrusion detection framework based on svm with feature augmentation		Abstract Network security is becoming increasingly important in our daily lives—not only for organizations but also for individuals. Intrusion detection systems have been widely used to prevent information from being compromised, and various machine-learning techniques have been proposed to enhance the performance of intrusion detection systems. However, higher-quality training data is an essential determinant that could improve detection performance. It is well known that the marginal density ratio is the most powerful univariate classifier. In this paper, we propose an effective intrusion detection framework based on a support vector machine (SVM) with augmented features. More specifically, we implement the logarithm marginal density ratios transformation to form the original features with the goal of obtaining new and better-quality transformed features that can greatly improve the detection capability of an SVM-based detection model. The NSL-KDD dataset is used to evaluate the proposed method, and the empirical results show that it achieves a better and more robust performance than existing methods in terms of accuracy, detection rate, false alarm rate and training speed.	intrusion detection system	Huiwen Wang;Jie Gu;Shanshan Wang	2017	Knowl.-Based Syst.	10.1016/j.knosys.2017.09.014	support vector machine;data mining;machine learning;artificial intelligence;network security;marginal distribution;computer science;training set;logarithm;intrusion detection system;constant false alarm rate;univariate;pattern recognition	ML	16.810684919309658	-39.71209971871909	54398
76e73c5933b82510b939eec130ec6754d3c4bfc5	an incremental self-trained ensemble algorithm		Incremental learning has boosted the speed of Data Mining algorithms without sacrificing much, or sometimes none, predictive accuracy. Instead, by saving computational resources, combination of such kind of algorithms with iterative procedures that improve the learned hypothesis utilizing vast amounts of available unlabeled data could be achieved efficiently, in contrast to supervised scenario where all this information is rejected because no exploitation mechanism exists. The scope of this work is to examine the ability of a learning scheme that operates under shortage of labeled data for classification tasks, based on an incrementally updated ensemble algorithm. Comparisons against 30 state-of-the art Semi-supervised methods over 50 publicly available datasets are provided, supporting our assumptions about the learning quality of the proposed algorithm.	algorithm;computational resource;data mining;iteration;semiconductor industry;supervised learning	Stamatis Karlos;Nikos Fazakis;Konstantinos Kaleris;Vasileios G. Kanas;Sotiris Kotsiantis	2018	2018 IEEE Conference on Evolving and Adaptive Intelligent Systems (EAIS)	10.1109/EAIS.2018.8397180	task analysis;incremental learning;data modeling;labeled data;statistical classification;economic shortage;algorithm;computer science	ML	17.35514974351914	-39.257588769657026	54467
a95d928b1907e75ed87109494fa972d639122fde	optimal classifiers with minimum expected error within a bayesian framework - part i: discrete and gaussian models	genomics;bayesian estimation;minimum mean square estimation;small samples;classification;error estimation	In recent years, biomedicine has faced a flood of difficult small-sample phenotype discrimination problems. A host of classification rules have been proposed to discriminate types of pathology, stages of disease and other diagnoses. Typically, these classification rules are heuristic algorithms, with very little understood about their performance. To give a concrete mathematical structure to the problem, recent work has utilized a Bayesian modeling framework based on an uncertainty class of feature-label distributions to both optimize and analyze error estimator performance. The current study uses the same Bayesian framework to also optimize classifier design. This completes a Bayesian theory of classification, where both the classifier error and the estimate of the error may be optimized and studied probabilistically within the model framework. This paper, the first of a two-part study, derives optimal classifiers in discrete and Gaussian models, demonstrates their superior performance over popular classifiers within the assumed model, and applies the method to real genomic data. The second part of the study discusses properties of these optimal Bayesian classifiers. & 2012 Elsevier Ltd. All rights reserved.	algorithm;bayesian network;carrier-to-noise ratio;error detection and correction;estimation theory;heuristic;information;loss function;mathematical optimization;mathematical structure;naive bayes classifier;optimal design	Lori A. Dalton;Edward R. Dougherty	2013	Pattern Recognition	10.1016/j.patcog.2012.10.018	bayesian average;genomics;bayes estimator;variable-order bayesian network;biological classification;computer science;machine learning;pattern recognition;statistics	ML	18.08964692549906	-38.348880327630845	54491
f6b4811c5e7111485e2c9cc5bf63f8ac80f3e2d7	face verification via class sparsity based supervised encoding	convolution;training;supervised feature learning face verification deep learning;face encoding training convolution machine learning face recognition algorithm design and analysis;face verification deep learning supervised feature learning autoencoders;face recognition;machine learning;face;encoding;algorithm design and analysis	"""Autoencoders are deep learning architectures that learn feature representation by minimizing the reconstruction error. Using an autoencoder as baseline, this paper presents a novel formulation for a class sparsity based supervised encoder, termed as CSSE. We postulate that features from the same class will have a common sparsity pattern/support in the latent space. Therefore, in the formulation of the autoencoder, a supervision penalty is introduced as a joint-sparsity promoting <inline-formula><tex-math notation=""""LaTeX"""">$l_{2,1}$</tex-math><alternatives> <inline-graphic xlink:href=""""vatsa-ieq1-2569436.gif""""/></alternatives></inline-formula>-norm. The formulation of CSSE is derived for a single hidden layer and it is applied for multiple hidden layers using a greedy layer-by-layer learning approach. The proposed CSSE approach is applied for learning face representation and verification experiments are performed on the LFW and PaSC face databases. The experiments show that the proposed approach yields improved results compared to autoencoders and comparable results with state-of-the-art face recognition algorithms."""	architecture as topic;assertion (software development);autoencoder;baseline (configuration management);class;code for the japanese graphic character set for information interchange (jis x 0208-1990),;database;deep learning;encode;encoder device component;experiment;facial recognition system;feature learning;greedy algorithm;pancreatic stellate cells;promotion (action);sparse matrix;tracer;verification of theories;xlink;anatomical layer	Angshul Majumdar;Richa Singh;Mayank Vatsa	2017	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2016.2569436	semi-supervised learning;facial recognition system;face;algorithm design;feature learning;speech recognition;computer science;machine learning;pattern recognition;mathematics;geometry;convolution;encoding	Vision	24.000998038635128	-47.152141625025095	54674
5c5cac021cd916f5e9cbbb54a62712fd25bc1b74	svm and rls models for cancer classification		Support Vector Machines (SVMs) are the state-of-the-art supervised learning techniques for cancer classification. Other machine learning approaches such as Regularized Least Square (RLS) classifiers may represent a highly suitable alternative for their simplicity and reliability. We then compared the performance of the RLS classifiers, originally proposed in regularization theory, with SVM. The performances of both approaches have been also investigated on three different benchmark datasets, also with respect to the number of selected genes and different gene selection strategies.	benchmark (computing);machine learning;performance;recursive least squares filter;supervised learning;support vector machine	Nicola Ancona;Annarita D'Addabbo;Sabino Liuni;Graziano Pesole;Rosalia Maglietta	2005			machine learning;artificial intelligence;pattern recognition;cancer;computer science;support vector machine	ML	12.083933550635045	-42.03486340935679	54685
5ca78543e5ccb594a7d453f06f477a2c674f2f31	a novel parametric-insensitive nonparallel support vector machine for regression	noise heteroscedastic model;bound estimation;期刊论文;regression estimation;support vector machine;parametric insensitive model	In this paper, a novel parametric-insensitive nonparallel support vector regression (PINSVR) algorithm for data regression is proposed. PINSVR indirectly finds a pair of nonparallel proximal functions with a pair of different parametric-insensitive nonparallel proximal functions by solving two smaller sized quadratic programming problems (QPPs). By using new parametric-insensitive loss functions, the proposed PINSVR automatically adjusts a flexible parametric-insensitive zone of arbitrary shape and minimal size to include the given data to capture data structure and boundary information more accurately. The experiment results compared with the e-SVR, e-TSVR, and TPISVR indicate that our PINSVR not only obtains comparable regression performance, but also obtains better bound estimations.	support vector machine	Zhi-Min Yang;Xiang-Yu Hua;Yuan-Hai Shao;Ya-Fen Ye	2016	Neurocomputing	10.1016/j.neucom.2015.07.003	support vector machine;computer science;machine learning;pattern recognition;mathematics;statistics	ML	21.34427072435268	-39.78331969007887	54914
2b936d50caedc4bb5d6dc6c36d1373dca4783c24	prior classification of stego containers as a new approach for enhancing steganalyzers accuracy		We introduce a novel “prior classification” approach which can be employed in order to enhance the accuracy of stego detectors as well as to estimate it more subtly. The prior classification is intended for selection a subset of a testing set with such a property that a detection error, calculated over this subset, may be substantially lower than that calculated over the whole set. Our experiments demonstrated that it is possible to select about 30 % of the BOSSbase images for which HUGO 0.4 bpp is detected with the error less than 0.003, while the error over the whole set is 0.141. We also demonstrated that it is possible to find about 5 % of the BOSSbase images which provide the detection error for HUGO 0.1 bpp less than 0.05, while the error, calculated over the whole set, is about 0.37 which is not quite a reliable accuracy.	steganography	Viktor Monarev;Andrey Pestunov	2015		10.1007/978-3-319-29814-6_38	information hiding;steganography;distributed computing;computer science;artificial intelligence;steganalysis;pattern recognition	HCI	16.24821411312503	-50.568121867135645	54947
b9706fcd99f1984962c24a02356d98c46039369d	improved adaptive splitting and selection: the hybrid training method of a classifier based on a feature space partitioning	classifier ensemble;combined classifier;machine learning;pattern classification;evolutionary algorithm;article;hybrid algorithm	Currently, methods of combined classification are the focus of intense research. A properly designed group of combined classifiers exploiting knowledge gathered in a pool of elementary classifiers can successfully outperform a single classifier. There are two essential issues to consider when creating combined classifiers: how to establish the most comprehensive pool and how to design a fusion model that allows for taking full advantage of the collected knowledge. In this work, we address the issues and propose an AdaSS+, training algorithm dedicated for the compound classifier system that effectively exploits local specialization of the elementary classifiers. An effective training procedure consists of two phases. The first phase detects the classifier competencies and adjusts the respective fusion parameters. The second phase boosts classification accuracy by elevating the degree of local specialization. The quality of the proposed algorithms are evaluated on the basis of a wide range of computer experiments that show that AdaSS+ can outperform the original method and several reference classifiers.	cluster analysis;control theory;decision support systems, clinical;decision support system;ensemble kalman filter;evolutionary algorithm;experiment;fr 139317;feature vector;genetic selection;learning classifier system;linear classifier;mathematical optimization;nl (complexity);numerical aperture;partial template specialization;performance tuning;personnameuse - assigned;population parameter;sl (complexity);sodium;space partitioning;statistical classification;status epilepticus;unit per liter;unstable medical device problem;usability;user (computing);weight;statistical cluster	Konrad Jackowski;Bartosz Krawczyk;Michal Wozniak	2014	International journal of neural systems	10.1142/S0129065714300071	random subspace method;margin classifier;cascading classifiers;hybrid algorithm;quadratic classifier;computer science;machine learning;evolutionary algorithm;linear classifier;pattern recognition;data mining	Vision	10.678670518204079	-45.10454649830063	55021
3a54b23cdbd159bb32c39c3adcba8229e3237e56	adversarial attacks on face detectors using neural net based constrained optimization		Adversarial attacks involve adding, small, often imperceptible, perturbations to inputs with the goal of getting a machine learning model to misclassifying them. While many different adversarial attack strategies have been proposed on image classification models, object detection pipelines have been much harder to break. In this paper, we propose a novel strategy to craft adversarial examples by solving a constrained optimization problem using an adversarial generator network. Our approach is fast and scalable, requiring only a forward pass through our trained generator network to craft an adversarial sample. Unlike in many attack strategies we show that the same trained generator is capable of attacking new images without explicitly optimizing on them. We evaluate our attack on a trained Faster R-CNN face detector on the cropped 300-W face dataset where we manage to reduce the number of detected faces to 0.5% of all originally detected faces. In a different experiment, also on 300-W, we demonstrate the robustness of our attack to a JPEG compression based defense typical JPEG compression level of 75% reduces the effectiveness of our attack from only 0.5% of detected faces to a modest 5.0%.		Avishek Joey Bose;Parham Aarabi	2018	2018 IEEE 20th International Workshop on Multimedia Signal Processing (MMSP)	10.1109/MMSP.2018.8547128	robustness (computer science);pattern recognition;artificial neural network;face detection;machine learning;artificial intelligence;computer science;scalability;object detection;constrained optimization;jpeg;contextual image classification	Vision	19.401363829335406	-51.18872182272978	55065
82758669f4db9ddc68f82f6d7763f2c8c02e9ea4	gksh: graph based image retrieval using supervised kernel hashing	graph of attribute association;supervised kernel hashing;graph kernel;image retrieval	The explosive growth of the massive image database brings great challenge to the fast and accurate image retrieval. To address this issue, we propose a kind of supervised hashing method based on the graph representation of the images, which translates images into representative attribute structural graphs. Compared with the traditional supervised methods, this kind of structural graphs can consider the spatial relations among regions in the image, and integrate unsupervised properties of images and supervised information of labels by adopting hashing with graph kernel based on random walks. The learnt hash codes can be a good tradeoff among retrieval speed, memory requirements and retrieval accuracy. The experiments on three image datasets PASCAL, MNIST and HOLIDAY demonstrate that, with the hash codes of the same length, the proposed supervised hashing method can achieve a higher precision and a better efficiency.	code;experiment;graph (abstract data type);graph kernel;hash function;image retrieval;kernel (operating system);mnist database;requirement;supervised learning;unsupervised learning	Haiying Shen;Bo Lang;Yang Liu	2016		10.1145/3007669.3007722	feature hashing;hash table;dynamic perfect hashing;theoretical computer science;machine learning;universal hashing;pattern recognition;mathematics;locality preserving hashing	Vision	23.242217300170715	-51.28421366552222	55078
733e4973aec6d9de139781a76ca2f6b3f05b293b	ensemble feature weighting based on local learning and diversity	ensemble;feature selection	Recently, besides the performance, the stability (robustness, i.e., the variation in feature selection results due to small changes in the data set) of feature selection is received more attention. Ensemble feature selection where multiple feature selection outputs are combined to yield more robust results without sacrificing the performance is an effective method for stable feature selection. In order to make further improvements of the performance (classification accuracy), the diversity regularized ensemble feature weighting framework is presented, in which the base feature selector is based on local learning with logistic loss for its robustness to huge irrelevant features and small samples. At the same time, the sample complexity of the proposed ensemble feature weighting algorithm is analyzed based on the VCtheory. The experiments on different kinds of data sets show that the proposed ensemble method can achieve higher accuracy than other ensemble ones and other stable feature selection strategy (such as sample weighting) without sacrificing stability.	algorithm;effective method;ensemble learning;experiment;feature selection;loss functions for classification;relevance;sample complexity;vapnik–chervonenkis theory	Yun Li;Su-Yan Gao;Songcan Chen	2012			ensembl;minimum redundancy feature selection;computer science;machine learning;kanade–lucas–tomasi feature tracker;pattern recognition;data mining;ensemble learning;feature selection;feature;dimensionality reduction	ML	14.84460464971488	-41.03388017287882	55370
3a4da7c78abc3a841943f740bb2a8c47037d8a10	semi-supervised correction of biased comment ratings	active learning;semi supervised learning;fixed point;iterative technique;empirical risk minimization;bias	In many instances, offensive comments on the internet attract a disproportionate number of positive ratings from highly biased users. This results in an undesirable scenario where these offensive comments are the top rated ones. In this paper, we develop semi-supervised learning techniques to correct the bias in user ratings of comments. Our scheme uses a small number of comment labels in conjunction with user rating information to iteratively compute user bias and unbiased ratings for unlabeled comments. We show that the running time of each iteration is linear in the number of ratings, and the system converges to a unique fixed point. To select the comments to label, we devise an active learning algorithm based on empirical risk minimization. Our active learning method incrementally updates the risk for neighboring comments each time a comment is labeled, and thus can easily scale to large comment datasets. On real-life comments from Yahoo! News, our semi-supervised and active learning algorithms achieve higher accuracy than simple baselines, with few labeled examples.	active learning (machine learning);algorithm;empirical risk minimization;fixed point (mathematics);internet;iteration;machine learning;real life;semi-supervised learning;semiconductor industry;supervised learning;time complexity	Abhinav Mishra;Rajeev Rastogi	2012		10.1145/2187836.2187862	semi-supervised learning;empirical risk minimization;computer science;machine learning;bias;data mining;fixed point;active learning	AI	18.017434987803163	-40.49902615221886	55475
5456166e3bfe78a353df988897ec0bd66cee937f	improved boosting performance by exclusion of ambiguous positive examples	computer and information science;data och informationsvetenskap	In visual object class recognition it is difficult to densely sample the set of positive examples. Therefore, frequently there will be areas of the feature space that are sparsely populated, in which uncommon examples are hard to disambiguate from surrounding negatives without overfitting. Boosting in particular struggles to learn optimal decision boundaries in the presence of such hard and ambiguous examples. We propose a twopass dataset pruning method for identifying ambiguous examples and subjecting them to an exclusion function, in order to obtain more optimal decision boundaries for existing boosting algorithms. We also provide an experimental comparison of different boosting algorithms on the VOC2007 dataset, training them with and without our proposed extension. Using our exclusion extension improves the performance of all the tested boosting algorithms except TangentBoost, without adding any additional test-time cost. In our experiments LogitBoost performs best overall and is also significantly improved by our extension. Our results also suggest that outlier exclusion is complementary to positive jittering and hard negative mining.	algorithm;boosting (machine learning);bootstrapping (compilers);experiment;feature vector;logitboost;overfitting;population;shortest seek first;weight function	Miroslav Kobetski;Josephine Sullivan	2013			computer science;artificial intelligence;machine learning;data mining;information and computer science	ML	16.641166563333158	-41.60456385802199	55519
729897f9d769826b0a80eaf526b4b7d1a6187849	sphere-structured support vector machines for multi-class pattern recognition	quadratic programming;learning algorithm;quadratic program;analisis estadistico;support vector machines;programmation quadratique;convex programming;convex quadratic programming;sphere structured;kernel function;programmation convexe;intelligence artificielle;aprendizaje probabilidades;algorithme apprentissage;classification;sphere;large scale;statistical analysis;statistical learning theory;multi class classification;multi class pattern recognition;machine exemple support;funcion nucleo;analyse statistique;fonction noyau;pattern recognition;apprentissage probabilites;artificial intelligence;programacion cuadratica;binary classification;esfera;inteligencia artificial;reconnaissance forme;support vector machine;vector support machine;reconocimiento patron;escala grande;algoritmo aprendizaje;clasificacion;large scale problem;probability learning;echelle grande;programacion convexa	Support vector machines (SVM) are learning algorithms derived from statistical learning theory. The SVM approach was originally developed for binary classification problems. For solving multi-class classification problem, there are some methods such as one-against-rest, one-against-one, alltogether and so on. But the computing time of all these methods are too long to solve large scale problem. In this paper SVMs architectures for multi-class problems are discussed, in particular we provide a new algorithm called spherestructured SVMs to solve the multi-class problem. We show the algorithm in detail and analyze its characteristics. Not only the number of convex quadratic programming problems in sphere-structured SVMs is small, but also the number of variables in each programming is least. The computing time of classification is reduced. Otherwise, the characteristics of sphere-structured SVMs make expand data easily.	algorithm;binary classification;machine learning;multiclass classification;pattern recognition;quadratic programming;structured support vector machine	Meilin Zhu;Yue Wang;Shifu Chen;Xiangdong Liu	2003		10.1007/3-540-39205-X_95	support vector machine;least squares support vector machine;computer science;artificial intelligence;machine learning;mathematics;quadratic programming;algorithm	ML	22.752203313977326	-38.63087792299389	55750
e7e4cf41b9931979c763134520eacef4766ab421	deterministic binary filters for convolutional neural networks		We propose Deterministic Binary Filters, an approach to Convolutional Neural Networks that learns weighting coefficients of predefined orthogonal binary basis instead of the conventional approach of learning directly the convolutional filters. This approach results in architectures offering significantly fewer parameters (4× to 16×) and smaller model sizes (up to 32× due to the use of binary rather than floating point precision). We show our deterministic filter design can be integrated into well-known network architectures (such as ResNet and SqueezeNet) with as little as 2% loss of accuracy under datasets like CIFAR-10. Under ImageNet, they are used in an architectures 3× smaller compared to sub-megabyte binary networks while reaching comparable accuracy levels.		Vincent W. S. Tseng;Sourav Bhattacharya;Javier Fernández-Marqués;Milad Alizadeh;Catherine Tong;Nicholas D. Lane	2018		10.24963/ijcai.2018/380	artificial intelligence;convolutional neural network;algorithm;machine learning;residual neural network;floating point;network architecture;filter design;binary number;computer science;weighting	AI	22.60562118284141	-50.698686831033605	55773
0535625be630c6a67f4c244ebf3aa61ad088fc70	ganomaly: semi-supervised anomaly detection via adversarial training		Anomaly detection is a classical problem in computer vision, namely the determination of the normal from the abnormal when datasets are highly biased towards one class (normal) due to the insufficient sample size of the other class (abnormal). While this can be addressed as a supervised learning problem, a significantly more challenging problem is that of detecting the unknown/unseen anomaly case that takes us instead into the space of a one-class, semi-supervised learning paradigm. We introduce such a novel anomaly detection model, by using a conditional generative adversarial network that jointly learns the generation of high-dimensional image space and the inference of latent space. Employing encoder-decoder-encoder sub-networks in the generator network enables the model to map the input image to a lower dimension vector, which is then used to reconstruct the generated output image. The use of the additional encoder network maps this generated image to its latent representation. Minimizing the distance between these images and the latent vectors during training aids in learning the data distribution for the normal samples. As a result, a larger distance metric from this learned data distribution at inference time is indicative of an outlier from that distribution — an anomaly. Experimentation over several benchmark datasets, from varying domains, shows the model efficacy and superiority over previous state-of-the-art approaches.		Samet Akcay;Amir Atapour Abarghouei;Toby P. Breckon	2018	CoRR		supervised learning;adversarial system;metric (mathematics);encoder;machine learning;generative grammar;artificial intelligence;outlier;pattern recognition;anomaly detection;computer science;inference	ML	23.231807827398796	-46.811455782609904	55866
bc8d342461142aad3a71f467731d0f999f75750f	labeled dbn learning with community structure knowledge		Learning interactions between dynamical processes is a widespread but difficult problem in ecological or human sciences. Unlike in other domains (bioinformatics, for example), data is often scarce, but expert knowledge is available. We consider the case where knowledge is about a limited number of interactions that drive the processes dynamics, and on a community structure in the interaction network. We propose an original framework, based on Dynamic Bayesian Networks with labeled-edge structure and parsimonious parameterization, and a Stochastic Block Model prior, to integrate this knowledge. Then we propose a restoration-estimation algorithm, based on 0-1 Linear Programing, that improves network learning when these two types of expert knowledge are available. The approach is illustrated on a problem of ecological interaction network learning.	algorithm;bioinformatics;circuit restoration;dynamic bayesian network;ecology;interaction network;occam's razor;stochastic block model	Etienne Auclair;Nathalie Peyrard;Régis Sabbadin	2017		10.1007/978-3-319-71246-8_10	machine learning;dynamic bayesian network;human science;community structure;linear programming;interaction network;stochastic block model;artificial intelligence;computer science	AI	12.834375727647203	-50.78575674459427	55904
3b7eb7d755bafc6b20458d3559603be8c31417d8	support vector optimization through hybrids: heuristics and math approach	quadratic program;conjugate gradient method;support vector;density estimation;quadratic optimization;support vector machine;large scale problem	This paper presents a strategy to optimize the learning phase of the Support Vector Machines algorithm (SVM). The SVM algorithm is widely used in solving different tasks like classification, regression, density estimation and clustering problems. However, the algorithm presents important disadvantages when learning large scale problems. Training a SVM involves finding the solution of a quadratic optimization problem (QP), which is very resource consuming. What is more, during the learning step, the best working set must be selected, which is a hard to perform task. In this work, we combine a heuristic approach, which selects the best working set data, with a projected conjugate gradient method, which is a fast and easy to implement algorithm that solves the quadratic programming problem involved in the SVM algorithm. We compare the performances of the optimization strategies using some well-known benchmark databases.	heuristic (computer science);vector optimization	Ariel García-Gamboa;Neil Hernández-Gress;Miguel González-Mendoza;Jaime Mora-Vargas	2009		10.1007/978-3-642-05258-3_21	support vector machine;least squares support vector machine;mathematical optimization;computer science;machine learning;pattern recognition;mathematics;sequential minimal optimization;ranking svm;quadratic programming;structured support vector machine	Logic	21.302097866302102	-38.55135211000386	55920
c7daacc324867cd4c057040c65d0d9de773a1a22	a novel information theoretic-interact algorithm (it-in) for feature selection using three machine learning algorithms	data mining;redundancy;machine learning;feature selection;hash function;relevance;correlation;classification accuracy;information theoretic;consistency	The inclusion of irrelevant, redundant, and inconsistent features in the data-mining model results in poor predictions and high computational overhead. This paper proposes a novel information theoretic-based interact (IT-IN) algorithm, which concerns the relevance, redundancy, and consistency of the features. The proposed IT-IN algorithm is compared with existing Interact, FCBF, Relief and CFS feature selection algorithms. To evaluate the classification accuracy of IT-IN and remaining four feature selection algorithms, Naive Bayes, SVM, and ELM classifier are used for ten UCI repository datasets. The proposed IT-IN performs better than existing above algorithms in terms of number of features. The specially designed hash function is used to speed up the IT-IN algorithms and provides minimum computation time than the Interact algorithms. The result clearly reveals that the proposed feature selection algorithm improves the classification accuracy for ELM, Naive Bayes, and SVM classifiers. The performance of proposed IT-IN with ELM classifier is superior to other classifiers.	algorithm;feature selection;information theory;machine learning	C. Deisy;S. Baskar;N. Ramaraj;J. Saravanan Koori;P. Jeevanandam	2010	Expert Syst. Appl.	10.1016/j.eswa.2010.04.084	hash function;relevance;minimum redundancy feature selection;computer science;machine learning;pattern recognition;data mining;redundancy;consistency;feature selection;correlation	ML	12.103226787570897	-43.37516304105107	56042
7ff9aee876325098c7eb7ec4aed5cda0a45a90e1	information discriminant analysis: feature extraction with an information-theoretic objective	bayes estimation;extraction information;information discriminant analysis;pattern classification bayes methods feature extraction information theory optimisation;optimisation;bayes error;optimizacion;information extraction;espace lineaire;bayes methods;analisis forma;bayes optimal information discriminant analysis feature extraction linear transformation low dimensional subspace numerical optimization information theoretic objective function linear discriminant analysis bayes error;information analysis feature extraction linear discriminant analysis mutual information data mining vectors principal component analysis sufficient conditions acceleration information theory;extraction forme;espacio lineal;vector space;informacion mutual;numerical optimization;intelligence artificielle;linear discriminate analysis;data mining;classification;sufficient conditions;fonction objectif;acceleration;objective function;discriminant analysis;analyse discriminante;analisis discriminante;estimacion bayes;information mutuelle;vectors;extraccion forma;feature extraction;principal component analysis;transformation lineaire;bayes optimal;pattern classification;pattern recognition;information theoretic objective function;linear transformation;mutual information;artificial intelligence;funcion objetivo;analyse information;optimization;pattern analysis;entropy;inteligencia artificial;espace vectoriel;reconnaissance forme;extraction caracteristique;theorie information;reconocimiento patron;linear space;bayes error feature extraction information theory mutual information entropy classification linear discriminant analysis;linear discriminant analysis;information theoretic;espacio vectorial;information analysis;pattern extraction;clasificacion;extraccion informacion;analyse forme;information theory;transformacion lineal;estimation bayes;low dimensional subspace;teoria informacion	Using elementary information-theoretic tools, we develop a novel technique for linear transformation from the space of observations into a low-dimensional (feature) subspace for the purpose of classification. The technique is based on a numerical optimization of an information-theoretic objective function, which can be computed analytically. The advantages of the proposed method over several other techniques are discussed and the conditions under which the method reduces to linear discriminant analysis are given. We show that the novel objective function enjoys many of the properties of the mutual information and the Bayes error and we give sufficient conditions for the method to be Bayes-optimal. Since the objective function is maximized numerically, we show how the calculations can be accelerated to yield feasible solutions. The performance of the method compares favorably to other linear discriminant-based feature extraction methods on a number of simulated and real-world data sets.	anterior descending branch of left coronary artery;chernoff bound;class;expectation–maximization algorithm;experiment;feature extraction;idarubicin;information theory;inosine dialdehyde;linear discriminant analysis;linear separability;loss function;mathematical optimization;mutual information;normal statistical distribution;numerical analysis;optimization problem;portable document format;simulation;solutions;sudamerlycaste	Zoran Nenadic	2007	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2007.1156	vector space;computer science;machine learning;pattern recognition;optimal discriminant analysis;mathematics;linear discriminant analysis;statistics	ML	23.403700466957623	-38.68024406416958	56049
b3720e3afe929b3fbf66dc967a58350c8c575568	text categorization using an ensemble classifier based on a mean co-association matrix	ensemble classification;text mining;consensus clustering;text categorization	Text Categorization (TC) has attracted the attention of the research community in the last decade. Algorithms like Support Vector Machines, Naïve Bayes or k Nearest Neighbors have been used with good performance, confirmed by several comparative studies. Recently, several ensemble classifiers were also introduced in TC. However, many of those can only provide a category for a given new sample. Instead, in this paper, we propose a methodology – MECAC – to build an ensemble of classifiers that has two advantages to other ensemble methods: 1) it can be run using parallel computing, saving processing time and 2) it can extract important statistics from the obtained clusters. It uses the mean co-association matrix to solve binary TC problems. Our experiments revealed that our framework performed, on average, 2.04% better than the best individual classifier on the tested datasets. These results were statistically validated for a significance level of 0.05 using the Friedman Test.	categorization;document classification;ensemble learning;experiment;k-nearest neighbors algorithm;naive bayes classifier;parallel computing;statistical classification;support vector machine	Luís Moreira-Matias;João Mendes-Moreira;João Gama;Pavel Brazdil	2012		10.1007/978-3-642-31537-4_41	random subspace method;text mining;computer science;machine learning;consensus clustering;pattern recognition;data mining;ensemble learning	ML	11.90323353495721	-42.36126101092534	56086
d301dc334ef78d791788dd2e9f23f4a8d25f02b8	tensor completion via multi-shared-modes canonical correlation analysis	high order tensor;multi shared modes;tucker n decomposition;low rank tensor completion;tensor canonical correlation analysis	Low-rank tensor completion (LRTC) has been applied in many real-world problems. But most of the existing LRTC methods recover a tensor on a single dataset with the low-rank assumption, suffering from a low accuracy due to the complicated structures of higher-order data. To address this issue, we propose a novel tensor completion method for two correlated tensor datasets obtained from different sources. We first introduce the correlated tensors with multiple shared modes via tensor canonical correlation analysis (TCCA), and reveal the relationship between the transformation matrices of TCCA and the Tucker decomposition. Then we develop a Tucker-n decomposition method with n invariant modes to capture the latent structures of incomplete tensors, in which sufficient discriminative information for TCCA can be flexibly maintained by varying the number of invariant modes. Finally, we combine the Tucker-n decomposition method for LRTC with the correlation of TCCA as a regularizer to improve the completion performance, and derive relative error bounds for our LRTC approach to guarantee the recovery accuracy. Experimental results on synthetic and real data demonstrate the accuracy and efficiency of the proposed approach, as well as the benefit of multiple shared modes.		Xiao Zhang;Shizhong Liao	2016	Neurocomputing	10.1016/j.neucom.2016.05.001	mathematical optimization;combinatorics;discrete mathematics;tensor;mathematics	AI	24.251549741024125	-43.758501227208015	56437
ac408b53292c4ad90ca9e7c20f804b2ff28ba5f0	exploring resampling with neighborhood bias on imbalanced regression problems		Imbalanced domains are an important problem that arises in predictive tasks causing a loss in the performance of the most relevant cases for the user. This problem has been intensively studied for classification problems. Recently it was recognized that imbalanced domains occur in several other contexts and for a diversity of types of tasks. This paper focus on imbalanced regression tasks. Resampling strategies are among the most successful approaches to imbalanced domains. In this work we propose variants of existing resampling strategies that are able to take into account the information regarding the neighborhood of the examples. Instead of performing sampling uniformly, our proposals bias the strategies for reinforcing some regions of the data sets. In an extensive set of experiments we provide evidence of the advantage of introducing a neighborhood bias in the resampling strategies.		Paula Branco;Luís Torgo;Rita P. Ribeiro	2017		10.1007/978-3-319-65340-2_42	machine learning;sampling (statistics);resampling;data set;computer science;pattern recognition;artificial intelligence	AI	14.692937261985252	-41.27891300538207	56668
06d6597aeaf272d2b7b1656d15c18f9e96ccdf19	harnessing the strengths of anytime algorithms for constant data streams	anytime algorithms;stream data mining;classification confidence	Anytime algorithms have been proposed for many different applications e.g. in data mining. Their strengths are the ability to first provide a result after a very short initialization and second to improve their result with additional time. Therefore, anytime algorithms have so far been used when the available processing time varies, e.g. on varying data streams. In this paper we propose to employ anytime algorithms on constant data streams, i.e. for tasks with constant time allowance. We introduce two approaches that harness the strengths of anytime algorithms on constant data streams and thereby improve the over all quality of the result with respect to the corresponding budget algorithm. We derive formulas for the expected performance gain and demonstrate the effectiveness of our novel approaches using existing anytime algorithms on benchmark data sets.#R##N##R##N#The goal that was set and reached in this paper is to improve the quality of the result over that of traditional budget approaches, which are used in an abundance of stream mining applications. Using anytime classification as an example application we show for SVM, Bayes and nearest neighbor classifiers that both our novel approaches improve the classification accuracy for slow and fast data streams. The results confirm our general theoretic models and show the effectiveness of our approaches. The simple yet effective idea can be employed for any anytime algorithm along with a quality measure and motivates further research in e.g. classification confidence measures or anytime algorithms.	anytime algorithm	Philipp Kranen;Thomas Seidl	2009		10.1007/978-3-642-04180-8_16	computer science;artificial intelligence;data science;machine learning;data mining	DB	13.769682862604363	-38.40840474236082	56675
0d6baaa4caec3a9a9eb0b786d373ac10806ca1ef	a comparison of different off-centered entropies to deal with class imbalance for decision trees	decision tree learning;shannon entropy;decision tree;class imbalance;data mining;imbalance class;off centered entropies;decision trees;imbalanced data sets	In data mining, large differences in prior class probabilities known as the class imbalance problem have been reported to hinder the performance of classifiers such as decision trees. Dealing with imbalanced and cost-sensitive data has been recognized as one of the 10 most challenging problems in data mining research. In decision trees learning, many measures are based on the concept of Shannon’s entropy. A major characteristic of the entropies is that they take their maximal value when the distribution of the modalities of the class variable is uniform. To deal with the class imbalance problem, we proposed an off-centered entropy which takes its maximum value for a distribution fixed by the user. This distribution can be the a priori distribution of the class variable modalities or a distribution taking into account the costs of misclassification. Others authors have proposed an asymmetric entropy. In this paper we present the concepts of the three entropies and compare their effectiveness on 20 imbalanced data sets. All our experiments are founded on the C4.5 decision trees algorithm, in which only the function of entropy is modified. The results are promising and show the interest of off-centered entropies to deal with the problem of class imbalance.	c4.5 algorithm;data mining;database;decision tree learning;experiment;maximal set;performance;shannon (unit)	Philippe Lenca;Stéphane Lallich;Thanh-Nghi Do;Nguyen-Khang Pham	2008		10.1007/978-3-540-68125-0_59	decision tree learning;computer science;machine learning;decision tree;pattern recognition;data mining;mathematics	ML	12.129518672385004	-39.27240207335863	56741
5e3ae55a8dc56a53ce85bc57d072ca8f6026e6e8	stream classification with recurring and novel class detection using class-based ensemble	training data models humans educational institutions electronic mail classification algorithms prediction algorithms;stream classification;data mining;recurring class;novel class;pattern classification;recurring class stream classification novel class;state of the art techniques stream classification class based ensemble novel class detection concept evolution data stream mining data stream classification techniques classifier prediction error computational resources memory waste class based ensemble technique chunk based ensemble approaches recurring class detection;pattern classification data mining	"""Concept-evolution has recently received a lot of attention in the context of mining data streams. Concept-evolution occurs when a new class evolves in the stream. Although many recent studies address this issue, most of them do not consider the scenario of recurring classes in the stream. A class is called recurring if it appears in the stream, disappears for a while, and then reappears again. Existing data stream classification techniques either misclassify the recurring class instances as another class, or falsely identify the recurring classes as novel. This increases the prediction error of the classifiers, and in some cases causes unnecessary waste in memory and computational resources. In this paper we address the recurring class issue by proposing a novel """"class-based"""" ensemble technique, which substitutes the traditional """"chunk-based"""" ensemble approaches and correctly distinguishes between a recurring class and a novel one. We analytically and experimentally confirm the superiority of our method over state-of-the-art techniques."""	benchmark (computing);computational resource;experiment;sensor;while	Tahseen Al-Khateeb;Mohammad M. Masud;Latifur Khan;Charu C. Aggarwal;Jiawei Han;Bhavani M. Thuraisingham	2012	2012 IEEE 12th International Conference on Data Mining	10.1109/ICDM.2012.125	computer science;machine learning;pattern recognition;data mining;data stream mining	DB	13.588530252319599	-38.67659903262216	56768
cdef2a30e1795d0835e8e364c30be39da6e3d6e7	comparative study of binary classification methods to analyze a massive dataset on virtual machine		Abstract Massive dataset can be analyzed by establishing physical distributed environment or by hiring cloud-based distributed environment. The advantage of cloud-based environment over physical environment is that, it provides scalable virtual resources on demand and thus makes it suitable for handling increase in volume of the data. The various hidden patterns in data can provide knowledge bases for decision making. The statistical or data mining based methods can be used for finding knowledge patterns. Among the decision tree based classification algorithms, implementable in distributed environment, an efficient algorithm can be selected based on few parameters such as execution time, accuracy of prediction and complexity of the tree structure. In this study, Apache Hadoop-based distributed environment is established on virtual machine. Apache Spark is installed to execute machine learning algorithms. The comparative study of binary classification methods such as decision tree, gradient boosted tree and random forest tree is performed to judge their performances on the basis of defined parameters. It is found that Random forest tree performs best among all three algorithms for the considered dataset.	binary classification;virtual machine	Neelam Naik;Seema Purohit	2017		10.1016/j.procs.2017.08.232	order statistic tree;machine learning;artificial intelligence;data mining;decision tree;tree traversal;incremental decision tree;tree structure;decision tree learning;computer science;fractal tree index;id3 algorithm	Metrics	12.734078110532664	-38.568619953012806	56797
2f831302f69a81006ee87edf2e9594337a18f73b	gmdh-based feature ranking and selection for improved classification of medical data	heart disease;learning algorithm;neural networks;receiver operator characteristic;neural network classifier;abductive networks;model complexity;ranking and selection;dimensionality reduction;roc analysis;classification error;roc curve;feature selection;medical application;feature ranking;classification accuracy;group method of data handling;dimensional reduction;breast cancer;roc characteristics;medical diagnosis;training algorithm;group selection;neural network	Medical applications are often characterized by a large number of disease markers and a relatively small number of data records. We demonstrate that complete feature ranking followed by selection can lead to appreciable reductions in data dimensionality, with significant improvements in the implementation and performance of classifiers for medical diagnosis. We describe a novel approach for ranking all features according to their predictive quality using properties unique to learning algorithms based on the group method of data handling (GMDH). An abductive network training algorithm is repeatedly used to select groups of optimum predictors from the feature set at gradually increasing levels of model complexity specified by the user. Groups selected earlier are better predictors. The process is then repeated to rank features within individual groups. The resulting full feature ranking can be used to determine the optimum feature subset by starting at the top of the list and progressively including more features until the classification error rate on an out-of-sample evaluation set starts to increase due to overfitting. The approach is demonstrated on two medical diagnosis datasets (breast cancer and heart disease) and comparisons are made with other feature ranking and selection methods. Receiver operating characteristics (ROC) analysis is used to compare classifier performance. At default model complexity, dimensionality reduction of 22 and 54% could be achieved for the breast cancer and heart disease data, respectively, leading to improvements in the overall classification performance. For both datasets, considerable dimensionality reduction introduced no significant reduction in the area under the ROC curve. GMDH-based feature selection results have also proved effective with neural network classifiers.	abductive reasoning;algorithm;artificial neural network;biological neural networks;default;dimensionality reduction;feature selection;group method of data handling;handling (psychology);heart diseases;machine learning;mammary neoplasms;overfitting;receiver operator characteristics;receiver operating characteristic;statistical classification;subgroup	R. E. Abdel-Aal	2005	Journal of biomedical informatics	10.1016/j.jbi.2005.03.003	computer science;machine learning;pattern recognition;data mining;receiver operating characteristic;artificial neural network;statistics	ML	10.325894932571835	-45.707651066523184	57167
d496c696007b8fe90c17d2d74b4578f26f592c0f	cluster-based minority over-sampling for imbalanced datasets		Synthetic over-sampling is a well-known method to solve class imbalance by modifying class distribution and generating synthetic samples. A large number of synthetic over-sampling techniques have been proposed; however, most of them suffer from the over-generalization problem whereby synthetic minority class samples are generated into the majority class region. Learning from an over-generalized dataset, a classifier could misclassify a majority class member as belonging to a minority class. In this paper a method called TRIM is proposed to overcome the over-generalization problem. The idea is to identify minority class regions that compromise between generalization and overfitting. TRIM identifies all the minority class regions in the form of clusters. Then, it merges a large number of small minority class clusters into more generalized clusters. To enhance the generalization ability, a cluster connection step is proposed to avoid over-generalization toward the majority class while increasing generalization of the minority class. As a result, the classifier is able to correctly classify more minority class samples while maintaining its precision. Compared with SMOTE and extended versions such as Borderline-SMOTE, experimental results show that TRIM exhibits significant performance improvement in terms of F-measure and AUC. TRIM can be used as a preprocessing step for synthetic over-sampling methods such as SMOTE and its extended versions. key words: imbalanced data, cluster-based minority over-sampling, synthetic minority over-sampling	computer cluster;f1 score;overfitting;oversampling;preprocessor;sampling (signal processing);synthetic data;synthetic intelligence	Kamthorn Puntumapon;Thanawin Rakthanmanon;Kitsana Waiyamai	2016	IEICE Transactions		artificial intelligence;pattern recognition;computer science;oversampling	ML	13.849104606223992	-41.11595276029603	57299
3aeb0a1b589aeea6fbab88e5e927cceb83e8408c	ontology sparse vector learning algorithm for ontology similarity measuring and ontology mapping via adal technology	ontology mapping;sparse vector learning;alternating direction augmented lagrangian;similarity measure;ontology	Ontology, a model of knowledge representation and storage, has had extensive applications in pharmaceutics, social science, chemistry and biology. In the age of “big data”, the constructed concepts are often represented as higher-dimensional data by scholars, and thus the sparse learning techniques are introduced into ontology algorithms. In this paper, based on the alternating direction augmented Lagrangian method, we present an ontology optimization algorithm for ontological sparse vector learning, and a fast version of such ontology technologies. The optimal sparse vector is obtained by an iterative procedure, and the ontology function is then obtained from the sparse vector. Four simulation experiments show that our ontological sparse vector learning model has a higher precision ratio on plant ontology, humanoid robotics ontology, biology ontology and physics education ontology data for similarity measuring and ontology mapping applications.	algorithm;semantic integration;sparse matrix	Wei Gao;Linli Zhu;Kaiyun Wang	2015	I. J. Bifurcation and Chaos	10.1142/S0218127415400349	upper ontology;ontology alignment;semantic integration;computer science;ontology;data science;ontology;data mining;ontology-based data integration;information retrieval;process ontology;suggested upper merged ontology	AI	15.436189684793007	-48.979538110003226	57445
68c1e2d4e1bedaec12bc9ca401d2c8e6e2fe7a25	soft-to-hard vector quantization for end-to-end learned compression of images and neural networks		In this work we present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives state-of-the-art results for both.	discretization;distortion;end-to-end principle;entropy (information theory);image compression;linear programming relaxation;neural networks;quantization (signal processing);rate–distortion theory;sample entropy;simulated annealing;unified framework;vector quantization	Eirikur Agustsson;Fabian Mentzer;Michael Tschannen;Lukas Cavigelli;Radu Timofte;Luca Benini;Luc Van Gool	2017	CoRR		simulation;learning vector quantization;computer science;theoretical computer science;machine learning;vector quantization	ML	22.559946478966875	-48.62296887216378	57679
6d7f7cb3b5ca011fba587c9acddb0a999628f70a	prbf kernels: a framework for the incorporation of task-specific properties into support vector methods	kernel tensor product prior knowledge support vector methods rbf;kernel training data support vector machines correlation standards labeling tensile stress;support vector machines;prior knowledge;rbf;radial basis function networks;support vector methods;learning prbf kernels task specific properties support vector methods svm inadequate training data broad domain knowledge;learning artificial intelligence;support vector machines learning artificial intelligence radial basis function networks;kernel tensor product	The incorporation of prior-knowledge into support vector machines (SVM) in order to compensate for inadequate training data has been the focus of previous research works and many found a kernel-based approach to be the most appropriate. However, they are more adapted to deal with broad domain knowledge (e.g. ``sets are invariant to permutations of the elements'') rather than task-specific properties (e.g. ``the weight of a person is cubically related to her height''). In this paper, we present the partially RBF (pRBF) kernels, our original framework for the incorporation of prior-knowledge about correlation patterns between specific features and the output label. pRBF kernels are based upon the tensor-product combination of the standard radial basis function (RBF) kernel with more specialized kernels and provide a natural way for the incorporation of a commonly available type of prior-knowledge. In addition to a theoretical validation of our framework, we propose an detailed empirical evaluation on real-life biological data which illustrates its ease-of-use and effectiveness. Not only pRBF kernels were able to improve the learning results in general but they also proved to perform particularly well when the training data set was very small or strongly biased, significantly broadening the field of application of SVMs.	kernel (operating system);linux;performance;radial (radio);radial basis function kernel;real life;requirement;support vector machine;test set	Antoine Veillard;Daniel Racoceanu;Stéphane Bressan	2012	2012 11th International Conference on Machine Learning and Applications	10.1109/ICMLA.2012.33	support vector machine;least squares support vector machine;kernel method;radial basis function kernel;computer science;artificial intelligence;machine learning;pattern recognition;data mining;mathematics;polynomial kernel	ML	18.009646170031292	-44.42388620448944	57711
12043933b0e093db37d36c059d352cfd721d2261	optimization of rich model based on fisher criterion for image steganalysis		The rich model features used for steganalysis of modern steganography usually have redundant features, which would influence the detection accuracy and performance. To solve this problem, this paper proposes a feature optimization method for rich model features based on the improved Fisher criterion. Based on the principle that “The within-class variance should be smaller and between-class variance should be larger”, the improved Fisher criterion is used in this paper to evaluate the separability of feature component, sub-model features and feature vector, respectively. Then, two strategies are presented to optimize the rich model features. In the experimental analysis, the proposed method is applied to optimize the typical rich model SRM that can be used to detect typical modern steganography HUGO, and the result is compared with existing feature selection methods. Experimental results show that, in addition to significantly reducing the dimensionality and shortening the detection time, the proposed rich model feature optimization method has a better performance on maintaining the detection accuracy of the original rich model features.	algorithm;bayesian information criterion;feature selection;feature vector;fisher–yates shuffle;linear separability;mathematical optimization;steganalysis;steganography;system reference manual	Yiqin Zhang;Fenlin Liu;Hongyan Jia;Jicang Lu;Chunfang Yang	2018	2018 Tenth International Conference on Advanced Computational Intelligence (ICACI)	10.1109/ICACI.2018.8377604	steganography;digital image;feature vector;feature selection;feature extraction;steganalysis;distortion;curse of dimensionality;pattern recognition;artificial intelligence;mathematics	AI	12.742549060494103	-44.583030984778546	57810
977560251c2bd4c28a6c7c707c29f4091c5e6247	lossy image compression with compressive autoencoders		We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.	algorithm;algorithmic efficiency;analysis of algorithms;approximation;autoencoder;codec;data compression;image compression;image resolution;jpeg 2000;lossy compression;new media;pixel;requirement	Lucas Theis;Wenzhe Shi;Andrew Cunningham;Ferenc Huszár	2017	CoRR		simulation;computer science;theoretical computer science;multimedia	ML	24.532708577911993	-51.52356076121812	58226
44f428b6d8a8c407b06bdd93a33831da02830b76	learning multiple metrics for ranking	direct optimization;learning to rank multiple measures direct optimization;multiple measures;learning to rank;xiubo geng xue qi cheng 研究性学习 直接优化 平均精度 假设空间 两个指标 信息检索 损失函数 性能比 learning multiple metrics for ranking	Directly optimizing an information retrieval (IR) metric has become a hot topic in the field of learning to rank. Conventional wisdom believes that it is better to train for the loss function on which will be used for evaluation. But we often observe different results in reality. For example, directly optimizing averaged precision achieves higher performance than directly optimizing precision@3 when the ranking results are evaluated in terms of precision@3. This motivates us to combine multiple metrics in the process of optimizing IR metrics. For simplicity we study learning with two metrics. Since we usually conduct the learning process in a restricted hypothesis space, e.g., linear hypothesis space, it is usually difficult to maximize both metrics at the same time. To tackle this problem, we propose a relaxed approach in this paper. Specifically, we incorporate one metric within the constraint while maximizing the other one. By restricting the feasible hypothesis space, we can get a more robust ranking model. Empirical results on the benchmark data set LETOR show that the relaxed approach is superior to the direct linear combination approach, and also outperforms other baselines.	algorithm;baseline (configuration management);benchmark (computing);experiment;information retrieval;lagrangian relaxation;learning to rank;loss function;machine learning;mathematical optimization	Xiubo Geng;Xueqi Cheng	2011	Frontiers of Computer Science in China	10.1007/s11704-011-0152-5	mathematical optimization;computer science;machine learning;data mining;learning to rank;statistics	Web+IR	21.457121314898462	-40.53987515497095	58539
6da9ab0e8764a8048c0bbf882a605bc705ffc2f0	constrained co-clustering with non-negative matrix factorisation	non negative;semi supervised;co clustering;constraint;cannot link;clustering;factorisation;matrix;must link	Co-clustering refers to the problem of deriving sub-matrices of the data matrix by simultaneously clustering the rows (data instances) and columns (features) of the matrix. While very effective in discovering useful knowledge, many of the co-clustering algorithms adopt a completely unsupervised approach. Integration of domain knowledge can guide the co-clustering process and greatly enhance the overall performance. We propose a semi-supervised Non-negative Matrix-factorisation (SS-NMF) based framework to integrate domain knowledge in the form of must-link and cannot-link constraints. Specifically, we augment the data matrix by integrating the constraints using metric learning and then perform NMF to obtain co-clustering. Under the proposed framework, we present two approaches to integrate domain knowledge, viz. a distance metric learning approach and an information theoretic metric learning approach. Through experiments performed on real-world web service data and publicly available text datasets, we demonstrate the performance of the proposed SS-NMF based approach for data co-clustering.	biclustering;cluster analysis;monoid factorisation	Amit Salunke;Xumin Liu;Manjeet Rege	2012	IJBIDM	10.1504/IJBIDM.2012.048728	constrained clustering;computer science;machine learning;pattern recognition;data mining;cluster analysis;constraint;factorization;biclustering;matrix	ML	23.46816198891293	-43.560803127665345	58608
654ecf37a64fb0e342c742a31884f88fa26c7369	a distance-based weighted undersampling scheme for support vector machines and its application to imbalanced classification		A support vector machine (SVM) plays a prominent role in classic machine learning, especially classification and regression. Through its structural risk minimization, it has enjoyed a good reputation in effectively reducing overfitting, avoiding dimensional disaster, and not falling into local minima. Nevertheless, existing SVMs do not perform well when facing class imbalance and large-scale samples. Undersampling is a plausible alternative to solve imbalanced problems in some way, but suffers from soaring computational complexity and reduced accuracy because of its enormous iterations and random sampling process. To improve their classification performance in dealing with data imbalance problems, this work proposes a weighted undersampling (WU) scheme for SVM based on space geometry distance, and thus produces an improved algorithm named WU-SVM. In WU-SVM, majority samples are grouped into some subregions (SRs) and assigned different weights according to their Euclidean distance to the hyper plane. The samples in an SR with higher weight have more chance to be sampled and put to use in each learning iteration, so as to retain the data distribution information of original data sets as much as possible. Comprehensive experiments are performed to test WU-SVM via 21 binary-class and six multiclass publically available data sets. The results show that it well outperforms the state-of-the-art methods in terms of three popular metrics for imbalanced classification, i.e., area under the curve, F-Measure, and G-Mean.	accidental falls;algorithm;area under curve;artificial neural network;computational complexity theory;diffusion weighted imaging;enterprise storage os (esos);equilibration disorder;euclidean distance;experiment;iteration;machine learning;maxima and minima;mental suffering;name;neural network simulation;overfitting;personnameuse - assigned;petrosal sinus sampling;probability;resampling (statistics);sampling (signal processing);sampling - surgical action;sequential minimal optimization;structural risk minimization;support vector machine;undersampling;unsupervised learning;weight;windows update	Qi Kang;Lei Shi;Mengchu Zhou;Xuesong Wang;Qidi Wu;Zhi Wei	2018	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2017.2755595	support vector machine;kernel (linear algebra);artificial intelligence;overfitting;computational complexity theory;euclidean distance;structural risk minimization;data set;pattern recognition;computer science;undersampling	ML	13.827847373636615	-40.418229305494016	58632
2705ad8f5eb88b1af01d6f529e9916652898fbf2	feature selection based on class-dependent densities for high-dimensional binary data	two stage feature selection;approximate algorithm;high dimensionality;frequency selective surface;approximation algorithms;prior knowledge;classification;mutual information markov processes redundancy algorithm design and analysis frequency selective surfaces accuracy approximation algorithms;accuracy;redundancy;theoretical analysis;high dimensional data;feature subset selection;markov process;pattern classification;knowledge management system;mutual information;feature selection;kernel ridge regression;binary data;markov processes;feature ranking;classification accuracy;algorithm design;algorithm design and analysis;classification feature ranking binary data feature subset selection two stage feature selection;gina data set feature selection algorithm high dimensional binary data class dependent density data management system knowledge management system feature ranking approach feature subset selection approach class dependent density based feature elimination filtrapper approach feature classification naive bayes classifier kernel ridge regression classifier nova data set hiva data set;frequency selective surfaces	Data and knowledge management systems employ feature selection algorithms for removing irrelevant, redundant, and noisy information from the data. There are two well-known approaches to feature selection, feature ranking (FR) and feature subset selection (FSS). In this paper, we propose a new FR algorithm, termed as class-dependent density-based feature elimination (CDFE), for binary data sets. Our theoretical analysis shows that CDFE computes the weights, used for feature ranking, more efficiently as compared to the mutual information measure. Effectively, rankings obtained from both the two criteria approximate each other. CDFE uses a filtrapper approach to select a final subset. For data sets having hundreds of thousands of features, feature selection with FR algorithms is simple and computationally efficient but redundant information may not be removed. On the other hand, FSS algorithms analyze the data for redundancies but may become computationally impractical on high-dimensional data sets. We address these problems by combining FR and FSS methods in the form of a two-stage feature selection algorithm. When introduced as a preprocessing step to the FSS algorithms, CDFE not only presents them with a feature subset, good in terms of classification, but also relieves them from heavy computations. Two FSS algorithms are employed in the second stage to test the two-stage feature selection idea. We carry out experiments with two different classifiers (naive Bayes' and kernel ridge regression) on three different real-life data sets (NOVA, HIVA, and GINA) of the”Agnostic Learning versus Prior Knowledge” challenge. As a stand-alone method, CDFE shows up to about 92 percent reduction in the feature set size. When combined with the FSS algorithms in two-stages, CDFE significantly improves their classification accuracy and exhibits up to 97 percent reduction in the feature set size. We also compared CDFE against the winning entries of the challenge and found that it outperforms the best results on NOVA and HIVA while obtaining a third position in case of GINA.	akaike information criterion;algorithmic efficiency;approximation algorithm;binary data;business motivation model;computation;computational complexity theory;diff utility;experiment;feature selection;flying-spot scanner;global information network architecture;knowledge management;linear separability;microsoft binary format;mutual information;naive bayes classifier;preprocessor;real life;relevance;selection algorithm;statistical classification	Kashif Javed;Haroon Atique Babri;Mehreen Saeed	2012	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2010.263	algorithm design;computer science;machine learning;pattern recognition;data mining;markov process;feature selection;statistics	ML	20.43921350144899	-39.878726369638684	58947
803219b5b79a05391f063f1e0323012a8a5a9ad0	autoencoder based residual deep networks for robust regression prediction and spatiotemporal estimation		To have a superior generalization, a deep learning neural network often involves a large size of training sample. With increase of hidden layers in order to increase learning ability, neural network has potential degradation in accuracy. Both could seriously limit applicability of deep learning in some domains particularly involving predictions of continuous variables with a small size of samples. Inspired by residual convolutional neural network in computer vision and recent findings of crucial shortcuts in the brains in neuroscience, we propose an autoencoder-based residual deep network for robust prediction. In a nested way, we leverage shortcut connections to implement residual mapping with a balanced structure for efficient propagation of error signals. The novel method is demonstrated by multiple datasets, imputation of high spatiotemporal resolution non-randomness missing values of aerosol optical depth, and spatiotemporal estimation of fine particulate matter<2.5 \mu m, achieving the cutting edge of accuracy and efficiency. Our approach is also a general-purpose regression learner to be applicable in diverse domains.		Lianfa Li;Ying Fang;Jun Wu;Jinfeng Wang	2018	CoRR			ML	22.301397368218982	-50.33801794598779	58968
986139329ca5863b4f65d9cbca0ce49de910d487	improved robustness to adversarial examples using lipschitz regularization of the loss		Adversarial training is an effective method for improving robustness to adversarial attacks. We show that adversarial training using the Fast Signed Gradient Method can be interpreted as a form of regularization. We implemented a more effective form of adversarial training, which in turn can be interpreted as regularization of the loss in the 2-norm, ‖∇x`(x)‖2. We obtained further improvements to adversarial robustness, as well as provable robustness guarantees, by augmenting adversarial training with Lipschitz regularization.		Chris Finlay;Adam M. Oberman;Bilal Abbasi	2018	CoRR		adversarial system;mathematical optimization;machine learning;gradient method;robustness (computer science);lipschitz continuity;effective method;nabla symbol;artificial intelligence;regularization (mathematics);mathematics	NLP	19.030815713345053	-50.66153295264641	59050
d74189f4c137df312863b05dff63b72f746be02a	dpatch: attacking object detectors with adversarial patches		Object detectors have witnessed great progress in recent years and have been widely deployed in various important real-world scenarios, such as autonomous driving and face recognition. Therefore, it is increasingly vital to investigate the vulnerability of modern object detectors to different types of attacks. In this work, we demonstrate that actually many mainstream detectors (e.g. Faster R-CNN) can be hacked by a tiny adversarial patch. It is a non-trivial task since the original adversarial patch method can only be applied to image-level classifiers and is not capable to deal with the region proposals involved in modern detectors. Instead, here we iteratively evolve a tiny patch inside the input image so that it invalidates both proposal generation and the subsequent region classification of Faster R-CNN, resulting in a successful attack. Specifically, the proposed adversarial patch (namely, DPatch) can be trained toward any targeted class so that all the objects in any region of the scene will be classified as that targeted class. One interesting observation is that the efficiency of DPatch is not influenced by its location: no matter where it resides, the patch can always invalidate RCNN after the same amount of iterations. Furthermore, we find that different target classes have different degrees of vulnerability; and an DPatch with a larger size can perform the attack more effectively. Extensive experiments show that our DPatch can reduce the mAP of a state-of-the-art detector on PASCAL VOC 2012 from 71% to 25% and below.	autonomous car;autonomous robot;experiment;facial recognition system;iteration;map;object detection;patch (computing);sensor	Xin Liu;Huanrui Yang;Linghao Song;Hai Li;Yiran Chen	2018	CoRR		machine learning;adversarial system;pattern recognition;computer science;artificial intelligence;detector;facial recognition system;vulnerability	Vision	19.806679533137018	-51.34332737332963	59104
a0cc9f66d2b1d1f87b0967602703f386aa858eb4	improving semi-supervised target alignment via label-aware base kernels		Semi-supervised kernel design is an essential step for obtaining good predictive performance in semi-supervised learning tasks. In the current literatures, a large family of algorithms builds the new kernel by using the weighted average of predefined base kernels. While optimal weighting schemes have been studied extensively, the choice of base kernels received much less attention. Many methods simply adopt the empirical kernel matrices or its eigenvectors. Such base kernels are computed irrespective of class labels and may not always reflect useful structures in the data. As a result, in case of poor base kernels, the generalization performance can be degraded however hard their weights are tuned. In this paper, we propose to construct high-quality base kernels with the help of label information to globally improve the final target alignment. In particular, we devise label-aware kernel eigenvectors under the framework of semi-supervised eigenfunction extrapolation, which span base kernels that are more useful for learning. Such base kernels are individually better aligned to the learning target, so their mixture will more likely generate a good classifier. Our approach is computationally efficient, and demonstrates encouraging performance in semisupervised classification and regression.	algorithm;algorithmic efficiency;database normalization;extrapolation;kernel (operating system);semi-supervised learning;semiconductor industry;statistical classification;supervised learning	Qiaojun Wang;Kai Zhang;Guofei Jiang;Ivan Marsic	2014			mathematical optimization;machine learning;pattern recognition;tree kernel	AI	18.459389913038816	-40.40356946525064	59122
1a079b0576fda4ab49b4400768d7a8f8bf5ee749	boosting for regression transfer	controlled experiment;transfer learning	The goal of transfer learning is to improve the learning of a new target concept given knowledge of related source concept(s). We introduce the first boosting-based algorithms for transfer learning that apply to regression tasks. First, we describe two existing classification transfer algorithms, ExpBoost and TrAdaBoost, and show how they can be modified for regression. We then introduce extensions of these algorithms that improve performance significantly on controlled experiments in a wide range of test domains.	algorithm;boosting (machine learning);experiment;matrix regularization;source data;stacking;test case	David Pardoe;Peter Stone	2010			transfer of learning;computer science;artificial intelligence;machine learning;pattern recognition;inductive transfer	ML	22.127935329851244	-45.05623709955113	59352
b1263ca3e6f8f705fd30c4c3495d371c2cce0a62	an index-inspired algorithm for anytime classification on evolving data streams		Due to the ever growing presence of data streams there has been a considerable amount of research on stream data mining over the past years. Anytime algorithms are particularly well suited for stream mining, since they flexibly use all available time on streams of varying data rates, and are also shown to outperform traditional budget approaches on constant streams. In this article we present an index-inspired algorithm for Bayesian anytime classification on evolving data streams and show its performance on benchmark data sets.	anytime algorithm;benchmark (computing);data mining	Philipp Kranen;Ira Assent;Thomas Seidl	2012	Datenbank-Spektrum	10.1007/s13222-012-0083-9	computer science;machine learning;data mining;database;data stream mining	ML	13.71628529976411	-38.084690384827866	59370
c051194df9f816136fb63e6829287deef094c3a2	heterogeneous defect prediction		Many recent studies have documented the success of cross-project defect prediction (CPDP) to predict defects for new projects lacking in defect data by using prediction models built by other projects. However, most studies share the same limitations: it requires homogeneous data; i.e., different projects must describe themselves using the same metrics. This paper presents methods for heterogeneous defect prediction (HDP) that matches up different metrics in different projects. Metric matching for HDP requires a “large enough” sample of distributions in the source and target projects—which raises the question on how large is “large enough” for effective heterogeneous defect prediction. This paper shows that empirically and theoretically, “large enough” may be very small indeed. For example, using a mathematical model of defect prediction, we identify categories of data sets were as few as 50 instances are enough to build a defect prediction model. Our conclusion for this work is that, even when projects use different metric sets, it is possible to quickly transfer lessons learned about defect prediction.	software bug	Jaechang Nam;Wei Fu;Sunghun Kim;Tim Menzies;Lin Tan	2018	IEEE Trans. Software Eng.	10.1109/TSE.2017.2720603	predictive modelling;computer science;software;data modeling;software metric;data mining;homogeneous;machine learning;data set;artificial intelligence	SE	15.241356902805963	-43.77837413241001	59581
8a2654d1352573c197048dd982f616ace21d5790	feature grouping based on ga and l-gem for human activity recognition		Human Activity Recognition is useful in many applications such as video surveillance and health care for elderly. Multiple Classifier System (MCS) has been proved to be better than single classifiers theoretically and empirically. Diversity is the key to improving the performance of an MCS. Feature grouping is one of the common methods. In this paper, we propose a new feature grouping method that uses Localized Generalization Error Model(L-GEM) as an evaluation criterion for optimizing MCS. Genetic Algorithm (GA) is used to evolve the weights of the members of MCS so that a set of diverse classifiers is combined to build an MCS which minimizes the localized generalization error. Experiments is performed based on a human activity recognition dataset and the results are compared with random subspace method to show that the MCS performs better.	activity recognition;closed-circuit television;generalization error;genetic algorithm;learning classifier system;multi categories security;random subspace method;software release life cycle	Y. B. Xue;Jing Liu;Jiamin Chen;Yun-Tao Zhang;Renhua Cao	2018	2018 International Conference on Machine Learning and Cybernetics (ICMLC)	10.1109/ICMLC.2018.8527017	random subspace method;genetic algorithm;machine learning;artificial intelligence;generalization error;statistical classification;pattern recognition;activity recognition;computer science	Robotics	11.226915798496554	-41.449371528063566	59698
35634e8971494be944db8ed6500280c02719b729	a recursive partitioning decision rule for nonparametric classification	adaptively generated features;general and miscellaneous mathematics computing and information science;mathematics;recursive partitioning;nonparametric classification;classification;mathematical logic 990200 mathematics computers;bayes risk;statistics;algorithms;kolmogorov smirnoff distance;decision rule;adaptively generated features kolmogorov smirnoff distance nonparametric classification recursive partitioning	A new criterion for deriving a recursive partitioning decision rule for nonparametric classification is presented. The criterion is both conceptually and computationally simple, and can be shown to have strong statistical merit. The resulting decision rule is asymptotically Bayes' risk efficient. The notion of adaptively generated features is introduced and methods are presented for dealing with missing features in both training and test vectors.	decision tree learning;naive bayes classifier;recursion (computer science)	Jerome H. Friedman	1977	IEEE Transactions on Computers	10.1109/TC.1977.1674849	biological classification;computer science;machine learning;pattern recognition;decision rule;mathematics;statistics;recursive partitioning	Vision	16.208226097715034	-38.267774340552144	59791
9f7c14b89f47b7dc593c5e1006be819aca3399e2	rank hash similarity for fast similarity search	journal;hash function;similarity ranking;similarity search	The paper is concerned with similarity search at large scale, which efficiently and effectively finds similar data points for a query data point. An efficient way to accelerate similarity search is to learn hash functions. The existing approaches for learning hash functions aim to obtain low values of Hamming distances for the similar pairs. However, these methods ignore the ranking order of these Hamming distances. This leads to the poor accuracy about finding similar items for a query data point. In this paper, an algorithm is proposed, referred to top k RHS (Rank Hash Similarity), in which a ranking loss function is designed for learning a hash function. The hash function is hypothesized to be made up of l binary classifiers. The issue of learning a hash function can be formulated as a task of learning l binary classifiers. The algorithm runs l rounds and learns a binary classifier at each round. Compared with the existing approaches, the proposed method has the same order of computational complexity. Nevertheless, experiment results on three text datasets show that the proposed method obtains higher accuracy than the baselines.	similarity search	Min Lu;Yalou Huang;Maoqiang Xie;Jie Liu	2013	Inf. Process. Manage.	10.1016/j.ipm.2012.07.003	feature hashing;double hashing;hash function;perfect hash function;dynamic perfect hashing;primary clustering;computer science;machine learning;pattern recognition;data mining;hash buster;mathematics;k-independent hashing;rolling hash;swifft;hash tree;hash filter	DB	19.03890666805371	-45.93805388700645	59857
03f8f6c69b8583450fce6807eef9a071a50b73a7	stacked k-means hashing quantization for nearest neighbor search		Nowadays, with such a huge amount of information available online, one key challenge is how to retrieve target data efficiently. A recent state-of-art solution, k-means hashing (KMH), codes data via a string of binary code obtained by iterative k-means clustering and binary code optimizing. To deal with high dimensional data, KMH divides the space into low-dimensional subspaces, places a hypercube in each subspace and finds its proper location by the mentioned optimizing process. However, the complexity of the optimization increases rapidly when the dimension of the hypercube increases. To address this issue, we propose an improved hashing method stacked k-means hashing (SKMH). The main idea is to increase the approximation by a coarse-to-fine multi-layer lower-dimensional cubes. With these kinds of lower-dimensional cubes, SKMH can achieve a similar approximation ability via a less optimizing time, compared with KMH method using higher-dimensional cubes. Extensive experiments have been conducted on two public databases, demonstrating the performance of our method by some common metrics in fast nearest neighbor search.	approximation;binary code;cluster analysis;database;experiment;hash function;iterative method;k-means clustering;layer (electronics);mathematical optimization;nearest neighbor search;olap cube	Yalin Chen;Zhiyang Li;Jia Shi;Zhaobin Liu;Wenyu Qu	2018	2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)	10.1109/BigMM.2018.8499296	binary code;algorithm;clustering high-dimensional data;hash function;hypercube;subspace topology;cluster analysis;k-means clustering;nearest neighbor search;computer science	DB	19.444784398561826	-46.44864938135915	59882
5b971909784c69ed143cb535ac8d3a05b08ba068	structure preserving dimension reduction for clustered text data based on the generalized singular value decomposition	linear algebra;15a18;generalizacion;generalized eigenvalue problem;decomposition valeur singuliere;eigenvalue problem;information retrieval system;dimension reduction;matrice diffusion;singular value decomposition;vector space;grupo de excelencia;reduction dimensionnelle;probleme valeur propre;generalized singular value decomposition;classification texte;matriz difusion;text classification;discriminant analysis;analyse discriminante;analisis discriminante;generalisation;62h30;ciencias basicas y experimentales;algebre lineaire;15a09;matematicas;scatter matrix;pattern recognition;structure preservation;scattering matrix;algebra lineal;68t10;tecnologias generalidades;decomposicion valor singular;reconnaissance forme;tecnologias;reconocimiento patron;dimensional reduction;65f15;generalization;problema valor propio;optimisation trace;trace optimization	In today’s vector space information retrieval systems, dimension reduction is imperative for efficiently manipulating the massive quantity of data. To be useful, this lower-dimensional representation must be a good approximation of the full document set. To that end, we adapt and extend the discriminant analysis projection used in pattern recognition. This projection preserves cluster structure by maximizing the scatter between clusters while minimizing the scatter within clusters. A common limitation of trace optimization in discriminant analysis is that one of the scatter matrices must be nonsingular, which restricts its application to document sets in which the number of terms does not exceed the number of documents. We show that by using the generalized singular value decomposition (GSVD), we can achieve the same goal regardless of the relative dimensions of the term-document matrix. In addition, applying the GSVD allows us to avoid the explicit formation of the scatter matrices in favor of working directly with the data matrix, thus improving the numerical properties of the approach. Finally, we present experimental results that confirm the effectiveness of our approach.	algorithm;approximation;computational complexity theory;dimensionality reduction;document retrieval;document-term matrix;imperative programming;information retrieval;linear discriminant analysis;mathematical optimization;numerical analysis;pattern recognition;preprocessor;singular value decomposition	Peg Howland;Moongu Jeon;Haesun Park	2003	SIAM J. Matrix Analysis Applications	10.1137/S0895479801393666	generalization;combinatorics;linear algebra;calculus;mathematics;algebra	ML	24.45777202844092	-38.72922967824066	59914
cc91d19192e5df6be433cdcfccd31d078fcd854a	efficient approximate similarity search using random projection learning	svm classifier;similarity query;high quality binary code;binary code;efficient similarity search;existing work;random projection;efficient approximate similarity search;random projection technique;high dimensional data;approximate similarity search problem;random projection learning	svm classifier;similarity query;high quality binary code;binary code;efficient similarity search;existing work;random projection;efficient approximate similarity search;random projection technique;high dimensional data;approximate similarity search problem;random projection learning	random projection;similarity search	Peisen Yuan;Chaofeng Sha;Xiaoling Wang;Bin Yang;Aoying Zhou	2011		10.1007/978-3-642-23535-1_44	computer science;machine learning;pattern recognition;data mining	Theory	18.727386140885308	-46.33545118163362	59942
670044b9eab3f90a001c8c56665ac8f5c4986063	unsupervised domain adaptation: a multi-task learning-based method		This paper presents a novel multi-task learningbased method for unsupervised domain adaptation. Specifically, the source and target domain classifiers are jointly learned by considering the geometry of target domain and the divergence between the source and target domains based on the concept of multi-task learning. Two novel algorithms are proposed upon the method using Regularized Least Squares and Support Vector Machines respectively. Experiments on both synthetic and real world cross domain recognition tasks have shown that the proposed methods outperform several state-of-the-art domain adaptation methods.	algorithm;computer multitasking;domain adaptation;experiment;multi-task learning;regularized least squares;support vector machine;synthetic intelligence;unsupervised learning	Jing Zhang;Wanqing Li;Philip Ogunbona	2018	CoRR		machine learning;pattern recognition;support vector machine;domain adaptation;computer science;artificial intelligence;divergence;multi-task learning;regularized least squares	Vision	23.678928608982332	-44.84412931677604	60031
54e06f95cccc4784e949550ed7d2eccb1f714d6d	sparse feature learning using ensemble model for highly-correlated high-dimensional data		High-dimensional highly correlated data exist in several domains such as genomics. Many feature selection techniques consider correlated features as redundant and therefore need to be removed. Several studies investigate the interpretation of the correlated features in domains such as genomics, but investigating the classification capabilities of the correlated feature groups is a point of interest in several domains. In this paper, a novel method is proposed by integrating the ensemble feature ranking and co-expression networks to identify the optimal features for classification. The main advantage of the proposed method lies in the fact, that it does not consider the correlated features as redundant. But, it shows the importance of the selected correlated features to improve the performance of classification. A series of experiments on five high dimensional highly correlated datasets with different levels of imbalance ratios show that the proposed method outperformed the state-of-the-art methods.		Ali Braytee;Ali Anaissi;Paul J. Kennedy	2018		10.1007/978-3-030-04182-3_37	point of interest;clustering high-dimensional data;machine learning;feature selection;pattern recognition;computer science;artificial intelligence;ranking;feature learning;ensemble forecasting	AI	11.377299158050299	-45.7398068624727	60056
b32485915bdf26e0e0d54530c7bf96d1c35c6822	stacked similarity-aware autoencoders		As one of the most popular unsupervised learning approaches, the autoencoder aims at transforming the inputs to the outputs with the least discrepancy. The conventional autoencoder and most of its variants only consider the one-to-one reconstruction, which ignores the intrinsic structure of the data and may lead to overfitting. In order to preserve the latent geometric information in the data, we propose the stacked similarity-aware autoencoders. To train each single autoencoder, we first obtain the pseudo class label of each sample by clustering the input features. Then the hidden codes of those samples sharing the same category label will be required to satisfy an additional similarity constraint. Specifically, the similarity constraint is implemented based on an extension of the recently proposed center loss. With this joint supervision of the autoencoder reconstruction error and the center loss, the learned feature representations not only can reconstruct the original data, but also preserve the geometric structure of the data. Furthermore, a stacked framework is introduced to boost the representation capacity. The experimental results on several benchmark datasets show the remarkable performance improvement of the proposed algorithm compared with other autoencoder based approaches.	algorithm;algorithmic efficiency;autoencoder;benchmark (computing);cluster analysis;code;computation;discrepancy function;feature learning;loss function;matrix regularization;one-to-one (data model);overfitting;unsupervised learning	Wenqing Chu;Deng Cai	2017		10.24963/ijcai.2017/216	machine learning;artificial intelligence;computer science	AI	23.970096735048973	-44.916806634988184	60074
18d5b0d421332c9321920b07e0e8ac4a240e5f1f	collaborative representation classification ensemble for face recognition.		Collaborative Representation Classification (CRC) for face recognition attracts a lot attention recently due to its good recognition performance and fast speed. Compared to Sparse Representation Classification (SRC), CRC achieve s a comparable recognition performance with 10-1000 times fas ter speed. In this paper, we propose to ensemble several CRC mode ls to promote the recognition rate, where each CRC model uses different and divergent randomly generated biologically-inspired features as the face representation. The proposed ensemble algorithm calculates an ensemble weight for each CRC model that guided by the underlying classification rule of CRC. The obtained weights reflect the confidences of those CRC models where the more confident CRC models have larger weights. The proposed weighted ensemble method proves to be very effective and improves the performance of each CRC model significantly. Extensive experiments are conducted to showthe superior performance of the proposed method.	algorithm;cyclic redundancy check;experiment;facial recognition system;feedback arc set;procedural generation;sparse	Xiaochao Qu;Suah Kim;Run Cui;Hyoung Joong Kim	2015	CoRR		artificial intelligence;machine learning;facial recognition system;pattern recognition;computer science;classification rule;sparse approximation	AI	16.12006875623817	-45.75405240921614	60535
2861592aa43b5a0f6ce0ee6c5b97521d6d93f920	multi-view embedding learning for incompletely labeled data	compact embedding;input multi-view feature space;semantic concept space;multi-view embedding;semantic gap;original space;multiple input space;heterogeneous feature;feature correlation;incompletely-labeled multi-view data;compact embedding space	In many applications, the data may be high dimensional, represented by multiple features, and associated with more than one labels. Embedding learning is an effective strategy for dimensionality reduction and for nearest neighbor search in massive datasets. We propose a novel method to seek compact embedding that allows efficient retrieval with incompletely-labeled multi-view data. Based on multi-graph Laplacian, we achieve the optimal combination of heterogeneous features to effectively describe data, which exploits the feature correlations between different views. We learn the embedding that preserves the neighborhood context in the original spaces, and obtain the complete labels simultaneously. Inter-label correlations are sufficiently leveraged in the proposed framework. Our goal is to find the maps from multiple input spaces to the compact embedding space and to the semantic concept space at the same time. There is semantic gap between the input multi-view feature spaces and the semantic concept space; and the compact embedding space can be looked on as the bridge between the above spaces. Experimental evaluation on three real-world datasets demonstrates the effectiveness of the proposed method.	dimensionality reduction;directed graph;laplacian matrix;map;nearest neighbor search	Wei Zhang;Ke Zhang;Pan Gu;Xiangyang Xue	2013			discrete mathematics;topology;machine learning;mathematics	AI	23.77601583595448	-44.60195458786818	60598
e8261dd641db0bc063609277973793c6f7dce0dc	ensembling sparse representation classifiers through layers of support vector machines	symbiosis;support vector machines;training;electric breakdown;mathematical model;optimization;data models	The paper aims at presenting an ensembling framework for Sparse Representation Classifier (SRC). The paper proposes a method of combining a series of Sparse Representation Classifier models using a network of Linear Support Vector Machines (LSVMs). One of the core ideas of the paper is that there exists a symbiosis between Sparse Representation Classifier and Linear Support Vector Machine. This symbiosis, we believe makes this ensemble of classifiers a very effective combination. This central argument of the paper is buttressed by empirical evidence, which displays the efficacy of the network of LSVMs in helping SRCs overcome their breakdown points. The paper discusses two important aspects of the ensembling framework: namely, diversity and percolation rate. Diversity is a measure of the ability of the series of SRCs to make uncorrelated errors, and percolation rate is the measure of the ability of the ensembling/combining scheme to pass the correct label as the final predicted label. The classification accuracy of the proposed ensembling framework compares favourably with traditional ensembling frameworks such as Random Forests and Artificial Neural Networks on standard datasets.	artificial neural network;baseline (configuration management);cluster analysis;framing (world wide web);mathematical optimization;neural networks;optimization problem;percolation;random forest;sample rate conversion;sparse approximation;support vector machine	Sudarshan Babu;Vikaasa Ramdas	2016	2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)	10.1109/ICMLA.2016.0157	data modeling;support vector machine;computer science;artificial intelligence;machine learning;pattern recognition;mathematical model;data mining;symbiosis	AI	21.136965569914928	-43.52898879578121	60611
0d8b643925fae2a33c9b2a8caf90b963172c4d02	catgan: coupled adversarial transfer for domain generation		This paper introduces a Coupled adversarial transfer GAN (CatGAN), an efficient solution to domain alignment. The basic principles of CatGAN focus on the domain generation strategy for adaptation which is motivated by the generative adversarial net (GAN) and the adversarial discriminative domain adaptation (ADDA). CatGAN is structured by shallow multilayer perceptrons (MLPs) for adversarial domain adaptation. The CatGAN comprises of two slim and symmetric sub-networks, which then formulates a coupled adversarial learning framework. With such symmetry, the input images from source/target domain can be fed into the MLP network for target/source domain generation, supervised by the coupled discriminators for confrontation. Notablely, each generator contains GAN loss and domain loss to guarantee the simple network work well. The content fidelity term aims at preserving the domain specific knowledge during generation. Another finding is that the classwise CatGAN is an effective alternative to conditional GAN without label constraint in generative learning. We show experimentally that the proposed model achieves competitive performance with state-of-the art approaches.	benchmark (computing);discrepancy function;domain adaptation;experiment;memory-level parallelism;multilayer perceptron;transfer-based machine translation	Shanshan Wang	2017	CoRR		discriminative model;machine learning;pattern recognition;generative grammar;perceptron;artificial intelligence;computer science;adversarial system;domain adaptation;generative model	AI	23.796991534613912	-49.10222434303395	60717
051a7e634773d7e3a4203d7786129204c203b373	a systematic comparison of different object-based classification techniques using high spatial resolution imagery in agricultural environments	mixed object;segmentation scale;classification;random forest;geobia;feature selection;training set size;obia;high spatial resolution	Geographic Object-Based Image Analysis (GEOBIA) is becoming more prevalent in remote sensing classification, especially for high-resolution imagery. Many supervised classification approaches are applied to objects rather than pixels, and several studies have been conducted to evaluate the performance of such supervised classification techniques in GEOBIA. However, these studies did not systematically investigate all relevant factors affecting the classification (segmentation scale, training set size, feature selection and mixed objects). In this study, statistical methods and visual inspection were used to compare these factors systematically in two agricultural case studies in China. The results indicate that Random Forest (RF) and Support Vector Machines (SVM) are highly suitable for GEOBIA classifications in agricultural areas and confirm the expected general tendency, namely that the overall accuracies decline with increasing segmentation scale. All other investigated methods except for RF and SVM are more prone to obtain a lower accuracy due to the broken objects at fine scales. In contrast to some previous studies, the RF classifiers yielded the best results and the k-nearest neighbor classifier were the worst results, in most cases. Likewise, the RF and Decision Tree classifiers are the most robust with or without feature selection. The results of training sample analyses indicated that the RF and adaboost. M1 possess a superior generalization capability, except when dealing with small training sample sizes. Furthermore, the classification accuracies were directly related to the homogeneity/heterogeneity of the segmented objects for all classifiers. Finally, it was suggested that RF should be considered in most cases for agricultural mapping.	object-based language	Manchun Li;Lei Ma;Thomas Blaschke;Liang Cheng;Dirk Tiede	2016	Int. J. Applied Earth Observation and Geoinformation	10.1016/j.jag.2016.01.011	random forest;geography;biological classification;computer science;machine learning;pattern recognition;data mining;feature selection;statistics	Visualization	14.023345489410106	-42.911543659639456	61014
404fbf337f6997f71c3ed5eb58a8d8f1dbd3edc2	ensemble of decision trees with global constraints for ordinal classification	decision tree;supervised learning;ensemble learning;global constraint;feature space;classification;machine learning decision trees global constraints ordinal classification conventional trees regression settings nominal classification decision making;ordinal data;pattern classification;decision trees optimization training machine learning labeling equations intelligent systems;regression analysis;credit rating;learning artificial intelligence;decision trees;ensemble learning decision trees supervised learning classification ordinal data;regression analysis decision trees learning artificial intelligence pattern classification	While ordinal classification problems are common in many situations, induction of ordinal decision trees has not evolved significantly. Conventional trees for regression settings or nominal classification are commonly induced for ordinal classification problems. On the other hand a decision tree consistent with the ordinal setting is often desirable to aid decision making in such situations as credit rating. In this work we extend a recently proposed strategy based on constraints defined globally over the feature space. We propose a bootstrap technique to improve the accuracy of the baseline solution. Experiments in synthetic and real data show the benefits of our proposal.	baseline (configuration management);booting;decision tree;decision tree learning;ensemble learning;experiment;feature vector;level of measurement;machine learning;ordinal data;synthetic intelligence	Ricardo Gamelas Sousa;Jaime S. Cardoso	2011	2011 11th International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2011.6121816	ordinal regression;computer science;machine learning;decision tree;pattern recognition;data mining;mathematics;ensemble learning;supervised learning	ML	19.270032569639667	-41.58418829237637	61091
97c1d0a5c91041c1784e5ad89b21be6c8ff7e8b3	which algorithm performs best: algorithm selection for community detection		A myriad of community detection methods have been designed to discover communities based on specific network features in different disciplines, such as sociology, physics, and computer science. Consequentially, we have to face the problem of Algorithm Selection for Community Detection (ASCD): Given a specific network, which algorithm should we select to reveal its latent community structures In this study, we propose a model called CYDES to address the ASCD problem. CYDES consists of two parts, namely feature matrix generation and algorithm classification. We combine three effective feature extraction methods with the idea of BOW model to construct a fixed-size feature matrix. After a nonlinear transformation to the feature matrix, a softmax regression model is utilized to generate a classification label representing the best community detection algorithm we select. Extensive experimental results demonstrate that CYDES has high algorithm selection quality for community detection in networks.	algorithm selection	Gaoyang Guo;Changping Wang;Xiang Ying	2018		10.1145/3184558.3186912	regression analysis;feature extraction;matrix (mathematics);nonlinear system;algorithm;softmax function;computer science;algorithm selection	NLP	21.625865137859428	-45.859065590018005	61194
63be9f1e85d3281475286f7b9cd04262d06bb2db	finding attractors in synchronous multiple-valued networks using sat-based bounded model checking	analytical models;graph theory;sra informations och kommunikationsteknik;organisms;decision diagrams;multivalued logic decision diagrams diseases genetics graph theory;elektroteknik och elektronik;naturvetenskap;electrical engineering electronic engineering information engineering;4 valued networks attractors synchronous multiple valued networks sat based bounded model checking discrete space discrete time model gene regulatory network state transition graph disease mutation decision diagram based approaches simulation based approaches;sra ict;signal analysis;decision diagram;logic;biological system modeling explosions diseases genetic mutations differential equations logic organisms large scale systems formal verification signal analysis;biological system modeling;discrete time;attractor;runtime;genetics;sat;bounded model checking;natural sciences;computational modeling;formal verification;model checking;network model;mathematical model;diseases;multiple valued network;explosions;differential equations;genetic mutations;multiple valued;multivalued logic;state transition graph;gene regulatory network;gene regulatory network bounded model checking sat multiple valued network attractor;algorithm design and analysis;regulators;large scale systems;biological process	Synchronous multiple-valued networks are a discrete-space discrete-time model of the gene regulatory network of living cells. In this model, cell types are represented by the cycles in the state transition graph of a network, called attractors. When the effect of a disease or a mutation on a cell is studied, attractors have to be re-computed each time a fault is injected in the model. This motivates research on algorithms for finding attractors. Existing decision diagram-based approaches have limited capacity due to the excessive memory requirements of decision diagrams. Simulation-based approaches can be applied to larger networks, however, they are incomplete. We present an algorithm for finding attractors which uses a SAT-based bounded model checking. Our model checking approach exploits the deterministic nature of the network model to reduce runtime. Although the idea of applying model checking to the analysis of gene regulatory networks is not new, to our best knowledge, we are the first to use it for computing all attractors in a model. The efficiency of the presented algorithm is evaluated by analyzing 7 networks models of real biological processes as well as 35.000 randomly generated 4-valued networks. The results show that our approach has a potential to handle an order of magnitude larger models than currently possible.	algorithm;boolean satisfiability problem;flow network;gene regulatory network;influence diagram;model checking;mutation (genetic algorithm);network model;procedural generation;requirement;scalability;simulation;solver;state diagram	Elena Dubrova;Maxim Teslenko	2010	2010 40th IEEE International Symposium on Multiple-Valued Logic	10.1109/ISMVL.2010.35	model checking;gene regulatory network;combinatorics;discrete mathematics;topology;formal verification;computer science;artificial intelligence;graph theory;theoretical computer science;network model;machine learning;mathematical model;mathematics;biological process;genetics;logic;attractor;algorithm	Embedded	12.61595728645889	-51.716229378625584	61256
26cf54106c32f3007ae58816ac1a693c3262e755	interactive reconstruction of monte carlo image sequences using a recurrent denoising autoencoder		We describe a machine learning technique for reconstructing image sequences rendered using Monte Carlo methods. Our primary focus is on reconstruction of global illumination with extremely low sampling budgets at interactive rates. Motivated by recent advances in image restoration with deep convolutional networks, we propose a variant of these networks better suited to the class of noise present in Monte Carlo rendering. We allow for much larger pixel neighborhoods to be taken into account, while also improving execution speed by an order of magnitude. Our primary contribution is the addition of recurrent connections to the network in order to drastically improve temporal stability for sequences of sparsely sampled input images. Our method also has the desirable property of automatically modeling relationships based on auxiliary per-pixel input channels, such as depth and normals. We show significantly higher quality results compared to existing methods that run at comparable speeds, and furthermore argue a clear path for making our method run at realtime rates in the near future.	acm transactions on graphics;autoencoder;circuit restoration;coherence (physics);convolutional neural network;gaussian blur;global illumination;image restoration;machine learning;monte carlo method;noise reduction;normal (geometry);pixel;recurrent neural network;rendering (computer graphics);sampling (signal processing)	Chakravarty R. Alla Chaitanya;Anton Kaplanyan;Christoph Schied;Marco Salvi;Aaron E. Lefohn;Derek Nowrouzezahrai;Timo Aila	2017	ACM Trans. Graph.	10.1145/3072959.3073601	image restoration;mathematical optimization;autoencoder;artificial intelligence;iterative reconstruction;pixel;rendering (computer graphics);computer vision;monte carlo method;sampling (statistics);global illumination;computer science	Graphics	23.924355215282578	-49.747552831305576	61379
85c0ece9e322e805760431417577afd2fb5b2a0b	detecting adversarial perturbations with saliency		In this paper we propose a novel method for detecting adversarial examples by training a binary classifier with both origin data and saliency data. In the case of image classification model, saliency simply explain how the model make decisions by identifying significant pixels for prediction. A model shows wrong classification output always learns wrong features and shows wrong saliency as well. Our approach shows good performance on detecting adversarial perturbations. We quantitatively evaluate generalization ability of the detector, showing that detectors trained with strong adversaries perform well on weak adversaries.		Chiliang Zhang;Zhimou Yang;Zuochang Ye	2018	2018 IEEE 3rd International Conference on Signal and Image Processing (ICSIP)		adversarial system;pixel;convolutional neural network;binary classification;salience (neuroscience);contextual image classification;detector;pattern recognition;computer science;artificial intelligence;perturbation (astronomy)	Vision	19.976509601381302	-51.541034364930944	61523
bc1bc891c4d80ca9ad9adcc3a44299b44ff45e2a	a novel one-parameter regularized linear discriminant analysis for solving small sample size problem in face recognition	reconnaissance visage;sample size;base donnee;regularisation;algorithm analysis;small sample size;facies;complexite calcul;tamano muestra;biometrie;biometrics;database;biometria;taille echantillon;base dato;linear discriminate analysis;regularization;resolucion problema;discriminant analysis;analyse discriminante;analisis discriminante;complejidad computacion;face recognition;computational complexity;pattern recognition;analyse algorithme;regularizacion;reconnaissance forme;reconocimiento patron;analisis algoritmo;problem solving;resolution probleme	In this paper, a new 1-parameter regularized discriminant analysis (1PRDA) algorithm is developed to deal with the small sample size (S3) problem The main limitation in regularization is that the computational complexity of determining the optimal parameters is very high In view of this limitation, we derive a single parameter (t) explicit expression formula for determining the 3 parameters A simple and efficient method is proposed to determine the value of t The proposed 1PRLDA method for face recognition has been evaluated with two public available databases, namely ORL and FERET databases The average recognition accuracy of 50 runs for ORL and FERET database are 96.65% and 94.00% respectively Comparing with existing LDA-based methods in solving the S3 problem, the proposed 1PRLDA method gives the best performance.	facial recognition system;linear discriminant analysis	Wensheng Chen;Pong C. Yuen;Jian Huang;Dao-Qing Dai	2004		10.1007/978-3-540-30548-4_37	facial recognition system;sample size determination;regularization;econometrics;facies;computer science;artificial intelligence;mathematics;linear discriminant analysis;computational complexity theory;biometrics;statistics	Vision	23.518920647336273	-39.117996634975555	61538
af4a312a609164ddd376dd4010bb21e21d6e29be	rough sets for handling imbalanced data: combining filtering and rule-based classifiers	rule induction;rule based;class imbalance;data mining;classification;rough sets;rough set;knowledge discovery	The paper addresses problems of improving performance of rule-based classifiers constructed from imbalanced data sets, i.e., data sets where the minority class of primary importance is under-represented in comparison to majority classes. We introduced two techniques to detect and process inconsistent examples from the majority classes in the boundary between the minority and majority classes. Both these techniques differ in the way of processing inconsistent boundary examples from the majority classes. The first approach removes them, while the other relabels them as belonging to the minority class. The experiments showed that the best results were obtained for the filtering technique, where inconsistent majority class examples were reassigned to the minority class, combined with a classifier composed of decision rules generated by the MODLEM algorithm.	rough set	Jerzy Stefanowski;Szymon Wilk	2006	Fundam. Inform.		rule-based system;rough set;computer science;machine learning;pattern recognition;data mining;mathematics;knowledge extraction	ML	13.330806975017417	-40.70834683806941	61546
2d6967a79ed0b093d616badd3e9d23217eef66d0	do normalization layers in a deep convnet really need to be distinct?		Yes, they do. This work investigates a perspective for deep learning: whether different normalization layers in a ConvNet require different normalizers. This is the first step towards understanding this phenomenon. We allow each convolutional layer to be stacked before a switchable normalization (SN) that learns to choose a normalizer from a pool of normalization methods. Through systematic experiments in ImageNet, COCO, Cityscapes, and ADE20K, we answer three questions: (a) Is it useful to allow each normalization layer to select its own normalizer? (b) What impacts the choices of normalizers? (c) Do different tasks and datasets prefer different normalizers? Our results suggest that (1) using distinct normalizers improves both learning and generalization of a ConvNet; (2) the choices of normalizers are more related to depth and batch size, but less relevant to parameter initialization, learning rate decay, and solver; (3) different tasks and datasets have different behaviors when learning to select normalizers.	convolutional neural network;database normalization;deep learning;emoticon;experiment;imagenet;solver	Ping Luo;Zhanglin Peng;Jiamin Ren;Ruimao Zhang	2018	CoRR			ML	21.611916645506874	-51.272630358488605	61616
e4a16fed6ed1b3aafd972e32df3a169ec9e39463	linear kernel combination using boosting		In this paper, we propose a novel algorithm to design multiclass kernels based on an iterative combination of weak kernels in a schema inspired from the boosting framework. Our solution has a complexity linear with the training set size. We evaluate our method for classification on a toy example by integrating our multi-class kernel into a kNN classifier and comparing our results with a reference iterative kernel design method. We also evaluate our method for image categorization by considering a classic image database and comparing our boosted linear kernel combination with the direct linear combination of all features in a linear SVM.	algorithm;boosting (machine learning);categorization;complexity;computation;computer vision;iterative method;kernel (operating system);multiclass classification;qr decomposition;test set;time complexity	Alexis Lechervy;Philippe Henri Gosselin;Frédéric Precioso	2012			mathematical optimization;machine learning;pattern recognition;mathematics	Vision	18.77499980491575	-41.34728347325673	61751
d7843cb54450dabfbab29e48a52087b93050ed68	gerome - a novel graph extraction robustness measure		The extraction of graph structures in Euclidean vector space is a topic of interest with applications in many fields, e.g., the biomedical domain. While a number of different approaches have been presented, a quantitative evaluation of those algorithms remains a challenging task: Manual generation of ground truth for real-world data is often timeconsuming and error-prone, and while tools for generating synthetic datasets with corresponding ground truth exist, this data often does not reflect the complexity in morphology and topology that real-world scenarios show. As a complementary or even alternative approach, we propose GERoMe, a novel graph extraction robustness measure, which quantifies the stability of algorithms that extract multigraphs with associated node positions from non-graph structures. Our method takes edgeassociated properties into consideration and does not necessarily require ground truth data. Moreover, available ground truth information can be incorporated to additionally evaluate the correctness of the graph extraction algorithm. We demonstrate the usefulness and applicability of our approach in an exemplary study on synthetic and real-world data.	cognitive dimensions of notations;correctness (computer science);embedded system;ground truth;hungarian algorithm;matching (graph theory);mathematical morphology;multigraph;similarity measure;synthetic data;synthetic intelligence	Dominik Drees;Aaron Scherzinger;Xiaoyi Jiang	2017		10.1007/978-3-319-58961-9_7		ML	15.011922277951468	-51.418425262181124	61762
87def5a3fdbe643e93cb1881b09bb8e0dc902b22	a cooperative coevolution-based pittsburgh learning classifier system embedded with memetic feature selection	nonparametric statistics;perforation;learning classifier system;accuracy memetics biological cells genetic algorithms genetics machine learning optimization;memetics;divide and conquer methods;genetics;embedded systems;accuracy;biological cells;machine learning;feature extraction;prediction accuracy;pattern classification;genetics based machine learning;genetic algorithm;genetic algorithms;feature selection;non parametric statistics;optimization;data reduction;data reduction techniques;learning artificial intelligence;learning classifier system cooperative coevolution based pittsburgh learning classifier system memetic feature selection real world classification feature selection data reduction technique classification model construction genetic based machine learning paradigm divide and conquer strategy feature subset population uci repository nonparametric statistical test;computational efficiency;pattern classification data reduction divide and conquer methods embedded systems feature extraction genetic algorithms learning artificial intelligence nonparametric statistics;divide and conquer;cooperative coevolution	Given that real-world classification tasks always have irrelevant or noisy features which degrade both prediction accuracy and computational efficiency, feature selection is an effective data reduction technique showing promising performance. This paper presents a cooperative coevolution framework to make the feature selection process embedded into the classification model construction within the genetic-based machine learning paradigm. The proposed approach utilizes the divide-and-conquer strategy to manage two populations in parallel, corresponding to the selected feature subsets and the rule sets of classifier respectively, in which a memetic feature selection algorithm is adopted to evolve the feature subset population while a Pittsburgh-style learning classifier system is used to carry out the classifier evolution. These two coevolving populations cooperate with each other regarding the fitness evaluation and the final solution is obtained via collaborations between the best individuals from each population. Empirical results on several benchmark data sets chosen from the UCI repository, together with a non-parametric statistical test, validate that the proposed approach is able to deliver classifiers of better prediction accuracy and higher stability with fewer selected features, compared with the original learning classifier system. In addition, the incorporated feature selection process is shown to help improve the computational efficiency as well.	benchmark (computing);computation;cooperative coevolution;depth perception;embedded system;feature selection;learning classifier system;machine learning;memetics;moose file system;population;programming paradigm;relevance;selection algorithm	Yun Wen;Hua Xu	2011	2011 IEEE Congress of Evolutionary Computation (CEC)	10.1109/CEC.2011.5949916	nonparametric statistics;genetic algorithm;quadratic classifier;computer science;artificial intelligence;machine learning;linear classifier;pattern recognition;data mining;learning classifier system;feature selection	ML	10.783760739566647	-41.61474991210943	62199
180cf927c0db46dfc4921fdd36e77071cef09ad8	classification of imbalanced data by oversampling in kernel space of support vector machines		Historical data sets for fault stage diagnosis in industrial machines are often imbalanced and consist of multiple categories or classes. Learning discriminative models from such data sets is challenging due to the lack of representative data and the bias of traditional classifiers toward the majority class. Sampling methods like synthetic minority oversampling technique (SMOTE) have been traditionally used for such problems to artificially balance the data set before being trained by a classifier. This paper proposes a weighted kernel-based SMOTE (WK-SMOTE) that overcomes the limitation of SMOTE for nonlinear problems by oversampling in the feature space of support vector machine (SVM) classifier. The proposed oversampling algorithm along with a cost-sensitive SVM formulation is shown to improve performance when compared to other baseline methods on multiple benchmark imbalanced data sets. In addition, a hierarchical framework is developed for multiclass imbalanced problems that have a progressive class order. The proposed WK-SMOTE and hierarchical framework are validated on a real-world industrial fault detection problem to identify deterioration in insulation of high-voltage equipments.	algorithm;baseline (configuration management);benchmark (computing);categories;class;dataspaces;discriminative model;elegant degradation;fault detection and isolation;feature vector;flux balance analysis;insulation device component;kernel (operating system);nonlinear system;oversampling;research data archiving;support vector machine;synthetic intelligence;user space;wikipedia;voltage	Josey Mathew;Chee Khiang Pang;Ming Luo;Weng Hoe Leong	2018	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2017.2751612	machine learning;kernel (linear algebra);artificial intelligence;support vector machine;discriminative model;fault detection and isolation;pattern recognition;computer science;oversampling;statistical classification;feature vector;data set	ML	14.590289863600882	-42.60912188591826	62526
1bd2c9a41a255b12cd521ed18c4de8ce48e00f39	do we really need more training data for object localization		The key factor for training a good neural network lies in both model capacity and large-scale training data. As more datasets are available nowadays, one may wonder whether the success of deep learning descends from data augmentation only. In this paper, we propose a new dataset, namely, Extended ImageNet Classification (EIC) dataset based on the original ILSVRC CLS 2012 set to investigate if more training data is a crucial step. We address the problem of object localization where given an image, some boxes (also called anchors) are generated to localize multiple instances. Different from previous work to place all anchors at the last layer, we split boxes of different sizes at various resolutions in the network, since small anchors are more prone to be identified at larger spatial location in the shallow layers. Inspired by the hourglass work, we apply a conv-deconv network architecture to generate object proposals. The motivation is to fully leverage high-level summarized semantics and to utilize their up-sampling version to help guide local details in the low-level maps. Experimental results demonstrate the effectiveness of such a design. Based on the newly proposed dataset, we find more data could enhance the average recall, but a more balanced data distribution among categories could obtain better results at the cost of fewer training samples.	artificial neural network;common language infrastructure;convolutional neural network;deep learning;earth inductor compass;html element;high- and low-level;imagenet;map;network architecture;sampling (signal processing)	Hongyang Li;Yu Liu;Xin Zhang;Zhecheng An;Jingjing Wang;Yibo Chen;Jihong Tong	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296386	artificial neural network;pattern recognition;network architecture;training set;feature extraction;deep learning;computer science;semantics;cls upper limits;artificial intelligence	Vision	22.868097715482254	-50.616756971165614	62707
c575c46c402da8647b2b154271059e22faafc9a4	approximate tree kernels	kernel methods;approximation;tree kernels;convolution kernels	Convolution kernels for trees provide simple means for lear ning with tree-structured data. The computation time of tree kernels is quadratic in the size of t he trees, since all pairs of nodes need to be compared. Thus, large parse trees, obtained from HTML d ocuments or structured network data, render convolution kernels inapplicable. In this art icle, we propose an effective approximation technique for parse tree kernels. The approximate tree kern els (ATKs) limit kernel computation to a sparse subset of relevant subtrees and discard redundant s tructures, such that training and testing of kernel-based learning methods are significantly acceler ated. We devise linear programming approaches for identifying such subsets for supervised and un supervised learning tasks, respectively. Empirically, the approximate tree kernels attain run-timeprovements up to three orders of magnitude while preserving the predictive accuracy of regular tree kernels. For unsupervised tasks, the approximate tree kernels even lead to more accurate predict ions by identifying relevant dimensions in feature space.	anti-spam techniques;approximation algorithm;cluster analysis;computation;convolution;experiment;feature vector;gigabyte;html;intrusion detection system;kernel principal component analysis;kilobyte;linear programming;machine learning;mathematical optimization;parse tree;parsing;performance;quadratic function;regular tree grammar;requirement;run time (program lifecycle phase);self-replicating machine;spamdexing;sparse matrix;supervised learning;time complexity;tree (data structure);unsupervised learning	Konrad Rieck;Tammo Krueger;Ulf Brefeld;Klaus-Robert Müller	2010	Journal of Machine Learning Research	10.1145/1756006.1756022	kernel method;mathematical optimization;computer science;machine learning;approximation;pattern recognition;mathematics;tree kernel	ML	20.45130346033041	-38.25219542412582	62857
335396f91a45e0bec6a39c616f86d7d03280e15c	unsupervised feature learning for optical character recognition	optical character recognition software standards irrigation recurrent neural networks training;feature learning word prediction ocr rnn;rnn;word prediction;ocr;feature learning	Most of the popular optical character recognition (OCR) architectures use a set of handcrafted features and a powerful classifier for isolated character classification. Success of these methods often depend on the suitability of these features for the language of interest. In recent years, whole word recognition based on Recurrent Neural Networks (RNN) has gained popularity. These methods use simple features such as raw pixel values or profiles. Success of these methods depend on the learning capabilities of these networks to encode the script and language information. In this work, we investigate the possibility of learning an appropriate set of features for designing OCR for a specific language. We learn the language specific features from the data with no supervision. This enables the seamless adaptation of the architecture across languages. In this work, we learn features using a stacked Restricted Boltzman Machines (RBM) and use it with the RNN based recognition solution. We validate our method on five different languages. In addition, these novel features also result in better convergence rate of the RNNs.		Devendra K. Sahu;C. V. Jawahar	2015	2015 13th International Conference on Document Analysis and Recognition (ICDAR)	10.1109/ICDAR.2015.7333920	feature learning;speech recognition;feature;computer science;intelligent word recognition;machine learning;pattern recognition	AI	22.787386762221487	-51.36026511911294	63007
6790f8803514342678b30b2d3e91c533e6967f4d	on the usage of the probability integral transform to reduce the complexity of multi-way fuzzy decision trees in big data classification problems		We present a new distributed fuzzy partitioning method to reduce the complexity of multi-way fuzzy decision trees in Big Data classification problems. The proposed algorithm builds a fixed number of fuzzy sets for all variables and adjusts their shape and position to the real distribution of training data. A two-step process is applied : 1) transformation of the original distribution into a standard uniform distribution by means of the probability integral transform. Since the original distribution is generally unknown, the cumulative distribution function is approximated by computing the q-quantiles of the training set; 2) construction of a Ruspini strong fuzzy partition in the transformed attribute space using a fixed number of equally distributed triangular membership functions. Despite the aforementioned transformation, the definition of every fuzzy set in the original space can be recovered by applying the inverse cumulative distribution function (also known as quantile function). The experimental results reveal that the proposed methodology allows the state-of-the-art multi-way fuzzy decision tree (FMDT) induction algorithm to maintain classification accuracy with up to 6 million fewer leaves.	approximation algorithm;big data;decision tree;fuzzy set;test set	Mikel Elkano;Mikel Uriz;Humberto Bustince;Mikel Galar	2018	2018 IEEE International Congress on Big Data (BigData Congress)	10.1109/BigDataCongress.2018.00011	cumulative distribution function;fuzzy logic;mathematical optimization;big data;computer science;fuzzy set;decision tree;quantile function;probability integral transform;uniform distribution (continuous)	ML	13.130317497577906	-40.984515019504826	63185
2677672ec54530e8911a758de2688bd80e1ade96	patch alignment for dimensionality reduction	eigenvalues and eigenfunctions;extraction information;principal component analysis linear discriminant analysis spectral analysis clustering algorithms algorithm design and analysis level measurement data mining computer vision application software laplace equations;alignement;optimisation;vision ordenador;analisis componente principal;empirical study;sample size;sistema experto;laplacian eigenmap;methode empirique;localite;computer science and information systems;spectral analysis computer vision data mining eigenvalues and eigenfunctions laplace equations optimisation principal component analysis;optimizacion;small sample size;analisis datos;information extraction;securite;local tangent space alignment;application software;dimension reduction;discriminative locality alignment patch alignment dimensionality reduction algorithm data mining computer vision spectral analysis principal component analysis laplacian eigenmap local tangent space alignment part optimization stage;tamano muestra;metodo empirico;laplacian;atelier genie logiciel;empirical method;taille echantillon;locality;part optimization stage;date;data mining;discriminative locality alignment dimensionality reduction spectral analysis patch alignment;journal article;computer vision;reduction dimension;data analysis;laplacien;laplace equations;laplaciano;dimensionality reduction;fecha;fouille donnee;principal component analysis;safety;analyse spectrale;alineamiento;level measurement;analyse composante principale;difference in differences;reduccion dimension;clustering algorithms;analyse donnee;analisis espectral;vision ordinateur;optimization;systeme expert;spectral analysis;seguridad;common property;linear discriminant analysis;patch alignment;dimensional reduction;busca dato;extraccion informacion;algorithm design and analysis;alignment;local linear embedding;software engineering workshop;local alignment;discriminative locality alignment;dimensionality reduction algorithm;expert system	"""Spectral analysis-based dimensionality reduction algorithms are important and have been popularly applied in data mining and computer vision applications. To date many algorithms have been developed, e.g., principal component analysis, locally linear embedding, Laplacian eigenmaps, and local tangent space alignment. All of these algorithms have been designed intuitively and pragmatically, i.e., on the basis of the experience and knowledge of experts for their own purposes. Therefore, it will be more informative to provide a systematic framework for understanding the common properties and intrinsic difference in different algorithms. In this paper, we propose such a framework, named """"patch alignment,rdquo which consists of two stages: part optimization and whole alignment. The framework reveals that (1) algorithms are intrinsically different in the patch optimization stage and (2) all algorithms share an almost identical whole alignment stage. As an application of this framework, we develop a new dimensionality reduction algorithm, termed discriminative locality alignment (DLA), by imposing discriminative information in the part optimization stage. DLA can (1) attack the distribution nonlinearity of measurements; (2) preserve the discriminative ability; and (3) avoid the small-sample-size problem. Thorough empirical studies demonstrate the effectiveness of DLA compared with representative dimensionality reduction algorithms."""	algorithm;arc diagram;computer vision;data mining;database;drive letter assignment;feret (facial recognition technology);information;laplacian matrix;local tangent space alignment;locality of reference;mathematical optimization;nonlinear dimensionality reduction;nonlinear system;principal component analysis;spectral density estimation;the matrix	Tianhao Zhang;Dacheng Tao;Xuelong Li;Jie Yang	2009	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2008.212	computer science;machine learning;pattern recognition;data mining;mathematics;linear discriminant analysis;empirical research;expert system;information extraction;statistics;dimensionality reduction	Vision	24.056082952437023	-39.38540256612854	63519
ae9f1e2e4cc4d63dc8310ea0111bd554b7b9bc27	robust network-based binary-to-vector encoding for scalable iot binary file retrieval		The goal of IoT binary file retrieval is to retrieve homologous binary files from a large IoT binary file database. Binary file retrieval has many applications, such as security analysis, OEM detection and plagiarism detection. However, traditional string-based approaches are hard to retrieve binary file which contains few or obfuscated strings. To solve this problem, we propose a novel neural network-based approach for encoding binary file into numerical vector based on non-string binary features. Moreover, by using this encoding method, the retrieval task can be accelerated by locality-sensitive hashing technique. For network training and testing, we compile 893 open source components into 71,129 labeled binary file pairs by using 16 different compilation configurations. We implement a prototype called B2V and compare it with IHB, a string-based approach, on both original and string obfuscated testing sets. The results show that the AUC of B2V is better than IHB (0.94 vs. 0.81) on the string obfuscated testing set, while still keeps comparable performance with IHB on the original testing set. Moreover, B2V can be easily retrained to adapt to string obfuscated scenarios with 15%–20% performance improvement. In the interest of open science, we also make our dataset publicly available to seed future improvements.	binary file	Yu Chen;Hong Li;Yuan Ma;Zhiqiang Shi;Limin Sun	2018		10.1007/978-3-319-94268-1_5	compiler;data mining;performance improvement;hash function;scalability;encoding (memory);computer science;plagiarism detection;obfuscation;distributed computing;binary number	Vision	18.89349073491128	-47.1452210376904	63631
818219d653329534c44fab14c49d1b4aac1281d0	cascadecnn: pushing the performance limits of quantisation		This work presents CascadeCNN, an automated toolflow that pushes the quantisation limits of any given CNN model, to perform highthroughput inference by exploiting the computation time-accuracy trade-off. Without the need for retraining, a two-stage architecture tailored for any given FPGA device is generated, consisting of a lowand a high-precision unit. A confidence evaluation unit is employed between them to identify misclassified cases at run time and forward them to the high-precision unit or terminate computation. Experiments demonstrate that CascadeCNN achieves a performance boost of up to 55% for VGG-16 and 48% for AlexNet over the baseline design for the same resource budget and accuracy.	baseline (configuration management);computation;error-tolerant design;experiment;field-programmable gate array;quantization (image processing);quantization (physics);run time (program lifecycle phase);terminate (software);time complexity	Alexandros Kouris;Stylianos I. Venieris;Christos-Savvas Bouganis	2018	CoRR		field-programmable gate array;machine learning;retraining;architecture;computation;artificial intelligence;inference;computer science	Robotics	22.71723587062862	-51.817525131573454	63884
724aa3a0598adfc548d833c8e9e70976181053cb	boosting the differences: a fast bayesian classifier neural network	bayesian classifier;neural networks;parallel processing networks;naive bayesian classifier;pattern recognition;gradient descent algorithm;boosting differences;neural network	A Bayesian classifier that up-weights the differences in the attribute values is discussed. Using four popular datasets from the UCI repository, some interesting features of the network are illustrated. The network is suitable for classification problems.	artificial neural network;bayesian network;boosting (machine learning);naive bayes classifier;statistical classification	Ninan Sajeeth Philip;K. Babu Joseph	2000	Intell. Data Anal.	10.3233/ida-2000-4602	gradient descent;margin classifier;naive bayes classifier;computer science;machine learning;pattern recognition;data mining;artificial neural network	ML	12.01690997216255	-40.24955190433854	63992
e278f023b203d71cbc702c0474d830aaeea14876	analysis of a two-stage bayes classifiers construction method: the 2-dimensional case		The paper discusses the properties of a two-stage Bayes classifier construction method in a case when the objects are represented with two quantitative features. The aim of the study was to show that, in general, the two-stage approach allows one to enhance the precision of the classification results. The main method of the investigation is statistical modeling while applying a pseudorandom number generator. In some cases, the statistical modeling results are compared with the results of theoretical inferences.	naive bayes classifier;pseudorandom number generator;sensitivity index;statistical model	Aivars Lorencs;Juris Sinica-Sinavskis	2013	Automatic Control and Computer Sciences	10.3103/S0146411613050040	bayes classifier;computer science;machine learning;pattern recognition;data mining;bayes error rate;statistics	Vision	11.434898537472357	-38.22881960396655	64021
90348df04b329cc52036695a816d89cad013865f	a neighbor selection method based on network community detection for collaborative filtering	community detection;collaborative filtering neighbor selection community detection evolution algorithm;time complexity neighbor selection method network community detection collaborative filtering recommender system knn evolutionary algorithm;collaborative filtering;communities optimization evolutionary computation accuracy computational modeling training collaboration;recommender systems collaborative filtering computational complexity evolutionary computation;evolution algorithm;neighbor selection	The neighbor selection that determines which users are exploited to estimate a target user's ratings has an important influence on the accuracy of recommendations of collaborative filtering based recommender system. Two kinds of ways for neighbor selection: KNN and cluster-based, are lack of specificity which refers to selecting different appropriate neighbors for different given target users, and thus limit the accuracy of recommendation. Therefore, in this paper, firstly, we propose a method that employs the evolutionary algorithm to optimize neighbors for all target users. Secondly, overcoming the high time complexity of the first one, we present another approach in which community detection algorithm is utilized as a preprocessing, and then the evolution algorithm is employed to optimize the neighborhood size for every community. We present experiments on a standard benchmark data-set, and the results show that the two methods both realize the specificity in neighbor selection, and accordingly lead to a higher accuracy of recommendations. Besides, the second one makes a good compromise between the specificity and time complexity.	benchmark (computing);collaborative filtering;evolutionary algorithm;experiment;k-nearest neighbors algorithm;preprocessor;recommender system;sensitivity and specificity;time complexity	Lin Guo;Qinke Peng	2014	2014 IEEE/ACIS 13th International Conference on Computer and Information Science (ICIS)	10.1109/ICIS.2014.6912122	computer science;collaborative filtering;machine learning;pattern recognition;data mining;recommender system	ML	10.069467902143687	-43.32870318543564	64164
3e2030e63782d093f56cd1732733abc5e9542ee2	a twin-hypersphere support vector machine classifier and the fast learning algorithm	twin hypersphere;期刊论文;nonparallel hyperplane;pattern recognition;support vector machine;gilbert s algorithm	This paper formulates a twin-hypersphere support vector machine (THSVM) classifier for binary recognition. Similar to the twin support vector machine (TWSVM) classifier, this THSVM determines two hyperspheres by solving two related support vector machine (SVM)-type problems, each one is smaller than the classical SVM, which makes the THSVM be more efficient than the classical SVM. In addition, the THSVM avoids the matrix inversions in its two dual quadratic programming problems (QPPs) compared with the TWSVM. By considering the characteristics of the dual QPPs of THSVM, an efficient Gilbert's algorithm for the THSVM based on the reduced convex hull (RCH) instead of directly optimizing its pair of QPPs is further presented. Computational results on several synthetic as well as benchmark datasets indicate the significant advantages of the THSVM classifier in the computational time and test accuracy.	algorithm;support vector machine	Xinjun Peng;Dong Xu	2013	Inf. Sci.	10.1016/j.ins.2012.09.009	margin classifier;support vector machine;quadratic classifier;computer science;machine learning;pattern recognition;mathematics;structured support vector machine;algorithm	ML	21.364813483992798	-39.03964265977557	64206
1ecdb411fb0ff182ca911d32879458b18b4b7823	divide and conquer approach for semi-supervised multi-category classification through localized kernel spectral clustering		In this paper, we propose ‘divide-and-conquer approach for multi-category semi-supervised’ (DAC-MSS) classification and a novel semi-supervised binary classifier termed as ‘twin support vector machine with localized kernel spectral clustering’ (TW-LKSC). DAC-MSS builds a multi-category classifier model organized in the form of a tree of binary classifiers. The tree consists of several TW-LKSC classifiers which use a training set consisting of few labeled samples and rest unlabeled samples to generate a pair of hyperplanes, by solving a system of linear equations. The propagation of labels to unlabeled patterns is achieved through localized kernel spectral clustering (LKSC) which is the core clustering model embedded in TW-LKSC. TW-LKSC also employs cluster prototype to localize the generation of hyperplanes and prevents them from extending infinitely. The strength of DAC-MSS is its better classification accuracy and improved learning time, due to divide and conquer approach, as compared to one-against-all based semi-supervised classification algorithms. This is proved experimentally for benchmark UCI datasets. We have applied DAC-MSS for color image segmentation of images from Berkley Segmentation Dataset.	cluster analysis;kernel (operating system);semi-supervised learning;semiconductor industry;spectral clustering	Pooja Saigal;Vaibhav Khanna;Reshma Rastogi	2017	Neurocomputing	10.1016/j.neucom.2017.01.065	machine learning;pattern recognition;data mining;mathematics	ML	18.45131264133014	-42.33497070517322	64232
cb82ca0abb0ab6d42309d81d39e40772fdbda76a	active evaluation of predictive models	fehlerschatzung;machine learning;error estimation;aktive evaluierung;statistische tests;statistical tests;predictive models;vorhersagemodelle;maschinelles lernen;active evaluation	The field of machine learning studies algorithms that infer predictive models from data. Predictive models are applicable for many practical tasks such as spam filtering, face and handwritten digit recognition, and personalized product recommendation. In general, they are used to predict a target label for a given data instance. In order to make an informed decision about the deployment of a predictive model, it is crucial to know the model’s approximate performance. To evaluate performance, a set of labeled test instances is required that is drawn from the distribution the model will be exposed to at application time. In many practical scenarios, unlabeled test instances are readily available, but the process of labeling them can be a timeand cost-intensive task and may involve a human expert. This thesis addresses the problem of evaluating a given predictive model accurately with minimal labeling effort. We study an active model evaluation process that selects certain instances of the data according to an instrumental sampling distribution and queries their labels. We derive sampling distributions that minimize estimation error with respect to different performance measures such as error rate, mean squared error, and F -measures. An analysis of the distribution that governs the estimator leads to confidence intervals, which indicate how precise the error estimation is. Labeling costs may vary across different instances depending on certain characteristics of the data. For instance, documents differ in their length, comprehensibility, and technical requirements; these attributes affect the time a human labeler needs to judge relevance or to assign topics. To address this, the sampling distribution is extended to incorporate instancespecific costs. We empirically study conditions under which the active evaluation processes are more accurate than a standard estimate that draws equally many instances from the test distribution. We also address the problem of comparing the risks of two predictive models. The standard approach would be to draw instances according to the test distribution, label the selected instances, and apply statistical tests to identify significant differences. Drawing instances according to an instrumental distribution affects the power of a statistical test. We derive a sampling procedure that maximizes test power when used to select instances, and thereby minimizes the likelihood of choosing the inferior model. Furthermore, we investigate the task of comparing several alternative models; the objective of an evaluation could be to rank the models according to the risk that they incur or to identify the model with lowest		Christoph Sawade	2013			computer science;machine learning;data mining;statistics	ML	10.66728954828069	-47.797541953542435	64497
30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9	model compression	supervised learning;model compression	"""Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers. Unfortunately, the space required to store this many classifiers, and the time required to execute them at run-time, prohibits their use in applications where test sets are large (e.g. Google), where storage space is at a premium (e.g. PDAs), and where computational power is limited (e.g. hea-ring aids). We present a method for """"compressing"""" large, complex ensembles into smaller, faster models, usually without significant loss in performance."""	computation;personal digital assistant;supervised learning	Cristian Bucila;Rich Caruana;Alexandru Niculescu-Mizil	2006		10.1145/1150402.1150464	computer science;machine learning;pattern recognition;data mining	ML	18.3636970790764	-48.209369519823724	64667
668b44bd9feb2b3034be6f4a75b7df1be58a39c4	a review on the combination of binary classifiers in multiclass problems	supervised learning;machine learning;binary classification;multiclass classification	Several real problems involve the classification of data into categories or classes. Given a data set containing data whose classes are known, Machine Learning algorithms can be employed for the induction of a classifier able to predict the class of new data from the same domain, performing the desired discrimination. Some learning techniques are originally conceived for the solution of problems with only two classes, also named binary classification problems. However, many problems require the discrimination of examples into more than two categories or classes. This paper presents a survey on the main strategies for the generalization of binary classifiers to problems with more than two classes, known as multiclass classification problems. The focus is on strategies that decompose the original multiclass problem into multiple binary subtasks, whose outputs are combined to obtain the final prediction.	algorithm;binary classification;category theory;machine learning;multiclass classification;stacking	Ana Carolina Lorena;André Carlos Ponce de Leon Ferreira de Carvalho;João Gama	2008	Artificial Intelligence Review	10.1007/s10462-009-9114-9	statistical classification;binary classification;computer science;machine learning;linear classifier;multiclass classification;classification rule;pattern recognition;data mining;supervised learning;structured support vector machine;one-class classification	ML	15.253076573555067	-42.27167756668121	64781
e30aa6949c51804751d97057ffeb9325dcde0f09	examining the use of neural networks for feature extraction: a comparative analysis using deep learning, support vector machines, and k-nearest neighbor classifiers		Neural networks in many varieties are touted as very powerful machine learning tools because of their ability to distill large amounts of information from different forms of data, extracting complex features and enabling powerful classification abilities. In this study, we use neural networks to extract features from both images and numeric data and use these extracted features as inputs for other machine learning models, namely support vector machines (SVMs) and k-nearest neighbor classifiers (KNNs), in order to see if neural-network-extracted features enhance the capabilities of these models. We tested 7 different neural network architectures in this manner, 4 for images and 3 for numeric data, training each for varying lengths of time and then comparing the results of the neural network independently to those of an SVM and KNN on the data, and finally comparing these results to models of SVM and KNN trained using features extracted via the neural network architecture. This process was repeated on 3 different image datasets and 2 different numeric datasets. The results show that, in many cases, the features extracted using the neural network significantly improve the capabilities of SVMs and KNNs compared to running these algorithms on the raw features, and in some cases also surpass the performance of the neural network alone. This in turn suggests that it may be a reasonable practice to use neural networks as a means to extract features for classification by other machine learning models for some datasets.	artificial neural network;deep learning;feature extraction;k-nearest neighbors algorithm;level of measurement;machine learning;network architecture;neural network software;support vector machine	Stephen Notley;Malik Magdon-Ismail	2018	CoRR		architecture;machine learning;artificial neural network;support vector machine;feature extraction;artificial intelligence;deep learning;mathematics;k-nearest neighbors algorithm	ML	18.60477406961428	-49.10182777382008	64825
2c149eff297ffe7d762fd453571fb7cb1761d7a5	in defence of negative mining for annotating weakly labelled data	multiple instance learning;image annotation;automatic annotation;weakly supervised learning;negative mining	We propose a novel approach to annotating weakly labelled data. In contrast to many existing approaches that perform annotation by seeking clusters of self-similar exemplars (minimising intra-class variance), we perform image annotation by selecting exemplars that have never occurred before in the much larger, and strongly annotated, negative training set (maximising inter-class variance). Compared to existing methods, our approach is fast, robust, and obtains state of the art results on two challenging data-sets – voc2007 (all poses), and the msr2 action data-set, where we obtain a 10% increase. Moreover, this use of negative mining complements existing methods, that seek to minimize the intra-class variance, and can be readily integrated with many of them.	automatic image annotation;experiment;programming in the large and programming in the small;self-similarity;test set	Parthipan Siva;Chris Russell;Tao Xiang	2012		10.1007/978-3-642-33712-3_43	computer science;bioinformatics;machine learning;data mining;automatic image annotation	Vision	17.12456164972701	-40.61593445401522	64934
c723826688a60e93db502f3b8b2ebd2ddf3245f4	adversarial examples that fool both computer vision and time-limited humans		Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by matching the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers.	artificial neural network;assumed;bandlimiting;cns disorder;classification;computer vision;convolutional neural network;humans;judgment;machine learning;neural network simulation;neuroscience discipline;observers	Gamaleldin F. Elsayed;Shreya Shankar;Brian Cheung;Nicolas Papernot;Alexey Kurakin;Ian J. Goodfellow;Jascha Sohl-Dickstein	2018			computer science;machine learning;artificial intelligence;architecture;adversarial system;computer vision;human visual system model	Vision	20.01161219719742	-52.010032650894054	65150
872ae0860f26c6c0ab00278e841107e51d258594	sparse coding for n-gram feature extraction and training for file fragment classification		File fragment classification is an important step in the task of file carving in digital forensics. In file carving, files must be reconstructed based on their content as a result of their fragmented storage on disk or in memory. Existing methods for classification of file fragments typically use hand-engineered features, such as byte histograms or entropy measures. In this paper, we propose an approach using sparse coding that enables automated feature extraction. Sparse coding, or sparse dictionary learning, is an unsupervised learning algorithm, and is capable of extracting features based simply on how well those features can be used to reconstruct the original data. With respect to file fragments, we learn sparse dictionaries for n-grams, continuous sequences of bytes, of different sizes. These dictionaries may then be used to estimate n-gram frequencies for a given file fragment, but for significantly larger n-gram sizes than are typically found in existing methods which suffer from combinatorial explosion. To demonstrate the capability of our sparse coding approach, we used the resulting features to train standard classifiers, such as support vector machines over multiple file types. Experimentally, we achieved significantly better classification results with respect to existing methods, especially when the features were used in supplement to existing hand-engineered features.	algorithm;autoencoder;automated reasoning;byte;computation;entropy (information theory);experiment;feature extraction;grams;long short-term memory;machine learning;n-gram;neural coding;recurrent neural network;sparse dictionary learning;sparse matrix;statistical classification;support vector machine;time complexity;unsupervised learning	Felix Hao Wang;Tu-Thach Quach;Jason W. Wheeler;James B. Aimone;Conrad D. James	2018	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2018.2823697	n-gram;artificial intelligence;feature extraction;pattern recognition;computer science;neural coding	ML	19.700612148359216	-47.401191305883145	65161
9c2251fd49a23e6ecb5c6128f9472dee456d29c1	weighted naive bayesian classifier	itemsets;probability;bayes methods;niobium;bayesian methods;pattern classification bayes methods;naive bayesian classifier;emerging pattern naive bayesian classifier crisp class;training data;emerging pattern;machine learning;pattern classification;crisp class;frequency;bayesian methods niobium training data itemsets frequency probability educational institutions power measurement machine learning;power measurement	The naive Bayesian (NB) classifier is one of the simple yet powerful classification methods. One of the important problems in NB (and many other classifiers) is that it is built using crisp classes assigned to the training data. In this paper, we propose an improvement over the NB classifier by employing emerging patterns (EPs) to weight the training instances. That is, we generalize the NB classifier so that it can take into account weighted classes assigned to the training data. EPs are those itemsets whose frequencies in one class are significantly higher than their frequencies in the other classes. Our experiments prove that our proposed method is superior to the original NB classifier.	bayesian network;experiment;naive bayes classifier	Hamad Alhammady	2007	2007 IEEE/ACS International Conference on Computer Systems and Applications	10.1109/AICCSA.2007.370918	margin classifier;niobium;training set;quadratic classifier;bayesian probability;computer science;machine learning;frequency;pattern recognition;probability;data mining;statistics	Vision	11.706603503977187	-39.59847622390235	65409
b1beb89b67736e3a585618d4678caadb4dd5d4af	a two-level approach to choose the cost parameter in support vector machines	mpec problem;nonlinear programming;data mining;objective function;support vector machine;numerical experiment;lower bound;cost parameter	"""A new SVM model used to calculate the optimal value of cost parameter C for particular problems of linearity non-separability of data is presented in this paper. The new SVM model is formulated in the form of one of MPEC problems with an integer objective function. A lower bound, positive number, C""""0 is required to provide for avoiding choosing a candidate set of C. Numerical experiments show that this model for choice of C is suitable for solving SVM problems."""	support vector machine	Yulin Dong;Zhonghang Xia;Zunquan Xia	2008	Expert Syst. Appl.	10.1016/j.eswa.2007.01.004	support vector machine;mathematical optimization;nonlinear programming;computer science;machine learning;data mining;mathematics;upper and lower bounds	ML	21.421491555863092	-38.80582019908012	65600
25d93f652f9fded2cf305703cf708cfc11b640a1	controlling false alarms with support vector machines	support vector machines;design support;error estimation false alarms support vector machines neyman pearson criterion;false alarm rate;neyman pearson;neyman pearson criterion;support vector machines support vector machine classification training data error analysis costs statistics neoplasms benign tumors cancer constraint theory;error estimation;parameter space;pattern classification;support vector machine;error estimate;support vector machines pattern classification;false alarms	We study the problem of designing support vector classifiers with respect to a Neyman-Pearson criterion. Specifically, given a user-specified level alpha isin (0,1), how can we ensure a false alarm rate no greater than q while minimizing the miss rate? We examine two approaches, one based on shifting the offset of a conventionally trained SVM and the other based on the introduction of class-specific weights. Our contributions include a novel heuristic for improved error estimation and a strategy for efficiently searching the parameter space of the second method. We also provide a characterization of the feasible parameter set of the 2v-SVM on which the second approach is based. The proposed methods are compared on four benchmark datasets	benchmark (computing);heuristic;support vector machine	Mark A. Davenport;Richard G. Baraniuk;Clayton D. Scott	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1661344	support vector machine;computer science;machine learning;pattern recognition;mathematics;relevance vector machine;structured support vector machine;statistics	Robotics	16.047903406879687	-43.35351323660586	66011
f0b324536167f13fdf9b10e08dfe73363f177405	compound classification models for recommender systems	pattern classification information filters consumer behaviour;customer behavior;linear support vector machines compound classification model recommender systems customer behavior nearest neighbor method collaborative filtering multiclass classification problem autocorrelation structure binary classification problem;nearest neighbor method;recommender system;collaborative filtering;multi class classification;pattern classification;consumer behaviour;recommender systems collaboration nearest neighbor searches information filtering information filters buildings context modeling autocorrelation support vector machines support vector machine classification;support vector machine;information filters	Recommender systems recommend products to customers based on ratings or past customer behavior. Without any information about attributes of the products or customers involved, the problem has been tackled most successfully by a nearest neighbor method called collaborative filtering in the context, while additional efforts invested in building classification models did not pay off and did not increase the quality. Therefore, classification methods have mainly been used in conjunction with product or customer attributes. Starting from a view on the plain recommendation task without attributes as a multi-class classification problem, we investigate two particularities, its autocorrelation structure as well as the absence of re-occurring items (repeat buying). We adapt the standard generic reductions 1-vs-rest and 1-vs-l of multi-class problems to a set of binary classification problems to these particularities and thereby provide a generic compound classifier for recommender systems. We evaluate a particular specialization thereof using linear support vector machines as member classifiers on MovieLens data and show that it outperforms state-of-the-art methods, i.e., item-based collaborative filtering.	autocorrelation;binary classification;collaborative filtering;data mining;heuristic;hoc (programming language);kerrison predictor;microsoft outlook for mac;movielens;multiclass classification;nearest neighbor search;parallel computing;partial template specialization;recommender system;support vector machine;test data	Lars Schmidt-Thieme	2005	Fifth IEEE International Conference on Data Mining (ICDM'05)	10.1109/ICDM.2005.46	nearest neighbour algorithm;support vector machine;computer science;collaborative filtering;machine learning;multiclass classification;pattern recognition;data mining;consumer behaviour;recommender system	ML	13.080779247013126	-45.6853769084722	66237
edcf832fd1f1ba7d1f948c3ec0392bd0899c590b	ecoc matrix pruning using accuracy information		The target of ensemble pruning is to increase efficiency by reducing the ensemble size of a multi classifier system and thus computational and storage costs, without sacrificing and preferably enhancing the generalization performance. However, most state-of-the-art ensemble pruning methods are based on unweighted or weighted voting ensembles; and their extensions to the Error Correcting Output Coding (ECOC) framework is not strongly evident or successful. In this study, a novel strategy for pruning ECOC ensembles which is based on a novel accuracy measure is presented. The measure is defined by establishing the link between the accuracies of the two-class base classifiers in the context of the main multiclass problem. The results show that the method outperforms the ECOC extensions of the state-of-the-art pruning methods in the majority of cases and that it is even possible to improve the generalization performance by only using 30% of the initial ensemble size in certain scenarios.		Cemre Zor;Terry Windeatt;Josef Kittler	2013		10.1007/978-3-642-38067-9_34	machine learning;pattern recognition;pruning	HCI	13.268158598110196	-39.77643756066107	66249
1c4d832c02eafa0f22b13591394fcc08805e1c2d	ensemble learning on large scale financial imbalanced data		This study focused on evaluating the performance of ensemble learning on handling imbalanced data. Imbalanced data is a special problem in classification task where the class distribution is not uniformed. Resampling (SMOTE and ENN) is employed to improve the classifier performance. Four metrics is applied for performance evaluation i.e., precision, recall, specificity, and F-1 score. Based on the experiments, Bagging has a superior performance compared to baseline classifiers (Naïve Bayes and Log Regression) and other ensemble learnings (Boosting and Random Forest). In addition, the combination of SMOTE and ENN successfully increase the classification performance and avoiding biased to the majority class.	baseline (configuration management);ensemble learning;experiment;naive bayes classifier;performance evaluation;random forest;sensitivity and specificity;statistical classification	Hadaiq Rolis Sanabila;Wisnu Jatmiko	2018	2018 International Workshop on Big Data and Information Security (IWBIS)	10.1109/IWBIS.2018.8471702	naive bayes classifier;boosting (machine learning);logistic regression;statistical classification;random forest;resampling;ensemble learning;artificial intelligence;computer science;pattern recognition	AI	13.733250274773477	-41.46317911317198	66557
f9fff0c794ac6d804667e4760b2d931d450554bd	unified classification and generation networks for co-creative systems.		This paper reports on a new deep machine learning architecture to classify and generate input for co-creative systems. Our approach combines the generational strengths of Variational Autoencoders with the image sharpness typically associated with Generative Adversarial Networks, thereby enabling a generative deep learning architecture for training co-creative agents called the Auxiliary Classifier Variational Autoencoder (AC-VAE). We report the experimental results of our network’s classification accuracy and generational loss on the MNIST numerical image dataset and TU-Berlin sketch data set. Results indicate our technique is effective for classifying and generating sketched object images, with larger sizes. We also describe how our network is particularly useful for co-creative agents since it can generate diverse concepts, as well as transform and morph user generated sketches while maintaining their concept identity.	autoencoder;decibel;deep learning;generative adversarial networks;mnist database;machine learning;numerical analysis;variational principle	Kunwar Yashraj Singh;Nicholas M. Davis;Chih-Pin Hsiao;Ricardo Macias;Brenda Lin	2017			computer science	AI	24.248231185646684	-49.596564952585396	66561
51fbe97a06735f71cf1dcda367102d98e4052e6d	seg-ssc: a framework based on synthetic examples generation for self-labeled semi-supervised classification	biological patents;cybernetics;reliability;biomedical journals;co training self labeled methods semi supervised classification synthetic examples;standards;manifolds;prototypes training reliability prediction algorithms cybernetics manifolds standards;text mining;europe pubmed central;prototypes;training;citation search;prediction algorithms;citation networks;research articles;abstracts;open access;life sciences;clinical guidelines;learning artificial intelligence pattern classification sampling methods;labeled data distribution seg ssc synthetic examples generation self labeled semisupervised classification self learning process supervised models labeled samples classification performance self labeled method synthetic labeled data oversampling technique positioning adjustment model;sampling methods learning artificial intelligence pattern classification;seg ssc synthetic examples generation self labeled semisupervised classification self learning process supervised models labeled samples classification performance self labeled method synthetic labeled data oversampling technique positioning adjustment model labeled data distribution;full text;synthetic examples co training self labeled methods semi supervised classification;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Self-labeled techniques are semi-supervised classification methods that address the shortage of labeled examples via a self-learning process based on supervised models. They progressively classify unlabeled data and use them to modify the hypothesis learned from labeled samples. Most relevant proposals are currently inspired by boosting schemes to iteratively enlarge the labeled set. Despite their effectiveness, these methods are constrained by the number of labeled examples and their distribution, which in many cases is sparse and scattered. The aim of this paper is to design a framework, named synthetic examples generation for self-labeled semi-supervised classification, to improve the classification performance of any given self-labeled method by using synthetic labeled data. These are generated via an oversampling technique and a positioning adjustment model that use both labeled and unlabeled examples as reference. Next, these examples are incorporated in the main stages of the self-labeling process. The principal aspects of the proposed framework are: 1) introducing diversity to the multiple classifiers used by using more (new) labeled data; 2) fulfilling labeled data distribution with the aid of unlabeled data; and 3) being applicable to any kind of self-labeled method. In our empirical studies, we have applied this scheme to four recent self-labeled methods, testing their capabilities with a large number of data sets. We show that this framework significantly improves the classification capabilities of self-labeled techniques.	algorithm;boosting (machine learning);computer assisted diagnosis;email filtering;experiment;inspiration function;machine learning;name;oversampling;sql server compact;self-similarity;semi-supervised learning;semiconductor industry;sparse matrix;supervised learning;synthetic intelligence;tracer	Isaac Triguero;Salvador García;Francisco Herrera	2015	IEEE Transactions on Cybernetics	10.1109/TCYB.2014.2332003	semi-supervised learning;text mining;prediction;cybernetics;manifold;computer science;artificial intelligence;data science;machine learning;data mining;reliability;prototype;statistics	ML	16.322945186676336	-41.65198864019625	66628
80c26bc902ef216f761fd55232aac569147166ad	ensembles of artmap-based neural networks: an experimental study	artmap based neural networks;complex system;ensemble systems;feature selection;multi layer perceptron;neural network	ARTMAP-based models are neural networks which use a match-based learning procedure. The main advantage of ARTMAP-based models over error-based models, such as Multi-Layer Perceptron, is the learning time, which is considered as significantly fast. This feature is extremely important in complex systems that require the use of several models, such as ensembles or committees, since they produce robust and fast classifiers. Subsequently, some extensions of the ARTMAP model have been proposed, such as: ARTMAP-IC, RePART, among others. Aiming to add an extra contribution to ARTMAP context, this paper presents an analysis of ARTMAP-based models in ensemble systems. As a result of this analysis, two main goals are aimed, which are: to analyze the influence of the RePART model in ensemble systems and to detect any relation between diversity and accuracy in ensemble systems in order to use this relation in the design of these systems.	artificial neural network;complex systems;experiment;perceptron;statistical classification	Anne M. P. Canuto;Araken M. Santos;Rogério R. Vargas	2009	Applied Intelligence	10.1007/s10489-009-0199-2	computer science;artificial intelligence;machine learning;pattern recognition;multilayer perceptron;feature selection;artificial neural network	AI	12.260794373236617	-40.559522198668354	67074
21a9acd956f1b33a2c79df75f8b4c24d71b3b695	on random weights and unsupervised feature learning		Recently two anomalous results in the literature have shown that certain feature learning architectures can perform very well on object recognition tasks, without training. In this paper we pose the question, why do random weights sometimes do so well? Our answer is that certain convolutional pooling architectures can be inherently frequency selective and translation invariant, even with random weights. Based on this we demonstrate the viability of extremely fast architecture search by using random weights to evaluate candidate architectures, thereby sidestepping the time-consuming learning process. We then show that a surprising fraction of the performance of certain state-of-the-art methods can be attributed to the architecture alone.	convolutional neural network;feature learning;outline of object recognition;random graph	Andrew M. Saxe;Pang Wei Koh;Zhenghao Chen;Maneesh Bhand;Bipin Suresh;Andrew Y. Ng	2011			semi-supervised learning;unsupervised learning;machine learning;pattern recognition;competitive learning	ML	21.942562744041684	-51.04662224626021	67139
01b0824507af1db512be72aede52000447ae641c	differential evolution-based parameters optimisation and feature selection for support vector machine	parameter optimisation;differential evolution;support vector machines;high dimensional classification;svm;feature selection	This paper addresses the problem of SVM classification optimisation. For this purpose, the authors propose an SVM classification system based on differential evolution (DE) to improve the generalisation performance of the SVM classifier. In the classification system, a method of simultaneous parameters optimisation and feature selection for support vector machine is put forward. The experiments are conducted on the basis of benchmark dataset. The obtained results clearly confirm the superiority of the DE-SVM-FS approach compared to default SVM classifier and DE-SVM algorithm; this suggests that further substantial improvements in terms of classification accuracy can be achieved by the proposed DE-SVM-FS classification system.	differential evolution;feature selection;mathematical optimization;support vector machine	Jun Li;Lixin Ding;Bo Li	2016	IJCSE	10.1504/IJCSE.2016.10001039	support vector machine;computer science;machine learning;linear classifier;pattern recognition;data mining;mathematics;feature selection;ranking svm;structured support vector machine	ML	11.92653030299084	-42.41195582508423	67168
2ee8ccf6f730926e46eba237163dc4c4a13d3d43	activity recognition based on a multi-sensor meta-classifier	aggregation;sensor network;human behavior;meta classifier;decision fusion;weighted decision;activity recognition	Ensuring ubiquity, robustness and continuity of monitoring is of key importance in activity recognition. To that end, multiple sensor configurations and fusion techniques are ever more used. In this paper we present a multi-sensor meta-classifier that aggregates the knowledge of several sensor-based decision entities to provide a unique and reliable activity classification. This model introduces a new weighting scheme which improves the rating of the impact that each entity has on the decision fusion process. Sensitivity and specificity are particularly considered as insertion and rejection weighting metrics instead of the overall accuracy classification performance proposed in a previous work. For the sake of comparison, both new and previous weighting models together with feature fusion models are tested on an extensive activity recognition benchmark dataset. The results demonstrate that the new weighting scheme enhances the decision aggregation thus leading to an improved recognition system.	activity recognition;benchmark (computing);concept drift;entity;multi-source;rejection sampling;robustness (computer science);scott continuity;sensitivity and specificity;sensor	Oresti Baños;Miguel Damas;Héctor Pomares;Ignacio Rojas	2013		10.1007/978-3-642-38682-4_24	wireless sensor network;computer science;machine learning;pattern recognition;data mining;human behavior;activity recognition	Vision	12.780709861947695	-43.6278063350693	67310
99a767c145fb8d9a0a4d237d09a5270953b152fe	comparing multi-class approaches for motor imagery using renyi entropy		One of the main problems that face Motor Imagery-based system is addressing multi-class problem. Various approaches have been used to tackle this problem. Most of these approaches tend to divide multi-class problem into binary sub problems. This study aims to address the multi-class problem by comparing five multi-class approaches; One-vs-One (OVO), One-vs-Rest (OVR), Divide u0026 Conquer (DC), Binary Hierarchy (BH), and Multi-class approaches. Renyi entropy was examined for feature extraction. Three linear classifiers were used to implement these five-approaches: Support Vector Machine (SVM), Multinomial Logistic Regression (MLR) and Linear Discriminant Analysis (LDA). These approaches were compared according to their performance and time consumption. The comparative results show that, Renyi entropy demonstrated its robustness not only as a feature extraction technique but also as a powerful dimension reduction technique, for multi-class problem. In addition, LDA proved to be the best classifier for almost all approaches with minimum execution time.	rényi entropy	Sahar Selim;Manal M. Tantawi;Howida A. Shedeed;Amr Badr	2018		10.1007/978-3-319-99010-1_12	support vector machine;feature extraction;multinomial logistic regression;robustness (computer science);dimensionality reduction;binary number;linear discriminant analysis;rényi entropy;artificial intelligence;mathematics;pattern recognition	ML	11.853287908032405	-42.54644755512154	67347
9880064cc9ceef4cfae4a8662e460583aeed4c9b	unsupervised feature selection with structured graph optimization	unsupervised feature selection;embedded method;spectral analysis	Since amounts of unlabelled and high-dimensional data needed to be processed, unsupervised feature selection has become an important and challenging problem in machine learning. Conventional embedded unsupervised methods always need to construct the similarity matrix, which makes the selected features highly depend on the learned structure. However real world data always contain lots of noise samples and features that make the similarity matrix obtained by original data can't be fully relied. We propose an unsupervised feature selection approach which performs feature selection and local structure learning simultaneously, the similarity matrix thus can be determined adaptively. Moreover, we constrain the similarity matrix to make it contain more accurate information of data structure, thus the proposed approach can select more valuable features. An efficient and simple algorithm is derived to optimize the problem. Experiments on various benchmark data sets, including handwritten digit data, face image data and biomedical data, validate the effectiveness of the proposed approach.	feature selection;mathematical optimization	Feiping Nie;Wei Zhu;Xuelong Li	2016			unsupervised learning;computer science;machine learning;pattern recognition;data mining	AI	22.02515408135975	-42.06606902056733	67445
5cd74e293f73103fab4919d6ff9de870a1eac52e	feature fusion using locally linear embedding for classification	lle based classification locally linear embedding method pattern classification feature extraction data dimensionality reduction kernel based feature fusion method optimization problem;kernel;raisonnement base sur cas;razonamiento fundado sobre caso;locally linear embedding;sensor fusion feature extraction pattern classification;supervised learning;analisis datos;methode noyau;dimension reduction;fuses;efficient algorithm;feature extraction kernel sun vectors bioinformatics data mining design optimization design methodology algorithm design and analysis fuses;supervised learning dimension reduction feature fusion locally linear embedding;kernel based feature fusion method;data mining;feature space;classification;locally linear embedding method;design optimization;lle based classification;algorithms classification decision support techniques handwriting humans linear models neural networks computer signal processing computer assisted;reduction dimension;optimization problem;data analysis;vectors;mathematical programming;feature extraction;metodo nucleo;sun;pattern classification;reduccion dimension;analyse donnee;kernel method;feature fusion;apprentissage supervise;sensor fusion;case based reasoning;aprendizaje supervisado;programmation mathematique;programacion matematica;clasificacion;algorithm design and analysis;local linear embedding;bioinformatics;design methodology;data dimensionality reduction	In most complex classification problems, many types of features have been captured or extracted. Feature fusion is used to combine features for better classification and to reduce data dimensionality. Kernel-based feature fusion methods are very effective for classification, but they do not reduce data dimensionality. In this brief, we propose an effective feature fusion method using locally linear embedding (LLE). The proposed method overcomes the limitations of LLE, which could not handle different types of features and is inefficient for classification. We propose an efficient algorithm to solve the optimization problem in obtaining weights of different features, and design an efficient method for LLE-based classification. In comparison to other kernel-based feature fusion methods, the proposed method fuses features to a significantly lower dimensional feature space with the same discriminant power. We have conducted experiments to demonstrate the effectiveness of the proposed feature fusion method.	algorithm;arc diagram;concatenation;digit structure;discriminant;embedding;experiment;extraction;feature vector;kernel (operating system);linear iga bullous dermatosis;mathematical optimization;nonlinear dimensionality reduction;optimization problem;weight	Bing-Yu Sun;Xiaoming Zhang;Jiuyong Li;Xue-Min Mao	2010	IEEE Transactions on Neural Networks	10.1109/TNN.2009.2036363	fuse;optimization problem;algorithm design;case-based reasoning;kernel method;kernel;multidisciplinary design optimization;feature vector;design methods;feature extraction;biological classification;computer science;machine learning;linear classifier;pattern recognition;data mining;mathematics;sensor fusion;supervised learning;data analysis;k-nearest neighbors algorithm;feature;dimensionality reduction	ML	23.668022734559354	-40.08049517771689	67542
8a7374b98a9d94b8c01e996e72340f86a4327869	comparing naive bayes, decision trees, and svm with auc and accuracy	learning algorithm;decision tree;performance evaluation;support vector machines;bayes methods;naive bayes;learning artificial intelligence decision trees support vector machines sensitivity analysis data mining bayes methods performance evaluation;data mining;receiver operating characteristic curve;decision trees support vector machines data mining machine learning algorithms accuracy machine learning support vector machine classification computer science area measurement guidelines;data mining application;sensitivity analysis;data mining algorithm;prediction accuracy;support vector machine accuracy prediction data mining algorithm roc receiver operating characteristics performance evaluation naive bayes method decision trees svm;support vector machine;learning artificial intelligence;decision trees	Predictive accuracy has often been used as the main and often only evaluation criterion for the predictive performance of classification or data mining algorithms. In recent years, the area under the ROC (Receiver Operating Characteristics) curve, or simply AUC, has been proposed as an alternative single-number measure for evaluating performance of learning algorithms. In our previous work, we proved that AUC is, in general, a better measure (defined precisely) than accuracy. Many popular data mining algorithms should then be re-evaluated in terms of AUC. For example, it is well accepted that Naive Bayes and decision trees are very similar in accuracy. How do they compare in AUC? Also, how does the recently developed SVM (Support Vector Machine) compare to traditional learning algorithms in accuracy and AUC? We will answer these questions in this paper. Our conclusions will provide important guidelines in data mining applications on real-world datasets.	c4.5 algorithm;code;data mining;decision tree learning;experiment;machine learning;naive bayes classifier;support vector machine;zhi-li zhang	Jin Huang;Jingjing Lu;Charles X. Ling	2003		10.1109/ICDM.2003.1250975	support vector machine;computer science;machine learning;decision tree;pattern recognition;data mining	ML	11.333053692803796	-38.36414241952461	67670
c725155cdf6430a59b961b09e583d3e12a78ad7c	hybrid k-nearest neighbor classifier	imbalance problem;supervised learning classification ensemble learning machine learning nearest neighbor classifier;supervised learning;ensemble learning;training noise measurement computational efficiency euclidean distance noise remote sensing;training;knowledge extraction;euclidean distance;classification;noise measurement;high dimensional space;pattern classification knowledge acquisition learning artificial intelligence;noise problem;query sample;machine learning;knowledge acquisition;remote sensing;rs hbknn hybrid k nearest neighbor classifier knn classification special datasets imbalance problem noise problem query sample high dimensional space knowledge extraction evolutionary learning dataset repository;pattern classification;rs hbknn;learning artificial intelligence;special datasets;nearest neighbor classifier;computational efficiency;evolutionary learning dataset repository;hybrid k nearest neighbor classifier;knn classification;noise	Conventional k -nearest neighbor (KNN) classification approaches have several limitations when dealing with some problems caused by the special datasets, such as the sparse problem, the imbalance problem, and the noise problem. In this paper, we first perform a brief survey on the recent progress of the KNN classification approaches. Then, the hybrid KNN (HBKNN) classification approach, which takes into account the local and global information of the query sample, is designed to address the problems raised from the special datasets. In the following, the random subspace ensemble framework based on HBKNN (RS-HBKNN) classifier is proposed to perform classification on the datasets with noisy attributes in the high-dimensional space. Finally, the nonparametric tests are proposed to be adopted to compare the proposed method with other classification approaches over multiple datasets. The experiments on the real-world datasets from the Knowledge Extraction based on Evolutionary Learning dataset repository demonstrate that RS-HBKNN works well on real datasets, and outperforms most of the state-of-the-art classification approaches.	disease response domain;entity name part qualifier - adopted;equilibration disorder;experiment;k-nearest neighbors algorithm;nearest neighbor search;nearest neighbour algorithm;population parameter;question (inquiry);reed–solomon error correction;silo (dataset);sparse matrix;supernumerary mandibular right primary canine;two-hybrid system techniques	Zhiwen Yu;Hantao Chen;Jiming Liu;Jane You;Hareton K. N. Leung;Guoqiang Han	2016	IEEE transactions on cybernetics	10.1109/TCYB.2015.2443857	biological classification;computer science;noise measurement;noise;machine learning;pattern recognition;data mining;euclidean distance;ensemble learning;knowledge extraction;supervised learning	ML	13.558660086570145	-42.38016279869369	67745
157a9bcdf9f6f84a5c97ee2be7e5f2c02b266f63	a study on the influence of shape in classifying small spectral data sets	object representation;small sample size;spectral data;classification;dissimilarity representation	Classification of spectral data has raised a growing interest in may research areas. However, this type of data usually suffers from the curse of dimensionality. This causes most statistical methods and/or classifiers to not perform well. A recently proposed alternative which can help avoiding this problem is the Dissimilarity Representation, in which objects are represented by their dissimilarities to representative objects of each class. However, this approach depends on the selection of a suitable dissimilarity measure. For spectra, the incorporation of information on their shape, can be significant for a good discrimination. In this paper, we make a study on the benefit of using a measure which takes shape of spectra into account. We show that the shape-based measure not only leads to better classification results, but that a certain number of objects is enough to achieve it. The experiments are conducted on three onedimensional data sets and a two-dimensional one.	curse of dimensionality;experiment	Diana Porro-Muñoz;Robert P. W. Duin;Isneri Talavera-Bustamante;Mauricio Orozco-Alzate	2011		10.1007/978-3-642-24471-1_22	machine learning;pattern recognition;data mining;mathematics	ML	11.43124899443381	-44.902858991785564	67763
7a5d9f8ba6dfee6565b2ec40afee0c75400bce08	aligning time series with genetically tuned dynamic time warping algorithm	euclidean distance;time series;genetics;genetic algorithm;classification accuracy;dynamic time warping;similarity measure;lower bound	It is well known that Dynamic Time Warping (DTW) is superior to Euclidean distance as a similarity measure in time series  analyses. Use of DTW with the recently introduced warping window constraints and lower bounding measures has significantly  increased the accuracy of time series classification while reducing the computational expense required. The warping window  technique learns arbitrary constraints on the warping path while performing time series alignment. This work utilizes genetic  algorithms to find the optimal warping window constraints which provide a better classification accuracy. Performance of the  proposed methodology has been investigated on two problems from diverse domains with favorable results.  	algorithm;dynamic time warping;time series	Pankaj Kumar;Ankur Gupta;Vaidyanathan K. Jayaraman;Bhaskar D. Kulkarni	2008		10.1007/978-3-540-72960-0_12	mathematical optimization;machine learning;dynamic time warping;pattern recognition;mathematics	DB	19.52240586978379	-40.21956753103643	67772
b593ed8e0fe0596c835175a221d4ec31973ba4e5	an aggregate ensemble for mining concept drifting data streams with noise	concept drift;learning algorithm;classifier ensemble;ensemble method;ensemble learning;noisy data streams;noisy data;data stream;journal article;concept drifting;model averaging;prediction accuracy	"""Recent years have witnessed a large body of research work on mining concept drifting data streams, where a primary assumption is that the up-to-date data chunk and the yet-to-come data chunk share identical distributions, so classifiers with good performance on the up-to-date chunk would also have a good prediction accuracy on the yet-to-come data chunk. This """"stationary assumption"""", however, does not capture the concept drifting reality in data streams. More recently, a """"learnable assumption"""" has been proposed and allows the distribution of each data chunk to evolve randomly. Although this assumption is capable of describing the concept drifting in data streams, it is still inadequate to represent real-world data streams which usually suffer from noisy data as well as the drifting concepts. In this paper, we propose a Realistic Assumption which asserts that the difficulties of mining data streams are mainly caused by both concept drifting and noisy data chunks. Consequently, we present a new Aggregate Ensemble (AE) framework, which trains base classifiers using different learning algorithms on different data chunks. All the base classifiers are then combined to form a classifier ensemble through model averaging. Experimental results on synthetic and real-world data show that AE is superior to other ensemble methods under our new realistic assumption for noisy data streams."""	aggregate function;noise (signal processing)	Peng Zhang;Xingquan Zhu;Yong Shi;Xindong Wu	2009		10.1007/978-3-642-01307-2_109	computer science;concept drift;machine learning;pattern recognition;data mining;ensemble learning	ML	14.770489168955146	-38.21269067980753	68069
775a39d6bcd0d21d64db40b8f720bb87ee95146f	local supervised learning through space partitioning		We develop a novel approach for supervised learning based on adaptively partitioning the feature space into different regions and learni ng local region-specific classifiers. We formulate an empirical risk minimization pr oblem that incorporates both partitioning and classification in to a single glo ba objective. We show that space partitioning can be equivalently reformulated a s a supervised learning problem and consequently any discriminative learning meth od can be utilized in conjunction with our approach. Nevertheless, we consider l ocal y linear schemes by learning linear partitions and linear region classifiers . Locally linear schemes can not only approximate complex decision boundaries and en sur low training error but also provide tight control on over-fitting and gene ralization error. We train locally linear classifiers by using LDA, logistic regr ssion and perceptrons, and so our scheme is scalable to large data sizes and high-dim ensions. We present experimental results demonstrating improved performance over state of the art classification techniques on benchmark datasets. We also sh ow improved robustness to label noise.	approximation algorithm;benchmark (computing);empirical risk minimization;feature vector;openclipart;overfitting;perceptron;scalability;space partitioning;supervised learning	Joseph Wang;Venkatesh Saligrama	2012			semi-supervised learning;empirical risk minimization;computer science;machine learning;linear classifier;pattern recognition;data mining;supervised learning;generalization error	ML	19.740422678092248	-40.733920692758566	68174
ac8744e5d994a8ac48c075c587059b1cb518c6e2	idk cascades: fast deep learning by learning not to overthink		Advances in deep learning have led to substantial increases in prediction accuracy but have been accompanied by increases in the cost of rendering predictions. We conjecture that for a majority of real-world inputs, the recent advances in deep learning have created models that effectively “over-think” on simple inputs. In this paper we revisit the classic question of building model cascades that primarily leverage class asymmetry to reduce cost. We introduce the “I Don’t Know” (IDK) prediction cascades framework, a general framework to systematically compose a set of pre-trained models to accelerate inference without a loss in prediction accuracy. We propose two search based methods for constructing cascades as well as a new cost-aware objective within this framework. The proposed IDK cascade framework can be easily adopted in the existing model serving systems without additional model retraining. We evaluate the proposed techniques on a range of benchmarks to demonstrate the effectiveness of the proposed framework.	amazon web services;analysis paralysis;benchmark (computing);cascade framework;computation;consistency model;deep learning;empirical risk minimization;flops;ibm notes;learning classifier system;loss function;multiclass classification;optimization problem;overhead (computing);reduced cost;web service	Xin Wang;Yujia Luo;Daniel Crankshaw;Alexey Tumanov;Joseph Gonzalez	2018			machine learning;leverage (finance);computer science;rendering (computer graphics);deep learning;artificial intelligence;inference;building model	ML	23.311796206495835	-51.91164274877854	68290
9e2e144872f5f26b86e9a372ec7d87d10e545c34	incremental fuzzy cluster ensemble learning based on rough set theory		To deal with the uncertainty, vagueness and overlapping distribution within the data sets, a novel incremental fuzzy cluster ensemble method based on rough set theory (IFCERS) is proposed by the idea of combining clustering analysis task with classification techniques. Firstly, on the basis of soft clustering results, the positive region, boundary region and negative region of clustering ensemble are obtained by applying the construction of rough approximation in rough set theory, and then a group structure within data points of positive region is obtained by adopting a fuzzy cluster ensemble method. Secondly, by combining with the supervised ensemble learning method, e.g., random forests, the obtained group structure is used to construct the random forests classifier to classify the data points in boundary region. Finally, all the acquired group structure is used to train the random forests classifier to classify the data points of negative region. Experimental evaluations on UCI machine learning repository datasets verify the effectiveness of the proposed method. It is also shown that the quality of the final solution has a weak correlation with the ensemble size, the parameter setting on the rough approximations construction is appropriate, and the proposed method is robust towards the diversity from hard clustering members.	approximation;cluster analysis;data point;ensemble learning;machine learning;random forest;rough set;set theory;vagueness	Jie Hu;Tianrui Li;Chuan Luo;Hamido Fujita;Yan Yang	2017	Knowl.-Based Syst.	10.1016/j.knosys.2017.06.020	fuzzy clustering;fuzzy logic;machine learning;data point;data mining;artificial intelligence;computer science;cluster analysis;random forest;ensemble learning;rough set;data set;pattern recognition	AI	10.400628993192463	-39.89098607930555	68344
63cccbbb0cc53c643a6e71192a4bf25aec7139a6	a nu-twin support vector machine (nu-tsvm) classifier and its geometric algorithms	support vector;geometric interpretation;twin support vector machine;theoretical analysis;computational complexity;pattern recognition;geometric algorithm;support vector machine;classification accuracy;convex hull;probabilistic speed up strategy	In this paper, a @n-twin support vector machine (@n-TSVM) is presented, improving upon the recently proposed twin support vector machine (TSVM). This @n-TSVM introduces a pair of parameters (@n) to control the bounds of the fractions of the support vectors and the error margins. The theoretical analysis shows that this @n-TSVM can be interpreted as a pair of minimum generalized Mahalanobis-norm problems on two reduced convex hulls (RCHs). Based on the well-known Gilbert's algorithm, a geometric algorithm for TSVM (GA-TSVM) and its probabilistic speed-up version, named PGA-TSVM, are presented. Computational results on several synthetic as well as benchmark datasets demonstrate the significant advantages of the proposed algorithms in terms of both computation complexity and classification accuracy.	algorithm;support vector machine	Xinjun Peng	2010	Inf. Sci.	10.1016/j.ins.2010.06.039	support vector machine;computer science;machine learning;pattern recognition;data mining;mathematics;relevance vector machine;structured support vector machine	DB	21.20480781613411	-38.946861523425746	68444
6872c464cfad92e7fb6a305ec35f1623b74bbe3c	active learning for multivariate time series classification with positive unlabeled data	time series learning artificial intelligence pattern classification;uncertainty;active learning;positive and unlabeled data;training;uncertainty time series analysis labeling training training data classification algorithms learning systems;active learning approach multivariate time series classification supervised learning algorithm labeled training data semisupervised learning learning method univariate time series data mts classification sample selection strategy manual labeling;learning systems;training data;multivariate time series;time series analysis;positive and unlabeled data multivariate time series active learning;classification algorithms;labeling	Traditional time series classification problem with supervised learning algorithm needs a large set of labeled training data. In reality, the number of labeled data is often smaller and there is huge number of unlabeled data. However, manually labeling these unlabeled examples is time-consuming and expensive, and sometimes it is even impossible. Although some semi-supervised and active learning methods were proposed to handle univariate time series data, few work have touched positive and unlabeled data for multivariate time series (MTS) classification due to the data being more complex. In this paper we focus on active learning for multivariate time series classification with positive unlabeled data. First, we propose a sample selection strategy to find the most informative unlabeled examples for manual labeling. Second, we introduce two active learning approaches to obtain a high-confident training dataset for classification. Experiments on real datasets demonstrate the validity of our proposed approaches.	active learning (machine learning);algorithm;information;semi-supervised learning;semiconductor industry;supervised learning;time series	Guoliang He;Yong Duan;Yifei Li;Tieyun Qian;Jinrong He;Xiangyang Jia	2015	2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)	10.1109/ICTAI.2015.38	semi-supervised learning;statistical classification;unsupervised learning;training set;labeling theory;uncertainty;computer science;machine learning;time series;pattern recognition;data mining;active learning;statistics	ML	14.820807547598028	-38.818438742112505	68563
62d2be041702fe9fb9828f87ce168ad6dd09b026	robust structural metric learning		Metric learning algorithms produce a linear transformation of data which is optimized for a prediction task, such as nearest-neighbor classification or ranking. However, when the input data contains a large portion of noninformative features, existing methods fail to identify the relevant features, and performance degrades accordingly. In this paper, we present an efficient and robust structural metric learning algorithm which enforces group sparsity on the learned transformation, while optimizing for structured ranking output prediction. Experiments on synthetic and real datasets demonstrate that the proposed method outperforms previous methods in both highand low-noise settings.	algorithm;expanded memory;experiment;ibm notes;input/output;learning to rank;machine learning;sparse matrix;synthetic intelligence	Daryl Lim;Gert R. G. Lanckriet;Brian McFee	2013			machine learning;pattern recognition;artificial intelligence;linear map;computer science;ranking	AI	22.191017324856983	-43.25254343419863	68574
24680371fab0cf4875c36909dfd6bcf3781b495b	discriminant sparse and collaborative preserving embedding for bearing fault diagnosis		Abstract Background noise and small sample size would usually add to the difficulty of extracting most effective information for bearing fault diagnosis in rotating electrical machines. To address this issue, a novel supervised dimensionality reduction algorithm referred to as discriminant sparse and collaborative preserving embedding (DSCPE) is proposed in this paper for bearing defect classification, which utilizes collaborative representation (CR) for an intrinsic graph and sparse representation (SR) for a penalty graph. In the intrinsic graph, CR helps to involve more bases of the dictionary composed by the same labeled samples and generate less sparse solutions; meanwhile in the penalty graph, SR would avoid the mix-class interference when using the dictionary constituted by different labeled samples. DSCPE aims to seek the optimal projection directions that could minimize the intraclass compactness and maximize the interclass separability. After dimension reduction and data projection by DSCPE, the 1-Nearest Neighbor method is applied to classify the bearing defects. The experimental results demonstrate that DSCPE possesses more effective and robust classification performance than other compared algorithms when dealing with small-sample-sized problem and interfered vibration signals.	discriminant;sparse matrix	Yue Ma;Xiaohua Wu	2018	Neurocomputing	10.1016/j.neucom.2018.06.028	machine learning;artificial intelligence;dimensionality reduction;sample size determination;pattern recognition;embedding;sparse approximation;mathematics;compact space;discriminant;bearing (mechanical);graph	AI	24.351279098363786	-42.50460396385465	68607
ba3890de5b387d903f4b7d067f14c5e4e49c2d06	novel ensembling methods for dermatological image classification		In this paper we investigate multiple novel techniques of ensembling deep neural networks with different hyperparameters and differently preprocessed data for skin lesion classification. To this end, we have utilized the datasets made public by two of the most recent “Skin Lesion Analysis Towards Melanoma Detection” grand challenges (ISIC2017 and ISIC2018). The datasets provided by these two challenges differ in multiple aspects: the size, quality and origin of the images, the number of possible target lesion categories and the metrics used for ranking. We will show that ensembling can be surprisingly useful not only for combining different machine learning models but also for combining different hyperparameter choices of these models and multiple strategies for preprocessing the input data at the task of skin lesion detection, outperforming more mainstream methods like hyperparameter optimization and test-time augmentation both in terms of speed and accuracy.		Tamás Nyíri;Attila Kiss	2018		10.1007/978-3-030-04070-3_34	hyperparameter;artificial neural network;hyperparameter optimization;contextual image classification;ensemble learning;machine learning;preprocessor;ranking;grand challenges;computer science;artificial intelligence	Vision	21.423020771741342	-51.12239367609851	68674
baa94190e367fe1ce1faccab81f8aa6d3919656d	a unified optimization based learning method for image retrieval	optimization methods learning systems image retrieval information retrieval image databases asia feedback content based retrieval support vector machines support vector machine classification;query processing optimization based learning method image retrieval graph model online relevance feedback offline semantic information iterative form general purpose image database;image database;visual databases image retrieval relevance feedback iterative methods learning artificial intelligence;optimization problem;iterative methods;semantic information;learning methods;global optimization;graph model;learning artificial intelligence;point of view;relevance feedback;visual databases;image retrieval	In this paper, an optimization based learning method is proposed for image retrieval from graph model point of view. Firstly, image retrieval is formulated as a regularized optimization problem, which simultaneously considers the constraints from low-level feature, online relevance feedback and offline semantic information. Then, the global optimal solution is developed in both closed form and iterative form, providing that the latter converges to the former. The proposed method is unified in the senses that 1) it makes use of the information from various aspects in a global optimization manner so that the retrieval performance might be maximally improved; 2) it provides a natural way to support two typical query scenarios in image retrieval. The proposed method has a solid mathematical ground. Systematic experimental results on a general-purpose image database demonstrate that it achieves significant improvements over existing methods.	general-purpose modeling;global optimization;high- and low-level;image retrieval;iterative method;mathematical optimization;online and offline;optimization problem;relevance feedback	Hanghang Tong;Jingrui He;Mingjing Li;Wei-Ying Ma;Changshui Zhang;HongJiang Zhang	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)	10.1109/CVPR.2005.54	optimization problem;computer vision;visual word;image retrieval;computer science;machine learning;data mining;mathematics;iterative method;automatic image annotation;vector space model;information retrieval;global optimization;human–computer information retrieval;divergence-from-randomness model	Vision	18.20026150608309	-43.40405835803208	68746
27bcca35d0a150a182375673fd2454dc11f6e24f	an empirical analysis of feature engineering for predictive modeling		Machine learning models, such as neural networks, decision trees, random forests and gradient boosting machines accept a feature vector and provide a prediction. These models learn in a supervised fashion where a set of feature vectors with expected output is provided. It is very common practice to engineer new features from the provided feature set. Such engineered features will either augment, or replace portions of the existing feature vector. These engineered features are essentially calculated fields, based on the values of the other features. Engineering such features is primarily a manual, time-consuming task. Additionally, each type of model will respond differently to different types of engineered features. This paper reports on empirical research to demonstrate what types of engineered features are best suited to which machine learning model type. This is accomplished by generating several datasets that are designed to benefit from a particular type of engineered feature. The experiment demonstrates to what degree the machine learning model is capable of synthesizing the needed feature on its own. If a model is capable of synthesizing an engineered feature, it is not necessary to provide that feature. The research demonstrated that the studied models do indeed perform differently with various types of engineered features.	artificial neural network;decision tree;feature engineering;feature vector;gradient boosting;machine learning;predictive modelling;random forest;supervised learning	Jeff Heaton	2016	SoutheastCon 2016		support vector machine;feature learning;feature vector;feature;feature extraction;computer science;linear regression;machine learning;linear classifier;pattern recognition;mathematical model;data mining;mathematics;deep learning;k-nearest neighbors algorithm;feature;feature model;dimensionality reduction	ML	18.353688182164884	-49.1399049393752	68811
0c1ff37974c69c36d6d657c310188675b80b49fb	a mixture of views network with applications to the classification of breast microcalcifications		In this paper we examine data fusion methods for multiview data classification. We present a decision concept which explicitly takes into account the input multi-view structure, where for each case there is a different subset of relevant views. This data fusion concept, which we dub Mixture of Views, is implemented by a special purpose neural network architecture. It is demonstrated on the task of classifying breast microcalcifications as benign or malignant based on CC and MLO mammography views. The single view decisions are combined by a datadriven decision, according to the relevance of each view in a given case, into a global decision. The method is evaluated on a large multi-view dataset extracted from the standardized digital database for screening mammography (DDSM). The experimental results show that our method outperforms previously suggested fusion methods.	artificial neural network;jamie wilkinson;network architecture;relevance	Yaniv Shachor;Hayit Greenspan;Jacob Goldberger	2018	CoRR		artificial intelligence;data classification;machine learning;computer science;pattern recognition;artificial neural network;mammography;sensor fusion;screening mammography;breast microcalcifications	Vision	17.108239875995448	-43.616612980477115	68967
fd67b9812fa4aef6c5dfb633df4406105cdb4e8f	zero-shot learning with generative latent prototype model		Zero-shot learning, which studies the problem of object classification for categories for which we have no training examples, is gaining increasing attention from community. Most existing ZSL methods exploit deterministic transfer learning via an in-between semantic embedding space. In this paper, we try to attack this problem from a generative probabilistic modelling perspective. We assume for any category, the observed representation, e.g. images or texts, is developed from a unique prototype in a latent space, in which the semantic relationship among prototypes is encoded via linear reconstruction. Taking advantage of this assumption, virtual instances of unseen classes can be generated from the corresponding prototype, giving rise to a novel ZSL model which can alleviate the domain shift problem existing in the way of direct transfer learning. Extensive experiments on three benchmark datasets show our proposed model can achieve state-of-the-art results.	benchmark (computing);encode;experiment;probabilistic database;prototype;statistical model	Yanan Li;Donghui Wang	2017	CoRR		generative grammar;machine learning;artificial intelligence;mathematics	AI	24.549971768331645	-48.685447915058944	69088
bf7388027b8118dfce1052b38bb3a4b8dc2f3bb4	rankgan: a maximum margin ranking gan for generating faces		We present a new stage-wise learning paradigm for training generative adversarial networks (GANs). The goal of our work is to progressively strengthen the discriminator and thus, the generators, with each subsequent stage without changing the network architecture. We call this proposed method the RankGAN. We first propose a margin-based loss for the GAN discriminator. We then extend it to a margin-based ranking loss to train the multiple stages of RankGAN. We focus on face images from the CelebA dataset in our work and show visual as well as quantitative improvements in face generation and completion tasks over other GAN approaches, including WGAN and LSGAN.		Rahul Dey;Felix Juefei-Xu;Vishnu Naresh Boddeti;Marios Savvides	2018	CoRR			Vision	24.393815604709012	-50.10785762589948	69215
3596214b0bf4e9f34e8e2d4d92d4724b519eedeb	learning two-layer contractive encodings	unsupervised learning;yield encoders;restricted boltzmann machine;two-layer contractive encodings;two-layer encoder;deep architecture;stable feature;two-layer encoders qualitatively;feature hierarchy;previous work;proposed encoder	Unsupervised learning of feature hierarchies is often a good initialization for supervised training of deep architectures. In existing deep learning methods, these feature hierarchies are built layer by layer in a greedy fashion using auto-encoders or restricted Boltzmann machines. Both yield encoders, which compute linear projections followed by a smooth thresholding function. In this work, we demonstrate that these encoders fail to find stable features when the required computation is in the exclusive-or class. To overcome this limitation, we propose a two-layer encoder which is not restricted in the type of features it can learn. The proposed encoder can be regularized by an extension of previous work on contractive regularization. We demonstrate the advantages of two-layer encoders qualitatively, as well as on commonly used benchmark datasets.	adaptive equalizer;artificial neural network;benchmark (computing);character encoding;computation;contraction mapping;deep learning;encoder;exclusive or;greedy algorithm;level of detail;low-density parity-check code;manifold regularization;modulo operation;parity bit;restricted boltzmann machine;thresholding (image processing);unsupervised learning	Hannes Schulz;Sven Behnke	2012		10.1007/978-3-642-33269-2_78	mathematical optimization;artificial intelligence;theoretical computer science;machine learning;mathematics	ML	23.363366108604595	-50.47938902888893	69227
5fb1c9a155163a0ad5bcca61e180f2683ac83f21	learning from complementary labels		Collecting labeled data is costly and thus a critical bottleneck in real-world classification tasks. To mitigate this problem, we propose a novel setting, namely learning from complementary labels for multi-class classification. A complementary label specifies a class that a pattern does not belong to. Collecting complementary labels would be less laborious than collecting ordinary labels, since users do not have to carefully choose the correct class from a long list of candidate classes. However, complementary labels are less informative than ordinary labels and thus a suitable approach is needed to better learn from them. In this paper, we show that an unbiased estimator to the classification risk can be obtained only from complementarily labeled data, if a loss function satisfies a particular symmetric condition. We derive estimation error bounds for the proposed method and prove that the optimal parametric convergence rate is achieved. We further show that learning from complementary labels can be easily combined with learning from ordinary labels (i.e., ordinary supervised learning), providing a highly practical implementation of the proposed method. Finally, we experimentally demonstrate the usefulness of the proposed methods.	algorithm;bottleneck (engineering);experiment;grid north;information;loss function;machine learning;mathematical optimization;multiclass classification;multidimensional digital pre-distortion;rate of convergence;stochastic optimization;supervised learning	Takashi Ishida;Gang Niu;Masashi Sugiyama	2017			machine learning;supervised learning;computer science;rate of convergence;labeled data;artificial intelligence;bias of an estimator;parametric statistics;bottleneck	ML	18.049092195438508	-39.95026781027551	69273
1768adc713a6e91e6ffc2d79f119ad0ab538baed	a statistical framework for positive data clustering with feature selection: application to object detection	gid;object detection positive data feature selection clustering mixture models gid mml;feature extraction object detection vectors accuracy data models conferences clustering algorithms;clustering;feature selection;positive data;mixture models;mml;object detection	In this paper, we concern ourselves with the problem of simultaneous positive data clustering and feature selection. We propose a statistical framework based on finite mixture models of generalized inverted Dirichlet (GID) distributions. The GID offers a more practical and flexible alternative to the inverted Dirichlet which has a very restrictive covariance structure. For learning the parameters of the resulting mixture, we propose an approach based on minimum message length (MML) criterion. We use synthetic data and real data generated from a challenging application that concerns objects detection to demonstrate the feasibility and advantages of the proposed method.	cluster analysis;feature selection;group identifier;minimum message length;mixture model;object detection;synthetic data	Mohamed Al Mashrgy;Nizar Bouguila;Khalid Daoudi	2013	21st European Signal Processing Conference (EUSIPCO 2013)		computer science;machine learning;pattern recognition;data mining	ML	18.007967563487345	-42.78408706203433	69336
36c188aa28ca4e801ea6635a5cc38107cdbdde59	cluster reduction support vector machine for large-scale data set classification	pattern clustering;kernel;sample selection;cluster reduction support vector machine data mining;support vector machines;support vector machines data mining learning artificial intelligence pattern classification pattern clustering;training;cluster reduction;data mining;svm training;support vector machines support vector machine classification large scale systems training data quadratic programming data mining kernel mathematics machine learning conferences;training data;accuracy;distance measurement;large scale;machine learning;large scale data set classification;uci database;pattern classification;cluster reduction support vector machine;machine learning cluster reduction support vector machine large scale data set classification svm training uci database data mining sample selection;support vector machine;learning artificial intelligence;classification accuracy	Support vector machine (SVM) has been a promising method for data mining and machine learning in recent years. However, the training complexity of SVM is highly dependent on the size of a data set. A cluster support vector machines (C-SVM) method for large-scale data set classification is presented to accelerate the training speed. By calculating cluster mirror radius ratio and representative sample selection in each cluster, the original training data set can be reduced remarkably without losing the classification information. The new method can provide an SVM with high quality samples in lower time consuming. Experiments with random data and UCI databases show that the C-SVM retains the high quality of training data set and the classification accuracy in data mining.	data mining;database;display resolution;experiment;machine learning;randomness;statistical classification;support vector machine;test set	Guangxi Chen;Yan Cheng;Jian Xu	2008	2008 IEEE Pacific-Asia Workshop on Computational Intelligence and Industrial Application	10.1109/PACIIA.2008.43	support vector machine;test set;computer science;machine learning;pattern recognition;data mining;relevance vector machine;structured support vector machine	ML	13.226275901723207	-40.830194519142054	69430
fdeda50c0ad300ced7f93e8222b1e1b572555563	heuristic approach to solve feature selection problem		One of the successful methods in classification problems is feature selection. Feature selection algorithms; try to classify an instance with lower dimension, instead of huge number of required features, with higher and ac- ceptable accuracy. In fact an instance may contain useless features which might result to misclassification. An appropriate feature selection methods tries to in- crease the effect of significant features while ignores insignificant subset of fea- tures. In this work feature selection is formulated as an optimization problem and a novel feature selection procedure in order to achieve to a better classifica- tion results is proposed. Experiments over a standard benchmark demonstrate that applying harmony search in the context of feature selection is a feasible ap- proach and improves the classification results.	feature selection;heuristic	Rana Forsati;Alireza Moayedikia;Bahareh Safarkhani	2011		10.1007/978-3-642-22027-2_59	minimum redundancy feature selection;machine learning;pattern recognition;data mining;feature selection;feature	AI	10.886124332965455	-43.27707683598086	69489
bc25c8f7615a5ab7584e292eebffb2acedd6cbb5	to trust or not to trust a classifier		Knowing when a classifier’s prediction can be trusted is useful in many applications and critical for safely using AI. While the bulk of the effort in machine learning research has been towards improving classifier performance, understanding when a classifier’s predictions should and should not be trusted has received far less attention. The standard approach is to use the classifier’s discriminant or confidence score; however, we show there exists an alternative that is more effective in many situations. We propose a new score, called the trust score, which measures the agreement between the classifier and a modified nearest-neighbor classifier on the testing example. We show empirically that high (low) trust scores produce surprisingly high precision at identifying correctly (incorrectly) classified examples, consistently outperforming the classifier’s confidence score as well as many other baselines. Further, under some mild distributional assumptions, we show that if the trust score for an example is high (low), the classifier will likely agree (disagree) with the Bayes-optimal classifier. Our guarantees consist of non-asymptotic rates of statistical consistency under various nonparametric settings and build on recent developments in topological data analysis.	discriminant;machine learning;nearest neighbour algorithm;statistical classification;topological data analysis	Heinrich Jiang;Been Kim;Maya R. Gupta	2018			artificial intelligence;machine learning;computer science;classifier (linguistics);topological data analysis;existential quantification;nonparametric statistics	AI	16.33074666835538	-39.293350194847	69659
00369d28d3a7d8df73fdd2c2379d04915b15b9d3	a method to identify feature constraints based on feature selections mining	product line;association rule;feature selection;software product line	"""In this paper, we describe a novel method to identify constraints among features in a software product line, based on feature selections made in the past. Our approach takes feature selections of derived products as the input and extracts association rules between features such as """"Products that selected feature i also selected feature j."""" We evaluated our method by applying it to a product line at Hitachi and identified 21 new constraints among 123 optional features."""		Kentaro Yoshimura;Yoshitaka Atarashi;Takeshi Fukuda	2010		10.1007/978-3-642-15579-6_30	association rule learning;computer science;engineering;machine learning;pattern recognition;data mining;feature selection;feature;feature model	ML	11.957849226643388	-46.488682061433	69706
0f9244fc04ae247b84a603543746a73e4ff5f619	hellinger distance decision trees are robust and skew-insensitive	decision tree;imbalanced data;hellinger distance	Learning from imbalanced data is an important and common problem. Decision trees, supplemented with sampling techniques, have proven to be an effective way to address the imbalanced data problem. Despite their effectiveness, however, sampling methods add complexity and the need for parameter selection. To bypass these difficulties we propose a new decision tree technique called Hellinger Distance Decision Trees (HDDT) which uses Hellinger distance as the splitting criterion. We analytically and empirically demonstrate the strong skew insensitivity of Hellinger distance and its advantages over popular alternatives such as entropy (gain ratio). We apply a comprehensive empirical evaluation framework testing against commonly used sampling and ensemble methods, considering performance across 58 varied datasets. We demonstrate the superiority (using robust tests of statistical significance) of HDDT on imbalanced data, as well as its competitive performance on balanced datasets. We thereby arrive at the particularly practical conclusion that for imbalanced data it is sufficient to use Hellinger trees with bagging (BG) without any sampling methods. We provide all the datasets and software for this paper online ( http://www.nd.edu/~dial/hddt ).	balanced ternary;binary classification;bootstrap aggregating;c4.5 algorithm;decision tree learning;ensemble learning;entropy (information theory);experiment;f1 score;ibm notes;job control (unix);performance;sampling (signal processing);schmitt trigger;ti advanced scientific computer	David A. Cieslak;T. Ryan Hoens;Nitesh V. Chawla;W. Philip Kegelmeyer	2011	Data Mining and Knowledge Discovery	10.1007/s10618-011-0222-1	computer science;machine learning;decision tree;pattern recognition;data mining;mathematics;statistics;hellinger distance	ML	15.421285357732696	-41.297596128567605	69808
cafc0078d50ef58c9f9ebd882a2948cf7a1bc601	learning the optimal kernel for fisher discriminant analysis via second order cone programming	conic programming;second order cone programming;selection problem;programmation conique;problema seleccion;programmation semi definie;analisis estadistico;semidefinite programming;support vector machines;convex programming;methode noyau;kernel methods;methode point interieur;informacion fisher;convex optimization;programmation convexe;intelligence artificielle;kernel optimization;discriminant analysis;analyse discriminante;optimization problem;classification a vaste marge;fisher discriminant analysis;analisis discriminante;metodo punto interior;statistical analysis;second order cone program;machine learning;kernel fisher discriminant analysis;mathematical programming;metodo nucleo;analyse statistique;ensemble convexe;artificial intelligence;kernel method;inteligencia artificial;convex set;support vector machine;maquina ejemplo soporte;vector support machine;programacion semi definida;interior point method;fisher discriminant analysis kernel methods machine learning kernel optimization support vector machines convex optimization second order cone programming semidefinite programming;programmation mathematique;information fisher;programacion matematica;programacion conica;conjunto convexo;fisher information;semidefinite program;semi definite programming;programacion convexa;probleme selection	Kernel Fisher discriminant analysis (KFDA) is a popular classification technique which requires the user to predefine an appropriate kernel. Since the performance of KFDA depends on the choice of the kernel, the problem of kernel selection becomes very important. In this paper we treat the kernel selection problem as an optimization problem over the convex set of finitely many basic kernels, and formulate it as a second order cone programming (SOCP) problem. This formulation seems to be promising because the resulting SOCP can be efficiently solved by employing interior point methods. The efficacy of the optimal kernel, selected from a given convex set of basic kernels, is demonstrated on UCI machine learning benchmark datasets.	kernel (operating system);linear discriminant analysis;second-order cone programming	Reshma Khemchandani;Jayadeva;Suresh Chandra	2010	European Journal of Operational Research	10.1016/j.ejor.2009.09.020	support vector machine;kernel method;mathematical optimization;combinatorics;kernel fisher discriminant analysis;string kernel;convex optimization;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;computer science;machine learning;mathematics;tree kernel;variable kernel density estimation;polynomial kernel;semidefinite programming;kernel smoother	ML	22.933008452604323	-38.684615693819346	69850
16abea5536a53251ae4d13bed7a3aac70d03a1e0	guiding infogan with semi-supervision		In this paper we propose a new semi-supervised GAN architecture (ss-InfoGAN) for image synthesis that leverages information from few labels (as little as (0.22%), max. (10%) of the dataset) to learn semantically meaningful and controllable data representations where latent variables correspond to label categories. The architecture builds on Information Maximizing Generative Adversarial Networks (InfoGAN) and is shown to learn both continuous and categorical codes and achieves higher quality of synthetic samples compared to fully unsupervised settings. Furthermore, we show that using small amounts of labeled data speeds-up training convergence. The architecture maintains the ability to disentangle latent variables for which no labels are available. Finally, we contribute an information-theoretic reasoning on how introducing semi-supervision increases mutual information between synthetic and real data. Code related to this chapter is available at: https://github.com/spurra/ss-infogan.	cascading style sheets;code;generative adversarial networks;generative model;information theory;latent variable;mnist database;mutual information;rendering (computer graphics);semiconductor industry;synthetic intelligence	Adrian Spurr;Emre Aksan;Otmar Hilliges	2017		10.1007/978-3-319-71249-9_8	artificial intelligence;machine learning;labeled data;generative grammar;pattern recognition;mutual information;categorical variable;computer science;architecture;convergence (routing);latent variable	ML	24.222995485637654	-48.46286017463214	70087
cb2bf529b2734f2cd98599d13d1b65fb068c5c41	embedded clustering via robust orthogonal least square discriminant analysis		In this paper, a novel embedded clustering (EC) method is derived from the perspective of extending the supervised orthogonal least square discriminant analysis (OLSDA) method to the unsupervised case, which proves to be closely related to k-means. To achieve more statistical and structural properties, the robust learning of unsupervised OLSDA is investigated to further derive the unsupervised robust OLSDA (ROLSDA) problem. For the convenience of solving the proposed ROLSDA problem, re-weighted counterpart of ROLSDA is utilized with self-adaptive weight, such that the smaller weight would be assigned to the term with larger outliers automatically. Consequently, aforementioned EC method is proposed with not only the robust outliers but also the optimal weighted cluster centroids. Comparative experiments are presented to show the effectiveness of the EC method under the proposed ROLSDA problem.	cluster analysis;embedded system;experiment;k-means clustering;linear discriminant analysis	Rui Zhang;Feiping Nie;Xuelong Li	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952573	robustness (computer science);cluster analysis;pattern recognition;outlier;artificial intelligence;machine learning;computer science;centroid;linear discriminant analysis;linear programming;least squares;cure data clustering algorithm	Robotics	21.529359854786712	-41.1458950517378	70125
70b420850e16ec2afe42d5c0006742d9045b3e7f	if you can't beat them, join them: learning with noisy data	probabilistic learning;label noise	Vision capabilities have been significantly enhanced in recent years due to the availability of powerful computing hardware and sufficiently large and varied databases. However, the labelling of these image databases prior to training still involves considerable effort and is a roadblock for truly scalable learning. For instance, it has been shown that tag noise levels in Flickr images are as high as 80%. In an effort to exploit large images datasets therefore, extensive efforts have been invested to reduce the tag noise of the data by refining the image tags or by developing robust learning frameworks. In this work, we follow the latter approach, where we propose a multi-layer neural network-based noisy learning framework that incorporates noise probabilities of a training dataset. These are then utilized effectively to perform learning with sustained levels of accuracy, even in the presence of significant noise levels. We present results on several datasets of varying sizes and complexity and demonstrate that the proposed mechanism is able to outperform existing methods, despite often employing weaker constraints and assumptions.	artificial neural network;complexity;computer hardware;database;flickr;layer (electronics);scalability;tag cloud	Pravin Kakar;Alex Yong-Sang Chia	2015		10.1145/2733373.2806231	computer science;data science;machine learning;pattern recognition;data mining;world wide web;statistics	ML	22.2585448749825	-50.2195410799667	70266
1e21b925b65303ef0299af65e018ec1e1b9b8d60	unsupervised cross-domain image generation		We study the ecological use of analogies in AI. Specifically, we address the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.	artificial intelligence;delay-tolerant networking;entity;loss function;map;unsupervised learning;visual basic[.net]	Yaniv Taigman;Adam Polyak;Lior Wolf	2016	CoRR		computer vision;artificial intelligence;machine learning;pattern recognition;mathematics	AI	23.879574286511808	-48.915751515257455	70323
e865d20adc2b940fe24e34ea03693e7bcf8d2ead	towards feature selection in network	regularized least squares;linear regression;optimization problem;laplacian regularized least squares;mixed integer program;independent and identically distributed;gradient descent;feature selection;graph regularization;network	Traditional feature selection methods assume that the data are independent and identically distributed (i.i.d.). However, in real world, there are tremendous amount of data which are distributing in a network. Existing features selection methods are not suited for networked data because the i.i.d. assumption no longer holds. This motivates us to study feature selection in a network. In this paper, we present a supervised feature selection method based on Laplacian Regularized Least Squares (LapRLS) for networked data. In detail, we use linear regression to utilize the content information, and adopt graph regularization to consider the link information. The proposed feature selection method aims at selecting a subset of features such that the empirical error of LapRLS is minimized. The resultant optimization problem is a mixed integer programming, which is difficult to solve. It is relaxed into a $L_{2,1}$-norm constrained LapRLS problem and solved by accelerated proximal gradient descent algorithm. Experiments on benchmark networked data sets show that the proposed feature selection method outperforms traditional feature selection method and the state of the art learning in network approaches.	algorithm;benchmark (computing);feature selection;gradient descent;integer programming;linear programming;mathematical optimization;optimization problem;proximal gradient methods for learning;regularized least squares;resultant	Quanquan Gu;Jiawei Han	2011		10.1145/2063576.2063746	gradient descent;independent and identically distributed random variables;optimization problem;mathematical optimization;computer science;linear regression;machine learning;pattern recognition;data mining;feature selection;statistics	ML	22.79976276440514	-41.474667722259426	70330
e5f96d258f37a84eb7e21fb72257903ca5231496	deep meta-learning: learning to learn in the concept space		Few-shot learning remains challenging for metalearning that learns a learning algorithm (metalearner) from many related tasks. In this work, we argue that this is due to the lack of a good representation for meta-learning, and propose deep metalearning to integrate the representation power of deep learning into meta-learning. The framework is composed of three modules, a concept generator, a meta-learner, and a concept discriminator, which are learned jointly. The concept generator, e.g. a deep residual net, extracts a representation for each instance that captures its high-level concept, on which the meta-learner performs fewshot learning, and the concept discriminator recognizes the concepts. By learning to learn in the concept space rather than in the complicated instance space, deep meta-learning can substantially improve vanilla meta-learning, which is demonstrated on various few-shot image recognition problems. For example, on 5-way-1-shot image recognition on CIFAR-100 and CUB-200, it improves Matching Nets from 50.53% and 56.53% to 58.18% and 63.47%, improves MAML from 49.28% and 50.45% to 56.65% and 64.63%, and improves Meta-SGD from 53.83% and 53.34% to 61.62% and 66.95%, respectively.	algorithm;computer vision;concept learning;deep learning;discriminator;experiment;high- and low-level;multi-task learning;synergy;tor messenger	Fengwei Zhou;Bin Wu;Zhenguo Li	2018	CoRR		machine learning;residual;discriminator;artificial intelligence;mathematics;deep learning	AI	24.395481954368105	-49.25538181038619	70391
94187ef33e34af2cdb42502083c6f9b4c3f5ba6b	practical black-box attacks against deep learning systems using adversarial examples		Machine learning (ML) models, e.g., state-of-the-art deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We then use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. After labeling 6,400 synthetic inputs to train our substitute (an order of magnitude smaller than the training set used by MetaMind), we find that their DNN misclassifies adversarial examples crafted with our substitute at a rate of 84.24%. We demonstrate that our strategy generalizes to many ML techniques like logistic regression or SVMs, regardless of the ML model chosen for the substitute. We instantiate the same attack against models hosted by Amazon and Google, using logistic regression substitutes trained with only 800 label queries. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder. We conclude with experiments exploring why adversarial samples transfer between models.	adversary (cryptography);algorithm;artificial neural network;black box;deep learning;experiment;logistic regression;machine learning;malware;oracle machine;synthetic data;synthetic intelligence;test set;vanishing gradient problem	Nicolas Papernot;Patrick D. McDaniel;Ian J. Goodfellow;Somesh Jha;Z. Berkay Celik;Ananthram Swami	2016	CoRR		machine learning;data mining;computer security	Security	18.982248390656178	-51.13433000619961	70521
b7c03b18478b7da5dec9e82efc9cdfeafa25605b	characterizing cancer subtypes as attractors of hopfield networks		MOTIVATION Cancer is a heterogeneous progressive disease caused by perturbations of the underlying gene regulatory network that can be described by dynamic models. These dynamics are commonly modeled as Boolean networks or as ordinary differential equations. Their inference from data is computationally challenging, and at least partial knowledge of the regulatory network and its kinetic parameters is usually required to construct predictive models.   RESULTS Here, we construct Hopfield networks from static gene-expression data and demonstrate that cancer subtypes can be characterized by different attractors of the Hopfield network. We evaluate the clustering performance of the network and find that it is comparable with traditional methods but offers additional advantages including a dynamic model of the energy landscape and a unification of clustering, feature selection and network inference. We visualize the Hopfield attractor landscape and propose a pruning method to generate sparse networks for feature selection and improved understanding of feature relationships.		Stefan Maetschke;Mark A. Ragan	2014	Bioinformatics	10.1093/bioinformatics/btt773	computer science;bioinformatics;artificial intelligence;machine learning;data mining;hopfield network	AI	12.182074422260204	-51.69814693159456	70639
fe6074af9e26c70a3409f951bce394bec192e707	large-scale linear nonparallel support vector machine solver	support vector machines;nonparallel;dual coordinate descent;classification;large scale	Twin support vector machines (TWSVMs), as the representative nonparallel hyperplane classifiers, have shown the effectiveness over standard SVMs from some aspects. However, they still have one serious defect restricting their further study and real applications: they have to compute and store the inverse matrices before training, it is intractable for many applications such as that data appear with a huge number of instances as well as features. This paper proposes a Linear Nonparallel Support Vector Machine, termed as  L  2 -TWSVM, to deal with large-scale data based on an efficient solver – dual coordinate descent (DCD) method. Both theoretical analysis and experiments indicate that our method is not only suitable for large scale problems, but also has better generalization performance than linear TWSVMs and linear SVMs.	solver;support vector machine	Yingjie Tian;Qin Zhang;Yuan Ping	2014	Neurocomputing	10.1016/j.neucom.2014.02.032	support vector machine;mathematical optimization;biological classification;computer science;machine learning;pattern recognition	AI	21.041977098779626	-38.32005473013621	70776
633ca56b50a57299395bdc09c668bbcef488586d	data access skipping for recursive partitioning methods		Abstract The memory performance of data mining applications became crucial due to increasing dataset sizes and multi-level cache hierarchies. Recursive partitioning methods such as decision tree and random forest learning are some of the most important algorithms in this field, and numerous researchers worked on improving the accuracy of model trees as well as enhancing the overall performance of the learning process. Most modern applications that employ decision tree learning favor creating multiple models for higher accuracy by sacrificing performance. In this work, we exploit the flexibility inherent in recursive partitioning based applications regarding performance and accuracy tradeoffs, and propose a framework to improve performance with negligible accuracy losses. This framework employs a data access skipping module (DASM) using which costly cache accesses are skipped according to the aggressiveness of the strategy specified by the user and a heuristic to predict skipped data accesses to keep accuracy losses at minimum. Our experimental evaluation shows that the proposed framework offers significant performance improvements (up to 25%) with relatively much smaller losses in accuracy (up to 8%) over the original case. We demonstrate that our framework is scalable under various accuracy requirements via exploring accuracy changes over time and replacement policies. In addition, we explore NoC/SNUCA systems for similar opportunities of memory performance improvement.	data access;decision tree learning;recursion	Orhan Kislal;Mahmut T. Kandemir	2018	Computer Languages, Systems & Structures	10.1016/j.cl.2018.03.003	theoretical computer science;computer science;decision tree;cache;performance improvement;scalability;decision tree learning;recursive partitioning;machine learning;heuristic;artificial intelligence;random forest	Theory	17.210553463291806	-38.730743228371665	70956
97f8428bcc78d10cb3b2722826a2d1066327447c	the data selection criteria for hsc and svm algorithms	nearest neighbor searches;data selection criteria;support vector machines;training;hsc;testing;support vector;generalization ability;hyper surface classification;accuracy;artificial neural networks;support vector machines support vector machine classification nearest neighbor searches neural networks testing laboratories information processing computers helium databases;consistent subsets cs;generalization ability data selection criteria hsc svm consaistent subsets selection criteria hyper surface classification minimal consistent subset for a disjoint cover set;classification algorithms;pattern classification;svm;minimal consistent subset for a disjoint cover set;consaistent subsets selection criteria;data selection criteria consistent subsets cs minimal consistent subset for a disjoint cover set;support vector machines pattern classification;selection criteria	This paper makes a discussion of consistent subsets (CS) selection criteria for hyper surface Classification (HSC) and SVM algorithms. The consistent subsets play an important role in the data selection. Firstly, the paper proposes that minimal consistent subset for a disjoint cover set (MCSC) plays an important role in the data selection for HSC. The MCSC can be applied to select a representative subset from the original sample set for HSC. MCSC has the same classification model with the entire sample set and can totally reflect its classification ability. Secondly, the number of MCSC is calculated. Thirdly, by comparing the performance of HSC and SVM on corresponding CS, we argue that it is not reasonable that using the same train data set to train different classifiers and then testing the classifiers by the same test data set for different algorithms. The experiments show that algorithms can respectively select the proper data set for training, which ensures good performance and generalization ability. MCSC is the best selection for HSC, and support vector set is the effective selection for SVM.	algorithm;experiment;support vector machine;test data;test set	Qing He;Fuzhen Zhuang;Zhongzhi Shi	2008	2008 Fourth International Conference on Natural Computation	10.1109/ICNC.2008.334	machine learning;pattern recognition;data mining;mathematics	AI	11.10249467319291	-39.403174621492724	70979
db51e5f40454c164ca495cfc3b9f9ffcc85edc05	an efficient partition of training data set improves speed and accuracy of cascade-correlation algorithm	efficient partition of training data set;algorithm;early stopping;cascade correlation	This study extends an application of efficient partition algorithm (EPA) for artificial neural network ensemble trained according to Cascade Correlation Algorithm. We show that EPA allows to decrease the number of cases in learning and validated data sets. The predictive ability of the ensemble calculated using the whole data set is not affected and in some cases it is even improved. It is shown that a distribution of cases selected by this method is proportional to the second derivative of the analyzed function.	algorithm;artificial neural network	Igor V. Tetko;Alessandro E. P. Villa	1997	Neural Processing Letters	10.1023/A:1009619010371	mathematical optimization;early stopping;computer science;machine learning;pattern recognition;mathematics	ML	13.774612415448594	-39.54666554253356	71153
42e898ca773dbd9e085ffa824c21d0bfda245345	lots about attacking deep features		Deep neural networks provide state-of-the-art performance on various tasks and are, therefore, widely used in real world applications. DNNs are becoming frequently utilized in biometrics for extracting deep features, which can be used in recognition systems for enrolling and recognizing new individuals. It was revealed that deep neural networks suffer from a fundamental problem, namely, they can unexpectedly misclassify examples formed by slightly perturbing correctly recognized inputs. Various approaches have been developed for generating these so-called adversarial examples, but they aim at attacking end-to-end networks. For biometrics, it is natural to ask whether systems using deep features are immune to or, at least, more resilient to attacks than end-to-end networks. In this paper, we introduce a general technique called the layerwise origin-target synthesis (LOTS) that can be efficiently used to form adversarial examples that mimic the deep features of the target. We analyze and compare the adversarial robustness of the end-to-end VGG Face network with systems that use Euclidean or cosine distance between gallery templates and extracted deep features. We demonstrate that iterative LOTS is very effective and show that systems utilizing deep features are easier to attack than the end-to-end network.		Andras Rozsa;Manuel Günther;Terrance E. Boult	2017	2017 IEEE International Joint Conference on Biometrics (IJCB)	10.1109/BTAS.2017.8272695	finance;machine learning;ask price;economics;robustness (computer science);artificial neural network;adversarial system;feature extraction;facial recognition system;biometrics;artificial intelligence;euclidean geometry	EDA	19.570960636142406	-51.71513800351879	71440
850c29c77c736f0e47a181c7a0702cc3dcc44f1a	divergence-based classification in learning vector quantization	cost function;distance measure;prototype based classifiers;receiver operator characteristic;kullback leibler divergence;color histogram;euclidean distance;classification;public domain;cassava mosaic disease;distance measures;similarity measures;classification accuracy;data consistency;similarity measure;breast cancer;learning vector quantization	We discuss the use of divergences in dissimilarity-based classification. Divergences can be employed whenever vectorial data consists of non-negative, potentially normalized features. This is, for instance, the case in spectral data or histograms. In particular, we introduce and study divergence based learning vector quantization (DLVQ). We derive cost function based DLVQ schemes for the family of g-divergences which includes the well-known Kullback–Leibler divergence and the so-called Cauchy–Schwarz divergence as special cases. The corresponding training schemes are applied to two different real world data sets. The first one, a benchmark data set (Wisconsin Breast Cancer) is available in the public domain. In the second problem, color histograms of leaf images are used to detect the presence of cassava mosaic disease in cassava plants. We compare the use of standard Euclidean distances with DLVQ for different parameter settings. We show that DLVQ can yield superior classification accuracies and Receiver Operating Characteristics. & 2011 Elsevier B.V. All rights reserved.	benchmark (computing);euclidean distance;kullback–leibler divergence;learning vector quantization;loss function;whole earth 'lectronic link	Ernest Mwebaze;Petra Schneider;Frank-Michael Schleif;Jennifer R. Aduwo;John A. Quinn;Sven Haase;Thomas Villmann;Michael Biehl	2011	Neurocomputing	10.1016/j.neucom.2010.10.016	color histogram;public domain;learning vector quantization;biological classification;computer science;breast cancer;machine learning;pattern recognition;data mining;euclidean distance;mathematics;distance measures;kullback–leibler divergence;data consistency;receiver operating characteristic;statistics	ML	20.79394091913258	-41.10390352132692	71481
6889d649c6bbd9c0042fadec6c813f8e894ac6cc	analysis of robust soft learning vector quantization and an application to facial expression recognition	004;learning vector quantization analysis facial expression recognition	Learning Vector Quantization (LVQ) [1] is a popular method for multiclass classification. Several variants of LVQ have been developed recently, of which Robust Soft Learning Vector Quantization (RSLVQ) [2] is a promising one. Although LVQ methods have an intuitive design with clear updating rules, their dynamics are not yet well understood. In simulations within a controlled environment RSLVQ performed very close to optimal. This controlled environment enabled us to perform a mathematical analysis as a first step in obtaining a better theoretical understanding of the learning dynamics. This extended abstract provides the outline of our theoretical analysis and its results. Moreover, we will focus on the practical application of RSLVQ to a real world data set containing extracted features from facial expression data.	generalization error;igor muttik;learning vector quantization;multiclass classification;rough set;runge–kutta methods;simulation;video processing	Gert-Jan de Vries;Michael Biehl	2009			speech recognition;learning vector quantization;computer science;machine learning;pattern recognition	ML	17.436178656496	-45.06136323384448	71502
269433828d26a7f7042a44d98efe9663b2d90886	an evaluation framework and database for mocap-based gait recognition methods	software evaluation framework;gait cycle database;human gaitrecognition;human gait recognition	As a contribution to reproducible research, this paper presents a framework and a database to improve the development, evaluation and comparison of methods for gait recognition from motion capture (MoCap) data. The evaluation framework comprises source codes of state-of-the-art human-interpretable geometric features as well as our own approaches where gait features are learned by a modification of Fisher’s Linear Discriminant Analysis with the Maximum Margin Criterion, and by a combination of Principal Component Analysis and Linear Discriminant Analysis. It includes a description and source codes of a mechanism for evaluating class separability coefficients of feature space and four classifier performance metrics. This framework also contains a tool for learning a custom classifier and for classifying a custom probe on a custom gallery. We provide an experimental database along with source codes for its extraction from the general CMU MoCap database.	algorithm;code;coefficient;documentation;experiment;feature vector;gait analysis;linear discriminant analysis;linear separability;machine learning;motion capture;principal component analysis;statistical classification	Michal Balazia;Petr Sojka	2016		10.1007/978-3-319-56414-2_3	computer science;machine learning;pattern recognition;data mining	Vision	13.318463447126328	-49.02060911802028	71515
79539d2ea2540e75948c60b4bbf0d5400ccb9aa8	sharing residual units through collective tensor factorization in deep neural networks		Residual units are wildly used for alleviating optimization difficulties when building deep neural networks. However, the performance gain does not well compensate the model size increase, indicating low parameter efficiency in these residual units. In this work, we first revisit the residual function in several variations of residual units and demonstrate that these residual functions can actually be explained with a unified framework based on generalized block term decomposition. Then, based on the new explanation, we propose a new architecture, Collective Residual Unit (CRU), which enhances the parameter efficiency of deep neural networks through collective tensor factorization. CRU enables knowledge sharing across different residual units using shared factors. Experimental results show that our proposed CRU Network demonstrates outstanding parameter efficiency, achieving comparable classification performance to ResNet-200 with the model size of ResNet-50. By building a deeper network using CRU, we can achieve state-of-the-art single model classification accuracy on ImageNet-1k and Places365-Standard benchmark datasets. (Code and trained models are available on GitHub)	deep learning;neural networks	Yunpeng Chen;Xiaojie Jin;Bingyi Kang;Jiashi Feng;Shuicheng Yan	2017	CoRR		mathematical optimization;artificial intelligence;machine learning;mathematics	ML	21.70056640202965	-49.271189865989896	71543
f2076ce8b8fe8cee1d7d1cf8f62f0c0df9ba0f8f	detection of adversarial training examples in poisoning attacks through anomaly detection		Machine learning has become an important component for many systems and applications including computer vision, spam filtering, malware and network intrusion detection, among others. Despite the capabilities of machine learning algorithms to extract valuable information from data and produce accurate predictions, it has been shown that these algorithms are vulnerable to attacks. Data poisoning is one of the most relevant security threats against machine learning systems, where attackers can subvert the learning process by injecting malicious samples in the training data. Recent work in adversarial machine learning has shown that the so-called optimal attack strategies can successfully poison linear classifiers, degrading the performance of the system dramatically after compromising a small fraction of the training dataset. In this paper we propose a defence mechanism to mitigate the effect of these optimal poisoning attacks based on outlier detection. We show empirically that the adversarial examples generated by these attack strategies are quite different from genuine points, as no detectability constrains are considered to craft the attack. Hence, they can be detected with an appropriate pre-filtering of the training dataset.	adversarial machine learning;algorithm;anomaly detection;attack (computing);computational learning theory;computer security;computer vision;data point;email filtering;internationalization and localization;intrusion detection system;linear classifier;malware;mathematical optimization	Andrea Paudice;Luis Muñoz-González;Andras Gyorgy;Emil C. Lupu	2018	CoRR		adversarial system;anomaly detection;artificial intelligence;mathematics;machine learning;malware;training set;intrusion detection system;adversarial machine learning	Security	19.0562546231022	-51.12065872095887	71620
da280b90fee687ccd398758dc33b07747178e999	optimized space frequency kernel for texture classification	kernel function;texture;support vector machines;classification;image classification;logical operator;image processing;image texture;error rate;statistical analysis;feature selection;support vector machine	The performance of the support vector machine (SVM) algorithm is highly dependent on the choice of the kernel function suited to the problem at hand. In a support vector machine algorithm feature selection is implicitly performed by kernel function. On the other hand, feature selection is the most important stage in any texture classification algorithm. In this work, the performance of SVM is improved by choosing an optimized space-frequency (SFR) kernel function. The proposed method is evaluated in a two-texture and multi-texture problems. The results are compared with the original SVM and other recently published texture classification methods. The comparison shows a significant improvement in error rates. Improvement of more than 40% in compare with original SVM and about 60% in compare with logical operators (LO) and wavelet co-occurrence features (WCOF) are obtained.	algorithm;feature selection;kernel (operating system);logical connective;support vector machine;wavelet	Mahdi Sabri;Javad Alirezaie	2004	2004 International Conference on Image Processing, 2004. ICIP '04.		support vector machine;least squares support vector machine;computer vision;kernel method;kernel embedding of distributions;radial basis function kernel;image processing;computer science;machine learning;pattern recognition;mathematics;feature selection;ranking svm;variable kernel density estimation;polynomial kernel	Robotics	12.048235396235395	-43.9270972757343	71711
1e3068886b138304ec5a7296702879cc8788143d	active rare class discovery and classification using dirichlet processes	active learning;classification;journal article;rare class discovery	Classification is used to solve countless problems. Many real world computer vision problems, such as visual surveillance, contain uninteresting but common classes alongside interesting but rare classes. The rare classes are often unknown, and need to be discovered whilst training a classifier. Given a data set active learning selects the members within it to be labelled for the purpose of constructing a classifier, optimising the choice to get the best classifier for the least amount of effort. We propose an active learning method for scenarios with unknown, rare classes, where the problems of classification and rare class discovery need to be tackled jointly. By assuming a non-parametric prior on the data the goals of new class discovery and classification refinement are automatically balanced, without any tunable parameters. The ability to work with any specific classifier is maintained, so it may be used with the technique most appropriate for the problem at hand. Results are provided for a large variety of problems, demonstrating superior performance.	active learning (machine learning);complexity class;computer performance;computer vision;refinement (computing);statistical classification;type class;while	Tom S. F. Haines;Tao Xiang	2013	International Journal of Computer Vision	10.1007/s11263-013-0630-3	biological classification;computer science;machine learning;pattern recognition;data mining;active learning;one-class classification	Vision	16.943843251792824	-40.13462675196265	71792
b76a229ede1985dad48664192a691e977690b2af	j-pmcri: a methodology for inducing pre-pruned modular classification rules	endnotes;computing;pubications	Inducing rules from very large datasets is one of the most challenging areas in data mining. Several approaches exist to scaling up classification rule induction to large datasets, namely data reduction and the parallelisation of classification rule induction algorithms. In the area of parallelisation of classification rule induction algorithms most of the work has been concentrated on the Top Down Induction of Decision Trees (TDIDT), also known as the ‘divide and conquer’ approach. However powerful alternative algorithms exist that induce modular rules. Most of these alternative algorithms follow the ‘separate and conquer’ approach of inducing rules, but very little work has been done to make the ‘separate and conquer’ approach scale better on large training data. This paper examines the potential of the recently developed blackboard based J-PMCRI methodology for parallelising modular classification rule induction algorithms that follow the ‘separate and conquer’ approach. A concrete implementation of the methodology is evaluated empirically on very large datasets.	algorithm;central processing unit;data mining;decision tree;image scaling;parallel computing;pseudocode;rule 90;rule induction;server (computing);speedup;windows legacy audio components;workstation	Frederic T. Stahl;Max Bramer;Mo Adda	2010		10.1007/978-3-642-15286-3_5	computer science;machine learning;data mining;algorithm	ML	13.862707268520266	-40.30212779116452	71835
15cbff0008f11bbb89776934463d6ebc4b1d6f60	estimating redundancy information of selected features in multi-dimensional pattern classification	conditional mutual information;speech music discrimination;multi dimensional;dynamic range;pattern classification;mutual information;feature selection;redundancy information;classification accuracy	This paper proposes a novel criterion for estimating the redundancy information of selected feature sets in multi-dimensional pattern classification. An appropriate feature selection process typically maximizes the relevancy of features to each class and minimizes the redundancy of features between selected features. Unlike to the relevancy information that can be measured by mutual information, however, it is difficult to estimate the redundancy information because its dynamic range is varied by the characteristics of features and classes. By utilizing the conceptual diagram of the relationship between candidate features, selected features, and class variables, this paper proposes a new criterion to accurately compute the amount of redundancy. Specifically, the redundancy term is estimated by conditional mutual information between selected and candidate features to each class variable, which does not need a cumbersome normalization process as the conventional algorithm does. The proposed algorithm is implemented into a speech/music discrimination system to evaluate classification performance. Experimental results by varying the number of selected features verify that the proposed method shows higher classification accuracy than conventional algorithms.		Chi-Sang Jung;Hyunson Seo;Hong-Goo Kang	2011	Pattern Recognition Letters	10.1016/j.patrec.2010.11.023	dynamic range;minimum redundancy feature selection;redundancy;computer science;machine learning;pattern recognition;data mining;mathematics;mutual information;conditional mutual information;feature selection;statistics	Vision	11.65724104049213	-44.52155224715851	71970
d5547d798333b612df6c9f2aa08b50576a02b993	thinet: a filter level pruning method for deep neural network compression		We propose an efficient and unified framework, namely ThiNet, to simultaneously accelerate and compress CNN models in both training and inference stages. We focus on the filter level pruning, i.e., the whole filter would be discarded if it is less important. Our method does not change the original network structure, thus it can be perfectly supported by any off-the-shelf deep learning libraries. We formally establish filter pruning as an optimization problem, and reveal that we need to prune filters based on statistics information computed from its next layer, not the current layer, which differentiates ThiNet from existing methods. Experimental results demonstrate the effectiveness of this strategy, which has advanced the state-of-the-art. We also show the performance of ThiNet on ILSVRC-12 benchmark. ThiNet achieves 3.31 x FLOPs reduction and 16.63× compression on VGG-16, with only 0.52% top-5 accuracy drop. Similar experiments with ResNet-50 reveal that even for a compact network, ThiNet can also reduce more than half of the parameters and FLOPs, at the cost of roughly 1% top-5 accuracy drop. Moreover, the original VGG-16 model can be further pruned into a very small model with only 5.05MB model size, preserving AlexNet level accuracy but showing much stronger generalization ability.	benchmark (computing);deep learning;experiment;flops;library (computing);mathematical optimization;optimization problem;prune and search;unified framework	Jian-Hao Luo;Jianxin Wu;Weiyao Lin	2017	2017 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2017.541	pruning;artificial neural network;fold (higher-order function);machine learning;flops;computer science;deep learning;pattern recognition;compression (physics);artificial intelligence;optimization problem;inference	Vision	21.886524297755507	-49.827572088630305	71971
0f2e2e4ec29fff639b1bcb6e0c42e808c5de1205	dropfilter: dropout for convolutions		Using a large number of weights, deep neural networks have achieved remarkable performance on computer vision and natural language processing tasks. However deep neural networks usually use lots of parameters and suffer from overfitting. Dropout is a widely use method to deal with overfitting. Although dropout can significantly regularize fully connected layers in neural networks, it usually leads to suboptimal results when used for convolutional layers. To tackle this problem, we propose DropFilter, a dropout method for convolutional layers. DropFilter considers the filters rather than the neural units as the basic data drop units. Because it is observed that co-adaptations are more likely to occur inter rather than intra filters in convolutional layers. Using DropFilter, we remarkably improve the performance of convolutional networks on CIFAR and ImageNet.	artificial neural network;computation;computer vision;convolution;deep learning;dropout (neural networks);imagenet;multipath propagation;natural language processing;overfitting;randomness	F-I Auzanneau;Jianwei Niu;Qi Tian	2018	CoRR		overfitting;machine learning;pattern recognition;artificial neural network;artificial intelligence;computer science;convolution	ML	22.409517057785976	-50.44487803044304	72080
9343ca007f751edfa9e5acac0b9d51fc7450bb64	adaptable multi-phase rules over the infrequent class		Decision trees are a classification model that allow rule generation. Depending upon the type of decision tree model, rules may have one to hundreds of conditions and with repeating data attributes over different conditional values causing the rules to be difficult to understand. To achieve more understandable rules, the number of nodes can be minimized to control the depth of the tree and, therefore, the number of conditions in the rules. Further, the study described in this paper seeks to optimize the decision tree for the generation of rules specific to the infrequent class which presents another challenge since the infrequent class may have few instances in the dataset. Rules that are generated using either decision trees or class association mining generally come from the major class of the dataset. These two mining techniques, decision trees and association mining, are utilized together through ensemble learning in an adaptable manner so that they expand and contract to accommodate the characteristics of the dataset. The ensemble learning occurs in phases: a partially generated or minimized decision tree mining phase, and association mining phase, to increase the probability of finding infrequent class rules. The ensemble learning technique developed in this study is found to generate understandable rules with increased coverage and confidence for the infrequent class with balanced or unbalanced datasets.		Soma Datta;Susan Mengel	2018	Soft Comput.	10.1007/s00500-018-3399-z	machine learning;decision tree model;artificial intelligence;decision tree;computer science;ensemble learning	ECom	14.51743988424302	-39.29578133217435	72124
f0af0029293dc8f242894f113baf15d68228ec4d	attentional factorization machines: learning the weight of feature interactions via attention networks		Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Despite effectiveness, FM can be hindered by its modelling of all feature interactions with the same weight, as not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noises and adversely degrade the performance. In this work, we improve FM by discriminating the importance of different feature interactions. We propose a novel model named Attentional Factorization Machine (AFM), which learns the importance of each feature interaction from data via a neural attention network. Extensive experiments on two real-world datasets demonstrate the effectiveness of AFM. Empirically, it is shown on regression task AFM betters FM with a 8.6% relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&Deep [Cheng et al., 2016] and DeepCross [Shan et al., 2016] with a much simpler structure and fewer model parameters. Our implementation of AFM is publicly available at: https://github. com/hexiangnan/attentional factorization machine	atomic-force microscopy;chua's circuit;deep learning;experiment;fm broadcasting;feature interaction problem;laplacian matrix;matrix regularization;nonlinear system;question answering;sampling (signal processing);semi-supervised learning;supervised learning;wang tile;yang	Jun Xiao;Hao Ye;Xiangnan He;Hanwang Zhang;Fei Wu;Tat-Seng Chua	2017		10.24963/ijcai.2017/435	supervised learning;machine learning;artificial intelligence;linear regression;deep learning;atomic force microscopy;computer science;factorization;pattern recognition	AI	19.594646135574727	-47.90358235591482	72294
0c1db61308e9a1ac9e15c193d82ded4ec5f0971f	color image segmentation with bounded generalized gaussian mixture model and feature selection		We present a novel method for color image segmentation based on an unsupervised learning model and feature selection. Our focus here is to develop an expectation maximization algorithm based on a mixture of bounded generalized Gaussian model combined with a feature selection mechanism. The developed statistical model offers more flexibility in data modeling than the Gaussian distribution and the feature selection mechanism aims at eliminating irrelevant features and then improving the segmentation performances. Obtained results performed on a large dataset of real world color images confirm the effectiveness of the proposed approach.	color image;data modeling;expectation–maximization algorithm;feature selection;image segmentation;mixture model;performance;relevance;statistical model;unsupervised learning	Ines Channoufi;Sami Bourouis;Nizar Bouguila;Kamel Hamrouni	2018	2018 4th International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)	10.1109/ATSIP.2018.8364459	mixture model;unsupervised learning;expectation–maximization algorithm;color image;feature selection;gaussian;bounded function;statistical model;artificial intelligence;mathematics;pattern recognition	Robotics	17.65307638146858	-40.40380328796647	72411
8a29808865a0daf55e80a626a555c21f31c479f4	scatter component analysis: a unified framework for domain adaptation and domain generalization	object recognition;kernel;object recognition domain adaptation domain generalization feature learning kernel methods scatter;standards;training;object recognition algorithm design and analysis training kernel optimization standards visualization;visualization;optimization;algorithm design and analysis	This paper addresses classification tasks on a particular target domain in which labeled training data are only available from source domains different from (but related to) the target. Two closely related frameworks, domain adaptation and domain generalization, are concerned with such tasks, where the only difference between those frameworks is the availability of the unlabeled target data: domain adaptation can leverage unlabeled target information, while domain generalization cannot. We propose Scatter Component Analyis (SCA), a fast representation learning algorithm that can be applied to both domain adaptation and domain generalization. SCA is based on a simple geometrical measure, i.e., scatter, which operates on reproducing kernel Hilbert space. SCA finds a representation that trades between maximizing the separability of classes, minimizing the mismatch between domains, and maximizing the separability of data; each of which is quantified through scatter. The optimization problem of SCA can be reduced to a generalized eigenvalue problem, which results in a fast and exact solution. Comprehensive experiments on benchmark cross-domain object recognition datasets verify that SCA performs much faster than several state-of-the-art algorithms and also provides state-of-the-art classification accuracy in both domain adaptation and domain generalization. We also show that scatter can be used to establish a theoretical generalization bound in the case of domain adaptation.	acclimatization;addresses (publication format);algorithm;benchmark (computing);cardiac arrest;class;data domain;domain adaptation;domain-driven design;experiment;feature learning;generalization (psychology);hilbert space;linear separability;machine learning;mathematical optimization;optimization problem;outline of object recognition;structure of superior cerebellar artery;tracer	Muhammad Ghifary;David Balduzzi;W. Bastiaan Kleijn;Mengjie Zhang	2017	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2016.2599532	kernel (linear algebra);artificial intelligence;pattern recognition;eigendecomposition of a matrix;computer science;algorithm design;kernel method;component analysis;machine learning;feature learning;reproducing kernel hilbert space;optimization problem	Vision	23.57643676347621	-44.43242439149175	72723
a4ee9f089ab9a48a6517a6967281247339a51747	resembled generative adversarial networks: two domains with similar attributes		We propose a novel algorithm, namely Resembled Generative Adversarial Networks (GAN), that generates two different domain data simultaneously where they resemble each other. Although recent GAN algorithms achieve the great success in learning the cross-domain relationship [9, 19, 22], their application is limited to domain transfers, which requires the input image. The first attempt to tackle the data generation of two domains was proposed by CoGAN [10]. However, their solution is inherently vulnerable for various levels of domain similarities. Unlike CoGAN, our Resembled GAN implicitly induces two generators to match feature covariance from both domains, thus leading to share semantic attributes. Hence, we effectively handle a wide range of structural and semantic similarities between various two domains. Based on experimental analysis on various datasets, we verify that the proposed algorithm is effective for generating two domains with similar attributes.	algorithm;context awareness;generative adversarial networks;structural similarity	Duhyeon Bang;Hyunjung Shim	2018			pattern recognition;computer science;adversarial system;generative grammar;machine learning;artificial intelligence;test data generation;covariance	AI	24.327028676646954	-48.66367470628841	72877
52aeb3f07222e87ee838e9e4f9923a7db50c4666	privacy-preserved big data analysis based on asymmetric imputation kernels	kernel ridge regression krr;data imputation;incomplete data analysis;privacy preservation;partial similarity;multiside similarity;data analytics;kernel method;missing values;cloud computing	This study presents an efficient approach for incomplete data classification, where the entries of samples are missing or masked due to privacy preservation. To deal with these incomplete data, a new kernel function with asymmetric intrinsic mappings is proposed in this study. Such a new kernel uses three-side similarities for kernel matrix formation. The similarity between a testing instance and a training sample relies not only on their distance but also on the relation between the testing sample and the centroid of the class, where the training sample belongs. This reduces biased estimation compared with typical methods when only one training sample is used for kernel matrix formation. Furthermore, centroid generation does not involve any clustering algorithms. The proposed kernel is capable of performing data imputation by using class-dependent averages. This enhances Fisher Discriminant Ratios and data discriminability. Experiments on two open databases were carried out for evaluating the proposed method. The result indicated that the accuracy of the proposed method was higher than that of the baseline. These findings thereby demonstrated the effectiveness of the proposed idea.	algorithm;baseline (configuration management);big data;case preservation;cluster analysis;data structure alignment;database;discrepancy function;entity–relationship model;geo-imputation;gramian matrix;intrinsic dimension;kernel (operating system);linear discriminant analysis;missing data	Bo-Wei Chen	2018	Future Generation Comp. Syst.	10.1016/j.future.2016.11.008	kernel embedding of distributions;kernel (statistics);polynomial kernel;kernel principal component analysis;computer science;kernel fisher discriminant analysis;data mining;kernel method;artificial intelligence;radial basis function kernel;pattern recognition;variable kernel density estimation	ML	19.7031842787402	-42.89181408410432	73007
46f67df2f13da78dea4f32a658e691ed1eb3655e	optimistic semi-supervised least squares classification	convergence;training;linear programming;optimization;encoding;semisupervised learning;labeling	The goal of semi-supervised learning is to improve supervised classifiers by using additional unlabeled training examples. In this work we study a simple self-learning approach to semi-supervised learning applied to the least squares classifier. We show that a soft-label and a hard-label variant of self-learning can be derived by applying block coordinate descent to two related but slightly different objective functions. The resulting soft-label approach is related to an idea about dealing with missing data that dates back to the 1930s. We show that the soft-label variant typically outperforms the hard-label variant on benchmark datasets and partially explain this behaviour by studying the relative difficulty of finding good local minima for the corresponding objective functions.	benchmark (computing);coordinate descent;iterative method;least squares;loss function;maxima and minima;missing data;optimization problem;semi-supervised learning;semiconductor industry;supervised learning	Jesse H. Krijthe;Marco Loog	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7899877	labeling theory;convergence;computer science;linear programming;machine learning;pattern recognition;data mining;mathematics;encoding	Vision	20.26987108510306	-40.48568745837879	73268
4391b5b8464bde0da128000e65334fc9502d221d	reliable representation of data on manifolds	learning model;manifold learning;data analysis;synthetic data;topology preservation	The manifold learning algorithms are promising data analysis tools. However, to fit an unseen point in a learned model, the point must be located in the training set, which limits its scalability. In this paper, we discuss how to select landmarks from the data to help locate the test points. Our method is for data on manifolds: the way the landmarks represent the data in the ambient space should resemble the way they represent the data on the manifold. Compared to the previous research, (i) Our test foregoes the requirement of knowing the intrinsic manifold dimension and thus is more applicable and robust. (ii) Our selection implies a provable topology preservation property. (iii) We also provide a way to improve existing landmarks. Experiments on the synthetic data and the real data have been done. The results support the proposed properties and algorithms.	algorithm;experiment;machine learning;nonlinear dimensionality reduction;provable security;scalability;synthetic data;test set	Jun Li;Pengwei Hao	2008		10.5244/C.22.113	computer science;machine learning;pattern recognition;data mining;mathematics;nonlinear dimensionality reduction;data analysis;statistics;synthetic data;manifold alignment	ML	22.233462855190744	-42.31448044137814	73435
8ce09f7010b9f7b86aca9f3215b89dce7e793260	evaluation of prototype learning algorithms for nearest-neighbor classifier in application to handwritten character recognition	stochastic gradient search;learning algorithm;lvq;handwritten chinese character recognition;objective function;signal processing;nearest neighbor;stochastic gradient;prototype learning;minimum classification error;handwritten numeral recognition;nearest neighbor classifier;character recognition;parameter optimization;handwritten character recognition;chinese character recognition	"""Prototype learning is e!ective in improving the classi""""cation performance of nearest-neighbor (NN) classi""""er and in reducing the storage and computation requirements. This paper reviews some prototype learning algorithms for NN classi""""er design and evaluates their performance in application to handwritten character recognition. The algorithms include the well-known LVQ and some parameter optimization approaches that aim to minimize an objective function by gradient search. We also propose some new algorithms based on parameter optimization and evaluate their performance together with the existing ones. Eleven prototype learning algorithms are tested in handwritten numeral recognition on the CENPARMI database and in handwritten Chinese character recognition on the ETL8B2 database. The experimental results show that the algorithms based on parameter optimization generally outperform the LVQ. Particularly, the minimum classi""""cation error (MCE) approach of Juang and Katagiri (IEEE Trans. Signal Process. 40 (12) (1992) 3043), the generalized LVQ (GLVQ) of Sato and Yamada (Proceedings of the 14th ICPR, Vol. I, Brisbane, 1998, p. 322) and a new algorithm MAXP1 yield best results. ( 2001 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved."""	computation;gradient descent;handwriting recognition;learning vector quantization;linuxmce;loss function;machine learning;mathematical optimization;nearest neighbour algorithm;nearest-neighbor interpolation;optical character recognition;optimization problem;pattern recognition;prototype;requirement;whole earth 'lectronic link	Cheng-Lin Liu;Masaki Nakagawa	2001	Pattern Recognition	10.1016/S0031-3203(00)00018-2	speech recognition;learning vector quantization;computer science;machine learning;signal processing;pattern recognition;k-nearest neighbors algorithm	AI	19.29204102630527	-38.92896667021287	73526
a364e4a38f545261aa14b3b26323317ce1d22670	an improved multiple birth support vector machine for pattern classification	smoothing technique;twin support vector machine;multi class classification;multiple birth support vector machine;support vector machine	Multiple birth support vector machine is a novel machine learning algorithm for multi-class classification, which is considered as an extension of twin support vector machine. Compared with training speeds of other multiclass classifiers based on twin support vector machine, the training speed of multiple birth support vector machine is faster, especially when the number of class is large. However, one of the disadvantages of multiple birth support vector machine is that when used to deal with some datasets such as “Cross planes” datasets, multiple birth support vector machine is likely to get bad results. In order to deal with this, we propose an improved multiple birth support vector machine. We add a modified item into multiple birth support vector machine to make the variance of the distances from each samples of a given class to their hyperplanes as small as possible. To predict a new sample, our method first determines an interval for each class depending on the distances between training samples and their hyperplanes, and then classifies the new sample depending on the distances between hyperplanes and the new sample which are in the corresponding intervals. In addition, smoothing technique is applied on our model, the first time it was used in multi-class twin support vector machine. The experimental results on artificial datasets and UCI datasets show that the proposed algorithm is efficient and has good classification performance.	algorithm;machine learning;multiclass classification;smoothing;support vector machine	Xiekai Zhang;Shifei Ding	2017	Neurocomputing	10.1016/j.neucom.2016.11.006	support vector machine;kernel method;feature vector;computer science;machine learning;multiclass classification;pattern recognition;data mining;mathematics;relevance vector machine;structured support vector machine	ML	13.77498143363856	-40.286981233273124	73870
fe8e31f43ddd29f8cd0f82b65da2d3f65eac08ca	probabilistic learning of similarity measures for tensor pca	tensor;probabilistic learning;principal component analysis;similarity measure	In order to extract low-dimensional features from image data, matrix-based subspace methods such as 2DPCA and tensor PCA have been recently proposed. Since these methods extract features based on 2D image matrices rather than 1D vectors, they can preserve useful information in image matrices and we can expect better classification performance by using the matrix features. In order to maximize the advantages of the matrix features, it is also important to use an appropriate similarity measure between two feature matrices. This paper proposes a method for learning similarity measures for feature matrices, which utilizes distribution properties of given data set and class membership. Through computational experiments with facial image data, we confirm that the obtained similarity measure by the proposed method can give better classification performance than conventional similarity measures for matrix data.		Kwanyong Lee;Hyeyoung Park	2012	Pattern Recognition Letters	10.1016/j.patrec.2012.03.019	tensor;computer science;machine learning;pattern recognition;data mining;mathematics;principal component analysis	Vision	24.42618970042154	-42.024112811443295	74216
af1b2684989553851c5e90c6b5d3ff08cc4d1e90	an improved multiple fuzzy nnc system based on mutual information and fuzzy integral	mutual information;nearest neighbor classifier;fuzzy integral and fuzzy measure;multiple classifier system;target perturbation	Multiple nearest neighbor classifier system (MNNCS) is a popular method to relax the curse of dimensionality. In previous work, most of the MNNCSs are designed by random methods. Random methods may generate unstable component classifiers. In order to relax the randomness, large amount of component classifiers are needed. This paper first extends nearest neighbor classifier into fuzzy nearest neighbor classifier, and proposes a new multiple fuzzy nearest neighbor classifier system based on mutual information and fuzzy integral, called MIFIMFNNCS. MIFI-MFNNCS adopts target perturbation. Target perturbation decomposes the original classification problem into several sub-problems, where one sub-problem represents one class data. Each sub-problem is described by the relevant data and features. Then it is classified by one component classifier. Therefore, the number of component classifiers can be fixed and reduced. For one component classifier, data may be selected according to its class. And feature is needed to be selected by mutual information. Mutual information can reduce the uncertainty of each component classifier. Feature selection by mutual information in MIFI-MFNNCS may be less affected by the interaction among different classes. The diversity decisions from sub-problem classifiers are combined by fuzzy integral to get the final decision. Here we propose a new method to compute density value according to mutual information, which is a simple method. To demonstrate the performance of the proposed MIFI-MFNNCS, we perform experimental comparisons using five UCI datasets. The results of component classifiers in MIFI-MFNNCS for Ionosphere are shown and analyzed. MIFI-MFNNCS is compared with (1) NNC (2) NNC after feature selection by mutual information (MI-FS-NNC). In multiple fuzzy nearest neighbor classifier system (MFNNCS), mutual information is compared with attribute bagging. And three combination methods are compared, including fuzzy integral, majority voting rule and average. The experimental results show that the accuracy of MIFI-MFNNCS is better than other methods. And mutual information is superior to attribute bagging. Fuzzy integral shows a better performance than majority voting rule and average.	artificial neural network;computation;control theory;curse of dimensionality;feature selection;learning classifier system;mutual information;nearest neighbour algorithm;portable document format;random subspace method;randomness;time complexity	Lijuan Wang	2011	Int. J. Machine Learning & Cybernetics	10.1007/s13042-010-0006-8	machine learning;pattern recognition;data mining;mathematics	AI	11.594001961067448	-41.51740976810933	74607
f3539538840f5cee0c0db46958417749459e0c85	kernel robust bias-aware prediction under covariate shift		Under covariate shift, training (source) data and testing (target) data differ in input space distribution, but share the same conditional label distribution. This poses a challenging machine learning task. Robust Bias-Aware (RBA) prediction provides the conditional label distribution that is robust to the worstcase logarithmic loss for the target distribution while matching feature expectation constraints from the source distribution. However, employing RBA with insufficient feature constraints may result in high certainty predictions for much of the source data, while leaving too much uncertainty for target data predictions. To overcome this issue, we extend the representer theorem to the RBA setting, enabling minimization of regularized expected target risk by a reweighted kernel expectation under the source distribution. By applying kernel methods, we establish consistency guarantees and demonstrate better performance of the RBA classifier than competing methods on synthetically biased UCI datasets as well as datasets that have natural covariate shift.		Anqi Liu;Rizal Fathony;Brian D. Ziebart	2017	CoRR		machine learning;artificial intelligence;kernel (linear algebra);logarithm;kernel method;source data;mathematics;representer theorem;covariate	AI	18.117286417039455	-38.181433887039134	74648
7f802b7a26002887ae33d7209fc62b539ded6392	realizing two-view tsk fuzzy classification system by using collaborative learning	support vector machines;fuzzy systems learning systems support vector machines training optimization correlation inference algorithms;training;learning systems;inference algorithms;optimization;correlation;takagi sugeno kang tsk fuzzy systems collaborative learning fuzzy classification system fcs large margin multiview learning;fuzzy systems	In this paper, a novel Takagi-Sugeno-Kang (TSK) fuzzy classification system (FCS) is firstly presented for pattern classification tasks. It is distinguished by having the large margin criterion properly integrated into its objective function. In order to exploit the applicability of fuzzy systems in multiview scenarios, the proposed TSK-FCS is extended to a two-view version, called two-view TSK-FCS (TwoV-TSK-FCS), by using a collaborative learning mechanism. The adopted collaborative learning mechanism not only fully considers the independent information of each view, but also effectively discovers the correlation information hidden in the two views. Thus, the performance of TwoV-TSK-FCS can be enhanced accordingly. Comprehensive experiments on two-view synthetic and UCI datasets demonstrate the effectiveness of the proposed two-view FCS.	experiment;fuzzy classification;fuzzy control system;loss function;optimization problem;synthetic intelligence	Yizhang Jiang;Zhaohong Deng;Korris Fu-Lai Chung;Shitong Wang	2017	IEEE Transactions on Systems, Man, and Cybernetics: Systems	10.1109/TSMC.2016.2577558	support vector machine;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;machine learning;data mining;correlation;fuzzy set operations;fuzzy control system	AI	20.54249848458506	-41.76425402098594	74673
ddd4287e62bf3bd422e0f372476c8d90ce115803	multifeature, sparse-based approach for defects detection and classification in semiconductor units	bagging;manufacturing automation;image classification;data mining;synthetic minority oversampling technique smote;boosting;stacking;feature extraction;image segmentation feature extraction inspection substrates data mining manufacturing histograms;sparse coding;synthetic minority oversampling technique smote automation bagging boosting data mining feature extraction image classification manufacturing automation sparse coding stacking;automation	Automated inspection systems play an important role in manufacturing to guarantee higher quality and reduce production costs. In the semiconductor manufacturing industry, assembly and testing processes are getting more complex, resulting in a greater tendency of defects to impact the production process. These defects can cause field failures and can result in customer dissatisfactions and returns. Currently available defect detection and classification systems are customized and hard-wired to the detection of particular classes of defects and cannot deal with new unknown classes of defects. This issue is aggravated by the very small sample size of available anomalies for learning, by the data imbalance problem, since the number of defective samples is significantly much smaller than the number of normal samples, and by the presence of noise. This paper presents a novel multifeature, sparse-based defect detection and classification approach that uses the stacking concept to enhance the classification accuracy. The stacking-based classifier is augmented with a novel adaptive over/downsampling technique to deal with the data imbalance problem. A new pruning technique is proposed to eliminate bad base learners. Shortage of defective units, similarities within different classes of defects, wide variation within the same defect class, and data imbalance are the basic challenges to deal with. Experimental results on real-world data from Intel show that the proposed approach results in a high classification accuracy as compared with the existing methods.Note to Practitioners—The basic motivation of this paper is to design an automated cost-effective, adaptive, and intelligent defect detection and classification system that is easy to train using a small-size sample set of defects and that is robust to noise. The system is scalable in terms of the numbers and types of defects and features, which leads to a shorter development cycle. The presented system is immediately applicable to different types of defects. Inputs of the system are grayscale images. These images are processed to perform defects detection, features extraction, and classification.	adaptive sampling;data (computing);decimation (signal processing);focus stacking;grayscale;high-level programming language;machine learning;oversampling;sampling (signal processing);scalability;semiconductor device fabrication;software bug;sparse matrix;statistical classification	Bashar M. Haddad;Sen Yang;Lina J. Karam;Jieping Ye;Nital S. Patel;Martin W. Braun	2018	IEEE Transactions on Automation Science and Engineering	10.1109/TASE.2016.2594288	computer science;boosting (machine learning);automation;sample size determination;feature extraction;scheduling (production processes);image segmentation;scalability;machine learning;artificial intelligence;contextual image classification;pattern recognition	SE	14.792573296482654	-43.85070067437083	74795
74f9d996fff2d0768553f28c0fa76d8601c00c3e	semi-supervised sub-manifold discriminant analysis	libre mercado;sub manifold discriminative embedding;supervised learning;outlier;semi supervised learning;marche concurrentiel;algorithme;discriminant analysis;analyse discriminante;etat actuel;observacion aberrante;algorithm;analisis discriminante;dimensionality reduction;state of the art;pattern recognition;observation aberrante;estado actual;reconnaissance forme;apprentissage supervise;open market;reconocimiento patron;aprendizaje supervisado;dimensional reduction;algoritmo	In this paper, we present a semi-supervised sub-manifold discriminant analysis algorithm. To separate each sub-manifold constructed by each class, we define the within-manifold scatter, between-manifold scatter and total-manifold scatter matrices. The scatter matrices are robust to outlier and diverse-density clusters. Kernelization and direct non-linear embedding are also developed. Experimental results show that our approach can give competitive results in comparison to the state-of-the-art algorithms. 2008 Elsevier B.V. All rights reserved.	algorithm;arc diagram;kernelization;linear discriminant analysis;nonlinear system;robustness (computer science);semi-supervised learning;semiconductor industry	Yangqiu Song;Feiping Nie;Changshui Zhang	2008	Pattern Recognition Letters	10.1016/j.patrec.2008.05.024	outlier;open market operation;computer science;artificial intelligence;machine learning;pattern recognition;mathematics;supervised learning;dimensionality reduction	AI	23.832049730227094	-39.26246358820218	74881
35c267191e3a6171cc0714c0daea2fc78bcf9b3a	learning and using taxonomies for fast visual categorization	classification trees;fast visual categorization;kernel;classification algorithm;classification tree;support vector machines;image databases;trees mathematics computational complexity greedy algorithms pattern classification;training;greedy algorithms;testing;trees mathematics;sublinear classification costs;indexes;visualization;computational complexity;classification algorithms;taxonomy;pattern classification;greedy algorithm;fasteners;taxonomy costs classification tree analysis testing computational complexity greedy algorithms classification algorithms fasteners indexes image databases;classification tree analysis;greedy algorithm fast visual categorization computational complexity sublinear classification costs classification trees	The computational complexity of current visual categorization algorithms scales linearly at best with the number of categories. The goal of classifying simultaneously Ncat = 104 - 105 visual categories requires sub-linear classification costs. We explore algorithms for automatically building classification trees which have, in principle, logNcat complexity. We find that a greedy algorithm that recursively splits the set of categories into the two minimally confused subsets achieves 5-20 fold speedups at a small cost in classification performance. Our approach is independent of the specific classification algorithm used. A welcome by-product of our algorithm is a very reasonable taxonomy of the Caltech-256 dataset.	categorization;computational complexity theory;decision tree;greedy algorithm;linear classifier;recursion;taxonomy (general);netcat	Gregory Griffin;Pietro Perona	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2008.4587410	greedy algorithm;computer science;machine learning;pattern recognition;data mining;mathematics;taxonomy	Vision	17.57144802699253	-45.70382496455778	75042
80f3ec80d1114bcd9a8ad9b56dd7b230ec16c048	parameter learning with truncated message-passing	pixel labeling benchmark;truncated message passing;message passing iterations;convergence training message passing image resolution computational modeling accuracy approximation algorithms;inference mechanisms;conditional random fields;pixel labeling benchmark parameter learning truncated message passing conditional random fields message passing inference double loop procedure message passing iterations;statistical analysis inference mechanisms learning artificial intelligence message passing;parameter learning;double loop procedure;statistical analysis;message passing inference;conditional random field;message passing;learning artificial intelligence	Training of conditional random fields often takes the form of a double-loop procedure with message-passing inference in the inner loop. This can be very expensive, as the need to solve the inner loop to high accuracy can require many message-passing iterations. This paper seeks to reduce the expense of such training, by redefining the training objective in terms of the approximate marginals obtained after message-passing is “truncated” to a fixed number of iterations. An algorithm is derived to efficiently compute the exact gradient of this objective. On a common pixel labeling benchmark, this procedure improves training speeds by an order of magnitude, and slightly improves inference accuracy if a very small number of message-passing iterations are used at test time.	approximation algorithm;benchmark (computing);conditional random field;definition;gradient;inner loop;iteration;message passing;pixel	Justin Domke	2011	CVPR 2011	10.1109/CVPR.2011.5995320	computer science;theoretical computer science;machine learning;distributed computing;conditional random field	Vision	23.339099322175585	-47.55184851478833	75066
5ac987c3b5c697535eb124826a707daaeeb360a2	fast statistical learning with kernel-based simple-fda	fisher linear discriminant analysis;kernel;learning algorithm;pattern recognition statistical learning kernel function simple fda face recognition;principal component analysis covariance matrices learning artificial intelligence;iterative learning algorithms;kernel function;face image analysis;simple fda;statistical learning kernel covariance matrix iterative algorithms principal component analysis pattern recognition neural networks linear discriminant analysis image analysis face recognition;statistical learning;face recognition;general methods;large sized covariance matrix fast statistical learning kernel function iterative learning algorithms principal component analysis fisher linear discriminant analysis face image analysis;space use;covariance matrices;principal component analysis;pattern recognition;image analysis;face;learning artificial intelligence;large sized covariance matrix;fast statistical learning;algorithm design and analysis;eigenvectors;covariance matrix;neural network	In this paper, new statistical learning algorithms with kernel function are presented. Recently, iterative learning algorithms for obtaining eigenvectors in the principal component analysis (PCA) have been presented in the field of pattern recognition and neural network. However, the Fisher linear discriminant analysis (FLDA) has been used in many fields, especially face image analysis. The drawback of FLDA is a long computational time based on a large-sized covariance matrix and the issue that the within-class covariance matrix is usually singular. In order to overcome this difficulty, we proposed the feature generation method simple-FLDA which is approximately derived from geometrical interpretation of FLDA. This algorithm is similar to simple-PCA and does not need matrix operation. In this paper, new statistical kernel based learning algorithms are presented. They are extended versions of simple-PCA and simple-FLDA to nonlinear space using the kernel function. Their preliminary simulation results are given for a simple face recognition problem.	algorithm;artificial neural network;computation;computer simulation;computer vision;eigen (c++ library);facial recognition system;image analysis;iterative method;kernel (operating system);linear discriminant analysis;machine learning;nonlinear system;numerical linear algebra;pattern recognition;principal component analysis;the matrix;time complexity	Kazuhiro Nakaura;Stephen Karungaru;Takuya Akashi;Yasue Mitsukura;Minoru Fukumi	2008	2008 IEEE International Conference on Signal Image Technology and Internet Based Systems	10.1109/SITIS.2008.52	kernel;facial recognition system;face;algorithm design;computer vision;kernel method;covariance matrix;kernel;image analysis;eigenvalues and eigenvectors;kernel principal component analysis;computer science;machine learning;pattern recognition;artificial neural network;statistics;principal component analysis	Robotics	24.56965363965244	-39.79639215572437	75645
1a71392d4c0a2de0ad739b1e773a847eab9edccb	domain adaptation meets disentangled representation learning and style transfer.		Many methods have been proposed to solve the domain adaptation problem recently. However, the success of them implicitly funds on the assumption that the information of domains are fully transferrable. If the assumption is not satisfied, the effect of negative transfer may degrade domain adaptation. In this paper, a better learning network has been proposed by considering three tasks domain adaptation, disentangled representation, and style transfer simultaneously. Firstly, the learned features are disentangled into common parts and specific parts. The common parts represent the transferrable features, which are suitable for domain adaptation with less negative transfer. Conversely, the specific parts characterize the unique style of each individual domain. Based on this, the new concept of feature exchange across domains, which can not only enhance the transferability of common features but also be useful for image style transfer, is introduced. These designs allow us to introduce five types of training objectives to realize the three challenging tasks at the same time. The experimental results show that our architecture can be adaptive well to full transfer learning and partial transfer learning upon a well-learned disentangled representation. Besides, the trained network also demonstrates high potential to generate style-transferred images.	domain adaptation;machine learning	Hoang Tran Vu;Ching-Chun Huang	2017	CoRR		machine learning;pattern recognition;transfer of learning;artificial intelligence;architecture;transferability;computer science;domain adaptation;negative transfer;feature learning	AI	24.587329073190638	-49.02280920424615	75841
8a56c6f6efb289b25897f0db237e6c099561d904	select-bagging: effectively combining gene selection and bagging for balanced bioinformatics data	statistical analysis bayes methods bioinformatics feature selection filtering theory genetics learning artificial intelligence pattern classification;feature selection high dimensionality ensemble bagging;high dimensionality;measurement;bagging;training;biological system modeling;ensemble;computational modeling;bagging bioinformatics biological system modeling training measurement data models computational modeling;statistical analysis select bagging gene selection gene bagging machine learning ensemble learning feature selection training dataset fs classifier balanced bioinformatics datasets filter based feature rankers feature subset sizes naive bayes classifier;feature selection;data models;bioinformatics	Bioinformatics datasets have historically been difficult to work with. However, within machine learning, there is a potentially effective tool to combat such problems: ensemble learning. Ensemble learning generates a series of models and combines their results to make a single decision. This process has the benefit of utilizing the power of multiple models but the overhead of having to compute the multiple models. Thus, we must ask whether the benefits outweigh the detriments. In this study, we seek to determine if the ensemble learning technique Select-Bagging improves classification results over feature selection on the training dataset followed by classification (denoted as FS-Classifier in this work) on a series of balanced bioinformatics datasets. We test the two approaches with two filter-based feature rankers, four feature subset sizes and the Naïve Bayes classifier. Our results show that Select-Bagging clearly outperforms FS-Classifier for nearly all scenarios. Subsequent statistical analysis shows that the increase in performance generated by Select-Bagging is statistically significantly better than FS-Classifier. Therefore, we can state that the inclusion of Select-Bagging is beneficial to the classification performance of models built on high-dimensional and balanced bioinformatics datasets and should be implemented. To our knowledge this is the first study which looks at the effectiveness of bagging in conjunction with internal feature selection for balanced bioinformatics datasets.	analysis of algorithms;bioinformatics;boosting (machine learning);computation;ensemble learning;feature selection;machine learning;naive bayes classifier;overhead (computing);statistical classification	David J. Dittman;Taghi M. Khoshgoftaar;Amri Napolitano;Alireza Fazelpour	2014	2014 IEEE International Conference on Bioinformatics and Bioengineering	10.1109/BIBE.2014.66	ensembl;data modeling;bootstrap aggregating;computer science;bioinformatics;machine learning;pattern recognition;data mining;ensemble learning;computational model;feature selection;measurement	SE	13.440769507937828	-42.27108602589547	76032
b23da38bfb3687424de4924efd87904a9cae303e	visual representation and classification by learning group sparse deep stacking network		"""Deep stacking networks (DSNs) have been successfully applied in classification tasks. Its architecture builds upon blocks of simplified neural network modules (SNNM). The hidden units are assumed to be independent in the SNNM module. However, this assumption prevents SNNM from learning the local dependencies between hidden units to better capture the information in the input data for the classification task. In addition, the hidden representations of input data in each class can be expectantly split into a group in real-world classification applications. Therefore, we propose two kinds of group sparse SNNM modules by mixing <inline-formula> <tex-math notation=""""LaTeX"""">$l_{1}$ </tex-math></inline-formula>-norm and <inline-formula> <tex-math notation=""""LaTeX"""">$l_{2}$ </tex-math></inline-formula>-norm. The first module learns the local dependencies among hidden units by dividing them into non-overlapping groups. The second module splits the representations of samples in different classes into separate groups to cluster the samples in each class. A group sparse DSN (GS-DSN) is constructed by stacking the group sparse SNNM modules. Experimental results further verify that our GS-DSN model outperforms the relevant classification methods. Particularly, GS-DSN achieves the state-of-the-art performance (99.1%) on 15-Scene."""	artificial neural network;assumed;biological neural networks;class;computer vision;face;facial nerve diseases;facial recognition system;focus stacking;gorasp1 gene;gs/os;gradient descent;network model;outline of object recognition;roland gs;sparse matrix;weight	Jun Li;Heyou Chang;Jian Yang;Wei Luo;Yun Fu	2018	IEEE Transactions on Image Processing	10.1109/TIP.2017.2765833	stacking;encoding (memory);architecture;artificial neural network;pattern recognition;machine learning;artificial intelligence;computer science;division (mathematics)	ML	24.55327192721128	-47.68499914396844	76394
dfb2433fa46ea6a5bb71c47f75cc55ceeb8310e0	research on svm and flda in classification with comparative experiments	multiclass;experimental analysis;performance indicator;support vector machines;magnetic heads;support vector machine svm;training;pattern classification flda svm comparative experiments classification techniques fisher linear discriminated analysis support vector machine;testing;linear discriminate analysis;fisher s linear discriminated analysis flda;accuracy;vectors;support vector machine svm pattern classification multiclass fisher s linear discriminated analysis flda;support vector machines training testing accuracy vectors head magnetic heads;pattern classification;head;support vector machine;classification accuracy;support vector machines pattern classification	The paper discusses two important classification techniques, Fisher's linear discriminated analysis (FLDA) and Support Vector Machine (SVM). First, we propose a theoretical discussion, and then implement FLDA and SVM on several datasets of two classes and multiclass, a comparative experimental analysis among these two techniques aims at exploring and assessing the performance of FLDA and SVM classifiers. To sustain such analysis, the two classification techniques are compared with different training data sets and testing data sets. Different performance indicators have been used to support our experimental studies in a detailed and accurate way such as the classification accuracy. The results obtained on different datasets conclude that FLDA and SVM are valid and effective approaches for pattern classification and conclude their different performance and problems with different size datasets. Meanwhile, the paper employs a non-traditional method to get the training and testing data set, and concludes detailed pros and cons from the experiment results.	experiment;support vector machine;test set	Yegan Qian;Gang Xiong;Yanjie Yao	2012	Proceedings of 2012 9th IEEE International Conference on Networking, Sensing and Control	10.1109/ICNSC.2012.6204955	support vector machine;computer science;machine learning;pattern recognition;data mining	Visualization	12.245528988525916	-42.672823149699994	76475
8f4c766a70cc0d011c9dac335f1d4305ede07f6b	deep class-aware image denoising		The increasing demand for high image quality in mobile devices brings forth the need for better computational enhancement techniques, and image denoising in particular. To this end, we propose a new fully convolutional deep neural network architecture which is simple yet powerful and achieves state-of-the-art performance for additive Gaussian noise removal. Furthermore, we claim that the personal photo-collections can usually be categorized into a small set of semantic classes. However simple, this observation has not been exploited in image denoising until now. We show that a significant boost in performance of up to 0.4dB PSNR can be achieved by making our network class-aware, namely, by fine-tuning it for images belonging to a specific semantic class. Relying on the hugely successful existing image classifiers, this research advocates for using a class-aware approach in all image enhancement tasks.	artificial neural network;categorization;classful network;deep learning;image editing;image quality;mobile device;network architecture;noise reduction;peak signal-to-noise ratio;utility functions on indivisible goods	Tal Remez;Or Litany;Raja Giryes;Alexander M. Bronstein	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296611	architecture;convolutional code;computer vision;noise measurement;noise reduction;image quality;artificial neural network;artificial intelligence;gaussian noise;computer science;mobile device	Vision	23.716829090392757	-50.714722750794614	76481
8a3d7517690304563b8369bdb0c84aa433ce273b	fuzziness-based online sequential extreme learning machine for classification problems		The qualities of new data used in the sequential learning phase of the online sequential extreme learning machine algorithm (OS-ELM) have a significant impact on the performance of OS-ELM. This paper proposes a novel data filter mechanism for OS-ELM from the perspective of fuzziness and a fuzziness-based online sequential extreme learning machine algorithm (FOS-ELM). In FOS-ELM, when new data arrive, a fuzzy classifier first picks out the meaningful data according to the fuzziness of each sample. Specifically, the new samples with high-output fuzziness are selected and then used in sequential learning. The experimental results on eight binary classification problems and three multiclass classification problems have shown that FOS-ELM updated by the new samples with high-output fuzziness has better generalization performance than OS-ELM. Since the unimportant data are discarded before sequential learning, FOS-ELM can save more memory and have higher computational efficiency. In addition, FOS-ELM can handle data one-by-one or chunk-by-chunk with fixed or varying sizes. The relationship between the fuzziness of new samples and the model performance is also studied in this paper, which is expected to provide some useful guidelines for improving the generalization ability of online sequential learning algorithms.		Weipeng Cao;Jinzhu Gao;Zhong Ming;Shubin Cai;Zhiguang Shan	2018	Soft Comput.	10.1007/s00500-018-3021-4	machine learning;fuzzy logic;artificial intelligence;extreme learning machine;computer science;binary classification;sequence learning;multiclass classification	AI	13.477721321675633	-38.477798981662936	76526
786a762bbcb661d2265d1417ab6dd8490f57bddc	universal perturbation generation for black-box attack using evolutionary algorithms		Image classifiers based on deep neural networks (DNNs) are vulnerable to tiny, imperceptible perturbations. Maliciously generated adversarial examples can exploit the instability of DNNs and mislead it into outputting a wrong classification result. Prior works showed the transferability of adversarial perturbations between models and between images. In this work, we shed light on the combination of source/target misclassification, black-box attack, and universal perturbation by employing improved evolutionary algorithms. We additionally find that the use of adversarial initialization enhances the efficiency of evolutionary algorithms finding universal perturbations. Experiments demonstrate impressive misclassification rates and surprising transferability for the proposed attack method using different models trained on CIFAR-10 and CIFAR-100 datasets. Our attach method also shows robustness against defensive measures like adversarial training.		Sivy Wang;Yucheng Shi;Yahong Han	2018	2018 24th International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2018.8546023	evolutionary computation;robustness (computer science);artificial intelligence;evolutionary algorithm;adversarial system;artificial neural network;initialization;instability;exploit;pattern recognition;computer science	Vision	19.26187581102935	-51.39866932439796	76584
1b33d1f6f99cec07860cc55254387eb7779bf320	transfer learning from unlabeled data via neural networks	neural networks;mapping function;期刊论文;transfer learning;base transfer	A machine learning framework which uses unlabeled data from a related task domain in supervised classification tasks is described. The unlabeled data come from related domains, which share the same class labels or generative distribution as the labeled data. Patterns in the unlabeled data are learned via a neural network and transferred to the target domain from where the labeled data are generated, so as to improve the performance of the supervised learning task. We call this approach self-taught transfer learning from unlabeled data. We introduce a general-purpose feature learning algorithm producing features that retain information from the unlabeled data. Information preservation assures that the features obtained will be useful for improving the classification performance of the supervised tasks.	algorithm;artificial neural network;case preservation;feature learning;general-purpose modeling;machine learning;neural network software;supervised learning	Huaxiang Zhang;Hua Ji;Xiaoqin Wang	2012	Neural Processing Letters	10.1007/s11063-012-9229-x	semi-supervised learning;unsupervised learning;multi-task learning;transfer of learning;computer science;machine learning;pattern recognition;data mining;supervised learning;artificial neural network	ML	22.239599640590047	-45.19037299170475	76835
afb00e6bc19fe0374e60fd909b272b4ef1cb94bb	an object recognition algorithm using maximum margin correlation filter and support vector machine	object recognition;object localization object recognition support vector machine maximum margin correlation filter;object localization;support vector machines correlation training vectors testing complexity theory search problems;maximum margin correlation filter;support vector machine	We consider the problem of detecting objects in two dimensional images and propose a new technique which uses Support Vector Machine (SVM) along with Maximum Margin Correlation Filter (MMCF). We have shown that our algorithm detects objects well and is robust with respect to scale changes. Introduction of Support Vector Machine (SVM) helps Maximum Margin Correlation Filter (MMCF) to deal with non-linearly separable data also to some extent. The algorithm also detects same object, if it is found several times at several scales, thus it helps avoiding redundant detection of same object and finally selects the best version of it.	algorithm;linear separability;outline of object recognition;sensor;support vector machine	Samya Bagchi;P. G. Poonacha	2014	2014 Twentieth National Conference on Communications (NCC)	10.1109/NCC.2014.6811272	margin classifier;computer vision;machine learning;pattern recognition;mathematics;relevance vector machine;structured support vector machine	ML	15.116018287707206	-45.47786988883779	76922
9560d6985eb636c30f600a3bd6d36fa315cb9a6c	evolutionary wrapper approaches for training set selection as preprocessing mechanism for support vector machines: experimental evaluation and support vector analysis	training set selection;statistical comparisons;nearest neighbor rule;support vector machines;instance selection;selection;interpretability;classification;multiple data sets;algorithms;classifiers;data reduction;prototype	One of the most powerful, popular and accurate classification techniques is support vector machines (SVMs). In this work, we want to evaluate whether the accuracy of SVMs can be further improved using training set selection (TSS), where only a subset of training instances is used to build the SVM model. By ccepted 3 September 2015 vailable online 30 September 2015	preprocessor;support vector machine;test set	Nele Verbiest;Joaquín Derrac;Chris Cornelis;Salvador García;Francisco Herrera	2016	Appl. Soft Comput.	10.1016/j.asoc.2015.09.006	support vector machine;selection;data reduction;biological classification;computer science;machine learning;pattern recognition;data mining;prototype	Web+IR	11.356441890230117	-41.52824592794575	76960
8d44310637cd1443f6292ce0ec419b4c1154822b	fast label embeddings via randomized linear algebra		Many modern multiclass and multilabel problems are characterized by increasingly large output spaces. For these problems, label embeddings have been shown to be a useful primitive that can improve computational and statistical efficiency. In this work we utilize a correspondence between rank constrained estimation and low dimensional label embeddings that uncovers a fast label embedding algorithm which works in both the multiclass and multilabel settings. The result is a randomized algorithm whose running time is exponentially faster than naive algorithms. We demonstrate our techniques on two large-scale public datasets, from the Large Scale Hierarchical Text Challenge and the Open Directory Project, where we obtain state of the art results.	apple open directory;computation;linear algebra;randomized algorithm;time complexity	Paul Mineiro;Nikos Karampatziakis	2015		10.1007/978-3-319-23528-8_3	mathematical optimization;combinatorics;machine learning;mathematics	ML	17.620916813691906	-46.799616437844485	77043
3ae1b29e6628be40f8c143b3ed6cecdf9935725e	lightweight lipschitz margin training for certified defense against adversarial examples		How can we make machine learning provably robust against adversarial examples in a scalable way? Since certified defense methods, which ensure -robust, consume huge resources, they can only achieve small degree of robustness in practice. Lipschitz margin training (LMT) is a scalable certified defense, but it can also only achieve small robustness due to over-regularization. How can we make certified defense more efficiently? We present LC-LMT, a light weight Lipschitz margin training which solves the above problem. Our method has the following properties; (a) efficient: it can achieve -robustness at early epoch, and (b) robust: it has a potential to get higher robustness than LMT. In the evaluation, we demonstrate the benefits of the proposed method. LC-LMT can achieve required robustness more than 30 epoch earlier than LMT in MNIST, and shows more than 90 % accuracy against both legitimate and adversarial inputs.	emoticon;epoch (reference date);logistic model tree;mnist database;machine learning;matrix regularization;robustness (computer science);scalability	Hajime Ono;Tsubasa Takahashi;Kazuya Kakizaki	2018	CoRR			ML	19.114064682819464	-50.92067502124437	77046
28d590513ecae1917b4817a030e08e8ba695f889	assessing binary classifiers using only positive and unlabeled data		Assessing the performance of a learned model is a crucial part of machine learning. Most evaluation metrics can only be computed with labeled data. Unfortunately, in many domains we have many more unlabeled than labeled examples. Furthermore, in some domains only positive and unlabeled examples are available, in which case most standard metrics cannot be computed at all. In this paper, we propose an approach that is able to estimate several widely used metrics including ROC and PR curves using only positive and unlabeled data. We provide theoretical bounds on the quality of our estimates. Empirically, we demonstrate that even given only a small number of positive examples and unlabeled data, we are able to reliable estimate both ROC and PR curves.	binary classification;contingency table;experiment;film-type patterned retarder;machine learning;receiver operating characteristic;semiconductor industry;statistical classification	Marc Claesen;Jesse Davis;Frank De Smet;Bart De Moor	2015	CoRR		pattern recognition;data mining;mathematics;statistics	ML	17.47334562961599	-40.01283858655236	77328
d78cd41e111e6a7152c8200af7c26883f7451d64	generic proposal evaluator: a lazy learning strategy toward blind proposal quality assessment		Existing detection or recognition systems typically select one state-of-the-art proposal algorithm to produce massive object-covered candidate windows, and a quality metric specifically designed for this algorithm is utilized to single out small amounts of proposals. However, in practice, the accuracies of different proposal algorithms significantly change from one image content to another one. To obtain more robust proposal results, a generic proposal evaluator (GPE) is highly desired, which could choose optimal candidate windows across multiple proposal algorithms. In this paper, we propose a lazy learning strategy to train the GPE, which aims to blindly estimate the quality of each proposal without accessing to its manual annotation. Unlike the traditional end-to-end framework that learns a universal model from all training samples, we try to build query-specific training subset for each given proposal, where only its  ${k}$ -nearest-neighborhoods are collected from all labeled candidate windows. Benefits from the capability of updating the regression parameters for different visual contents, the proposed method delivers a higher quality prediction accuracy even with respect to the deep neural network learned by end-to-end method. Experimental results confirm that the proposed algorithm significantly outperforms many state-of-the-art proposal quality metrics.	algorithm;artificial neural network;deep learning;end-to-end encryption;end-to-end principle;interpreter (computing);lazy evaluation;lazy learning;microsoft windows;sampling (signal processing);sorting;standard test image	Qingbo Wu;Hongliang Li;Fanman Meng;King Ngi Ngan	2018	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2017.2750070	lazy learning;artificial neural network;data mining;visualization;algorithm design;annotation;computer science	Vision	17.148148703120967	-44.04302095825919	77504
6c5a70223d06ffa709899656feeb3d47c6a6b674	improving accuracy and speed of optimum-path forest classifier using combination of disjoint training subsets	forest classifiers;combination of classifiers;combining schemes;optimum path forest classifier;fast methods;training algorithms;distributed combination of classifiers;combining method;set theory;final decision;training sets;random sample;training subsets;learning procedures;trabalho apresentado em evento;algorithms;majority vote;pasting small votes;pattern recognition systems;fixed numbers;parallel or distributed processing;real data sets	The Optimum-Path Forest (OPF) classifier is a recent and promising method for pattern recognition, with a fast training algorithm and good accuracy results. Therefore, the investigation of a combining method for this kind of classifier can be important for many applications. In this paper we report a fast method to combine OPF-based classifiers trained with disjoint training subsets. Given a fixed number of subsets, the algorithm chooses random samples, without replacement, from the original training set. Each subset accuracy is improved by a learning procedure. The final decision is given by majority vote. Experiments with simulated and real data sets showed that the proposed combining method is more efficient and effective than naive approach provided some conditions. It was also showed that OPF training step runs faster for a series of small subsets than for the whole training set. The combining scheme was also designed to support parallel or distributed processing, speeding up the procedure even more.	algorithm;boosting (machine learning);bootstrap aggregating;distributed computing;experiment;futures studies;machine learning;multi-core processor;multiprocessing;parallel computing;pattern recognition;pseudo-random number sampling;test set;time complexity	Moacir P. Ponti;João Paulo Papa	2011		10.1007/978-3-642-21557-5_26	machine learning;pattern recognition;data mining;mathematics	ML	14.278920810994746	-40.533574548184376	77505
a6293fc91be733f01266413cbeb0e4385afdee5f	explaining black-box classifiers with ilp - empowering lime with aleph to approximate non-linear decisions with relational rules		We propose an adaption of the explanation-generating system LIME. While LIME relies on sparse linear models, we explore how richer explanations can be generated. As application domain we use images which consist of a coarse representation of ancient graves. The graves are divided into two classes and can be characterised by meaningful features and relations. This domain was generated in analogy to a classic concept acquisition domain researched in psychology. Like LIME, our approach draws samples around a simplified representation of the instance to be explained. The samples are labelled by a generator – simulating a black-box classifier trained on the images. In contrast to LIME, we feed this information to the ILP system Aleph. We customised Aleph’s evaluation function to take into account the similarity of instances. We applied our approach to generate explanations for different variants of the ancient graves domain. We show that our explanations can involve richer knowledge thereby going beyond the expressiveness of sparse linear models.	application domain;black box;concept learning;convolution;evaluation function;existential quantification;farid f. abraham;limewire;linear model;self-propelled particles;simulation;sparse matrix;universal instantiation	Johannes Rabold;Michael Siebers;Ute Schmid	2018		10.1007/978-3-319-99960-9_7	application domain;artificial intelligence;machine learning;linear model;lime;classifier (linguistics);analogy;evaluation function;nonlinear system;computer science;aleph	Vision	16.888267281570695	-49.77350696798532	77562
a5e97e55091ae047ffd914388035cb010bd18d4a	sparse hidden units activation in restricted boltzmann machine		Sparsity has become a concept of interest in machine learning for many years. In deep learning sparse solutions play crucial role in obtaining robust and discriminative features. In this paper, we study a new regularization term for sparse hidden units activation in the context of Restricted Boltzmann Machine (RBM). Our proposition is based on the symmetric Kullback-Leibler divergence applied to compare the actual and the desired distribution over the active hidden units. We compare our method against two other enforcing sparsity regularization terms by evaluating the empirical classification error using two datasets: (i) for image classification (MNIST), (ii) for document classification (20newsgroups).	computer vision;deep learning;document classification;experiment;kullback–leibler divergence;mnist database;machine learning;manifold regularization;matrix regularization;restricted boltzmann machine;sparse matrix	Jakub M. Tomczak;Adam Gonczarek	2014		10.1007/978-3-319-08422-0_27	computer science;theoretical computer science;machine learning;pattern recognition;restricted boltzmann machine	ML	22.705095590063813	-44.97149926389261	77907
bd43ec4e704b8d66834c54ad838675acdb366f51	enhancing semi-supervised learning through label-aware base kernels	label aware base kernels;multiple kernel learning;semi supervised learning;kernel target alignment	"""Currently, a large family of kernel design methods for semi-supervised learning (SSL) problems builds the kernel by weighted averaging of predefined base kernels (i.e., those spanned by kernel eigenvectors). Optimization of the base kernel weights has been studied extensively in the literature. However, little attention was devoted to designing high-quality base kernels. The eigenvectors of the kernel matrix, which are computed irrespective of class labels, may not always reveal useful structures of the target. As a result, the generalization performance can be poor however hard the base kernel weighting is tuned. On the other hand, there are many SSL algorithms whose focus are not on kernel design but on the estimation of the class labels directly. Motivated by the label propagation approach, in this paper we propose to construct novel kernel eigenvectors by injecting the class label information under the framework of eigenfunction extrapolation. A set of """"label-aware"""" base kernels can be obtained with greatly improved quality, which leads to higher target alignment and henceforth better performance. Our approach is computationally efficient, and demonstrates encouraging performance in semi-supervised classification and regression tasks."""	semi-supervised learning;semiconductor industry;supervised learning	Qiaojun Wang;Kai Zhang;Zhengzhang Chen;Dequan Wang;Guofei Jiang;Ivan Marsic	2016	Neurocomputing	10.1016/j.neucom.2015.07.072	semi-supervised learning;kernel method;mathematical optimization;string kernel;kernel embedding of distributions;radial basis function kernel;computer science;machine learning;pattern recognition;graph kernel;mathematics;tree kernel;variable kernel density estimation;polynomial kernel;kernel smoother	ML	18.737378235788096	-40.38511759379672	77948
62e6ea3072546a73a1871aee606c885bc174d5e2	a new line symmetry distance based pattern classifier	pattern classification problems;nearest neighbor searches;nearest neighbor rule;complexity theory;training;training patterns;anova;symmetry based distance;distance measurement;artificial neural networks;statistical analysis;distance measurement nearest neighbor searches artificial neural networks complexity theory training equations classification algorithms;line symmetry;nearest neighbor;classification algorithms;kd tree;pattern classification;statistical analysis pattern classification;pattern classifier;nearest neighbor search;line symmetry based classifier;classification accuracy;line symmetry pattern classification kd tree symmetry based distance nearest neighbor rule;line symmetry distance;anova line symmetry distance pattern classifier pattern classification problems line symmetry based classifier training patterns nearest neighbor search statistical analysis	In this paper, a new line symmetry based classifier (LSC) is proposed to deal with pattern classification problems. In order to measure total amount of line symmetry of a particular point in a class, a new definition of line symmetry based distance is also proposed in this paper. The proposed line symmetry based classifier (LSC) utilizes this new definition of line symmetry distance for classifying an unknown test sample. LSC assigns an unknown test sample pattern to that class with respect to whose major axis it is most symmetric. The mean of all the training patterns belonging to that particular class is taken as the prototype of that class. Thus training constitutes of computing only the class prototypes and the major axes of those classes. Kd-tree based nearest neighbor search is used for reducing complexity of line symmetry distance computation. The performance of LSC is demonstrated in classifying twelve artificial and real-life data sets of varying complexities. Experimental results show that LSC achieves, in general, higher classification accuracy compared to k-NN classifier. Results indicate that the proposed novel line symmetry based classifier is well-suited for classifying data sets having symmetrical classes, irrespective of any convexity, overlap and size. Statistical analysis, ANOVA is also performed to compare the performance of these classifications techniques.	apache axis;computation;k-nearest neighbors algorithm;nearest neighbor search;prototype;real life;software prototyping;statistical classification	Sriparna Saha;Sanghamitra Bandyopadhyay;Chingtham Tejbanta Singh	2008	2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)	10.1109/IJCNN.2008.4633984	statistical classification;combinatorics;analysis of variance;computer science;machine learning;pattern recognition;k-d tree;mathematics;nearest neighbor search;k-nearest neighbors algorithm;artificial neural network	Vision	15.197088938296865	-44.687557738034535	78005
a8af3b067150ba21bc6d420ae9a5002806dd4f08	weighted symbols-based edit distance for string-structured image classification	information retrieval;triangle inequality;tree edit distance;edit distance;image classification;dynamic program;classification accuracy;string edit distance	As an alternative to vector representations, a recent trend in image classification suggests to integrate additional structural information in the description of images in order to enhance classification accuracy. Rather than being represented in a p-dimensional space, images can typically be encoded in the form of strings, trees or graphs and are usually compared either by computing suited metrics such as the (string or tree)-edit distance, or by testing subgraph isomorphism. In this paper, we propose a new way for representing images in the form of strings whose symbols are weighted according to a TF-IDF-based weighting scheme, inspired from information retrieval. To be able to handle such real-valued weights, we first introduce a new weighted string edit distance that keeps the properties of a distance. In particular, we prove that the triangle inequality is preserved which allows the computation of the edit distance in quadratic time by dynamic programming. We show on an image classification task that our new weighted edit distance not only significantly outperforms the standard edit distance but also seems very competitive in comparison with standard histogram distances-based approaches.	computation;computer vision;dynamic programming;edit distance;information retrieval;social inequality;string (computer science);subgraph isomorphism problem;tf–idf;time complexity	Cécile Barat;Christophe Ducottet;Élisa Fromont;Anne-Claire Legrand;Marc Sebban	2010		10.1007/978-3-642-15880-3_11	contextual image classification;damerau–levenshtein distance;discrete mathematics;edit distance;computer science;wagner–fischer algorithm;machine learning;pattern recognition;triangle inequality;mathematics;string-to-string correction problem;distance;jaro–winkler distance	Vision	18.94628008474313	-47.12033676295451	78287
05c4db530898a52edbdc126844ae9edaa6080ebe	binary classification with a pseudo exponential model and its application for multi-task learning	itakura saito distance;pseudo model;multi task learning;un normalized model	In this paper, we investigate the basic properties of binary classification with a pseudo model based on the Itakura–Saito distance and reveal that the Itakura–Saito distance is a unique appropriate measure for estimation with the pseudo model in the framework of general Bregman divergence. Furthermore, we propose a novel multi-task learning algorithm based on the pseudo model in the framework of the ensemble learning method. We focus on a specific setting of the multi-task learning for binary classification problems. The set of features is assumed to be common among all tasks, which are our targets of performance improvement. We consider a situation where the shared structures among the dataset are represented by divergence between underlying distributions associated with multiple tasks. We discuss statistical properties of the proposed method and investigate the validity of the proposed method with numerical experiments.	algorithm;binary classification;bregman divergence;computer multitasking;ensemble learning;experiment;itakura–saito distance;multi-task learning;numerical analysis	Takashi Takenouchi;Osamu Komori;Shinto Eguchi	2015	Entropy	10.3390/e17085673	multi-task learning;machine learning;pattern recognition;mathematics;statistics	AI	23.306494222255793	-43.39742323354897	78336
9a01026d63a0533786e39dff95df5463e608a387	sparse dual graph-regularized nmf for image co-clustering		Abstract Nonnegative matrix factorization (NMF) as fundamental technique for clustering has been receiving more and more attention. This is because it can effectively reduce high dimensional data and produce parts-based, linear image representations of nonnegative data. For practical clustering tasks, NMF ignores the geometric structures of both data manifold and feature manifold. In addition, recent research results showed that leveraging sparseness can greatly improve the ability of learning parts. Motivated by the two aspects above mentioned, we propose a novel co-clustering algorithm to enhance the clustering performance, called sparse dual graph-regularized nonnegative matrix factorization (SDGNMF). It aims for finding a parts-based, linear representation of the non-negative data and facilitating the learning tasks. SDGNMF jointly incorporates the dual graph-regularized and sparseness constraints as additional conditions to uncover the intrinsic geometrical, discriminative structures of the data space and feature space. The iterative updating scheme for the optimization problem of SDGNMF and its convergence proofs are also given in detail. Experimental results of clustering on three benchmark datasets demonstrated that SDGNMF algorithm outperforms the compared state-of-the-art methods in image co-clustering.	biclustering;cluster analysis;dual graph;non-negative matrix factorization;sparse	Jing Sun;Zhihui Wang;Fuming Sun;Haojie Li	2018	Neurocomputing	10.1016/j.neucom.2018.07.062	discriminative model;clustering high-dimensional data;machine learning;artificial intelligence;feature vector;dual graph;cluster analysis;biclustering;mathematics;pattern recognition;non-negative matrix factorization;optimization problem	ML	24.52138206940791	-42.64118595027732	78344
112b1f366f41223458b8aa8927d88dff032c6aaf	structure learning of deep networks via dna computing algorithm		Convolutional Neural Network (CNN) has gained state-ofthe-art results in many pattern recognition and computer vision tasks. However, most of the CNN structures are manually designed by experienced researchers. Therefore, automatically building high performance networks becomes an important problem. In this paper, we introduce the idea of using DNA computing algorithm to automatically learn highperformance architectures. In DNA computing algorithm, we use short DNA strands to represent layers and long DNA strands to represent overall networks. We found that most of the learned models perform similarly, and only those performing worse during the first runs of training will perform worse finally than others. The indicates that: 1) Using DNA computing algorithm to learn deep architectures is feasible; 2) Local minima should not be a problem of deep networks; 3) We can use early stop to kill the models with the bad performance just after several runs of training. In our experiments, an accuracy 99.73% was obtained on the MNIST data set and an accuracy 95.10% was obtained on the CIFAR-10 data set.	artificial neural network;computation;convolutional neural network;dna computing;deep learning;evolutionary algorithm;experiment;maxima and minima;pattern recognition;simulation	Guoqiang Zhong;Tao Li;Wenxue Liu;Yang Chen	2018	CoRR		convolutional neural network;machine learning;dna computing;artificial intelligence;algorithm;maxima and minima;mnist database;computer science	Vision	20.880765181778305	-50.74157143789356	78350
a28119a0480956ebb4162653504eb1190cd45139	supervised locally linear embedding with probability-based distance for classification	microarray data;high dimensionality;supervised learning;dimension reduction;manifold learning;microarray data sets;classification;probability distribution;dimensional reduction;local linear embedding	We present a novel dimension reduction method for classification based on probabilitybased distance and the technique of locally linear embedding (LLE). Logistic Discrimination (LD) is adopted for estimating the probability distribution as well as for classification on the reduced data. Different from the supervised locally linear embedding (SLLE) that is only used for the dimension reduction of training data, our probability-based locally linear embedding (PLLE) can be applied on both training and testing data. Five microarray data sets in high-dimensional spaces, the IRIS data, and a real set of handwritten digits are experimented. The numerical results show the proposed methodology performs better, comparedwith the LD classifiers applied on the lower-dimensional embedding coordinates computed by LLE or SLLE. © 2008 Elsevier Ltd. All rights reserved.	arc diagram;microarray;nonlinear dimensionality reduction;numerical analysis	Lingxiao Zhao;Zhenyue Zhang	2009	Computers & Mathematics with Applications	10.1016/j.camwa.2008.10.055	probability distribution;microarray analysis techniques;discrete mathematics;biological classification;machine learning;pattern recognition;mathematics;nonlinear dimensionality reduction;supervised learning;dimensionality reduction	ML	23.46605081031633	-40.245883823401094	78378
2b89c4b48b60e3f8f42d357be877b8c2bd872bfe	an innovative feature selection using fuzzy entropy	high dimensional dataset;fuzzy measure;fuzzy entropy;clustering;number of clusters;feature subset selection;feature selection;classification accuracy	In this paper, a new feature subset selection approach is introduced. The proposed approach consists of two phases. In the first phase, we tried to reduce the run time order of the algorithm which is critical for high dimensional datasets. In this phase, first entire dataset is classified and according to silhouette value, the best number of clusters in the dataset is found. Using this value, second, each feature is classified alone with the same cluster number and proposed entropy fuzzy measures for them are calculated. In the second phase, it is tried to find a feature subset that meets the boundaries to get a high accuracy degree. The proposed method is examined on different datasets. The examination results show that the proposed method leans to find and select the minimum number of features with negligible removing final classification accuracy, among different feature subset selection methods.	feature selection;fuzzy concept	Hamid Parvin;Behrouz Minaei-Bidgoli;Hossein Ghaffarian	2011		10.1007/978-3-642-21111-9_65	computer science;machine learning;pattern recognition;data mining;mathematics;cluster analysis;feature selection;feature	Robotics	11.71235234100888	-45.85049353548726	78580
afa2efec7a888c16d9afd329cf3d56edfc803915	grammar variational autoencoder		Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as natural images, artwork, and audio. However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges. Crucially, state-of-the-art methods often produce outputs that are not valid. We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar. We propose a variational autoencoder which directly encodes from and decodes to these parse trees, ensuring the generated outputs are always syntactically valid. Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs. We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecule generation.	autoencoder;bayesian optimization;calculus of variations;coherence (physics);context-free grammar;context-free language;discrete mathematics;generative modelling language;mathematical optimization;parse tree;parsing;symbolic regression;variational principle	Matt J. Kusner;Brooks Paige;José Miguel Hernández-Lobato	2017			autoencoder;symbolic regression;parse tree;bayesian optimization;artificial intelligence;machine learning;generative grammar;parsing;pattern recognition;theoretical computer science;computer science;expression (mathematics);grammar	ML	22.868957814208464	-48.58780857344692	78591
1b91ac8b018568ef6e4bde02d067cc9214d762ac	application of the random forest method in studies of local lymph node assay based skin sensitization data	random forest	The random forest and classification tree modeling methods are used to build predictive models of the skin sensitization activity of a chemical. A new two-stage backward elimination algorithm for descriptor selection in the random forest method is introduced. The predictive performance of the random forest model was maximized by tuning voting thresholds to reflect the unbalanced size of classification groups in available data. Our results show that random forest with a proposed backward elimination procedure outperforms a single classification tree and the standard random forest method in predicting Local Lymph Node Assay based skin sensitization activity. The proximity measure obtained from the random forest is a natural similarity measure that can be used for clustering of chemicals. Based on this measure, the clustering analysis partitioned the chemicals into several groups sharing similar molecular patterns. The improved random forest method demonstrates the potential for future QSAR studies based on a large number of descriptors or when the number of available data points is limited.		Shengqiao Li;Adam Fedorowicz;Harshinder Singh;Sidney C. Soderholm	2005	Journal of chemical information and modeling	10.1021/ci050049u	decision tree learning;random forest;similarity measure;statistics;local lymph node assay;cluster analysis;sensitization;quantitative structure–activity relationship;mathematics	ML	10.1854166916922	-51.45994214950124	78723
753af73a9a2c7f06ca2570829b981414a5058335	wind turbines fault diagnosis using ensemble classifiers	wind turbines;ensemble classifiers;angular resampling;fault diagnosis	Fault diagnosis in machines that work under a wide range of speeds and loads is currently an active area of research. Wind turbines are one of the most recent examples of these machines in industry. Conventional vibration analysis applied to machines throughout their operation is of limited utility when the speed variation is too high. This work proposes an alternative methodology for fault diagnosis in machines: the combination of angular resampling techniques for vibration signal processing and the use of data mining techniques for the classification of the operational state of wind turbines. The methodology has been validated over a test-bed with a large variation of speeds and loads which simulates, on a smaller scale, the real conditions of wind turbines. Over this test-bed two of the most common typologies of faults in wind turbines have been generated: imbalance and misalignment. Several data mining techniques have been used to analyze the dataset obtained by order analysis, having previously processed signals with angular resampling technique. Specifically, the methods used are ensemble classifiers built with Bagging, Adaboost, Geneneral Boosting Projection and Rotation Forest; the best results having been achieved with Adaboost using C4.5 decision trees as base classifiers.	adaboost;angularjs;c4.5 algorithm;data mining;decision tree;ensemble kalman filter;signal processing;testbed	Pedro Santos;Luisa F. Villa;Aníbal Reñones;Andrés Bustillo;Jesús Maudes	2012		10.1007/978-3-642-31488-9_6	wind power;machine learning;pattern recognition;data mining	ML	14.659554089463056	-42.91895963111724	78868
180dd1dc6c80c29c569e947821ff8114f3879e50	learning classifiers from only positive and unlabeled data	unlabeled data;supervised learning;text mining;molecular biology;unlabeled examples;conditional probability;problem solving;bioinformatics	The input to an algorithm that learns a binary classifier normally consists of two sets of examples, where one set consists of positive examples of the concept to be learned, and the other set consists of negative examples. However, it is often the case that the available training data are an incomplete set of positive examples, and a set of unlabeled examples, some of which are positive and some of which are negative. The problem solved in this paper is how to learn a standard binary classifier given a nontraditional training set of this nature.  Under the assumption that the labeled examples are selected randomly from the positive examples, we show that a classifier trained on positive and unlabeled examples predicts probabilities that differ by only a constant factor from the true conditional probabilities of being positive. We show how to use this result in two different ways to learn a classifier from a nontraditional training set. We then apply these two new methods to solve a real-world problem: identifying protein records that should be included in an incomplete specialized molecular biology database. Our experiments in this domain show that models trained using the new methods perform better than the current state-of-the-art biased SVM method for learning from positive and unlabeled examples.	algorithm;binary classification;experiment;randomness;statistical classification;support vector machine;test set	Charles Elkan;Keith Noto	2008		10.1145/1401890.1401920	text mining;conditional probability;computer science;machine learning;pattern recognition;data mining;supervised learning;statistics	ML	15.908765258247607	-38.66418310001027	78968
f14bcef7c87e52a971388af3033eef2f691b46d8	intelligent partitioning for feature selection	data mining;analysis of algorithms;feature selection;entropy	This paper develops a new optimization-based feature-selection framework for knowledge discovery in databases. Algorithms following this new framework have attractive theoretical properties such as proven convergence to an optimal set of relevant features and the ability for deriving rigorous statements regarding the quality of the set that is found. Within this framework both wrapper and filter algorithms are derived, and numerical experiments show the new methodology to perform well with respect to accuracy and simplicity of the set of features found to be relevant.	feature selection	Sigurdur Ólafsson;Jaekyung Yang	2005	INFORMS Journal on Computing	10.1287/ijoc.1040.0104	entropy;computer science;analysis of algorithms;machine learning;pattern recognition;data mining;feature selection	Crypto	18.177440166137078	-39.21532235094546	78976
867fdf3e06e6d9b56d5ae52f2ea805ce7bf4b89a	a multi-objective evolutionary algorithm-based ensemble optimizer for feature selection and classification with neural network models	multi objective optimization;feature selection;ensemble models;evolutionary algorithm;neural network classifiers	In this paper, we propose a new multi-objective evolutionary algorithm-based ensemble optimizer coupled with neural network models for undertaking feature selection and classification problems. Specifically, the Modified micro Genetic Algorithm (MmGA) is used to form the ensemble optimizer. The aim of the MmGA-based ensemble optimizer is two-fold, i.e. to select a small number of input features for classification and to improve the classification performances of neural network models. To evaluate the effectiveness of the proposed system, a number of benchmark problems are first used, and the results are compared with those from other methods. The applicability of the proposed system to a human motion detection and classification task is then evaluated. The outcome positively demonstrates that the proposed MmGA-based ensemble optimizer is able to improve the classification performances of neural network models with a smaller number of input features. & 2013 Elsevier B.V. All rights reserved.	approximation algorithm;artificial neural network;benchmark (computing);evolutionary algorithm;feature selection;genetic algorithm;kinesiology;mathematical optimization;multi-objective optimization;optimizing compiler;pareto efficiency;performance;rl (complexity)	Choo Jun Tan;Chee Peng Lim;Yu-N Cheah	2014	Neurocomputing	10.1016/j.neucom.2012.12.057	computer science;multi-objective optimization;machine learning;evolutionary algorithm;ensemble forecasting;pattern recognition;data mining;feature selection	AI	10.274180757424345	-42.25129163319671	79041
2faec814d589de33ebd454cec782d3ed1f977f61	the choice of vantage objects for image retrieval	stepwise forward selection;euclidean distance;shape similarity;pattern recognition;cross validation;leave one out;image retrieval	Suppose that we have a matrix of dissimilarities between n images of a database. For a new image, we would like to select the most similar image of our database. Because it may be too expensive to compute the dissimilarities for the new object to all images of our database, we want to 5nd p n “vantage objects” (Pattern Recognition 35 (2002) 69) from our database in order to select a matching image according to the least Euclidean distance between the vector of dissimilarities between the new image and the vantage objects and the corresponding vector for the images of the database. In this paper, we treat the choice of suitable vantage objects. We suggest a loss measure to assess the quality of a set of vantage objects: For every image, we select a matching image from the remaining images of the database by use of the vantage set, and we average the resulting dissimilarities. We compare two classes of choice strategies: The 5rst one is based on a stepwise forward selection of vantage objects to optimize the loss measure. The second is to choose objects as representative as possible for the whole range of the database. ? 2003 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.	british undergraduate degree classification;cluster analysis;cross-validation (statistics);database;euclidean distance;expectation–maximization algorithm;image retrieval;loss function;mathematical optimization;pattern recognition;selection bias;stepwise regression	Christian Hennig;Longin Jan Latecki	2003	Pattern Recognition	10.1016/S0031-3203(02)00326-6	computer vision;image retrieval;computer science;machine learning;pattern recognition;data mining;euclidean distance;mathematics;cross-validation	Vision	22.680738925145025	-41.135623407580006	79046
09f943fdcd4601ac8f71f330820e6c8dee12b57e	asymmetric bagging and feature selection for activities prediction of drug molecules	drugs;quantitative structure activity relationship;experimental method;drug molecules;quantitative structure activity relationship models;ensemble learning;support vector machines;buffers;pharmaceutical technology;worst case ratio;embedded feature selection asymmetric bagging activities prediction drug molecules quantitative structure activity relationship models support vector machines prifeab;parallel machine scheduling;computational complexity;scheduling;feature extraction;support vector machines drugs pharmaceutical technology;prediction accuracy;activities prediction;parallel machines;feature selection;support vector machine;scheduling computational complexity parallel machines;prifeab;optimal algorithm;lower bound;embedded feature selection;asymmetric bagging;bagging drugs support vector machines machine learning accuracy predictive models costs humans bioinformatics learning systems;parallel machines high performance computing concurrent computing processor scheduling algorithm design and analysis sun application software computer science scheduling algorithm;buffers parallel machine scheduling worst case ratio optimal algorithm	Activities of drug molecules can be predicted by QSAR (quantitative structure activity relationship) models, which overcome the disadvantage of high cost and long cycle by employing the traditional experimental method. With the fact that the number of drug molecules with positive activity is rather fewer than that with negatives, it is important to predict molecular activities considering such an unbalanced situation. Here, asymmetric bagging and feature selection are introduced into the problem and asymmetric bagging of support vector machines (AB-SVM) is proposed on predicting drug activities to treat the unbalanced problem. At the same time, the features extracted from the structures of drug molecules aspects prediction accuracy of QSAR models. Therefore, a novel algorithm named PRIFEAB is proposed, which applies an embedded feature selection method to remove redundant and irrelevant features for AB-SVM. Numerical experimental results on a data set of molecular activities show that AB-SVM improves the A UC values of molecular activities, and PRIFEAB with feature selection further helps to improve the prediction ability.	feature selection	Guozheng Li;Hao-Hua Meng;Mary Yang;Jack Y. Yang	2007		10.1109/IMSCCS.2007.89	computer science;machine learning;pattern recognition;data mining	Vision	12.409714308482933	-49.30511014796249	79217
67114e20bb177893f33bc1d7537b6518b3a80839	dynamic feature scaling for online learning of binary classifiers		Scaling feature values is an important step in numerous machine learning tasks. Different features can have different value ranges and some form of a feature scaling is often required in order to learn an accurate classifier. However, feature scaling is conducted as a preprocessing task prior to learning. This is problematic in an online setting because of two reasons. First, it might not be possible to accurately determine the value range of a feature at the initial stages of learning when we have observed only a few number of training instances. Second, the distribution of data can change over the time, which render obsolete any feature scaling that we perform in a pre-processing step. We propose a simple but an effective method to dynamically scale features at train time, thereby quickly adapting to any changes in the data stream. We compare the proposed dynamic feature scaling method against more complex methods for estimating scaling parameters using several benchmark datasets for binary classification. Our proposed feature scaling method consistently outperforms more complex methods on all of the benchmark datasets and improves classification accuracy of a state-of-the-art online binary classifier algorithm.	algorithm;benchmark (computing);effective method;feature scaling;image scaling;linear classifier;machine learning;preprocessor;statistical classification;supervised learning;traverse;unsupervised learning;wavelet	Danushka Bollegala	2017	Knowl.-Based Syst.	10.1016/j.knosys.2017.05.010	dimensionality reduction;feature scaling;data mining;feature (computer vision);artificial intelligence;machine learning;linear classifier;scaling;computer science;effective method;k-nearest neighbors algorithm;feature learning;pattern recognition	ML	18.757388558322027	-45.36870191252334	79414
4017172baa1dfd6fec03d01acb249da1f3167fa8	node label matching improves classification performance in deep belief networks	image processing;node label matching classification performance deep belief networks artificial neural network classifiers class label predictors partial knowledge learning speed classification accuracy maximum average correlation small labeled validation dataset supervised learning unsupervised learning contrastive divergence pretraining restricted boltzmann machines back propagation fine tuning segmented density random binary dataset mnist benchmark;neural networks correlation unsupervised learning algorithm design and analysis computer science knowledge engineering impedance matching;unsupervised learning backpropagation belief networks boltzmann machines pattern classification pattern matching;pattern recognition and data mining;neural evolutionary and fuzzy computation	If output signals of artificial neural network classifiers are interpreted per node as class label predictors then partial knowledge encoded by the network during the learning procedure can be exploited in order to reassign which output node should represent each class label so that learning speed and final classification accuracy are improved. Our method for computing these reassignments is based on the maximum average correlation between actual node outputs and target labels over a small labeled validation dataset. Node Label Matching is an ancillary method for both supervised and unsupervised learning in artificial neural networks and we demonstrate its integration with Contrastive Divergence pre-training in Restricted Boltzmann Machines and Back Propagation fine-tuning in Deep Belief Networks. We introduce the Segmented Density Random Binary dataset and present empirical results of Node Label Matching on both our synthetic data and a subset of the MNIST benchmark.	artificial neural network;bayesian network;benchmark (computing);best, worst and average case;deep learning;dimensionality reduction;feature learning;feature selection;greedy algorithm;mnist database;oblique projection;restricted boltzmann machine;software propagation;supervised learning;synthetic data;synthetic intelligence;time complexity;unsupervised learning	Allan Campbell;Victor Ciesielski;A. Kai Qin	2016	2016 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2016.7727395	unsupervised learning;image processing;computer science;machine learning;pattern recognition;data mining;deep learning;competitive learning;deep belief network;artificial neural network	ML	18.733698297856876	-41.99147268862423	79544
75a34343fa0c3145d33140ffd2aa7d3aaacef1e2	kohonen’s map approach for the belief mass modeling	multivariate data belief mass modeling evidence theory mass function estimation kohonen map feature space classifier smart mass belief assignment basic belief assignment bba;training;quantization signal;neurons estimation training indexes clustering algorithms quantization signal learning systems;learning systems;indexes;self organising feature maps belief networks inference mechanisms;estimation;clustering algorithms;neurons;mass belief assignment estimation evidence theory kohonen map	In the framework of the evidence theory, several approaches for estimating belief functions are proposed. However, they generally suffer from the problem of masses attribution in the case of compound hypotheses that lose much conceptual contribution of the theory. In this paper, an original method for estimating mass functions using Kohonen's map derived from the initial feature space and an initial classifier is proposed. Our approach allows a smart mass belief assignment, not only for simple hypotheses but also for disjunctions and conjunctions of hypotheses. Thus, it can model at the same time ignorance, imprecision, and paradox. The proposed method for a basic belief assignment (BBA) is of interest for solving estimation mass functions problems where a large quantity of multivariate data is available. Indeed, the use of Kohonen's map simplifies the process of assigning mass functions. The proposed method has been compared with the state-of-the-art BBA technique on benchmark database and applied on remote sensing data for image classification purpose. Experimentation shows that our approach gives similar or better results than other methods presented in the literature so far, with an ability to handle a large amount of data.	bachelor of business administration;benchmark (computing);computer vision;estimated;feature vector;self-organizing map;attribution	Imen Hammami;Grégoire Mercier;Atef Hamouda;Jean Dezert	2016	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2015.2480772	database index;estimation;computer science;artificial intelligence;machine learning;data mining;mathematics;cluster analysis;statistics	ML	17.698804747762818	-42.480205065477584	79903
4642916ee5c2697356d02afdb3cd13b30b8051ba	multi-label learning with weak label	convex optimization;data association;learning methods;low density	Multi-label learning deals with data associated with multiple labels simultaneously. Previous work on multi-label learning assumes that for each instance, the “full” label set associated with each training instance is given by users. In many applications, however, to get the full label set for each instance is difficult and only a “partial” set of labels is available. In such cases, the appearance of a label means that the instance is associated with this label, while the absence of a label does not imply that this label is not proper for the instance. We call this kind of problem “weak label” problem. In this paper, we propose the WELL (WEak Label Learning) method to solve the weak label problem. We consider that the classification boundary for each label should go across low density regions, and that each label generally has much smaller number of positive examples than negative examples. The objective is formulated as a convex optimization problem which can be solved efficiently. Moreover, we exploit the correlation between labels by assuming that there is a group of low-rank base similarities, and the appropriate similarities between instances for different labels can be derived from these base similarities. Experiments validate the performance of WELL.	convex optimization;experiment;low-rank approximation;mathematical optimization;multi-label classification;optimization problem;quadratic programming	Yu-Yin Sun;Yin Zhang;Zhi-Hua Zhou	2010			convex optimization;machine learning;pattern recognition;data mining	AI	24.043261223674676	-43.753718200653495	80343
1125d76df63f332a7a4eee982fe2135ce7fd1a79	robust image annotation via simultaneous feature and sample outlier pursuit	low rank representation;sample and feature outlier removal	Graph-based semi-supervised image annotation has achieved great success in a variety of studies, yet it essentially and intuitively suffers from both the irrelevant/noisy features (referred to as feature outliers) and the unusual/corrupted samples (referred to as sample outliers). In this work, we investigate how to derive robust sample affinity matrix via simultaneous feature and sample outlier pursuit. This task is formulated as a Dual-outlier and Prior-driven Low-Rank Representation (DP-LRR) problem, which possesses convexity in objective function. In DP-LRR, the clean data are assumed to be self-reconstructible with low-rank coefficient matrix as in LRR; while the error matrix is decomposed as the sum of a row-wise sparse matrix and a column-wise sparse matrix, the ℓ2,1-norm minimization of which encourages the pursuit of feature and sample outliers respectively. The DP-LRR is further regularized by the priors from side information, that is, the inhomogeneous data pairs. An efficient iterative procedure based on linearized alternating direction method is presented to solve the DP-LRR problem, with closed-form solutions within each iteration. The derived low-rank reconstruction coefficient matrix is then fed into any graph based semi-supervised label propagation algorithm for image annotation, and as a by-product, the cleaned data from DP-LRR can also be utilized as a better image representation to generally boost image annotation performance. Extensive experiments on MIRFlickr, Corel30K, NUS-WIDE-LITE and NUS-WIDE databases well demonstrate the effectiveness of the proposed formulation for robust image annotation.	automatic image annotation;coefficient;database;experiment;iteration;iterative method;label propagation algorithm;loss function;low-rank approximation;optimization problem;processor affinity;relevance;semi-supervised learning;semiconductor industry;software propagation;sparse matrix	Jian Dong;Bin Cheng;Xiangyu Chen;Tat-Seng Chua;Shuicheng Yan;Xi Zhou	2013	TOMCCAP	10.1145/2501643.2501646	mathematical optimization;computer science;machine learning;pattern recognition;statistics	Vision	24.310506388798228	-43.23826297038174	80371
50442d3c72bd7cf0191397936bcb75086826db9d	learning similarity preserving representations with neural similarity encoders		Many dimensionality reduction or manifold learning algorithms optimize for retaining the pairwise similarities, distances, or local neighborhoods of data points. Spectral methods like Kernel PCA (kPCA) or isomap achieve this by computing the singular value decomposition (SVD) of some similarity matrix to obtain a low dimensional representation of the original data. However, this is computationally expensive if a lot of training examples are available and, additionally, representations for new (out-of-sample) data points can only be created when the similarities to the original training examples can be computed. We introduce similarity encoders (SimEc), which learn similarity preserving representations by using a feed-forward neural network to map data into an embedding space where the original similarities can be approximated linearly. The model optimizes the same objective as kPCA but in the process it learns a linear or non-linear embedding function (in the form of the tuned neural network), with which the representations of novel data points can be computed even if the original pairwise similarities of the training set were generated by an unknown process such as human ratings. By creating embeddings for both image and text datasets, we demonstrate that SimEc can, on the one hand, reach the same solution as spectral methods, and, on the other hand, obtain meaningful embeddings from similarities based on human labels.	analysis of algorithms;approximation algorithm;arc diagram;artificial neural network;data point;encoder;feedforward neural network;isomap;kernel principal component analysis;machine learning;nonlinear dimensionality reduction;nonlinear system;semantic similarity;similarity measure;singular value decomposition;spectral method;test set	Franziska Horn;Klaus-Robert Müller	2017	CoRR	10.24425/bpas.2018.125929	machine learning;pattern recognition;data mining;mathematics;statistics	ML	23.43133529220606	-41.80238875773858	80499
2c14fe5d330a727b39767ddf7aa23aafec291491	a comparison of classification strategies in genetic programming with unbalanced data	genetic program;learning algorithm;naive bayes;class imbalance;machine learning;binary classification;support vector machine;fitness function	Machine learning algorithms like Genetic Programming (GP) can evolve biased classifiers when data sets are unbalanced. In this paper we compare the effectiveness of two GP classification strategies. The first uses the standard (zero) class-threshold, while the second uses the “best” class-threshold determined dynamically on a solution-by-solution basis during evolution. These two strategies are evaluated using five different GP fitness across across a range of binary class imbalance problems, and the GP approaches are compared to other popular learning algorithms, namely, Naive Bayes and Support Vector Machines. Our results suggest that there is no overall difference between the two strategies, and that both strategies can evolve good solutions in binary classification when used in combination with an effective fitness function.	algorithm;binary classification;effective fitness;fitness function;genetic programming;machine learning;naive bayes classifier;support vector machine;unbalanced circuit	Urvesh Bhowan;Mengjie Zhang;Mark Johnston	2010		10.1007/978-3-642-17432-2_25	binary classification;support vector machine;naive bayes classifier;computer science;machine learning;pattern recognition;data mining;mathematics;fitness function	ML	12.689978414568161	-41.650715448411674	80521
87c7647c659aefb539dbe672df06714d935e0a3b	benchmarking neural network robustness to common corruptions and surface variations		In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, IMAGENET-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Unlike recent robustness research, this benchmark evaluates performance on commonplace corruptions not worst-case adversarial corruptions. We find that there are negligible changes in relative corruption robustness from AlexNet to ResNet classifiers, and we discover ways to enhance corruption robustness. Then we propose a new dataset called ICONS-50 which opens research on a new kind of robustness, surface variation robustness. With this dataset we evaluate the frailty of classifiers on new styles of known objects and unexpected instances of known classes. We also demonstrate two methods that improve surface variation robustness. Together our benchmarks may aid future work toward networks that learn fundamental class structure and also robustly generalize.	adversary (cryptography);artificial neural network;benchmark (computing);best, worst and average case;imagenet	Dan Hendrycks;Thomas G. Dietterich	2018	CoRR		robustness (computer science);machine learning;artificial neural network;residual neural network;benchmarking;artificial intelligence;mathematics;corruption;classifier (linguistics)	Vision	18.96458235906272	-51.36112260860126	80523
2c9bbc4374b4e476a0cacac041c779fd27964367	a hybrid approach of interpolations and cnn to obtain super-resolution		We propose a novel architecture that learns an end-to-end mapping function to improve the spatial resolution of the input natural images. The model is unique in forming a nonlinear combination of three traditional interpolation techniques using the convolutional neural network. Another proposed architecture uses a skip connection with nearest neighbor interpolation, achieving almost similar results. The architectures have been carefully designed to ensure that the reconstructed images lie precisely in the manifold of high-resolution images, thereby preserving the high-frequency components with fine details. We have compared with the state of the art and recent deep learning based natural image super-resolution techniques and found that our methods are able to preserve the sharp details in the image, while also obtaining comparable or better PSNR than them. Since our methods use only traditional interpolations and a shallow CNN with less number of smaller filters, the computational cost is kept low. We have reported the results of two proposed architectures on five standard datasets, for an upscale factor of 2. Our methods generalize well in most cases, which is evident from the better results obtained with increasingly complex datasets. For 4-times upscaling, we have designed similar architectures for comparing with other methods.	algorithm;algorithmic efficiency;artificial neural network;bicubic interpolation;bilinear filtering;channel (digital image);computation;convolutional neural network;deep learning;end-to-end principle;experiment;image resolution;nearest-neighbor interpolation;nonlinear system;peak signal-to-noise ratio;salt-and-pepper noise;super-resolution imaging	Ram Krishna Pandey;A. G. Ramakrishnan	2018	CoRR		interpolation;convolutional neural network;machine learning;computer science;architecture;deep learning;nearest-neighbor interpolation;superresolution;nonlinear system;artificial intelligence;image resolution	Vision	24.352379643447133	-51.823346237111004	80643
e12695f80896d2734da84a3dc27d683ff8e8fcac	topology reduction in deep convolutional feature extraction networks		Deep convolutional neural networks (CNNs) used in practice employ potentially hundreds of layers and 10,000s of nodes. Such network sizes entail significant computational complexity due to the large number of convolutions that need to be carried out; in addition, a large number of parameters needs to be learned and stored. Very deep and wide CNNs may therefore not be well suited to applications operating under severe resource constraints as is the case, e.g., in low-power embedded and mobile platforms. This paper aims at understanding the impact of CNN topology, specifically depth and width, on the network’s feature extraction capabilities. We address this question for the class of scattering networks that employ either Weyl-Heisenberg filters or wavelets, the modulus non-linearity, and no pooling. The exponential feature map energy decay results in Wiatowski et al., 2017, are generalized to O(a−N ), where an arbitrary decay factor a > 1 can be realized through suitable choice of the Weyl-Heisenberg prototype function or the mother wavelet. We then show how networks of fixed (possibly small) depth N can be designed to guarantee that ((1 − ε) · 100)% of the input signal’s energy are contained in the feature vector. Based on the notion of operationally significant nodes, we characterize, partly rigorously and partly heuristically, the topology-reducing effects of (effectively) band-limited input signals, band-limited filters, and feature map symmetries. Finally, for networks based on Weyl-Heisenberg filters, we determine the prototype function bandwidth that minimizes—for fixed network depth N—the average number of operationally significant nodes per layer.	artificial neural network;bandlimiting;computational complexity theory;convolution;convolutional neural network;embedded system;feature extraction;feature vector;heuristic;low-power broadcasting;mobile device;modulus of continuity;nonlinear system;prototype;time complexity;wavelet	Thomas Wiatowski;Philipp Grohs;Helmut Bölcskei	2017	CoRR		convolutional neural network;mathematics;machine learning;mathematical optimization;wavelet;artificial intelligence;feature extraction;computational complexity theory;feature vector;exponential function;topology;convolution;bandwidth (signal processing)	EDA	20.322080494163185	-48.77582941186897	80726
98fc093422a8215b59c956a2d76276e989e67a0f	balancing interpretability and predictive accuracy for unsupervised tensor mining		The PARAFAC tensor decomposition has enjoyed an increasing success in exploratory multi-aspect data mining scenarios. A major challenge remains the estimation of the number of latent factors (i.e., the rank) of the decomposition, which is known to yield high-quality, interpretable results. Previously, AutoTen, an automated tensor mining method which leverages a well-known quality heuristic from the field of Chemometrics, the Core Consistency Diagnostic (CORCONDIA), in order to automatically determine the rank for the PARAFAC decomposition, was proposed. In this work, building upon AutoTen, we set out to explore the trade-off between 1) the interpretability of the results (as expressed by CORCONDIA), and 2) the predictive accuracy of the decomposition, towards improving rank estimation quality. Our preliminary results indicate that striking a good balance in that trade-off yields high-quality rank estimation, towards achieving unsupervised tensor mining.	chemometrics;data mining;heuristic;latent variable	Ishmam Zabir;Evangelos E. Papalexakis	2017	2017 51st Asilomar Conference on Signals, Systems, and Computers	10.1109/ACSSC.2017.8335435	tensor;stress (mechanics);computer science;chemometrics;data modeling;heuristic;machine learning;interpretability;artificial intelligence;pattern recognition	ML	18.6976319070068	-41.33744076143991	80799
9e893daf771ad54bd21c5a3f23097d1bfa656a06	semi-supervised learning: exploiting unlabeled data with symmetrical distribution and high confidence	unlabeled data;symmetrical distribution;semi supervised learning;incremental learning;high confidence	Current existing representative works to semi-supervised incremental learning prefer to select unlabeled instances predicted with high confidence for model retraining. However, this strategy may degrade the classification performance rather than improve it, because relying on high confidence for data selection can lead to an erroneous estimate to the true distribution, especially when the confidence annotator is highly correlated with the confidence annotator. In this paper, a new semi-supervised incremental learning algorithm was proposed, which selected the high confidence unlabeled instances with symmetrical distribution from unlabeled data, it can reduce the bias in the estimation in some degree. In detail, expectation maximization algorithm was used to estimate the confidence of each instance, and Gaussian function was used to calculate the data distribution, then the selected unlabeled data was used for retraining model with classifier algorithm. The experimental results based on a large number of UCI data sets show that our algorithm can effectively exploit unlabeled data to enhance the learning performance.	semi-supervised learning;semiconductor industry;supervised learning	Yihao Zhang;Junhao Wen;Fangfang Tang;Zhuo Jiang	2012	IJPRAI	10.1142/S0218001412510032	semi-supervised learning;unsupervised learning;symmetric probability distribution;computer science;machine learning;pattern recognition;data mining;statistics	ML	15.978777170985747	-40.10860340131926	80974
30f113d985d876a3974838b2ead49a069b474e57	guided upsampling network for real-time semantic segmentation		Semantic segmentation architectures are mainly built upon an encoder-decoder structure. These models perform subsequent downsampling operations in the encoder. Since operations on high-resolution activation maps are computationally expensive, usually the decoder produces output segmentation maps by upsampling with parameters-free operators like bilinear or nearest-neighbor. We propose a Neural Network named Guided Upsampling Network which consists of a multiresolution architecture that jointly exploits high-resolution and large context information. Then we introduce a new module named Guided Upsampling Module (GUM) that enriches upsampling operators by introducing a learnable transformation for semantic maps. It can be plugged into any existing encoderdecoder architecture with little modifications and low additional computation cost. We show with quantitative and qualitative experiments how our network benefits from the use of GUM module. A comprehensive set of experiments on the publicly available Cityscapes dataset demonstrates that Guided Upsampling Network can efficiently process high-resolution images in real-time while attaining state-of-the art performances.	analysis of algorithms;artificial neural network;bilinear filtering;chroma subsampling;computation;conditional random field;decimation (signal processing);encoder;experiment;floating point systems;graphics processing unit;image resolution;linear algebra;map;multiresolution analysis;nearest-neighbor interpolation;network architecture;performance;random neural network;real-time clock;real-time web;semantic mapper;sound quality;test set;titan;upsampling	Davide Mazzini	2018			operator (computer programming);architecture;computer vision;computer science;encoder;machine learning;computation;artificial neural network;artificial intelligence;exploit;bilinear interpolation;upsampling	Vision	24.381410901933144	-51.63317044887174	80979
1fa004b00b48dffa3e86138ccb62064f452e3b8c	editing training sets from imbalanced data using fuzzy-rough sets	classification performance;instance selection;imbalanced data;rough set theory;fuzzy rough sets	In this research, we study several instance selection methods based on rough set theory and propose an approach able to deal with inconsistency caused by noise and imbalanced data. Recent attention has focused on the significant results obtained in selecting instances from noisy data using fuzzy-rough sets. For imbalanced data, fuzzy-rough sets approach is also applied before and after using balancing methods in order to improve classification performance. In this study, we propose an approach that uses different criteria for minority and majority classes in fuzzy-rough instance selection. It thus eliminates the step of using balancing techniques employed in controversial approach. We also carry out some experiments, measure classification performance and make comparisons with other methods.	rough set	Do Van Nguyen;Keisuke Ogawa;Kazunori Matsumoto;Masayuki Hashimoto	2015		10.1007/978-3-319-23868-5_9	rough set;computer science;machine learning;pattern recognition;data mining	Vision	13.518917143626222	-40.945360174989794	80990
fafe3ac2b092ad49bdc5fed4ab703d9ad8f44b9a	a comparison of performance metrics for event classification in non-intrusive load monitoring		In this work, we analyse experimentally the behaviour of 18 different performance metrics when applied to classification algorithms in event-based Non-Intrusive Load Monitoring, identifying relationships and clusters between the measures. Our results indicate that performance metrics have more in common than what was initially expected. Our results also suggest that in this multi-class classification problem, researchers should avoid micro-average and unweighted macro-average metrics in favor of their weighted macro-average counterparts. Finally, the results also suggest that probabilistic measures can provide important information that is not available when using more traditional performance metrics.	algorithm;experiment;multiclass classification;software performance testing	Lucas Pereira;Nuno Nunes	2017	2017 IEEE International Conference on Smart Grid Communications (SmartGridComm)	10.1109/SmartGridComm.2017.8340682	real-time computing;harmonic analysis;smart grid;machine learning;principal component analysis;probabilistic logic;statistical classification;engineering;artificial intelligence	Robotics	12.61087189661516	-43.17203363307386	81023
48f1079a654e964521806208c705b7d6f1f1c7e0	a comparative study of minimax probability machine-based approaches for face recognition	reconnaissance visage;probabilidad error;evaluation performance;metodo estadistico;analisis componente principal;base donnee;performance evaluation;image processing;minimum error minimax probability machine;learning;metodo minimax;biometrie;estudio comparativo;evaluacion prestacion;maquina vector soporte;biometrics;minimax method;database;biometria;procesamiento imagen;base dato;statistical method;traitement image;algorithme;aprendizaje;discriminant analysis;analyse discriminante;etude comparative;algorithm;machine vecteur support;analisis discriminante;apprentissage;automatic recognition;face recognition;machine learning;methode statistique;principal component analysis;signal classification;methode minimax;comparative study;analyse composante principale;pattern recognition;classification signal;minimax probability machine;error probability;reconnaissance forme;support vector machine;reconocimiento patron;probabilite erreur;reconocimiento automatico;reconnaissance automatique;algoritmo	Automatic face recognition is a challenging problem in the biometric recognition area. Minimax Probability Machine (MPM) and its extension, Minimum Error Minimax Probability Machine, have shown advantages in the machine learning literature. In this paper, we incorporate the MPM-based approaches into our face recognition system for further study. To test the performance of our new system, we compare the MPM-based approaches with SVM, a PCA-based and a LDA-based algorithms on the FERET database for both verification and identification. The experimental results demonstrate that MPM-based approaches are promising for automatic face recognition. 2007 Elsevier B.V. All rights reserved.	algorithm;biometrics;computation;feret database;facial recognition system;kernel method;linear discriminant analysis;local-density approximation;machine learning;material point method;minimax;support vector machine	Johnny K. C. Ng;Yuzhuo Zhong;Shiqiang Yang	2007	Pattern Recognition Letters	10.1016/j.patrec.2007.05.021	support vector machine;speech recognition;image processing;computer science;artificial intelligence;probability of error;machine learning;comparative research;pattern recognition;biometrics;principal component analysis	AI	23.502619495314516	-39.19269340214539	81105
ce1ff1a833f38e411c0ecdbf84426cfea8842646	adversarial examples that fool both human and computer vision		Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we create the first adversarial examples designed to fool humans, by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by modifying models to more closely match the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers.	bandlimiting;computer vision;machine learning	Gamaleldin F. Elsayed;Shreya Shankar;Brian Cheung;Nicolas Papernot;Alexey Kurakin;Ian J. Goodfellow;Jascha Sohl-Dickstein	2018	CoRR		adversarial system;architecture;computer vision;human visual system model;computer science;artificial intelligence	Vision	20.047910064380726	-52.025732306106626	81108
7281011c60b8d94897718d6bd35016b5a8dded2e	comparison among methods of ensemble learning	random forest bagging boosting stacking;uci datasets ensemble learning target function bagging boosting stacking random forest training time regression problems;bagging;data analysis;boosting;stacking;regression analysis data analysis learning artificial intelligence;random forest;bagging boosting training classification algorithms training data vegetation stacking;regression analysis;learning artificial intelligence	Ensemble learning refers to a collection of methods that learn a target function by training a number of individual learners and combining their predictions. We explore four popular methods (bagging, boosting, stacking and random forest) of combining their outputs, for classification and training time and regression problems. Following this, experimental evaluations are performed on UCI datasets.	bootstrap aggregating;ensemble learning;random forest;stacking	Shaohua Wan;Hua Yang	2013	2013 International Symposium on Biometrics and Security Technologies	10.1109/ISBAST.2013.50	random forest;computer science;machine learning;pattern recognition;data mining;gradient boosting	ML	13.73094545806273	-41.35786585657951	81277
ed6f1cff6252cdb387b80f7e3a5dde435f90c5a1	unifying local and non-local signal processing with graph cnns		This paper deals with the unification of local and non-local signal processing on graphs within a single convolutional neural network (CNN) framework. Building upon recent works on graph CNNs, we propose to use convolutional layers that take as inputs two variables, a signal and a graph, allowing the network to adapt to changes in the graph structure. This also allows us to learn through training the optimal mixing of locality and non-locality, in cases where the graph is built on the input signal itself. We demonstrate the versatility and the effectiveness of our framework on several types of signals (greyscale and color images, color palettes and speech signals) and on several applications (style transfer, color transfer, and denoising).	action at a distance;artificial neural network;color mapping;convolutional neural network;grayscale;locality of reference;noise reduction;signal processing;unification (computer science)	Gilles Puy;Srdan Kitic;Patrick Pérez	2017	CoRR		computer vision;theoretical computer science;machine learning;pattern recognition;mathematics	ML	22.82887787036789	-48.54232555494299	81341
11004129062501a1a9728e5f9dc9f24f3d64b1fc	a unified framework for semi-supervised dimensionality reduction	unlabeled data;locality preserving projection;metodo estadistico;analisis componente principal;supervised learning;methode noyau;classification non supervisee;statistical method;linear discriminate analysis;semi supervised learning;algorithme;discriminant analysis;analyse discriminante;algorithm;accuracy;analisis discriminante;precision;dimensionality reduction;methode statistique;principal component analysis;clasificacion no supervisada;metodo nucleo;high dimensional data;signal classification;analyse composante principale;classification signal;unsupervised classification;kernel method;apprentissage supervise;aprendizaje supervisado;dimensional reduction;manifold analysis;algoritmo	In practice, many applications require a dimensionality reduction method to deal with the partially labeled problem. In this paper, we propose a semi-supervised dimensionality reduction framework, which can efficiently handle the unlabeled data. Under the framework, several classical methods, such as principal component analysis (PCA), linear discriminant analysis (LDA), maximum margin criterion (MMC), locality preserving projections (LPP) and their corresponding kernel versions can be seen as special cases. For high-dimensional data, we can give a low-dimensional embedding result for both discriminating multi-class sub-manifolds and preserving local manifold structure. Experiments show that our algorithms can significantly improve the accuracy rates of the corresponding supervised and unsupervised approaches.	dimensionality reduction;semi-supervised learning;semiconductor industry;unified framework	Yangqiu Song;Feiping Nie;Changshui Zhang;Shiming Xiang	2008	Pattern Recognition	10.1016/j.patcog.2008.01.001	diffusion map;computer science;machine learning;pattern recognition;mathematics;accuracy and precision;supervised learning;statistics;dimensionality reduction	Vision	23.779667478262585	-39.22575794515464	81384
dba0547ee870e025275e8afd48d890507edfb50a	joint learning for voice based disease detection		Abstract Voice analysis provides a non-invasive way for disease detection, in which most methods only consider a single audio, although different audios contain complementary information and a fusion of them is beneficial. In this paper, a novel model JOLL4R (JOint Learning based on Label Relaxed low-Rank Ridge Regression) is proposed to fuse audios for voice based disease detection. First, the model couples the regression losses from two audios together to jointly learn a transformation matrix for each audio. Secondly, the conventional zero-one regression targets are relaxed by the ϵ-dragging technique so that the margins between different classes are enlarged. Third, low-rank constraint is imposed to exploit the correlation structure among different classes. The proposed algorithm not only enables to consider multiple audios, but also adjusts the weight of each audio adaptively. Due to the design of losses coupling, ϵ-dragging technique, and low rank constraint, high performance is achieved. Experiments conducted on two disease detection tasks, each with six types of fusion, show that our fusion approach outperforms the case of using a single audio and another two fusion methods. Finally, key factors in JOLL4R are analyzed.		Kebin Wu;David Zhang;Guangming Lu;Zhenhua Guo	2019	Pattern Recognition	10.1016/j.patcog.2018.09.013	machine learning;fusion;artificial intelligence;voice analysis;exploit;transformation matrix;pattern recognition;mathematics	Vision	23.106300148778754	-46.096233780249705	81429
848e230177f4a9294572e92651e4c24d5ecdcbca	evolving neural-symbolic systems guided by adaptive training schemes: applications in finance	induction machine;genetic program;credit scoring;sensitivity analysis;decision making process;decision rule	The paper presents a hybrid and adaptive intelligent methodology, based on neural logic networks and grammar-guided genetic programming. The aim of the study is to demonstrate how to generate efficient neural logic networks with the aid of genetic programming methods trained adaptively through an innovative scheme. The proposed adaptive training scheme of the genetic programming mechanism, leads to the generation of high diversity solutions and small sized individuals. The overall methodology is advantageous due to the adaptive training scheme proposed, for offering both, accurate and interpretable results in the form of expert rules. Moreover, a sensitivity analysis study is provided within the paper, comparing the performance of the proposed evolutionary neural logic networks methodology, with well-known competitive inductive machine learning approaches. Two financial domains of application have been selected to demonstrate the capabilities of the proposed methodology, (a) classification of credit applicants for consumer loans of a German bank and (b) the credit-scoring decision-making process in an Australian bank. Results seem encouraging since the proposed methodology outperforms a number of competitive existing statistical and intelligent methodologies, while it also produces handy decision rules, short in length and transparent in meaning and use.	emoticon;genetic programming;handy board;machine learning;statistical classification	Athanasios Tsakonas;Georgios Dounias	2007	Applied Artificial Intelligence	10.1080/08839510701492603	decision-making;computer science;artificial intelligence;machine learning;data mining;decision rule;sensitivity analysis	AI	10.607598241938527	-38.848840360166804	81544
6921335229a09506964d63dc991e70b4da4c5f1f	a hybrid nearest-neighbor and nearest-hyperrectangle algorithm	hybrid method;compact representation;nearest neighbor	"""Algorithms based on Nested Generalized Exemplar (NGE) theory [10] classify new data points by computing their distance to the nearest """"generalized exemplar"""" (i.e. an axis-parallel multidimensional rectangle). An improved version of NGE, called BNGE, was previously shown to perform comparably to the Nearest Neighbor algorithm. Advantages of the NGE approach include compact representation of the training data and fast training and classification. A hybrid method that combines BNGE and the k-Nearest Neighbor algorithm, called KBNGE, is introduced for improved classification accuracy. Results from eleven domains show that KBNGE achieves generalization accuracies similar to the k-Nearest Neighbor algorithm at improved classification speed. KBNGE is a fast and easy to use inductive learning algorithm that gives very accurate predictions in a variety of domains and represents the learned knowledge in a manner that can be easily interpreted by the"""	apache axis;data point;inductive reasoning;k-nearest neighbors algorithm	Dietrich Wettschereck	1994		10.1007/3-540-57868-4_67	nearest-neighbor chain algorithm;large margin nearest neighbor;nearest neighbor graph;best bin first;machine learning;pattern recognition;nearest neighbor search	ML	11.456194241213371	-42.49617468145457	81644
b11184bc8a32efe418b2aa75a11c67dd94f06538	incremental dictionary learning for unsupervised domain adaptation		Domain adaptation (DA) methods attempt to solve the domain mismatch problem between source and target data. In this paper, we propose an incremental dictionary learning method where some target data called supportive samples are selected to assist adaptation. Supportive samples are close to the source domain and have two properties: first, their predicted class labels are reliable and can be used for building more discriminative classification models; second, they act as a bridge to connect the two domains and reduce the domain mismatch. Theoretical analysis shows that both properties are important for adaptation, enabling the idea of adding supportive samples to the source domain. A stopping criterion is designed to guarantee that the domain mismatch decreases monotonically during adaptation. Experimental results on several widely used visual datasets show that the proposed approach performs better than many state-of-the-art methods.	dictionary;domain adaptation;experiment;facial recognition system;machine learning;smoothing	Boyu Lu;Rama Chellappa;Nasser M. Nasrabadi	2015		10.5244/C.29.108	unsupervised learning;pattern recognition	AI	23.046317343588218	-46.071070636190086	81681
c4ec8c0df2b8e214a98a2543678eccb990092b0c	multi-view based multi-label propagation for image annotation	image annotation;multi label;multi view	Multi-view learning and multi-label propagation are two common approaches to address the problem of image annotation. Traditional multi-view methods disregard the consistencies among different views while existing algorithms toward multi-label propagation ignore the underlying mutual correlations among different labels. In this paper, we present a novel image annotation algorithm by exploring the heterogeneities from both the view level and the label level. For a single label, its propagation from one view should agree with the propagation from another view. Similarly, for a single view, the propagations of related labels should be similar. We call the proposed approach as Multi-view based Multi-label Propagation for image annotation (MMP). MMP handles the consistencies among different views by requiring them to generate the same annotation result, and captures the correlations among different labels by imposing the similarity constraints. By taking full advantage of the dual-heterogeneity from views and labels, MMP is able to propagate the labels better than state of the art. Furthermore, we introduce an iterative algorithm to solve the optimization problem. Extensive experiments on real image data have shown that the proposed framework has effective image annotation performance.	automatic image annotation;multi-label classification;software propagation	Zhanying He;Chun Chen;Jiajun Bu;Ping Li;Deng Cai	2015	Neurocomputing	10.1016/j.neucom.2015.05.039	computer vision;computer science;bioinformatics;data mining;automatic image annotation	Vision	24.10368478426097	-43.882778020717794	81685
54ab2f60fd2fc34b488527b17bef84d6bc2b52a1	a sparsity driven kernel machine based on minimizing a generalization error bound	generalization error;learning;methode noyau;taux erreur;signal analysis;maquina vector soporte;analisis de senal;fonction perte;funcion perdida;classification;support vector;fonction objectif;algorithme;aprendizaje;objective function;algorithm;machine vecteur support;apprentissage;kernel machine;sparsity;linearisation;funcion penalidad;statistical learning theory;linearizacion;data dependence;loss function;metodo nucleo;signal classification;classification signal;error rate;linear program;funcion objetivo;linearization;kernel method;support vector machine;classification automatique;fonction penalite;automatic classification;generalization error bounds;indice error;clasificacion automatica;dimensional reduction;analyse signal;penalty function;algoritmo	A new sparsity driven kernel classifier is presented based on the minimization of a recently derived data-dependent generalization error bound. The objective function consists of the usual hinge loss function penalizing training errors and a concave penalty function of the expansion coefficients. The problem of minimizing the non-convex bound is addressed by a successive linearization approach, whereby the problem is transformed into a sequence of linear programs. The algorithm produced comparable error rates to the standard Support Vector Machine but significantly reduced the number of support vectors and the concomitant classification time.	algorithm;coefficient;concave function;data dependency;generalization error;hinge loss;kernel (operating system);kernel method;linear programming;loss function;penalty method;sparse matrix;support vector machine	Dori Peleg;Ron Meir	2009	Pattern Recognition	10.1016/j.patcog.2009.03.006	support vector machine;kernel method;computer science;linear programming;machine learning;signal processing;mathematics;algorithm;statistics	ML	22.98034667201626	-38.53327013232358	81750
235d74455db8208db1ca3c49b57b22a541a03807	dynamic multi-objective evolution of classifier ensembles for video face recognition	face recognition in video;artmap neural networks;multi classifier systems;incremental learning;ensemble methods;dynamic multi objective optimization;adaptive biometrics	Due to a limited control over changing operational conditions and personal physiology, systems used for video-based face recognition are confronted with complex and changing pattern recognition environments. Although a limited amount of reference data is initially available during enrollment, new samples often become available over time, through re-enrollment, post analysis and labeling of operational data, etc. Adaptive multi-classifier systems (AMCSs) are therefore desirable for the design and incremental update of facial models. For real time recognition of individuals appearing in video sequences, facial regions are captured with one or more cameras, and an AMCS must perform fast and efficient matching against the facial model of individual enrolled to the system. In this paper, an incremental learning strategy based on particle swarm optimization (PSO) is proposed to efficiently evolve heterogeneous classifier ensembles in response to new reference data. This strategy is applied to an AMCS where all parameters of a pool of fuzzy ARTMAP (FAM) neural network classifiers (i.e., a swarm of classifiers), each one corresponding to a particle, are co-optimized such that both error rate and network size are minimized. To provide a high level of accuracy over time while minimizing the computational complexity, the AMCS integrates information from multiple diverse classifiers, where learning is guided by an aggregated dynamical niching PSO (ADNPSO) algorithm that optimizes networks according both these objectives. Moreover, pools of FAM networks are evolved to maintain (1) genotype diversity of solutions around local optima in the optimization search space and (2) phenotype diversity in the objective space. Accurate and low cost ensembles are thereby designed by selecting classifiers on the basis of accuracy, and both genotype and phenotype diversity. For proof-of-concept validation, the proposed strategy is compared to AMCSs where incremental learning of FAM networks is guided through monoand multi-objective optimization. Performance is assessed in terms of video-based error rate and resource requirements under different incremental learning scenarios, where new data is extracted from real-world video streams (IIT-NRC and MoBo). Simulation results indicate that the proposed strategy provides a level of accuracy that is comparable to that of using mono-objective optimization and reference face recognition systems, yet requires a fraction of the computational cost (between 16% and 20% of a mono-objective strategy se an depending on the data ba	algorithm;algorithmic efficiency;artificial neural network;computation;computational complexity theory;facial recognition system;fuzzy associative matrix;fuzzy concept;high-level programming language;incremental backup;integrated information theory;international journal of applied mathematics and computer science;local optimum;mathematical optimization;multi-objective optimization;particle swarm optimization;pattern recognition;requirement;simulation;streaming media	Jean-François Connolly;Eric Granger;Robert Sabourin	2013	Appl. Soft Comput.	10.1016/j.asoc.2012.08.039	mathematical optimization;computer science;artificial intelligence;machine learning;pattern recognition;ensemble learning	ML	13.76938392718702	-45.97823978831574	81805
1ff61c9940696b10b4a028753638ffddd5a07d58	combining bagging, boosting, rotation forest and random subspace methods	ensembles of classifiers;learning algorithm;ensemble method;ensemble of classifiers;data mining;machine learning;pattern recognition;subspace method	Bagging, boosting, rotation forest and random subspace methods are well known re-sampling ensemble methods that generate and combine a diversity of learners using the same learning algorithm for the base-classifiers. Boosting and rotation forest algorithms are considered stronger than bagging and random subspace methods on noise-free data. However, there are strong empirical indications that bagging and random subspace methods are much more robust than boosting and rotation forest in noisy settings. For this reason, in this work we built an ensemble of bagging, boosting, rotation forest and random subspace methods ensembles with 6 sub-classifiers in each one and then a voting methodology is used for the final prediction. We performed a comparison with simple bagging, boosting, rotation forest and random subspace methods ensembles with 25 sub-classifiers, as well as other well known combining methods, on standard benchmark datasets and the proposed technique had better accuracy in most cases.	algorithm;approximation;artificial neural network;benchmark (computing);boosting (machine learning);decision tree;ensemble learning;gradient descent;greedy algorithm;local optimum;machine learning;mathematical optimization;random forest;random subspace method;sampling (signal processing);supervised learning;test set;variance reduction	Sotiris B. Kotsiantis	2010	Artificial Intelligence Review	10.1007/s10462-010-9192-8	random subspace method;random forest;computer science;machine learning;pattern recognition;data mining;gradient boosting	AI	14.772143212520202	-40.95668993806542	82035
8a7252aa442a0e59611c19455a96d98392b38993	representation learning via semi-supervised autoencoder for multi-task learning	semi supervised learning multi task learning representation learning autoencoder;decoding;training;autoencoder;会议论文;semi supervised learning;logistics;multi task learning;regression analysis knowledge representation learning artificial intelligence;predictive models;saml semisupervised autoencoder multitask learning knowledge sharing feature representation learning framework model parameter regularization methods regularized multitask softmax regression method;organizations;encoding;representation learning;training data models predictive models encoding decoding organizations logistics;data models	Multi-task learning aims at learning multiple related but different tasks. In general, there are two ways for multi-task learning. One is to exploit the small set of labeled data from all tasks to learn a shared feature space for knowledge sharing. In this way, the focus is on the labeled training samples while the large amount of unlabeled data is not sufficiently considered. Another way has a focus on how to share model parameters among multiple tasks based on the original features space. Here, the question is whether it is possible to combine the advantages of both approaches and develop a method, which can simultaneously learn a shared subspace for multiple tasks and learn the prediction models in this subspace? To this end, in this paper, we propose a feature representation learning framework, which has the ability in combining the autoencoders, an effective way to learn good representation by using large amount of unlabeled data, and model parameter regularization methods into a unified model for multi-task learning. Specifically, all the tasks share the same encoding and decoding weights to find their latent feature representations, based on which a regularized multi-task softmax regression method is used to find a distinct prediction model for each task. Also, some commonalities are considered in the prediction models according to the relatedness of multiple tasks. There are several advantages of the proposed model: 1) it can make full use of large amount of unlabeled data from all the tasks to learn satisfying representations, 2) the learning of distinct prediction models can benefit from the success of autoencoder, 3) since we incorporate the labeled information into the softmax regression method, so the learning of feature representation is indeed in a semi-supervised manner. Therefore, our model is a semi-supervised autoencoder for multi-task learning (SAML for short). Finally, extensive experiments on three real-world data sets demonstrate the effectiveness of the proposed framework. Moreover, the feature representation obtained in this model can be used by other methods to obtain improved results.	autoencoder;computer multitasking;experiment;feature learning;feature vector;machine learning;multi-task learning;multinomial logistic regression;security assertion markup language;semi-supervised learning;semiconductor industry;softmax function;unified model	Fuzhen Zhuang;Dan Luo;Xin Jin;Hui Xiong;Ping Luo;Qing He	2015	2015 IEEE International Conference on Data Mining	10.1109/ICDM.2015.22	semi-supervised learning;unsupervised learning;logistics;data modeling;feature learning;multi-task learning;computer science;organization;machine learning;pattern recognition;data mining;predictive modelling;autoencoder;encoding	ML	23.06687093373152	-45.605366486478324	82251
2948665db60d368624f5dd40e54b4941dc3487f7	kernelized sparse hashing for scalable image retrieval	kernel methods;hashing;sparse coding;image retrieval	Recently, hashing has been widely applied to large scale image retrieval applications due to its appealing query speed and low storage cost. The key idea of hashing is to learn a hash function that maps high dimensional data into compact binary codes while preserving the similarity structure in the original feature space. In this paper, we propose a new method called the Kernelized Sparse Hashing, which generates sparse hash codes with ?1 and non-negative regularizations. Compared to traditional hashing methods, our method only activates a small number of relevant bits on the hash code and hence provides a more compact and interpretable representation of data. Moreover, the kernel trick is introduced to capture the nonlinear similarity of features, and the local geometrical structure of data is explicitly considered in our method to improve the retrieval accuracy. Extensive experiments on three large-scale image datasets demonstrate the superior performance of our proposed method over the examined state-of-the-art techniques.	hash function;image retrieval;kernel method;scalability;sparse matrix	Yin Zhang;Weiming Lu;Yang Liu;Fei Wu	2016	Neurocomputing	10.1016/j.neucom.2015.02.080	feature hashing;hopscotch hashing;kernel method;hash table;double hashing;hash function;extendible hashing;dynamic perfect hashing;image retrieval;open addressing;computer science;theoretical computer science;machine learning;universal hashing;pattern recognition;k-independent hashing;locality preserving hashing;neural coding;2-choice hashing;locality-sensitive hashing	Vision	19.743014851666487	-46.48389537494366	82332
ad52dac8f267c8c75f30ac5b0c6c6bc980217285	stacking bagged and dagged models	bag stacking;learning algorithm;bagging;dagging;dag stacking;majority voting;working paper	In this paper, we investigate the method of stacked generalization in combining models derived from diierent subsets of a training dataset by a single learning algorithm, as well as diierent algorithms. The simplest way to combine predictions from competing models is majority vote, and the eeect of the sampling regime used to generate training subsets has already been studied in this context|when bootstrap samples are used the method is called bagging, and for disjoint samples we call it dagging. This paper extends these studies to stacked generalization, where a learning algorithm is employed to combine the models. This yields new methods dubbed bag-stacking and dag-stacking. We demonstrate that bag-stacking and dag-stacking can be eeective for classiication tasks even when the training samples cover just a small fraction of the full dataset. In contrast to earlier bagging results, we show that bagging and bag-stacking work for stable as well as unstable learning algorithms, as do dagging and dag-stacking. We nd that bag-stacking (dag-stacking) almost always has higher predictive accuracy than bagging (dagging), and we also show that bag-stacking models derived using two diier-ent algorithms is more eeective than bagging.	algorithm;bag-of-words model;bootstrap aggregating;control theory;directed acyclic graph;ensemble learning;focus stacking;machine learning;sampling (signal processing)	Kai Ming Ting;Ian H. Witten	1997			majority rule;bootstrap aggregating;computer science;machine learning;pattern recognition;data mining	ML	15.482427810920303	-40.128722279682236	82690
8aa57cf61e273d32a3f67cd1f461eb495cfa39b2	implementation and comparison of svm-based multi-task learning methods	model selection;support vector machines learning artificial intelligence;svm plus svm;svm plus svm classification land mine data model selection multi task learning mtl support vector machine;support vector machines;support vector machines kernel training standards zirconium vectors training data;classification;land mine data;support vector machine;learning artificial intelligence;multi task learning mtl;mtl svm based multitask learning methods inductive learning active research machine learning supervised learning applications multitask learning	Exploiting additional information to improve traditional inductive learning is an active research area in machine learning. In many supervised-learning applications, data can be naturally separated into several groups, or tasks, and incorporating this information into learning may improve generalization. There are many Multi-Task Learning (MTL) techniques for classification recently proposed in machine learning. This paper focuses on analysis and comparison of the two recent SVM-based MTL techniques: regularized MTL (rMTL) and SVM+ based MTL (SVM+MTL). In particular, our analysis shows how these two methods can be implemented using standard SVM software. Further, we present extensive empirical comparisons between these two methods, which relates advantages/limitations of each method to statistical characteristics of the training data.	computer multitasking;inductive reasoning;machine learning;multi-task learning;nonlinear system;supervised learning;support vector machine;synthetic intelligence	Han-Tai Shiao;Vladimir Cherkassky	2012	The 2012 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2012.6252442	semi-supervised learning;support vector machine;multi-task learning;least squares support vector machine;instance-based learning;computer science;online machine learning;machine learning;pattern recognition;data mining;computational learning theory;ranking svm;active learning;structured support vector machine;generalization error	ML	19.743554882797394	-41.8236927068878	82741
432d8cba544bf7b09b0455561fea098177a85db1	towards a neural statistician		An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.	artificial neural network;autoencoder;cluster analysis;generative model;supervised learning;unsupervised learning;variational principle	Harrison A Edwards;Amos J. Storkey	2016	CoRR		computer science;data science;machine learning;data mining	ML	21.33522092266552	-45.06940126391922	82985
55b010e6c4482c32ab46132d5a83c46fa3725cb5	a quantum-inspired version of the nearest mean classifier		We introduce a framework suitable for describing standard classification problems using the mathematical language of quantum states. In particular, we provide a one-to-one correspondence between real objects and pure density operators. This correspondence enables us: i) to represent the Nearest Mean Classifier (NMC) in terms of quantum objects, ii) to introduce a quantum-inspired version of the NMC called Quantum Classifier (QC). By comparing the QC with the NMC on different datasets, we show how the first classifier is able to provide additional information that can be beneficial on a classical computer with respect to the second classifier.	bloch sphere;cognition;computational complexity theory;computational linguistics;density matrix;feature model;hoc (programming language);namecoin;one-to-one (data model);pattern recognition;projection-slice theorem;quantum algorithm;quantum computing;quantum state;stereoscopy;trace distance	Giuseppe Sergioli;Enrica Santucci;Luca Didaci;Jaroslaw Adam Miszczak;Roberto Giuntini	2018	Soft Comput.	10.1007/s00500-016-2478-2	margin classifier;quadratic classifier;machine learning;data mining;mathematics;algorithm	ML	17.063261984158316	-45.341969942588655	83586
7dace71442b7d90685ee394be47ba74cb9c7021e	unified dual for bi-class svm approaches	kernels;qp problem;dual problem;large margin principle;svm;binary classification	SVM theory was originally developed on the basis of a separable binary classification problem, and other approaches have been later introduced. In this paper is demonstrated that all these approaches admit the same dual problem formulation. Let Z = ((x1, y1), . . . , (xn, yn)) = (z1, . . . , zn) ∈ (X × Y)n be a training set, where X is the input space and Y = {θ1, θ2} = {−1, +1} the output space. Let φ : X → F be a feature mapping and x def = φ(x) ∈ F be the representation of x ∈ X . A linear classifier, fw(x) = 〈x,w〉− b is sought in the space F , with fw : X → R, and outputs are obtained as hw(x) = sign(fw(x)). Let Z1 and Z2 be the patterns belonging to the classes labelled as {+1,−1}, respectively, and ni = #Zi, then it is defined, β = minzi∈Z1 〈xi,w〉, α = maxzj∈Z2 〈xj,w〉 so that when classes are linearly separable in the feature space, then α ≤ β. Hence, classifier w with the largest geometrical margin on a given training sample Z is wSV M def = arg max w∈F ;α,β∈IR β − α ‖w‖ Several approaches can be derived to translate this problem in an optimization problem (see Table 1). Main contribution of this work is that all the existing QP problem formulations are the same when dualized, as established by the following theorem. Theorem 1 Dual expression of the optimization problems (1), (2), (3) and (4) can be formulated as: min u∈IRn1 ,v∈IRn2 1 2 ∥∥∥∥∥ n1 ∑	binary classification;binary prefix;duality (optimization);feature vector;like button;linear classifier;linear separability;mathematical optimization;maxima and minima;optimization problem;test set	Luis González Abril;Cecilio Angulo;Francisco Velasco Morente;Andreu Català	2005	Pattern Recognition	10.1016/j.patcog.2005.03.019	binary classification;support vector machine;mathematical optimization;duality;computer science;machine learning;pattern recognition;mathematics	ML	22.157473953301977	-38.75698231154639	83719
9b948d2dd9c1740ae9705d47ba2bd05a0241e49b	a new mutual information based measure for feature selection	filter evaluation function;journal article;information measure;mutual information;feature selection	In this paper, we discuss the problem of feature selection and the importance of using mutual information in evaluating the discrimination ability of feature subsets between class labels. Because of the difficulties associated with estimating the exact value of mutual information, we propose a new evaluation measure that is based on the information gain and takes into consideration the interaction between features. The proposed measure is integrated into a robust feature selection scheme and compared with the well-known mutual information feature selection (MIFS) algorithm using the problems of texture classification, speech segment classification and speaker identification.	feature selection;mutual information	Ahmed Al-Ani;Mohamed A. Deriche;Jalel Chebil	2003	Intell. Data Anal.		variation of information;computer science;multivariate mutual information;machine learning;pattern recognition;data mining;mathematics;mutual information;feature selection;interaction information;feature;statistics;pointwise mutual information	AI	11.804760179390046	-43.935888270743284	84028
75fbe237e5893eee0f38468aff6734d4c71cd814	supersparse linear integer models for predictive scoring systems	supervised learning;interpretability;sparsity;binary classification;scoring systems	Scoring systems are classification models that make predictions using a sparse linear combination of variables with integer coefficients. Such systems are frequently used in medicine because they are interpretable; that is, they only require users to add, subtract and multiply a few meaningful numbers in order to make a prediction. See, for instance, these commonly used scoring systems: (Gage et al. 2001; Le Gall et al. 1984; Le Gall, Lemeshow, and Saulnier 1993; Knaus et al. 1985). Scoring systems strike a delicate balance between accuracy and interpretability that is difficult to replicate with existing machine learning algorithms. Current linear methods such as the lasso, elastic net and LARS are not designed to create scoring systems, since regularization is primarily used to improve accuracy as opposed to sparsity and interpretability (Tibshirani 1996; Zou and Hastie 2005; Efron et al. 2004). These methods can produce very sparse models through heavy regularization or feature selection methods (Guyon and Elisseeff 2003); however, feature selection often relies on greedy optimization and cannot guarantee an optimal balance between sparsity and accuracy. Moreover, the interpretability of scoring systems requires integer coefficients, which these methods do not produce. Existing approaches to interpretable modeling include decision trees and lists (Rüping 2006; Quinlan 1986; Rivest 1987; Letham et al. 2013). We introduce a formal approach for creating scoring systems, called Supersparse Linear Integer Models (SLIM). SLIM produces scoring systems that are accurate and interpretable using a mixed-integer program (MIP) whose objective penalizes the training error, L0-norm and L1-norm of its coefficients. SLIM can create scoring systems for datasets with thousands of training examples and tens to hundreds of features larger than the sizes of most studies in medicine, where scoring systems are often used.	coefficient;decision tree;elastic map;elastic net regularization;feature selection;greedy algorithm;integer programming;lasso;linear programming;machine learning;mathematical optimization;matrix regularization;ross quinlan;self-replicating machine;sparse matrix;taxicab geometry;word lists by frequency	Berk Ustun;Stefano Traca;Cynthia Rudin	2013			binary classification;computer science;machine learning;pattern recognition;data mining;mathematics;supervised learning;sparsity-of-effects principle;statistics	AI	20.876617830046747	-38.30559294253831	84063
d951f2e8cc5df639d5b3b420f9c817f68f681406	avc: selecting discriminative features on basis of auc by maximizing variable complementarity	computational biology bioinformatics;algorithms;computer appl in life sciences;microarrays;bioinformatics	The Receiver Operator Characteristic (ROC) curve is well-known in evaluating classification performance in biomedical field. Owing to its superiority in dealing with imbalanced and cost-sensitive data, the ROC curve has been exploited as a popular metric to evaluate and find out disease-related genes (features). The existing ROC-based feature selection approaches are simple and effective in evaluating individual features. However, these approaches may fail to find real target feature subset due to their lack of effective means to reduce the redundancy between features, which is essential in machine learning. In this paper, we propose to assess feature complementarity by a trick of measuring the distances between the misclassified instances and their nearest misses on the dimensions of pairwise features. If a misclassified instance and its nearest miss on one feature dimension are far apart on another feature dimension, the two features are regarded as complementary to each other. Subsequently, we propose a novel filter feature selection approach on the basis of the ROC analysis. The new approach employs an efficient heuristic search strategy to select optimal features with highest complementarities. The experimental results on a broad range of microarray data sets validate that the classifiers built on the feature subset selected by our approach can get the minimal balanced error rate with a small amount of significant features. Compared with other ROC-based feature selection approaches, our new approach can select fewer features and effectively improve the classification performance.	avc brand of sulfanilamide;area under curve;dimensions;feature selection;h.264/mpeg-4 avc;heuristic;heuristics;machine learning;microarray;roc curve;receiver operator characteristics;subgroup	Lei Sun;Jun Wang;Jinmao Wei	2017		10.1186/s12859-017-1468-4	biology;dna microarray;computer science;bioinformatics;machine learning;pattern recognition;data mining;feature	AI	11.19691868796389	-45.21736518100781	84114
75735e111d5b9f24b9518242edc1f20492490423	transductive multi-label learning for video concept detection	transductive learning;video concept detection;support vector machine;multi label interdependence	Transductive video concept detection is an effective way to handle the lack of sufficient labeled videos. However, another issue, the multi-label interdependence, is not essentially addressed in the existing transductive methods. Most solutions only applied the transductive single-label approach to detect each individual concept separately, but ignoring the concept relation, or simply imposed the smoothness assumption over the multiple labels for each video, without indeed exploring the interdependence between the concepts. On the other hand, the semi-supervised extension of supervised multi-label classifiers, such as correlative multi-label support vector machines, is usually intractable and hence impractical due to the quite expensive computational cost. In this paper, we propose an effective transductive multi-label classification approach, which simultaneously models the labeling consistency between the visually similar videos and the multi-label interdependence for each video in an integrated framework. We compare the performance between the proposed approach and several representative transductive single-label and supervised multi-label classification approaches for the video concept detection task over the widely-used TRECVID data set. The comparative results demonstrate the superiority of the proposed approach.	algorithmic efficiency;computation;interdependence;multi-label classification;semiconductor industry;support vector machine	Jingdong Wang;Yinghai Zhao;Xiuqing Wu;Xian-Sheng Hua	2008		10.1145/1460096.1460145	support vector machine;transduction;computer science;machine learning;pattern recognition;data mining	AI	24.161057148399493	-44.28385254879761	84115
74aeca09284f0941a9c7e7ace0af1cabf0795156	a novel multi-task tensor correlation neural network for facial attribute prediction		Face multi-attribute prediction benefits substantially from multi-task learning (MTL), which learns multiple face attributes simultaneously to achieve shared or mutually related representations of different attributes. The most widely used MTL convolutional neural network is heuristically or empirically designed by sharing all of the convolutional layers and splitting at the fully connected layers for task-specific losses. However, it is improper to view all low and midlevel features for different attributes as being the same, especially when these attributes are only loosely related. In this paper, we propose a novel multi-attribute tensor correlation neural network (MTCN) for face attribute prediction. The structure shares the information in low-level features (e.g., the first two convolutional layers) but splits that in high-level features (e.g., from the third convolutional layer to the fully connected layer). At the same time, during high-level feature extraction, each subnetwork (e.g., AgeNet, Gender-Net, ..., and Smile-Net) excavates closely related features from other networks to enhance its features. Then, we project the features of the C9 layers of the finetuned subnetworks into a highly correlated space by using a novel tensor correlation analysis algorithm (NTCCA). The final face attribute prediction is made based on the correlation matrix. Experimental results on benchmarks with multiple face attributes (CelebA and LFWA) show that the proposed approach has superior performance compared to state-of-the-art methods.		Mingxing Duan;Keqin Li;Qi Tian	2018	CoRR		machine learning;artificial neural network;tensor;artificial intelligence;pattern recognition;computer science;convolutional neural network;covariance matrix;feature extraction;heuristic;subnetwork;correlation	AI	24.55062795286096	-46.38519880061238	84260
e1949127fee1d61f28afece5f25a7f4ceddbfbe5	bagging schemes on the presence of class noise in classification	classification noise;imprecise probabilities;ensemble decision trees;information based uncertainty measures;imprecise dirichlet model	In this paper, we study one application of Bagging credal decision tree, i.e. decision trees built using imprecise probabilities and uncertainty measures, on data sets with class noise (data sets with wrong assignations of the class label). For this aim, previously we also extend a original method that build credal decision trees to one which works with continuous features and missing data. Through an experimental study, we prove that Bagging credal decision trees outperforms more complex Bagging approaches on data sets with class noise. Finally, using a bias–variance error decomposition analysis, we also justify the performance of the method of Bagging credal decision trees, showing that it achieves a stronger reduction of the variance error component.		Joaquín Abellán;Andrés R. Masegosa	2012	Expert Syst. Appl.	10.1016/j.eswa.2012.01.013	machine learning;pattern recognition;data mining;mathematics;statistics	Crypto	15.79400027109974	-38.65296774496681	84561
53d39f9260c6ee29dbf118382be91ea09475e628	interactive objects retrieval with efficient boosting	range query;supervised learning;time complexity;efficiency;lsh;object retrieval;boosting;local features;scalability;relevance feedback;similarity search	This paper presents an efficient local features boosting strategy for interactive objects retrieval tasks such as on-line supervised learning or relevance feedback. The prediction time complexity of most existing methods is indeed usually linear in dataset size since the retrieval works by applying a trained classifier on the images of the dataset one by one. In our method, the trained classifier can be computed directly on the whole dataset in sublinear time thanks to distance-based weak classifiers. The idea is to speed-up drastically the prediction of each weak classifier on the whole dataset by performing approximate range queries with an efficient similarity search structure. Experiments on Caltech 256 dataset show that the technique is up to 250 times faster than the naive exhaustive method. Thanks to this efficiency improvement, we developed a relevance feedback mechanism on image regions freely selected by the user and we show how it improves the effectiveness of the retrieval.	acm-mm;allen holub;approximation algorithm;boosting (machine learning);cvpr;computer vision;comstock–needham system;database;image retrieval;information retrieval;locality of reference;locality-sensitive hashing;mach;newton's method;online and offline;outline of object recognition;pattern recognition;performance;range query (data structures);relevance feedback;scale-invariant feature transform;similarity search;supervised learning;time complexity;lsh	Saloua Ouertani-Litayem;Alexis Joly;Nozha Boujemaa	2009		10.1145/1631272.1631352	time complexity;range query;scalability;computer science;machine learning;pattern recognition;data mining;efficiency;supervised learning;boosting	Vision	18.04016889537811	-46.544316676567966	84609
c17296984bffdfdfe478f19bb700a69c94d6f8ef	learning graph-structured sum-product networks for probabilistic semantic maps		We introduce Graph-Structured Sum-Product Networks (GraphSPNs), a probabilistic approach to structured prediction for problems where dependencies between latent variables are expressed in terms of arbitrary, dynamic graphs. While many approaches to structured prediction place strict constraints on the interactions between inferred variables, many real-world problems can be only characterized using complex graph structures of varying size, often contaminated with noise when obtained from real data. Here, we focus on one such problem in the domain of robotics. We demonstrate how GraphSPNs can be used to bolster inference about semantic, conceptual place descriptions using noisy topological relations discovered by a robot exploring large-scale office spaces. Through experiments, we show that GraphSPNs consistently outperform the traditional approach based on undirected graphical models, successfully disambiguating information in global semantic maps built from uncertain, noisy local evidence. We further exploit the probabilistic nature of the model to infer marginal distributions over semantic descriptions of as yet unexplored places and detect spatial environment configurations that are novel and incongruent with	experiment;file spanning;graph (abstract data type);graph (discrete mathematics);graphical model;interaction;latent variable;map;marginal model;mobile robot;robotics;robustness (computer science);semantic mapper;semantic network;sensor;structured prediction;substitution-permutation network	Kaiyu Zheng;Andrzej Pronobis;Rajesh P. N. Rao	2018			marginal distribution;machine learning;artificial intelligence;mathematics;exploit;probabilistic logic;inference;graph;structured prediction;graphical model;latent variable	AI	21.660138846166188	-46.774692180479335	84653
da28deea8fca7ab773f0ee7765d424682e5b8c5f	minimum classification error training for choquet integrals with applications to landmine detection	sugeno lambda measure;fuzzy measure;landmine detection;least square error;integral equations;image fusion;least squared error lse;objective function;choquet integral;minimum classification error mce;discriminative training;fuzzy measures;minimum classification error	A novel algorithm for discriminative training of Choquet-integral-based fusion operators is described. Fusion is performed by Choquet integration of classifier outputs with respect to fuzzy measures. The fusion operators are determined by the parameters of fuzzy measures. These parameters are found by minimizing a minimum classification error (MCE) objective function. The minimization is performed with respect to a special class of measures, the Sugeno lambda-measures. An analytic expression is derived for the gradient of the Choquet integral with respect to the Sugeno lambda-measure. The new algorithm is applied to a landmine detection problem, and compared to previous techniques.	algorithm;fuzzy measure theory;gradient;linuxmce;loss function;optimization problem	Andres Mendez-Vazquez;Paul D. Gader;James M. Keller;Kenneth Chamberlin	2008	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2007.902024	mathematical optimization;machine learning;pattern recognition;fuzzy measure theory;mathematics;choquet integral;image fusion;integral equation	Vision	20.029569078018127	-40.00368611252528	84732
4327ddae0b00fdd5fc388595f6b61f0b9fed90a6	cost-sensitive self-training	cost sensitive;naive bayes;self training	In some real-world applications, it is time-consuming or expensive to collect much labeled data, while unlabeled data is easier to obtain. Many semi-supervised learning methods have been proposed to deal with this problem by utilizing the unlabeled data. On the other hand, on some datasets, misclassifying different classes causes different costs, which challenges the common assumption in classification that classes have the same misclassification cost. For example, misclassifying a fraud as a legitimate transaction could be more serious than misclassifying a legitimate transaction as fraudulent. In this paper, we propose a cost-sensitive self-training method (CS-ST) to improve the performance of Naive Bayes when labeled instances are scarce and different misclassification errors are associated with different costs. CS-ST incorporates the misclassification costs into the learning process of self-training, and approximately estimates the misclassification error to help select unlabeled instances. Experiments on 13 UCI datasets and three text datasets show that, in terms of the total misclassification cost and the number of correctly classified instances with higher costs, CS-ST has better performance than the self-training method and the base classifier learned from the original labeled data only.	elegant degradation;ensemble learning;experiment;iteration;naive bayes classifier;sampling (signal processing);semi-supervised learning;semiconductor industry;sensitivity and specificity;statistical classification;supervised learning;teaching method	Yuanyuan Guo;Harry Zhang;Bruce Spencer	2012		10.1007/978-3-642-30353-1_7	naive bayes classifier;computer science;machine learning;pattern recognition;data mining;statistics	AI	15.884231367144992	-40.60164494515614	84854
bb1bc9df5e9cec3e8a03a027b8016b8fc25be73a	improving bi-directional generation between different modalities with variational autoencoders		We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. A major approach to achieve this objective is to train a model that integrates all the information of different modalities into a joint representation and then to generate one modality from the corresponding other modality via this joint representation. We simply applied this approach to variational autoencoders (VAEs), which we call a joint multimodal variational autoencoder (JMVAE). However, we found that when this model attempts to generate a large dimensional modality missing at the input, the joint representation collapses and this modality cannot be generated successfully. Furthermore, we confirmed that this difficulty cannot be resolved even using a known solution. Therefore, in this study, we propose two models to prevent this difficulty: JMVAE-kl and JMVAE-h. Results of our experiments demonstrate that these methods can prevent the difficulty above and that they generate modalities bi-directionally with equal or higher likelihood than conventional VAE methods, which generate in only one direction. Moreover, we confirm that these methods can obtain the joint representation appropriately, so that they can generate various variations of modality by moving over the joint representation or changing the value of another modality.	autoencoder;experiment;generative model;iterative method;modality (human–computer interaction);multimodal interaction;norm (social);variational principle	Masahiro Suzuki;Kotaro Nakayama;Yutaka Matsuo	2018	CoRR		generative grammar;artificial intelligence;machine learning;autoencoder;mathematics;modalities;versa	AI	23.65697132942413	-48.846746001789946	84912
d797b1a8656acf2763b62976d03a33e0342c9f8d	geometrical learning, descriptive geometry, and biomimetic pattern recognition	biomimetic pattern recognition;high dimensional descriptive geometry;learning algorithm;high dimensionality;information geometry;radial basis function;machine learning;geometry learning;pattern recognition;higher dimensions;learning problems;descriptive geometry;support vector machine;rbf network	Studies on learning problems from geometry perspective have attracted an ever increasing attention in machine learning, leaded by achievements on information geometry. This paper proposes a different geometrical learning from the perspective of high-dimensional descriptive geometry. Geometrical properties of high-dimensional structures underlying a set of samples are learned via successive projections from the higher dimension to the lower dimension until two-dimensional Euclidean plane, under guidance of the established properties and theorems in high-dimensional descriptive geometry. Specifically, we introduce a hyper sausage like geometry shape for learning samples and provides a geometrical learning algorithm for specifying the hyper sausage shapes, which is then applied to biomimetic pattern recognition. Experimental results are presented to show that the proposed approach outperforms three types of support vector machines with either a three degree polynomial kernel or a radial basis function kernel, especially in the cases of high-dimensional samples of a finite size.	biomimetics;pattern recognition	Shoujue Wang;Jiangliang Lai	2005	Neurocomputing	10.1016/j.neucom.2004.11.034	support vector machine;radial basis function;computer science;artificial intelligence;machine learning;pattern recognition;mathematics;information geometry	Vision	23.002166337431046	-40.53597562923356	85227
d09062618b8347aea259d2e78762d837d9f63f20	generative adversarial networks with inverse transformation unit		In this paper we introduce a new structure to Generative Adversarial Networks by adding an inverse transformation unit behind the generator. We present two theorems to claim the convergence of the model, and two conjectures to nonideal situations when the transformation is not bijection. A general survey on models with different transformations was done on the MNIST dataset and the Fashion-MNIST dataset, which shows the transformation does not necessarily need to be bijection. Also, with certain transformations that blurs an image, our model successfully learned to sharpen the images and recover blurred images, which was additionally verified by our measurement of sharpness.	acutance;algorithm;computer vision;convolution;discriminator;experiment;generative adversarial networks;mnist database	Zhifeng Kong;Shuo Ding	2017	CoRR		bijection;pattern recognition;computer science;artificial intelligence;generative grammar;machine learning;inverse;mnist database;adversarial system;convergence (routing)	Vision	23.45086882128317	-49.44000779159034	85244
2b3078d7af96e585bbe5386701f4c4645262f744	a scalable approach to simultaneous evolutionary instance and feature selection	instance based learning;instance selection;very large problems;simultaneous instance and feature selection;feature selection	An enormous amount of information is continually being produced in current research, which poses a challenge for data mining algorithms. Many of the problems in extremely active research areas, such as bioinformatics, security and intrusion detection and text mining, involve large or enormous datasets. These datasets pose serious problems for many data mining algorithms. One method to address very large datasets is data reduction. Among the most useful data reduction methods is simultaneous instance and feature selection. This method achieves a considerable reduction in the training data while maintaining, or even improving, the performance of the data-mining algorithm. However, it suffers from a high degree of scalability problems, even for medium-sized datasets. In this paper, we propose a new evolutionary simultaneous instance and feature selection algorithm that is scalable to millions of instances and thousands of features. This proposal is based on the divide-and-conquer principle combined with bookkeeping. The divide-and-conquer principle allows the execution of the algorithm in linear time. Furthermore, the proposed method is easy to implement using a parallel environment and can work without loading the entire dataset into memory. Using 50 medium-sized datasets, we will demonstrate our method's ability to match the results of state-of-the-art instance and feature selection methods while significantly reducing the time requirements. Using 13 very large datasets, we will demonstrate the scalability of our proposal to millions of instances and thousands of features.		Nicolás García-Pedrajas;Aida de Haro-García;Javier Pérez-Rodríguez	2013	Inf. Sci.	10.1016/j.ins.2012.10.006	computer science;machine learning;pattern recognition;data mining;feature selection	AI	11.925843573135323	-40.138571851594676	85541
78adff23dae71167d3bc4d642622d8ccdf492d02	support vector machines and perceptrons		Support vector machines (SVMs) have been successfully used in a variety of data mining and machine learning applications. One of the most popular applications is pattern classification. SVMs are so well-known to the pattern classification community that by default, researchers in this area use them as baseline classifiers to establish the superiority of the classifier proposed by them. In this chapter, we introduce some of the important terms associated with support vector machines and a brief history of their evolution.	baseline (configuration management);data mining;machine learning;perceptron;statistical classification;support vector machine	M. N. Murty;Rashmi Raghava	2016		10.1007/978-3-319-41063-0	support vector machine;perceptron;machine learning;artificial intelligence;computer science	ML	11.685159039931012	-40.70641914712976	85561
dc86e21375eff61e20f8721f0f89c76a473e810a	relevant pattern selection for subspace learning	support vector machines support vector machine classification degradation feature extraction design methodology algorithm design and analysis training data clustering algorithms entropy scattering;data selection method;pattern recognition data handling;subspace learning;support vector machines;niobium;visual tracking problems;visualization;performance improvement;feature extraction;neighborhood property;classification algorithms;pattern recognition;class information;relevant pattern selection;data handling;decision boundary pattern;decision nonboundary pattern;target tracking;visual tracking;visual tracking problems relevant pattern selection subspace learning data selection method decision boundary pattern decision nonboundary pattern class information neighborhood property;data models	In this paper, we propose a scheme to improve the performance of subspace learning by using a pattern (data) selection method as preprocessing. Generally, a training set for subspace learning contains irrelevant or unreliable samples, and removing these samples can improve the learning performance. For this purpose, we use pattern selection preprocessing which discriminates decision boundary/non-boundary patterns by class information and neighborhood property, and removes boundary patterns. Performance improvement by pattern selection is investigated for classification and visual tracking problems, and compared with those of the previous methods.	cisco pix;decision boundary;feature extraction;preprocessor;relevance;test set;transformation matrix;video tracking	Jin Hee Na;Seok Min Yun;Minsoo Kim;Jin Young Choi	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761269	statistical classification;data modeling;support vector machine;niobium;visualization;eye tracking;feature extraction;computer science;machine learning;group method of data handling;pattern recognition;data mining	Vision	15.048013320646556	-45.59967864916636	86013
558c054b2b8774a3165a7be024c491eadc82515f	relevance feedback for content-based image retrieval using proximal support vector machine	regularized least squares;image database;support vector machine;content based image retrieval;relevance feedback	  In this paper, we present a novel relevance feedback algorithm for content-based image retrieval using the PSVM (Proximal  Support Vector Machine). The PSVM seeks to find the optimal separating hyperplane by “regularized least squares”. The obtained  hyperplane comprises the positive and negative “proximal planes”. We interpret the proximal vectors on the proximal planes  as the representatives among training samples, and propose to use the distance from the positive proximal plane as a measure  of image dissimilarity. In order to reduce computational time for relevance feedback, we introduce the “expanded sets” derived  from the pre-computed dissimilarity matrix, and apply the feedback algorithm to these expanded sets rather than the entire  image database, while preserving the comparable precision rate. We demonstrate the efficacy of the proposed scheme using unconstrained  image databases that were obtained from the Web.    	content-based image retrieval;relevance feedback;support vector machine	YoungSik Choi;JiSung Noh	2004		10.1007/978-3-540-24709-8_99	support vector machine;computer science;machine learning;pattern recognition;relevance vector machine;automatic image annotation;structured support vector machine;information retrieval	Vision	24.050591764997247	-41.25813616495703	86232
d65b1fa54572cd87a25e5ec9d26a9493693a9f1e	calibration for stratified classification models		In classification problems, sampling bias between training data and testing data is critical to the ranking performance of classification scores. Such bias can be both unintentionally introduced by data collection and intentionally introduced by the algorithm, such as under-sampling or weighting techniques applied to imbalanced data. When such sampling bias exists, using the raw classification score to rank observations in the testing data can lead to suboptimal results. In this paper, I investigate the optimal calibration strategy in general settings, and develop a practical solution for one specific sampling bias case, where the sampling bias is introduced by stratified sampling. The optimal solution is developed by analytically solving the problem of optimizing the ROC curve. For practical data, I propose a ranking algorithm for general classification models with stratified data. Numerical experiments demonstrate that the proposed algorithm effectively addresses the stratified sampling bias issue. Interestingly, the proposed method shows its potential applicability in two other machine learning areas: unsupervised learning and model ensembling, which can be future research topics.	algorithm;discriminant;experiment;generative model;machine learning;numerical method;receiver operating characteristic;sampling (signal processing);semiconductor industry;statistical classification;stratified sampling;unsupervised learning	Chandler Zuo	2017	CoRR		calibration;statistics;mathematics;data collection;unsupervised learning;stratified sampling;sampling bias;test data;ranking;weighting	ML	17.15871381953127	-39.603778207533516	86237
8e79cfade127e1f49cb1e92e612cb823136c08ab	feature selection via decision tree surrogate splits	pediatrics;decision tree;importance estimation feature selection decision tree surrogate splits cart variable ranking;scattering;decision trees impurities biomedical measurements entropy phase locked loops laboratories genetic algorithms training data;accuracy;importance sampling decision trees;pattern recognition;feature selection;importance sampling;decision trees;impurities;mimics	CARTpsilas ldquovariable rankingrdquo provides a quick estimate of the importance of an individual feature in a decision tree, and it is based on surrogate splits. We extend this estimate to arbitrary subsets. We have applied our estimate (called ldquodIrdquo) to three datasets. The performance of dI as an importance estimate is very dependent on the underlying performance of the tree used to generate the surrogate splits.	decision tree;feature selection	Clayton Springer;W. Philip Kegelmeyer	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761257	decision tree learning;computer science;machine learning;decision tree;pattern recognition;incremental decision tree;data mining;feature selection;id3 algorithm;statistics	DB	11.94517391460231	-39.02871045665763	86327
6cc42d54af8833fcce83a555fccde2fe1e28b249	fisher vectors meet neural networks: a hybrid classification architecture	computer architecture kernel standards training pipelines principal component analysis feature extraction;cnn fisher vectors convolutional neural networks image classification hybrid classification architecture fv classifiers unsupervised layers subsequent fully connected supervised layers back propagation;neural nets backpropagation image classification	Fisher Vectors (FV) and Convolutional Neural Networks (CNN) are two image classification pipelines with different strengths. While CNNs have shown superior accuracy on a number of classification tasks, FV classifiers are typically less costly to train and evaluate. We propose a hybrid architecture that combines their strengths: the first unsupervised layers rely on the FV while the subsequent fully-connected supervised layers are trained with back-propagation. We show experimentally that this hybrid architecture significantly outperforms standard FV systems without incurring the high cost that comes with CNNs. We also derive competitive mid-level features from our architecture that are readily applicable to other class sets and even to new tasks.	backpropagation;computer vision;convolutional neural network;experiment;farmville;fisher–yates shuffle;neural networks;pipeline (computing);software propagation	Florent Perronnin;Diane Larlus	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2015.7298998	computer science;artificial intelligence;machine learning;pattern recognition	Vision	22.9764404826089	-51.310199129811345	86372
ca5741bfe06aa1e07ed70156c21b5d1bd53cf326	a general framework for co-training and its applications	social activity recognition;semi supervised learning;human action recognition;co training;multi view	Co-training is one of the major semi-supervised learning paradigms in which two classifiers are alternately trained on two distinct views and they teach each other by adding the predictions of unlabeled data to the training set of the other view. Co-training can achieve promising performance, especially when there is only a small number of labeled data. Hence, cotraining has received considerable attention, and many variant co-training algorithms have been developed. It is essential and informative to provide a systematic framework for a better understanding of the common properties and differences in these algorithms. In this paper, we propose a general framework for co-training according to the diverse learners constructed in co-training. Specifically, we provide three types of co-training implementations, including co-training on multiple views, co-training on multiple classifiers, and co-training on multiple manifolds. Finally, comprehensive experiments of different methods are conducted on the UCF-iPhone dataset for human action recognition and the USAA dataset for social activity recognition. The experimental results demonstrate the effectiveness of the proposed solutions.	activity recognition;algorithm;co-training;experiment;information;semi-supervised learning;semiconductor industry;supervised learning;test set	Weifeng Liu;Yang Li;Dapeng Tao;Yanjiang Wang	2015	Neurocomputing	10.1016/j.neucom.2015.04.087	semi-supervised learning;computer science;machine learning;pattern recognition;data mining	ML	22.70202786663246	-44.51565784229887	86470
7e829a9dd8958679865834dc6ef069efbf097f8c	simultaneously removing noise and selecting relevant features for high dimensional noisy data	machine learning high dimensional noisy data classification noise removal relevant feature selection genetic algorithm noise detection prototype selection;pattern classification feature extraction genetic algorithms learning artificial intelligence;relevant feature selection;high dimensionality;noisy data;prototype selection;noise measurement;training data;accuracy;machine learning;feature extraction;classification algorithms;pattern classification;prototype selection noise detection outlier detction feature selection genetic algorithm;genetic algorithm;genetic algorithms;feature selection;classification tree analysis;noise detection;learning artificial intelligence;high dimensional noisy data classification;classification accuracy;high dimension;noise removal;outlier detction;noise;genetic algorithms training data filtering filters machine learning application software computer science noise generators prototypes nearest neighbor searches	The classification for the noisy training data in high dimension suffers from concurrent negative effects by noise and irrelevant/redundant features. Noise disrupts the training data and irrelevant/redundant features prevent the classifier from picking relevant features in building the model. Therefore they may reduce classification accuracy. This paper introduces a novel approach to improve the quality of training data sets with noisy dependent variable and high dimensionality by simultaneously removing noisy instances and selecting relevant features for classification. Our approach relies on two genetic algorithms, one for noise detection and the other for feature selection, and allows them to exchange their results periodically at certain generation intervals. Prototype selection is used to improve the performance along with the genetic algorithm in the noise detection method. This paper shows that our approach enhances the quality of noisy training data sets with high dimension and substantially increases the classification accuracy.	feature selection;genetic algorithm;negative feedback;prototype;relevance;signal-to-noise ratio;statistical classification	Boseon Byeon;Khaled Rasheed	2008	2008 Seventh International Conference on Machine Learning and Applications	10.1109/ICMLA.2008.87	genetic algorithm;computer science;machine learning;pattern recognition;data mining;feature selection	Robotics	12.783825945097023	-42.31876429864359	86508
bdacadfd332b94bd2f70c0a2b9354cbe9b53f182	cell algorithms with data inflation for non-parametric classification	remote sensing image;k nn classifier;data inflation;nearest neighbor;classification error;non parametric classification;k nearest neighbor	The k-nearest neighbor (k-NN) classifier represents one of the most popular non-parametric classification tools. Its main drawback is the computational cost required during the search for the nearest neighbors. In this paper, we propose using two cell algorithms with data inflation as tools capable to achieve interesting tradeoffs between classification error and computational cost. The performances of the proposed algorithms are assessed experimentally on the basis of a multisensor remotely sensed image and a pen-based handwritten digit data set.	algorithm;cell (microprocessor)	Alessandro M. Palau;Farid Melgani;Sebastiano B. Serpico	2006	Pattern Recognition Letters	10.1016/j.patrec.2005.11.001	large margin nearest neighbor;computer science;machine learning;pattern recognition;data mining;k-nearest neighbors algorithm	Vision	14.01941081337506	-43.39658898778343	86509
f73ef90f769ddc221db6ecfddfb7ef8a6f3cbfe9	a conflict-based confidence measure for associative classification	itemsets;electronic mail;sorting;particle measurements;confidence measure;testing;association rules;data mining;training data;interleaved codes;accuracy;distance measurement;association rule;classification algorithms;mathematical model;classification accuracy;training data distance measurement classification algorithms accuracy testing mathematical model equations;imbalanced data sets;power measurement;electric variables measurement	Associative classification has aroused significant attention recently and achieved promising results. In the rule ranking process, the confidence measure is usually used to sort the class association rules (CARs). However, it may be not good enough for a classification task due to a low discrimination power to instances in the other classes. In this paper, we propose a novel conflict-based confidence measure with an interleaving ranking strategy for re-ranking CARs in an associative classification framework, which better captures the conflict between a rule and a training data instance. In the experiments, the traditional confidence measure and our proposed conflict-based confidence measure with the interleaving ranking strategy are applied as the primary sorting criterion for CARs. The experimental results show that the proposed associative classification framework achieves promising classification accuracy with the use of the conflict-based confidence measure, particularly for an imbalanced data set.	association rule learning;experiment;forward error correction;linear discriminant analysis;principle of good enough;sorting	Peerapon Vateekul;Mei-Ling Shyu	2008	2008 IEEE International Conference on Information Reuse and Integration	10.1109/IRI.2008.4583039	statistical classification;association rule learning;computer science;machine learning;pattern recognition;data mining;statistics	Robotics	11.45882687399184	-39.7049554938353	86545
2cb9fd2ca0d42fcd8dfb66d6d8b74c4b666751cb	fast semi-supervised svm classifiers using a priori metric information	unlabeled data;supervised classification;semi supervised learning;objective function;mathematical programming;linear programming;linear program;unconstrained optimization;svm;feature selection;support vector machine;parametric optimization;linear equations;generalized convexity	This paper describes a support vector machine-based (SVM) parametric optimization method for semi-supervised classification, called LIAM (for LInear hyperplane classifier with A-priori Metric information). Our method takes advantage of similarity information to leverage the unlabeled data in training SVMs. In addition to the smoothness constraints in existing semi-supervised methods, LIAM incorporates local class similarity constraints, that we empirically show, improved the accuracies in the presence of a few labeled points. We present and discuss a general convex mathematical-programming-based formulation to solve the inductive semi-supervised problem; i.e., our proposed algorithm directly classifies test samples not present when training. This general formulation results in different variants depending on the choice of the norms that are used in the objective function. For example, when using the 1-norm the proposed formulation becomes a linear programming problem (LP) that has the advantage of generating sparse solutions depending on a minimal set of the original features (feature selection). On the other hand, one of the proposed formulations results in an unconstrained quadratic problem for which solutions can be obtained by solving a simple system of linear equations, resulting in a fast competitive alternative to state-of-the-art semi-supervised algorithms. Our experiments on public benchmarks indicate that LIAM is at least one order of magnitude faster and at least as or more accurate (in most of the cases) than other state-of-the-art semi-supervised classification methods.	algorithm;experiment;feature selection;inner class;linear equation;linear programming;loss function;machine learning;mathematical optimization;optimization problem;quadratic equation;semi-supervised learning;semiconductor industry;sparse matrix;supervised learning;support vector machine;system of linear equations	Volkan Vural;Glenn Fung;Jennifer G. Dy;R. Bharat Rao	2008	Optimization Methods and Software	10.1080/10556780802102750	support vector machine;least squares support vector machine;mathematical optimization;linear programming;machine learning;linear classifier;pattern recognition;mathematics	ML	21.697133210753886	-39.03967678319854	86716
398713c17e55558deb03d638f82bfd8d00ec470f	generative adversarial trainer: defense to adversarial perturbations with gan		We propose a novel technique to make neural network robust to adversarial examples using a generative adversarial network. We alternately train both classifier and generator networks. The generator network generates an adversarial perturbation that can easily fool the classifier network by using a gradient of each image. Simultaneously, the classifier network is trained to classify correctly both original and adversarial images generated by the generator. These procedures help the classifier network to become more robust to adversarial perturbations. Furthermore, our adversarial training framework efficiently reduces overfitting and outperforms other regularization methods such as Dropout. We applied our method to supervised learning for CIFAR datasets, and experimantal results show that our method significantly lowers the generalization error of the network. To the best of our knowledge, this is the first method which uses GAN to improve supervised learning.	adversary (cryptography);artificial neural network;dropout (neural networks);generalization error;gradient;inner loop;mathematical optimization;overfitting;perturbation theory;statistical classification;supervised learning;teaching method	Hyeungill Lee;Sungyeob Han;Jungwoo Lee	2017	CoRR		computer science;machine learning;pattern recognition;data mining	ML	19.218811417514733	-50.80156068569748	86851
269a26037b856e0d1c7aae0ff83de55ec33ba627	leaf identification using a deep convolutional neural network		Convolutional neural networks (CNNs) have become popular especially in computer vision in the last few years because they achieved outstanding performance on different tasks, such as image classifications. We propose a ninelayer CNN for leaf identification using the famous Flavia and Foliage datasets. Usually the supervised learning of deep CNNs requires huge datasets for training. However, the used datasets contain only a few examples per plant species. Therefore, we apply data augmentation and transfer learning to prevent our network from overfitting. The trained CNNs achieve recognition rates above 99% on the Flavia and Foliage datasets, and slightly outperform current methods for leaf classification.		Christoph Wick;Frank Puppe	2017	CoRR		transfer of learning;supervised learning;artificial intelligence;pattern recognition;convolutional neural network;overfitting;machine learning;computer science	ML	22.29278552614666	-50.78309812554349	86946
75d3b3c3550983ec95f35b70145833da1e1941af	self-learning based land-cover classification using sequential class patterns from past land-cover maps		To improve the accuracy of classification with a small amount of training data, this paper presents a self-learning approach that defines class labels from sequential patterns using a series of past land-cover maps. By stacking past land-cover maps, unique sequence rule information from sequential change patterns of land-covers is first generated, and a rule-based class label image is then prepared for a given time. After the most informative pixels with high uncertainty are selected from the initial classification, rule-based class labels are assigned to the selected pixels. These newly labeled pixels are added to training data, which then undergo an iterative classification process until a stopping criterion is reached. Time-series MODIS NDVI data sets and cropland data layers (CDLs) from the past five years are used for the classification of various crop types in Kansas. From the experiment results, it is found that once the rule-based labels are derived from past CDLs, the labeled informative pixels could be properly defined without analyst intervention. Regardless of different combinations of past CDLs, adding these labeled informative pixels to training data increased classification accuracy and the maximum improvement of 8.34 percentage points in overall accuracy was achieved when using three CDLs, compared to the initial classification result using a small amount of training data. Using more than three consecutive CDLs showed slightly better classification accuracy than when using two CDLs (minimum and maximum increases were 1.56 and 2.82 percentage points, respectively). From a practical viewpoint, using three or four CDLs was the best choice for this study area. Based on these experiment results, the presented approach could be applied effectively to areas with insufficient training data but access to past land-cover maps. However, further consideration should be given to select the optimal number of past land-cover maps and reduce the impact of errors of rule-based labels.	experiment;information;iterative method;logic programming;map;pixel;stacking;statistical classification	Yeseul Kim;No-Wook Park;Kyung-Do Lee	2017	Remote Sensing	10.3390/rs9090921	normalized difference vegetation index;pixel;change patterns;land cover;training set;data mining;data set;percentage point;computer science	AI	15.693531886303779	-39.73050564892124	87108
1b21cf47e72d03f4fcec3ef76dcea072cc2f6c6f	enhancing semi-supervised clustering: a feature projection perspective	pairwise instance constraints;unsupervised clustering;high dimensionality;k means;feature projection;high dimensional data;sparse data;expert knowledge;semi supervised clustering	Semi-supervised clustering employs limited supervision in the form of labeled instances or pairwise instance constraints to aid unsupervised clustering and often significantly improves the clustering performance. Despite the vast amount of expert knowledge spent on this problem, most existing work is not designed for handling high-dimensional sparse data. This paper thus fills this crucial void by developing a Semi-supervised Clustering method based on spheRical K-mEans via fEature projectioN (SCREEN). Specifically, we formulate the problem of constraint-guided feature projection, which can be nicely integrated with semi-supervised clustering algorithms and has the ability to effectively reduce data dimension. Indeed, our experimental results on several real-world data sets show that the SCREEN method can effectively deal with high-dimensional data and provides an appealing clustering performance.	algorithm;cluster analysis;k-means clustering;projection screen;semi-supervised learning;semiconductor industry;sparse matrix	Wei Tang;Hui Xiong;Shi Zhong;Jie Wu	2007		10.1145/1281192.1281268	correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;sparse matrix;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;hierarchical clustering;cluster analysis;brown clustering;dbscan;biclustering;affinity propagation;k-means clustering;clustering high-dimensional data;conceptual clustering	ML	21.66627824674251	-42.3942859254908	87150
f484329b4632d93501c7aa7791577ccb069f8797	kernel optimization using nonparametric fisher criterion in the subspace		Kernel optimization plays an important role in kernel-based dimensionality reduction algorithms, such as kernel principal components analysis (KPCA) and kernel discriminant analysis (KDA). In this paper, a nonparametric Fisher criterion is proposed as the objective function to find the optimized kernel parameters. Unlike other criterions that rooted in the kernel feature space, the proposed criterion works in the low-dimensional subspace to measure the separability of different patterns. Experiments on 13 different benchmark datasets show the effectiveness of the proposed method, in comparison with other criterions and the kernel space methods.	kernel (operating system);mathematical optimization	Xingyu Wu;Xia Mao;Lijiang Chen;Yu-Li Xue;Alberto Rovetta	2015	Pattern Recognition Letters	10.1016/j.patrec.2014.11.016	kernel;principal component regression;kernel regression;kernel method;mathematical optimization;kernel fisher discriminant analysis;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;machine learning;pattern recognition;mathematics;variable kernel density estimation;polynomial kernel;fisher kernel;kernel smoother	Vision	24.006248766964973	-40.060657354309	87231
a1172d0de6164bb7eaadbcdbc10e7b03b773b6ad	deep models of interactions across sets		We use deep learning to model interactions across two or more sets of objects, such as user–movie ratings, protein–drug bindings, or ternary useritem-tag interactions. The canonical representation of such interactions is a matrix (or a higherdimensional tensor) with an exchangeability property: the encoding’s meaning is not changed by permuting rows or columns. We argue that models should hence be Permutation Equivariant (PE): constrained to make the same predictions across such permutations. We present a parameter-sharing scheme and prove that it could not be made any more expressive without violating PE. This scheme yields three benefits. First, we demonstrate state-of-the-art performance on multiple matrix completion benchmarks. Second, our models require a number of parameters independent of the numbers of objects, and thus scale well to large datasets. Third, models can be queried about new objects that were not available at training time, but for which interactions have since been observed. In experiments, our models achieved surprisingly good generalization performance on this matrix extrapolation task, both within domains (e.g., new users and new movies drawn from the same distribution used for training) and even across domains (e.g., predicting music ratings after training on movies).		Jason S. Hartford;Devon R. Graham;Kevin Leyton-Brown;Siamak Ravanbakhsh	2018			tensor;mathematics;machine learning;matrix completion;canonical form;permutation;extrapolation;matrix (mathematics);artificial intelligence;deep learning;equivariant map	ML	21.54462840201384	-48.07282383507343	87326
1da7825d3e3d0d962fab973e465f2a174d649a54	clustering of the poincare vectors	unsupervised learning;pattern clustering;polarimetry;image segmentation;large dataset;image classification;unsupervised learning pattern clustering polarimetry vectors image classification image segmentation matrix algebra;matrix algebra;data clustering;vectors;clustering algorithms optical polarization image segmentation optical surface waves optical imaging layout electromagnetic wave polarization stokes parameters testing polarimetry;unsupervised classification poincare vectors data clustering k means algorithm competitive neural technique classification accuracy segmentation accuracy passive polarimetric images;k means algorithm	Data clustering is useful for discovering significant patterns and characteristics in large datasets. In this paper, we address the problem of clustering the Poincare vectors. Three variants of the k-means algorithm and a competitive neural technique are tested and compared. The empirical performance of the different methods in terms of classification and segmentation accuracy is evaluated. The results obtained on real life passive polarimetric images demonstrate the usefulness of such approach for exploiting the polarimetric information.	algorithm;cluster analysis;k-means clustering;polarimetry;real life	Said Lakroum;Vincent Devlaminck;Patrick Terrier;Philippe Biela Enberg;Jack-Gérard Postaire	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1530274	unsupervised learning;correlation clustering;computer vision;contextual image classification;polarimetry;computer science;canopy clustering algorithm;machine learning;segmentation-based object categorization;pattern recognition;cure data clustering algorithm;mathematics;image segmentation;cluster analysis;scale-space segmentation;biclustering;k-means clustering	Robotics	14.019010671316575	-48.041449371887005	87347
69ec14eccc56e839312db8273b9a061f175a44fe	a transductive framework of distance metric learning by spectral dimensionality reduction	eigenvalue problem;distance metric learning;representation theorem;synthetic data;dimensional reduction	Distance metric learning and nonlinear dimensionality reduction are two interesting and active topics in recent years. However, the connection between them is not thoroughly studied yet. In this paper, a transductive framework of distance metric learning is proposed and its close connection with many nonlinear spectral dimensionality reduction methods is elaborated. Furthermore, we prove a representer theorem for our framework, linking it with function estimation in an RKHS, and making it possible for generalization to unseen test samples. In our framework, it suffices to solve a sparse eigenvalue problem, thus datasets with 105 samples can be handled. Finally, experiment results on synthetic data, several UCI databases and the MNIST handwritten digit database are shown.	mnist database;nonlinear dimensionality reduction;nonlinear system;representer theorem;sparse matrix;synthetic data	Fuxin Li;Jian Yang;Jue Wang	2007		10.1145/1273496.1273561	combinatorics;machine learning;pattern recognition;mathematics;statistics;synthetic data	ML	24.210696080241345	-40.85108664199851	87552
784b542cb18b547f4ed95ce52599f1def131136c	a multi-verse optimizer approach for feature selection and optimizing svm parameters based on a robust system architecture		Support vector machine (SVM) is a well-regarded machine learning algorithm widely applied to classification tasks and regression problems. SVM was founded based on the statistical learning theory and structural risk minimization. Despite the high prediction rate of this technique in a wide range of real applications, the efficiency of SVM and its classification accuracy highly depends on the parameter setting as well as the subset feature selection. This work proposes a robust approach based on a recent nature-inspired metaheuristic called multi-verse optimizer (MVO) for selecting optimal features and optimizing the parameters of SVM simultaneously. In fact, the MVO algorithm is employed as a tuner to manipulate the main parameters of SVM and find the optimal set of features for this classifier. The proposed approach is implemented and tested on two different system architectures. MVO is benchmarked and compared with four classic and recent metaheuristic algorithms using ten binary and multi-class labeled datasets. Experimental results demonstrate that MVO can effectively reduce the number of features while maintaining a high prediction accuracy.	batch file;bitwise operation;feature selection;firefly;genetic algorithm;machine learning;mathematical optimization;metaheuristic;particle swarm optimization;software release life cycle;statistical learning theory;structural risk minimization;support vector machine;systems architecture;tv tuner card;verse protocol	Hossam Faris;Mohammad A. Hassonah;Ala' M. Al-Zoubi;Seyed Mohammad Mirjalili;Ibrahim Aljarah	2016	Neural Computing and Applications	10.1007/s00521-016-2818-2	computer science;machine learning;pattern recognition;data mining	ML	11.618116125952053	-41.98958903769756	88205
b00068e70883ef7e7576af632a269290c6d40444	key course selection in academic warning with sparse regression		Many colleges and universities are paying more attention to academic warning which warns large numbers of students who have unsatisfactory academic performance. Academic warning becomes a new part in the teaching management constitution but lacks of unified and scientific standards under the establishment of this stipulation at present. This paper solves the current setting of academic warning through wellknown methods lasso and l1-norm support vector regression with εinsensitive loss function which can select key courses based on the failed credits in one semester. The experiments are made on our collected academic warning datasets which are incomplete data. We impute them with one nearest neighbor method. The experimental results show that sparse regression is effective for colleges and universities to remind the students of key courses.	experiment;lasso;loss function;nearest neighbor search;sparse matrix;support vector machine;taxicab geometry	Min Yin;Xijiong Xie;Shiliang Sun	2016		10.1007/978-981-10-3002-4_59	computer science;machine learning;pattern recognition;data mining	ML	22.29918545375871	-42.28722728245328	88207
f03857db0ca5c681d8ed918365f920939ef3ea40	a pre-clustering technique for optimizing subclass discriminant analysis	dimensionalidad;optimisation;optimizacion;learning;pre clustering technique;complexite calcul;algorithme k moyenne;image databank;dimension reduction;image database;dimensionality;linear discriminate analysis;subclass discriminant analysis sda;processing time;aprendizaje;reduction dimension;discriminant analysis;analyse discriminante;temps calcul;accuracy;analisis discriminante;complejidad computacion;apprentissage;precision;dimensionality reduction;computational complexity;dimensionnalite;banco imagen;banque image;signal classification;classification signal;reduccion dimension;temps traitement;algoritmo k media;k means algorithm;optimization;mixture of gaussians;tiempo computacion;classification automatique;computation time;classification accuracy;automatic classification;clasificacion automatica;dimensional reduction;tiempo proceso;k means clustering;conditional distribution;linear discriminant analysis lda	Subclass discriminant analysis (SDA) [Zhu, M., Martinez, A.M., 2006. Subclass discriminant analysis. IEEE Trans. Pattern Anal. Machine Intell., 28(8), pp. 1274–1286] is a dimensionality reduction method that has proven successful for different types of class distributions. In SDA, the reduction of dimensionality is not achieved by assuming that each class is represented by a single cluster, but rather by approximating the underlying distribution with a mixture of Gaussians. The advantage of SDA is that since it does not treat the class-conditional distributions as uni-modal ones, the nonlinearly separable problems can be handled as linear ones. The problem with this strategy, however, is that to estimate the number of subclasses needed to represent the distribution of each class, i.e., to find out the best partition, all possible solutions should be verified. Therefore, this approach leads to an associated high computational cost. In this paper, we propose a method that optimizes the computational burden of SDA-based classification by simply reducing the number of classes to be examined through choosing a few classes of the training set prior to the execution of the SDA. To select the classes to be partitioned, the intra-set distance is employed as a criterion and a k-means clustering is performed to divide them. Our experimental results for an artificial data set of XOR-type samples and three benchmark image databases of Kimia, AT&T, and Yale demonstrate that the processing CPU-time of the SDA optimized with the proposed scheme could be reduced dramatically without either sacrificing classification accuracy or increasing computational complexity. 2009 Elsevier B.V. All rights reserved.	algorithmic efficiency;benchmark (computing);central processing unit;cluster analysis;computation;computational complexity theory;comstock–needham system;database;dimensionality reduction;exclusive or;k-means clustering;linear discriminant analysis;local-density approximation;mixture model;modal logic;nonlinear system;selection algorithm;test set	Sang-Woon Kim	2010	Pattern Recognition Letters	10.1016/j.patrec.2009.07.007	computer science;machine learning;mathematics;accuracy and precision;linear discriminant analysis;algorithm;statistics;dimensionality reduction;k-means clustering	AI	23.452042744294456	-39.14929800316902	88379
1152d9bfb6cbfda1b919ff6e9013f48344f9926f	a multiple resampling method for learning from imbalanced data sets	decision tree;class imbalance;multiple resampling;text classification;inductive learning;resampling method;decision trees;imbalanced data sets;class imbalance problem	Resampling methods are commonly used for dealing with the class-imbalance problem. Their advantage over other methods is that they are external and thus, easily transportable. Although such approaches can be very simple to implement, tuning them most effectively is not an easy task. In particular, it is unclear whether oversampling is more effective than undersampling and which oversampling or undersampling rate should be used. This paper presents an experimental study of these questions and concludes that combining different expressions of the resampling approach is an effective solution to the tuning problem. The proposed combination scheme is evaluated on imbalanced subsets of the Reuters-21578 text collection and is shown to be quite effective for these problems.	experiment;oversampling;resampling (statistics);undersampling	Andrew Estabrooks;Taeho Jo;Nathalie Japkowicz	2004	Computational Intelligence	10.1111/j.0824-7935.2004.t01-1-00228.x	computer science;machine learning;decision tree;pattern recognition;data mining	AI	13.999258031420222	-41.08564018762911	88405
5ccedc8a28ed6294a60980cd934bc2fcf74d8180	a memetic algorithm with support vector machine for feature selection and classification		The memetic algorithm (MA) is an evolutionary metaheuristic that can be viewed as a hybrid genetic algorithm combined with some kinds of local search. In this paper, we propose a memetic algorithm combined with a support vector machine (SVM) for feature selection and classification in Data mining. The proposed approach tries to find a subset of features that maximizes the classification accuracy rate of SVM. In addition, another hybrid algorithm of MA and SVM with optimized parameters is also developed. The two versions of our proposed method are evaluated on some datasets and compared with some well-known classifiers for data classification. The computational experiments show that the hybrid method MA + SVM with optimized parameters provides competitive results and finds high quality solutions.	feature selection;memetic algorithm;support vector machine	Messaouda Nekkaa;Dalila Boughaci	2015	Memetic Computing	10.1007/s12293-015-0153-2	computer science;machine learning;pattern recognition;data mining;structured support vector machine;memetic algorithm	ML	10.137609243252548	-42.93706643741336	88519
3b2c20c6563101fc044595c77e324302ebe5acb1	multi-objective semi-supervised feature selection and model selection based on pearson's correlation coefficient	institutional repositories;optimal solution;model selection;relief;fedora;semi supervised;pearson;vital;feature selection;vtls;correlation coefficient;pareto optimality;ils;optimization model;neural network	This paper presents a Semi-Supervised Feature Selection Method based on a univariate relevance measure applied to a multiobjective approach of the problem. Along the process of decision of the optimal solution within Pareto-optimal set, atempting to maximize the relevance indexes of each feature, it is possible to determine a minimum set of relevant features and, at the same time, to determine the optimal model of the neural network.	artificial neural network;coefficient;coherence (physics);feature selection;machine learning;model selection;pareto efficiency;relevance;semi-supervised learning;semiconductor industry;statistical learning theory;supervised learning	Frederico Gualberto F. Coelho;Antônio de Pádua Braga;Michel Verleysen	2010		10.1007/978-3-642-16687-7_67	computer science;machine learning;pattern recognition;feature selection;model selection;statistics	ML	20.75548044377779	-40.57630336784962	88672
a80355dcee5156b064e31b39c6b72037044ed87c	l-tree: a local-area-learning-based tree induction algorithm for image classification	decision tree;ensemble tree;image classification;self-organizing map	The decision tree is one of the most effective tools for deriving meaningful outcomes from image data acquired from the visual sensors. Owing to its reliability, superior generalization abilities, and easy implementation, the tree model has been widely used in various applications. However, in image classification problems, conventional tree methods use only a few sparse attributes as the splitting criterion. Consequently, they suffer from several drawbacks in terms of performance and environmental sensitivity. To overcome these limitations, this paper introduces a new tree induction algorithm that classifies images on the basis of local area learning. To train our predictive model, we extract a random local area within the image and use it as a feature for classification. In addition, the self-organizing map, which is a clustering technique, is used for node learning. We also adopt a random sampled optimization technique to search for the optimal node. Finally, each trained node stores the weights that represent the training data and class probabilities. Thus, a recursively trained tree classifies the data hierarchically based on the local similarity at each node. The proposed tree is a type of predictive model that offers benefits in terms of image's semantic energy conservation compared with conventional tree methods. Consequently, it exhibits improved performance under various conditions, such as noise and illumination changes. Moreover, the proposed algorithm can improve the generalization ability owing to its randomness. In addition, it can be easily applied to ensemble techniques. To evaluate the performance of the proposed algorithm, we perform quantitative and qualitative comparisons with various tree-based methods using four image datasets. The results show that our algorithm not only involves a lower classification error than the conventional methods but also exhibits stable performance even under unfavorable conditions such as noise and illumination changes.	algorithm;anatomic node;architecture design and assessment system;autonomous car;boosting (machine learning);bootstrap aggregating;cns disorder;cluster analysis;computer vision;decision tree;exhibits as topic;generalization (psychology);image noise;mathematical optimization;microsoft windows;nut hypersensitivity;object detection;organizing (structure);predictive modelling;preprocessor;probability;randomness;recursion;sampling - surgical action;self-organization;self-organizing map;silo (dataset);sparse matrix;weight;benefit;sensor (device);statistical cluster	Jaesung Choi;Eungyeol Song;Sangyoun Lee	2018		10.3390/s18010306	randomness;self-organizing map;decision tree;cluster analysis;recursion;decision tree model;algorithm;contextual image classification;engineering;training set	ML	12.926897363557915	-42.834652098647105	88903
5de2447bc9d800d27d26eccb7d7d08f5b9b6fc3c	an optimized naive bayesian method for face recognition	face recognition;laplace smoothing;maximum filtering;naive bayesian	Naive Bayesian is a simple and powerful classification algorithm. In this paper, we propose an optimized naive Bayesian algorithm with the application to face recognition. Firstly, the algorithm estimates the probability distribution of each pixel at each gray level. Secondly, it performs Laplace smoothing to resolve the zero probability problem. Thirdly, the maximum filtering is used to optimize the probability distribution matrix for classification. Experiments on three face databases show that the proposed algorithm is effective and performs better than some state-of-the-art algorithms.	facial recognition system	Rui Yan;Jie Wen;Jian Cao;Yong Xu;Jian Yang	2016		10.1007/978-981-10-5230-9_14	pixel;filter (signal processing);probability distribution;naive bayes classifier;facial recognition system;additive smoothing;mathematics;artificial intelligence;matrix (mathematics);pattern recognition	Vision	18.789746605695743	-39.73251122002784	88959
8f684080d2b81d3178d681d6917cb077c082a9e1	fast and accurate single image super-resolution via information distillation network		Recently, deep convolutional neural networks (CNNs) have been demonstrated remarkable progress on single image super-resolution. However, as the depth and width of the networks increase, CNN-based super-resolution methods have been faced with the challenges of computational complexity and memory consumption in practice. In order to solve the above questions, we propose a deep but compact convolutional network to directly reconstruct the high resolution image from the original low resolution image. In general, the proposed model consists of three parts, which are feature extraction block, stacked information distillation blocks and reconstruction block respectively. By combining an enhancement unit with a compression unit into a distillation block, the local long and short-path features can be effectively extracted. Specifically, the proposed enhancement unit mixes together two different types of features and the compression unit distills more useful information for the sequential blocks. In addition, the proposed network has the advantage of fast execution due to the comparatively few numbers of filters per layer and the use of group convolution. Experimental results demonstrate that the proposed method is superior to the state-of-the-art methods, especially in terms of time performance. Code is available at https://github.com/Zheng222/IDN-Caffe.		Zheng Hui;Xiumei Wang;Xinbo Gao	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00082	iterative reconstruction;convolutional neural network;computer vision;distillation;image restoration;feature extraction;computational complexity theory;artificial intelligence;convolution;image resolution;computer science	Vision	24.397988939089437	-51.99970041200979	88989
3471c07a3020b40f5b3920bb87559f5e3f2d3e90	using sequential unconstrained minimization techniques to simplify svm solvers	support vector machines;smo;svm	In this paper, we apply Sequential Unconstrained Minimization Techniques (SUMTs) to the classical formulations of both the classical L1 norm SVM and the least squares SVM. We show that each can be solved as a sequence of unconstrained optimization problems with only box constraints. We propose relaxed SVM and relaxed LSSVM formulations that correspond to a single problem in the corresponding updating individual Lagrange multipliers. The methods yield comparable or better results on large benchmark datasets than classical SVM and LSSVM formulations, at substantially higher speeds. & 2011 Elsevier B.V. All rights reserved.	benchmark (computing);lagrange multiplier;least squares;mathematical optimization;t-norm;taxicab geometry	Sachindra Joshi;Jayadeva;Ganesh Ramakrishnan;Suresh Chandra	2012	Neurocomputing	10.1016/j.neucom.2011.07.010	support vector machine;mathematical optimization;computer science;machine learning;mathematics;algorithm	AI	21.654861499055656	-38.41380675415392	89026
3a192510834f2df52ab56a3d6967f6b80efd5067	multi-source image auto-annotation	image processing;learning artificial intelligence image processing;inter source structure regularizers multi source image annotation multitask learning;risky biases multisource image auto annotation labelled to be annotated images unseen to be annotated images labelled data model training source specific model multitask learning model annotation models intersource structure regularizers parameter constraints information sharing;learning artificial intelligence	Though the field of image auto-annotation has been extensively researched, most previous work concentrated on the single-source problem, assuming that both labelled and unseen to-be-annotated images are from a single source (e.g. an identical website), while in practice they are generally collected from multiple sources (e.g. different websites). In that case, treating each source independently may suffer from the insufficiency of labelled data for model training, while merging with labelled images from other sources can bring risky biases to the source-specific model. In this paper, we propose a multi-task learning model to alleviate the multi-source image auto-annotation problem, with each task defined as performing auto-annotation for the corresponding source. Specifically, the proposed model trains annotation models for all sources in parallel with the introduction of inter-source structure regularizers and parameter constraints for sharing information and enhancing the overall performance. Experiments conducted on three different-source benchmark datasets and their combinations yield inspiring results and demonstrate that the proposed model can well utilize the shared information and relieve the risky biases.	benchmark (computing);computer multitasking;convergence insufficiency;multi-source;multi-task learning	Zijia Lin;Guiguang Ding;Mingqing Hu	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738529	computer vision;image processing;computer science;machine learning;data mining	Vision	22.87843365455595	-45.088037537097414	89155
541e47e20eefa3623376bf540148c8743792d539	hierarchical k-means clustering using new support vector machines for multi-class classification	support vector machines pattern classification pattern clustering;pattern clustering;support vector machines;support vector machines support vector machine classification testing design methodology kernel databases computational modeling pattern recognition computational complexity classification algorithms;binary hierarchical classification structure;k means;kernel function;discrimination machine;support vector;infra red;hierarchical classification;parameter selection;multi class classification;pattern classification;support vector machine;hierarchical design;discrimination machine hierarchical k means clustering support vector machines binary hierarchical classification structure hierarchical design method;k means clustering;hierarchical k means clustering;hierarchical design method	We propose a binary hierarchical classification structure to address the multi-class classification problem with a new hierarchical design method, k-means SVRM (support vector representation machine) clustering. This greatly improves upon our prior IJCNN hierarchical design. At each node in the hierarchy, we apply the SVRDM (support vector representation and discrimination machine) classifier, which offers generalization and good rejection ability. We also provide new theoretical bases and methods for our choice of the kernel function and new SVRDM parameter selection rules. Classification and rejection test results are presented on new databases of both simulated and real infra-red (IR) data.	binary classification;cluster analysis;database;k-means clustering;multiclass classification;rejection sampling;selection rule;support vector machine	Yu-Chiang Frank Wang;David Casasent	2006	The 2006 IEEE International Joint Conference on Neural Network Proceedings	10.1109/IJCNN.2006.247350	support vector machine;computer science;machine learning;pattern recognition;data mining;structured support vector machine	ML	10.383370899396429	-40.338529519207654	89190
d973b47a1de26c719a1099be303a90a4831efeee	improving classification performance of support vector machine by genetically optimising kernel shape and hyper-parameters	multiple kernel;hyper parameters optimisation;genetic programming;hybrid model;classification problems;kernel of kernels;svm	Support Vector Machines (SVMs) deliver state-of-the-art performance in real-world applications and are now established as one of the standard tools for machine learning and data mining. A key problem of these methods is how to choose an optimal kernel and how to optimise its parameters. The real-world applications have also emphasised the need to consider a combination of kernels—a multiple kernel—in order to boost the classification accuracy by adapting the kernel to the characteristics of heterogeneous data. This combination could be linear or non-linear, weighted or un-weighted. Several approaches have been already proposed to find a linear weighted kernel combination and to optimise its parameters together with the SVM parameters, but no approach has tried to optimise a non-linear weighted combination. Therefore, our goal is to automatically generate and adapt a kernel combination (linear or non-linear, weighted or un-weighted, according to the data) and to optimise both the kernel parameters and SVM parameters by evolutionary means in a unified framework. We will denote our combination as a kernel of kernels (KoK). Numerical experiments show that the SVM algorithm, involving the evolutionary kernel of kernels (eKoK) we propose, performs better than well-known classic kernels whose parameters were optimised and a state of the art convex linear and an evolutionary linear, respectively, kernel combinations. These results emphasise the fact that the SVM algorithm could require a non-linear weighted combination of kernels.	algorithm;algorithmic efficiency;coefficient;computation;convex function;data mining;exptime;experiment;image scaling;kernel (operating system);machine learning;nonlinear system;numerical analysis;numerical method;polynomial;radial basis function;sigmoid function;support vector machine;unified framework	Laura Diosan;Alexandrina Rogozan;Jean-Pierre Pécuchet	2010	Applied Intelligence	10.1007/s10489-010-0260-1	genetic programming;support vector machine;least squares support vector machine;kernel method;mathematical optimization;kernel embedding of distributions;radial basis function kernel;computer science;machine learning;pattern recognition;graph kernel;tree kernel;variable kernel density estimation;polynomial kernel;kernel smoother	ML	21.583474201340113	-38.81111694907638	89210
85d506c0b0900bcb4258446db6aba60f67baf6ce	atm: a distributed, collaborative, scalable system for automated machine learning		In this paper, we present Auto-Tuned Models, or ATM, a distributed, collaborative, scalable system for automated machine learning. Users of ATM can simply upload a dataset, choose a subset of modeling methods, and choose to use ATM's hybrid Bayesian and multi-armed bandit optimization system. The distributed system works in a load-balanced fashion to quickly deliver results in the form of ready-to-predict models, confusion matrices, cross-validation results, and training timings. By automating hyperparameter tuning and model selection, ATM returns the emphasis of the machine learning workflow to its most irreducible part: feature engineering. We demonstrate the usefulness of ATM on 420 datasets from OpenML and train over 3 million classifiers. Our initial results show ATM can beat human-generated solutions for 30% of the datasets, and can do so in 1/100th of the time.	atm turbo;aggregate data;alternating turing machine;application programming interface;auto-tune;confusion matrix;cross-validation (statistics);database tuning;distributed computing;emoticon;end system;end-to-end principle;feature engineering;geo-imputation;interaction;irreducibility;load balancing (computing);machine learning;mathematical optimization;model selection;multi-armed bandit;multi-user;openml;organizing (structure);performance;ps (unix);recommender system;scalability;sparse matrix;traverse;the matrix;undefined behavior;undefined value;upload	Thomas Swearingen;Will Drevo;Bennett Cyphers;Alfredo Cuesta-Infante;Arun Ross;Kalyan Veeramachaneni	2017	2017 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2017.8257923	machine learning;confusion;artificial intelligence;scalability;model selection;computer science;hyperparameter;data modeling;feature engineering;workflow;upload	ML	14.305338696249029	-47.31358539052803	89272
493b16a5f61a603e832f19b35e2c0f520b8b4027	learning classifier competence based on graph for dynamic classifier selection	approximation algorithms;training;prediction algorithms;training data;classification algorithms;optimization;biological neural networks	Classifier competence is critical important for classifier ensemble. This study proposes an optimization problem on the neighborhood graph of data and develops an iteration algorithm to learn the competences of classifiers. The learned competences of classifiers not just reflect the competitiveness of classifiers, but also vary smooth on the neighboring data. Experimental results on five different data sets show the dynamic classifier selection based classification systems with the learned classifier competence perform competitively.	algorithm;competitive analysis (online algorithm);iteration;iterative method;mathematical optimization;optimization problem	Cuiqin Hou;Yingju Xia;Zhuoran Xu;Jun Sun	2016	2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2016.7603343	random subspace method;statistical classification;margin classifier;training set;margin;prediction;cascading classifiers;quadratic classifier;computer science;machine learning;linear classifier;pattern recognition;data mining;approximation algorithm	ML	19.0229130767872	-40.53100340167086	89439
2c699cbb441f786412a2634c445e25f450268478	linear multilayer ica using adaptive pca	processus gauss;metodo adaptativo;optimisation;analisis componente principal;approximate algorithm;image processing;matriz correlacion;optimizacion;analisis datos;algoritmo adaptativo;base donnee tres grande;procesamiento imagen;adaptive pca;source signal;methode adaptative;base de datos a gran escala;independent component analysis;traitement image;higher order;adaptive algorithm;data analysis;algorithme adaptatif;principal component analysis;adaptive method;analyse composante principale;analyse composante independante;analyse donnee;matrice correlation;signal sources;optimization;gaussian process;numerical experiment;very large databases;analisis componente independiente;correlation matrix;efficient estimation;massive data analysis;proceso gauss;independent component	Linear multilayer independent component analysis (LMICA) is an approximate algorithm for ICA. In LMICA, approximate independent components are efficiently estimated by optimizing only highly dependent pairs of signals when all the sources are super-Gaussian. In this paper, the nonlinear functions in LMICA are generalized, and a new method using adaptive PCA is proposed for the selection of pairs of highly dependent signals. In this method, at first, all the signals are sorted along the first principal axis of their higher-order correlation matrix. Then, the sorted signals are divided into two groups so that relatively highly correlated signals are collected in each group. Lastly, each of them is sorted recursively. This process is repeated until each group consists of only one or two signals. Because a well-known adaptive PCA algorithm named PAST is utilized for calculating the first principal axis, this method is quite simple and efficient. Some numerical experiments verify the effectiveness of LMICA with this improvement.	apache axis;approximation algorithm;crystal structure;experiment;independent component analysis;nonlinear system;numerical analysis;recursion	Yoshitatsu Matsuda;Kazunori Yamaguchi	2009	Neural Processing Letters	10.1007/s11063-009-9114-4	independent component analysis;econometrics;covariance matrix;higher-order logic;image processing;computer science;gaussian process;mathematics;data analysis;algorithm;statistics;principal component analysis	ML	23.69900498259339	-38.12761705798168	89491
0e38ac16d2aec27eaa99567153a083885c294f7b	empirical comparison of dynamic classifier selection methods based on diversity and accuracy for building ensembles	accuracy classification algorithms testing diversity reception artificial neural networks support vector machines clustering methods;support vector machines;multiclassifier systems;classifier system;pattern classifier dynamic classifier selection methods multiclassifier systems ensemble members classifier member selection methods static selection method;testing;diversity reception;accuracy;artificial neural networks;performance improvement;classification algorithms;static selection method;pattern classification;classifier member selection methods;pattern classification learning artificial intelligence;pattern classifier;dynamic classifier selection methods;learning artificial intelligence;clustering methods;ensemble members	In the context of Ensembles or Multi-Classifier Systems, the choice of the ensemble members is a very complex task, in which, in some cases, it can lead to ensembles with no performance improvement. In order to avoid this situation, there is a great deal of research to find effective classifier member selection methods. In this paper, we propose a selection criterion based on both the accuracy and diversity of the classifiers in the initial pool. Also, instead of using a static selection method, we use a Dynamic Classifier Selection (DSC) procedure. In this case, the member classifiers to form the ensemble are chosen at the test (use) phase. That is, different testing patterns can be classified by different ensemble configurations.		Marcílio Carlos Pereira de Souto;Rodrigo G. F. Soares;Alixandre Santana;Anne M. P. Canuto	2008	2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)	10.1109/IJCNN.2008.4633992	random subspace method;statistical classification;support vector machine;cascading classifiers;computer science;machine learning;pattern recognition;data mining;accuracy and precision;software testing;artificial neural network	Robotics	12.396988923453094	-41.88173260676398	89890
c858eeac0145ef18df4ea15dd445e1cf907b76de	multiple classifier integration for the prediction of protein structural classes	multiple classifier integration;protein structural class;amino acid compositions;weka;multiple classifiers;protein structure;minimum redundancy maximum relevance	Supervised classifiers, such as artificial neural network, partition trees, and support vector machines, are often used for the prediction and analysis of biological data. However, choosing an appropriate classifier is not straightforward because each classifier has its own strengths and weaknesses, and each biological dataset has its own characteristics. By integrating many classifiers together, people can avoid the dilemma of choosing an individual classifier out of many to achieve an optimized classification results (Rahman et al., Multiple Classifier Combination for Character Recognition: Revisiting the Majority Voting System and Its Variation, Springer, Berlin, 2002, 167-178). The classification algorithms come from Weka (Witten and Frank, Data Mining: Practical Machine Learning Tools and Techniques, Morgan Kaufmann, San Francisco, 2005) (a collection of software tools for machine learning algorithms). By integrating many predictors (classifiers) together through simple voting, the correct prediction (classification) rates are 65.21% and 65.63% for a basic training dataset and an independent test set, respectively. These results are better than any single machine learning algorithm collected in Weka when exactly the same data are used. Furthermore, we introduce an integration strategy which takes care of both classifier weightings and classifier redundancy. A feature selection strategy, called minimum redundancy maximum relevance (mRMR), is transferred into algorithm selection to deal with classifier redundancy in this research, and the weightings are based on the performance of each classifier. The best classification results are obtained when 11 algorithms are selected by mRMR method, and integrated together through majority votes with weightings. As a result, the prediction correct rates are 68.56% and 69.29% for the basic training dataset and the independent test dataset, respectively. The web-server is available at http://chemdata.shu.edu.cn/protein_st/.		Lei Chen;Lin Lu;Kairui Feng;Wenjin Li;Jie Song;Lulu Zheng;Youlang Yuan;Zhenbing Zeng;Kai-Yan Feng;Wen-Cong Lu;Yu Dong Cai	2009	Journal of computational chemistry	10.1002/jcc.21230	protein structure;chemistry;data mining;mathematics	ML	10.167369983072739	-45.39895530653505	90275
7da2c0d52d614d1b9fc5d6312f702ec58136cebf	оценка эффективности бинарных классификаторов на основе логистической регрессии методом roc-анализа	roc кривые;логистическая регрессия;точка отсечения;предсказательная способность;roc анализ	Logistic regression models and their usage as binary classifiers are considered. A technique of performance evaluation of derived classifiers utilizing ROC analysis is proposed. The results of practical use of the technique are given. The ways of further improvement of the technique are mentioned		Л. Ю. Богданов	2009		10.1007/978-0-387-39940-9_4122	pattern recognition;data mining;statistics	Vision	11.516490550139306	-38.33143789383959	90313
8e903736667e87c99a993ad20a95cb276a26c8cc	non-asymptotic analysis of ℓ1-norm support vector machines		Support Vector Machines (SVM) with ℓ1 penalty became a standard tool in analysis of highdimensional classification problems with sparsity constraints in many applications including bioinformatics and signal processing. Although SVM have been studied intensively in the literature, this paper has to our knowledge first non-asymptotic results on the performance of ℓ1-SVM in identification of sparse classifiers. We show that a d-dimensional s-sparse classification vector can be (with high probability) well approximated from only O(s log(d)) Gaussian trials. The methods used in the proof include concentration of measure and probability in Banach spaces.	approximation algorithm;bioinformatics;signal processing;sparse matrix;support vector machine;taxicab geometry;with high probability	Anton Kolleck;Jan Vybíral	2017	IEEE Trans. Information Theory	10.1109/TIT.2017.2710979	machine learning;pattern recognition;data mining;mathematics	ML	19.349309813800517	-38.60915441502362	90803
c8b376f42030e4a11d8283fa8389b31878e8f25e	class-aware fully convolutional gaussian and poisson denoising		We propose a fully convolutional neural-network architecture for image denoising which is simple yet powerful. Its structure allows to exploit the gradual nature of the denoising process, in which the shallow layers handle local noise statistics, while deeper layers recover edges and enhance textures. Our method advances the state of the art when trained for different noise levels and distributions (both Gaussian and Poisson). In addition, we show that making the denoiser class-aware by exploiting semantic class information boosts the performance, enhances the textures, and reduces the artifacts.	convolutional neural network;morphologic artifacts;network architecture;noise reduction;normal statistical distribution;anatomical layer	Tal Remez;Or Litany;Raja Giryes;Alexander M. Bronstein	2018	IEEE Transactions on Image Processing	10.1109/TIP.2018.2859044	artificial intelligence;architecture;artificial neural network;noise measurement;pattern recognition;gaussian;convolution;mathematics;noise reduction;exploit;poisson distribution	Vision	24.362871568609147	-50.79995681959934	90826
d5c6220c0484cc6ef44074c49f11cfac0e4a8658	constructing a fast algorithm for multi-label classification with support vector data description	loss measurement;k label problem;support vector machines data mining probability regression analysis;kernel;ridge regression;classification algorithm;probability;decomposition;support vector machines;time complexity;multilabel classification;training;support vector data description;linear ridge regression;one by one data decomposition trick;support vector data description multi label classification kernel decomposition;posterior probability;data mining;classification;multi label;pseudo posterior probability;problem transform algorithm;computational complexity;fast algorithm;classification algorithms;transforms;linear ridge regression multilabel classification support vector data description problem transform algorithm one by one data decomposition trick k label problem pseudo posterior probability;support vector machine classification;regression analysis;support vector machine;classification algorithms training support vector machine classification transforms loss measurement kernel	For multi-label classification, problem transform algorithms have received more attention due to their good performance and low computational complexity. But how to speed up training and test procedures is still a challenging issue. In this paper, one-by-one data decomposition trick is adopted to divide a k-label problem into k sub-problems, where a specific sub-problem only consists of instances with a specific class. We train each sub-classifier using support vector data description that learns a smallest hyper-sphere to capture the majority of training instances of each class, and integrate k sub-classifiers into an entire multi-label classification algorithm using both pseudo posterior probabilities and linear ridge regression. Our new method has the lowest time complexity, compared with existing problem transform support vector machines for multi-label classification. Experimental results on the Yeast dataset illustrate that our algorithm works better than several state-of-the-art ones.	algorithm;benchmark (computing);computational complexity theory;multi-label classification;support vector machine;time complexity	Jianhua Xu	2010	2010 IEEE International Conference on Granular Computing	10.1109/GrC.2010.107	statistical classification;support vector machine;computer science;machine learning;pattern recognition;data mining;mathematics	Vision	20.21009853215046	-39.7190598501157	91102
eacdfb178a846a6863a4fef3f02788c20332e08e	multi-label learning with discriminative features for each label	multi label classification;problem transformation;spectral clustering;multi label learning	During the last decade, multi-label learning has attracted the attention of more and more researchers in machine learning field due to wide real-world applications. Existing approaches often predict an unseen example for all labels based on the same feature vector. However, this strategy might be suboptimal since different labels usually depend on different aspects of the feature vector. Furthermore, for each label there is close relationship between positive and negative instances, which is quite informative for classification. In this paper, we propose a new algorithm called ML-DFL, which trains a model for each label with newly constructed discriminative features. In order to form these features, we also propose a spectral clustering algorithm SIA to find the closely located local structures between positive and negative instances, which are assumed to be of more discriminative information, and then transform the original data set by consulting the clustering results in a simple but effective way. Comprehensive experiments are conducted on a collection of benchmark data sets. The results clearly validate the superiority of ML-DFL to various competitors. This paper proposes a novel multi-label learning algorithm, Multi-label Learning with Discriminative Features for each Label (ML-DFL), based on binary relevance by exploiting the correlations between positive and negative instances for each label.To investigate the correlations as stated in Point 1, we propose a new spectral clustering algorithm Spectral Instance Alignment (SIA).Substantial experiments across six data sets from three different application domains validate the superiority of our proposed ML-DFL to five competitors.		Jujie Zhang;Min Fang;Xiao Li	2015	Neurocomputing	10.1016/j.neucom.2014.11.062	computer science;machine learning;pattern recognition;data mining;spectral clustering	Vision	21.93626841480498	-43.05604148799678	91337
fe49c8b50c7bf4f1e6e1a31c81ed74200e99697a	text categorization with diversity random forests		Text categorization (TC), has many typical traits, such as large and difficult category taxonomies, noise and incremental data, etc. Random Forests, one of the most important but simple state-of-the-art ensemble methods, has been used to solve such type of subjects with good performance. most current Random Forests approaches with diversity-related issues focus on maximizing tree diversity while producing and training component trees. There are much diverse characteristics for component trees in TC trained on data of noise, huge categories and features. Consequently, given numerous component trees from the original Random Forests, we propose a novel method, Diversity Random Forests, which diversely and adaptively select and combine tree classifiers with diversity learning and sample weighting. Diversity Random Forests includes two key issues. First, by designing a matrix for the data distribution creatively, we formulate a unified optimization model for learning and selecting diverse trees, where tree weights are learned through a convex quadratic programming problem with given sample weights. Second, we propose a new self-training algorithm to iteratively run the convex optimization and automatically learn the sample weights. Extensive experiments on a variety of text categorization benchmark data sets show that the proposed approach consistently outperforms state-of-the-art methods.	categorization;document classification;random forest	Chun Yang;Xu-Cheng Yin;Kaizhu Huang	2014		10.1007/978-3-319-12643-2_39	natural language processing;pattern recognition;data mining	NLP	19.13254305558099	-42.268716195619916	91482
502a227d27aee2842091e55dcf9a8b8e733cb801	m-isomap: orthogonal constrained marginal isomap for nonlinear dimensionality reduction	graph theory;pattern clustering;data visualization manifolds euclidean distance benchmark testing databases coordinate measuring machines joining processes;visualization isomap manifold learning nonlinear dimensionality reduction dr pairwise constraints pcs;data visualisation;state of the art dr algorithms m isomap orthogonal constrained marginal isomap nonlinear dimensionality reduction geodesic distances nonlinear manifolds synthetic data set visualisation manifold learning pairwise cannot link constraints pairwise must link constraints shortest path distances constrained neighborhood graphs nonlinear dr method intraclass clusters interclass clusters university of california irvine benchmark real olivetti research library yale cmu pose expression databases illumination databases cmu pose databases data clustering power clustering evaluations;constraint handling;visual databases constraint handling data reduction data visualisation graph theory pattern clustering;data reduction;visual databases	Isomap is a well-known nonlinear dimensionality reduction (DR) method, aiming at preserving geodesic distances of all similarity pairs for delivering highly nonlinear manifolds. Isomap is efficient in visualizing synthetic data sets, but it usually delivers unsatisfactory results in benchmark cases. This paper incorporates the pairwise constraints into Isomap and proposes a marginal Isomap (M-Isomap) for manifold learning. The pairwise Cannot-Link and Must-Link constraints are used to specify the types of neighborhoods. M-Isomap computes the shortest path distances over constrained neighborhood graphs and guides the nonlinear DR through separating the interclass neighbors. As a result, large margins between both interand intraclass clusters are delivered and enhanced compactness of intracluster points is achieved at the same time. The validity of M-Isomap is examined by extensive simulations over synthetic, University of California, Irvine, and benchmark real Olivetti Research Library, YALE, and CMU Pose, Illumination, and Expression databases. The data visualization and clustering power of M-Isomap are compared with those of six related DR methods. The visualization results show that M-Isomap is able to deliver more separate clusters. Clustering evaluations also demonstrate that M-Isomap delivers comparable or even better results than some state-of-the-art DR algorithms.	algorithm;benchmark (computing);cluster analysis;computer cluster;data visualization;database;databases;dhrystone;distance;evaluation;graph - visual representation;imagery;isomap;marginal model;nonlinear dimensionality reduction;nonlinear system;short;shortest path problem;simulation;synthetic data;manifold;statistical cluster	Zhaoxing Zhang;Tommy W. S. Chow;Ming-Bo Zhao	2013	IEEE Transactions on Cybernetics	10.1109/TSMCB.2012.2202901	mathematical optimization;data reduction;artificial intelligence;graph theory;machine learning;pattern recognition;data mining;mathematics;statistics	Visualization	22.03933156786547	-41.74568431085198	91540
05814fe2997b77936c8991172a2f63e1ac4ab92d	erratum to: a method for metric learning with multiple-kernel embedding		Distance metric learning is rather important for measuring the similarity (/dissimilarity) of two instances in many pattern recognition algorithms. Although many linear Mahalanobis metric learning methods can be extended to their kernelized versions for dealing with the nonlinear structure data, choosing the proper kernel and determining the kernel parameters are still tough problems. Furthermore, the single kernel embedded metric is not suited for the problems with multi-view feature representations. In this paper, we address the problem of metric learning with multiple kernels embedding. By analyzing the existing formulations of metric learning with multiple-kernel embedding, we propose a new framework to learn multi-metrics as well as the corresponding weights jointly, the objective function can be shown to be convex and it can be converted to be a multiple kernel learning-support vector machine problem, which can be solved by existing methods. The experiments on single-view and multi-view data show the effectiveness of our method.	algorithm;baseline (configuration management);benchmark (computing);concatenation;converge;data structure;embedded system;experiment;free viewpoint television;global optimization;k-nearest neighbors algorithm;kernel (operating system);kernel method;loss function;multiple kernel learning;nonlinear system;optimization problem;pattern recognition;supervised learning;support vector machine	Xiao Lu;Yaonan Wang;Xuanyu Zhou;Zhigang Ling	2015	Neural Processing Letters	10.1007/s11063-015-9468-8	machine learning;artificial intelligence;kernel (linear algebra);embedding;mathematics;pattern recognition	AI	24.529145648722686	-42.27220787370207	91885
60a928020cc80b2f320450aa4c03bfd57f8711ff	interpretable recurrent neural networks using sequential sparse recovery		Recurrent neural networks (RNNs) are powerful and effective for processing sequential data. However, RNNs are usually considered “black box” models whose internal structure and learned parameters are not interpretable. In this paper, we propose an interpretable RNN based on the sequential iterative soft-thresholding algorithm (SISTA) for solving the sequential sparse recovery problem, which models a sequence of correlated observations with a sequence of sparse latent vectors. The architecture of the resulting SISTA-RNN is implicitly defined by the computational structure of SISTA, which results in a novel stacked RNN architecture. Furthermore, the weights of the SISTA-RNN are perfectly interpretable as the parameters of a principled statistical model, which in this case include a sparsifying dictionary, iterative step size, and regularization parameters. In addition, on a particular sequential compressive sensing task, the SISTA-RNN trains faster and achieves better performance than conventional state-of-the-art black box RNNs, including long-short term memory (LSTM) RNNs.	algorithm;artificial neural network;black box;compressed sensing;computation;dictionary;iterative method;long short-term memory;matrix regularization;neural networks;random neural network;recurrent neural network;sparse matrix;statistical model;thresholding (image processing)	Scott Wisdom;Thomas Powers;James W. Pitton;Les E. Atlas	2016	CoRR		computer science;machine learning;pattern recognition;data mining;statistics	ML	21.94300493805919	-47.585548030564894	91993
144abbe9ce275993ead8e1fdea020584f29757c1	robust feature-sample linear discriminant analysis for brain disorders diagnosis		A wide spectrum of discriminative methods is increasingly used in diverse applications for classification or regression tasks. However, many existing discriminative methods assume that the input data is nearly noise-free, which limits their applications to solve real-world problems. Particularly for disease diagnosis, the data acquired by the neuroimaging devices are always prone to different sources of noise. Robust discriminative models are somewhat scarce and only a few attempts have been made to make them robust against noise or outliers. These methods focus on detecting either the sample-outliers or feature-noises. Moreover, they usually use unsupervised de-noising procedures, or separately de-noise the training and the testing data. All these factors may induce biases in the learning process, and thus limit its performance. In this paper, we propose a classification method based on the least-squares formulation of linear discriminant analysis, which simultaneously detects the sample-outliers and feature-noises. The proposed method operates under a semi-supervised setting, in which both labeled training and unlabeled testing data are incorporated to form the intrinsic geometry of the sample space. Therefore, the violating samples or feature values are identified as sample-outliers or feature-noises, respectively. We test our algorithm on one synthetic and two brain neurodegenerative databases (particularly for Parkinson’s disease and Alzheimer’s disease). The results demonstrate that our method outperforms all baseline and state-of-the-art methods, in terms of both accuracy and the area under the ROC curve.	algorithm;baseline (configuration management);computer multitasking;database;de-identification;discriminative model;least squares;linear discriminant analysis;multi-task learning;receiver operating characteristic;semi-supervised learning;semiconductor industry;sensor;statistical classification;synthetic intelligence;unsupervised learning	Ehsan Adeli-Mosabbeb;Kimhan Thung;Le An;Feng Shi;Dinggang Shen	2015			computer science;machine learning;pattern recognition;statistics	ML	21.237583010117927	-46.603177164071504	92011
14590f2761518cbd29848312268a09a62c605c91	catenary support vector machines	support vector machine;face detection	Many problems require making sequential decisions. For these problems, the benefit of acquiring further information must be weighed against the costs. In this paper, we describe the catenary support vector machine (catSVM), a margin-based method to solve sequential stopping problems. We provide theoretical guarantees for catSVM on future testing examples. We evaluated the performance of catSVM on UCI benchmark data and also applied it to the task of face detection. The experimental results show that catSVM can achieve a better cost tradeoff than single-stage SVM and chained boosting.	algorithm;benchmark (computing);binary classification;concave function;convex optimization;cutting-plane method;data dependency;early stopping;face detection;mathematical optimization;optimization problem;out of memory;scalability;solver;support vector machine	Kin Fai Kan;Christian R. Shelton	2008		10.1007/978-3-540-87479-9_57	support vector machine;face detection;computer science;machine learning;pattern recognition;data mining	ML	20.80613046532413	-38.532522683507175	92114
900438e5296cdef7d74b1b8e397b4880d8763473	online class imbalance learning for quality estimation in manufacturing		Online machine learning has become increasingly important recently as more and more machines are being connected and data is being sent to the decision making node in real time. Traditional batch based machine learning is no longer suitable for such streaming data scenario. Here, an online classification algorithm to classify good and defective product, under imbalance streaming environment, is proposed. The proposed method exploits the assumption that different classes should be far away from each other. Even when the raw data might appear to be close, the algorithm learns and projects them into some specific manifold where different classes are far from each other. The algorithm classifies good and defective product in an imbalanced environment where good product outweighs defective product. The algorithm uses only single pass of the data, where the data is used once and then discarded. The approach is then being validated using industry data and the result indicates better performance in term of G-Mean and F1-score.	algorithm;artificial neural network;backdrop cms;data point;f1 score;internet of things;online machine learning;streaming media	Kee Jin Lee	2018	2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA)	10.1109/ETFA.2018.8502569		ML	14.00956988491331	-39.22237464132319	92278
1f3be552eec6e64b95190c7ca4088aaf645edbd9	a parallel genetic algorithm for adaptive hardware and its application to ecg signal classification		This paper presents a parallel genetic algorithm (GA) called the cellular compact genetic algorithm (c-cGA) and its implementation for adaptive hardware. An adaptive hardware based on the c-cGA is proposed to automate real-time classification of ECG signals. The c-cGA not only provides a strong search capability while maintaining genetic diversity using multiple GAs but also has a cellular-like structure and is a straight-forward algorithm suitable for hardware implementation. The c-cGA hardware and an adaptive digital filter structure also perform an adaptive feature selection in real time. The c-cGA is applied to a block-based neural network (BbNN) for online learning in the hardware. Using an adaptive hardware approach based on the c-cGA, an adaptive hardware system for classifying ECG signals is feasible. The proposed adaptive hardware can be implemented in a field programmable gate array (FPGA) for an adaptive embedded system applied to personalised ECG signal classifications for long-term patient monitoring.	adaptive filter;application-specific integrated circuit;artificial neural network;backpropagation;central processing unit;computer simulation;digital electronics;digital filter;display resolution;embedded system;evolutionary algorithm;feature selection;field-programmable gate array;fixed point (mathematics);fixed-point arithmetic;forward algorithm;genetic algorithm;mathematical optimization;mike lesser;multi-core processor;multiplexing;real-time clock;software propagation;software release life cycle;speedup;statistical model	Yutana Jewajinda;Prabhas Chongstitvatana	2012	Neural Computing and Applications	10.1007/s00521-012-0963-9	real-time computing;computer science;theoretical computer science;machine learning	ML	10.23274954037036	-41.34999506762516	92287
e9579d7a0a446137fab0c2031c255bafc64a02ef	an experimental study on rotation forest ensembles	rotation forest;decision tree;classifier ensemble;ensemble method;discriminant analysis;feature extraction;random forest;pattern recognition;classifier ensembles;random projection	Rotation Forest is a recently proposed method for building classifier ensembles using independently trained decision trees. It was found to be more accurate than bagging, AdaBoost and Random Forest ensembles across a collection of benchmark data sets. This paper carries out a lesion study on Rotation Forest in order to find out which of the parameters and the randomization heuristics are responsible for the good performance. Contrary to common intuition, the features extracted through PCA gave the best results compared to those extracted through non-parametric discriminant analysis (NDA) or random projections. The only ensemble method whose accuracy was statistically indistinguishable from that of Rotation Forest was LogitBoost although it gave slightly inferior results on 20 out of the 32 benchmark data sets. It appeared that the main factor for the success of Rotation Forest is that the transformation matrix employed to calculate the (linear) extracted features is sparse.	adaboost;benchmark (computing);best practice;decision tree;experiment;feature vector;heuristic (computer science);linear discriminant analysis;logitboost;principal component analysis;random forest;random projection;sparse matrix;test set;transformation matrix	Ludmila I. Kuncheva;Juan José Rodríguez Diez	2007		10.1007/978-3-540-72523-7_46	machine learning;pattern recognition;data mining;mathematics	ML	14.590122117660208	-41.835807995320174	92305
1e30011d19703d5167621aa8de38687915fed7eb	particle competition in complex networks for semi-supervised classification	distributed data;community detection;complex network;supervised classification;semi supervised learning;machine learning;loss function;computer simulation	Semi-supervised learning is an important topic in machine learning. In this paper, a network-based semi-supervised classification method is proposed. Class labels are propagated by combined randomdeterministic walking of particles and competition among them. Different from other graph-based methods, our model does not rely on loss function or regularizer. Computer simulations were performed with synthetic and real data, which show that the proposed method can classify arbitrarily distributed data, including linear non-separable data. Moreover, it is much faster due to lower order of complexity and it can achieve better results with few pre-labeled data than other graph based methods.	algorithm;computational complexity theory;computer simulation;iteration;loss function;machine learning;semi-supervised learning;semiconductor industry;supervised learning;synthetic intelligence	Fabricio A. Breve;Liang Zhao;Marcos G. Quiles	2009		10.1007/978-3-642-02466-5_14	computer simulation;semi-supervised learning;unsupervised learning;learning vector quantization;computer science;online machine learning;machine learning;linear classifier;pattern recognition;data mining;supervised learning;multilayer perceptron;complex network;loss function;generalization error	ML	19.623286192454408	-40.695280022387784	92477
e5f473c46ec7deebea26fb9b93a848fcb5335511	image augmentation using radial transform for training deep neural networks		Deep learning models have a large number of free parameters that must be estimated by efficient training of the models on a large number of training data samples to increase their generalization performance. In real-world applications, the data available to train these networks is often limited or imbalanced. We propose a sampling method based on the radial transform in a polar coordinate system for image augmentation to facilitate the training of deep learning models from limited source data. This pixel-wise transform provides representations of the original image in the polar coordinate system by generating a new image from each pixel. This technique can generate radial transformed images up to the number of pixels in the original image to increase the diversity of poorly represented image classes. Our experiments show improved generalization performance in training deep convolutional neural networks with radial transformed images.	artificial neural network;convolutional neural network;deep learning;experiment;neural network software;pixel;radial (radio);radial basis function;sampling (signal processing);source data	Hojjat Salehinejad;Shahrokh Valaee;Tim Dowdell;Joseph Barfett	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462241	free parameter;convolutional neural network;pixel;artificial neural network;deep learning;pattern recognition;artificial intelligence;source data;data modeling;polar coordinate system;computer science	Vision	23.301918838455975	-50.81123162702694	92610
bab7bb6780cba3b3a7eef2b2b23b8506ff390dbf	ai lessons learned from experiments in insider threat detection		Although AI has been successfully applied to many different domains with different characteristics, the task of applyi ng a solution that is successful in one problem domain to a different domain remains far from automatic. Even the simpler task of applying a solution to a related but different domain is problematic. In this paper, we discuss various problems that can occur when trying to solve a classification problem in a new problem domain (insider threat) by trying previousl y successful approaches in a related problem domain (intrusi on detection). We examine in depth why our results in the new problem domain did not reflect the successes from the previous domain. We conclude with various lessons learned that can be used when approaching a new problem domain.	experiment;insider threat;problem domain	Alexander Liu;Cheryl E. Martin;Tom Hetherington;Sara Matzner	2006			artificial intelligence;computer science;machine learning;insider threat;intrusion detection system;problem domain	AI	18.11745256698853	-51.544703021993065	92643
e7e29cb9b6f69e68dacc28c072241916a921be62	constrained laplacian eigenmap for dimensionality reduction	document clustering;laplacian eigenmap;dimensionality reduction;machine learning;high dimensional data;graph embedding;dimensional reduction	Dimensionality reduction is a commonly used tool in machine learning, especially when dealing with high dimensional data. We consider semi-supervised graph based dimensionality reduction in this paper, and a novel dimensionality reduction algorithm called constrained Laplacian Eigenmap (CLE) is proposed. Suppose the data set contains r classes, and for each class we have some labeled points. CLE from others by using label information. CLE constrains the solution space of Laplacian Eigenmap only to contain embedding results that are consistent with the labels. Then, each point is represented as a rdimensional vector. Labeled points belonging to the same class are merged together, labeled points belonging to different classes are separated, and similar points are close to one another. We perform semi-supervised document clustering using CLE on two standard corpora. Experimental results show that CLE is very effective. & 2009 Elsevier B.V. All rights reserved.	algorithm;cluster analysis;conformal loop ensemble;data point;dimensionality reduction;eigenface;experiment;facial recognition system;feasible region;k-means clustering;linear discriminant analysis;locality of reference;machine learning;non-negative matrix factorization;nonlinear system;principal component analysis;semi-supervised learning;semiconductor industry;similarity measure;text corpus;unsupervised learning	Chun Chen;Lijun Zhang;Jiajun Bu;Can Wang;Wei Chen	2010	Neurocomputing	10.1016/j.neucom.2009.08.021	combinatorics;graph embedding;document clustering;computer science;machine learning;pattern recognition;mathematics;spectral clustering;dimensionality reduction;clustering high-dimensional data	AI	24.1827539446703	-41.92203246344915	92722
ea3365700c7c4925a218abe1d4e9e08874734686	an effective feature selection approach driven genetic algorithm wrapped bayes naïve		In this paper, an advanced novel feature selection FS algorithm is presented, the hybrid genetic algorithm GA with Bayes naive BN, which selects the most relevant optimum feature subset to increase the classification accuracy performance and computational timesaving. Based on GA, the proposed algorithm uses the highest genetic operatoru0027s values that involve a consistent classification accuracy. The performance of the algorithm is evaluated using 34 UCI/KEEL datasets from different domains. The classification accuracy is compared with recent literature works. On the other hand, the algorithm is also compared with an implemented sequential forward selection SFS technique coupled with different ML methods. The experimental results show the effectiveness of the algorithm.	feature selection;genetic algorithm;naivety	Sidahmed Mokeddem;Baghdad Atmani;Mostéfa Mokaddem	2016	IJDATS	10.1504/IJDATS.2016.10000317	support vector machine;computer science;machine learning;pattern recognition;data mining;feature selection;population-based incremental learning	AI	10.436408846673142	-43.240021122423215	92726
45b7b5514a65126d39a51d5a68da53e7aa244c1f	understanding and simplifying one-shot architecture search		Experiments on this dataset were reported in Table 2. When we compared our models to previously published mobile results such as Howard et al. (2017) and Zoph et al. (2017), we found that although our models had higher accuracies for the same number of parameters, they also had more MAC (Multiply-Add) operations. In order to make our results more directly comparable, we made the following changes to our search space:		Gabriel Bender;Pieter-Jan Kindermans;Barret Zoph;Vijay Vasudevan;Quoc V. Le	2018			machine learning;architecture;artificial intelligence;computer science	NLP	14.768429078746943	-49.43130840019668	92809
1e84acf2c1d233aa3736e8de9ff138d40067b430	uncertainty sampling and transductive experimental design for active dual supervision	experimental design;empirical study;active learning;domain knowledge;text classification;quality model	Dual supervision refers to the general setting of learning from both labeled examples as well as labeled features. Labeled features are naturally available in tasks such as text classification where it is frequently possible to provide domain knowledge in the form of words that associate strongly with a class. In this paper, we consider the novel problem of active dual supervision, or, how to optimally query an example and feature labeling oracle to simultaneously collect two different forms of supervision, with the objective of building the best classifier in the most cost effective manner. We apply classical uncertainty and experimental design based active learning schemes to graph/kernel-based dual supervision models. Empirical studies confirm the potential of these schemes to significantly reduce the cost of acquiring labeled data for training high-quality models.	active directory;active learning (machine learning);design of experiments;document classification;duality (optimization);graph kernel;image-line fl studio;sampling (signal processing)	Vikas Sindhwani;Prem Melville;Richard D. Lawrence	2009		10.1145/1553374.1553496	computer science;artificial intelligence;machine learning;pattern recognition;data mining;active learning;empirical research;design of experiments;domain knowledge	ML	20.261467968998883	-43.178447250966464	92819
06e7b2c03b2fdaaf11e3e2863c9770ad645b5aaa	two stage classifier chain architecture for efficient pair-wise multi-label learning	classifier chains;two stage architecture;predictive models testing training computational complexity computer architecture computational modeling;tscca two stage classifier chain architecture pairwise multilabel learning problem transformation methods dichotomizing classifiers pairwise decomposition strategy quadratic number learning problems;classifier chains multi label learning two stage architecture;prediction accuracy;pattern classification;pattern classification learning artificial intelligence;learning problems;learning artificial intelligence;multi label learning	A common approach for solving multi-label learning problems using problem-transformation methods and dichotomizing classifiers is the pair-wise decomposition strategy. One of the problems with this approach is the need for querying a quadratic number of binary classifiers for making a prediction that can be quite time consuming, especially in learning problems with large number of labels. To tackle this problem we propose a Two Stage Classifier Chain Architecture (TSCCA) for efficient pair-wise multi-label learning. Six different real-world datasets were used to evaluate the performance of the TSCCA. The performance of the architecture was compared with six methods for multi-label learning and the results suggest that the TSCCA outperforms the concurrent algorithms in terms of predictive accuracy. In terms of testing speed TSCCA shows better performance comparing to the pair-wise methods for multi-label learning.	concurrent algorithm;multi-label classification;naive bayes classifier	Dejan Gjorgjevikj;Gjorgji Madjarov	2011	2011 IEEE International Workshop on Machine Learning for Signal Processing	10.1109/MLSP.2011.6064599	semi-supervised learning;computer science;artificial intelligence;machine learning;pattern recognition;learning classifier system;stability	Vision	19.030849013039546	-39.636418250041494	92872
6cc929069e55c2823a8d6b7c6f9a6cadf80d9e0f	improved group sparse classifier	reconnaissance visage;metodo cuadrado menor;iterative method;metodo caso peor;methode moindre carre;optimisation;non convex programming;image processing;non convex optimization;least squares method;optimizacion;learning;convex programming;biometrie;biometrics;database;biometria;procesamiento imagen;base dato;programmation non convexe;convex optimization;programmation convexe;classification;traitement image;metodo iterativo;algorithme;aprendizaje;optimization problem;algorithm;programacion no convexa;apprentissage;automatic recognition;reconnaissance caractere;face recognition;methode iterative;signal classification;base de donnees;methode cas pire;pattern recognition;classification signal;optimization;reconnaissance forme;classification automatique;reconocimiento patron;automatic classification;iterative reweighted least squares;optimal algorithm;worst case method;clasificacion automatica;character recognition;quasi convex optimization;reconocimiento caracter;reconocimiento automatico;reconnaissance automatique;algoritmo;programacion convexa	This work proposes a new classifier based on the assumption that the training samples of a particular class approximately form a linear basis for any new test sample belonging to that class. This is not a new assumption; two previous works namely Sparse Classifier (Yang et al., 2009) and Group Sparse Classifier (Majumdar and Ward, 2009a) has been built upon it. However both the previous works are fraught with certain shortcomings. The two optimization algorithms proposed in the two previous works do not capture all the implications of the said assumption perfectly. This work, accounts for all the intricacies of the said assumption and consequently requires solving a new non-convex optimization problem. We have developed an elegant approach to solve the optimization problem based on Iterative Reweighted Least Squares method. Our classifier is very flexible; the previous classifiers are actually two special cases of the said classifier. The proposed classifier has been rigorously compared against the previous ones stemming from the said assumption on benchmark classification databases. The results indicate that in most cases the proposed classifier is significantly better than the previous ones and in the worst case the proposed classifier is as good as the previous ones.	sparse	Angshul Majumdar;Rabab Kreidieh Ward	2010	Pattern Recognition Letters	10.1016/j.patrec.2010.06.014	margin classifier;optimization problem;margin;convex optimization;quadratic classifier;biological classification;computer science;artificial intelligence;machine learning;mathematics;iterative method;least squares;algorithm;biometrics	Vision	23.455991498736815	-39.086538782974635	92972
51670689932a711897888360c34eae21c27257c3	classifiers for dissimilarity-based pattern recognition	decision theory pattern classification feature extraction;nearest neighbor searches;rank based method dissimilarity pattern recognition pattern classification decision rules distance representation feature space;distance representation;normal distribution;prototypes;testing;feature space;support vector classifier;physics;pattern recognition testing space technology physics nearest neighbor searches prototypes;feature extraction;learning from examples;decision theory;decision rules;pattern classification;pattern recognition;dissimilarity;space technology;rank based method;decision rule	In the traditional way of learning from examples of objects the classifiers are built in a feature space. However, alternative ways can be found by constructing decision rules on dissimilarity (distance) representations, instead. In such a recognition process a new object is described by its distances to (a subset of) the training samples. In this paper a number of methods to tackle this type of classification problem are investigated: the feature-based (i.e. interpreting the distance representation as a feature space) and rank-based (i.e. considering the given relations) decision rules. The experiments demonstrate that the feature-based (especially normal-based) classifiers often outperform the rank-based ones. This is to be expected, since summation-based distances are, under general conditions, approximately normally distributed. In addition, the support vector classifier achieves also a high accuracy.	experiment;feature vector;pattern recognition;support vector machine	Elzbieta Pekalska;Robert P. W. Duin	2000		10.1109/ICPR.2000.906008	random subspace method;computer science;machine learning;linear classifier;pattern recognition;data mining;decision rule;mathematics;feature;statistics	AI	11.775690428251522	-45.03388668583718	93116
0471834190f14aafc59d92b5b648d11e440b519f	milis: multiple instance learning with instance selection	iterative method;optimisation;learning algorithm;analisis estadistico;optimizacion;medicion densidad;supervised learning;support vector machines;density measurement;instance selection;algoritmo adaptativo;multiple instance learning;optimization framework;institute for integrated and intelligent systems;prototypes;training;extraction forme;faculty of science environment engineering and technology;training support vector machines machine learning training data prototypes algorithm design and analysis optimization;training process;multiplicite;algorithme apprentissage;iterative manner;journal article;training data sets;computer vision;metodo iterativo;iterative methods;training data;classification a vaste marge;real world data;adaptive algorithm;algorithme adaptatif;statistical analysis;machine learning;extraccion forma;milis;methode iterative;080109;state of the art;kernel density estimator;analyse statistique;multiplicidad;classifier learning;kernel density estimate;pattern recognition and data mining;speed ups;mesure densite;adapti alternating optimization;feature selection;optimization;real world data sets application;kernel density estimators;support vector machine;apprentissage supervise;maquina ejemplo soporte;vector support machine;learning artificial intelligence;alternating optimization multiple instance learning support vector machine feature selection;keywords alternating optimization;aprendizaje supervisado;algoritmo aprendizaje;learning artificial intelligence iterative methods;080104;pattern extraction;algorithm design;multiplicity;alternating optimization;algorithm design and analysis;kernel density estimator milis multiple instance learning instance selection supervised learning real world data sets application iterative manner	Multiple instance learning (MIL) is a paradigm in supervised learning that deals with the classification of collections of instances called bags. Each bag contains a number of instances from which features are extracted. The complexity of MIL is largely dependent on the number of instances in the training data set. Since we are usually confronted with a large instance space even for moderately sized real-world data sets applications, it is important to design efficient instance selection techniques to speed up the training process without compromising the performance. In this paper, we address the issue of instance selection in MIL. We propose MILIS, a novel MIL algorithm based on adaptive instance selection. We do this in an alternating optimization framework by intertwining the steps of instance selection and classifier learning in an iterative manner which is guaranteed to converge. Initial instance selection is achieved by a simple yet effective kernel density estimator on the negative instances. Experimental results demonstrate the utility and efficiency of the proposed approach as compared to the state of the art.	algorithm;bag device component;baseline (configuration management);categorization;collections (publication);converge;electron microscopy;extraction;feature model;genetic selection;instance unique identifier;iterative method;kernel density estimation;mathematical optimization;moderate response;multiple instance learning;outline of object recognition;programming paradigm;single-instance storage;speedup;subgroup;supervised learning;test set	Zhouyu Fu;Antonio Robles-Kelly;Jun Zhou	2011	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2010.155	kernel density estimation;support vector machine;algorithm design;instance-based learning;instance variable;computer science;artificial intelligence;machine learning;pattern recognition;iterative method;supervised learning;feature selection;statistics	ML	22.384518215071214	-38.098741210548866	93409
6b9abcf8755a3e56bf4a34a6c41769c9f7fc5bc6	building a practical and reliable classifier for malware detection		Having a machine learning algorithm that can correctly classify malicious software has become a necessity as old methods of detection based on hashes and hand written heuristics tend to fail when dealing with the intensive flow of new malware. However, in order to be practical, the machine learning classifiers must also have a reasonable training time and a very small amount, preferably zero, of false positives. There were a few authors who addressed both these issues in their papers but creating such a model is more difficult when more than 3 million files are involved/needed in the training. We mapped a zero false positive perceptron in a new space, applied a feature selection algorithm and used the resulted model in an ensemble, voting or a rule based clustering system we’ve managed to achieve a detection rate around 99 % and 0.07 % false positives while keeping the training time suitable for large data sets.	cloud computing;cluster analysis;decision tree;feature selection;heuristic (computer science);machine learning;malware;perceptron;protection mechanism;selection algorithm;statistical classification	Cristina Vatamanu;Dragos Gavrilut;Razvan Benchea	2013	Journal of Computer Virology and Hacking Techniques	10.1007/s11416-013-0188-1	computer science;artificial intelligence;machine learning;data mining	ML	12.614112662766477	-40.04051909892106	93569
379db361d272ac3ac0d878351f0fdaa1a561b7c0	a kernel fused perceptron for the online classification of large-scale data	online algorithm;kernel perceptron;online learning;large scale;fuseptron;nonlinear problem;nonlinear online learning;scalable online learning	To solve online nonlinear problems, usually, a set of misclassified observed examples (defined as support set) should be stored in the internal memory for computing kernel values. With the increase of a large scale of training data, computing all the kernel values is expensive and also can lead to an out-of-memory problem. In the paper, a fusion strategy is proposed to compress the size of support set for online learning and the fused kernel can best represent the current instance and its nearest one in the support set in the previous time. The proposed algorithm is based on Perceptron-like method, and thus it is called as Fuseptron. Different from the most recently proposed nonlinear online algorithms, the internal memory can be bounded in Fuseptron and the mistake bound is also derived. Experiments carried out on one synthetic and four real large-scale datasets validate the effectiveness and efficiency of Fuseptron compared to the state-of-the-art algorithms.	computer data storage;kernel (operating system);nonlinear system;online algorithm;out of memory;perceptron;synthetic intelligence	Huijun He;Mingmin Chi;Wenqiang Zhang	2012		10.1145/2351316.2351332	kernel method;kernel embedding of distributions;radial basis function kernel;computer science;online machine learning;machine learning;pattern recognition;data mining;tree kernel;polynomial kernel	AI	20.311560354438058	-38.86894530312284	93602
b2b17434c6d6f16ce1fee8fac496d9edf2f1873f	integration of graph clustering with ant colony optimization for feature selection	ant colony optimization;graph clustering;feature selection;filter method	Feature selection is an important preprocessing step in machine learning and pattern recognition. The ultimate goal of feature selection is to select a feature subset from the original feature set to increase the performance of learning algorithms. In this paper a novel feature selection method based on the graph clustering approach and ant colony optimization is proposed for classification problems. The proposed method’s algorithm works in three steps. In the first step, the entire feature set is represented as a graph. In the second step, the features are divided into several clusters using a community detection algorithm and finally in the third step, a novel search strategy based on the ant colony optimization is developed to select the final subset of features. Moreover the selected subset of each ant is evaluated using a supervised filter based method called novel separability index. Thus the proposed method does not need any learning model and can be classified as a filter based feature selection method. The proposed method integrates the community detection algorithm with a modified ant colony based search process for the feature selection problem. Furthermore, the sizes of the constructed subsets of each ant and also size of the final feature subset are determined automatically. The performance of the proposed method has been compared to those of the state-of-the-art filter and wrapper based feature selection methods on ten benchmark classification problems. The results show that our method has produced consistently better classification accuracies. 2015 Elsevier B.V. All rights reserved.	ant colony optimization algorithms;benchmark (computing);cluster analysis;computation;computational complexity theory;f1 score;feature selection;graph (discrete mathematics);linear separability;machine learning;mathematical optimization;pattern recognition;preprocessor;problem domain;relevance;run time (program lifecycle phase);selection algorithm;significant figures	Parham Moradi;Mehrdad Rostami	2015	Knowl.-Based Syst.	10.1016/j.knosys.2015.04.007	ant colony optimization algorithms;computer science;machine learning;pattern recognition;data mining;clustering coefficient;feature selection;feature	AI	10.60231418076084	-43.512317269646616	93668
dd8084b2878ca95d8f14bae73e1072922f0cc5da	model distillation with knowledge transfer in face classification, alignment and verification		Knowledge distillation is a potential solution for model compression. The idea 1 is to make a small student network imitate the target of a large teacher network, 2 then the student network can be competitive to the teacher one. Most previous 3 studies focus on model distillation in the classification task, where they propose 4 different architects and initializations for the student network. However, only the 5 classification task is not enough, and other related tasks such as regression and 6 retrieval are barely considered. To solve the problem, in this paper, we take face 7 recognition as a breaking point and propose model distillation with knowledge 8 transfer from face classification to alignment and verification. By selecting appro9 priate initializations and targets in the knowledge transfer, the distillation can be 10 easier in non-classification tasks. Experiments on the CelebA and CASIA-WebFace 11 datasets demonstrate that the student network can be competitive to the teacher 12 one in alignment and verification, and even surpasses the teacher network under 13 specific compression rates. In addition, to achieve stronger knowledge transfer, we 14 also use a common initialization trick to improve the distillation performance of 15 classification. Evaluations on the CASIA-Webface and large-scale MS-Celeb-1M 16 datasets show the effectiveness of this simple trick. 17		Chong Wang;Xipeng Lan	2017	CoRR			Vision	21.297685789373134	-49.75414467827206	93880
47ad6c9e4de45097b284e0a0ca6a4924b5a706ae	consistency-based anomaly detection with adaptive multiple-hypotheses predictions		In one-class-learning tasks, only the normal case can be modeled with data, whereas the variation of all possible anomalies is too large to be described sufficiently by samples. Thus, due to the lack of representative data, the wide-spread discriminative approaches cannot cover such learning tasks, and rather generative models, which attempt to learn the input density of the normal cases, are used. However, generative models suffer from a large input dimensionality (as in images) and are typically inefficient learners. We propose to learn the data distribution more efficiently with a multi-hypotheses autoencoder. Moreover, the model is criticized by a discriminator, which prevents artificial data modes not supported by data, and which enforces diversity across hypotheses. This consistency-based anomaly detection (ConAD) framework allows the reliable identification of outof-distribution samples. For anomaly detection on CIFAR-10, it yields up to 3.9% points improvement over previously reported results. On a real anomaly detection task, the approach reduces the error of the baseline models from 6.8% to 1.5%.	anomaly detection;autoencoder;baseline (configuration management);discriminator;generative model	Duc Tam Nguyen;Zhongyu Lou;Michael Klar;Thomas Brox	2018	CoRR		discriminative model;machine learning;artificial intelligence;generative grammar;anomaly detection;outlier;pattern recognition;curse of dimensionality;computer science	ML	19.860871590108356	-49.25520800153006	93900
7b239dcb2a95a7e9e249069226488f5aa1e86f60	a new dual nu -support vector machine.	classification algorithm;support vector machines;ν support vector machine;classification precision	ν-Support Vector Machine is one of the most widely used support vector machines because it provides a way to control the fraction of margin errors and the fraction of support vectors. However, being biased towards the class with the more training samples prevent it from being applied to some applications in which the size of classes is uneven. Although some updated ν-SVMs have been proposed to solve this problem, there are some issues in the formulations of these updated ν-SVMs. In this paper, a new ν-SVM, Double-ν Support Vector Machine, is proposed. It introduces ν+1 and ν−1 to control the upper bound on the fraction of bound support vectors and the lower bound on the fraction of support vectors of the positive class and the negative class respectively. Moreover, it reduces the complexity of formulations by eliminating a redundant constrain in ν-SVM formulations.		Yinshan Jia;Wang Yumei	2006		10.1007/11893028_91	margin classifier;support vector machine;mathematical optimization;computer science;machine learning;pattern recognition;mathematics;relevance vector machine;structured support vector machine	ML	16.0578625687844	-40.57107780162683	94034
9c8c9a42e251b4aa98b629830ae6f093d6cc30a0	faster learning of deep stacked autoencoders on multi-core systems using synchronized layer-wise pre-training		Deep neural networks are capable of modelling highly nonlinear functions by capturing different levels of abstraction of data hierarchically. While training deep networks, first the system is initialized near a good optimum by greedy layer-wise unsupervised pre-training. However, with burgeoning data and increasing dimensions of the architecture, the time complexity of this approach becomes enormous. Also, greedy pre-training of the layers often turns detrimental by over-training a layer causing it to lose harmony with the rest of the network. In this paper a synchronized parallel algorithm for pre-training deep networks on multi-core machines has been proposed. Different layers are trained by parallel threads running on different cores with regular synchronization. Thus the pre-training process becomes faster and chances of overtraining are reduced. This is experimentally validated using a stacked autoencoder for dimensionality reduction of MNIST handwritten digit database. The proposed algorithm achieved 26% speed-up compared to greedy layer-wise pre-training for achieving the same reconstruction accuracy substantiating its potential as an alternative.	artificial neural network;autoencoder;automatic parallelization;deep learning;dimensionality reduction;experiment;greedy algorithm;mnist database;multi-core processor;nonlinear system;parallel algorithm;parallel computing;principle of abstraction;time complexity	Anirban Santara;Debapriya Maji;D. P. Tejas;Pabitra Mitra;Arobinda Gupta	2016	CoRR		computer science;artificial intelligence;theoretical computer science;machine learning	AI	21.310618031706813	-49.12401653384198	94068
4ae0252be697282144395bb2be753024e2589e48	generative prior knowledge for discriminative classification	english language;prior knowledge;artificial intelligent;second order cone program;background knowledge;support vector machine;bilevel programming;classification accuracy	We present a novel framework for integrating prior knowledge into discriminative classifiers. Our framework allows discriminative classifiers such as Support Vector Machines (SVMs) to utilize prior knowledge specified in the generative setting. The dual objective of fitting the data and respecting prior knowledge is formulated as a bilevel program, which is solved (approximately) via iterative application of second-order cone programming. To test our approach, we consider the problem of using WordNet (a semantic database of English language) to improve low-sample classification accuracy of newsgroup categorization. WordNet is viewed as an approximate, but readily available source of background knowledge, and our framework is capable of utilizing it in a flexible way.	approximation algorithm;bilevel optimization;categorization;discriminative model;iterative method;second-order cone programming;support vector machine;wordnet	Gerald DeJong;Arkady Epshteyn	2006	J. Artif. Intell. Res.	10.1613/jair.1934	support vector machine;computer science;artificial intelligence;english;machine learning;pattern recognition;data mining;bilevel optimization	ML	18.652131786838392	-43.04840767054706	94116
b0dae9eb18caa88bbbef0e242485a67cf723d6a6	generalised bottom-up pruning: a model level combination of decision trees	churn prediction;ensemble methods;decision tree pruning	Highlights? Bottom-up pruning is extended to multiple tree context. ? Suitable pruning criteria are proposed. ? Method is tested on a number of UCI datasets. ? Method produces single trees with good performance/compactness. ? Applied to a churn prediction problem, interpretable trees were produced. A generalisation of bottom-up pruning is proposed as a model level combination method for a decision tree ensemble. Bottom up pruning on a single tree involves choosing between a subtree rooted at a node, and a leaf, dependant on a pruning criterion. A natural extension to an ensemble of trees is to allow subtrees from other ensemble trees to be grafted onto a node in addition to the operations of pruning to a leaf and leaving the existing subtree intact. Suitable pruning criteria are proposed and tested for this multi-tree pruning context. Gains in both performance and in particular compactness over individually pruned trees are observed in tests performed on a number of datasets from the UCI database. The method is further illustrated on a churn prediction problem in the telecommunications domain.	bottom-up parsing;decision tree	Mark Eastwood;Bogdan Gabrys	2012	Expert Syst. Appl.	10.1016/j.eswa.2012.02.061	null-move heuristic;computer science;machine learning;principal variation search;pattern recognition;data mining;mathematics;ensemble learning;killer heuristic;pruning	ML	13.156315175670269	-39.57663932937502	94199
8d0051d1d6f531eef455da7234e81e0edda0eee5	deep comparison: relation columns for few-shot learning		Few-shot deep learning is a topical challenge area for scaling visual recognition to open-ended growth in the space of categories to recognise. A promising line work towards realising this vision is deep networks that learn to match queries with stored training images. However, methods in this paradigm usually train a deep embedding followed by a single linear classifier. Our insight is that effective generalpurpose matching requires discrimination with regards to features at multiple abstraction levels. We therefore propose a new framework termed Deep Comparison Network (DCN) that decomposes embedding learning into a sequence of modules, and pairs each with a relation module. The relation modules compute a non-linear metric to score the match using the corresponding embedding module’s representation. To ensure that all embedding module’s features are used, the relation modules are deeply supervised. Finally generalisation is further improved by a learned noise regulariser. The resulting network achieves state of the art performance on both miniImageNet and tieredImageNet, while retaining the appealing simplicity and efficiency of deep metric learning approaches.	2.5d;columns;deep learning;dynamic circuit network;feature extraction;linear classifier;nonlinear gameplay;nonlinear system;overfitting;programming paradigm;relational operator;vector graphics	Xueting Zhang;Flood Sung;Yuting Qiang;Yongxin Yang;Timothy M. Hospedales	2018	CoRR			Vision	24.04728494155566	-49.5908230662368	94314
9665247ea3421929f9b6ad721f139f11edb1dbb8	learning longer memory in recurrent neural networks		Recurrent neural network is a powerful model that learns temporal patterns in sequential data. For a long time, it was believed that recurrent networks are difficult to train using simple optimizers, such as stochastic gradient descent, due to the so-called vanishing gradient problem. In this paper, we show that learning longer term patterns in real data, such as in natural language, is perfectly possible using gradient descent. This is achieved by using a slight structural modification of the simple recurrent neural network architecture. We encourage some of the hidden units to change their state slowly by making part of the recurrent weight matrix close to identity, thus forming a kind of longer term memory. We evaluate our model on language modeling tasks on benchmark datasets, where we obtain similar performance to the much more complex Long Short Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997).	artificial neural network;benchmark (computing);computation;language model;long short-term memory;multilayer perceptron;natural language;network architecture;neural networks;persistence (computer science);recurrent neural network;stochastic gradient descent;vanishing gradient problem	Tomas Mikolov;Armand Joulin;Sumit Chopra;Michaël Mathieu;Marc'Aurelio Ranzato	2014	CoRR		computer science;artificial intelligence;recurrent neural network;machine learning;deep learning;artificial neural network;algorithm	ML	20.911392920731146	-48.69009943738789	94351
82abe98e9dc484b4f8f0df8c3cbd24acc7f38d81	robustness analysis of eleven linear classifiers in extremely high-dimensional feature spaces	gene expression profile;classification algorithm;robustness analysis;high dimensionality;mean error;statistical significance;feature space;high dimensional data;error rate;classification accuracy;dimensional reduction;large margin classifier	In this study we address the linear classification of noisy high-dimensional data in a two class scenario. We assume that the cardinality of the data is much lower than its dimensionality. The problem of classification in this setting is intensified in the presence of noise. Eleven linear classifiers were compared on two-thousand-one-hundred-and-fifty artificial datasets from four different experimental setups, and five real world gene expression profile datasets, in terms of classification accuracy and robustness. We specifically focus on linear classifiers as the use of more complex concept classes would make over-adaptation even more likely. Classification accuracy is measured by mean error rate and mean rank of error rate. These criteria place two large margin classifiers, SVM and ALMA, and an online classification algorithm called PA at the top, with PA being statistically different from SVM on the artificial data. Surprisingly, these algorithms also outperformed statistically significant all classifiers investigated with dimensionality reduction.	spaces	Ludwig Lausser;Hans A. Kestler	2010		10.1007/978-3-642-12159-3_7	random subspace method;feature vector;word error rate;computer science;machine learning;linear classifier;pattern recognition;data mining;mathematics;mean squared error;statistical significance;statistics;clustering high-dimensional data	ML	12.988354194736992	-42.94510155734413	94644
74939e7282d3e9e01592516dd0f957a577014c10	prediction of helix, strand segments from primary protein sequences by a set of neural networks	prediction method;protein sequence;secondary structure;neural network	In prediction of secondary structure of proteins there are always some suspected segments. These suspected segments confuse people and lower the accuracy of prediction methods. To deal with this problem, a set of neural networks (NNs) are built based on helix, strand and coil segments selected from PDB. The test performance of these NNs on training data is perfect without surprise. However the prediction on test data is not good enough because the training data are lake of great representativeness. The results support the fact that closer neighbor vectors have the similar outputs of NNs. One can improve representativeness of training data without enlarging data scale as long as select less data from dense region and more from sparse region on condition that distribution of sample data has been known.		Zhuo Song;Ning Zhang;Zhuo Yang;Tao Zhang	2007		10.1007/978-3-540-72393-6_147	computer science;artificial intelligence;machine learning;protein sequencing;data mining;artificial neural network;protein secondary structure	ML	12.326060630618992	-50.03447238092338	94690
0c0aa58d29a25981f4eecdbde17750eb7749ac4a	detecting learning vs memorization in deep neural networks using shared structure validation sets		Abstract: The roles played by learning and memorization represent an important topic in deep learning research. Recent work on this subject has shown that the optimization behavior of DNNs trained on shuffled labels is qualitatively different from DNNs trained with real labels. Here, we propose a novel permutation approach that can differentiate memorization from learning in deep neural networks (DNNs) trained as usual (i.e., using the real labels to guide the learning, rather than shuffled labels). The evaluation of weather the DNN has learned and/or memorized, happens in a separate step where we compare the predictive performance of a shallow classifier trained with the features learned by the DNN, against multiple instances of the same classifier, trained on the same input, but using shuffled labels as outputs. By evaluating these shallow classifiers in validation sets that share structure with the training set, we are able to tell apart learning from memorization. Application of our permutation approach to multi-layer perceptrons and convolutional neural networks trained on image data corroborated many findings from other groups. Most importantly, our illustrations also uncovered interesting dynamic patterns about how DNNs memorize over increasing numbers of training epochs, and support the surprising result that DNNs are still able to learn, rather than only memorize, when trained with pure Gaussian noise as input.	artificial neural network;convolutional neural network;deep learning;layer (electronics);mathematical optimization;multilayer perceptron;neural network software;sensor;shallow parsing;test set	Elias Chaibub Neto	2018	CoRR		perceptron;permutation;machine learning;mathematics;artificial intelligence;convolutional neural network;artificial neural network;gaussian noise;deep learning;memorization;training set	ML	20.434102376382697	-50.586277543426064	94972
f09a62d8f4203a5ce76ac2d1d515d5653d3dea02	an adaptive rule-based classifier for mining big biological data	decision tree;genomic data;classification;brugada syndrome;rule based classifier	In this paper, we introduce a new adaptive rule-based classifier for multi-class classification of biological data, where several problems of classifying biological data are addressed: overfitting, noisy instances and class-imbalance data. It is well known that rules are interesting way for representing data in a human interpretable way. The proposed rule-based classifier combines the random subspace and boosting approaches with ensemble of decision trees to construct a set of classification rules without involving global optimisation. The classifier considers random subspace approach to avoid overfitting, boosting approach for classifying noisy instances and ensemble of decision trees to deal with class-imbalance problem. The classifier uses two popular classification techniques: decision tree and k-nearest-neighbor algorithms. Decision trees are used for evolving classification rules from the training data, while k-nearest-neighbor is used for analysing the misclassified instances and removing vagueness between the contradictory rules. It considers a series of k iterations to develop a set of classification rules from the training data and pays more attention to the misclassified instances in the next iteration by giving it a boosting flavour. This paper particularly focuses to come up with an optimal ensemble classifier that will help for improving the prediction accuracy of DNA variant identification and classification task. The performance of proposed classifier is tested with compared to well-approved existing machine learning and data mining algorithms on genomic data (148 Exome data sets) of Brugada syndrome and 10 real benchmark life sciences data sets from the UCI (University of California, Irvine) machine learning repository. The experimental results indicate that the proposed classifier has exemplary classification accuracy on different types of biological data. Overall, the proposed classifier offers good prediction accuracy to new DNA variants classification where noisy and misclassified variants are optimised to increase test performance. © 2016 Elsevier Ltd. All rights reserved.	benchmark (computing);boosting (machine learning);data mining;decision tree learning;ensemble learning;global optimization;iteration;k-nearest neighbors algorithm;logic programming;machine learning;mathematical optimization;multiclass classification;overfitting;statistical classification;vagueness	Dewan Md. Farid;M. Abdulla Al Mamun;Bernard Manderick;Ann Nowé	2016	Expert Syst. Appl.	10.1016/j.eswa.2016.08.008	margin classifier;probabilistic classification;bayes classifier;margin;quadratic classifier;biological classification;computer science;artificial intelligence;machine learning;decision tree;linear classifier;pattern recognition;data mining;statistics;pruning	ML	10.954893347651756	-42.77961245332327	95272
c1fb953aaf81a7ae53da6d66b15b62d0c169d33e	churn prediction via support vector classification: an empirical comparison		An empirical framework for customer churn prediction modeling is presented in this work. This task represents a very interesting business analytics challenge, given its highly class imbalanced nature, and the presence of noisy variables that adversely affect the prediction capabilities of classification models. In this work, two SVM-based techniques are compared: Support Vector Data Description (SVDD), and standard two-class SVMs. The proposed methodology involves the comparison of these two methods under different conditions of class imbalance and using different subsets of variables. Feature ranking is performed via the Fisher Score Criterion, while the class imbalance problem is dealt with through resampling techniques, namely random undersampling and SMOTE oversampling. Experiments on four customer churn prediction datasets show the advantages of SVDD: it outperforms standard SVM in terms of predictive performance, demonstrating the importance of techniques that take the class imbalance problem into account.	support vector machine	Sebastián Maldonado	2015	Intell. Data Anal.	10.3233/IDA-150774	computer science;machine learning;pattern recognition;data mining;statistics	ML	14.600128635508717	-41.748380689460944	95310
c6e5a7800ed32d3f367a789a57efc2768965f0eb	distortion-invariant recognition via jittered querie	image recognition;image classification;classification;training set;image recognition jitter image classification;chromium;query execution;distortion invariant recognition;memory based learning;jitter;memory based learning distortion invariant recognition classification training set query execution	This paper presents a new approach for achieving distortion-invariant recognition and classification. A test example to be classified is viewed as a query intended to find similar examples in the training set (or to find similar class models that represent a compression of the training set). The key idea is that instead of querying with a single pattern, we construct a more robust query, based on the family of patterns formed by distorting the test example. Although query execution is slower than if the invariances were successfully pre-compiled during training, there are significant advantages in several contexts: (i) providing invariances in memory-based learning, (ii) in model selection, where reducing training time at the expense of test time is a desirable trade-off, and (iii) in enabling robust, ad hoc searches based on a single example. Preliminary tests for memory-based learning on the NIST handwritten digit database with a limited set of shearing and translation distortions produced an error rate of 1.35%.	distortion;query language	Michael C. Burl	2000		10.1109/CVPR.2000.855893	computer vision;training set;chromium;web query classification;jitter;computer science;machine learning;pattern recognition	Vision	18.18700846465779	-47.930335158002656	95343
2885973722eaa0398452a7454d62fc9258ec7684	euclidean distance based label noise cleaning		Quality of the datasets play an important role in performance of supervised classifiers. In presence of mislabelled examples, the performance of such classifiers degrades severely. In this paper we propose a label noise cleaning approach based on euclidean distance. The examples most likely to be mislabelled generally have similar mean euclidean distances with positive and negative examples. Selecting such examples for an expert's review can help in cleaning the label noise. Since support vector examples contain most of the mislabelled examples, we propose that reviewing the top 50% support vector examples with similar euclidean distances for positive and negative examples can clean most of the label noise from the datasets. An important contribution of our proposed method is that it requires only 1 iteration to clean the label noise, which can be helpful in cases where datasets need to be made quickly. We show that our proposed method removed more than 90% of the label noise with relatively less examples reviewed.	euclidean distance;iteration;noise (electronics);plasma cleaning;sputter cleaning;supervised learning;whole earth 'lectronic link	Muhammad Ammar Malik;Moonsoo Kang	2017	2017 Ninth International Conference on Ubiquitous and Future Networks (ICUFN)	10.1109/ICUFN.2017.7993783	support vector machine;euclidean distance;computer science;euclidean geometry;artificial intelligence;machine learning;pattern recognition	ML	15.867509863386166	-41.05666600333328	95363
5d2a28a7746580d6d7deef9395f4c547d2beb001	novelty detection using soft partitioning and hierarchical models		In this paper, we study novelty detection problem and introduce an online algorithm. The algorithm sequentially receives an observation, generates a decision and then updates its parameters. In the first step, to model the underlying distribution, algorithm constructs a score function. In the second step, this score function is used to make the final decision for the observed data. After thresholding procedure is applied, the final decision is made. We obtain the score using versatile and adaptive nested decision tree. We employ nested soft decision trees to partition the observation space in an hierarchical manner. Based on the sequential performance, we optimize all the components of the tree structure in an adaptive manner. Although this in time adaptation provides powerful modeling abilities, it might suffer from overfitting. To circumvent overfitting problem, we employ the intermediate nodes of tree in order to generate subtrees and we then combine them in an adaptive manner. The experiments illustrate that the introduced algorithm significantly outperforms the state of the art methods.	bayesian network;decision tree;experiment;novelty detection;online algorithm;overfitting;thresholding (image processing);tree (data structure);tree structure	Tolga Ergen;Kaan Gokcesu;Mustafa Simsek;Suleyman Serdar Kozat	2017	2017 25th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2017.7960213	pattern recognition;online algorithm;computer science;overfitting;tree structure;incremental decision tree;decision tree;novelty detection;machine learning;thresholding;artificial intelligence;id3 algorithm	ML	16.112178492078122	-38.581008690115475	95536
affa0e1a48299316b57ac51b5fd5c934c70e4e34	non-parametric bayesian annotator combination		Relying on a single imperfect human annotator is not recommended in real crowdsourced classification problems. In practice, several annotators’ propositions are generally aggregated to obtain a better classification accuracy. Bayesian approaches, by modelling the relationship between each annotator’s output and the possible true labels (classes), have been shown to outperform other simpler models. Unfortunately, they assume that the total number of true labels is known. This is not the case in lots of realistic scenarios such as open-world classification where the number of possible labels is undetermined and may change over time. In this paper, we show how to set a non-parametric prior over the possible label set using the Dirichlet process in order to overcome this limitation. We illustrate this prior over the Bayesian annotator combination (BAC) model from the state of the art, resulting in the so-called non-parametric BAC (NPBAC). ∗Corresponding author Email address: servajean@lirmm.fr (M. Servajean) Preprint submitted to Information Sciences January 8, 2018 We show how to derive its variational equations to evaluate the model and how to assess it when the Dirichlet process has a prior using the Laplace method. We apply the model to several scenarios related to closed-world classification, open-world classification and novelty detection on a dataset previously published and on two datasets related to plant classification. Our experiments show that NPBAC is able to determine the true number of labels, but also and surprisingly, it largely outperforms the parametric annotator combination by modelling more complex confusions, in particular when few or no training data are available.		Maximilien Servajean;Romain Chailan;Alexis Joly	2018	Inf. Sci.	10.1016/j.ins.2018.01.020	mathematics;machine learning;artificial intelligence;novelty detection;laplace transform;laplace's method;parametric statistics;nonparametric statistics;bayesian probability;crowdsourcing;dirichlet process	ML	16.735399497080643	-38.06459091122479	95771
a59800f16ad02f550c600fff4179167bad0b8654	neonatal pain expression recognition using transfer learning.		Transfer learning using pre-trained Convolutional Neural Networks (CNNs) has been successfully applied to images for different classification tasks. In this paper, we propose a new pipeline for pain expression recognition in neonates using transfer learning. Specifically, we propose to exploit a pre-trained CNN that was originally trained on a relatively similar dataset for face recognition (VGG Face) as well as CNNs that were pre-trained on a relatively different dataset for image classification (iVGG F,M, and S) to extract deep features from neonates' faces. In the final stage, several supervised machine learning classifiers are trained to classify neonates' facial expression into pain or no pain expression. The proposed pipeline achieved, on a testing dataset, 0.841 AUC and 90.34 accuracy, which is approx. 7 higher than the accuracy of handcrafted traditional features. We also propose to combine deep features with traditional features and hypothesize that the mixed features would improve pain classification performance. Combining deep features with traditional features achieved 92.71 accuracy and 0.948 AUC. These results show that transfer learning, which is a faster and more practical option than training CNN from the scratch, can be used to extract useful features for pain expression recognition in neonates. It also shows that combining deep features with traditional handcrafted features is a good practice to improve the performance of pain expression recognition and possibly the performance of similar applications.	approximation;artificial neural network;computer vision;convolutional neural network;facial recognition system;feature engineering;machine learning;supervised learning	Ghada Zamzami;Dmitry B. Goldgof;Rangachar Kasturi;Yu Sun	2018	CoRR		transfer of learning;convolutional neural network;pattern recognition;artificial intelligence;machine learning;scratch;facial recognition system;facial expression;computer science;contextual image classification	AI	22.206362909504257	-50.92768503478836	95942
5dee86a2b010d8517fe6052988c7590983a71170	data mining approach for predicting the likelihood of a disease	data mining		data mining	Manuel Penaloza;Paul Breeden;Donna Kliche	1999			computer science;data mining	ML	11.419622776534581	-48.142367775780286	95943
a87f90228d2cc46217c79c9febbcc58428867ece	an integrated k-means - laplacian cluster ensemble approach for document datasets	cluster ensemble;k means;laplacian;cluster analysis	Cluster ensemble has become an important extension to traditional clustering algorithms, yet the cluster ensemble problem is very challenging due to the inherent difficulty in resolving the label correspondence problem. We adapted the integrated K-means - Laplacian clustering approach to solve the cluster ensemble problem by exploiting both the attribute information embedded in the cluster labels and the pairwise relations among the objects. The optimal solution of the proposed approach requires computing the pseudo inverse of the normalized Laplacian matrix and the eigenvalue decomposition of a large matrix, which can be computationally burdensome for large scale document datasets. We devised an effective algebraic transformation method for efficiently carrying out the aforementioned computations and proposed an integrated K-means - Laplacian cluster ensemble approach (IKLCEA). Experimental results with benchmark document datasets demonstrate that IKLCEA outperforms other cluster ensemble techniques on most cases. In addition, IKLCEA is computationally efficient and can be readily employed in large scale document applications.	k-means clustering	Sen Xu;Kung-Sik Chan;Jun Gao;Xiufang Xu;Xianfeng Li;Xiaopeng Hua;Jing An	2016	Neurocomputing	10.1016/j.neucom.2016.06.034	laplace operator;computer science;machine learning;pattern recognition;data mining;mathematics;cluster analysis;k-means clustering	NLP	23.493710208080895	-41.89635733249324	95961
91cd4420ab86407229eb0f194421085319e2458e	learning based image transformation using convolutional neural networks		We have developed a learning-based image transformation framework and successfully applied it to three common image transformation operations: downscaling, decolorization, and high dynamic range image tone mapping. We use a convolutional neural network (CNN) as a non-linear mapping function to transform an input image to a desired output. A separate CNN network trained for a very large image classification task is used as a feature extractor to construct the training loss function of the image transformation CNN. Unlike similar applications in the related literature such as image super-resolution, none of the problems addressed in this paper have a known ground truth or target. For each problem, we reason about a suitable learning objective function and develop an effective solution. This is the first work that uses deep learning to solve and unify these three common image processing tasks. We present experimental results to demonstrate the effectiveness of the new technique and its state-of-the-art performances.	artificial neural network;computer vision;convolutional neural network;deep learning;downscaling;dynamic range;ground truth;high-dynamic-range imaging;image processing;loss function;nonlinear system;optimization problem;performance;randomness extractor;range imaging;super-resolution imaging;tone mapping	Xianxu Hou;Yuanhao Gong;Bozhi Liu;Ke Sun;Jun Liu;Bolei Xu;Jiang Duan;Guoping Qiu	2018	IEEE Access	10.1109/ACCESS.2018.2868733	task analysis;image processing;convolutional neural network;deep learning;tone mapping;distributed computing;machine learning;contextual image classification;ground truth;computer science;artificial intelligence;image resolution	Vision	24.414674267581713	-51.444049762220274	96175
28bb3f8a7baafa48a9fe976adcddca23f4b878df	discriminative feature extraction with deep neural networks	restricted boltzmann machines;stochastic programming backpropagation boltzmann machines feature extraction;high dimensionality;dimension reduction;low dimensional discriminative feature learning;backpropagation;feature space;fisher criterion discriminative feature extraction deep neural networks low dimensional discriminative feature learning nonlinear discriminant analysis stochastic optimization layer wise training restricted boltzmann machines modified back propagation algorithm;nonlinear discriminant analysis;discriminant analysis;modified back propagation algorithm;stochastic optimization;deep neural networks;discriminative feature extraction;feature extraction;fisher criterion;back propagation algorithm;layer wise training;boltzmann machine;stochastic programming;boltzmann machines;neural network	We propose a framework for optimizing Deep Neural Networks (DNN) with the objective of learning low-dimensional discriminative features from high-dimensional complex patterns.	deep learning;feature extraction;neural network software	André Stuhlsatz;Jens Lippel;Thomas Zielke	2010	The 2010 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2010.5596329	boltzmann machine;speech recognition;computer science;stochastic optimization;machine learning;pattern recognition;mathematics;deep learning;artificial neural network	ML	23.231632921430847	-40.89668494300879	96206
633e96ad4aee710b49337a80bb9c781cfe318966	probing the space of optimal markov logic networks for sequence labeling		Discovering relational structure between input features in sequence labeling models has shown to improve their accuracies in several problem settings. The problem of learning relational structure for sequence labeling can be posed as learning Markov Logic Networks (MLN) for sequence labeling, which we abbreviate as Markov Logic Chains (MLC). This objective in propositional space can be solved efficiently and optimally by a Hierarchical Kernels based approach, referred to as StructRELHKL, which we recently proposed. However, the applicability of StructRELHKL in complex first order settings is non-trivial and challenging. We present the challenges and possibilities for optimally and simultaneously learning the structure as well as parameters of MLCs (as against learning them separately and/or greedily). Here, we look into leveraging the StructRELHKL approach for optimizing the MLC learning steps to the extent possible. To this end, we categorize first order MLC features based on their complexity and show that complex features can be constructed from simpler ones. We define a self-contained class of features called absolute features (AF), which can be conjoined to yield complex MLC features. Our approach first generates a set of relevant AFs and then makes use of the algorithm for StructRELHKL to learn their optimal conjunctions. We demonstrate the efficiency of our approach by evaluating on a publicly available activity recognition dataset.	activity recognition;anisotropic filtering;categorization;experiment;feature learning;greedy algorithm;inductive logic programming;inductive reasoning;markov chain;markov logic network;multi-level cell;sequence labeling	Naveen Nair;Ajay Nagesh;Ganesh Ramakrishnan	2012		10.1007/978-3-642-38812-5_14	mathematical optimization;combinatorics;discrete mathematics;mathematics	ML	23.739661146304492	-47.24142677197796	96254
8d62380b31bf0173dae185eb05eba569ed6246c7	wild patterns: ten years after the rise of adversarial machine learning		Learning-based pattern classifiers, including deep networks, have shown impressive performance in several application domains, ranging from computer vision to cybersecurity. However, it has also been shown that adversarial input perturbations carefully crafted either at training or at test time can easily subvert their predictions. The vulnerability of machine learning to such wild patterns (also referred to as adversarial examples), along with the design of suitable countermeasures, have been investigated in the research field of adversarial machine learning. In this work, we provide a thorough overview of the evolution of this research area over the last ten years and beyond, starting from pioneering, earlier work on the security of non-deep learning algorithms up to more recent work aimed to understand the security properties of deep learning algorithms, in the context of computer vision and cybersecurity tasks. We report interesting connections between these apparently-different lines of work, highlighting common misconceptions related to the security evaluation of machine-learning algorithms. We review the main threat models and attacks defined to this end, and discuss the main limitations of current work, along with the corresponding future challenges towards the design of more secure learning algorithms.	adversarial machine learning;adversary (cryptography);anomaly detection;artificial intelligence;computer security;computer vision;countermeasure (computer);deep learning;formal verification;open world;pattern recognition;randomized algorithm;threat model;zero-day (computing)	Battista Biggio;Fabio Roli	2018	Pattern Recognition	10.1016/j.patcog.2018.07.023	adversarial system;artificial neural network;machine learning;deep learning;malware;computer science;biometrics;adversarial machine learning;vulnerability;cognitive neuroscience of visual object recognition;artificial intelligence	ML	18.617248548889954	-51.51290504647347	96288
ece016d98b66d21489169cc4c7bbb5973d658e79	deep collaborative filtering via marginalized denoising auto-encoder	matrix factorization;collaborative filtering;deep learning;denoising auto encoder	Collaborative filtering (CF) has been widely employed within recommender systems to solve many real-world problems. Learning effective latent factors plays the most important role in collaborative filtering. Traditional CF methods based upon matrix factorization techniques learn the latent factors from the user-item ratings and suffer from the cold start problem as well as the sparsity problem. Some improved CF methods enrich the priors on the latent factors by incorporating side information as regularization. However, the learned latent factors may not be very effective due to the sparse nature of the ratings and the side information. To tackle this problem, we learn effective latent representations via deep learning. Deep learning models have emerged as very appealing in learning effective representations in many applications. In particular, we propose a general deep architecture for CF by integrating matrix factorization with deep feature learning. We provide a natural instantiations of our architecture by combining probabilistic matrix factorization with marginalized denoising stacked auto-encoders. The combined framework leads to a parsimonious fit over the latent features as indicated by its improved performance in comparison to prior state-of-art models over four large datasets for the tasks of movie/book recommendation and response prediction.	cold start;collaborative filtering;deep learning;encoder;feature learning;latent variable;matrix regularization;noise reduction;occam's razor;recommender system;sparse matrix	Sheng Li;Jaya Kawale;Yun Fu	2015		10.1145/2806416.2806527	computer science;data science;collaborative filtering;machine learning;pattern recognition;data mining;deep learning;probabilistic latent semantic analysis;matrix decomposition	AI	22.704107029593363	-45.22679130120444	96358
398036d763fc066d7cbc0e799e569791e3482964	two neural network methods for multidimensional scaling	neural network;multidimensional scaling	Multidimensional scaling (MDS) embeds points in a Euclidean space given only dissimilarity data. Only very recently MDS has gotten some attention from neural network researchers. We propose two neural network methods for MDS and evaluate them using both artiicially generated and real data. Training uses two inputs at a time.	artificial neural network;image scaling;multidimensional scaling	Michiel C. van Wezel;Joost N. Kok;Walter A. Kosters	1997			machine learning;pattern recognition;artificial intelligence;artificial neural network;computer science;multidimensional scaling	ML	23.12590507127455	-41.62669887009391	96374
e8cb5c6ceaf0de0a0afbebdf9ebe870965f592a7	a kkt condition based ddagsvm classifier	kernel;support vector machines support vector machine classification kernel sun missiles lagrangian functions polynomials training data;support vector machines;binary classification problems;radar information;karush kuhn tucher condition;multiclass classifications;accuracy;radar information decision directed acyclic graph support vector machine ddagsvm classifier binary classification problems karush kuhn tucher condition binary classification multiclass classifications esm radar;multi class classification;radar antennas;classification algorithms;ddagsvm classifier;decision directed acyclic graph support vector machine;binary classification;esm radar;support vector machine;classification accuracy;decision trees;algorithm design and analysis;decision directed;radar;support vector machines decision trees radar	Decision directed acyclic graph support vector machine (DDAGSVM) has been proposed to extend SVM from binary classification problems to multi-class classifications. But the generalization ability is subject to the structure of DDAG. To improve the classification accuracy, a novel separability measure is defined based on Karush-Kuhn-Tucher (KKT) condition, and an improved DDAGSVM has been given. The experimental results show that this algorithm has higher generalization ability.	algorithm;binary classification;directed acyclic graph;karush–kuhn–tucker conditions;linear separability;support vector machine	Wei Sun;Zhao-hui Shi;Dongying Bai	2009	2009 2nd International Conference on Biomedical Engineering and Informatics	10.1109/BMEI.2009.5305527	statistical classification;support vector machine;computer science;machine learning;pattern recognition;data mining;mathematics	Robotics	16.760692911187036	-42.647070288335904	96437
0af3c97068638ec2b79b93ff8b3fde9bd999f153	deep approximately orthogonal nonnegative matrix factorization for clustering		Nonnegative Matrix Factorization (NMF) is a widely used technique for data representation. Inspired by the expressive power of deep learning, several NMF variants equipped with deep architectures have been proposed. However, these methods mostly use the only nonnegativity while ignoring task-specific features of data. In this paper, we propose a novel deep approximately orthogonal nonnegative matrix factorization method where both nonnegativity and orthogonality are imposed with the aim to perform a hierarchical clustering by using different level of abstractions of data. Experiment on two face image datasets showed that the proposed method achieved better clustering performance than other deep matrix factorization methods and state-of-the-art single layer NMF variants.	cluster analysis;data (computing);deep learning;hierarchical clustering;non-negative matrix factorization;sparse matrix	Yuning Qiu;Guoxu Zhou;Kan Xie	2017	CoRR		machine learning;cluster analysis;deep learning;mathematics;matrix decomposition;hierarchical clustering;external data representation;nonnegative matrix;artificial intelligence;orthogonality;non-negative matrix factorization;pattern recognition	AI	24.13022073279625	-43.265846209390794	96541
5697361f3bd45d195ccabf9c854f248b7e007b27	an online clustering algorithm that ignores outliers: application to hierarchical feature learning from sensory data	pattern clustering;hebbian rule outlier spherical clustering;neural nets;outlier;neurons clustering algorithms data models computer architecture feedforward neural networks videos radio frequency;pattern clustering data handling learning artificial intelligence neural nets;spherical clustering;hebbian rule;data handling;learning artificial intelligence;online clustering algorithm manually tunable parameters recursive layer by layer spherical clustering time varying sensory data space varying sensory data sparse feature dictionaries spatially coincident patterns temporally coincident patterns multilayered convergent neural architecture unstructured big data surveillance sensors sensory data hierarchical feature learning	Surveillance sensors are a major source of unstructured Big Data. Discovering and recognizing spatiotemporal objects (e.g., events) in such data is of paramount importance to the security and safety of facilities and individuals. Hierarchical feature learning is at the crux to the problems of discovery and recognition. We present a multilayered convergent neural architecture for storing repeating spatially and temporally coincident patterns in data at multiple levels of abstraction. The bottom-up weights in each layer are learned to encode a hierarchy of over complete and sparse feature dictionaries from space- and time-varying sensory data by recursive layer-by-layer spherical clustering. This density-based clustering algorithm ignores outliers by the use of a unique adaptive threshold in each neuron's transfer function. The model scales to full-sized high-dimensional input data and also to an arbitrary number of layers, thereby possessing the capability to capture features at any level of abstraction. It is fully-learnable with only two manually tunable parameters. The model was deployed to learn meaningful feature hierarchies from audio, images and videos which can then be used for recognition and reconstruction. Besides being online, operations in each layer of the model can be implemented in parallelized hardware, making it very efficient for real world Big Data applications.	algorithm;artificial neuron;big data;biological neuron model;bottom-up parsing;cluster analysis;computation;dictionary;encode;feature learning;feed forward (control);feedforward neural network;hebbian theory;mnist database;network architecture;neural coding;parallel computing;principle of abstraction;recursion;sensor;sparse matrix;spatiotemporal database;transfer function;walter pitts;winner-take-all (computing)	Bonny Banerjee;Jayanta K. Dutta	2013	2013 IEEE 13th International Conference on Data Mining Workshops	10.1109/ICDMW.2013.135	correlation clustering;data stream clustering;outlier;hebbian theory;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;group method of data handling;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;brown clustering;artificial neural network;clustering high-dimensional data;conceptual clustering	ML	17.077714740150718	-47.47850605705385	96572
0fb22ecc42853ced3851fe9fd1bf5ed1121e5ddc	using node relationships for hierarchical classification	vegetation standards optimization sun training proposals feature extraction;standards;training;vegetation;feature extraction;sun;optimization;proposals;node relationship hierarchical classification error propagation	Hierarchical classification is a computational efficient approach for large-scale image classification. The main challenging issue of this approach is to deal with error propagation. Irrelevant branching decision made at a parent node cannot be corrected at its child nodes in traversing the tree for classification. This paper presents a novel approach to reduce branching error at a node by taking its relative relationship into account. Given a node on the tree, we model each candidate branch by considering classification response of its child nodes, grandchild nodes and their differences with siblings. A maximum margin classifier is then applied to select the most discriminating candidate. Our proposed approach outperforms related approaches on Caltech-256, SUN-397 and ILSVRC2010-1K.	computer vision;hierarchical clustering;margin classifier;propagation of uncertainty;relevance;software propagation;tree (data structure)	Tien-Dung Mai;Thanh Duc Ngo;Duy-Dinh Le;Duc Anh Duong;Kiem Hoang;Shin'ichi Satoh	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532410	feature extraction;computer science;machine learning;pattern recognition;data mining;vegetation	Robotics	13.430813739500975	-46.74896511890463	96872
c20ded1ec1f68ea336490df056c8d082f5cc3ebc	a windowing strategy for distributed data mining optimized through gpus		Abstract This paper introduces an optimized Windowing based strategy for inducing decision trees in Distributed Data Mining scenarios. Windowing consists in selecting a sample of the available training examples (the window) to induce a decision tree with an usual algorithm, e.g., J48; finding instances not covered by this tree (counter examples) in the remaining training examples, adding them to the window to induce a new tree; and repeating until a termination criterion is met. In this way, the number of training examples required to induce the tree is reduced considerably, while maintaining the expected accuracy levels; which is paid in terms of time performance. Our proposed enhancements solve this by searching for counter examples on GPUs and further reducing their number in the window. The resulting strategy is implemented in JaCa-DDM, our agents u0026 artifacts tool for Distributed Data Mining, keeping the benefits of Windowing, while distributing the process and being faster than the traditional centralized approach, even performing similarly to Bagging and Random Forests in some cases. Experiments in data mining tasks are addressed, including a case study on pixel-based segmentation for the detection of precancerous cervical lesions on medical images.	data mining;graphics processing unit	Xavier Limón;Alejandro Guerra-Hernández;Nicandro Cruz-Ramírez;Héctor-Gabriel Acosta-Mesa;Francisco Grimaldo	2017	Pattern Recognition Letters	10.1016/j.patrec.2016.11.006	pattern recognition;pixel;artificial intelligence;data mining;decision tree;random forest;multi-agent system;c4.5 algorithm;machine learning;computer science	ML	14.08078676412012	-39.1981312593316	96900
68e596972d99635b5431357630512f1828ce90b4	weakly supervised rbm for semantic segmentation		In this paper, we propose a weakly supervised Restricted Boltzmann Machines (WRBM) approach to deal with the task of semantic segmentation with only image-level labels available. In WRBM, its hidden nodes are divided into multiple blocks, and each block corresponds to a specific label. Accordingly, semantic segmentation can be directly modeled by learning the mapping from visible layer to the hidden layer of WRBM. Specifically, based on the standard RBM, we import another two terms to make full use of image-level labels and alleviate the effect of noisy labels. First, we expect the hidden response of each superpixel is suppressed on the labels outside its parent image-level label set, and a non-image-level label suppression term is formulated to implicitly import the image-level labels as weak supervision. Second, semantic graph propagation is employed to exploit the cooccurrence between visually similar regions and labels. Besides, we deal with the problems of label imbalance and diverse backgrounds by adapting the block size to the label frequency and appending hidden response blocks corresponding to backgrounds respectively. Extensive experiments on two real-world datasets demonstrate the good performance of our approach compared with some state-of-the-art methods.	block size (cryptography);experiment;restricted boltzmann machine;software propagation;zero suppression	Yong Li;Yuhang Wang;Hanqing Lu;Songde Ma	2015			computer science;machine learning;pattern recognition	AI	23.17024818372888	-46.39851376792728	97128
b988f0ecbc53420f76b15e7c7328e16a6af41af0	tsfs: a novel algorithm for single view co-training	eigenvalues and eigenfunctions;unlabeled data;training;singular value decomposition;greedy algorithms;set theory;data mining;energy function;lower dimensional subspace;training data;accuracy;svd;feature extraction;classification algorithms;two view subspace feature splitting;greedy two view feature selection strategy;orthogonal feature;feature selection;greedy two view feature selection strategy tsfs two view subspace feature splitting single view co training svd singular value decomposition lower dimensional subspace orthogonal feature;data handling;learning artificial intelligence;single view co training;tsfs;training data supervised learning singular value decomposition semisupervised learning mutual information computer networks automation automatic control communication system control control systems;singular value decomposition data handling feature extraction greedy algorithms learning artificial intelligence set theory	"""Co-training has been validated to be effective in various applications. However, it is a challenging task to apply co-training on the data without two independent and """"good enough"""" views. In this paper, we propose a novel subspace feature set splitting algorithm, called Two-view Subspace Feature Splitting (TSFS), to make co-training better usable on single view data. We first project both labeled and unlabeled data into a lower dimensional subspace through Singular Value Decomposition (SVD), in which all features of data are orthogonal to each other. And then a greedy two-view feature selection strategy is proposed for feature set splitting. We introduce the energy function of each view to guarantee the quality of each split feature set. Experimental results well validated the effectiveness of TSFS in contrast to several recent studies on single view co-training."""	automated clearing house;co-training;display resolution;feature selection;greedy algorithm;mathematical optimization;principle of good enough;set splitting problem;singular value decomposition	Wen Zhang;Quan Zheng	2009	2009 International Joint Conference on Computational Sciences and Optimization	10.1109/CSO.2009.251	mathematical optimization;computer science;machine learning;pattern recognition;data mining;mathematics;singular value decomposition;feature selection	Vision	23.355924186075686	-42.93645155413496	97130
b18c815859457e50a793b9f778d2dcc842cb0cc5	support vector machines on large data sets: simple parallel approaches		Support Vector Machines (SVMs) are well-known for their excellent performance in the field of statistical classification. Still, the high computational cost due to the cubic runtime complexity is problematic for larger data sets. To mitigate this, Graf et al. (2005) proposed the Cascade SVM. It is a simple, stepwise procedure, in which the SVM is iteratively trained on subsets of the original data set and support vectors of resulting models are combined to create new training sets. The general idea is to bound the size of all considered training sets and therefore obtain a significant speedup. Another relevant advantage is that this approach can easily be parallelized because a number of independent models have to be fitted during each stage of the cascade. Initial experiments show that even moderate parallelization can reduce the computation time considerably, with only minor loss in accuracy. We compare the Cascade SVM to the standard SVM and a simple parallel bagging method w.r.t. both classification accuracy and training time. We also introduce a new stepwise bagging approach that exploits parallelization in a better way than the Cascade SVM and contains an adaptive stopping-time to select the number of stages for improved accuracy.	adaptive grammar;algorithm;computation;computational complexity theory;cubic function;database tuning;experiment;microsoft outlook for mac;parallel computing;speedup;statistical classification;stepwise regression;support vector machine;time complexity	Oliver Meyer;Bernd Bischl;Claus Weihs	2012		10.1007/978-3-319-01595-8_10	data mining	ML	14.024054587187297	-39.94761633962113	97136
8bd01b1c44c7ff8b1ccf5987fa85fedef49d1daa	large scale multiple kernel learning	model selection;learning algorithm;one class classification;quadratic program;support vector regression;multiple kernel learning;support vector;large scale;string kernel;machine learning;linear program;source code;computational biology;column generation	While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lankriet et al. (2004) considered conic combinations of kernel matrices for classification, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-infinite linear program that can be efficiently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classification. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice dataset from computational biology. We integrated Multiple Kernel Learning in our Machine Learning toolbox SHOGUN for which the source code is publicly available at http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun.	algorithm;central processing unit;computational biology;experiment;kernel (operating system);kernel principal component analysis;linear programming;linux;loss function;math kernel library;model selection;multiple kernel learning;multiprocessing;one-class classification;quadratic programming;quadratically constrained quadratic program;semiconductor industry;shallow parsing;shogun machine learning toolbox;sparse matrix;speedup;splice (system call);string kernel;unsupervised learning	Sören Sonnenburg;Gunnar Rätsch;Christin Schäfer;Bernhard Schölkopf	2006	Journal of Machine Learning Research		semi-supervised learning;support vector machine;least squares support vector machine;kernel method;mathematical optimization;string kernel;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;computer science;linear programming;online machine learning;machine learning;pattern recognition;graph kernel;mathematics;tree kernel;variable kernel density estimation;quadratic programming;polynomial kernel;kernel smoother	ML	21.005188336381757	-38.11359857632025	97369
3337a0c3a48215ffe8646d47ae5c6ec2da6c1bce	online bayesian dictionary learning for large datasets	variational bayes;variational techniques bayes methods dictionaries learning artificial intelligence signal processing statistical analysis;psnr;bayes methods;variational techniques;bayesian methods;online learning;beta process dictionary learning online learning variational bayes factor analysis;computational modeling;statistical analysis;vectors;factor analysis;image reconstruction;signal processing;online learning online bayesian dictionary learning large datasets data adaptive dictionary statistical model variational bayesian inference sparse coding beta process factor analysis;dictionaries;dictionary learning;learning artificial intelligence;beta process;encoding;dictionaries vectors image reconstruction bayesian methods psnr computational modeling encoding	The problem of learning a data-adaptive dictionary for a very large collection of signals is addressed. This paper proposes a statistical model and associated variational Bayesian (VB) inference for simultaneously learning the dictionary and performing sparse coding of the signals. The model builds upon beta process factor analysis (BPFA), with the number of factors automatically inferred, and posterior distributions are estimated for both the dictionary and the signals. Crucially, an online learning procedure is employed, allowing scalability to very large datasets which would be beyond the capabilities of existing batch methods. State-of-the-art performance is demonstrated by experiments with large natural images containing tens of millions of pixels.	bayesian programming;dictionary;experiment;factor analysis;machine learning;neural coding;pixel;scalability;sparse matrix;statistical model;variational principle	Lingbo Li;Jorge G. Silva;Mingyuan Zhou;Lawrence Carin	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288339	iterative reconstruction;peak signal-to-noise ratio;k-svd;bayesian probability;computer science;machine learning;signal processing;pattern recognition;data mining;mathematics;factor analysis;computational model;encoding;statistics	ML	22.739580495089243	-46.525844189442616	97538
ed3bd655c6c428c0ea0c9d505e8de09950700170	recent advances in autoencoder-based representation learning		Learning useful representations with little or no supervision is a key challenge in artificial intelligence. We provide an in-depth review of recent advances in representation learning with a focus on autoencoder-based models. To organize these results we make use of meta-priors believed useful for downstream tasks, such as disentanglement and hierarchical organization of features. In particular, we uncover three main mechanisms to enforce such properties, namely (i) regularizing the (approximate or aggregate) posterior distribution, (ii) factorizing the encoding and decoding distribution, or (iii) introducing a structured prior distribution. While there are some promising results, implicit or explicit supervision remains a key enabler and all current methods use strong inductive biases and modeling assumptions. Finally, we provide an analysis of autoencoder-based representation learning through the lens of rate-distortion theory and identify a clear tradeoff between the amount of prior knowledge available about the downstream tasks, and how useful the representation is for this task.		Michael Tschannen;Olivier Bachem;Mario Lucic	2018	CoRR			AI	22.026307714026668	-46.68629046185218	97637
3d2891950f1b76f783a9ba77b3c55b8e68b95fbe	disentangling 3d pose in a dendritic cnn for unconstrained 2d face alignment		Heatmap regression has been used for landmark localization for quite a while now. Most of the methods use a very deep stack of bottleneck modules for heatmap classification stage, followed by heatmap regression to extract the keypoints. In this paper, we present a single dendritic CNN, termed as Pose Conditioned Dendritic Convolution Neural Network (PCD-CNN), where a classification network is followed by a second and modular classification network, trained in an end to end fashion to obtain accurate landmark points. Following a Bayesian formulation, we disentangle the 3D pose of a face image explicitly by conditioning the landmark estimation on pose, making it different from multi-tasking approaches. Extensive experimentation shows that conditioning on pose reduces the localization error by making it agnostic to face pose. The proposed model can be extended to yield variable number of landmark points and hence broadening its applicability to other datasets. Instead of increasing depth or width of the network, we train the CNN efficiently with Mask-Softmax Loss and hard sample mining to achieve upto 15% reduction in error compared to state-of-the-art methods for extreme and medium pose face images from challenging datasets including AFLW, AFW, COFW and IBUG.		Amit Kumar;Rama Chellappa	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00052	machine learning;task analysis;artificial intelligence;convolutional neural network;pattern recognition;modular design;bottleneck;end-to-end principle;computer science;solid modeling	Vision	24.341054612831133	-50.794213080335126	97668
39868129a7edadf0f6a89f2ee87f423488e7a326	a greedy approach for building classification cascades	classification complexity;complexity theory;support vector machines;classification cascade building;training;classification cascade;greedy algorithms;accuracy;near to optimal cascade system greedy approach classification cascade building classification complexity ad hoc procedure;computational complexity;classification algorithms;pattern classification;classifiers;greedy approach;ad hoc procedure;cascade systems;pattern classification cascade systems computational complexity greedy algorithms;classifiers classification cascade;near to optimal cascade system;algorithm design and analysis;object detection machine learning support vector machines support vector machine classification timing resource management particle swarm optimization simulated annealing power generation greedy algorithms	Classification cascade is a well-known technique to reduce classification complexity (recognition time) while attaining high accuracy. While cascades are usually built using ad-hoc procedures, in this paper we introduce a principle way of building cascades using a greedy approach. Given a large pool of classifiers, our approach sequentially builds a near-to-optimal cascade. The approach is fully automated, fast, and scales to large number of classifiers in the pool.	greedy algorithm;hoc (programming language)	Sherif Abdelazeem	2008	2008 Seventh International Conference on Machine Learning and Applications	10.1109/ICMLA.2008.81	statistical classification;support vector machine;algorithm design;greedy algorithm;computer science;machine learning;pattern recognition;data mining;mathematics;accuracy and precision;computational complexity theory	SE	10.662890609013258	-41.06111330603736	97793
bd1c23a407b3818dfb86263bed3d0be22e2471c6	an improved kernel principal component analysis for large-scale data set	kernel principal component analysis;order statistic;kernel function;large scale;polynomial matrix	To deal with the computational and storage problem for the large-scale data set, an improved Kernel Principal Component Analysis based on 1-order and 2-order statistical quantity, is proposed By dividing the large scale data set into small subsets, we could treat 1-order and 2-order statistical quantity (mean and autocorrelation matrix) of each subset as the special computational unit A novel polynomial-matrix kernel function is also adopted to compute the similarity between the data matrices in place of vectors The proposed method can greatly reduce the size of kernel matrix, which makes its computation possible Its effectiveness is demonstrated by the experimental results on the artificial and real data set.	kernel principal component analysis	Weiya Shi;Dexian Zhang	2010		10.1007/978-3-642-13318-3_2	kernel;principal component regression;kernel regression;kernel method;mathematical optimization;polynomial matrix;kernel fisher discriminant analysis;string kernel;order statistic;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;machine learning;pattern recognition;mathematics;tree kernel;kernel;variable kernel density estimation;polynomial kernel;statistics;kernel smoother	ML	22.29379484524275	-39.52510091007676	97882
f12926e7267435d41c305f87bb4488c37c718a58	scalable and accurate online feature selection for big data	extremely high dimensionality;big data;group features;online feature selection	Feature selection is important in many big data applications. Two critical challenges closely associate with big data. First, in many big data applications, the dimensionality is extremely high, in millions, and keeps growing. Second, big data applications call for highly scalable feature selection algorithms in an online manner such that each feature can be processed in a sequential scan. We present SAOLA, a  S calable and  A ccurate  O n L ine  A pproach for feature selection in this paper. With a theoretical analysis on bounds of the pairwise correlations between features, SAOLA employs novel pairwise comparison techniques and maintains a parsimonious model over time in an online manner. Furthermore, to deal with upcoming features that arrive by groups, we extend the SAOLA algorithm, and then propose a new group-SAOLA algorithm for online group feature selection. The group-SAOLA algorithm can online maintain a set of feature groups that is sparse at the levels of both groups and individual features simultaneously. An empirical study using a series of benchmark real datasets shows that our two algorithms, SAOLA and group-SAOLA, are scalable on datasets of extremely high dimensionality and have superior performance over the state-of-the-art feature selection methods.	big data;feature selection	Kui Yu;Xindong Wu;Wei Ding;Jian Pei	2016	TKDD	10.1145/2976744	big data;computer science;machine learning;pattern recognition;data mining	ML	13.68617188891563	-38.027596705504216	97983
73133a59f458c478dd1d72670dd181e34a34a824	an experimental study of the extended nrbf regression model and its enhancement for classification problem	t technology general;mixture of experts;regression model;classification;gaussian mixture model;regression;radial basis function;function approximation;expectation maximization;information processing;bayesian ying yang	As an extension of the traditional normalized radial basis function (NRBF) model, the extended normalized RBF (ENRBF) model was proposed by Xu [RBF nets, mixture experts, and Bayesian Ying-Yang learning, Neurocomputing 19 (1998) 223-257]. In this paper, we perform a supplementary study on ENRBF with several properly designed experiments and some further theoretical discussions. It is shown that ENRBF is able to efficiently improve the learning accuracies under some circumstances. Moreover, since the ENRBF model is initially proposed for the regression and function approximation problems, a further step is taken in this work to modify the ENRBF model to deal with the classification problems. Both the original ENRBF model and the new proposed ENRBF classifier (ENRBFC) can be viewed as the special cases of the mixture-of-experts (ME) model that is discussed in Xu et al. [An alternative model for mixtures of experts, in: Advances in Neural Information Processing Systems, MIT Press, Cambridge, MA, 1995]. Experimental results show the potentials of ENRBFC compared to some other related classifiers.	experiment	Liang Ma;Abdul Wahab;Geok See Ng;Sevki S. Erdogan	2008	Neurocomputing	10.1016/j.neucom.2007.12.011	radial basis function;regression;expectation–maximization algorithm;information processing;function approximation;biological classification;computer science;artificial intelligence;machine learning;mixture model;regression analysis;statistics	ECom	18.722505656307188	-38.893025283654794	98118
ef29d5c85b70fd9dbe04a7b839fbc7d413b161e6	tganv2: efficient training of large models for video generation with multiple subsampling layers		In this paper, we propose a novel method to efficiently train a Generative Adversarial Network (GAN) on high dimensional samples. The key idea is to introduce a differentiable subsampling layer which appropriately reduces the dimensionality of intermediate feature maps in the generator during training. In general, generators require large memory and computational costs in the latter stages of the network as the feature maps become larger, though the latter stages have relatively fewer parameters than the earlier stages. It makes training large models for video generation difficult due to the limited computational resource. We solve this problem by introducing a method that gradually reduces the dimensionality of feature maps in the generator with multiple subsampling layers. We also propose a network (Temporal GAN v2) with such layers and perform video generation experiments. As a consequence, our model trained on the UCF101 dataset at 192× 192 pixels achieves an Inception Score (IS) of 24.34, which shows a significant improvement over the previous state-of-the-art score of 14.56.	chroma subsampling;computation;computational resource;dimensionality reduction;experiment;map;norm (social);pixel	Masaki Saito;Shunta Saito	2018	CoRR			Vision	23.453713597709374	-49.98534146632475	98150
3b6c06cd74d6157ed7af6359d12cadd7916c1698	semi-supervised learning combining co-training with active learning	confidence estimation;informative instances;active learning;semi supervised learning;期刊论文;co training	Co-training is a good paradigm of semi-supervised, which requires the data set to be described by two views of features. There are a notable characteristic shared by many co-training algorithm: the selected unlabeled instances should be predicted with high confidence, since a high confidence score usually implies that the corresponding prediction is correct. Unfortunately, it is not always able to improve the classification performance with these high confidence unlabeled instances. In this paper, a new semi-supervised learning algorithm was proposed combining the benefits of both co-training and active learning. The algorithm applies co-training to select the most reliable instances according to the two criterions of high confidence and nearest neighbor for boosting the classifier, also exploit the most informative instances with human annotation for improve the classification performance. Experiments on several UCI data sets and natural language processing task, which demonstrate our method achieves more significant improvement for sacrificing the same amount of human effort.	co-training;semi-supervised learning;semiconductor industry;supervised learning	Yihao Zhang;Junhao Wen;Xibin Wang;Zhuo Jiang	2014	Expert Syst. Appl.	10.1016/j.eswa.2013.09.035	semi-supervised learning;unsupervised learning;instance-based learning;computer science;machine learning;pattern recognition;data mining;active learning	ML	16.17864498391083	-40.275871143677	98177
dbea6ad7b18b66fb878a5a67e32ae424f788e513	magnitude preserving based ontology regularization algorithm		In recent years, the ontology problem has gained attention in machine learning and it has many applications in various fields. Ontology similarity computation plays a critical role in practical implementations. In ontology learning setting, one learns a real-valued score function that assigns scores to ontology vertices. Then, the similarity between vertices is weighted in terms of the difference between their corresponding scores. The purpose of this paper is to report a new ontology learning algorithm for ontology similarity measuring and ontology mapping by means of magnitude preserving. The classes of ontology loss function are considered in regularization ontology framework, the ontology function is supposed to be linear, and the gradient descent implement is presented for getting the optimal ontology function. The result data from our four simulation experiments imply that the new proposed ontology trick has high efficiency and accuracy in biology and plant science with regard to ontology similarity measure, and humanoid robotics and education science with regard to ontology mapping.	algorithm;computation;experiment;gradient descent;humanoid robot;loss function;machine learning;mathematical optimization;ontology learning;robotics;semantic integration;similarity measure;simulation;vertex (geometry)	Linli Zhu;Yu Pan;Mohammad Reza Farahani;Wei Gao	2017	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-169363	artificial intelligence;machine learning;magnitude (mathematics);mathematics;ontology;regularization (mathematics);pattern recognition	AI	15.459829676852593	-48.960347744236955	98197
b3acb6f183b5f4b651f53c0eec5cb5c805224ac1	efficient gan-based anomaly detection		Generative adversarial networks (GANs) are able to model the complex highdimensional distributions of real-world data, which suggests they could be effective for anomaly detection. However, few works have explored the use of GANs for the anomaly detection task. We leverage recently developed GAN models for anomaly detection, and achieve state-of-the-art performance on image and network intrusion datasets, while being several hundred-fold faster at test time than the only published GAN-based method.	anomaly detection;generative adversarial networks;intrusion detection system	Houssam Zenati;Chuan Sheng Foo;Bruno Lecouat;Gaurav Manek;Vijay Ramaseshan Chandrasekhar	2018	CoRR		anomaly detection;machine learning;artificial intelligence;intrusion;mathematics	ML	19.93910270018225	-49.3785255783	98243
cd46750a07d1afc6bfe59f01c725e9f2e956d626	a center sliding bayesian binary classifier adopting orthogonal polynomials	bayesian inference;orthogonal polynomials;pattern recognition;binary classification	A center sliding Bayesian design adopting orthogonal polynomials for binary pattern classification is studied in this paper. Essentially, a Bayesian weight solution is coupled with a center sliding scheme in feature space which provides an easy tuning capability for binary classification. The proposed method is compared with several state-of-the-art binary classifiers in terms of their solution forms, decision thresholds and decision boundaries. Based on the center sliding Bayesian framework, a novel orthogonal polynomial classifier is subsequently developed. The orthogonal polynomial classifier is evaluated using two representative orthogonal polynomials for feature mapping. Our experimental results show promising potential of the orthogonal polynomial classifier since it achieves both desired accuracy and computational efficiency. HighlightsProposed a center sliding Bayesian classifier with deterministic solution.Proposed to adopt orthogonal polynomials for efficient feature expansion.Developed an easy tuning mechanism for the classification framework.Provided an extensive evaluation of the proposed classifier.	binary classification;polynomial	Lei Sun;Kar-Ann Toh;Zhiping Lin	2015	Pattern Recognition	10.1016/j.patcog.2014.12.010	binary classification;mathematical optimization;computer science;machine learning;pattern recognition;mathematics;orthogonal polynomials;bayesian inference;statistics	Vision	18.21191587530432	-40.31362548468703	98382
120cbb951543ff8bbb6485a9160bf9990c844518	clustering with side information: further efforts to improve efficiency		This paper examines the issues of constrained clustering and active selection of clustering constraints in a unified approach. A fuzzy clustering method specially crafted to deal with non-spherical clusters and explicit pairwise constraints is proposed in this paper as a core clustering method. An active method for constraints selection is embedded into the core clustering method for querying beneficial constraints during clustering. The proposed approach has two major advantages relative to traditional methods. First, it considers the dependency of constraints effectiveness on the clustering algorithm by unifying both clustering and constraints selection in a uniform, principled framework. Second, a constraints selection method is embedded into the core clustering method based on the fact that constraints will be more useful if they are selected according to the current state of clustering. Experiments conducted on synthetic and real-world datasets show the effectiveness of the proposed method.		Ahmad Ali Abin	2016	Pattern Recognition Letters	10.1016/j.patrec.2016.10.013	correlation clustering;constrained clustering;mathematical optimization;data stream clustering;fuzzy clustering;flame clustering;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;cluster analysis;brown clustering;biclustering;affinity propagation;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	Vision	20.624671316897246	-42.303409655213734	98505
58f53347c401310cd61655e514a8c62e6de5a9c8	cluster-based weighted oversampling for ordinal regression (cwos-ord)	imbalanced dataset;oversampling;clustering;ordinal regression	A new oversampling method called Cluster-based Weighted Oversampling for Ordinal Regression (CWOS-Ord) is proposed for addressing ordinal regression with imbalanced datasets. Ordinal regression is a supervised approach for learning the ordinal relationship between classes. In many applications, the dataset is highly imbalanced where the instances of some classes (majority classes) occur much more frequently than instances of other classes (minority classes). This significantly degrades the classification performance as classifiers tend to strongly favor the majority classes. Standard oversampling methods can be used to improve the dataset class distribution; however, they do not consider the ordinal relationship between the classes. The proposed CWOS-Ord method aims to address this problem by first clustering minority classes and then oversampling them based on their distances and ordering relationship to other classes’ instances. The final size to oversample the clusters depends on their complexity and their initial size so that more synthetic instances are generated for more complex and smaller clusters while fewer instances are generated for less complex and larger clusters. As a secondary contribution, existing oversampling methods for two-class classification have been extended for ordinal regression. Results demonstrate that the proposed CWOS-Ord method provides significantly better results compared to other methods based on the performance measures. & 2016 Elsevier B.V. All rights reserved.	algorithm;cluster analysis;object-relational database;ordinal data;ordinal regression;oversampling;supervised learning;synthetic intelligence	Iman Nekooeimehr;Susana K. Lai-Yuen	2016	Neurocomputing	10.1016/j.neucom.2016.08.071	ordinal regression;oversampling;computer science;machine learning;pattern recognition;mathematics;cluster analysis;ordinal data;statistics	AI	13.864001496035629	-41.17673870298384	98837
fc365c00bca9f02ecb4cd37bb120304848045327	on competitive unsupervised clustering	optimal solution;feature vectors;cluster algorithm;optimisation;unsupervised clustering;image segmentation;image databases;distance measure;color;feature vectors competitive unsupervised clustering generalization objective clustering functions color images image segmentation image retrieval content based retrieval color distribution;influenza;feature vector;color distribution;color images;image colour analysis;spatial databases;clustering algorithms;cluster validity;clustering algorithms image segmentation color image retrieval content based retrieval image databases equations spatial databases influenza partitioning algorithms;content based retrieval;optimisation image segmentation image retrieval visual databases image colour analysis;generalization;competitive unsupervised clustering;partitioning algorithms;color image segmentation;visual databases;objective clustering functions;image retrieval	In this paper, we focus on the problem of unsupervised clustering wihich allows automatic setting of optinzal clusters number. We present a generalization of the competitive agglomeration clustering algorithm firstly introduced in [ I ] . This generalization is inspired by the regularization theory and suggests a new schema for using various cluster validity criteria continuously proposed in the literature. As a consequence of this generalization, we introduce new objective clustering fitnctions, and present their associated optimal solutions. We present an application of this competitive clustering schema to color image segmentation in order to pellform partial queries in the context of image retrieval by content. In this case, each pixel i s represented by the color distribution in its vicinity. Clustering algorithm has to incorporate an appropriate distance measure to compare feature vectors similarity.	algorithm;cluster analysis;color image;feature vector;image retrieval;image segmentation;pixel	Nozha Boujemaa	2000		10.1109/ICPR.2000.905417	correlation clustering;constrained clustering;computer vision;data stream clustering;feature vector;k-medians clustering;fuzzy clustering;image retrieval;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;mathematics;cluster analysis;single-linkage clustering;brown clustering;biclustering;clustering high-dimensional data;conceptual clustering	Vision	20.17963181738833	-43.6162484047479	98977
527e6af2e34c608c58365ef3b2a6587c42f28f2b	an embedded two-layer feature selection approach for microarray data analysis	image processing;indexing terms;hybrid;microarray analysis;microarray data analysis;combinatorial chemistry;filter;genetic algorithm;feature selection;classification accuracy;microarrays;wrapper	Feature selection is an important technique in dealing with application problems with large number of variables and limited training samples, such as image processing, combin atorial chemistry, and microarray analysis. Commonly employed fea ture selection strategies can be divided into filter and wrapper.In this study, we propose an embedded two-layer feature select ion approach to combining the advantages of filter and wrapper algorithms while avoiding their drawbacks. The hybrid algorithm, called GAEF (Genetic Algorithm with embedded filter) , divides the feature selection process into two stages. In th e first stage, Genetic Algorithm (GA) is employed to pre-select fea tures while in the second stage a filter selector is used to further identify a small feature subset for accurate sample classifi cation. Three benchmark microarray datasets are used to evaluate the proposed algorithm. The experimental results suggest t hat this embedded two-layer feature selection strategy is ableto improve the stability of the selection results as well as thesample classification accuracy.	benchmark (computing);embedded system;feature selection;genetic algorithm;hybrid algorithm;image processing;microarray;software release life cycle	Pengyi Yang;Zili Zhang	2009	IEEE Intelligent Informatics Bulletin		minimum redundancy feature selection;computer science;bioinformatics;pattern recognition;data mining	Robotics	10.121628511773325	-44.6955730208359	99078
08b26efae6b61b1650699cfe8c90cbe453c67884	designing fusers on the basis of discriminants - evolutionary and neural methods of training	neural networks;ensemble of classifiers;combining classifier;pattern recognition;evolutionary algorithms;fuser design;combining classifiers;evolutionary algorithm;neural network	The combining approach to classification is nowadays one of the most promising directions in pattern recognition There are many methods of decision-making that can be used by an ensemble of classifiers The most popular methods have their origins in voting, where the decision of a common classifier is a combination of individual classifiers' outputs, i.e class numbers or values of discriminants This work focuses on the problem of fuser design We propose to train a fusion block by algorithms that have their origin in neural and evolutionary approaches As we have shown in previous works, we can produce better combining classifiers than Oracle can Presented results of experiments confirm our previous observations.		Michal Wozniak;Marcin Zmyslony	2010		10.1007/978-3-642-13769-3_72	random subspace method;computer science;artificial intelligence;machine learning;evolutionary algorithm;pattern recognition;artificial neural network	ML	11.318633971297206	-41.75631991746255	99091
711a313706afd96aece9b24d8811b5b331df591f	adversarial autoencoders with constant-curvature latent manifolds		Constant-curvature Riemannian manifolds (CCMs) have been shown to be ideal embedding spaces in many application domains, as their non-Euclidean geometry can naturally account for some relevant properties of data, like hierarchy and circularity. In this work, we introduce the CCM adversarial autoencoder (CCM-AAE), a probabilistic generative model trained to represent a data distribution on a CCM. Our method works by matching the aggregated posterior of the CCM-AAE with a probability distribution defined on a CCM, so that the encoder implicitly learns to represent the data on the CCM in order to fool a discriminator network. The geometrical constraint is also explicitly imposed by jointly training the CCM-AAE to maximise the membership degree of the embeddings to the CCM. While several works in recent literature make use of either hyperspherical or hyperbolic manifolds for different learning tasks, ours is the first unified framework to seamlessly deal with CCMs of different curvatures. We show the effectiveness of our model on three different datasets characterised by non-trivial geometry: semi-supervised classification on MNIST, link prediction on two popular citation datasets, and graph-based molecule generation using the QM9 chemical database. Results show that our model compares favourably to other autoencoders based on Euclidean and non-Euclidean geometries on all tasks taken into account.		Daniele Grattarola;Lorenzo Livi;Cesare Alippi	2018	CoRR			ML	21.75287660836069	-46.49301304178532	99104
ce71de67bfefd61ea9a6c2d6e7e5e94b51021607	anytime learning of decision trees	sample size;decision tree;top down;anytime algorithm;continuous improvement;biased sampling;global optimization;decision tree induction	The majority of existing algorithms for learning decision trees are greedy---a tree is induced top-down, making locally optimal decisions at each node. In most cases, however, the constructed tree is not globally optimal. Even the few non-greedy learners cannot learn good trees when the concept is difficult. Furthermore, they require a fixed amount of time and are not able to generate a better tree if additional time is available. We introduce a framework for anytime induction of decision trees that overcomes these problems by trading computation speed for better tree quality. Our proposed family of algorithms employs a novel strategy for evaluating candidate splits. A biased sampling of the space of consistent trees rooted at an attribute is used to estimate the size of the minimal tree under that attribute, and an attribute with the smallest expected tree is selected. We present two types of anytime induction algorithms: a contract algorithm that determines the sample size on the basis of a pre-given allocation of time, and an interruptible algorithm that starts with a greedy tree and continuously improves subtrees by additional sampling. Experimental results indicate that, for several hard concepts, our proposed approach exhibits good anytime behavior and yields significantly better decision trees when more time is available.	anytime algorithm;decision tree	Saher Esmeir;Shaul Markovitch	2007	Journal of Machine Learning Research		optimal binary search tree;sample size determination;segment tree;mathematical optimization;vantage-point tree;sampling bias;decision tree learning;computer science;order statistic tree;machine learning;decision tree;top-down and bottom-up design;incremental decision tree;data mining;k-ary tree;interval tree;mathematics;fractal tree index;search tree;id3 algorithm;tree traversal;statistics;global optimization	ML	14.950461267759959	-39.585134132375416	99203
9f62aefceb988dc036146506eb5a31228be1f9b3	multilevel sensor fusion with deep learning		In the context of deep learning, this article presents an original deep network, namely CentralNet, for the fusion of information coming from different sensors. This approach is designed to efficiently and automatically balance the tradeoff between early and late fusion (i.e., between the fusion of low-level versus high-level information). More specifically, at each level of abstraction—the different levels of deep networks—unimodal representations of the data are fed to a central neural network which combines them into a common embedding. In addition, a multiobjective regularization is also introduced, helping to both optimize the central network and the unimodal networks. Experiments on four multimodal datasets not only show the state-of-the-art performance but also demonstrate that CentralNet can actually choose the best possible fusion strategy for a given problem.		Valentin Vielzeuf;Alexis Lechervy;Keiju Nakazawa;Ines M. Raschendorfer	2019	IEEE Sensors Letters		machine learning;task analysis;artificial neural network;fusion;deep learning;visualization;embedding;computer science;sensor fusion;regularization (mathematics);artificial intelligence	DB	22.000001121055984	-48.04182285153192	99286
753077a26cae8df2022ee5d18d6d7d28a2f99962	feature selection approach based on whale optimization algorithm		In this paper, a feature selection system is introduced applies the whale optimization algorithm (WOA). WOA is a recently introduced meta-heuristic optimization algorithm that mimics the natural behavior of the humpback whales. The proposed model applies the wrapper-based method to reach the optimal subset of features. This technique was applied to find the best feature subset that maximizes the accuracy of the classification while preserving the minimum number of features. The proposed model is compared with the particle swarm optimization (PSO) and genetic algorithm (GA) using a number of assessment indicators on 16 different data-sets from UCI data repository. The results demonstrate the advantage of the introduced algorithm compared to the other optimizers.	feature selection;genetic algorithm;heuristic;mathematical optimization;particle swarm optimization;research data archiving;web-oriented architecture	Marwa Sharawi;Hossam M. Zawbaa;Eid Emary	2017	2017 Ninth International Conference on Advanced Computational Intelligence (ICACI)	10.1109/ICACI.2017.7974502	multi-swarm optimization;genetic algorithm;whale;information repository;meta-optimization;feature selection;machine learning;algorithm;artificial intelligence;particle swarm optimization;computer science	Robotics	10.044584622087324	-43.02671106875622	99384
a16fa5b3fa380e040ac42d6fe7e4eb03915dc89c	fuzzy kernel associative memories with application in classification		In this paper we introduce the class of fuzzy kernel associative memories (fuzzy KAMs). Fuzzy KAMs are derived from single-step generalized exponential bidirectional fuzzy associative memories by interpreting the exponential of a fuzzy similarity measure as a kernel function. The output of a fuzzy KAM is obtained by summing the desired responses weighted by a normalized evaluation of the kernel function. Furthermore, in this paper we propose to estimate the parameter of a fuzzy KAM by maximizing the entropy of the model. We also present two approaches for pattern classification using fuzzy KAMs. Computational experiments reveal that fuzzy KAM-based classifiers are competitive with well-known classifiers from the literature.	kernel (operating system)	Aline Cristina de Souza;Marcos Eduardo Valle	2018		10.1007/978-3-319-95312-0_25	kernel (linear algebra);kernel (statistics);fuzzy logic;machine learning;normalization (statistics);similarity measure;artificial intelligence;associative property;computer science;exponential function	ML	19.574015340822317	-39.73692836387936	99449
5ce544d33fa395fa12b4c0e6905d6ce5ae5bf7d6	large scale distributed multiclass logistic regression		Multiclass logistic regression (MLR) is a fundamental machine learning model to do multiclass classification. However, it is very challenging to perform MLR on large scale data where the feature dimension is high, the number of classes is large and the number of data samples is numerous. In this paper, we build a distributed framework to support large scale multiclass logistic regression. Using stochastic gradient descent to optimize MLR, we find that the gradient matrix is computed as the outer product of two vectors. This grants us an opportunity to greatly reduce communication cost: instead of communicating the gradient matrix among machines, we can only communicate the two vectors and use them to reconstruct the gradient matrix after communication. We design a Sufficient Vector Broadcaster (SVB) to support this communication pattern. SVB synchronizes the parameter matrix of MLR by broadcasting the sufficient vectors among machines and migrates gradient matrix computation on the receiver side. SVB can reduce the communication cost from quadratic to linear without incurring any loss of correctness. We evaluate the system on the ImageNet dataset and demonstrate the efficiency and effectiveness of our distributed framework.	computation;correctness (computer science);feature model;imagenet;learning to rank;logistic regression;machine learning;multiclass classification;numerical linear algebra;outer product;stochastic gradient descent	Pengtao Xie;Jin Kyu Kim;Eric P. Xing	2014	CoRR		logistic regression;logistic model tree;multinomial logistic regression	ML	20.60034480160648	-45.759511814054875	99531
6dc638dcefeb1a9d7ef5534e26fe64571c5f842e	weighted discriminant embedding: discriminant subspace learning for imbalanced medical data classification				Tobey H. Ko;Zhonglei Gu;Yang Liu	2018				ML	19.429421929106457	-42.942920683379114	99599
05f948ccf9aae2692f1617c828f12541b12854a2	an unbalanced data hybrid-sampling algorithm based on multi-information fusion		The emergence of big data bringsnewissues and challenges for the data imbalance problem.Therefore, unbalanced data sampling technology has been a hot research topic in the field of big data.However, the existing sampling methods cannot accurately define the harmful and useless samplescontained in the originaldataset. That is, based on the single information of the dataset, a large number of actuallyharmful samples are being used for sampling, which results in a sharp decline in the identifiable performance of the sampled data. In order to overcome the problems caused by only using one kind of information, an unbalanced data hybrid-sampling algorithm based on multi-information fusion(MIFS)is presented in this paper. The MIFS combines the feature information learned by the boostingmodel with the position information of the data to define the sample, and then divides the samples into different subsets by the information contained. According to the definition of samples, the algorithm performs corresponding under-sampling and over-sampling on these subsets. Experiments show that the MIFS method can improve the performance of sampling operations and produce a high F-score and AUC against bothminority and majority classes in the classification of balanced data.	algorithm;big data;boosting (machine learning);emergence;experiment;gibbs sampling;oversampling;sampling (signal processing);total correlation;unbalanced circuit	Sijia Chen;Bin Song;Jie Guo;Xiaojiang Du	2017	GLOBECOM 2017 - 2017 IEEE Global Communications Conference	10.1109/GLOCOM.2017.8254481	fusion;sampling (statistics);algorithm;euclidean distance;data modeling;big data;algorithm design;computer science	Visualization	13.927997188312148	-41.60041092433779	99643
21c9dd68b908825e2830b206659ae6dd5c5bfc02	embed to control: a locally linear latent dynamics model for control from raw images		We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.	autoencoder;dynamical system;generative model;nonlinear system;optimal control;pixel;variational principle	Manuel Watter;Jost Tobias Springenberg;Joschka Boedecker;Martin A. Riedmiller	2015			computer vision;simulation;machine learning;mathematics	ML	22.844493274395123	-48.18332269451722	99714
363372702a65480b4ebcceccf6715eb3001771dc	combination techniques for hyperspectral image interpretation		In this work, we propose two main contributions to hyperspectral image interpretation. Firstly, while the traditional Weighted Linear Combination optimized by Genetic Algorithms (WLC-GA) [1] intends to give more discriminant power to those classification approaches contributing the most, we extend it to make a fine tuning over the class probabilities within the combination process. Then, we compare both methods (WLC-GA and its extension) with a more complex non-linear meta learning strategy called Stacked Generalization in which Support Vector Machines with Radial Basis Function kernel was used as combiner [2]. The experimental results, considering two widely used data sets, the Indian Pines and the Pavia University, are conducted in three different scenarios. Results show that both WLC-GA and its extended version achieve the best overall accuracy, and the proposed classification approach overcomes the accuracies of the other traditional ones used in this study.	diplexer;discriminant;genetic algorithm;nonlinear system;outlook.com;radial basis function kernel;software release life cycle;support vector machine	Andrey Bicalho Santos;Arnaldo de Albuquerque Araújo;Jefersson Alex dos Santos;William Robson Schwartz;David Menotti	2017	2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2017.8127789	fine-tuning;genetic algorithm;artificial intelligence;support vector machine;computer vision;linear combination;discriminant;computer science;hyperspectral imaging;data set;pattern recognition;radial basis function kernel	Vision	11.91036940349513	-42.36298412750914	99797
0e9eff51c05a84bec2928568271ebfa43032f0a2	efficiently updating and tracking the dominant kernel principal components	esquema;algorithme rapide;iterative method;traitement signal;prediccion;kernel gram matrix;metodo estadistico;kernel principal component analysis;analisis componente principal;pistage;mise a jour;batch production;methode noyau;etude experimentale;metodo descomposicion;singular value decomposition;methode decomposition;rastreo;procede discontinu;statistical method;large scale data;intelligence artificielle;eigenvalues;eigenvalues and eigenvectors;eigenvector;prinicipal components;schema;reduccion ruido;metodo iterativo;eigenvalue;actualizacion;dominating set;vector propio;algorithm;refinement method;psychometrie;decomposition method;large scale;produccion por lote;machine learning;methode statistique;methode iterative;feature extraction;signal processing;principal component analysis;noise reduction;fast algorithm;metodo nucleo;valor propio;production par lot;poursuite cible;psychometrics;reduction bruit;batch process;analyse composante principale;pattern recognition;artificial intelligence;medicine and health sciences;kernel method;procedimiento discontinuo;valeur propre;rapport signal bruit;relacion senal ruido;inteligencia artificial;reconnaissance forme;methode raffinement;target tracking;reconocimiento patron;signal to noise ratio;psicometria;sista;procesamiento senal;metodo afinamiento;estudio experimental;scheme;prediction;pca;dominant eigenspace;algoritmo rapido;vecteur propre;updating;tracking;eigenvectors;principal component	The dominant set of eigenvectors of the symmetrical kernel Gram matrix is used in many important kernel methods (like e.g. kernel Principal Component Analysis, feature approximation, denoising, compression, prediction) in the machine learning area. Yet in the case of dynamic and/or large-scale data, the batch calculation nature and computational demands of the eigenvector decomposition limit these methods in numerous applications. In this paper we present an efficient incremental approach for fast calculation of the dominant kernel eigenbasis, which allows us to track the kernel eigenspace dynamically. Experiments show that our updating scheme delivers a numerically stable and accurate approximation for eigenvalues and eigenvectors at every iteration in comparison to the batch algorithm.	algorithm;approximation;compression;computation;gramian matrix;increment;iteration;kernel (operating system);kernel method;kernel principal component analysis;machine learning;noise reduction;numerical stability;gram	Luc Hoegaerts;Lieven De Lathauwer;Ivan Goethals;Johan A. K. Suykens;Joos Vandewalle;Bart De Moor	2007	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2006.09.012	principal component regression;kernel method;string kernel;kernel embedding of distributions;radial basis function kernel;eigenvalues and eigenvectors;kernel principal component analysis;computer science;artificial intelligence;machine learning;signal processing;mathematics;psychometrics;kernel;variable kernel density estimation;polynomial kernel;algorithm;statistics;kernel smoother	ML	23.74551753242732	-38.039866256811315	99874
8a211dfa76377fb580af284527e458479cf40b89	comparative implementation of two fusion schemes for multiple complementary flir imagery classifiers	flir imagery;k nearest neighbor classifier;classifier ensemble;meta fuser;infra red;neural net;fusion of classifiers;bayes;dempster shafer;k nearest neighbor;bayes classifier	Several classifiers for forward looking infra-red imagery are designed and implemented, and their relative performance is benchmarked on 2545 images belonging to 8 different ship classes, from which 11 attributes are extracted. These are a Bayes classifier, a Dempster–Shafer classifier ensemble in which specialized classifiers are optimized to return a single ship class, a k-nearest neighbor classifier, and an optimized neural net classifier. Two different methods are then studied to fuse the results of selected subsets of these classifiers. The first method consists of using the outputs of various classifiers as inputs to a second neural net fuser. The second method consists of converting the outputs of these classifiers into masses for use in a Dempster–Shafer fuser. In both approaches, the fused classifier achieves better results than the best classifier for any given class. Crown Copyright 2004 Published by Elsevier B.V. All rights reserved.	artificial neural network;benchmark (computing);crown group;experiment;feedforward neural network;fuser (unix);k-nearest neighbors algorithm;linear classifier;naive bayes classifier;nearest neighbour algorithm;statistical classification	Pierre Valin;François Rhéaume;Claude Tremblay;Dominic Grenier;Anne-Laure Jousselme;Éloi Bossé	2006	Information Fusion	10.1016/j.inffus.2004.09.001	random subspace method;margin classifier;probabilistic classification;bayes classifier;naive bayes classifier;infrared;dempster–shafer theory;cascading classifiers;quadratic classifier;computer science;machine learning;pattern recognition;data mining;bayes' theorem;k-nearest neighbors algorithm;artificial neural network	AI	16.159979182761703	-42.98587043273485	99888
1408038f29550c49517a356cc338cca097413661	on trivial solution and scale transfer problems in graph regularized nmf	document clustering;normalized cut-like constraint;nonnegative matrix;scale transfer problem;combining graph regularization;state-of-the-art clustering method;traditional nonnegative matrix;non-negative matrix;factorization model;trivial solution;cluster assignment matrix	Combining graph regularization with nonnegative matrix (tri-)factorization (NMF) has shown great performance improvement compared with traditional nonnegative matrix (tri-)factorization models due to its ability to utilize the geometric structure of the documents and words. In this paper, we show that these models are not well-defined and suffering from trivial solution and scale transfer problems. In order to solve these common problems, we propose two models for graph regularized nonnegative matrix (tri-)factorization, which can be applied for document clustering and co-clustering respectively. In the proposed models, a Normalized Cut-like constraint is imposed on the cluster assignment matrix to make the optimization problem well-defined. We derive a multiplicative updating algorithm for the proposed models, and prove its convergence. Experiments of clustering and coclustering on benchmark text data sets demonstrate that the proposed models outperform the original models as well as many other state-of-the-art clustering methods.	algorithm;benchmark (computing);biclustering;cluster analysis;mathematical optimization;non-negative matrix factorization;optimization problem;text corpus;triangular function	Quanquan Gu;Chris H. Q. Ding;Jiawei Han	2011		10.5591/978-1-57735-516-8/IJCAI11-218	mathematical optimization;combinatorics;machine learning;mathematics	AI	23.60391918376125	-41.850828972106264	100025
d713584307cc45a9e7ac053695c5d10cf2d5fd90	kcnn: extremely-efficient hardware keypoint detection with a compact convolutional neural network		Keypoint detection algorithms are typically based on handcrafted combinations of derivative operations implemented with standard image filtering approaches. The early layers of Convolutional Neural Networks (CNNs) for image classification, whose implementation is nowadays often available within optimized hardware units, are characterized by a similar architecture. Therefore, the exploration of CNNs for keypoint detection is a promising avenue to obtain a low-latency implementation, also enabling to effectively move the computational cost of the detection to dedicated Neural Network processing units. This paper proposes a methodology for effective keypoint detection by means of an efficient CNN characterized by a compact three-layer architecture. A novel training procedure is proposed for learning values of the network parameters which allow for an approximation of the response of handcrafted detectors, showing that the proposed architecture is able to obtain results comparable with the state of the art. The capability of emulating different detectors allows to deploy a variety of algorithms to dedicated hardware by simply retraining the network. A sensor-based FPGA implementation of the introduced CNN architecture is presented, allowing latency smaller than 1 [ms].		Paolo Di Febbo;Carlo Dal Mutto;Kinh Tieu;Stefano Mattoccia	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2018.00111	architecture;computer hardware;field-programmable gate array;convolutional neural network;filter (signal processing);latency (engineering);artificial neural network;contextual image classification;computer science	Vision	23.0581408627157	-51.837472252555536	100163
49c00fca17df83e1cdeb56c348328995636059cc	ensemble selection from libraries of models	cross entropy;learning algorithm;performance metric	We present a method for constructing ensembles from libraries of thousands of models. Model libraries are generated using different learning algorithms and parameter settings. Forward stepwise selection is used to add to the ensemble the models that maximize its performance. Ensemble selection allows ensembles to be optimized to performance metric such as accuracy, cross entropy, mean precision, or ROC Area. Experiments with seven test problems and ten metrics demonstrate the benefit of ensemble selection.	algorithm;cross entropy;experiment;library (computing);machine learning;stepwise regression	Rich Caruana;Alexandru Niculescu-Mizil;Geoff Crew;Alex Ksikes	2004		10.1145/1015330.1015432	computer science;machine learning;pattern recognition;ensemble learning;cross entropy;statistics	ML	12.271468095674757	-41.63066876702173	100334
232f9bd104892b20a77820760629f6bc9c21560b	adaptive semi-unsupervised weighted oversampling (a-suwo) for imbalanced datasets	imbalanced dataset;oversampling;classification;clustering	A new oversampling method for imbalanced dataset classification is presented.It clusters the minority class and identifies borderline minority instances.Considering majority class during minority class clustering improves oversampling.Cluster size after oversampling should be dependent on its misclassification error.Generated synthetic instances improved subsequent classification. In many applications, the dataset for classification may be highly imbalanced where most of the instances in the training set may belong to one of the classes (majority class), while only a few instances are from the other class (minority class). Conventional classifiers will strongly favor the majority class and ignore the minority instances. In this paper, we present a new oversampling method called Adaptive Semi-Unsupervised Weighted Oversampling (A-SUWO) for imbalanced binary dataset classification. The proposed method clusters the minority instances using a semi-unsupervised hierarchical clustering approach and adaptively determines the size to oversample each sub-cluster using its classification complexity and cross validation. Then, the minority instances are oversampled depending on their Euclidean distance to the majority class. A-SUWO aims to identify hard-to-learn instances by considering minority instances from each sub-cluster that are closer to the borderline. It also avoids generating synthetic minority instances that overlap with the majority class by considering the majority class in the clustering and oversampling stages. Results demonstrate that the proposed method achieves significantly better results in most datasets compared with other sampling methods.	oversampling;semiconductor industry	Iman Nekooeimehr;Susana K. Lai-Yuen	2016	Expert Syst. Appl.	10.1016/j.eswa.2015.10.031	oversampling;biological classification;computer science;machine learning;pattern recognition;data mining;mathematics;cluster analysis	NLP	13.925065307145212	-41.35934275437357	100465
ef7ecbbe702f667da46cb4dd247933c7fd7e4091	support top irrelevant machine: learning similarity measures to maximize top precision for image retrieval		Top precision is one of the most popular performance measures for content-based image retrieval task, while similarity function is the most critical component of a content-based image retrieval system. However, surprisingly, there is no existing similarity function learning method proposed to maximize the top precision measure. To fill this gap, in this paper, we propose the problem of maximum top precision similarity learning, and the first solution to this problem. The similarity is a linear function of the conjunction of features of a query image and a database image. To learn the similarity function parameter matrix, we propose to maximize the top precision measures of the training queries and also minimize the squared $$\ell _2$$ ℓ 2 norm of the parameter matrix. The optimization problem is translated to a quadratic programming problem with regard to the Lagrange multipliers of top irrelevant images. The proposed algorithm, named as support top irrelevant machine, is evaluated over four benchmark image databases and is advantage over other similarity learning methods measured by top precision is shown.	algorithm;bayesian network;benchmark (computing);content-based image retrieval;database;experiment;lagrange multiplier;linear function;machine learning;mathematical optimization;optimization problem;parameter (computer programming);quadratic programming;relevance;similarity learning;similarity measure;test set;time complexity	Jiandong Meng;Yan Jiang;Xiaoliang Xu;Irfani Priananda	2016	Neural Computing and Applications	10.1007/s00521-016-2431-4	machine learning;pattern recognition;mathematics;information retrieval	Web+IR	21.53086188738494	-40.40203233825032	100520
1bfa209a6029077555ead74ef34369bfabca9698	applying a new localized generalization error model to design neural networks trained with extreme learning machine	architecture selection algorithm;extreme learning machine;principal component analysis;localized generalization error model	High accuracy and low overhead are two key features of a well-designed classifier for different classification scenarios. In this paper, we propose an improved classifier using a single-hidden layer feedforward neural network (SLFN) trained with extreme learning machine. The novel classifier first utilizes principal component analysis to reduce the feature dimension and then selects the optimal architecture of the SLFN based on a new localized generalization error model in the principal component space. Experimental and statistical results on the NSL-KDD data set demonstrate that the proposed classifier can achieve a significant performance improvement compared with previous classifiers.	artificial neural network;data mining;feedforward neural network;generalization error;overhead (computing);principal component analysis;statistical classification	Qiang Liu;Jianping Yin;Victor C. M. Leung;Jun-Hai Zhai;Zhiping Cai;Jiarun Lin	2014	Neural Computing and Applications	10.1007/s00521-014-1549-5	margin classifier;margin;quadratic classifier;computer science;artificial intelligence;machine learning;pattern recognition;bayes error rate;principal component analysis;generalization error	ML	12.947851719459209	-39.640738192821985	100891
00ecf8f2d255b5bde7cabab527e65d0d218b8e8c	robust perception through analysis by synthesis		The intriguing susceptibility of deep neural networks to minimal input perturbations suggests that the gap between human and machine perception is still large. We here argue that despite much effort, even on MNIST the most successful defenses are still far away from the robustness of human perception. We here reconsider MNIST and establish a novel defense that is inspired by the abundant feedback connections present in the human visual cortex. We suggest that this feedback plays a role in estimating the likelihood of a sensory stimulus with respect to the hidden causes inferred by the cortex and allow the brain to mute distracting patterns. We implement this analysis by synthesis idea in the form of a discriminatively fine-tuned Bayesian classifier using a set of class-conditional variational auto encoders (VAEs). To evaluate model robustness we go to great length to find maximally effective adversarial attacks, including decision-based, score-based and gradient-based attacks. The results suggest that this ansatz yields state-of-the-art robustness on MNIST against L0, L2 and L∞ perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.	adversary (cryptography);artificial neural network;deep learning;discriminative model;encoder;feedback;goto;gradient;mnist database;machine perception;mute;naive bayes classifier;speech coding;variational principle	Lukas Schott;Jonas Rauber;Wieland Brendel;Matthias Bethge	2018	CoRR		robustness (computer science);naive bayes classifier;ansatz;perception;artificial neural network;mnist database;machine perception;visual cortex;mathematics;pattern recognition;artificial intelligence	ML	19.979336934183483	-51.95033045453954	100979
4bac186ef19cd3e7cc062b989b342eee86ade1d6	classifier fusion based on weighted voting - analytical and experimental results	classifier fusion;sensor fusion decision making decision theory estimation theory pattern classification probability;computers;estimation theory;probability;classifier system;posterior probability estimator multiple classifier system classifier fusion weighted voting pattern recognition decision making discrimination function classifier error estimation;frequency estimation;discriminant function;upper bound;combining classifier;estimation;discrimination function;mutiple classifier systems;posterior probability estimator;probability distribution;linear fuser;decision theory;weighted voting;classification algorithms;pattern classification;pattern recognition;linear fuser mutiple classifier systems combining classifier;sensor fusion;classifier error estimation;voting pattern recognition decision making computer errors guidelines medical diagnosis upper bound estimation error decision theory intelligent systems;multiple classifier system;medical diagnostic imaging	The multiple classifier systems are nowadays one of the most promising directions in pattern recognition. There are many methods of decision making by the group of classifiers. The most popular are methods that have their origin in vote methods, where the decision of the common classifier is a combination of simple classifiers decisions. There exists a trend of combined classifiers, which are making their decisions basing on the discrimination function, this function is a combination of above-mentioned simple classifier functions. This work presents an attempt to estimate the classifier error, which bases on the combined discrimination function. Obtained from this estimation conclusions will serve to formulate project guidelines for this type of decision-making systems. At the end experimental results of combining algorithms are presented, both from computer generated data and for real problem from the medical diagnostics field.	algorithm;pattern recognition	Michal Wozniak	2008	2008 Eighth International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2008.216	random subspace method;statistical classification;probability distribution;margin classifier;estimation;margin;cascading classifiers;decision theory;quadratic classifier;computer science;machine learning;pattern recognition;probability;data mining;discriminant function analysis;sensor fusion;upper and lower bounds;estimation theory;weighted voting;statistics	Robotics	11.964753509641675	-38.22372939665955	101080
154e519235933c594624243775fc383b87480e6c	forensictransfer: weakly-supervised domain adaptation for forgery detection		Distinguishing fakes from real images is becoming increasingly difficult as new sophisticated image manipulation approaches come out by the day. Convolutional neural networks (CNN) show excellent performance in detecting image manipulations when they are trained on a specific forgery method. However, on examples from unseen manipulation approaches, their performance drops significantly. To address this limitation in transferability, we introduce ForensicTransfer (FT). ForensicTransfer tackles two challenges in multimedia forensics. First, we devise a learningbased forensic detector which adapts well to new domains, i.e., novel manipulation methods. Second we handle scenarios where only a handful of fake examples are available during training. To this end, we learn a forensic embedding that can be used to distinguish between real and fake imagery. We are using a new autoencoder-based architecture which enforces activations in different parts of a latent vector for the real and fake classes. Together with the constraint of correct reconstruction this ensures that the latent space keeps all the relevant information about the nature of the image. Therefore, the learned embedding acts as a form of anomaly detector; namely, an image manipulated from an unseen method will be detected as fake provided it maps sufficiently far away from the cluster of real images. Comparing with prior works, FT shows significant improvements in transferability, which we demonstrate in a series of experiments on cutting-edge benchmarks. For instance, on unseen examples, we achieve up to 80-85% in terms of accuracy compared to 50-59%, and with only a handful of seen examples, our performance already reaches around 95%.		Davide Cozzolino;Justus Thies;Andreas Rossler;Christian Riess;Matthias Nießner;Luisa Verdoliva	2018	CoRR			Vision	20.19676640408202	-51.26954789734684	101163
3d6c41716408182a2743e8a54edbf1316f048dd2	relative margin machines		In classification problems, Support Vector Machines maximize the margin of separation between two classes. While the paradigm has been successful, the solution obtained by SVMs is dominated by the directions with large data spread and biased to separate the classes by cutting along large spread directions. This article proposes a novel formulation to overcome such sensitivity and maximizes the margin relative to the spread of the data. The proposed formulation can be efficiently solved and experiments on digit datasets show drastic performance improvements over SVMs.	experiment;programming paradigm;support vector machine	Pannagadatta K. Shivaswamy;Tony Jebara	2008			artificial intelligence;machine learning;data mining	ML	15.655169779582762	-41.073750928735606	101179
f0b042dfd5979e5372f153d4b6a0aeb767f1d862	an iterative similarity based adaptation technique for cross-domain text classification		Supervised machine learning classification algorithms assume both train and test data are sampled from the same domain or distribution. However, performance of the algorithms degrade for test data from different domain. Such cross domain classification is arduous as features in the test domain may be different and absence of labeled data could further exacerbate the problem. This paper proposes an algorithm to adapt classification model by iteratively learning domain specific features from the unlabeled test data. Moreover, this adaptation transpires in a similarity aware manner by integrating similarity between domains in the adaptation setting. Cross-domain classification experiments on different datasets, including a real world dataset, demonstrate efficacy of the proposed algorithm over state-of-theart.	algorithm;document classification;domain adaptation;experiment;iteration;iterative method;machine learning;statistical classification;test data	Himanshu S. Bhatt;Deepali Semwal;Shourya Roy	2015			computer science;machine learning;pattern recognition;data mining	AI	17.73869127131002	-41.87762609768683	101192
8e54aa2e02af2efd9061f7f22fdc5a05e876eb0f	co-selection of features and instances for unsupervised rare category analysis		Rare category analysis is of key importance both in theory and in practice. Previous research work focuses on supervised rare category analysis, such as rare category detection and rare category classification. In this paper, for the first time, we address the challenge of unsupervised rare category analysis, including feature selection and rare category selection. We propose to jointly deal with the two correlated tasks so that they can benefit from each other. To this end, we design an optimization framework which is able to coselect the relevant features and the examples from the rare category (a.k.a. the minority class). It is well justified theoretically. Furthermore, we develop the Partial Augmented Lagrangian Method (PALM) to solve the optimization problem. Experimental results on both synthetic and real data sets show the effectiveness of the proposed method.	augmented lagrangian method;feature selection;mathematical optimization;optimization problem;synthetic intelligence	Jingrui He;Jaime G. Carbonell	2010		10.1137/1.9781611972801.46	computer science;artificial intelligence;pattern recognition;machine learning	ML	22.569888924747236	-43.17213517105765	101354
cee4604ef4947866f87f59df1f8b547e06c9968a	identification of multimodal human-robot interaction using combined kernels		In this paper we propose a methodology to build multiclass classifiers for the human-robot interaction problem. Our solution uses kernel-based classifiers and assumes that each data type is better represented by a different kernel. The kernels are then combined into one single kernel that uses all the dataset involved in the HRI process. The results on real data shows that our proposal is capable of obtaining lower generalization errors due to the use of specific kernels for each data type. Also, we show that our proposal is more robust when presented to noise in either or both data types.	human–robot interaction;multimodal interaction	Saith Rodríguez;Katherín Pérez;Carlos A. Quintero;Jorge López;Eyberth Rojas;Juan M. Calderón	2015		10.1007/978-3-319-28031-8_23	kernel (linear algebra);data type;artificial intelligence;human–robot interaction;pattern recognition;computer science	Robotics	17.25507819362783	-45.09066868218404	101581
4c99f0987fce84d3f3cd3bab094f784ccecc2397	dimensionality reduction by minimal distance maximization	optimisation;covariance matrix face principal component analysis support vector machines pattern recognition optimization iris;support vector machines;minimal distance maximization;discriminant analysis;dimensionality reduction;statistical analysis;discriminant analysis method;principal component analysis;statistical analysis optimisation;pattern recognition;face;optimization;discriminant analysis method dimensionality reduction minimal distance maximization;iris;dimensional reduction;covariance matrix	"""In this paper, we propose a novel discriminant analysis method, called Minimal Distance Maximization (MDM). In contrast to the traditional LDA, which actually maximizes the average divergence among classes, MDM attempts to find a low-dimensional subspace that maximizes the minimal (worst-case) divergence among classes. This ``minimal"""" setting solves the problem caused by the ``average"""" setting of LDA that tends to merge similar classes with smaller divergence when used for multi-class data. Furthermore, we elegantly formulate the worst-case problem as a convex problem, making the algorithm solvable for larger data sets. Experimental results demonstrate the advantages of our proposed method against five other competitive approaches on one synthetic and six real-life data sets."""	a library for support vector machines;best, worst and average case;convex optimization;decision problem;dimensionality reduction;expectation–maximization algorithm;linear discriminant analysis;local-density approximation;master data management;real life;synthetic intelligence	Bo Xu;Kaizhu Huang;Cheng-Lin Liu	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.144	face;support vector machine;covariance matrix;computer science;machine learning;pattern recognition;mathematics;linear discriminant analysis;statistics;dimensionality reduction;principal component analysis	DB	24.056792610683317	-40.36086905283725	101592
b9d8173c4a3eba884bc812540fd3e4bc39cb603a	sketch recognition based on manifold learning	sketch recognition;manifold learning	Current feature-based methods for sketch recognition systems rely on human-selected features. Certain machine learning techniques have been found to be good nonlinear features extractors. In this paper, we apply a manifold learning method, kernel Isomap, with a new algorithm for multi-stroke sketch recognition, which significantly outperforms the standard featurebased techniques. INTRODUCTION Sketching is a natural form of input for many domains, such as drawing mathematical symbols, graphs, binary trees, finite state machines, electrical circuit diagrams, military course of action diagrams and many more. As in other recognition domains (such as image and speech), feature selection is crucial for efficient and qualified performance in sketch recognition. Previous research has provided several suggestions for good feature sets. Rubine suggested 13 features based on drawing-style characteristics and time (Rubine 1991) and Long suggested 22 features modifying Rubine’s (Longet al. 2000). Such hand-selected feature sets were well designed, but they are based on manual entry of methods to extract them, which becomes tiresome. Moreover, feature design is sensitive to problems such as jitters. More interestingly, machine learning research in other domains have shown that computers are able to select their own features and often times provide clear advantages in classification. To our knowledge, however, manifold learning has not yet been applied to sketch recognition despite evidence that has shown manifold learning methods to be good nonlinear feature extractors. Manifold learning is an effective method for representation that works by recovering meaningful low dimensional structures hidden in high-dimensional data (Seung & Lee 2000). In this work, we apply the kernel Isomap manifold learning method (Choi & Choi 2007) to classify sketch data, because it has a projection property which provides the ability to project (map) new test data into (onto) the same feature space as training data. Kernel Isomap requires a dissimilarity matrix to find a low-dimensional mapping from which it produces a feature set. We introduce Copyright c © 2008,Associationfor the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. a new algorithm to measure dissimilarity distance between shapes that is based on both spatial and temporal information. We also show how this algorithm can be modified to accommodate for shapes drawn with multiple strokes. We compare our method with the widely used Rubine featurebased method for recognizing shapes (Rubine 1991). IMPLEMENTATION In order to use manifold learning for recognition, we need a projection property (or generalization property). A projection property provides the ability to project new test data into the same feature space as training data. In contrast to Isomap or LLE, kernel Isomap provides the projection property itself. Thus, we chose to use the kernel Isomap manifold learning technique. See (Choi & Choi 2007) for the algorithm and the projection property. Dissimilarity from Sketch Data To apply kernel Isomap to sketch classification, we need to use the time structure in data, as well as, the generalization property. In a kernel Isomap, what we need is not the data points, but a dissimilarity matrix. The first step in our algorithm is to scale each character to be the same width and height. Then, we need a consistent (and large) number of points in each sketch. Thus, we interpolate points between any two consecutive points that are adjacent in time, but far away in distance to ensure that each gesture has the same number of points. Some characters such as ‘4’ and ‘5’ are drawn using multiple strokes. We simply connect and interpolate between the last point of the previous stroke and the first point of the next stroke. To calculate the dissimilarity matrix, we calculate the sum of squared distance between two points of the same order in each sketch. The dissimilarity between ith sketch data and jth sketch data, Dij , is then given by Dij = √	algorithm;artificial intelligence;binary tree;choi–williams distribution function;computer;data point;diagram;distance matrix;effective method;feature selection;feature vector;finite-state machine;interpolation;isomap;machine learning;nonlinear dimensionality reduction;nonlinear system;sketch recognition;test data	Heeyoul Choi;Tracy Anne Hammond	2008			computer vision;feature;computer science;artificial intelligence;machine learning;pattern recognition;nonlinear dimensionality reduction;sketch recognition;manifold alignment	AI	16.734004892295477	-48.270633377644735	101743
1bf6d4b31fa26aa9e0c2b759e6bee9ec718dcae7	exploiting high-order information in heterogeneous multi-task feature learning		Multi-task feature learning (MTFL) aims to improve the generalization performance of multiple related learning tasks by sharing features between them. It has been successfully applied to many pattern recognition and biometric prediction problems. Most of current MTFL methods assume that different tasks exploit the same feature representation, and thus are not applicable to the scenarios where data are drawn from heterogeneous domains. Existing heterogeneous transfer learning (including multi-task learning) approaches handle multiple heterogeneous domains by usually learning feature transformations across different domains, but they ignore the high-order statistics (correlation information) which can only be discovered by simultaneously exploring all domains. We therefore develop a tensor based heterogeneous MTFL (THMTFL) framework to exploit such high-order information. Specifically, feature transformations of all domains are learned together, and finally used to derive new representations. A connection between all domains is built by using the transformations to project the pre-learned predictive structures of different domains into a common subspace, and minimizing their divergence in the subspace. By exploring the high-order information, the proposed THMTFL can obtain more reliable feature transformations compared with existing heterogeneous transfer learning approaches. Extensive experiments on both text categorization and social image annotation demonstrate superiority of the proposed method.	algorithm;angular defect;automatic image annotation;biometrics;categorization;computer multitasking;document classification;experiment;feature learning;mathematical optimization;multi-task learning;nonlinear system;pattern recognition;protein structure prediction	Yong Luo;Dacheng Tao;Yonggang Wen	2017		10.24963/ijcai.2017/340	artificial intelligence;machine learning;computer science;feature learning	AI	23.608193976774405	-43.939380984111544	102172
61926102f3c4113c1285dddddb5e6404927f7abf	mutual information and intrinsic dimensionality for feature selection	normalized feature space;support vector machines;receiver operator characteristic;support vector machines data analysis feature extraction maximum likelihood estimation pattern classification principal component analysis;receiver operating characteristics analysis;maximum likelihood estimation;feature space;pattern recognition feature selection mutual information intrinsic dimensionality;receiver operating characteristics analysis mutual information intrinsic dimensionality feature selection normalized feature space minimal redundancy maximal relevance criterion data property principal component analysis maximum likelihood estimator support vector machine data classification;data analysis;maximum likelihood estimate;maximum likelihood estimator;feature extraction;principal component analysis;roc analysis;classification algorithms;maximum likelihood estimation principal component analysis mutual information support vector machines classification algorithms breast cancer;pattern classification;pattern recognition;classification error;mutual information;feature selection;support vector machine;intrinsic dimensionality;minimal redundancy maximal relevance criterion;data classification;breast cancer;data property	In this article we proposed a feature selection method based on mutual information (MI) and intrinsic dimensionality (ID) estimators. First, MI ranks the normalized feature space in accordance to minimal-redundancy-maximal-relevance (mRMR) criterion. Next, ID estimates the minimum number of features to represent the observed properties of the data. Two techniques of ID were tested: principal component analysis (PCA) and maximum likelihood estimator (MLE). Support vector machine (SVM) was used to classify five medical datasets. Receiver operating characteristics (ROC) analysis evaluated the classification performance before and after feature selection. Results showed that MI and ID are effective techniques for feature selection to reduce the classification error.	feature selection;feature vector;maximal set;mutual information;pattern recognition;principal component analysis;receiver operating characteristic;relevance;support vector machine	Walter Gómez;Lorenzo Leija;A. Díaz-Pérez	2010	2010 7th International Conference on Electrical Engineering Computing Science and Automatic Control	10.1109/ICEEE.2010.5608600	support vector machine;computer science;machine learning;pattern recognition;mathematics;maximum likelihood;feature selection;statistics;dimensionality reduction	ML	10.524053944490426	-46.57265548543511	102253
d4571bd041bbae52087eb0076f4a8175ff365b3c	multiple kernel multivariate performance learning using cutting plane algorithm	loss measurement;kernel;support vector machines;training;pattern classification iterative methods learning artificial intelligence minimisation;kernel hilbert space optimization training loss measurement support vector machines iterative methods;hilbert space;iterative methods;optimization;iterative algorithm multiple kernel multivariate performance learning cutting plane algorithm multikernel classifier learning algorithm nonlinear multivariate classifier nonsmoonth multivariate classifier kernel function selection kernel parameter tuning optimal kernel weighted linear combination;cutting plane algorithm pattern recognition multiple kernel multivariate performance measures	In this paper, we propose a multi-kernel classifier learning algorithm to optimize a given nonlinear and nonsmoonth multivariate classifier performance measure. Moreover, to solve the problem of kernel function selection and kernel parameter tuning, we proposed to construct an optimal kernel by weighted linear combination of some candidate kernels. The learning of the classifier parameter and the kernel weight are unified in a single objective function considering to minimize the upper boundary of the given multivariate performance measure. The objective function is optimized with regard to classifier parameter and kernel weight alternately in an iterative algorithm by using cutting plane algorithm. The developed algorithm is evaluated on two different pattern classification methods with regard to various multivariate performance measure optimization problems. The experiment results show the proposed algorithm outperforms the competing methods.	algorithm;big data;bioinformatics;cutting-plane method;integer programming;integrated circuit design;iterative method;kernel (operating system);kernel method;linear function;mathematical optimization;network security;nonlinear system;optimization problem	Jingbin Wang;Haoxiang Wang;Yihua Zhou;Nancy McDonald	2015	2015 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2015.327	kernel;principal component regression;kernel regression;support vector machine;least squares support vector machine;kernel method;mathematical optimization;kernel;kernel embedding of distributions;radial basis function kernel;kernel adaptive filter;quadratic classifier;kernel principal component analysis;machine learning;pattern recognition;graph kernel;multivariate kernel density estimation;mathematics;iterative method;tree kernel;variable kernel density estimation;polynomial kernel;kernel smoother;hilbert space	Robotics	21.443448994419228	-39.69446787382932	102309
1d0651c31ff4c54646a7e187c3544b502e4df630	sub-domain adaptation learning methodology	support vector machines;multi label classification;maximum mean discrepancy;projected maximum local weighted mean discrepancy;local weighted mean	Regarded as global methods, Maximum Mean Discrepancy (MMD) based transfer learning frameworks only reflect the global distribution discrepancy and structural differences between domains; they can reflect neither the inner local distribution discrepancy nor the structural differences between domains. To address this problem, a novel transfer learning framework with local learning ability, a Sub-domain Adaptation Learning Framework (SDAL), is proposed. In this framework, a Projected Maximum Local Weighted Mean Discrepancy (PMLMD) is constructed by integrating the theory and method of Local Weighted Mean (LWM) into MMD. PMLMD reflects global distribution discrepancy between domains through accumulating local distribution discrepancies between the local sub-domains in domains. In particular, we formulate in theory that PMLMD is one of the generalized measures of MMD. On the basis of SDAL, two novel methods are proposed by using Multi-label Classifiers (MLC) and Support Vector Machine (SVM). Finally, tests on artificial data sets, high dimensional text data sets and face data sets show the SDAL-based transfer learning methods are superior to or at least comparable with benchmarking methods.	domain adaptation	Jun Gao;Rong Huang;Han-Xiong Li	2015	Inf. Sci.	10.1016/j.ins.2014.11.041	support vector machine;mathematical optimization;computer science;machine learning;pattern recognition;mathematics	AI	21.77754322869989	-43.120599767109304	102546
2055f44a4b358ff55b131f2b8ea630f4242d16c1	generative poisoning attack method against neural networks		Poisoning attack is identified as a severe security threat to machine learning algorithms. In many applications, for example, deep neural network (DNN) models collect public data as the inputs to perform re-training, where the input data can be poisoned. Although poisoning attack against support vector machines (SVM) has been extensively studied before, there is still very limited knowledge about how such attack can be implemented on neural networks (NN), especially DNNs. In this work, we first examine the possibility of applying traditional gradient-based method (named as the direct gradient method) to generate poisoned data against NNs by leveraging the gradient of the target model w.r.t. the normal data. We then propose a generative method to accelerate the generation rate of the poisoned data: an auto-encoder (generator) used to generate poisoned data is updated by a reward function of the loss, and the target NN model (discriminator) receives the poisoned data to calculate the loss w.r.t. the normal data. Our experiment results show that the generative method can speed up the poisoned data generation rate by up to 239.38× compared with the direct gradient method, with slightly lower model accuracy degradation. A countermeasure is also designed to detect such poisoning attack methods by checking the loss of the target model.	algorithm;autoencoder;big data;deep learning;discriminator;elegant degradation;encoder;experiment;gradient method;information privacy;mnist database;machine learning;neural networks;reinforcement learning;speedup;support vector machine;gift	Chaofei Yang;Qing Wu;Hai Li;Yiran Chen	2017	CoRR		machine learning;data mining;computer security	ML	19.11655289266239	-50.80529010716429	102942
d436af0ff6041f75f95fc682bdd00868a01f7d13	attribute weighting: how and when does it work for bayesian network classification	cll;estimation theory;attribute weighting cll conditional log likelihood roc curve ranking aode averaged one dependence estimation hnb hidden naive bayes naive bayes classification bnc graphical model bayesian network classification;annealing;attribute weighting;niobium bayes methods training mutual information estimation educational institutions annealing;bayes methods;training;niobium;bnc;averaged one dependence estimation;pattern classification bayes methods estimation theory;estimation;naive bayes classification;bayesian network classification;hnb;pattern classification;roc curve ranking;graphical model;mutual information;aode;hidden naive bayes;conditional log likelihood;conference proceeding	A Bayesian Network (BN) is a graphical model which can be used to represent conditional dependency between random variables, such as diseases and symptoms. A Bayesian Network Classifier (BNC) uses BN to characterize the relationships between attributes and the class labels, where a simplified approach is to employ a conditional independence assumption between attributes and the corresponding class labels, i.e., the Naive Bayes (NB) classification model. One major approach to mitigate NB's primary weakness (the conditional independence assumption) is the attribute weighting, and this type of approach has been proved to be effective for NB with simple structure. However, for weighted BNCs involving complex structures, in which attribute weighting is embedded into the model, there is no existing study on whether the weighting will work for complex BNCs and how effective it will impact on the learning of a given task. In this paper, we first survey several complex structure models for BNCs, and then carry out experimental studies to investigate the effectiveness of the attribute weighting strategies for complex BNCs, with a focus on Hidden Naive Bayes (HNB) and Averaged One-Dependence Estimation (AODE). Our studies use classification accuracy (ACC), area under the ROC curve ranking (AUC), and conditional log likelihood (CLL), as the performance metrics. Experiments and comparisons on 36 benchmark data sets demonstrate that attribute weighting technologies just slightly outperforms unweighted complex BNCs with respect to the ACC and AUC, but significant improvement can be observed using CLL.	averaged one-dependence estimators;bayesian network;benchmark (computing);embedded system;graphical model;naive bayes classifier;receiver operating characteristic	Jia Wu;Zhihua Cai;Shirui Pan;Xingquan Zhu;Chengqi Zhang	2014	2014 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2014.6889536	niobium;estimation;annealing;variable-order bayesian network;machine learning;pattern recognition;mathematics;graphical model;mutual information;estimation theory;statistics	AI	11.807948031619466	-38.92692230816365	103087
dbcf9b96ae3c5e6122176bd4bfdfa9f785c26d92	semi-supervised dimensionality reduction for analyzing high-dimensional data with constraints	kernel methods;semi supervised learning;dimensionality reduction;clustering;generalized eigenproblem;linear transformation;framework	In this paper, we present a novel semi-supervised dimensionality reduction technique to address the problems of inefficient learning and costly computation in coping with high-dimensional data. Our method named the Dual Subspace Projections (DSP) embeds high-dimensional data in an optimal low-dimensional space, which is learned with a few user-supplied constraints and the structure of input data. The method projects data into two different subspaces respectively the kernel space and the original input space. Each projection is designed to enforce one type of constraints and projections in the two subspaces interact with each other to satisfy constraints maximally and preserve the intrinsic data structure. Compared to existing techniques, our method has the following advantages: (1) It benefits from constraints even when only a few are available; (2) It is robust and free from overfitting; and (3) It handles nonlinearly separable data, but learns a linear data transformation. As a conclusion, our method can be easily generalized to new data points and is efficient in dealing with large datasets. An empirical study using real data validates our claims so that significant improvements in learning accuracy can be obtained after the DSP based dimensionality reduction is applied to high-dimensional data.	cluster analysis;computation;constrained clustering;constraint (mathematics);data point;data structure;dimensionality reduction;embedded system;k-means clustering;kernel (linear algebra);nonlinear system;overfitting;semi-supervised learning;semiconductor industry;transformation matrix;user space	Su Yan;Sofien Bouaziz;Dongwon Lee;Jesse L. Barlow	2012	Neurocomputing	10.1016/j.neucom.2011.03.057	semi-supervised learning;kernel method;mathematical optimization;computer science;software framework;machine learning;pattern recognition;mathematics;linear map;cluster analysis;statistics;dimensionality reduction	ML	23.73170520481823	-42.4891324907407	103152
85c33b2c6476b40a9c466307be76679254fbf140	support of contextual classifier ensembles design		An idea of contextual classifier ensembles extends the application possibility of additional measures of quality of base and ensemble classifiers in the process of contextual ensembles design. These measures besides the obvious classifier accuracy and diversity/similarity take under consideration the complexity, interpretability and significance. The complexity (the number of used measures and multi level measure structure), the diversity of the scales of used measures and the necessity of the fusion of different measures to one assessment value are the reasons for user support in contextual classifier ensembles design using fuzzy logic and multi criteria analysis. The aim for this paper is an idea of the framework of the process of contextual ensemble design.	experience;experiment;fuzzy logic;multi-storey car park;naive bayes classifier;problem solving	Janina Anna Jakubczyc;Mieczyslaw L. Owoc	2015	2015 Federated Conference on Computer Science and Information Systems (FedCSIS)	10.15439/2015F353	machine learning;pattern recognition	Vision	10.47578535418803	-39.734157861271974	103178
c76df4fa86312abe50393ae97641f1b686acc878	forming ensembles of soft one-class classifiers with weighted bagging	classifier ensemble;one-class classification;bagging;wagging;ensemble pruning;soft classifier	For many real-life problems obtaining representative examples from a given class is relatively easy, while for the remaining ones are difficult, or even impossible. However, we would still like to construct a pattern classifier that could distinguish between the known and unknown cases. In such cases we are dealing with one-class classification, or learning in the absence of counterexamples. Such recognition systems must display a high robustness to new, unseen objects that may belong to an unknown class. That is why ensemble learning has become an attractive perspective in this field. In our work, we propose a novel one-class ensemble classifier, based on weighted Bagging. Wagging method is used to obtain randomized weights and utilize them directly in the process of training Weighted One-Class Support Vector Machines. This introduces a diversity into the pool of one-class classifiers and extends the competence of formed ensemble. Additionally, to discard similar or weak classifiers we propose to add a clustering-based pruning procedure to our ensemble. It works on the basis of similarities between weights used by each base model and detecting groups of similar predictors. This allows us to reduce the number of classifiers in the pool by selecting a single representative for each cluster. Experimental analysis, carried out on a number of benchmarks and backed-up with statistical analysis proves that the proposed method can outperform state-of-the-art ensembles dedicated to one-class classification.	bayesian information criterion;benchmark (computing);cluster analysis;decision boundary;ensemble learning;one-class classification;optical carrier transmission rates;randomized algorithm;real life;relevance;robustness (computer science);sampling (signal processing);sensor;statistical classification;support vector machine	Bartosz Krawczyk	2015	New Generation Computing	10.1007/s00354-015-0406-0	random subspace method;cascading classifiers;artificial intelligence;machine learning;pattern recognition;data mining	AI	14.705294681969004	-41.356735208632976	103403
16c527f74f029be1d6526175b7cc2d938f7e7c69	deep learning for multi-label classification		In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multilabel methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature.	deep learning;feature vector;interdependence;multi-label classification;redundancy (engineering);restricted boltzmann machine;scalability	Jesse Read;Fernando Pérez-Cruz	2014	CoRR		computer science;artificial intelligence;machine learning;data mining	AI	21.846664388266227	-48.82967343615739	103426
12715108dedad65b44f18948730e7beb7cfbfc51	data subset selection for efficient svm training		Training a support vector machine (SVM) on large data sets is a computationally intensive task. In this paper, we study the problem of selecting a subset of data for training the SVM classifier under requirement that the loss of performance due to training data reduction is low. A function quantifying suitability of a selected subset is proposed, and a greedy algorithm for solving the subset selection problem is introduced. The algorithm is evaluated on hand digit recognition and other binary classification tasks, and its performance is compared to stratified sampling methods.	algorithmic efficiency;binary classification;greedy algorithm;sampling (signal processing);stratified sampling;support vector machine	Sara Mourad;Ahmed H. Tewfik;Haris Vikalo	2017	2017 25th European Signal Processing Conference (EUSIPCO)	10.23919/EUSIPCO.2017.8081324	support vector machine;binary classification;stratified sampling;approximation algorithm;classifier (linguistics);machine learning;data set;linear programming;computer science;artificial intelligence;pattern recognition;greedy algorithm	ML	12.254350419724172	-41.463070968784926	103440
7d1a028b5e40bea0a0705b1163a3a8d888ef973b	a sparse-response deep belief network based on rate distortion theory	kullback leibler divergence;unsupervised feature learning;deep belief network;journal;rate distortion theory;期刊论文;information entropy	Deep belief networks (DBNs) are currently the dominant technique for modeling the architectural depth of brain, and can be trained efficiently in a greedy layer-wise unsupervised learning manner. However, DBNs without a narrow hidden bottleneck typically produce redundant, continuous-valued codes and unstructured weight patterns. Taking inspiration from rate distortion (RD) theory, which encodes original data using as few bits as possible, we introduce in this paper a variant of DBN, referred to as sparse-response DBN (SR-DBN). In this approach, Kullback–Leibler divergence between the distribution of data and the equilibrium distribution defined by the building block of DBN is considered as a distortion function, and the sparse response regularization induced by L1-norm of codes is used to achieve a small code rate. Several experiments by extracting features from different scale image datasets show that our approach SR-DBN learns codes with small rate, extracts features at multiple levels of abstraction mimicking computations in the cortical hierarchy, and obtains more discriminative representation than PCA and several basic algorithms of DBNs. & 2014 Elsevier Ltd. All rights reserved.	bayesian network;code rate;computation;deep belief network;distortion;experiment;greedy algorithm;kullback–leibler divergence;markov chain;principle of abstraction;rate–distortion theory;ruby document format;sparse matrix;taxicab geometry;unsupervised learning	Nannan Ji;Jiang-She Zhang;Chun-Xia Zhang	2014	Pattern Recognition	10.1016/j.patcog.2014.03.025	computer vision;rate–distortion theory;theoretical computer science;machine learning;pattern recognition;mathematics;kullback–leibler divergence;deep belief network;statistics;entropy	ML	22.319270607173824	-48.52157311683519	103463
0feeaed3322239999a9487beeaee0aee2ac716e7	distance metric learning for conditional anomaly detection	anomaly detection;distance metric	Anomaly detection methods can be very useful in identifying unusual or interesting patterns in data. A recently proposed conditional anomaly detection framework extends anomaly detection to the problem of identifying anomalous patterns on a subset of attributes in the data. The anomaly always depends (is conditioned) on the value of remaining attributes. The work presented in this paper focuses on instance-based methods for detecting conditional anomalies. The methods depend heavily on the distance metric that lets us identify examples in the dataset that are most critical for detecting the anomaly. To optimize the performance of the anomaly detection methods we explore and study metric learning methods. We evaluate the quality of our methods on the Pneumonia PORT dataset by detecting unusual admission decisions for patients with the community-acquired pneumonia. The results of our metric learning methods show an improved detection performance over standard distance metrics, which is very promising for building automated anomaly detection systems for variety of intelligent monitoring applications.	anomaly detection;error detection and correction;gene distance metric;patients;sensor;silo (dataset);subgroup;subject-matter expert;community acquired pneumonia	Michal Valko;Milos Hauskrecht	2008	Proceedings of the ... International Florida AI Research Society Conference. Florida AI Research Symposium		anomaly detection;text mining;metric;computer science;artificial intelligence;data science;machine learning;data mining;statistics	ML	10.619818024540118	-47.79189044999331	103552
7e58d76bd9ebcb1a5ee803d8958b12f21a91a9a1	confirming robustness of fuzzy support vector machine via ξ-α bound	generalization performance;bound;ξ α;ζ α;performance estimation;fuzzy support vector machine;robustness	Robustness is an important characteristic of a classifier. With higher robustness, a classifier can resist much more against the noise of a contaminated dataset which is usually occurred in the real-world applications. With its excellence in robustness, a fuzzy support vector machine (fuzzy SVM) developed by Lin and Wang deserves the most attention among varieties of support vector machines. The main goal of this paper is to gain the Joachims' ?-α bound of the fuzzy SVM. Based on the decoupled α and ? terms in its expression, the ?-α based estimation is particularly suitable for robustness analysis of the fuzzy SVM. The study re-examines the theory of the fuzzy SVM having an additional fuzzy input si in details with the ?-α estimation, and conducts a relatively contracted condition for upper bounding the corresponding performance. The bound confirms the crucial robustness which the fuzzy SVM can achieve analytically, and would be helpful for the works such as model selection or model adaption for further applications.	support vector machine	Chan-Yun Yang;Jianjun Wang;Jui-Jen Chou;Feng-Li Lian	2015	Neurocomputing	10.1016/j.neucom.2015.03.046	defuzzification;computer science;fuzzy number;machine learning;pattern recognition;data mining;mathematics;robustness	ML	11.949192560814001	-38.63399150531022	103635
4703f9ced7be37820a6973304d37209c13042eee	incorporating a priori knowledge from detractor points into support vector classification	support vector machines;support vector;public domain;a priori knowledge;stock price;support vector classification;support vector machine	In this article, we extend the idea of a priori knowledge in the form of detractor points presented recently for Support Vector Classification. We show that detractor points can belong to the new type of support vectors - training samples which lie outside a margin bounded region. We present the new application for a priori knowledge from detractor points - improving generalization performance of Support Vector Classification while reducing a complexity of a model by removing a bunch of support vectors. The experiments show that indeed the new type of a priori knowledge improves generalization performance of reduced models. The tests were performed on selected classification data sets, and on stock price data from public domain repositories.	support vector machine	Marcin Orchel	2011		10.1007/978-3-642-20267-4_35	support vector machine;computer science;machine learning;pattern recognition;data mining;mathematics	ML	20.861437831244192	-39.883725184160895	103737
921a9895bf5c20a013fb477b0fab8c1bcfad952e	the als-svm based multi-task learning classifiers	multi-task learning;support vector machine;asymmetric least squared loss	The multi-task learning support vector machines (SVMs) have recently attracted considerable attention since the conventional single task learning ones usually ignore the relatedness among multiple related tasks and train them separately. Different from the single task learning, the multi-task learning methods can capture the correlation among tasks and achieve an improved performance by training all tasks simultaneously. In this paper, we make two assumptions on the relatedness among tasks. One is that the normal vectors of the related tasks share a certain common parameter value; the other is that the models of the related tasks are close enough and share a common model. Under these assumptions, we propose two multi-task learning methods, named as MTL-aLS-SVM I and MTL-aLS-SVM II respectively, for binary classification by taking full advantages of multi-task learning and the asymmetric least squared loss. MTL-aLS-SVM I seeks for a trade-off between the maximal expectile distance for each task model and the closeness of each task model to the averaged model. MTL-aLS-SVM II can use different kernel functions for different tasks, and it is an extension of the MTL-aLS-SVM I. Both of them can be easily implemented by solving quadratic programming. In addition, we develop their special cases which include L2-SVM based multi-task learning methods (MTL-L2-SVM I and MTL-L2-SVM II) and the least squares SVM (LS-SVM) based multi-task learning methods (MTL-LS-SVM I and MTL-LS-SVM II). Although the MTL-L2-SVM II and MTL-LS-SVM II appear in the form of special cases, they are firstly proposed in this paper. The experimental results show that the proposed methods are very encouraging.	binary classification;boyce–codd normal form;centrality;computer multitasking;experiment;inductive reasoning;least squares;maximal set;mean squared error;monoidal t-norm logic;multi-task learning;personally identifiable information;programming paradigm;quadratic programming;support vector machine	Liyun Lu;Qiang Lin;Huimin Pei;Ping Zhong	2017	Applied Intelligence	10.1007/s10489-017-1087-9	support vector machine;kernel (statistics);artificial intelligence;machine learning;semi-supervised learning;pattern recognition;computer science;random subspace method;cascading classifiers;binary classification;quadratic programming;multi-task learning	ML	22.49095795851121	-42.69792845613654	104150
4c500c84e16e5ebb50b33f9bcff36854e5131c16	all-transfer learning for deep neural networks and its application to sepsis classification		In this article, we propose a transfer learning method for deep neural networks (DNNs). Deep learning has been widely used in many applications. However, applying deep learning is problematic when a large amount of training data are not available. One of the conventional methods for solving this problem is transfer learning for DNNs. In the field of image recognition, state-of-the-art transfer learning methods for DNNs re-use parameters trained on source domain data except for the output layer. However, this method may result in poor classification performance when the amount of target domain data is significantly small. To address this problem, we propose a method called All-Transfer Deep Learning, which enables the transfer of all parameters of a DNN. With this method, we can compute the relationship between the source and target labels by the source domain knowledge. We applied our method to actual twodimensional electrophoresis image (2-DE image) classification for determining if an individual suffers from sepsis; the first attempt to apply a classification approach to 2-DE images for proteomics, which has attracted considerable attention as an extension beyond genomics. The results suggest that our proposed method outperforms conventional transfer learning methods for DNNs.	artificial neural network;computer vision;deep learning;fixatdl;machine learning;neural network software;proteomics;rtfm	Yoshihide Sawada;Yoshikuni Sato;Toru Nakada;Kei Ujimoto;Nobuhiro Hayashi	2016		10.3233/978-1-61499-672-9-1586	transfer of learning;machine learning;computer science;pattern recognition;artificial intelligence;artificial neural network;domain knowledge;deep learning;training set	ML	22.12995192028581	-50.16579321218761	104170
5dbe555b09307b5427e8fea2fcf6e37e0406e42f	miles: m ulticlass i mbalanced l earning in e nsembles through s elective sampling		Imbalanced learning is the problem of learning from datasets when the class proportions are highly imbalanced. Imbalanced datasets are increasingly seen in many domains and pose a challenge to traditional classification techniques. Learning from imbalanced multiclass data (three or more classes) creates additional complexities. Studies suggest that ensemble learners can be trained to emphasize different segments of data pertaining to different classes and thereby produce more accurate results than regular imbalance learning techniques. Thus, we propose a new approach to building ensembles of classifiers for multiclass imbalanced datasets, called Multiclass Imbalance Learning in Ensembles through Selective Sampling (MILES). Each member of MILES is trained with the data selectively sampled from the bands around cluster centroids in a way that diversity is aggressively encouraged within the ensemble. Resampling techniques are utilized to balance the distribution of the data that comes from each cluster. We performed several experiments applying our approach to different datasets demonstrating improved performance for recognizing minority class examples and balancing the G-mean and Mean Area Under the Curve (MAUC). We further applied MILES to classify prolonged emergency department (ED) stays with consistently higher performance as compared to existing methods.	ensembles of classifiers;experiment;resampling (statistics);sampling (signal processing)	Ali Azari;Vandana Pursnani Janeja;Scott Levin	2017		10.1145/3019612.3019667	ensembles of classifiers;sampling (statistics);resampling;ensemble learning;machine learning;artificial intelligence;multiclass classification;computer science	ML	14.248680584455105	-41.37452419733532	104189
afe079079b97862c7bf0d82c7b886e90ac6eb74e	composite and multiple kernel learning for brain computer interface		High-performance feature engineering and classification algorithms are significantly important for motor imagery (MI) related brain-computer interface (BCI) applications. In this research, we offer a new composite kernel support vector machine (CKSVM) based method to extract significant common spatial pattern (CSP) feature components from multiple temporal-frequency segments in a data-driven manner. Furthermore, we firstly introduce a multiple kernel discriminant analysis (MKDA) method for MI EEG classification. The experimental results on BCI competition IV data set 2b clearly showed the effectiveness of our method outperforming other similar approaches in the literature.	brain–computer interface;kernel (operating system);multiple kernel learning	Minmin Miao;Hong Zeng;Aimin Wang	2017		10.1007/978-3-319-70096-0_82	common spatial pattern;kernel (linear algebra);support vector machine;machine learning;multiple kernel learning;pattern recognition;artificial intelligence;kernel fisher discriminant analysis;feature engineering;statistical classification;computer science;composite number	ML	22.850641367334738	-42.827972960501256	104305
cd128dfe1fd00e9819e53ff81d82db0a8ede0baf	unsupervised adversarial invariance		Data representations that contain all the information about target variables but are invariant to nuisance factors benefit supervised learning algorithms by preventing them from learning associations between these factors and the targets, thus reducing overfitting. We present a novel unsupervised invariance induction framework for neural networks that learns a split representation of data through competitive training between the prediction task and a reconstruction task coupled with disentanglement, without needing any labeled information about nuisance factors or domain knowledge. We describe an adversarial instantiation of this framework and provide analysis of its working. Our unsupervised model outperforms state-of-the-art methods, which are supervised, at inducing invariance to inherent nuisance factors, effectively using synthetic data augmentation to learn invariance, and domain adaptation. Our method can be applied to any prediction task, eg., binary/multi-class classification or regression, without loss of generality.	algorithm;artificial neural network;authorization;biasing;convolutional neural network;domain adaptation;kerrison predictor;machine learning;multiclass classification;nuisance variable;overfitting;supervised learning;synthetic data;universal instantiation	Ayush Jaiswal;Yue Wu;Wael AbdAlmageed;Premkumar Natarajan	2018			nuisance;machine learning;without loss of generality;overfitting;computer science;supervised learning;artificial intelligence;artificial neural network;domain knowledge;synthetic data;invariant (mathematics)	ML	22.90387910832561	-45.78425447399526	104308
efaa0c8f9c19f0676f1391146dc40818050315fb	boosting unsupervised additive clustering using cluster-wise optimization and multi-label learning	pattern clustering;optimisation;pattern clustering data handling learning artificial intelligence optimisation;optimization technique;candecomp parafac;unsupervised;multi label;scaling up;supervised additive clustering multi label unsupervised;clustering;clustering algorithms motion pictures training additives optimization algorithm design and analysis scalability;additive;clusterwise optimization technique boosting unsupervised additive clustering clusterwise optimization multilabel learning overlapping clustering data cluster structure adclus indclus parafac models candecomp models;data handling;cluster model;learning artificial intelligence;supervised	Additive or overlapping clustering is a technique that is used to analyze overlapping cluster structure in data. In this paper, we motivate the overlapping clustering problem using an example of categorizing movies. We describe the ADCLUS and INDCLUS overlapping clustering models as discrete versions of the CANDECOMP/PARAFAC models. We describe the scalability problems inherent in current overlapping clustering approaches. We give a framework and algorithm for scaling up unsupervised overlapping clustering using a combination of a cluster-wise optimization technique and techniques from multi-label learning. Our framework uses a subset of data to find a training solution and then uses multi-label techniques to find labels for the remaining data.	additive model;algorithm;categorization;cluster analysis;image scaling;mathematical optimization;multi-label classification;scalability	Stephen L. France;Ahmed Abbasi	2011	2011 IEEE 11th International Conference on Data Mining Workshops	10.1109/ICDMW.2011.40	correlation clustering;constrained clustering;data stream clustering;fuzzy clustering;computer science;canopy clustering algorithm;machine learning;group method of data handling;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;single-linkage clustering;clustering high-dimensional data;conceptual clustering	DB	20.687806317277065	-41.96695352955744	104345
052ea9d4c43ca843898ea88c1a56c4d4d7acbda8	multi-class pattern classification using single, multi-dimensional feature-space feature extraction evolved by multi-objective genetic programming and its application to network intrusion detection	feature extraction;feature selection;multi objective genetic programming;multi class pattern classification	In this paper we investigate using multi-objective genetic programming to evolve a feature extraction stage for multiple-class classifiers. We find mappings which transform the input space into a new, multi-dimensional decision space to increase the discrimination between all classes; the number of dimensions of this decision space is optimized as part of the evolutionary process. A simple and fast multi-class classifier is then implemented in this multi-dimensional decision space. Mapping to a single decision space has significant computational advantages compared to k-class-to-2-class decompositions; a key design requirement in this work has been the ability to incorporate changing priors and/or costs associated with mislabeling without retraining. We have employed multi-objective optimization in a Pareto framework incorporating solution complexity as an independent objective to be minimized in addition to the main objective of the misclassification error. We thus give preference to simpler solutions which tend to generalize well on unseen data, in accordance with Occam’s Razor. We obtain classification results on a series of benchmark problems which are essentially identical to previous, more complex decomposition approaches. Our solutions are much simpler and computationally attractive as well as able to readily incorporate changing priors/costs. In addition, we have also applied our approach to the KDD-99 intrusion detection dataset and obtained results which are highly competitive with the KDD-99 Cup winner but with a significantly simpler classification framework.	benchmark (computing);binary classification;computation;confusion matrix;decision tree;discriminant;division by zero;experiment;feature extraction;feature selection;feature vector;film-type patterned retarder;finite-state machine;fitness function;genetic programming;intrusion detection system;linear genetic programming;linked list;logic programming;machine learning;map;mathematical optimization;multi-objective optimization;occam's razor;organizing (structure);pareto efficiency;preprocessor;self-organization;space mapping;spectral leakage;unary operation;occam	Khaled M. S. Badran;Peter Rockett	2011	Genetic Programming and Evolvable Machines	10.1007/s10710-011-9143-4	feature extraction;computer science;artificial intelligence;machine learning;pattern recognition;data mining;feature selection	ML	11.160241733113956	-41.3748334744181	104385
2ccfe11ec28f87b4fc282611cbe8e6471fd32d76	latent variable discovery in classification models	bayesian network;latent variables;learning;latent variable;naive bayes;scientific discovery;machine learning;bayesian networks machine learning;medical application;naive bayes model;bayesian networks	The naive Bayes model makes the often unrealistic assumption that the feature variables are mutually independent given the class variable. We interpret a violation of this assumption as an indication of the presence of latent variables, and we show how latent variables can be detected. Latent variable discovery is interesting, especially for medical applications, because it can lead to a better understanding of application domains. It can also improve classification accuracy and boost user confidence in classification models.	application domain;latent variable;naive bayes classifier	Nevin Lianwen Zhang;Thomas D. Nielsen;Finn Verner Jensen	2004	Artificial intelligence in medicine	10.1016/j.artmed.2003.11.004	latent class model;latent variable;bayesian programming;computer science;machine learning;pattern recognition;bayesian network;data mining;probabilistic latent semantic analysis	AI	16.60789023286977	-38.426014370640615	104410
6111a27ddbc0255b32713d926272bf754bf96a5f	efficient model selection for regularized linear discriminant analysis	high dimensional dataset;model selection;small sample size;dimension reduction;linear discriminate analysis;regularization;theoretical analysis;cross validation;linear discriminant analysis	Classical Linear Discriminant Analysis (LDA) is not applicable for small sample size problems due to the singularity of the scatter matrices involved. Regularized LDA (RLDA) provides a simple strategy to overcome the singularity problem by applying a regularization term, which is commonly estimated via cross-validation from a set of candidates. However, cross-validation may be computationally prohibitive when the candidate set is large. An efficient algorithm for RLDA is presented that computes the optimal transformation of RLDA for a large set of parameter candidates, with approximately the same cost as running RLDA a small number of times. Thus it facilitates efficient model selection for RLDA.An intrinsic relationship between RLDA and Uncorrelated LDA (ULDA), which was recently proposed for dimension reduction and classification is presented. More specifically, RLDA is shown to approach ULDA when the regularization value tends to zero. That is, RLDA without any regularization is equivalent to ULDA. It can be further shown that ULDA maps all data points from the same class to a common point, under a mild condition which has been shown to hold for many high-dimensional datasets. This leads to the overfitting problem in ULDA, which has been observed in several applications. Thetheoretical analysis presented provides further justification for the use of regularization in RLDA. Extensive experiments confirm the claimed theoretical estimate of efficiency. Experiments also show that, for a properly chosen regularization parameter, RLDA performs favorably in classification, in comparison with ULDA, as well as other existing LDA-based algorithms and Support Vector Machines (SVM).	algorithm;cross-validation (statistics);data point;dimensionality reduction;experiment;linear discriminant analysis;local-density approximation;map;matrix regularization;model selection;overfitting;regularized meshless method;support vector machine;technological singularity	Jieping Ye;Tao Xiong;Qi Li;Ravi Janardan;Jinbo Bi;Vladimir Cherkassky;Chandra Kambhamettu	2006		10.1145/1183614.1183691	regularization;mathematical optimization;computer science;machine learning;linear discriminant analysis;cross-validation;model selection;statistics;dimensionality reduction	ML	24.12229505941411	-39.697045457128276	104497
9dd0e7ee7d61f35e3377410af349eb27ab1e898c	an incremental linear discriminant analysis using fixed point method	metodo adaptativo;analisis componente principal;learning rate;streaming;mise a jour;fixed point method;data stream;relacion convergencia;matrice covariance;punto fijo;taux convergence;methode adaptative;convergence rate;matriz covariancia;linear discriminate analysis;power method;fixed point;actualizacion;discriminant analysis;analyse discriminante;analisis discriminante;transmission en continu;point fixe;principal component analysis;adaptive method;analyse composante principale;pattern recognition;scattering matrix;reconnaissance forme;transmision fluyente;reseau neuronal;reconocimiento patron;fix point;red neuronal;updating;covariance matrix;neural network	Linear Discriminant Analysis (LDA) is a very powerful method in pattern recognition. But it is difficult to realize online processing for data stream. In this paper, a new adaptive LDA method is proposed. We decompose the online LDA problem into two adaptive PCA problems and develop a fixed point adaptive PCA to implement adaptive LDA. Online updating of in-class scatter matrix () w t S and covariance matrix () x t C are derived in this paper. Simulation results show that the proposed method has no learning rate, fast convergence and less time-consuming.	fixed-point iteration;linear discriminant analysis	Dongyue Chen;Liming Zhang	2006		10.1007/11759966_198	s-matrix;fixed-point iteration;econometrics;covariance matrix;power iteration;computer science;machine learning;mathematics;fixed point;rate of convergence;linear discriminant analysis;artificial neural network;algorithm;statistics;principal component analysis	Vision	23.584725557891712	-38.03091596324256	104525
6e08a2fbfb4e215e00b709af8dc1d08913df1e12	weights optimization for multi-instance multi-label rbf neural networks using steepest descent method	machine learning;multi-instance multi-label learning;neural networks;radial basis function	Multi-instance multi-label learning (MIML) is an innovative learning framework where each sample is represented by multiple instances and associated with multiple class labels. In several learning situations, the multi-instance multi-label RBF neural networks (MIMLRBF) can exploit connections between the instances and the labels of an MIML example directly, while most of other algorithms cannot learn that directly. However, the singular value decomposition (SVD) method used to compute the weights of the output layer will cause augmented overall error in network performance when training data are noisy or not easily discernible. This paper presents an improved approach to learning algorithms used for training MIMLRBF. The steepest descent (SD) method is used to optimize the weights after they are initialized by the SVD method. Comparing results employing diverse learning strategies shows interesting outcomes as have come out of this paper.	algorithm;artificial neural network;gradient descent;k-medoids;machine learning;mathematical optimization;medoid;multi-label classification;network performance;radial basis function;singular value decomposition;supervised learning	Cunhe Li;Guoqiang Shi	2012	Neural Computing and Applications	10.1007/s00521-012-0815-7	mathematical optimization;computer science;artificial intelligence;machine learning	ML	19.78019795749067	-40.18649826880657	104605
e7481bfafe5dca9706d11121814c1355145b1e99	sparse representation-based archetypal graphs for spectral clustering		We propose sparse representation-based archetypal graphs as input to spectral clustering for anomaly and change detection. The graph consists of vertices defined by data samples and edges which weights are determines by sparse representation. Besides relationships between all data samples, the graph also encodes the relationship to extremal points, so-called archetypes, which leads to an easily interpretable clustering result. We compare our approach to k-means clustering performed on the original feature representation and to k-means clustering performed on the sparse representation activations. Experiments show that our approach is able to deliver accurate and interpretable results for anomaly and change detection.	anomaly detection;cluster analysis;directed graph;experiment;k-means clustering;sparse approximation;sparse matrix;spectral clustering	Ribana Roscher;Lukas Drees;Susanne Wenzel	2017	2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2017.8127425	computer science;computer vision;spectral clustering;artificial intelligence;vertex (geometry);cluster analysis;sparse matrix;sparse approximation;hyperspectral imaging;change detection;graph;pattern recognition	Vision	16.63020828746179	-47.065553044337996	105137
bce43e485279f83e1f8189a8afc5f06c7539228f	minimum class variance svm+ for data classification		In this paper, a new Support Vector Machine Plus (SVM+) type model called Minimum Class Variance SVM+ (MCVSVM+) is presented. Similar to SVM+, the proposed model utilizes the group information in the training data. We show that MCVSVM+ has both the advantages of SVM+ and Minimum Class Variance Support Vector Machine (MCVSVM). That is, MCVSVM+ not only considers class distribution characteristics in its optimization problem but also utilizes the additional information (i.e. group information) hidden in the data, in contrast to SVM+ that takes into consideration only the samples that are in the class boundaries. The experimental results demonstrate the validity and advantage of the new model compared with the standard SVM, SVM+ and MCVSVM.		Wenxin Zhu;Ping Zhong	2017	Adv. Data Analysis and Classification	10.1007/s11634-015-0212-z	machine learning;pattern recognition;data mining;mathematics;ranking svm	ML	13.673970798382491	-40.49396866139875	105258
d1a760d034200c0a34aa1dbdaa0620756c2aa5e8	deep feature extraction in the dct domain	image coding;training;quantization signal;computational modeling;discrete cosine transforms;feature extraction	We explore the effectiveness of deep features extracted by Convolutional Neural Networks(CNNs) in the Discrete Cosine Transform(DCT) domain for various image classification tasks such as pedestrian and face detection, material identification and object recognition. We perform the DCT operation on the feature maps generated by convolutional layers in CNNs. We compare the performance of the same network on the same datasets, with the same hyper-parameters with or without the DCT step. Our results indicate that a DCT operation incorporated into the network after convolution+thresholding and before pooling can have certain advantages such as convergence over fewer training epochs and sparser weight matrices that are more conducive to pruning and hashing techniques.	artificial neural network;computer vision;convolutional neural network;cryptographic hash function;discrete cosine transform;face detection;feature extraction;map;network convergence;outline of object recognition;quantization (signal processing);technological convergence	Arthita Ghosh;Rama Chellappa	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7900182	computer vision;feature extraction;computer science;machine learning;pattern recognition;computational model	Vision	23.982381091947225	-51.18848864407631	105261
1ddae403d161a121b012f12ddbaed90e26fbda07	fraud detection with density estimation trees		We consider the problem of anomaly detection in finance. An application of interest is the detection of first-time fraud where new classes of fraud need to be detected using unsupervised learning to augment the existing supervised learning techniques that capture known classes of frauds. This domain usually has the following requirements – (i) the ability to handle data containing both numerical and categorical features, (ii) very low latency real-time detection, and (iii) interpretability. We propose the use of a variant of density estimation trees (DETs) (Ram and Gray, 2011) for anomaly detection using distributional properties of the data. We formally present a procedure for handling data sets with both categorical and numerical features while Ram and Gray (2011) focused mainly on data sets with all numerical features. DETs have demonstrably fast prediction times, orders of magnitude faster than other density estimators like kernel density estimators. The estimation of the density and the anomalousness score for any new item can be done very efficiently. Beyond the flexibility and efficiency, DETs are also quite interpretable. For the task of anomaly detection, DETs can generate a set of decision rules that lead to high anomalous-ness scores. We empirically demonstrate these capabilities on a publicly available fraud data set.	anomaly detection;baseline (configuration management);categorical variable;decision tree;discretization;earthbound;k-means clustering;numerical analysis;ordinal data;preprocessor;product binning;quantization (signal processing);random-access memory;real-time clock;requirement;supervised learning;unsupervised learning	Parikshit Ram;Alexander G. Gray	2017			artificial intelligence;computer science;machine learning;data mining;density estimation	ML	17.165465579725776	-38.34586534213115	105298
fd65a224e71dd25507f8492d8efcc8829a03a1a4	persistence codebooks for topological data analysis		Topological data analysis, such as persistent homology has shown beneficial properties for machine learning in many tasks. Topological representations, such as the persistence diagram (PD), however, have a complex structure (multiset of intervals) which makes it difficult to combine with typical machine learning workflows. We present novel compact fixed-size vectorial representations of PDs based on clustering and bag of words encodings that cope well with the inherent sparsity of PDs. Our novel representations outperform state-of-the-art approaches from topological data analysis and are computationally more efficient.	bag-of-words model;cluster analysis;codebook;dhrystone;diagram;encode;homology (biology);machine learning;persistence (computer science);persistent homology;sparse matrix;topological data analysis;unsupervised learning	Bartosz Zielinski;Mateusz Juda;Matthias Zeppelzauer	2018	CoRR		machine learning;diagram;mathematics;artificial intelligence;bag-of-words model;cluster analysis;topological data analysis;multiset;persistent homology;persistence (computer science)	ML	12.65234293670867	-48.30562017714868	105303
b77a9fc7915fa97549cb38e109da9e4607dbefb4	lazy learner on decision tree for ranking	decision tree;classification;probability based ranking;lazy learner	This paper aims to improve probability-based ranking (e.g.AUC) under decision-tree paradigm. We observe the fact that probability-based ranking is to sort samples in terms of their class probabilities. Therefore, ranking is a relative evaluation metric among those samples. This motivates us to use a lazy learner to explicitly yield a set of unique class probabilities for a testing sample based on its similarities to the training samples within its neighborhood. We embed lazy learners at the leaves of a decision tree to give class probability assignments. This results in the first model, named Lazy Distancebased Tree (LDTree). Then we further improve this model by continuing to grow the tree for the second time, and call the resulting model Eager Distance-based Tree (EDTree). In addition to the benefits of lazy learning, EDTree also takes advantage of the finer resolution of a large tree structure. We compare our models with C4.5, C4.4 and their variants in AUC on a large suite of UCI sample sets. The improvement shows that our method follows a new path that leads to better ranking performance.	bayesian network;best practice;c4.5 algorithm;decision tree;lazy evaluation;lazy learning;programming paradigm;tree structure	Yuhong Yan;Han Liang	2008	International Journal on Artificial Intelligence Tools	10.1142/S0218213008003819	biological classification;computer science;machine learning;decision tree;pattern recognition;incremental decision tree;data mining	AI	14.827032214502811	-39.96468958816589	105328
ba2be4898fcd3f3dfdb89e9df01728a7ac127efb	artificial immune system for attribute weighted naive bayes classification	probability;bayes methods;probability artificial immune systems bayes methods pattern classification;niobium immune system accuracy cloning sociology statistics mutual information;pattern classification;artificial immune systems;conference proceeding;learning process attribute weighted naive bayes classification artificial immune system based weighted naive bayes classifier aiswnb immunity theory optimal weight values conditional independence assumption conditional probability artificial immune system search mechanism	Naive Bayes (NB) is a popularly used classification method. One potential weakness of NB is the strong conditional independence assumption between attributes, which may deteriorate the classification accuracy. In this paper, we propose a new Artificial Immune System based Weighted Naive Bayes (AISWNB) classifier. AISWNB uses immunity theory in artificial immune systems to find optimal weight values for each attribute. The adjusted weight values will alleviate the conditional independence assumption and help calculate the conditional probability in an accurate way. Because AISWNB uses artificial immune system search mechanism to find optimal weights, it does not need to know the importance of individual attributes nor the relevance among attributes. As a result, it can obtain optimal weight value for each attribute during the learning process. Experiments and comparisons on 36 benchmark data sets demonstrate that AISWNB outperforms other state-of-the-art attribute weighted NB algorithms.	algorithm;artificial immune system;benchmark (computing);evolutionary algorithm;experiment;naive bayes classifier;need to know;relevance;statistical classification;weight function	Jia Wu;Zhihua Cai;Sanyou Zeng;Xingquan Zhu	2013	The 2013 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2013.6706818	bayes classifier;naive bayes classifier;machine learning;pattern recognition;probability;data mining;mathematics;statistics	AI	11.754726666934559	-39.293564134943665	105389
218eeeb5e470f0ac95d25096c517b5c3e8babbb5	an end-to-end textspotter with explicit alignment and attention		Text detection and recognition in natural images have long been considered as two separate tasks that are processed sequentially. Jointly training two tasks is non-trivial due to significant differences in learning difficulties and convergence rates. In this work, we present a conceptually simple yet efficient framework that simultaneously processes the two tasks in a united framework. Our main contributions are three-fold: (1) we propose a novel text-alignment layer that allows it to precisely compute convolutional features of a text instance in arbitrary orientation, which is the key to boost the performance; (2) a character attention mechanism is introduced by using character spatial information as explicit supervision, leading to large improvements in recognition; (3) two technologies, together with a new RNN branch for word recognition, are integrated seamlessly into a single model which is end-to-end trainable. This allows the two tasks to work collaboratively by sharing convolutional features, which is critical to identify challenging text instances. Our model obtains impressive results in end-to-end recognition on the ICDAR 2015 [19], significantly advancing the most recent results [2], with improvements of F-measure from (0.54, 0.51, 0.47) to (0.82, 0.77, 0.63), by using a strong, weak and generic lexicon respectively. Thanks to joint training, our method can also serve as a good detector by achieving a new state-of-the-art detection performance on related benchmarks. Code is available at https://github.com/tonghe90/textspotter.		Tong He;Zhi Tian;Weilin Huang;Chunhua Shen;Yu Qiao;Changming Sun	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00527	machine learning;artificial intelligence;convolutional code;task analysis;spatial analysis;feature extraction;word recognition;end-to-end principle;lexicon;computer science;pattern recognition;convergence (routing)	Vision	21.676381334036037	-52.01441337757434	105502
a405d5a0384e9e7f3cbb51ccf2101273fe9990e5	clustering analysis for semi-supervised learning improves classification performance of digital pathology		Purpose: Completely labeled datasets of pathology slides are often difficult and time consuming to obtain. Semi-supervised learning methods are able to learn reliable models from small number of labeled instances and large quantities of unlabeled data. In this paper, we explored the potential of clustering analysis for semi-supervised support vector machine (SVM) classifier. Method: A clustering analysis method was proposed to find regions of high density prior to finding the decision boundary using a supervised SVM and was compared with another state-of-the-art semi-supervised technique. Different percentages of labeled instances were used to train supervised and semi-supervised SVM learners from an image dataset generated from 50 whole-mount images (8 patients) of breast specimen. Their cross-validated classification performances were compared with each other using the area under the ROC curve measure. Result: Our proposed clustering analysis for semisupervised learning was able to produce a reliable classification model from small amounts of labeled data. Comparing the proposed method in this study with a well-known implementation of semi-supervised SVM, our method performed much faster and produced better results.	biological specimen;cluster analysis;decision boundary;performance;receiver operating characteristic;semi-supervised learning;semiconductor industry;supervised learning;support vector machine	Mohammad Peikari;Judit T. Zubovits;Gina M. Clarke;Anne L. Martel	2015		10.1007/978-3-319-24888-2_32	semi-supervised learning;computer science;machine learning;pattern recognition;data mining	ML	13.785893978303895	-44.55875297243092	105567
8e649e223f7f2427ad709c273118d763febae2bb	recursive similarity-based algorithm for deep learning	k nearest neighbors;info eu repo semantics article;machine learning;deep networks;similarity based learning	Recursive Similarity-Based Learning algorithm (RSBL) follows the deep learning idea, exploiting similarity-based methodology to recursively generate new features. Each transformation layer is generated separately, using as inputs information from all previous layers, and as new features similarity to the k nearest neighbors scaled using Gaussian kernels. In the feature space created in this way results of various types of classifiers, including linear discrimination and distance-based methods, are significantly improved. As an illustrative example a few non-trivial benchmark datasets from the UCI Machine Learning Repository are analyzed.	approximation algorithm;benchmark (computing);computational intelligence;data model;deep learning;feature selection;feature vector;global optimization;k-nearest neighbors algorithm;linear discriminant analysis;machine learning;mathematical optimization;online and offline;online machine learning;random projection;recursion (computer science);stationary process	Tomasz Maszczyk;Wlodzislaw Duch	2012		10.1007/978-3-642-34487-9_48	wake-sleep algorithm;computer science;online machine learning;machine learning;pattern recognition;data mining;stability;k-nearest neighbors algorithm;active learning;generalization error	ML	21.896435725295685	-41.039850690506654	105722
ef56193b18e53480e1e39aabfa2613fe0aca1539	multiplying the mileage of your dataset with subwindowing	book chapter;window size;overlapping window;electroencephalogram;article	This study is focused on improving the classification performance of EEG data through the use of some data restructuring methods. In this study, the impact of having more training instances/samples vs. using shorter window sizes is investigated. The BCI2003 IVa dataset is used to examine the results. The results not surprisingly indicate that, up to a certain point, having higher numbers of training instances significantly improves the classification performance while the use of shorter window sizes tends to worsen performance in a way that usually cannot fully be compensated for by the additional instances, but tends to provide useful gain in overall performance for small divisors into two or three subepochs. We have moreover determined that use of an incomplete set of overlapping windows can have little effect, and is inapplicable for the smallest divisors, but that use of overlapping subepochs from three specific non-overlapping areas (start, middle and end) of a superepoch tends to contribute significant additional information. Examination of a division into five equal non-overlapping areas indicates that for some subjects the first or last fifth contributes significantly less information than the middle three fifths.	electroencephalography;microsoft windows;window function	Adham Atyabi;Sean P. Fitzgibbon;David M. W. Powers	2011		10.1007/978-3-642-23605-1_19	simulation;computer science;artificial intelligence;data mining	Metrics	14.257549490907252	-42.059597223769345	105769
85535bce3c25222f90ce2b75dd514c6e573dcbe3	rule weight update in parallel distributed fuzzy genetics-based machine learning with data rotation	rule weight update incremental learning fuzzy rules training data rotation parallel distributed model subpopulations fuzzy gbml algorithms island model parallel distributed fuzzy genetics based machine learning;pattern classification genetics based machine learning gbml genetic fuzzy systems gfs parallel evolutionary computation pittsburgh approach fuzzy rule based classifiers;training data accuracy computational modeling data models training sociology statistics;genetic algorithms;learning artificial intelligence;knowledge based systems;parallel algorithms genetic algorithms knowledge based systems learning artificial intelligence;parallel algorithms	In our former study, we have already proposed a parallel distributed model for the speedup of fuzzy genetics-based machine learning (GBML). Our model is an island model for parallel implementation of fuzzy GBML algorithms where a population is divided into multiple subpopulations. A single subpopulation is assigned to each island. Training data are also divided and distributed over the islands. When we have N islands (i.e., N CPUs for parallel computation), the speedup is the order of the square of N. This is because both the population and the training data are divided into N subsets. One characteristic feature of our parallel distributed model is training data rotation over the islands. Each of the N training data subsets is assigned to one of the N islands. The assigned training data subsets are rotated over the islands periodically (e.g., every 100 generations). This means that the environment of each island is changed periodically. The focus of this paper is how to update existing fuzzy rules at each island after the training data rotation. One extreme setting is to totally update fuzzy rules using the newly assigned training data subset. Another extreme setting is to use existing fuzzy rules with no changes. In this paper, we examine incremental learning, which can be viewed as an intermediate mechanism between the two extreme settings.	algorithm;central processing unit;computation;fuzzy rule;incremental backup;logic programming;machine learning;mathematical optimization;parallel computing;speedup;test set	Hisao Ishibuchi;Masakazu Yamane;Yusuke Nojima	2013	2013 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2013.6622572	genetic algorithm;computer science;artificial intelligence;neuro-fuzzy;knowledge-based systems;machine learning;data mining;parallel algorithm	DB	10.979123974416149	-40.1252533383396	106012
7b205999fa50f84efe8d8f23f4509d092d2c1fe7	a transductive svm with quasi-linear kernel based on cluster assumption for semi-supervised classification	kernel;support vector machines;support vector machines pattern classification pattern clustering set theory;accuracy;minimal sets transductive svm quasilinear kernel semisupervised classification transductive support vector machine tsvm clustering assumption modified density clustering method pairwise label switching method;kernel switches support vector machines accuracy;switches	This paper presents a Transductive Support Vector Machine (TSVM) with quasi-linear kernel based on a clustering assumption for semi-supervised classification. Since the potential separating boundary is located in low density area between classes, a modified density clustering method by considering label information is firstly introduced to extract the information of potential separating boundary in low density region between different classes. Then the information is used to compose a quasi-linear kernel for the TSVM. The optimization of TSVM is further speeded up by developing a pairwise label switching method on minimal sets. Experiment results on benchmark datasets show that the proposed method is effective and improves classification performances.	benchmark (computing);cluster analysis;cluster hypothesis;f1 score;machine learning;mathematical optimization;performance;precision and recall;semi-supervised learning;semiconductor industry;simulation;supervised learning;support vector machine	Bo Zhou;Di Fu;Chao Dong;Jinglu Hu	2015	2015 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2015.7280485	support vector machine;kernel method;kernel;radial basis function kernel;network switch;computer science;machine learning;pattern recognition;data mining;mathematics;accuracy and precision;variable kernel density estimation;polynomial kernel;statistics	ML	18.550916787880016	-42.168274881784015	106032
c61f9399547869f2f312f64f2760f6408f8ae039	pattern classification with imbalanced and multiclass data for the prediction of albendazole adverse event outcomes		Abstract   Class imbalance problem is one of the important problems for classification studies in data mining. In this study, a comparative analysis of some sampling methods was performed based on the evaluation of four classification algorithms for the prediction of albendazole adverse events outcomes. Albendazole is one of the main medications used for the treatment of a variety of parasitic worm infestations. The dataset was created from the public release of the FDA's FAERS database. Four sampling algorithms were used to analyze the dataset and their performance was evaluated by using four classifiers. Among the algorithms, ID3 with resample algorithm has higher accuracy results than the others after the application of sampling methods. This study supported that sampling methods are capable to improve the performance of learning algorithms.		Pinar Yildirim	2016		10.1016/j.procs.2016.04.216	computer science;data science;machine learning;data mining	ML	13.377227342629045	-42.53836600697013	106058
4ca786b61c4b8c6d9dc934a2f094d44d8dcb350d	elitist and ensemble strategies for cascade generalization	decision tree;ensemble method;elitist strategy;data mining;machine learning;voting method;cascade generalization	Several methods have been proposed for cascading other classification algorithms with decision tree learners to alleviate the representational bias of decision trees and, potentially, to improve classification accuracy. Such cascade generalization of decision trees increases the flexibility of the decision boundaries between classes and promotes better fitting of the training data. However, more flexible models may not necessarily lead to more predictive power. Because of potential overfitting problems, the true classification accuracy on test data may not increase. Recently, a generic method for cascade generalization has been proposed. The method uses a parameter — the maximum cascading depth — to constrain the degree that other classification algorithms are cascaded with decision tree learners. A method for efficiently learning a collection (i.e., a forest) of generalized decision trees, each with other classification algorithms cascaded to a particular depth, also has been developed. In this article, we propose several new strategies, including elitist and ensemble (weighted or unweighted), for using the various decision trees in such a collection in the prediction phase. Our empirical evaluation using 32 data sets in the UCI machine learning repository shows that, on average, the elitist strategy outperforms the weighted full ensemble strategy, which, in turn, outperforms the unweighted full ensemble strategy. However, no strategy is universally superior across all applications. Since the same training process can be used to evaluate the various strategies, we recommend that several promising strategies be evaluated and compared before selecting the one to use for a given application.		Huimin Zhao;Atish P. Sinha;Sudha Ram	2006	J. Database Manag.	10.4018/jdm.2006070105	decision tree learning;computer science;machine learning;decision tree;pattern recognition;data mining;ensemble learning	ML	14.508524204241908	-40.63385472826935	106147
84f3ae507d775b0b5a0431362c9e3297503485e5	a combination classification algorithm based on outlier detection and c4.5	classification algorithm;decision tree;data distribution;outlier detection;machine learning;imbalanced data sets	The performance of traditional classifier skews towards the majority class for imbalanced data, resulting in high misclassification rate for minority samples. To solve this problem, a combination classification algorithm based on outlier detection and C4.5 is presented. The basic idea of the algorithm is to make the data distribution balance by grouping the whole data into rare clusters and major clusters through the outlier factor. Then C4.5 algorithm is implemented to build the decision trees on both the rare clusters and the major clusters respectively. When classifying a new object, the decision tree for evaluation will be chosen according to the type of the cluster which the new object is nearest. We use the datasets from the UCI Machine Learning Repository to perform the experiments and compare the effects with other classification algorithms; the experiments demonstrate that our algorithm performs much better for the extremely imbalanced data sets.	anomaly detection;c4.5 algorithm;cluster analysis;decision tree;experiment;machine learning;naive bayes classifier;xslt/muenchian grouping	ShengYi Jiang;Wen Yu	2009		10.1007/978-3-642-03348-3_50	anomaly detection;computer science;machine learning;decision tree;pattern recognition;data mining;one-class classification	ML	13.510823273125068	-41.003189812568166	106379
b4589b25fd94152436524c9cfbe418154fc31584	kernel principal components are maximum entropy projections	traitement signal;processus gauss;optimisation;analisis componente principal;entropia;optimum;analisis estadistico;separacion ciega;metodo entropia maxima;optimizacion;separation aveugle source;methode noyau;blind source separation;probabilistic approach;feature space;interpretacion informacion;interpretation information;blind separation;statistical analysis;enfoque probabilista;approche probabiliste;signal processing;principal component analysis;metodo nucleo;optimo;entropie;analyse statistique;gaussian kernel;analyse composante principale;separation aveugle;information theoretic learning;kernel pca;kernel method;optimization;entropy;entropy projections;gaussian process;theorie information;methode entropie maximum;proceso gauss;procesamiento senal;information theoretic;method of maximum entropy;information interpretation;maximum entropy;information theory;principal component;teoria informacion	Principal Component Analysis (PCA) is a very well known statistical tool. Kernel PCA is a nonlinear extension to PCA based on the kernel paradigm. In this paper we characterize the projections found by Kernel PCA from a information theoretic perspective. We prove that Kernel PCA provides optimum entropy projections in the input space when the Gaussian kernel is used for the mapping and a sample estimate of Renyi’s entropy based on the Parzen window method is employed. The information theoretic interpretation motivates the choice and specifices the kernel used for the transformation to feature space.	feature vector;information theory;kernel (operating system);kernel density estimation;kernel principal component analysis;nonlinear system;principle of maximum entropy;programming paradigm;window function	António R. C. Paiva;Jian-Wu Xu;José Carlos Príncipe	2006		10.1007/11679363_105	kernel;principal component regression;kernel regression;kernel density estimation;kernel method;econometrics;entropy;kernel fisher discriminant analysis;kernel embedding of distributions;radial basis function kernel;information theory;kernel principal component analysis;computer science;machine learning;signal processing;pattern recognition;mathematics;variable kernel density estimation;polynomial kernel;statistics;kernel smoother;principal component analysis	ML	23.698022041014486	-38.50061503361193	106551
8f9ccdde21a1ea0043d6ea5c0bc168783a75af45	a balanced accuracy fitness function leads to robust analysis using grammatical evolution neural networks in the case of class imbalance	ホワイトデー お返し;health research;uk clinical guidelines;biological patents;クロエ;robustness analysis;neural networks;オンラインブランド品;tokuya;ヴィトン;バレンシアガ;europe pubmed central;2way;gene gene interactions;爽快ドラッグ;買取;citation search;安全靴;class imbalance;computational method;ケイトスペード;財布;シャネル;エルメス;スニーカー メンズ;グッチ;クロムハーツ;オンライン ホイール1本 単品 crimson club linea l450 bk mc 19インチ 10 0j pcd 120 穴数 5 インセット 30;uk phd theses thesis;ボッテガヴェネタ;トリーバーチ;ティファニー;grammatical evolution;nike リュック;life sciences;classification error;ディオール;オンラインカーペット 激安 通販 サンゲツのロールカーペット 半額以下 ロールカーペット 横364 縦340cm ロック加工カーペット;genetic epidemiology;バッグ;sampling methods;uk research reports;コーチ;medical journals;iphone6s ケース 手帳型;europe pmc;グッチ gucci 財布 ネップツイード ブラウン ggキャンバス 展示品 新品 二つ折り 二つ折り長財布 254012 2067 19920 対応;フェラガモ;biomedical research;fitness function;single nucleotide polymorphism;neural network;ショルダーバッグ レディース 斜めがけ;bioinformatics	Grammatical Evolution Neural Networks (GENN) is a computational method designed to detect gene-gene interactions in genetic epidemiology, but has so far only been evaluated in situations with balanced numbers of cases and controls. Real data, however, rarely has such perfectly balanced classes. In the current study, we test the power of GENN to detect interactions in data with a range of class imbalance using two fitness functions (classification error and balanced error), as well as data re-sampling. We show that when using classification error, class imbalance greatly decreases the power of GENN. Re-sampling methods demonstrated improved power, but using balanced accuracy resulted in the highest power. Based on the results of this study, balanced error has replaced classification error in the GENN algorithm	artificial neural network;biological evolution;class;fitness function;grammatical evolution;hereditary diseases;interaction;neural network simulation;neural tube defects;sampling (signal processing);sampling - surgical action;algorithm	Nicholas E. Hardison;Theresa J. Fanelli;Scott M. Dudek;David M. Reif;Marylyn DeRiggi Ritchie;Alison A. Motsinger-Reif	2008	Genetic and Evolutionary Computation Conference : [proceedings]. Genetic and Evolutionary Computation Conference	10.1145/1389095.1389159	single-nucleotide polymorphism;sampling;computer science;bioinformatics;artificial intelligence;machine learning;mathematics;grammatical evolution;operations research;fitness function;artificial neural network	Robotics	10.648181225729362	-50.27585279320805	106569
631c9b7dc841c4972b6234e7fb09e4395eb35967	optimism in active learning with gaussian processes	gaussian processes;active learning;classification;multi armed bandits;classifiaction;optimism in the face of uncertainty	In the context of Active Learning for classification, the classification error depends on the joint distribution of samples and their labels which is initially unknown. The minimization of this error requires estimating this distribution. Online estimation of this distribution involves a trade-off between exploration and exploitation. This is a common problem in machine learning for which multi-armed bandit theory, building upon Optimism in the Face of Uncertainty, has been proven very efficient these last years. We introduce two novel algorithms that use Optimism in the Face of Uncertainty along with Gaussian Processes for the Active Learning problem. The evaluation lead on real world datasets shows that these new algorithms compare positively to state-of-the-art methods.	active learning (machine learning);algorithm;exploit (computer security);gaussian process;machine learning;multi-armed bandit	Timothé Collet;Olivier Pietquin	2015		10.1007/978-3-319-26535-3_18	biological classification;artificial intelligence;machine learning;gaussian process;active learning;active learning	ML	18.254077833018112	-39.03888242855446	106746
b7e889f2be0c23916c910a02a4ae1aa7ef65a7ec	optimization of combined kernel function for svm based on large margin learning theory	large margin learning;kernel;support vector machines;training;kernel function;support vector machine optimization combined kernel function svm large margin learning theory genetic algorithm;testing;accuracy;theoretical analysis;support vector machines genetic algorithms;large margin learning theory;genetic algorithm;genetic algorithms;svm;optimization;support vector machine;optimization combined kernel function svm large margin learning genetic algorithm;learning theory;combined kernel function;kernel support vector machines support vector machine classification genetic algorithms optimization methods educational institutions mathematics computer science mathematical model testing;numerical simulation;generalization capability	Kernel function plays a very important role in the performance of SVM. In order to improve generalization capability of SVM classifier, this paper proposes a new mechanism to optimize the parameters of combined kernel function by using large margin learning theory and a genetic algorithm, which aims to search the optimal parameters for the combined kernel function. This approach leads SVM to attain the maximum margin in the training dataset. The combined kernel function and the parameters obtained by the proposed approach leads to a better performance and results in a better SVM classifier. Both numerical simulation results and theoretical analysis show the effectiveness and feasibility of the proposed approach.	computer simulation;genetic algorithm;support vector machine	Mingzhu Lu;C. L. Philip Chen;Jianbing Huo;Xizhao Wang	2008	2008 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2008.4811301	computer simulation;support vector machine;kernel method;mathematical optimization;margin;radial basis function kernel;computer science;machine learning;pattern recognition;polynomial kernel;kernel smoother	Robotics	19.69170049744615	-39.133098338254094	106863
8574213f054af07f6a1b3bd305a5445ed4d93034	evolving artificial datasets to improve interpretable classifiers	machine learning algorithms;naive bayes classifiers;differential evolution;evolution based approach;evolutionary computation;cost function;support vector machines;training;vectors training cost function machine learning algorithms training data sociology;artificial data differential evolution supervised machine learning interpretable models;supervised machine learning;conference contribution;linear support vector machines;artificial training datasets;training data;differential evolution parameters;statistical analysis;vectors;supervised classifiers;statistical characteristics;interpretable models;statistical characteristics interpretable classifiers artificial training datasets machine learning algorithms supervised classifiers linear support vector machines naive bayes classifiers evolutionary optimization evolution based approach predictive classification performance artificial data optimization process differential evolution parameters;support vector machines evolutionary computation learning artificial intelligence pattern classification statistical analysis;pattern classification;artificial data optimization process;learning artificial intelligence;evolutionary optimization;artificial data;predictive classification performance;sociology;interpretable classifiers	Differential Evolution can be used to construct effective and compact artificial training datasets for machine learning algorithms. In this paper, a series of comparative experiments are performed in which two simple interpretable supervised classifiers (specifically, Naive Bayes and linear Support Vector Machines) are trained (i) directly on “real” data, as would be the normal case, and (ii) indirectly, using special artificial datasets derived from real data via evolutionary optimization. The results across several challenging test problems show that supervised classifiers trained indirectly using our novel evolution-based approach produce models with superior predictive classification performance. Besides presenting the accuracy of the learned models, we also analyze the sensitivity of our artificial data optimization process to Differential Evolution's parameters, and then we examine the statistical characteristics of the artificial data that is evolved.	algorithm;differential evolution;experiment;machine learning;mathematical optimization;naive bayes classifier;supervised learning;support vector machine	Michael Mayo;Quan Sun	2014	2014 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2014.6900238	differential evolution;support vector machine;training set;computer science;machine learning;pattern recognition;data mining;evolutionary computation	ML	12.819652958754947	-42.16549519065191	107007
9e79abb3cde783f5c1bf18baa3bcbe45cb2ff1f9	a cerebellar model classifier for data mining with linear time complexity	training;data mining;classification;linear time;cerebellar model articulation controller	Techniques for automated classification need to be efficient when applied to large datasets. Machine learning techniques such as neural networks have been successfully applied to this class of problem, but training times can blow out as the size of the database increases. Some of the desirable features of classification algorithms for large databases are linear time complexity, training with only a single pass of the data, and accountability for class assignment decisions. A new training algorithm for classifiers based on the Cerebellar Model Articulation Controller (CMAC) possesses these features. An empirical investigation of this algorithm has found it to be superior to the traditional CMAC training algorithm, both in accuracy and time required to learn mappings between input vectors and class labels.	algorithm;artificial neural network;assignment (computer science);biconnected component;cerebellar model articulation controller;data mining;database;machine learning;nl-complete;numerical aperture;one-key mac;time complexity	David Cornforth	2006	International Journal of Computational Intelligence and Applications	10.1142/S1469026806002015	time complexity;biological classification;computer science;artificial intelligence;machine learning;pattern recognition	ML	12.75232025041433	-39.90376999786625	107312
dad895b572e01a6a83bddbac1662e9bc629aa693	deep unsupervised representation learning for remote sensing images		Scene classification plays a key role in interpreting the remotely sensed high-resolution images. With the development of deep learning, supervised learning in classification of Remote Sensing with convolutional networks (CNNs) has been frequently adopted. However, researchers paid less attention to unsupervised learning in remote sensing with CNNs. In order to filling the gap, this paper proposes a set of CNNs called Multiple lAyeR feaTure mAtching(MARTA) generative adversarial networks (GANs) to learn representation using only unlabeled data. There will be two models of MARTA GANs involved: (1) a generative model G that captures the data distribution and provides more training data; (2) a discriminative model D that estimates the possibility that a sample came from the training data rather than G and in this way a well-formed representation of dataset can be learned. Therefore, MARTA GANs obtain the state-of-the-art results which outperform the results got from UC-Merced Land-use dataset and Brazilian Coffee Scenes dataset.	deep learning;discriminative model;feature learning;generative adversarial networks;generative model;image resolution;machine learning;supervised learning;uc browser;unsupervised learning;well-formed element	DaoYu Lin	2016	CoRR		computer vision;artificial intelligence;machine learning;pattern recognition;statistics	ML	24.167757630788408	-49.78085158339769	107395
857c81d7187a06cbc4eb6de925276e1e2e768b85	controlling complex networks with compensatory perturbations	emergency response;food web;ecosystem management;complex network;chaotic dynamics;information network;genetic network;disordered system;neural network	The response of complex networks to perturbations is of utmost importance in areas as diverse as ecosystem management, emergency response, and cell reprogramming. A fundamental property of networks is that the perturbation of one node can affect other nodes, in a process that may cause the entire or substantial part of the system to change behavior and possibly collapse. Recent research in metabolic and food-web networks has demonstrated the concept that network damage caused by external perturbations can often be mitigated or reversed by the application of compensatory perturbations. Compensatory perturbations are constrained to be physically admissible and amenable to implementation on the network. However, the systematic identification of compensatory perturbations that conform to these constraints remains an open problem. Here, we present a method to construct compensatory perturbations that can control the fate of general networks under such constraints. Our approach accounts for the full nonlinear behavior of real complex networks and can bring the system to a desirable target state even when this state is not directly accessible. Applications to genetic networks show that compensatory perturbations are effective even when limited to a small fraction of all nodes in the network and that they are far more effective when limited to the highest-degree nodes. The approach is conceptually simple and computationally efficient, making it suitable for the rescue, control, and reprogramming of large complex networks in various domains.	admissible heuristic;algorithmic efficiency;complex network;gene regulatory network;nonlinear system;perturbation theory	Sean P. Cornelius;William L. Kath;Adilson E. Motter	2011	CoRR		biology;ecosystem management;simulation;artificial intelligence;artificial neural network;complex network;food web	ML	14.075300377072919	-52.01216659727118	107603
c0c5ad1fd74d9fffc78f05bf5400ce8547265654	evolutionary dbn for the customers' sentiment classification with incremental rules		An increasing number of reviews from the customers have been available online. Thus, sentiment classification for such reviews has attracted more and more attention from the natural language processing (NLP) community. Related literature has shown that sentiment analysis can benefit from Deep Belief Networks (DBN). However, determining the structure of the deep network and improving its performance still remains an open question. In this paper, we propose a sophisticated algorithm based on fuzzy mathematics and genetic algorithm, called evolutionary fuzzy deep belief networks with incremental rules (EFDBNI). We evaluate our proposal using empirical data sets that are dedicated for sentiment classification. The results show that EFDBNI brings out significant improvement over existing methods.		Ping Yang;Dan Wang;Xiaolin Du;Meng Wang	2018		10.1007/978-3-319-95786-9_9	fuzzy mathematics;fuzzy logic;genetic algorithm;sentiment analysis;deep belief network;fuzzy set;deep learning;machine learning;artificial intelligence;computer science;data set	NLP	11.233397976591885	-40.384845612401605	107863
25e4a58fe7bcd94aa46021bac645243d027131ef	fast k most similar neighbor classifier for mixed data based on approximating and eliminating	approximating eliminating search algorithms;triangle inequality;search algorithm;satisfiability;nearest neighbor;fast nearest neighbor search;pattern recognition;k nearest neighbor;nearest neighbors rule;nearest neighbor search;mixed data	The k nearest neighbor (k-NN) classifier has been a widely used nonparametric technique in Pattern Recognition. In order to decide the class of a new prototype, the k-NN classifier performs an exhaustive comparison between the prototype to classify (query) and the prototypes in the training set T. However, when T is large, the exhaustive comparison is expensive. To avoid this problem, many fast k-NN algorithms have been developed. Some of these algorithms are based on Approximating-Eliminating search. In this case, the Approximating and Eliminating steps rely on the triangle inequality. However, in soft sciences, the prototypes are usually described by qualitative and quantitative features (mixed data), and sometimes the comparison function does not satisfy the triangle inequality. Therefore, in this work, a fast k most similar neighbour classifier for mixed data (AEMD) is presented. This classifier consists of two phases. In the first phase, a binary similarity matrix among the prototypes in T is stored. In the second phase, new Approximating and Eliminating steps, which are not based on the triangle inequality, are presented. The proposed classifier is compared against other fast k-NN algorithms, which are adapted to work with mixed data. Some experiments with real datasets are presented		Selene Hernández-Rodríguez;Jesús Ariel Carrasco-Ochoa;José Francisco Martínez Trinidad	2008		10.1007/978-3-540-68125-0_66	m-tree;best bin first;computer science;machine learning;pattern recognition;data mining;mathematics;nearest neighbor search;k-nearest neighbors algorithm	DB	11.279099463722495	-42.82909499621178	108226
02c18c62fa74fcf7ede6ad8b4273c38d1a9c2e59	fast optimization methods for l1 regularization: a comparative study and two new approaches	optimal method;loss function;comparative study;feature selection;gradient projection method;constrained optimization problem	L1 regularization is effective for feature selection, but the resulting optimization is challenging due to the non-differentiability of the 1-norm. In this paper we compare state-of-the-art optimization techniques to solve this problem across several loss functions. Furthermore, we propose two new techniques. The first is based on a smooth (differentiable) convex approximation for the L1 regularizer that does not depend on any assumptions about the loss function used. The other technique is a new strategy that addresses the non-differentiability of the L1-regularizer by casting the problem as a constrained optimization problem that is then solved using a specialized gradient projection method. Extensive comparisons show that our newly proposed approaches consistently rank among the best in terms of convergence speed and efficiency by measuring the number of function evaluations required.	approximation;constrained optimization;constraint (mathematics);feature selection;loss function;manifold regularization;mathematical optimization;optimization problem;stochastic gradient descent	Mark W. Schmidt;Glenn Fung;Rómer Rosales	2007		10.1007/978-3-540-74958-5_28	optimization problem;mathematical optimization;constrained optimization;proximal gradient methods for learning;combinatorics;random search;machine learning;mathematics;random optimization	ML	22.164214177685917	-38.24796772126652	108712
5bfdc0efb121f04d2c05eae32219461bf6c7c45a	a local online learning approach for non-linear data		The efficiency and scalability of online learning methods make them a popular choice for solving the learning problems with big data and limited memory. Most of the existing online learning approaches are based on global models, which consider the incoming example as linear separable. However, this assumption is not always valid in practice. Therefore, local online learning framework was proposed to solve non-linear separable task without kernel modeling. Weights in local online learning framework are based on the first-order information, thus will significantly limit the performance of online learning. Intuitively, the second-order online learning algorithms, e.g., Soft Confidence-Weighted (SCW), can significantly alleviate this issue. Inspired by the second-order algorithms and local online learning framework, we propose a Soft Confidence-Weighted Local Online Learning (SCW-LOL) algorithm, which extends the single hyperplane SCW to the case with multiple local hyperplanes. Those local hyperplanes are connected by a common component and will be optimized simultaneously. We also examine the theoretical relationship between the single and multiple hyperplanes. The extensive experimental results show that the proposed SCW-LOL learns an online convergence boundary, overall achieving the best performance over almost all datasets, without any kernel modeling and parameter tuning.		Xinxing Yang;Jun Zhou;Peilin Zhao;Cen Chen;Chaochao Chen;Xiaolong Li	2018		10.1007/978-3-319-93037-4_34	machine learning;artificial intelligence;kernel (linear algebra);hyperplane;computer science;big data;scalability;nonlinear system;separable space;convergence (routing)	ML	20.478362912408343	-38.99587367186507	108727
669ef60e22e57969417301fdf6dd5eb7cad85a2f	neural network committee to predict the amen of poultry feedstuffs	metabolizable energy;committee machine;hpd intervals;ensemble estimators	A committee of neural networks is the aggregation of two or more neural networks for making overall predictions that are supposedly more accurate than those obtained by the individual networks. The objective of this paper was to assign some uncertainty over the predictions of neural networks, using a network committee to estimate the nitrogen-corrected metabolizable energy (AMEn) values of the energetic and protein concentrate feedstuffs for broilers. The dataset used to implement each expert network contains 568 experimental results. Another dataset with 48 bioassay results was used as test data. From several implemented multilayer perceptrons, the networks that presented the best generalization performance were selected to constitute the committee. The percentage of correct predictions was used as the criterion to compare committees that contained different numbers of networks. The highest probability density intervals were obtained for each feedstuff in the test data in this comparison. The estimator that ensured more accurate predictions was selected. The highest accuracy for predicting the AMEn values of concentrate feedstuffs for broilers was achieved by a committee with 1,000 networks with the use of the mode of the empirical distribution obtained from 1,000 estimated values of the AMEn. The accuracy of the models was evaluated based on their values of error measures between the observed and predicted values, in which the mode of the empirical distribution presented lower values of mean squared error (MSE = 45,285.43), mean absolute deviation (MAD = 177.66) and mean absolute percentage error (MAPE = 5.97 %) compared to the mean and the median.	approximation error;artificial neural network;expert network;mad;mean squared error;multilayer perceptron;test data	F. C. M. Q. Mariano;R. R. Lima;R. R. Alvarenga;P. B. Rodrigues;Wilian Soares Lacerda	2014	Neural Computing and Applications	10.1007/s00521-014-1680-3	econometrics;data mining;statistics	ML	10.156642864391872	-48.773390516708425	108796
a2b554d79530c8d8900deb867742d5e586eff116	electricity fraud detection using committee semi-supervised learning		Electricity fraud results in significant losses to utilities. This paper proposes the use of a semi-supervised learning framework to derive an electricity fraud detector from data lacking information on the presence of fraud for the majority of samples. Utilities are only able to make a limited number of inspections, resulting in a lack of data representing cases of fraud. Using a co-training by committee semi-supervised learning framework, the detection performance is improved in comparison to the use of supervised models only trained with labeled data. The framework starts by training a random forest classifier on the labeled data. Next, the unlabeled data that the model can classify with the most confidence is added iteratively to the set of labeled samples, augmenting the data available for model training. The electricity fraud detector achieves a classification performance of 84% true positive rate, 11% false positive rate and 0.89 area under the receiver operating characteristic curve under a positive class balance of 5% and 90% unlabeled samples in the training data.	artificial neural network;co-training;deep learning;generative model;mathematical optimization;neural networks;performance evaluation;random forest;receiver operating characteristic;semi-supervised learning;semiconductor industry;sensitivity and specificity;software inspection;supervised learning;test set	Joaquim L. Viegas;Emmelieke M C van der Linden;Susana M. Vieira	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8489389	false positive rate;semi-supervised learning;labeled data;feature extraction;artificial intelligence;receiver operating characteristic;data modeling;computer science;training set;pattern recognition;random forest	ML	16.623670581390158	-41.048449693186	109103
1595dd4a54c78637b0e9adec0b8d1a9490ea98b6	extending case adaptation with automatically-generated ensembles of adaptation rules		Case-based regression often relies on simple case adaptati on methods. This paper investigates new approaches to enriching the ada ptation capabilities of case-based regression systems, based on the use of ensemble s of adaptation rules generated from the case base. The paper explores both local a nd global methods for generating adaptation rules from the case base, and p resents methods for ranking the generated rules and combining the resulting ens emble of adaptation rules to generate new solutions. It tests these methods in fiv e standard domains, evaluating their performance compared to four baseline met hods, standard k-NN, linear regression, locally weighted linear regression, an d ensemble of k-NN predictors with different feature subsets. The results dem onstrate that the proposed method generally outperforms the baselines and that t he ccuracy of adaptation based on locally-generated rules is highly competit iv with that of global rule-generation methods with much greater computational c ost.	ada;baseline (configuration management);computation;ensemble learning;k-nearest neighbors algorithm;scope (computer science);yet another	Vahid Jalali;David B. Leake	2013		10.1007/978-3-642-39056-2_14	machine learning;computer science;artificial intelligence;baseline (configuration management);linear regression;mean absolute error;ranking	AI	11.183739181632767	-40.758987931759336	109277
af753a040fc0667737acf6d062c66c2309dd8621	optimizing clustering to promote data diversity when generating an ensemble classifier		In this paper, we propose a method to generate an optimized ensemble classifier. In the proposed method, a diverse input space is created by clustering training data incrementally within a cycle. A cycle is one complete round that includes clustering, training, and error calculation. In each cycle, a random upper bound of clustering is chosen and data clusters are generated. A set of heterogeneous classifiers are trained on all generated clusters to promote structural diversity. An ensemble classifier is formed in each cycle and generalization error of that ensemble is calculated. This process is optimized to find the set of classifiers which can have the lowest generalization error. The process of optimization terminates when generalization error can no longer be minimized. The cycle with the lowest error is then selected and all trained classifiers of that particular cycle are passed to the next stage. Any classifier having lower accuracy than the average accuracy of the pool is discarded, and the remaining classifiers form the proposed ensemble classifier. The proposed ensemble classifier is tested on classification benchmark datasets from UCI repository. The results are compared with existing state-of-the-art ensemble classifier methods including Bagging and Boosting. It is demonstrated that the proposed ensemble classifier performs better than the existing ensemble methods.	benchmark (computing);boosting (machine learning);cluster analysis;ensemble learning;generalization error;heterogeneous system architecture;mathematical optimization;optimizing compiler	Zohaib M. Jan;Brijesh Verma;Sam Fletcher	2018		10.1145/3205651.3208245	artificial neural network;artificial intelligence;boosting (machine learning);machine learning;computer science;ensemble learning;classifier (linguistics);cluster analysis;particle swarm optimization;upper and lower bounds;evolutionary algorithm	ML	14.896489479408032	-39.99033076991458	109280
f4d6d7f54f96e204095549630c1c20dad767ba65	client-friendly classification over random hyperplane hashes	locality sensitive hashing;empirical evidence;computational complexity;high dimensional data;communication cost;feature selection;error bound;bag of words	In this work, we introduce a powerful and general feature representation based on a locality sensitive hash scheme called random hyperplane hashing. We are addressing the problem of centrally learning (linear) classification models from data that is distributed on a number of clients, and subsequently deploying these models on the same clients. Our main goal is to balance the accuracy of individual classifiers and different kinds of costs related to their deployment, including communication costs and computational complexity. We hence systematically study how well schemes for sparse high-dimensional data adapt to the much denser representations gained by random hyperplane hashing, how much data has to be transmitted to preserve enough of the semantics of each document, and how the representations affect the overall computational complexity. This paper provides theoretical results in the form of error bounds and margin based bounds to analyze the performance of classifiers learnt over the hash-based representation. We also present empirical evidence to illustrate the attractive properties of random hyperplane hashing over the conventional baseline representation of bag of words with and without feature selection.	angularjs;bag-of-words model;baseline (configuration management);computational complexity theory;feature selection;hash function;linear classifier;locality of reference;locality-sensitive hashing;preprocessor;random projection;software deployment;sparse matrix	Shyamsundar Rajaram;Martin Scholz	2008		10.1007/978-3-540-87481-2_17	feature hashing;hash table;empirical evidence;dynamic perfect hashing;computer science;artificial intelligence;bag-of-words model;theoretical computer science;machine learning;universal hashing;pattern recognition;data mining;mathematics;k-independent hashing;locality preserving hashing;computational complexity theory;feature selection;2-choice hashing;locality-sensitive hashing;statistics;clustering high-dimensional data	ML	20.17507547335361	-45.69148900810123	109397
3aea3d7e736fa558449c66adadc6bb197ecedd83	max-entropy feed-forward clustering neural network		The outputs of non-linear feed-forward neural network are positive, which could be treated as probability when they are normalized to one. If we take Entropy-Based Principle into consideration, the outputs for each sample could be represented as the distribution of this sample for different clusters. Entropy-Based Principle is the principle with which we could estimate the unknown distribution under some limited conditions. As this paper defines two processes in Feed-Forward Neural Network, our limited condition is the abstracted features of samples which are worked out in the abstraction process. And the final outputs are the probability distribution for different clusters in the clustering process. As Entropy-Based Principle is considered into the feed-forward neural network, a clustering method is born. We have conducted some experiments on six open UCI datasets, comparing with a few baselines and applied purity as the measurement . The results illustrate that our method outperforms all the other baselines that are most popular clustering methods. Keywords—Feed-Forward Neural Network, Clustering, Max-Entropy Principle, Probabilistic Models	artificial neural network;baseline (configuration management);cluster analysis;experiment;feedforward neural network;neural networks;nonlinear system;overfitting;pure function	Han Xiao;Xiaoyan Zhu	2015	CoRR		correlation clustering;fuzzy clustering;machine learning;data mining;mathematics;statistics	ML	17.84652224235712	-40.37340729984663	109447
d272f746b84f213eb3e372647b8e612e81fcb17c	multivariable stream data classification using motifs and their temporal relations	classification algorithm;stream data mining;data mining;motifs;multivariable stream;data model;temporal relations;feature extraction;stream data modeling;classification accuracy;data classification	Multivariable stream data is becoming increasingly common as diverse types of sensor devices and networks are deployed. Building accurate classification models for such data has attracted a lot of attention from the research community. Most of the previous works, however, relied on features extracted from individual streams, and did not take into account the dependency relations among the features within and across the streams. In this work, we propose new classification models that exploit temporal relations among features. We showed that consideration of such dependencies does significantly improve the classification accuracy. Another benefit of employing temporal relations is the improved interpretability of the resulting classification models, as the set of temporal relations can be easily translated to a rule using a sequence of inter-dependent events characterizing the class. We evaluated the proposed scheme using different classification models including the Naive Bayesian, TFIDF, and vector distance models. We showed that the proposed model can be a useful addition to the set of existing stream classification algorithms.	sequence motif	Sungbo Seo;Jaewoo Kang;Keun Ho Ryu	2009	Inf. Sci.	10.1016/j.ins.2009.06.036	feature extraction;data model;computer science;machine learning;pattern recognition;data mining;data stream mining	AI	11.742355249743701	-46.53133837342379	109450
22a353d757c56f2e04a77fac9f71745c496b22cf	online bayesian multiple kernel bipartite ranking	variational approximation;learning frameworks;data augmentation;decision functions;area under the roc curve;kernel selection;auc maximizations;posterior distributions	Bipartite ranking aims to maximize the area under the ROC curve (AUC) of a decision function. To tackle this problem when the data appears sequentially, existing online AUC maximization methods focus on seeking a point estimate of the decision function in a linear or predefined single kernel space, and cannot learn effective kernels automatically from the streaming data. In this paper, we first develop a Bayesian multiple kernel bipartite ranking model, which circumvents the kernel selection problem by estimating a posterior distribution over the model weights. To make our model applicable to streaming data, we then present a kernelized online Bayesian passive-aggressive learning framework by maintaining a variational approximation to the posterior based on data augmentation. Furthermore, to efficiently deal with large-scale data, we design a fixed budget strategy which can effectively control online model complexity. Extensive experimental studies confirm the superiority of our Bayesian multi-kernel approach.	approximation;computer multitasking;convolutional neural network;decision boundary;expectation–maximization algorithm;kernel (operating system);kernel method;multi-source;receiver operating characteristic;selection algorithm;source data;stream (computing);streaming media;user space;variational principle	Changying Du;Changde Du;Guoping Long;Qing He;Yucheng Li	2016			kernel;kernel embedding of distributions;radial basis function kernel;machine learning;pattern recognition;mathematics;variable kernel density estimation;statistics	ML	20.747776043689985	-39.977202061749566	109650
17307b4925959a15b2af55bdbb861df41ab879d2	k-local hyperplane and convex distance nearest neighbor algorithms	prior knowledge;tangent distance;nearest neighbor;k nearest neighbor	Guided by an initial idea of building a complex (non linear) d ecision surface with maximalocal margin in input space, we give a possible geometrical intuition as to why K-Nearest Neighbor (KNN) al gorithms often perform more poorly than SVMs on classification tasks. We then propose modified K-Nearest Neighbor algorithms to overcome the perceived problem. The approach is similar in spirit to Tangent Distance , but with invariances inferred from the local neighborhood rath er than prior knowledge. Experimental results on real world classificati on asks suggest that the modified KNN algorithms often give a dramatic im provement over standard KNN and perform as well or better than SVMs .	flaw hypothesis methodology;k-nearest neighbors algorithm;linear system	Pascal Vincent;Yoshua Bengio	2001			large margin nearest neighbor;computer science;machine learning;pattern recognition;data mining;mathematics;nearest neighbor search;k-nearest neighbors algorithm	ML	23.51118884302284	-40.98322988640381	109687
859fae0a2329c631031fc26179520973cd84bb1d	convolutional neural networks for breast cancer screening: transfer learning with exponential decay		In this paper, we propose a Computer Assisted Diagnosis (CAD) system based on a deep Convolutional Neural Network (CNN) model, to build an end-to-end learning process that classifies breast mass lesions. We investigate the impact that has transfer learning when large data is scarce, and explore the proper way to fine-tune the layers to learn features that are more specific to the new data. The proposed approach showed better performance compared to other proposals that classified the same dataset. 1 Background and objectives Breast cancer is the most common invasive disease among women [Siegel et al., 2014] Optimistically, an early diagnosis of the disease increases the chances of recovery dramatically and as such, makes the early detection crucial. Mammography is the recommended screening technique, but it is not enough, we also need the radiologist expertise to check the mammograms for lesions and give a diagnosis, which can be a very challenging task[Kerlikowske et al., 2000]. Radiologists often resort to biopsies and this ends up adding exorbitant expenses to an already burdened patient and health care system [Sickles, 1991]. We propose a Computer Assisted Diagnosis (CAD) system, based on a deep Convolutional Neural Network (CNN) model, designed to be used as a “second-opinion” to help the radiologist give more accurate diagnoses. Deep Learning requires large datasets to train networks of a certain depth from scratch, which are lacking in the medical domain especially for breast cancer. Transfer learning proved to be efficient to deal with little data, even if the knowledge transfer is between two very different domains [Shin et al., 2016]. But still using the technique can be tricky, especially with medical datasets that tend to be unbalanced and limited. And when using the state-of-the art CNNs which are very deep, the models are highly inclined to suffer from overfitting even with the use of many tricks like data augmentation, regularization and dropout. The number of layers to fine-tune and the optimization strategy play a substantial role on the overall performance [Yosinski et al., 2014]. This raises few questions: • Is Transfer Learning really beneficial for this application? • How can we avoid overfitting with our small dataset ? • How much fine-tuning do we need? and what is the proper way to do it? 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. ar X iv :1 71 1. 10 75 2v 1 [ cs .C V ] 2 9 N ov 2 01 7 We investigate the proper way to perform transfer learning and fine-tuning, which will allow us to take advantage of the pre-trained weights and adapt them to our task of interest. We empirically analyze the impact of the fine-tuned fraction on the final results, and we propose to use an exponentially decaying learning rate to customize all the pre-trained weights from ImageNet[Deng et al., 2009] and make them more suited to our type of data. The best model can be used as a baseline to predict if a new “never-seen” breast mass lesion is benign or malignant.	baseline (configuration management);computer-aided design;convolutional neural network;deep learning;dropout (neural networks);end-to-end principle;information processing;mathematical optimization;medical imaging;nips;overfitting;radiology;unbalanced circuit	Hiba Chougrad;Hamid Zouaki;Omar Alheyane	2017	CoRR		breast cancer screening;transfer of learning;artificial intelligence;machine learning;convolutional neural network;pattern recognition;computer science;exponential decay	AI	21.930195355780114	-50.20324174161546	109804
81b026220f5fb8bcc4089539c61ae4be8bd113b9	fast support vector data description using k-means clustering	support vector data description;large data sets;support vector;divide and conquer;k means clustering	Support Vector Data Description (SVDD) has a limitation for dealing with a large data set in which computational load drastically increases as training data size becomes large. To handle this problem, we propose a new fast SVDD method using K-means clustering method. Our method uses divide-and-conquer strategy; trains each decomposed subproblems to get support vectors and retrains with the support vectors to find a global data description of a whole target class. The proposed method has a similar result to the original SVDD and reduces computational cost. Through experiments, we show efficiency of our method.	algorithmic efficiency;cluster analysis;computation;experiment;k-means clustering	Pyo Jae Kim;Hyung Jin Chang;Dong Sung Song;Jin Young Choi	2007		10.1007/978-3-540-72395-0_64	support vector machine;divide and conquer algorithms;computer science;theoretical computer science;machine learning;data mining;mathematics;k-means clustering	ML	20.62572577201689	-39.37639056322833	109826
c677c38a3b1464407b5e589c6267636236b08463	the use of kernel principal component analysis to model data distributions	density estimation;principal components;kernel pca;support vector machines;kernel functions;feature extraction;kernel principal component analysis;kernel function;principal component;support vector machine	We describe the use of kernel principal component analysis (KPCA) to model data distributions in high-dimensional spaces. We show that a previous approach to representing non-linear data constraints using KPCA is not generally valid, and introduce a new ‘proximity to data’ measure that behaves correctly. We investigate the relation between this measure and the actual density for various low-dimensional data distributions. We demonstrate the effectiveness of the method by applying it to the higher-dimensional case of modelling an ensemble of images of handwritten digits, showing how it can be used to extract the digit information from noisy input images.	kernel principal component analysis	Carole J. Twining;Christopher J. Taylor	2003	Pattern Recognition	10.1016/S0031-3203(02)00051-1	kernel (statistics);kernel embedding of distributions;kernel principal component analysis;machine learning;kernel method;principal component regression;principal component analysis;artificial intelligence;statistics;pattern recognition;radial basis function kernel;variable kernel density estimation;mathematics	Vision	23.764932862744505	-40.16960380513278	109951
0dbaa828fc08e89395033b6b92139bdde43e02d3	radial basis function cascade correlation networks	artificial neural networks;radial basis function;bootstrap latin partition;cascade correlation	A cascade correlation learning architecture has been devised for the first time for radial basis function processing units. The proposed algorithm was evaluated with two synthetic data sets and two chemical data sets by comparison with six other standard classifiers. The ability to detect a novel class and an imbalanced class were demonstrated with synthetic data. In the chemical data sets, the growth regions of Italian olive oils were identified by their fatty acid profiles; mass spectra of polychlorobiphenyl compounds were classified by chlorine number. The prediction results by bootstrap Latin partition indicate that the proposed neural network is useful for pattern recognition.	algorithm;artificial neural network;pattern recognition;radial (radio);radial basis function;synthetic data	Weiying Lu;Peter de Boves Harrington	2009	Algorithms	10.3390/a2031045	radial basis function;computer science;artificial intelligence;machine learning;mathematics;radial basis function network;artificial neural network;statistics	ML	13.568761903679594	-45.63325951384388	110032
4683a3e04082d2030119d09d0a0313937843021e	cautious classification with nested dichotomies and imprecise probabilities		In some applications of machine learning and information retrieval (e.g. medical diagnosis, image recognition, pre-classification...), it can be preferable to provide less informative but more reliable predictions. This can be done by making partial predictions in the form of class subsets when the available information is insufficient to provide a reliable unique class. Imprecise probabilistic approaches offer nice tools to learn models from which such cautious predictions can be produced. However, the learning and inference processes of such models are computationally harder than their precise counterparts. In this paper, we introduce and study a particular binary decomposition strategy, nested dichotomies, that offer computational advantages in both the learning (due to the binarization process) and the inference (due to the decomposition strategy) processes. We show with experiments that these computational advantages do not lower the performances of the classifiers, and can even improve them when the class space has some structure.		Gen Yang;Sébastien Destercke;Marie-Hélène Masson	2017	Soft Comput.	10.1007/s00500-016-2287-7	artificial intelligence;computer science;nice;machine learning;mathematical optimization;ordinal regression;dichotomy;probabilistic logic;inference;multiclass classification;pattern recognition	NLP	16.682428888268454	-38.8123635048458	110088
cb6f31c5840a784be7d381f03d5860238412636e	multiple instance learning with random forests and applications in industrial optical inspection	004;004 data processing computer science;310;670;310 general statistics;670 manufacturing	Automatic defect detection in industrial optical inspection requires algorithms that can learn from data. A special challenge is data with incomplete labels. One of the methods that the field of machine learning has brought forth to deal with incomplete labels is multiple instance learning. One trait of this setting is that it groups datapoints (instances) into bags. We propose a novel method to predict bag probabilities from given instance probabilities that has the advantage that its results do not depend on bag size. Also, we propose an extension of the multiple instance model that allows the user to steer the number of instances that are classified as positive. We implement these methods with an algorithm based on the well-known random forest classifier. Results on a standard benchmark dataset show competitive performance. Furthermore, we apply this algorithm to image data that reflects the challenges of industrial optical inspection, and we show that in this setting it improves over the standard random forest.	multiple instance learning;random forest	Matthias Wieler	2014			instance-based learning;computer science;machine learning;pattern recognition;data mining	ML	15.889374668935393	-39.88392893453209	110215
a3e196bca65680e27062e201e63fd2cabd51aca5	transformation based ensembles for time series classification		Until recently, the vast majority of data mining time series classification (TSC) research has focused on alternative distance measures for 1-Nearest Neighbour (1-NN) classifiers based on either the raw data, or on compressions or smoothing of the raw data. Despite the extensive evidence in favour of 1-NN classifiers with Euclidean or Dynamic Time Warping distance, there has also been a flurry of recent research publications proposing classification algorithms for TSC. Generally, these classifiers describe different ways of incorporating summary measures in the time domain into more complex classifiers. Our hypothesis is that the easiest way to gain improvement on TSC problems is to simply transform into an alternative data space where the discriminatory features are more easily detected. To test our hypothesis, we perform a range of benchmarking experiments in the time domain, before evaluating nearest neighbour classifiers on data transformed into the power spectrum, the autocorrelation function, and the principal component space. We demonstrate that on some problems there is dramatic improvement in the accuracy of classifiers built on the transformed data over classifiers built in the time domain, but that there is also a wide variance in accuracy for a particular classifier built on different data transforms. To overcome this variability, we propose a simple transformation based ensemble, then demonstrate that it improves performance and reduces the variability of classifiers built in the time domain only. Our advice to a practitioner with a real world TSC problem is to try transforms before developing a complex classifier; it is the easiest way to get a potentially large increase in accuracy, and may provide further insights into the underlying relationships that characterise the problem.	authorization;autocorrelation;boosting (machine learning);c4.5 algorithm;cross-validation (statistics);data mining;dataspaces;diversification (finance);dynamic time warping;embedded system;experiment;filter (signal processing);heart rate variability;k-nearest neighbors algorithm;kullback–leibler divergence;model selection;naive bayes classifier;preprocessor;principal component analysis;remote desktop services;resampling (statistics);smoothing;spatial variability;spectral density;statistical classification;time series;weka	Anthony J. Bagnall;Luke M. Davis;Jon Hills;Jason Lines	2012		10.1137/1.9781611972825.27	random subspace method;machine learning;pattern recognition;data mining;mathematics;statistics	ML	15.306516569203772	-47.24770215891933	110231
4af34f3ae21ca71f55c4d429d50e63cf005d3018	a robust unsupervised consensus control chart pattern recognition framework	unsupervised learning;control chart pattern recognition;k means;data mining;spectral clustering;graph partitioning;machine learning;ensemble methods;unsupervised learing;clustering;consensus clustering	We propose an unsupervised consensus approach for the control chart pattern recognition problem.We provide computational evidence of consensus clustering robustness.We provide motivation for further research on unsupervised learning for the CCPR problem. Early identification and detection of abnormal patterns is vital for a number of applications. In manufacturing for example, slide shifts and alterations of patterns might be indicative of some production process anomaly, such as machinery malfunction. Usually due to the continuous flow of data, monitoring of manufacturing processes and other types of applications requires automated control chart pattern recognition (CCPR) algorithms. Most of the CCPR literature consists of supervised classification algorithms. Fewer studies consider unsupervised versions of the problem. Despite the profound advantage of unsupervised methodology for less manual data labeling their use is limited due to the fact that their performance is not robust enough and might vary significantly from one algorithm to another. In this paper, we propose the use of a consensus clustering framework that takes care of this shortcoming and produces results that are robust with respect to the chosen pool of algorithms. Computational results show that the proposed method achieves not less than 79.10% G-mean with most of test instances achieving higher than 90%. This happens even when in the algorithmic pool are included algorithms with performance less than 15%. To our knowledge, this is the first paper proposing an unsupervised consensus learning approach in CCPR. The proposed approach is promising and provides a new research direction in unsupervised CCPR literature.	pattern recognition	Siavash Haghtalab;Petros Xanthopoulos;Kaveh Madani	2015	Expert Syst. Appl.	10.1016/j.eswa.2015.04.069	unsupervised learning;computer science;graph partition;machine learning;consensus clustering;pattern recognition;data mining;cluster analysis;spectral clustering;k-means clustering	Vision	15.772332140820506	-41.82204255937667	110374
6dc85ecfc83b8ea72017b69f404ecd8ed6f11b72	exemplar learning for extremely efficient anomaly detection in real-valued time series	exemplar learning;anomaly detection;time series	We investigate algorithms for efficiently detecting anomalies in real-valued one-dimensional time series. Past work has shown that a simple brute force algorithm that uses as an anomaly score the Euclidean distance between nearest neighbors of subsequences from a testing time series and a training time series is one of the most effective anomaly detectors. We investigate a very efficient implementation of this method and show that it is still too slow for most real world applications. Next, we present a new method based on summarizing the training time series with a small set of exemplars. The exemplars we use are feature vectors that capture both the high frequency and low frequency information in sets of similar subsequences of the time series. We show that this exemplar-based method is both much faster than the efficient brute force method as well as a prediction-based method and also handles a wider range of anomalies. We compare our algorithm across a large variety of publicly available time series and encourage others to do the same. Our exemplar-based algorithm is able to process time series in minutes that would take other methods days to process.	algorithm;anomaly detection;brute-force search;euclidean distance;feature vector;sensor;time series	Michael Jones;Daniel Nikovski;Makoto Imamura;Takahisa Hirata	2015	Data Mining and Knowledge Discovery	10.1007/s10618-015-0449-3	anomaly detection;computer science;machine learning;time series;pattern recognition;data mining;statistics	ML	15.614319576193928	-47.387014808508695	110500
c25c4702b7e8fe766fa9512d1e490fe1f791947d	impact of noise and data sampling on stability of feature selection	stability criteria;high dimensionality;noise radio frequency noise measurement gene expression stability criteria niobium;niobium;class imbalance;classification model data sampling technique feature selection stability high dimensionality problem data mining feature selection technique feature ranker biology dataset artificial class noise gain ratio;data mining;classification;noise measurement;bioinformatics feature selection class imbalance noise injection stability classification;stability;gene expression;stability data mining noise pattern classification sampling methods;radio frequency;sampling technique;pattern classification;feature selection;noise injection;sampling methods;noise;bioinformatics	High dimensionality is one of the major problems in data mining, occurring when there is a large abundance of attributes. One common technique used to alleviate high dimensionality is feature selection, the process of selecting the most relevant attributes and removing irrelevant and redundant ones. Much research has been done towards evaluating the performance of classifiers before and after feature selection, but little work has been done examining how sensitive the selected feature subsets are to changes (additions/deletions) in the dataset. In this study we evaluate the robustness of six commonly used feature selection techniques, investigating the impact of data sampling and class noise on the stability of feature selection. All experiments are carried out with six commonly used feature rankers on four groups of datasets from the biology domain. We employ three sampling techniques, and generate artificial class noise to better simulate real-world datasets. The results demonstrate that although no ranker consistently outperforms the others, Gain Ratio shows the least stability on average. Additional tests using our feature rankers for building classification models also show that a feature ranker's stability is not an indicator of its performance in classification.	data mining;display resolution;experiment;feature selection;feature vector;image noise;radio frequency;relevance;sampling (signal processing);simulation	Ahmad Abu Shanab;Taghi M. Khoshgoftaar;Randall Wald	2011	2011 10th International Conference on Machine Learning and Applications and Workshops	10.1109/ICMLA.2011.74	sampling;computer science;machine learning;pattern recognition;data mining;feature selection;statistics	Vision	13.423806849360234	-42.492965700995946	110976
5994ae3c6fe98099d4bc4db491434cd2a560ce6b	tackling label noise with multi-class decomposition using fuzzy one-class support vector machines	complexity theory;support vector machines;data uncertainty machine learning label noise one class classification multi class decomposition fuzzy classifiers;training;multiclass classification multiclass decomposition fuzzy one class support vector machines class label noise data level difficulty highly complex intraclass noise binary classifiers one class classification decomposition robust data description learned decision boundary statistical analysis fuzzy one class classifier decomposition;support vector machines data description fuzzy set theory pattern classification statistical analysis;noise measurement;fuzzy logic;training data;robustness;support vector machines noise measurement training robustness training data complexity theory fuzzy logic	Class label noise is a data-level difficulty associated with training objects with incorrectly assigned labels. This problem may originate from poorly documented historic data, errors during data generation process or mistakes made by human experts. Inclusion of such examples during the training process will mislead the classifier by presenting a falsified class distribution and consequently lead to degradation of models' generalization abilities. This phenomenon becomes even more troublesome in multi-class scenarios that may be affected by highly complex intra-class noise. Decomposition strategies with binary classifiers were proven to alleviate this difficulty by using simplified binary subtasks that are less affected by the noise. In this paper we propose to extend this approach by using the one-class classification decomposition. In this scenario each class has assigned individual one-class classifier that aims at capturing its distinguishing characteristics. This allows to create a robust data description and then apply a dedicated classifier combination in order to reconstruct the original multi-class task. We further extend this concept by using fuzzy one-class classifiers that allow to associate membership values with each training objects. This allows us to reduce the influence of uncertain and potentially noisy samples on the shape of learned decision boundary. Experimental study backed-up with statistical analysis shows that fuzzy one-class classifier decomposition offers an excellent robustness to noise in multi-class classification.	benchmark (computing);binary classification;concept drift;decision boundary;elegant degradation;embedded system;experiment;fuzzy control system;fuzzy logic;hoc (programming language);membership function (mathematics);multiclass classification;one-class classification;qr decomposition;signal-to-noise ratio;support vector machine	Bartosz Krawczyk;José A. Sáez;Michal Wozniak	2016	2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2016.7737786	fuzzy logic;support vector machine;training set;fuzzy classification;computer science;noise measurement;artificial intelligence;machine learning;pattern recognition;data mining;statistics;robustness	Vision	14.974061471173389	-41.58108342312158	111212
cdabcb2a7aa7c1ba54f534670a2f135647a171ca	experimenting multiresolution analysis for identifying regions of different classification complexity	domain complexity estimation;domain analysis;data preprocessing	Systems for assessing the classification complexity of a dataset have received increasing attention in research activities on pattern recognition. These systems typically aim at quantifying the overall complexity of a domain, with the goal of comparing different datasets. In this work, we propose a method for partitioning a dataset into regions of different classification complexity, so to highlight sources of complexity inside the dataset. Experiments have been carried out on relevant datasets, proving the effectiveness of the proposed method.	cluster analysis;experiment;linear separability;multiresolution analysis;pattern recognition;population	Giuliano Armano;Emanuele Tamponi	2014	Pattern Analysis and Applications	10.1007/s10044-014-0446-y	domain analysis;computer science;machine learning;pattern recognition;data mining;data pre-processing	AI	11.202377783014862	-46.15990660131535	111337
7b924a1b684d5a5ec819f9b6fed9031c8f56cb7b	embedding feature selection for large-scale hierarchical classification	memory management;training;computational modeling;logistics;big data;predictive models;optimization	Large-scale Hierarchical Classification (HC) involves datasets consisting of thousands of classes and millions of training instances with high-dimensional features posing several big data challenges. Feature selection that aims to select the subset of discriminant features is an effective strategy to deal with large-scale HC problem. It speeds up the training process, reduces the prediction time and minimizes the memory requirements by compressing the total size of learned model weight vectors. Majority of the studies have also shown feature selection to be competent and successful in improving the classification accuracy by removing irrelevant features. In this work, we investigate various filter-based feature selection methods for dimensionality reduction to solve the large-scale HC problem. Our experimental evaluation on text and image datasets with varying distribution of features, classes and instances shows upto 3x order of speed-up on massive datasets and upto 45% less memory requirements for storing the weight vectors of learned model without any significant loss (improvement for some datasets) in the classification accuracy. Source Code: https://cs.gmu.edu/∼mlbio/featureselection.	big data;computer multitasking;dimensionality reduction;discriminant;feature selection;hierarchical clustering;ibm notes;information theory;mason;multi-task learning;relevance;requirement;run time (program lifecycle phase)	Azad Naik;Huzefa Rangwala	2016	2016 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2016.7840725	computer science;machine learning;pattern recognition;data mining	ML	18.42207752091097	-44.857907594471754	111563
0a61c4c33a5a28435e39431a14a815cd448da40c	online representation learning with single and multi-layer hebbian networks for image classification		Unsupervised learning permits the development of algorithms that are able to adapt to a variety of different datasets using the same underlying rules thanks to the autonomous discovery of discriminating features during training. Recently, a new class of Hebbian-like and local unsupervised learning rules for neural networks have been developed that minimise a similarity matching cost-function. These have been shown to perform sparse representation learning. This study tests the effectiveness of one such learning rule for learning features from images. The rule implemented is derived from a nonnegative classical multidimensional scaling cost-function, and is applied to both single and multi-layer architectures. The features learned by the algorithm are then used as input to an SVM to test their effectiveness in classification on the established CIFAR-10 image dataset. The algorithm performs well in comparison to other unsupervised learning algorithms and multi-layer networks, thus suggesting its validity in the design of a new class of compact, online learning networks.	algorithm;artificial neural network;autonomous robot;british informatics olympiad;computer vision;deep learning;encoder;feature learning;hebbian theory;layer (electronics);learning rule;linear classifier;loss function;machine learning;multidimensional scaling;performance;sparse approximation;sparse dictionary learning;sparse matrix;unsupervised learning	Yanis Bahroun;Andrea Soltoggio	2017		10.1007/978-3-319-68600-4_41	pattern recognition;machine learning;support vector machine;artificial intelligence;artificial neural network;competitive learning;unsupervised learning;hebbian theory;computer science;feature learning;learning rule;leabra	Robotics	22.074948739110997	-45.303988892068055	111609
6183294235afc2241e527929612e61f4f57a791a	decision tree and instance-based learning for label ranking	instance based learning;decision tree;decision tree induction	The label ranking problem consists of learning a model that maps instances to total orders over a finite set of predefined labels. This paper introduces new methods for label ranking that complement and improve upon existing approaches. More specifically, we propose extensions of two methods that have been used extensively for classification and regression so far, namely instance-based learning and decision tree induction. The unifying element of the two methods is a procedure for locally estimating predictive probability models for label rankings.	decision tree;instance-based learning;map	Weiwei Cheng;Jens Christian Hühn;Eyke Hüllermeier	2009		10.1145/1553374.1553395	instance-based learning;decision tree learning;computer science;machine learning;decision tree;pattern recognition;alternating decision tree;incremental decision tree;data mining;mathematics;id3 algorithm;decision stump	ML	15.973661809047847	-38.97721430248775	111958
5f946db224f4d968b93b575eec65e18350aae893	deep self-organizing reservoir computing model for visual object recognition	ridge regression deep learning recurrent neural network reservoir computing self organizing feature map;unsupervised learning neural nets object recognition;noncnn features deep self organizing reservoir computing model visual object recognition kohonen self organizing map shesn network self organizing shesn mcculloch pitts type reservoir neuron radial basis function neuron unsupervised competitive learning weights training deep so shesn model well trained reservoir layers trial and readout learning algorithm layer wise reservoir pretraining mnist benchmark dataset machine learning approaches;reservoirs neurons biological neural networks computational modeling machine learning visualization	Reservoir computing becomes increasingly a hot spot in recent years. In this paper, we propose a deep self-organizing reservoir computing model for visual object recognition. First, through combination of Kohonen's self-organizing map and SHESN network, we present a self-organizing SHESN (SO-SHESN). In the new model, we adopt the same mechanism of generating reservoir as SHESN, but McCulloch-Pitts type reservoir neuron is replaced with radial basis function neuron. Correspondingly, unsupervised competitive learning is exploited to train both input weights and reservoir weights of SO-SHESN. Second, we propose a deep SO-SHESN model through a stack of well-trained reservoir layers. In such a stacked structure, a novel trial-and-readout learning algorithm is used for pre-training of layer-wise reservoir, in which each layer is trained independently from each other. Finally, the experimental results obtained on MNIST benchmark dataset show that our SO-SHESN achieves the test recognition error rate of 5.66%, which improves classical ESN and SHESN by 6.44% and 1.74%, respectively. Furthermore, the test error rate of our deep SO-SHESN could reach up to 1.39%, which outperforms SO-SHESN with single reservoir layer by 4.27% and approximately approaches the state-of-the-art result of 1% among existing traditional machine learning approaches with non-CNN features.	algorithm;benchmark (computing);competitive learning;echo state network;mnist database;machine learning;neuron;organizing (structure);outline of object recognition;radial (radio);radial basis function;reservoir computing;self-organization;self-organizing map;unsupervised learning;walter pitts	Zhidong Deng;Chengzhi Mao;Xiong Chen	2016	2016 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2016.7727351	unsupervised learning;computer science;artificial intelligence;machine learning;pattern recognition;deep learning;deep belief network;artificial neural network	ML	22.430816368066857	-51.216816465146536	111997
4f2e933bb26a9daa887c9b98a0b27f9e5be28a0b	mrbf: a method for predicting hiv-1 drug resistance	multiple linear regression;protease inhibitor;mean square error;prediction accuracy;cross validation;support vector machine;drug resistance;rbf network	This paper presents the MRBF network, a new algorithm adapted from the RBF network, to construct the classifiers for predicting phenotypic resistance on 6 protease inhibitors. The performance of the prediction was measured by 10-fold cross-validation. The results show that MRBF gives the lowest average mean square error (MSE) when compared with the traditional RBF network and multiple linear regression analysis (REG). Moreover, it provides the best average predictive accuracy when compared with HIVdb, REG, and Support Vector Machines (SVM).	algorithm;cross-validation (statistics);feature selection;mean squared error;network function virtualization;preprocessor;radial basis function kernel;radial basis function network;support vector machine	Anantaporn Srisawat;Boonserm Kijsirikul	2006		10.1007/978-0-387-44641-7_34	engineering;machine learning;pattern recognition;data mining	ML	10.530637566612322	-51.09194307695493	112134
e49f06ca0d4c602512a6c12a932dc86ed2a739b1	the class imbalance problem: a systematic study	c5 0;class imbalances;support vector machines;misclassification costs;class imbalance;multi layer perceptrons;re sampling;concept learning	In machine learning problems, differences in prior class probabilities -- or class imbalances -- have been reported to hinder the performance of some standard classifiers, such as decision trees. This paper presents a systematic study aimed at answering three different questions. First, we attempt to understand the nature of the class imbalance problem by establishing a relationship between concept complexity, size of the training set and class imbalance level. Second, we discuss several basic re-sampling or cost-modifying methods previously proposed to deal with the class imbalance problem and compare their effectiveness. The results obtained by such methods on artificial domains are linked to results in real-world domains. Finally, we investigate the assumption that the class imbalance problem does not only affect decision tree systems but also affects other classification systems such as Neural Networks and Support Vector Machines.		Nathalie Japkowicz;Shaju Stephen	2002	Intell. Data Anal.		support vector machine;concept learning;computer science;artificial intelligence;machine learning;data mining;mathematics	ECom	14.39032787779846	-41.46001049849926	112166
0e7f629592d89ab03d55394d51bd5d52dcad7696	clustered partial linear regression	multiple models;supervised learning;multistrategy learning;linear regression;regression model;regression;clustering;clustering method;multiple model;multiple regression	This paper presents a new method that deals with a supervised learning task usually known as multiple regression. The main distinguishing feature of our technique is the use of a multistrategy approach to this learning task. We use a clustering method to form sub-sets of the training data before the actual regression modeling takes place. This pre-clustering stage creates several training sub-samples containing cases that are “nearby” to each other from the perspective of the multidimensional input space. Supervised learning within each of these sub-samples is easier and more accurate as our experiments show. We call the resulting method clustered partial linear regression. Predictions using these models are preceded by a cluster membership query for each test case. The cluster membership probability of a test case is used as a weight in an averaging process that calculates the final prediction. This averaging process involves the predictions of the regression models associated to the clusters for which the test case may belong. We have tested this general multistrategy approach using several regression techniques and we have observed significant accuracy gains in several data sets. We have also compared our method to bagging that also uses an averaging process to obtain predictions. This experiment showed that the two methods are significantly different. Finally, we present a comparison of our method with several state-of-the-art regression methods showing its competitiveness.	algorithm;bootstrap aggregating;cluster analysis;competitive analysis (online algorithm);computation;decision tree;experiment;linear model;mathematical model;supervised learning;test case	Luís Torgo;Joaquim Pinto da Costa	2003	Machine Learning	10.1023/A:1021770020534	segmented regression;principal component regression;proper linear model;computer science;linear regression;machine learning;bayesian multivariate linear regression;pattern recognition;regression diagnostic;mathematics;supervised learning;nonparametric regression;regression analysis;statistics	ML	13.614977356798693	-39.6046670750824	112231
31ea88f29e7f01a9801648d808f90862e066f9ea	deep multi-task representation learning: a tensor factorisation approach		Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.	algorithm;computer multitasking;deep learning;end-to-end principle;experiment;feature learning;linear model;machine learning;monoid factorisation;monoidal t-norm logic;multi-task learning;the matrix	Yongxin Yang;Timothy M. Hospedales	2016	CoRR		artificial intelligence;theoretical computer science;machine learning;mathematics;deep belief network	AI	21.650795553460117	-46.23975111902375	112326
05aa4ff25f685b321a9d4393f667c6ff0bc8c645	on the use of linear programming for unsupervised text classification	unsupervised learning;lsi;generic model;supervised learning;large dataset;singular value decomposition;latent class models;feature space;text classification;dimensionality reduction;mixture model;latent semantic indexing;l1 norm;linear programming;linear program;technical report;computer science;support vector machine;generative models;mixture models;dimensional reduction;latent class model	We propose a new algorithm for dimensionality reduction and unsupervised text classification. We use mixture models as underlying process of generating corpus and utilize a novel, L1-norm based approach introduced by Kleinberg and Sandler [19]. We show that our algorithm performs extremely well on large datasets, with peak accuracy approaching that of supervised learning based on Support Vector Machines (SVMs) with large training sets. The method is based on the same idea that underlies Latent Semantic Indexing (LSI). We find a good low-dimensional subspace of a feature space and project all documents into it. However our projection minimizes different error, and unlike LSI we build a basis, that in many cases corresponds to the actual topics. We present results of testing of our algorithm on the abstracts of arXiv - an electronic repository of scientific papers, and the 20 Newsgroup dataset - a small snapshot of 20 specific newsgroups.	algorithm;dimensionality reduction;document classification;feature vector;linear programming;mixture model;scientific literature;snapshot (computer storage);supervised learning;support vector machine;taxicab geometry	Mark B. Sandler	2005		10.1145/1081870.1081901	computer science;linear programming;machine learning;pattern recognition;mixture model;data mining	ML	21.655089870480985	-42.18176023367348	112793
0b20f75dbb0823766d8c7b04030670ef7147ccdd	feature selection using nearest attributes	high dimensionality;feature vector;high dimensional data;pattern classification;nearest neighbour;feature selection	Feature selection is an important problem in high-dimensional data analysis and classification. Conventional feature selection approaches focus on detecting the features based on a redundancy criterion using learning and feature searching schemes. In contrast, we present an approach that identifies the need to select features based on their discriminatory ability among classes. Area of overlap between inter-class and intra-class distances resulting from feature to feature comparison of an attribute is used as a measure of discriminatory ability of the feature. A set of nearest attributes in a pattern having the lowest area of overlap within a degree of tolerance defined by a selection threshold is selected to represent the best available discriminable features. State of the art recognition results are reported for pattern classification problems by using the proposed feature selection scheme with the nearest neighbour classifier. These results are reported with benchmark databases having high dimensional feature vectors in the problems involving images and micro array data.	benchmark (computing);clustering high-dimensional data;database;feature selection;feature vector;microarray;naive bayes classifier;sensor;statistical classification	Alex Pappachen James;Sima Dimitrijev	2012	CoRR		minimum redundancy feature selection;feature vector;feature;feature extraction;computer science;machine learning;kanade–lucas–tomasi feature tracker;pattern recognition;data mining;mathematics;feature selection;k-nearest neighbors algorithm;feature;dimensionality reduction;clustering high-dimensional data	ML	11.450944768831842	-44.930674663586096	112861
6ef7a980c4dd8c718c0606487adf538a1a40e2fa	adversarial network embedding		Learning low-dimensional representations of networks has proved effective in a variety of tasks such as node classification, link prediction and network visualization. Existing methods can effectively encode different structural properties into the representations, such as neighborhood connectivity patterns, global structural role similarities and other highorder proximities. However, except for objectives to capture network structural properties, most of them suffer from lack of additional constraints for enhancing the robustness of representations. In this paper, we aim to exploit the strengths of generative adversarial networks in capturing latent features, and investigate its contribution in learning stable and robust graph representations. Specifically, we propose an Adversarial Network Embedding (ANE) framework, which leverages the adversarial learning principle to regularize the representation learning. It consists of two components, i.e., a structure preserving component and an adversarial learning component. The former component aims to capture network structural properties, while the latter contributes to learning robust representations by matching the posterior distribution of the latent representations to given priors. As shown by the empirical results, our method is competitive with or superior to state-of-the-art approaches on benchmark network embedding tasks. The source code will be available online.	benchmark (computing);component-based software engineering;encode;feature learning;generative adversarial networks;graph drawing;machine learning;mathematical optimization;minimax;optimization problem	Quanyu Dai;Qiang Li;Jian Tang;Dan Wang	2018			machine learning;artificial intelligence;robustness (computer science);computer science;prior probability;adversarial system;embedding;graph drawing;feature learning;exploit;posterior probability;pattern recognition	AI	23.344788865215754	-46.46457663221325	113014
bd9834c57903db8d0095a92e9569317e5478cbbd	nearest-neighbor search algorithms on non-euclidean manifolds for computer vision applications	object recognition;region covariance;approximation method;texture classification;nearest neighbor method;computer vision;hashing;face recognition;grassmann manifold;nearest neighbor;euclidean space;nearest neighbor search;shapes;manifold;activity recognition	Nearest-neighbor searching is a crucial component in many computer vision applications such as face recognition, object recognition, texture classification, and activity recognition. When large databases are involved in these applications, it is also important to perform these searches in a fast manner. Depending on the problem at hand, nearest neighbor strategies need to be devised over feature and model spaces which in many cases are not Euclidean in nature. Thus, metrics that are tuned to the geometry of this space are required which are also known as geodesics. In this paper, we address the problem of fast nearest neighbor searching in non-Euclidean spaces, where in addition to dealing with the large size of the dataset, the significant computational load involves geodesic computations. We study the applicability of the various classes of nearest neighbor algorithms toward this end. Exact nearest neighbor methods that rely solely on the existence of a metric can be extended, albeit with a huge computational cost. We derive an approximate method of searching via approximate embeddings using the logarithmic map. We study the error incurred in such an embedding and show that it performs well in real experiments.	activity recognition;algorithmic efficiency;approximation algorithm;computation;computer vision;database;experiment;facial recognition system;fast fourier transform;k-nearest neighbors algorithm;nearest neighbor search;outline of object recognition;search algorithm	Pavan K. Turaga;Rama Chellappa	2010		10.1145/1924559.1924597	nearest-neighbor chain algorithm;large margin nearest neighbor;r-tree;ball tree;nearest neighbor graph;combinatorics;best bin first;machine learning;pattern recognition;mathematics;cover tree;3d single-object recognition;nearest neighbor search;fixed-radius near neighbors;k-nearest neighbors algorithm;locality-sensitive hashing	Vision	18.881295472133043	-47.11390882414746	113146
32bb747d9aad3a91bf3b8e60a036061fd226050b	enhancing the accuracy of mapping to multidimensional optimal regions using pca		Mapping to Multidimensional Optimal Regions (MOR) is a special purposed method for multiclass classification task. It reduces computational complexity in comparison to the other concepts of classifiers. In order to increase the accuracy of MOR, its code assignment process is enriched using PCA. In addition to the increment in accuracy, corresponding enhancement eliminates the unwanted variance of the results from the previous version of MOR. Another advantage is more controllability on the upper bound of V.C. dimension of MOR which results in a better control on its generalization ability. Additionally, the computational complexity of the enhanced-optimal code assignment algorithm is reduced in training phase. By the other side, partitioning the feature space in MOR is an NP hard problem. PCA plays a key role in the greedy feature selection presented in this paper. Similar to the new code assignment process, corresponding greedy strategy increases the accuracy of the enhanced MOR.	alexey chervonenkis;code;computational complexity theory;feature selection;feature vector;greedy algorithm;hausdorff dimension;hilbert space;information;model order reduction;multiclass classification;online machine learning;principal component analysis;vapnik–chervonenkis theory	Elham Bavafaye Haghighi;Mohammad Rahmati	2012			pattern recognition;machine learning;artificial intelligence;computer science	ML	22.092958719648312	-41.43187768531316	113549
09f2fd01a2789cea4dc21cb6050e05aade51f2c3	unsupervised deep hashing with pseudo labels for scalable image retrieval		In order to achieve efficient similarity searching, hash functions are designed to encode images into low-dimensional binary codes with the constraint that similar features will have a short distance in the projected Hamming space. Recently, deep learning-based methods have become more popular, and outperform traditional non-deep methods. However, without label information, most state-of-the-art unsupervised deep hashing (DH) algorithms suffer from severe performance degradation for unsupervised scenarios. One of the main reasons is that the ad-hoc encoding process cannot properly capture the visual feature distribution. In this paper, we propose a novel unsupervised framework that has two main contributions: 1) we convert the unsupervised DH model into supervised by discovering pseudo labels; 2) the framework unifies likelihood maximization, mutual information maximization, and quantization error minimization so that the pseudo labels can maximumly preserve the distribution of visual features. Extensive experiments on three popular data sets demonstrate the advantages of the proposed method, which leads to significant performance improvement over the state-of-the-art unsupervised hashing algorithms.	binary code;deep learning;encode;elegant degradation;expectation–maximization algorithm;experiment;hamming space;hash function;hoc (programming language);image retrieval;marijuana abuse;mutual information;projections and predictions;pseudo brand of pseudoephedrine;quantization (signal processing);sample variance;scalability;window function	Haofeng Zhang;Li Liu;Yang Long;Ling Shao	2018	IEEE Transactions on Image Processing	10.1109/TIP.2017.2781422	deep learning;robustness (computer science);pattern recognition;hamming space;quantization (signal processing);artificial intelligence;feature extraction;image retrieval;mathematics;mutual information;hash function	Vision	22.561235260416378	-46.6729366803604	113623
2d0e8aa53d12233735fdcf37d4ecde8cff1cc2c2	a novel handwritten digits recognition method based on subclass low variances guided support vector machine		Handwritten Digits Recognition (HWDR) is one of the very popular application in computer vision and it has always been a challenging task in pattern recognition. But it is very hard practical problem and many problems are still unresolved. To develop a high performance automatic HWDR, several learning algorithms have been proposed, studied and modified. Much of the effort involved in Handwritten digits classification with Support Vector Machine (SVM). More specifically, in the current study we are focusing on one-class SVM (OSVM) approaches which are of huge interest for our problem. Covariance Guided OSVM (COSVM) algorithm improves up on the OSVM method, by emphasizing the low variance directions. However, COSVM does not handle multi-modal target class data. Thus, we design a new subclass algorithm based on COSVM, which takes advantage of the target class clusters variance information. To investigate the effectiveness of the novel Subclass COSVM (SCOSVM), we compared our proposed approach with other methods based on other contemporary one-class classifiers, on well-known standard MNIST benchmark datasets and Optical Recognition of Handwritten Digits datasets. The experimental results verify the significant superiority of our	algorithm;anomaly detection;benchmark (computing);computer vision;facial recognition system;guided local search;mnist database;machine learning;modal logic;optical character recognition;pattern recognition;support vector machine	Soumaya Nheri;Riadh Ksantini;Mouhamed Bécha Kaâniche;Adel Bouhoula	2018		10.5220/0006611100280036	subclass;support vector machine;machine learning;computer science;artificial intelligence	AI	12.068256177165969	-42.32308712590485	113659
503213ae40c8be29b91c7fd94c606f128031d4e9	conformal prediction with neural networks	detectors autocorrelation laplace equations shape robustness artificial intelligence testing lighting gaussian processes kernel;differential invariant;matrix algebra;simulation experiment;scale space;single parameter transforms;autocorrelation matrix;feature extraction;4th differential invariant;affine transformation;affine transforms;scale space theory;scale space selection function;affine transforms affine invariant region detector 4th differential invariant scale space theory autocorrelation matrix scale space selection function single parameter transforms;matrix algebra affine transforms feature extraction;affine invariant region detector	Conformal prediction (CP) is a method that can be used for complementing the bare predictions produced by any traditional machine learning algorithm with measures of confidence. CP gives good accuracy and confidence values, but unfortunately it is quite computationally inefficient. This computational inefficiency problem becomes huge when CP is coupled with a method that requires long training times, such as neural networks. In this paper we use a modification of the original CP method, called inductive conformal prediction (ICP), which allows us to a neural network confidence predictor without the massive computational overhead of CP The method we propose accompanies its predictions with confidence measures that are useful in practice, while still preserving the computational efficiency of its underlying neural network.	algorithm;artificial neural network;computation;inductive reasoning;kerrison predictor;machine learning;neural networks;overhead (computing);test set	Harris Papadopoulos;Vladimir Vovk;Alexander Gammerman	2007	19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007)	10.1109/ICTAI.2007.47	autocorrelation matrix;affine geometry;affine space;complex space;mathematical analysis;discrete mathematics;scale space;topology;affine coordinate system;affine involution;homothetic transformation;feature extraction;computer science;affine plane;affine geometry of curves;affine hull;affine arithmetic;affine transformation;harris affine region detector;mathematics;affine shape adaptation;affine combination;affine group;hessian affine region detector	ML	17.48675802545467	-46.18737953905036	113704
ed2209e9182fb05ef7b92752d8c48810d5ed6863	safer classification by synthesis		The discriminative approach to classification using deep neural networks has become the de-facto standard in various fields. Complementing recent reservations about safety against adversarial examples, we show that conventional discriminative methods can easily be fooled to provide incorrect labels with very high confidence to out of distribution examples. We posit that a generative approach is the natural remedy for this problem, and propose a method for classification using generative models. At training time, we learn a generative model for each class, while at test time, given an example to classify, we query each generator for its most similar generation, and select the class corresponding to the most similar one. Our approach is general and can be used with expressive models such as GANs and VAEs. At test time, our method accurately “knows when it does not know,” and provides resilience to out of distribution examples while maintaining competitive performance for standard examples.		William Wang;Angelina Wang;Aviv Tamar;Xi Chen;Pieter Abbeel	2017	CoRR		machine learning;safer;discriminative model;psychological resilience;generative grammar;artificial intelligence;artificial neural network;natural remedy;mathematics;generative model;adversarial system	ML	19.86426163291134	-49.5887051021839	113800
d1fb7e9a1e06e6b3cdb29e958e78d169e90ba750	regularized minimum class variance extreme learning machine for language recognition	signal image and speech processing;acoustics;mathematics in music;engineering acoustics;期刊论文	Support vector machines (SVMs) have played an important role in the state-of-the-art language recognition systems. The recently developed extreme learning machine (ELM) tends to have better scalability and achieve similar or much better generalization performance at much faster learning speed than traditional SVM. Inspired by the excellent feature of ELM, in this paper, we propose a novel method called regularized minimum class variance extreme learning machine (RMCVELM) for language recognition. The RMCVELM aims at minimizing empirical risk, structural risk, and the intra-class variance of the training data in the decision space simultaneously. The proposed method, which is computationally inexpensive compared to SVM, suggests a new classifier for language recognition and is evaluated on the 2009 National Institute of Standards and Technology (NIST) language recognition evaluation (LRE). Experimental results show that the proposed RMCVELM obtains much better performance than SVM. In addition, the RMCVELM can also be applied to the popular i-vector space and get comparable results to the existing scoring methods.		Jiaming Xu;Wei-Qiang Zhang;Jia Liu;Shanhong Xia	2015	EURASIP J. Audio, Speech and Music Processing	10.1186/s13636-015-0066-5	speech recognition;acoustics;computer science;machine learning;pattern recognition;physics	AI	17.792669723017106	-43.7239238151691	113834
0200506b4a0b582859ef24b9a946871d29dde0b4	fcnn: fourier convolutional neural networks		The Fourier domain is used in computer vision and machine learning as image analysis tasks in the Fourier domain are analogous to spatial domain methods but are achieved using different operations. Convolutional Neural Networks (CNNs) use machine learning to achieve state-of-the-art results with respect to many computer vision tasks. One of the main limiting aspects of CNNs is the computational cost of updating a large number of convolution parameters. Further, in the spatial domain, larger images take exponentially longer than smaller image to train on CNNs due to the operations involved in convolution methods. Consequently, CNNs are often not a viable solution for large image computer vision tasks. In this paper a Fourier Convolution Neural Network (FCNN) is proposed whereby training is conducted entirely within the Fourier domain. The advantage offered is that there is a significant speed up in training time without loss of effectiveness. Using the proposed approach larger images can therefore be processed within viable computation time. The FCNN is fully described and evaluated. The evaluation was conducted using the benchmark Cifar10 and MNIST datasets, and a bespoke fundus retina image dataset. The results demonstrate that convolution in the Fourier domain gives a significant speed up without adversely affecting accuracy. For simplicity the proposed FCNN concept is presented in the context of a basic CNN architecture, however, the FCNN concept has the potential to improve the speed of any neural network system involving convolution.	algorithmic efficiency;artificial neural network;benchmark (computing);bespoke;computation;computer vision;convolution;convolutional neural network;image analysis;liverpool;mnist database;machine learning;neural network software;time complexity	Harry Pratt;Bryan M. Williams;Frans Coenen;Yalin Zheng	2017		10.1007/978-3-319-71249-9_47	convolutional neural network;fourier transform;artificial neural network;mnist database;architecture;machine learning;speedup;computation;convolution;computer science;artificial intelligence	Vision	23.695696118489685	-51.75350621703639	114140
e90bea773400601fe825615d229aa3c36b2e229b	estimation and optimisation of right-censored data in survival analysis by neural network	optimisation;fuzzy regression;adaptive neural networks;survival time;censored data;right censored data;survival analysis;data modelling;medical statistics;neural network	The main topic in medical statistics is survival time. It is difficult to obtain complete data in studies of survival time because of several reasons. One aspect of such difficulty is related to death of all patients. A study is often completed before the death of all patients. Moreover, incomplete information is kept with respect to death of all patients. Usually, exponential and Weibull distribution are used to estimate actual time of the censored data. In general, censored data models describe situations where there are variables of interest that cannot always be observed directly and may deviate from certain values. This study presents an adaptive neural network (ANN) approach for improved estimation of right censored data. To show the superiority and advantages of the ANN approach, it has been compared with fuzzy mathematical programming and conventional approaches. It is shown that ANN provides better estimation for right censored data in comparison to fuzzy mathematical programming and exponential and Weibull distributions. This is the first study that utilises ANN for improved estimation of right censored data.	artificial neural network;censoring (statistics);data model;mathematical optimization;time complexity	Ali Azadeh;Abbas Keramati;Hazhir Tolouei;Reza Parvari;Shima Pashapour	2013	IJBIS	10.1504/IJBIS.2013.056720	data modeling;econometrics;computer science;data mining;survival analysis;medical statistics;censoring;statistics	ML	12.175131507303627	-38.6411019497904	114228
327f837aba290e2475aed1ecbb45293b830a3aff	kernel regression with order preferences	unlabeled data;kernel regression;semi supervised learning;optimization problem;linear program;house prices;risk minimization;side information	We propose a novel kernel regression algorithm which takes into account order preferences on unlabeled data. Such preferences have the form that point x1 has a larger target value than that of x2, although the target values for x1, x2 are unknown. The order preferences can be viewed as side information or a form of weak labels, and our algorithm can be related to semi-supervised learning. Learning consists of formulating the order preferences as additional regularization in a risk minimization framework. We define a linear program to effectively solve the optimization problem. Experiments on benchmark datasets, sentiment analysis, and housing price problems show that the proposed algorithm outperforms standard regression, even when the order preferences are noisy.	algorithm;benchmark (computing);experiment;heuristic;level of measurement;linear programming;mathematical optimization;matrix regularization;optimization problem;semi-supervised learning;semiconductor industry;sentiment analysis;supervised learning	Xiaojin Zhu;Andrew B. Goldberg	2007			semi-supervised learning;optimization problem;kernel regression;computer science;linear programming;machine learning;pattern recognition;data mining;statistics	AI	21.12785831594522	-40.44594576950301	114338
626cef6a66976edade0df9099f896ad07e728035	random brains	multilayer perceptrons;pattern classification;learning artificial intelligence	In this paper, we introduce and evaluate a novel method, called random brains, for producing neural network ensembles. The suggested method, which is heavily inspired by the random forest technique, produces diversity implicitly by using bootstrap training and randomized architectures. More specifically, for each base classifier multilayer perceptron, a number of randomly selected links between the input layer and the hidden layer are removed prior to training, thus resulting in potentially weaker but more diverse base classifiers. The experimental results on 20 UCI data sets show that random brains obtained significantly higher accuracy and AUC, compared to standard bagging of similar neural networks not utilizing randomized architectures. The analysis shows that the main reason for the increased ensemble performance is the ability to produce effective diversity, as indicated by the increase in the difficulty diversity measure.	artificial neural network;bootstrap aggregating;futures studies;multilayer perceptron;neural ensemble;random forest;randomized algorithm;randomness	Ulf Johansson;Tuve Löfström;Henrik Boström	2013	The 2013 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2013.6707026	computer science;artificial intelligence;machine learning;pattern recognition	ML	13.777470283318223	-41.78790813752771	114433
a31ddcdbe678070f3164834b19c01d49caca04e3	a method of feature selection using contribution ratio based on boosting	object recognition;support vector machines;training;image classification;boosting feature selection contribution ratio adaboost support vector machine algorithm object recognition classification performance feature sets svm classifier;support vector machines feature extraction image classification object recognition;boosting;machine learning;chromium;feature extraction;robustness;feature selection;boosting support vector machines support vector machine classification chromium object recognition frequency selective surfaces robustness equations computer science computational efficiency;support vector machine;sonar;frequency selective surfaces	AdaBoost and support vector machines (SVM) algorithms are commonly used in the field of object recognition. As classifiers, their classification performance is sensitive to affected by feature sets. To improve this performance, in addition to using the classifiers for accurate selection of feature sets, attention must be given to determining which feature subset to use in the classifier. Evaluating feature sets using a margin of the decision boundary of an SVM classifier proposed by Kugler is a solution for this problem. However, the margin in an SVM is sometimes large due to outliers. This paper presents a feature selection method that uses a contribution ratio based on boosting, which is effective for evaluating features. By comparing our method to the conventional one that uses a confident margin, we found that our method can select better feature sets using the contribution ratio obtained from boosting.	adaboost;algorithm;boosting (machine learning);decision boundary;experiment;feature model;feature selection;outline of object recognition;psi protein classifier;selection (genetic algorithm);support vector machine;test set	Masamitsu Tsuchiya;Hironobu Fujiyoshi	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761348	margin classifier;support vector machine;boosting methods for object categorization;computer science;machine learning;pattern recognition;data mining;feature selection;feature	Vision	14.971573393052795	-45.30670613844896	114907
7b15988d855f94387a899cb387ce71b241bc257f	multi-class pattern classification in imbalanced data	relational data;the australian standard research classification 210000 science general;ensemble learning;ensemble learning multi class classification imbalanced class problem;training data glass pattern recognition probabilistic logic training artificial neural networks testing;data distribution;conference paper;multi class classification imbalanced class problem ensemble learning;multi class classification;multiclass imbalanced learning multiclass pattern classification techniques balanced datasets imbalanced data distribution multi im probabilistic relational technique imbalanced relational data;pattern classification;pattern classification learning artificial intelligence;imbalanced class problem;learning artificial intelligence	The majority of multi-class pattern classification techniques are proposed for learning from balanced datasets. However, in several real-world domains, the datasets have imbalanced data distribution, where some classes of data may have few training examples compared for other classes. In this paper we present our research in learning from imbalanced multi-class data and propose a new approach, named Multi-IM, to deal with this problem. Multi-IM derives its fundamentals from the probabilistic relational technique (PRMs-IM), designed for learning from imbalanced relational data for the two-class problem. Multi-IM extends PRMs-IM to a generalized framework for multi-class imbalanced learning for both relational and non-relational domains.	instant messaging;multiclass classification;pattern recognition;statistical classification	Amal Saleh Ghanem;Svetha Venkatesh;Geoff A. W. West	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.706	relational database;computer science;machine learning;multiclass classification;pattern recognition;data mining;ensemble learning	Vision	15.316925543792959	-42.33478012974367	115007
081f3654939eb69d0d6ed55c04b2635ec848db5c	improving convolutional neural network design via variable neighborhood search		An unsupervised method for convolutional neural network (CNN) architecture design is proposed. The method relies on a variable neighborhood search-based approach for finding CNN architectures and hyperparameter values that improve classification performance. For this purpose, t-Distributed Stochastic Neighbor Embedding (t-SNE) is applied to effectively represent the solution space in 2D. Then, k-Means clustering divides this representation space having in account the relative distance between neighbors. The algorithm is tested in the CIFAR-10 image dataset. The obtained solution improves the CNN validation loss by over (15%) and the respective accuracy by (5%). Moreover, the network shows higher predictive power and robustness, validating our method for the optimization of CNN design.	convolutional neural network;variable neighborhood search	Teresa Araújo;Guilherme Aresta;Bernardo Almada-Lobo;Ana Maria Mendonça;Aurélio J. C. Campilho	2017		10.1007/978-3-319-59876-5_41	convolutional neural network;robustness (computer science);computer science;pattern recognition;artificial intelligence;architecture;hyperparameter;cluster analysis;variable neighborhood search;machine learning;embedding	AI	23.729484812934867	-47.42870434890427	115157
83fff014338dac9637c1679fd933d4813c3b39bd	de-biasing covariance-regularized discriminant analysis		Fisher’s Linear Discriminant Analysis (FLD) is a well-known technique for linear classification, feature extraction and dimension reduction. The empirical FLD relies on two key estimations from the data – the mean vector for each class and the (inverse) covariance matrix. To improve the accuracy of FLD under the High Dimension Low Sample Size (HDLSS) settings, Covariance-Regularized FLD (CRLD) has been proposed to use shrunken covariance estimators, such as Graphical Lasso, to strike a balance between biases and variances. Though CRLD could obtain better classification accuracy, it usually incurs bias and converges to the optimal result with a slower asymptotic rate. Inspired by the recent progress in de-biased Lasso, we propose a novel FLD classifier, DBLD, which improves classification accuracy of CRLD through de-biasing. Theoretical analysis shows that DBLD possesses better asymptotic properties than CRLD. We conduct experiments on both synthetic datasets and real application datasets to confirm the correctness of our theoretical analysis and demonstrate the superiority of DBLD over classical FLD, CRLD and other downstream competitors under HDLSS settings.	algorithm;baseline (configuration management);biasing;correctness (computer science);dimensionality reduction;downstream (software development);experiment;feature extraction;lasso;linear classifier;linear discriminant analysis;regularized meshless method;synthetic intelligence	Haoyi Xiong;Wei Cheng;Yanjie Fu;Wenqing Hu;Jiang Bian;Zhishan Guo	2018		10.24963/ijcai.2018/401	machine learning;computer science;covariance;biasing;linear discriminant analysis;artificial intelligence;pattern recognition	AI	16.322689265424692	-40.804429302350904	115489
44c6d727639badd9df7409f7b6e178905e434945	analysis and design of convolutional networks via hierarchical tensor decompositions		The driving force behind convolutional networks – the most successful deep learning architecture to date, is their expressive power. Despite its wide acceptance and vast empirical evidence, formal analyses supporting this belief are scarce. The primary notions for formally reasoning about expressiveness are efficiency and inductive bias. Expressive efficiency refers to the ability of a network architecture to realize functions that require an alternative architecture to be much larger. Inductive bias refers to the prioritization of some functions over others given prior knowledge regarding a task at hand. In this paper we overview a series of works written by the authors, that through an equivalence to hierarchical tensor decompositions, analyze the expressive efficiency and inductive bias of various convolutional network architectural features (depth, width, strides and more). The results presented shed light on the demonstrated effectiveness of convolutional networks, and in addition, provide new tools for network design. 1	algorithmic efficiency;convolution;convolutional neural network;deep learning;expressive power (computer science);inductive bias;microsoft windows;network architecture;network planning and design;turing completeness	Nadav Cohen;Or Sharir;Yoav Levine;Ronen Tamari;David Yakira;Amnon Shashua	2017	CoRR		computer science;theoretical computer science;machine learning;mathematics;algorithm;statistics	AI	21.574652359642496	-47.37131821875675	115492
dabba34711f15f5535a9c34ecbdf5bad7594995c	extensions of manifold learning algorithms in kernel feature space	manifold learning;feature space;data visualization	Manifold learning algorithms have been proven to be capable of discovering some nonlinear structures. However, it is hard for them to extend to test set directly. In this paper, a simple yet effective extension algorithm called PIE is proposed. Unlike LPP, which is linear in nature, our method is nonlinear. Besides, our method will never suffer from the singularity problem while LPP and KLPP will. Experimental results of data visualization and classification validate the effectiveness of our proposed method.	feature vector;kernel (operating system)	Yaoliang Yu;Peng Guan;Liming Zhang	2007		10.1007/978-3-540-72383-7_53	mathematical optimization;feature vector;computer science;machine learning;pattern recognition;mathematics;nonlinear dimensionality reduction;data visualization;manifold alignment	ML	24.46191530107536	-41.22224703922366	115530
e39def6af623fb6031220aafd0475dc3a9abf176	high dimensional low sample size activity recognition using geometric classifiers	multimedia analysis;geometric classification;action recognition;high dimension low sample size classification	Research on high dimension, low sample size (HDLSS) data has revealed their neighborless nature. This paper addresses the classification of HDLSS image or video data for human activity recognition. Existing approaches often use off-the-shelf classifiers such as nearest neighbor techniques or support vector machines and tend to ignore the geometry of underlying feature distributions. Addressing this issue, we investigate different geometric classifiers and affirm the lack of neighborhoods within HDLSS data. As this undermines proximity based methods and may cause over-fitting for discriminant methods, we propose a QR factorization approach to Nearest Affine Hull (NAH) classification which remedies the HDLSS dilemma and noticeably reduces time and memory requirements of existing methods. We show that the resulting non-parametric models provide smooth decision surfaces and yield efficient and accurate solutions in multiclass HDLSS scenarios. On several action recognition benchmarks, the proposed NAH classifier outperforms other instance based methods and shows competitive or superior performance than SVMs. In addition, for online settings, the proposed NAH method is faster than online SVMs. This paper addresses classification of High Dimensional, Low Sample Size Data (HDLSS).An efficient QR factorization base Nearest Affine Hull approach NAH-lsq is proposed.Extensive evaluation of 8 approaches is carried out on 5 HDLSS datasets.NAH-lsq outperforms other Geometrical approaches in accuracy and efficiency.In online settings, it achieves faster classification than online SVMs.	activity recognition	Shahzad Cheema;Abdalrahman Eweiwi;Christian Bauckhage	2015	Digital Signal Processing	10.1016/j.dsp.2015.03.019	machine learning;pattern recognition;data mining;mathematics;statistics	HCI	20.698716392187027	-44.61270759350227	115713
c6c0e987d7d2132706fd9d7bbdf43b2c167cd7c1	research of support vector regression algorithm based on granularity	coke mechanical strength prediction model;cluster algorithm;pattern clustering;kernel;support vector machines;training;support vector regression;regression model;coarse granularity level;support vector machines coke computational complexity mechanical strength pattern clustering regression analysis;density clustering algorithm;coke;computational complexity;support vector machines mechanical strength of coke predictive model granularity;granularity;classification algorithms;clustering algorithms;computational complexity support vector machines regression method density clustering algorithm coarse granularity level coke mechanical strength prediction model;regression analysis;prediction model;support vector machine;mechanical strength of coke;mechanical strength;high speed;algorithm design and analysis;regression method;predictive model;support vector machines clustering algorithms training kernel noise classification algorithms algorithm design and analysis;noise	A regression method of Support Vector Machines in the case of a large number of sample data. Hierarchies of various granularities for the data set are constructed by density clustering algorithm. In coarse-granularity level, abnormal sample data are excluded, while part of dense repeated samples are removed in fine-granularity level. After pretreating the sample set by the method mentioned above, Support Vector Regression is trained to construct a regression model. In this paper, the prediction model of coke mechanical strength is established by the means. The result indicates that Support Vector Regression Algorithm based on granularity has low computational complexity and high speed, moreover eliminating noise sample data and removing the dense samples do not affect the distribution and prediction effect of the original sample set. It is an effective measure of regression with the large sample data.	algorithm;cluster analysis;computational complexity theory;focal (programming language);signal-to-noise ratio;support vector machine;test set	Qing Lv;Xiaoming Han;Gang Xie;Gaowei Yan;Jun Xie	2010	2010 IEEE International Conference on Granular Computing	10.1109/GrC.2010.17	statistical classification;support vector machine;computer science;machine learning;pattern recognition;data mining;mathematics	Robotics	12.633736592159353	-40.58747040952183	116218
00d3523a619cef85c538c610f6b5aadf8892fd3c	binary coded genetic algorithm with ensemble classifier for feature selection in jpeg steganalysis	steganalysis extreme learning machine jpeg steganography undetectable data hiding;steganalysis;testing;transform coding;accuracy;jpeg steganography;extreme learning machine;feature extraction;transform coding genetic algorithms feature extraction testing error probability accuracy optimization;undetectable data hiding;genetic algorithms;optimization;error probability	In this paper, we propose a Binary Coded Genetic Algorithm with Ensemble Classification feature selection procedure designed for steganalysis. Proposed feature selection method was used for searching the most appropriate subset of features from 22510 dimension feature space superior for JPEG steganalysis. Reduced set of features shows better classification accuracy for JPEG steganalysis compared to complete set of features. In our method we used an ensemble classifier to approximate the functional relationship between the reduced feature set and class label. Search for optimal subset of features requires to solve two optimization problems: define the optimal number of features and define the optimal subset itself. Proposed Binary Coded Genetic algorithm enables to solve two optimization problems together. Each feature is coded as a binary coefficient in a binary string, which represent one solution of the feature selection problem. Genetic operations executed for binary strings (parents) results new binary strings (child) with good chance to have higher classification accuracy for JPEG steganalysis. Experimental results clearly indicate the advantage of using the proposed reduced set of features for JPEG steganalysis.	approximation algorithm;coefficient;ensemble learning;experiment;feature selection;feature vector;genetic algorithm;jpeg;mathematical optimization;optimization problem;performance;selection algorithm;steganalysis	Vasiliy Sachnev;Hyoung Joong Kim	2014	2014 IEEE Ninth International Conference on Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP)	10.1109/ISSNIP.2014.6827700	steganalysis;theoretical computer science;machine learning;pattern recognition;mathematics	Robotics	10.067211974282522	-42.405068624999096	116253
e91644230143dcff0543c5d4c6e75ded0d380743	modified minimum squared error algorithm for robust classification and face recognition experiments	minimum squared error mse;face recognition;pattern recognition	In this paper, we improve the minimum squared error (MSE) algorithm for classification by modifying its classification rule. Differing from the conventional MSE algorithm which first obtains the mapping that can best transform the training sample into its class label and then exploits the obtained mapping to predict the class label of the test sample, the modified minimum squared error classification (MMSEC) algorithm simultaneously predicts the class labels of the test sample and the training samples nearest to it and combines the predicted results to ultimately classify the test sample. Besides this paper, for the first time, proposes the idea to take advantage of the predicted class labels of the training samples for classification of the test sample, it devises a weighted fusion scheme to fuse the predicted class labels of the training sample and test sample. The paper also interprets the rationale of MMSEC. As MMSEC generalizes better than conventional MSE, it can lead to more robust classification decisions. The face recognition experiments show that MMSEC does obtain very promising performance. & 2014 Elsevier B.V. All rights reserved.	cyclic redundancy check;design rationale;experiment;facial recognition system;nearest neighbour algorithm;retro city rampage;statistical classification	Yong Xu;Xiaozhao Fang;Qi Zhu;Yan Chen;Jane You;Hong Liu	2014	Neurocomputing	10.1016/j.neucom.2013.11.025	facial recognition system;computer science;machine learning;pattern recognition;mathematics;statistics	AI	15.892769795873242	-42.5927724681506	116424
b9a01701c4a3b0daf4d5c010b628b353ee246ea4	concentration based feature construction approach for spam detection	libraries;human immune system;electronic mail;cost function;support vector machines;training;clonal particle swarm optimization;e mail classification;clonal particle swarm optimization spam detection human immune system concentration based feature construction feature vector e mail classification;classification;multi layer neural network;optimization problem;feature vector;unsolicited e mail classification e mail filters particle swarm optimisation;particle swarm optimizer;feature construction;e mail filters;humans immune system libraries electronic mail cost function particle swarm optimization multi layer neural network support vector machines support vector machine classification robustness;particle swarm optimization;concentration based feature construction;classification algorithms;immune system;support vector machine classification;robustness;optimization;humans;support vector machine;unsolicited e mail;particle swarm optimisation;spam detection;neural network	Inspired by human immune system, a concentration based feature construction (CFC) approach which utilizes a two-element concentration vector as the feature vector is proposed for spam detection in this paper. In the CFC approach, ‘self’ and ‘non-self’ concentrations are constructed by using ‘self’ and ‘non-self’ gene libraries, respectively, and subsequently are used to form a vector with two elements of concentrations for characterizing the e-mail efficiently. As a result, the design of classifier actually amounts to establishing a mapping between two real-value inputs and one binary output. The classification of the e-mail is considered as an optimization problem aiming at minimizing a formulated cost function. A clonal particle swarm optimization (CPSO) algorithm proposed by the leading author is also employed for this purpose. Several classifiers including linear discriminant, multi-layer neural networks and support vector machine are used to verify the effectiveness and robustness of the CFC approach. Experimental results demonstrate that the proposed CFC approach not only has a very much fast speed but also gives 97% and 99% of accuracy just using a two-element concentration feature vector on corpus PU1 and Ling, respectively.	algorithm;anti-spam techniques;artificial neural network;email;feature vector;layer (electronics);library (computing);linear discriminant analysis;loss function;mathematical optimization;optimization problem;particle swarm optimization;spamming;statistical classification;support vector machine	Ying Tan;Chao Deng;Guangchen Ruan	2009	2009 International Joint Conference on Neural Networks	10.1109/IJCNN.2009.5178651	statistical classification;support vector machine;immune system;computer science;machine learning;pattern recognition;data mining;artificial neural network	ML	19.54235744158662	-39.698653777266145	116556
d26c59db75a050cbd986f14c3b538104268e63ed	adversarial ladder networks.		The use of unsupervised data in addition to supervised data has lead to a significant improvement when training discriminative neural networks. However, the best results were achieved with a training process that is divided in two parts: first an unsupervised pre-training step is done for initializing the weights of the network and after these weights are refined with the use of supervised data. Recently, a new neural network topology called Ladder Network, where the key idea is based in some properties of hierarchichal latent variable models, has been proposed as a technique to train a neural network using supervised and unsupervised data at the same time with what is called semi-supervised learning. This technique has reached state of the art classification. On the other hand adversarial noise has improved the results of classical supervised learning. In this work we add adversarial noise to the ladder network and get state of the art classification, with several important conclusions on how adversarial noise can help in addition with new possible lines of investigation. We also propose an alternative to add adversarial noise to unsupervised data.	artificial neural network;electronic filter topology;latent variable;network topology;semi-supervised learning;semiconductor industry;supervised learning;unsupervised learning	Juan Maroñas Molano;Alberto Albiol Colomer;Roberto Paredes	2016	CoRR		supervised learning;discriminative model;machine learning;artificial intelligence;adversarial system;artificial neural network;initialization;computer science;latent variable	ML	22.017499065735915	-49.22510935814377	116655
24fb34ca5103e6501567f0a163f4c717ac418a14	analytic gain in probabilistic decompression sickness models	scuba diving;decompression sickness;survival analysis;optimization;mathematical modeling;parameter estimation	Decompression sickness (DCS) is a disease known to be related to inert gas bubble formation originating from gases dissolved in body tissues. Probabilistic DCS models, which employ survival and hazard functions, are optimized by fitting model parameters to experimental dive data. In the work reported here, I develop methods to find the survival function gain parameter analytically, thus removing it from the fitting process. I show that the number of iterations required for model optimization is significantly reduced. The analytic gain method substantially improves the condition number of the Hessian matrix which reduces the model confidence intervals by more than an order of magnitude.	analysis of algorithms;approximation algorithm;body tissue;brute-force search;cluster analysis;condition number;confidence intervals;converge;convergence (action);data compression;decompression sickness;diving (activity);equivalent weight;evaluation;expectation–maximization algorithm;failure rate;feasible region;global optimization;greater than;hessian;illness (finding);initial condition;introduction to algorithms;iteration;mandibular right second molar tooth;mathematical optimization;maxima and minima;nmath;noble gases;population parameter;propagation of uncertainty;randomized algorithm;silo (dataset);software propagation;solutions;test set;xfig;statistical cluster	Laurens E. Howle	2013	Computers in biology and medicine	10.1016/j.compbiomed.2013.07.026	mathematical optimization;simulation;mathematical model;control theory;mathematics;survival analysis;estimation theory;statistics	ML	11.272426141164848	-51.760549364236	116829
4bdce7bf5292ec1ad09d705e8d8d5f91ae6b1906	a comparative study of bayes net, naive bayes and averaged one-dependence estimators for osteoporosis analysis		This paper presents an evaluation of the accuracy of the Bayesian classifiers: Bayes Net, Naive Bayes and Averaged One-Dependence Estimator, to support diagnoses of osteopenia and osteoporosis. All classifiers showed good results, thus, given data, it is possible to produce a reasonably accurate estimate of the diagnosis.		Priscyla Waleska Targino de Azevedo Simões;Leandro Luiz Mazzuchello;Larissa Letieli Toniazzo de Abreu;Diego Garcia;Maitê Gabriel dos Passos;Ramon Venson;Luciane Bisognin Ceretta;Ana Carolina Veiga Silva;Maria Inês da Rosa;Paulo João Martins	2015	Studies in health technology and informatics	10.3233/978-1-61499-564-7-1075	statistics;data mining;naive bayes classifier;bayes factor;estimator;averaged one-dependence estimators;bayesian network;bayes estimator;computer science;bayesian probability;bayes error rate	ML	11.807523647875358	-38.04807829162385	116895
5fd04ab2c9ff5043b4ca3efe8850ede324da52e5	cost-sensitive neural networks and editing techniques for imbalance problems	cost function;backpropagation;editing;multi class imbalance	The multi-class imbalance problem in supervised pattern recognition methods is receiving growing attention. Imbalanced datasets means that some classes are represented by a large number of samples while the others classes only contain a few. In real-world applications, imbalanced training sets may produce an important deterioration of the classifier performance when neural networks are applied in the classes less represented. In this paper we propose training cost-sentitive neural networks with editing techniques for handling the class imbalance problem on multi-class datasets. The aim is to remove majority samples while compensating the class imbalance during the training process. Experiments with real data sets demonstrate the effectiveness of the strategy here proposed.		Roberto Alejo;José Martínez Sotoca;Vicente García;Rosa Maria Valdovinos	2010		10.1007/978-3-642-15992-3_20	editing;computer science;artificial intelligence;backpropagation;machine learning;data mining	AI	13.64770103322721	-40.5270928071572	116957
552b0b049e594b7f38a2327a216df4e871350c49	effectiveness of basic and advanced sampling strategies on the classification of imbalanced data. a comparative study using classical and novel metrics		The imbalanced class problem is noteworthy given its impact on the induction of predictive models and its constant presence in several application areas. It is a challenge in supervised classification, since most of classifiers are very sensitive to class distributions. Consequently, the predictive model is biased to the majority class, which leads to a low performance. In this paper, we analyze the reliability of resampling strategies through the influence of some factors such as dataset characteristics and the classifiers used for building the models, in order to improve the performance and determine which resampling method will be used according to these factors. Experiments over 24 real datasets with different imbal‐ ance ratio, using six different classifiers, seven resampling algorithms and six performance evaluation measures have been conducted aiming at showing which resampling method will be the most suitable depending on these factors.	algorithm;characterization test;decision tree;experiment;f1 score;infiniband;machine learning;performance evaluation;predictive modelling;qualitative comparative analysis;random forest;resampling (statistics);sampling (signal processing);supervised learning;undersampling	Mohamed S. Kraiem;María N. Moreno García	2017		10.1007/978-3-319-59650-1_20	artificial intelligence;machine learning;resampling;random forest;sampling (statistics);computer science;pattern recognition	ML	13.753905303127478	-41.90223224104071	117082
04074480ddbfc566a313cdf490347896e8bc714e	inverse of lorentzian mixture for simultaneous training of prototypes and weights		This paper presents a novel distance-based classifier based on the multiplicative inverse of Lorentzian mixture, which can be regarded as a natural extension of the conventional nearest neighbor rule. We show that prototypes and weights can be trained simultaneously by General Loss Minimization, which is a generalized version of supervised learning framework used in Generalized Learning Vector Quantization. Experimental results for UCI machine learning repository reveal that the proposed method achieves almost the same as or higher classification accuracy than Support Vector Machine with a much fewer prototypes than support vectors.	learning vector quantization;machine learning;mixture model;rule 90;supervised learning;support vector machine	Atsushi Sato;Masato Ishii	2013			econometrics;mathematical optimization;machine learning;mathematics	ML	18.672274220071888	-39.23937102720536	117730
8f4f9a225c4f1d47428c2f38a130d70dd9e4dd94	a comparison of three voting methods for bagging with the mlem2 algorithm	rule induction;mathematics and computing	This paper presents results of experiments on some data sets using bagging on the MLEM2 rule induction algorithm. Three different methods of ensemble voting, based on support (a non-democratic voting in which ensembles vote with their strengths), strength only (an ensemble with the largest strength decides to which concept a case belongs) and democratic voting (each ensemble has at most one vote) were used. Our conclusions are that though in most cases democratic voting was the best, it is not significantly better than voting based on support. The strength voting was the worst voting method.	algorithm	Clinton Cohagan;Jerzy W. Grzymala-Busse;Zdzislaw S. Hippe	2010		10.1007/978-3-642-15381-5_15	bullet voting;computer science;machine learning;pattern recognition;data mining;mathematics;cardinal voting systems;positional voting system;preferential block voting;anti-plurality voting;condorcet method	AI	13.759643907883897	-40.5519937898958	117822
35d615275cbf8fc9ea88a9553c26e4c368801dd9	feature selection for computer-aided polyp detection using mrmr	minimum redundancy maximum relevance mrmr;computer aided diagnosis;bagging;cad;qa75 electronic computers computer science;adaboost;computer aided detection;computing systems;feature selection;fitness function	In building robust classifiers for computer-aided detection (CAD) of lesions, selection of relevant features is of fundamental importance. Typically one is interested in determining which, of a large number of potentially redundant or noisy features, are most discriminative for classification. Searching all possible subsets of features is impractical computationally. This paper proposes a feature selection scheme combining AdaBoost with the Minimum Redundancy Maximum Relevance (MRMR) to focus on the most discriminative features. A fitness function is designed to determine the optimal number of features in a forward wrapper search. Bagging is applied to reduce the variance of the classifier and make a reliable selection. Experiments demonstrate that by selecting just 11 percent of the total features, the classifier can achieve better prediction on independent test data compared to the 70 percent of the total features selected by AdaBoost.	adaboost;computer-aided design;experiment;feature selection;fitness function;relevance;statistical classification;test data	Xiaoyun Yang;Boray Tek;Gareth Beddoe;Gregory G. Slabaugh	2010		10.1117/12.844165	adaboost;bootstrap aggregating;minimum redundancy feature selection;computer science;machine learning;pattern recognition;data mining;cad;feature selection;fitness function	AI	11.13683340750484	-43.51492177872466	117858
0674058618d04def58c79a0b28174301ef591433	rainforest—a framework for fast decision tree construction of large datasets	decision tree classifier;tecnologia electronica telecomunicaciones;classification algorithm;classification tree;decision tree;generic algorithm;large dataset;data mining;classification;performance improvement;scalability;tecnologias;grupo a;rain forest;decision trees	Classification of large datasets is an important data mining problem. Many classification algorithms have been proposed in the literature, but studies have shown that so far no algorithm uniformly outperforms all other algorithms in terms of quality. In this paper, we present a unifying framework called Rain Forest for classification tree construction that separates the scalability aspects of algorithms for constructing a tree from the central features that determine the quality of the tree. The generic algorithm is easy to instantiate with specific split selection methods from the literature (including C4.5, CART, CHAID, FACT, ID3 and extensions, SLIQ, SPRINT and QUEST). In addition to its generality, in that it yields scalable versions of a wide range of classification algorithms, our approach also offers performance improvements of over a factor of three over the SPRINT algorithm, the fastest scalable classification algorithm proposed previously. In contrast to SPRINT, however, our generic algorithm requires a certain minimum amount of main memory, proportional to the set of distinct values in a column of the input relation. Given current main memory costs, this requirement is readily met in most if not all workloads.	c4.5 algorithm;computer data storage;data mining;decision tree learning;fastest;generic programming;id3 algorithm;random forest;scalability	Johannes Gehrke;Raghu Ramakrishnan;Venkatesh Ganti	1998	Data Mining and Knowledge Discovery	10.1023/A:1009839829793	decision tree learning;classification tree method;computer science;theoretical computer science;machine learning;decision tree;incremental decision tree;data mining;fractal tree index;id3 algorithm	DB	14.099313881748348	-39.83839615633381	117876
4a8ea07b8ddb8ab0b805341ca31ac564256050ff	a multi-task learning strategy for unsupervised clustering via explicitly separating the commonality	pattern clustering;signal representation;dictionaries;signal representation dictionaries learning artificial intelligence pattern clustering;dictionaries clustering algorithms linear programming vectors encoding pattern recognition algorithm design and analysis;feature space multitask learning strategy unsupervised clustering mtcluster signal representation common pattern pool sharing cluster specific dictionary;learning artificial intelligence	In this paper, we propose an unsupervised cluster method via a multi-task learning strategy, called Mt-Cluster. Our MtCluster learns a cluster-specific dictionary for each cluster to represent its sample signals and a shared common pattern pool (the commonality) for the essentially complemental representation. By treating learning the cluster-specific dictionary as a single task, MtCluster works in a multi-task learning manner, in which all the tasks are connected by simultaneously learning the commonality. Actually, the learned cluster-specific dictionary spans the feature space of the corresponding cluster, and the commonality is just used for necessary complemental representation. To evaluate our method, we perform several experiments on public available datasets, and the promising results demonstrate the effectiveness of MtCluster.	cluster analysis;computer cluster;computer multitasking;dictionary;experiment;feature vector;k-means clustering;machine learning;multi-task learning;unsupervised learning	Shu Kong;Donghui Wang	2012	Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)		unsupervised learning;multi-task learning;k-svd;computer science;machine learning;pattern recognition;data mining;rule-based machine translation;cluster analysis;conceptual clustering	Vision	23.40003104689446	-43.258301851418516	118011
67ff539079cda88eaa923b31c6d0b8360b44c27d	semi-supervised kernel-based fuzzy c-means with pairwise constraints	fuzzy c means algorithm;pattern clustering;fuzzy c mean;kernel;fuzzy clustering algorithm;kernel clustering algorithms machine learning distance measurement euclidean distance iris equations;semisupervised kernel based fuzzy c means;pairwise constrained competitive agglomeration semisupervised kernel based fuzzy c means pairwise constraints machine learning data mining fuzzy clustering algorithm;euclidean distance;data mining;semi supervised learning;fuzzy set theory;objective function;distance measurement;fuzzy clustering;machine learning;parameter selection;pairwise constraints;clustering algorithms;pattern clustering data mining fuzzy set theory learning artificial intelligence;kernel method;learning artificial intelligence;iris;pairwise constrained competitive agglomeration	Clustering with constraints is an active area in machine learning and data mining. In this paper, a semi-supervised kernel-based fuzzy C-means algorithm called PCKFCM is proposed which incorporates both semi-supervised learning technique and the kernel method into traditional fuzzy clustering algorithm. The clustering is achieved by minimizing a carefully designed objective function. A kernel-based fuzzy term defined by the violation of constraints is included. The proposed PCKFCM is compared with other clustering techniques on benchmark and the experimental results convince that effective use of constraints improves the performance of kernel-based clustering. As for the effect of key parameter selection and the non-linear capability, it outperforms a similar semi-supervised fuzzy clustering approach Pairwise Constrained Competitive Agglomeration (PCCA).	algorithm;benchmark (computing);cluster analysis;data mining;fuzzy clustering;kernel (operating system);kernel method;loss function;machine learning;nonlinear system;optimization problem;semi-supervised learning;semiconductor industry;supervised learning	Na Wang;Xia Li;Xuehui Luo	2008	2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)	10.1109/IJCNN.2008.4633936	semi-supervised learning;correlation clustering;constrained clustering;kernel method;data stream clustering;kernel;kernel embedding of distributions;radial basis function kernel;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;euclidean distance;mathematics;fuzzy set;cluster analysis;tree kernel;variable kernel density estimation;polynomial kernel;clustering high-dimensional data;conceptual clustering	Vision	21.108651519916876	-40.999186138235906	118458
2a281d5ad692a22371ca8aa871d00bff47da0402	research paper: ignoring dependency between linking variables and its impact on the outcome of probabilistic record linkage studies	empirical study;study design;research paper;simulation study;record linkage	OBJECTIVES This study sought to examine the differences between ignoring (naïve) and incorporating dependency (nonnaïve) among linkage variables on the outcome of a probabilistic record linkage study. DESIGN AND MEASUREMENTS We used the outcomes of a previously developed probabilistic linkage procedure for different registries in perinatal care assuming independence among linkage variables. We estimated the impact of ignoring dependency by re-estimating the linkage weights after constructing a variable that combines the outcomes of the comparison of 2 correlated linking variables. The results of the original naïve and the new nonnaïve strategy were systematically compared for 3 scenarios: the empirical dataset using 9 variables, the empirical dataset using 5 variables, and a simulated dataset using 5 variables. RESULTS The linking weight for agreement on 2 correlated variables among nonmatches was estimated considerably higher in the naïve strategy than in the nonnaïve strategy (16.87 vs. 13.55). Therefore, ignoring dependency overestimates the amount of identifying information if both correlated variables agree. The impact on the number of pairs that was classified differently with both approaches was modest in the situation in which there were many different linking variables but grew substantially with fewer variables. The simulation study confirmed the results of the empirical study and suggests that the number of misclassifications can increase substantially by ignoring dependency under less favorable linking conditions. CONCLUSION Dependency often exists between linking variables and has the potential to bias the outcome of a linkage study. The nonnaïve approach is a straightforward method for creating linking weights that accommodate dependency. The impact on the number of misclassifications depends on the quality and number of linking variables relative to the number of correlated linking variables.	classification;estimated;linkage (software);naivety;perinatal care;silo (dataset);simulation;weight;emotional dependency;genetic linkage	Miranda Tromp;Nora Méray;Anita C. J. Ravelli;Johannes B. Reitsma;Gouke J. Bonsel	2008	Journal of the American Medical Informatics Association : JAMIA	10.1197/jamia.M2265	econometrics;record linkage;computer science;data mining;empirical research;clinical study design;statistics	NLP	11.328200192992817	-48.719747891144536	118465
38fc64f0ed79f818ea75d846aa12e8f8789bb350	a gradient approach for value weighted classification learning in naive bayes	classification;gradient descent;bayesian learning;feature weighting	Propose a new weighting method in the context of naive Bayes classification learning.Assign different weights for each feature value.A gradient approach for automatically calculating the weights of each feature value.Its performance is compared with that of other state-of-the-art methods.Experiments show the method could improve the performance of naive Bayes. Feature weighting has been an important topic in classification learning algorithms. In this paper, we propose a new paradigm of assigning weights in classification learning, called value weighting method. While the current weighting methods assign a weight to each feature, we assign a different weight to the values of each feature. The proposed method is implemented in the context of naive Bayesian learning, and optimal weights of feature values are calculated using a gradient approach. The performance of naive Bayes learning with value weighting method is compared with that of other state-of-the-art methods for a number of datasets. The experimental results show that the value weighting method could improve the performance of naive Bayes significantly.	gradient;naive bayes classifier;statistical classification	Chang-Hwan Lee	2015	Knowl.-Based Syst.	10.1016/j.knosys.2015.04.020	gradient descent;naive bayes classifier;biological classification;computer science;machine learning;pattern recognition;data mining;bayesian inference	AI	17.758793408046124	-40.855592448795235	118653
321fd1621281b4a77ad2376a260c3b0cce250dd6	an efficient ensemble method for classifying skewed data streams	skewed data streams;ensemble classifiers;misclassified positive instances;f1 value	Class distributions of data streams in real application are usually unbalanced, they are hence called Skewed Data Streams (abbreviated as SDS). However, in the classification of SDS, it is a challenge for traditional methods because of the difficulty in the recognition of minority classes. Therefore, many approaches have been proposed to improve the recognition rate of minority classes, while they are time-consuming. Motivated by this, we propose an efficient Ensemble method for Classifying SDS called ECSDS. Our algorithm creates multiple classifiers based on C4.5, and adopts the threshold of F1-value to limit the updating frequency of classifiers. Meanwhile, it adds misclassified positive instances into the training data to guarantee the effectiveness of classifiers when updating. Experimental studies demonstrate that our proposed method enables reducing the time overhead and maintains a good performance on the classification accuracy.	c4.5 algorithm;experiment;overhead (computing);unbalanced circuit	Juan Zhang;Xuegang Hu;Yuhong Zhang;Pei-Pei Li	2011		10.1007/978-3-642-24553-4_21	random subspace method;computer science;machine learning;pattern recognition;data mining;statistics	AI	13.662215053432574	-40.769321084046545	118694
1eec21d56d223bf8212d17e819a224ff6c32d6e1	c-svddnet: an effective single-layer network for unsupervised feature learning		In this paper, we investigate the problem of learning feature representation from unlabeled data using a single-layer K-means network. A K-means network maps the input data into a feature representation by finding the nearest centroid for each input point, which has attracted researchers’ great attention recently due to its simplicity, effectiveness, and scalability. However, one drawback of this feature mapping is that it tends to be unreliable when the training data contains noise. To address this issue, we propose a SVDD based feature learning algorithm that describes the density and distribution of each cluster from Kmeans with an SVDD ball for more robust feature representation. For this purpose, we present a new SVDD algorithm called CSVDD that centers the SVDD ball towards the mode of local density of each cluster, and we show that the objective of CSVDD can be solved very efficiently as a linear programming problem. Additionally, previous single-layer networks favor a large number of centroids but a crude pooling size, resulting in a representation that highlights the global aspects of the object. Here we explore an alternative network architecture with much smaller number of nodes but with much finer pooling size, hence emphasizing the local details of the object. The architecture is also extended with multiple receptive field scales and multiple pooling sizes. Extensive experiments on several popular object recognition benchmarks, such as MINST, NORB, CIFAR-10 and STL-10, shows that the proposed C-SVDDNet method yields comparable or better performance than that of the previous state of the art methods.	algorithm;artificial neural network;experiment;feature learning;feature model;k-means clustering;linear programming;map;network architecture;outline of object recognition;scalability;scale-invariant feature transform	Dong Wang;Xiaoyang Tan	2014	CoRR		semi-supervised learning;unsupervised learning;feature learning;wake-sleep algorithm;machine learning;pattern recognition;competitive learning;deep belief network	ML	23.68928141842255	-50.65984566627936	119065
070b4d9ee04164d3a2b1e8602ce6297cd1290f77	semi-supervised dimension reduction for multi-label classification	dimension reduction;data mining;classification;semi supervised learning	A significant challenge to make learning techniques more suitable for general purpose use in AI is to move beyond i) complete supervision, ii) low dimensional data and iii) a single label per instance. Solving this challenge would allow making predictions for high dimensional large dataset with multiple (but possibly incomplete) labelings. While other work has addressed each of these problems separately, in this paper we show how to address them together, namely the problem of semi-supervised dimension reduction for multi-labeled classification, SSDR-MC. To our knowledge this is the first paper that attempts to address all challenges together. In this work, we study a novel joint learning framework which performs optimization for dimension reduction and multi-label inference in semi-supervised setting. The experimental results validate the performance of our approach, and demonstrate the effectiveness of connecting dimension reduction and learning.	dimensionality reduction;mathematical optimization;multi-label classification;semi-supervised learning;semiconductor industry	Buyue Qian;Ian Davidson	2010			semi-supervised learning;biological classification;computer science;artificial intelligence;machine learning;pattern recognition;data mining;statistics;dimensionality reduction	AI	22.73976022011291	-43.8179705193367	119345
cee700093d6672df48d169ef194861026fe31e8e	hashing on nonlinear manifolds	nonlinear manifolds learning based hashing method binary codes euclidean similarity large scale embedding binary embeddings nonparametric manifold learning t distributed stochastic neighbor embedding quantization error orthogonal rotation supervised inductive manifold hashing semantic retrieval;stochastic processes image coding image retrieval learning artificial intelligence;manifold learning;journal article;hashing binary code learning manifold learning image retrieval;binary code learning;manifolds educational institutions eigenvalues and eigenfunctions binary codes training prototypes learning systems;hashing;semantic retrieval nonlinear manifolds learning based hashing method binary codes euclidean similarity large scale embedding binary embeddings nonparametric manifold learning t distributed stochastic neighbor embedding quantization error orthogonal rotation supervised inductive manifold hashing;image retrieval hashing binary code learning manifold learning;image coding image retrieval learning artificial intelligence stochastic processes;image retrieval	Learning-based hashing methods have attracted considerable attention due to their ability to greatly increase the scale at which existing algorithms may operate. Most of these methods are designed to generate binary codes preserving the Euclidean similarity in the original space. Manifold learning techniques, in contrast, are better able to model the intrinsic structure embedded in the original high-dimensional data. The complexities of these models, and the problems with out-of-sample data, have previously rendered them unsuitable for application to large-scale embedding, however. In this paper, how to learn compact binary embeddings on their intrinsic manifolds is considered. In order to address the above-mentioned difficulties, an efficient, inductive solution to the out-of-sample data problem, and a process by which nonparametric manifold learning may be used as the basis of a hashing method are proposed. The proposed approach thus allows the development of a range of new hashing techniques exploiting the flexibility of the wide variety of manifold learning approaches available. It is particularly shown that hashing on the basis of t-distributed stochastic neighbor embedding outperforms state-of-the-art hashing methods on large-scale benchmark data sets, and is very effective for image classification with very short code lengths. It is shown that the proposed framework can be further improved, for example, by minimizing the quantization error with learned orthogonal rotations without much computation overhead. In addition, a supervised inductive manifold hashing framework is developed by incorporating the label information, which is shown to greatly advance the semantic retrieval performance.	algorithm;algorithmic efficiency;benchmark (computing);binary code;computation;computer vision;cryptographic hash function;effective method;feature extraction;hash function;image retrieval;imagenet;indexes;inductive reasoning;lookup table;marijuana abuse;nonlinear dimensionality reduction;nonlinear system;overhead (computing);population parameter;quantization (signal processing);short code;silo (dataset);t-distributed stochastic neighbor embedding;time complexity;window function;manifold	Fumin Shen;Chunhua Shen;Qinfeng Shi;Anton van den Hengel;Zhenmin Tang;Heng Tao Shen	2015	IEEE Transactions on Image Processing	10.1109/TIP.2015.2405340	feature hashing;hash table;hash function;dynamic perfect hashing;image retrieval;computer science;theoretical computer science;machine learning;universal hashing;pattern recognition;mathematics;nonlinear dimensionality reduction;locality preserving hashing;locality-sensitive hashing;manifold alignment	AI	19.320571442794712	-46.72815522043857	119558
a91ed023bf80bdfe442d0bd953e40d10d5e22951	classification of dissimilarity data with a new flexible mahalanobis-like metric	processus gauss;mahalanobis distance;analisis estadistico;mahalanobis distance gaussian classifier;gaussian classifier;analisis datos;metric;outlier;probabilistic approach;classification;similitude;discriminant analysis;analyse discriminante;regle decision;observacion aberrante;classification a vaste marge;data analysis;analisis discriminante;statistical analysis;enfoque probabilista;approche probabiliste;robustesse;analyse statistique;similarity;statistical pattern recognition;pattern recognition;error rate;observation aberrante;analyse donnee;metrico;robustness;reconnaissance forme;gaussian process;support vector machine;regla decision;similitud;maquina ejemplo soporte;numerical experiment;vector support machine;point of view;reconocimiento patron;proceso gauss;dissimilarity data;metrique;decision rule;robustez	Statistical pattern recognition traditionally relies on feature-based representation. For many applications, such vector representation is not available and we only possess proximity data (distance, dissimilarity, similarity, ranks, etc.). In this paper, we consider a particular point of view on discriminant analysis from dissimilarity data. Our approach is inspired by the Gaussian classifier and we defined decision rules to mimic the behavior of a linear or a quadratic classifier. The number of parameters is limited (two per class). Numerical experiments on artificial and real data show interesting behavior compared to Support Vector Machines and to kNN classifier: (a) lower or equivalent error rate, (b) equivalent CPU time, (c) more robustness with sparse dissimilarity data.	best practice;binary number;central processing unit;coherence (physics);cross-validation (statistics);data mining;data structure;emoticon;experiment;heuristic;huygens software;intrinsic dimension;linear discriminant analysis;mathematical optimization;maximum parsimony (phylogenetics);missing data;monte carlo method;numerical linear algebra;occam's razor;pattern recognition;quadratic classifier;recursion;self-organizing map;simulation;sparse matrix;supervised learning;support vector machine	Agata Manolova;Anne Guérin-Dugué	2008	Pattern Analysis and Applications	10.1007/s10044-008-0101-6	margin classifier;support vector machine;outlier;similarity;metric;quadratic classifier;biological classification;word error rate;computer science;mahalanobis distance;similitude;machine learning;pattern recognition;decision rule;gaussian process;mathematics;linear discriminant analysis;data analysis;statistics;robustness	ML	23.73606480518996	-38.887362144415455	119637
d2bb11b4fc1146bcbc2edc0f029a813b1a0d5b3d	sparse ls-svm two-steps model selection method	quadratic programming;least squares approximations;support vector machines;support vector machines least squares approximations quadratic programming;sonar abstracts heart;localized generalization error minimization sparse ls svm two steps model selection method least square support vector machine hinge loss function least square loss function quadratic programming training method linear system solving problem pruning procedure two step hyper parameter selection method kernel parameters penalty parameters distance between two classes method between class separation maximization feature space;sparse ls svms localized generalization error model distance between two classes sensitivity measure model selection	Least Square Support Vector Machine (LS-SVM) converts the hinge loss function of SVM into a least square loss function which simplified the original quadratic programming training method to a linear system solving problem. Sparse LS-SVM is obtained with a pruning procedure. The performance of sparse LS-SVM depends on the selection of hyper-parameters (i.e. kernel and penalty parameters). Currently, CV and LOO are the most common methods to select hyper-parameters for LS-SVM. However, CV is computationally expensive while LOO yields a high variance of validation error which may mislead the selection of hyper-parameters. Selecting both kernel and penalty parameters simultaneously needs to search in a high dimensional parameter space. In this work, we propose a new two-step hyper-parameter selection method. Distance between Two Classes (DBTC) method is adopted to select the kernel parameters based on a maximization of between-class separation of the projected samples in the feature space. However, data distribution could not be helpful in penalty parameter selection. Therefore, we propose to select the penalty parameter via a minimization of a Localized Generalization Error to enhance the generalization capability of the LS-SVM. Experimental results comparing to existing methods show the proposed two-step method yields better LS-SVMs in term of average testing accuracies.	analysis of algorithms;expectation–maximization algorithm;feature vector;generalization error;hinge loss;kernel (operating system);least squares;linear system;loss function;model selection;quadratic programming;selection (genetic algorithm);sparse approximation;sparse language;sparse matrix;support vector machine;teaching method	Binbin Sun;Daniel S. Yeung	2012	2012 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2012.6358967	support vector machine;least squares support vector machine;mathematical optimization;computer science;machine learning;pattern recognition;mathematics;quadratic programming;statistics	ML	24.31822869797875	-38.68088217975894	119751
0700d36427b3a6cb6223a639ffea11570400ab25	doing the best we can with what we have: multi-label balancing with selective learning for attribute prediction		Attributes are human describable features, which have been used successfully for face, object, and activity recognition. Facial attributes are intuitive descriptions of faces and have proven to be very useful in face recognition and verification. Despite their usefulness, to date there is only one large-scale facial attribute dataset, CelebA (Liu et al. 2015). Impressive results have been achieved on this dataset, but it exhibits a variety of very significant biases. As CelebA contains mostly frontal idealized images of celebrities, it is difficult to generalize a model trained on this data for use on another dataset (of non celebrities). A typical approach to dealing with imbalanced data involves sampling the data in order to balance the positive and negative labels, however, with a multi-label problem this becomes a non-trivial task. By sampling to balance one label, we affect the distribution of other labels in the data. To address this problem, we introduce a novel Selective Learning method for deep networks which adaptively balances the data in each batch according to the desired distribution for each label. The bias in CelebA can be corrected for in this way, allowing the network to learn a more robust attribute model. We argue that without this multi-label balancing, the network cannot learn to accurately predict attributes that are poorly represented in CelebA. We demonstrate the effectiveness of our method on the problem of facial attribute prediction on CelebA, LFWA, and the new University of Maryland Attribute Evaluation Dataset (UMD-AED), outperforming the state-of-the-art on each dataset (Liu et al. 2015). Introduction Attributes are semantic features which have been used for many different applications, ranging from face verification to action recognition (Kumar et al. 2009; Zheng et al. 2014). Accurately predicting facial attributes (e.g. gender, hair color, eye color, etc.) from images is a very difficult problem, and has become of recent interest in the computer vision community with the introduction of CelebA (Liu et al. 2015). CelebA is the first and only widely available largescale facial attribute dataset. Since it’s introduction, there has been a lot of progress made on the problem of facial attribute recognition from images. Deep CNNs have proven to be very effective in attribute prediction, with state-of-the-art Copyright c © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. methods using CNNs for feature extraction (Liu et al. 2015; Rudd, Gunther, and Boult 2016; Wang, Cheng, and Feris 2016). Impressive results have been achieved on CelebA (Liu et al. 2015), however this dataset exhibits some extreme biases. It consists of mostly frontal, high-quality, posed images of celebrities, which is not representative of real-world imagery. Models trained on this data without accounting for its biases are not likely to perform well on another domain (e.g. images of non celebrities, or low quality images). A traditional method for handling imbalanced data is to sample the data so as to balance a particular label. However, for the multi-label setting, the data cannot be balanced in this way. Sampling the data so that one label is balanced changes the distribution for the other labels. In an ideal world, we would have access to unbiased, balanced, precisely-labeled datasets, but with CelebA as the only widely available, largescale attribute dataset, we must find ways to overcome its biases in order to move towards a solution to the attribute prediction problem. In (Rudd, Gunther, and Boult 2016), the authors try to adjust for the imbalance in CelebA by introducing a mixed objective loss, which adjusts the back-propagation weights according to a given target distribution. This method is a step in the right direction, but does not take full advantage of deep networks. Deep CNNs use batch learning, and so the label imbalance must be addressed in each batch. We argue that in order to truly account for the bias in CelebA, label balancing must be performed at the batch level when training a CNN. To this end, we propose a domain-adaptive batch re-sampling method for training CNNs, which we call Selective Learning. Selective Learning adapts each batch separately for every attribute according to a given target distribution for that attribute. There are many under-represented attributes in CelebA, including bald, mustache, gray hair, etc, and several over-represented attributes, such as young, and nobeard. In order to remove the bias in CelebA, Selective Learning adapts the batches for each attribute, allowing the model to learn from balanced data in every batch. This multi-label balancing allows our deep network to learn features which truly represent the facial attributes, not just the bias in the training data. We evaluate the proposed Selective Learning on CelebA, LFWA, and a new dataset: UMD-AED. UMD-AED consists The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)	activity recognition;artificial intelligence;backpropagation;computer vision;deep learning;facial recognition system;feature extraction;gibbs sampling;granular computing;language localisation;loss function;multi-label classification;protein structure prediction;sampling (signal processing);sigmoid function;software propagation;test set;universal media disc	Emily M. Hand;Carlos D. Castillo;Rama Chellappa	2018			machine learning;artificial intelligence;computer science	AI	19.563829265580633	-47.87492714899202	119798
8bcff86569c3601480f3acb46992a4521487a3a1	class-specific mutual information variation for feature selection		Abstract Feature selection plays a critical role in pattern recognition. Feature selection aims to eliminate irrelevant and redundant features. A drawback of traditional feature selection methods is that they ignore the dynamic change of selected features with the class. To address this problem, we develop a novel linear feature selection method, namely, Dynamic Change of Selected Feature with the class (DCSF). In DCSF, we introduce a new term: the conditional mutual information between the selected features and the class when a candidate feature is considered. In addition, we replace the traditional feature relevancy term with a term that is based on conditional mutual information. To evaluate our method, we compare DCSF with five traditional methods and two state-of-the-art methods on 20 benchmark data sets. Experimental results show that DCSF outperforms seven other methods in terms of average classification accuracy and highest classification accuracy.	feature selection;pointwise mutual information	Wanfu Gao;Liang Hu;Ping Zhang	2018	Pattern Recognition	10.1016/j.patcog.2018.02.020	machine learning;artificial intelligence;feature selection;mutual information;conditional mutual information;data set;mathematics;pattern recognition	Vision	11.983227568596288	-43.41974253764197	120015
65858e2ed31459c468ef4d0b6a52837714d63b31	robust support vector machines for classification and computational issues	primal dual interior point method;learning algorithm;support vector machines;robust classification;robust support vector machines;convex optimization;second order cone program;statistical learning theory;learning problems;linear program;support vector machine	In this paper, we investigate the theoretical and numerical aspects of robust classification using support vector machines (SVMs) by providing second order cone programming and linear programming formulations. SVMs are learning algorithms introduced by Vapnik used either for classification or regression. They show good generalization properties and they are based on statistical learning theory. The resulting learning problems are convex optimization problems suitable for application of primal-dual interior points methods. We investigate the training of a SVM in the case where a bounded perturbation is added to the value of an input xi∈n. A robust SVM provides a decision function that is immune to data perturbations. We consider both cases where our training data are either linearly separable or non linearly separable respectively and provide computational results for real data sets.	support vector machine	Theodore B. Trafalis;Robin C. Gilbert	2007	Optimization Methods and Software	10.1080/10556780600883791	semi-supervised learning;support vector machine;least squares support vector machine;mathematical optimization;convex optimization;linear programming;perceptron;online machine learning;machine learning;linear classifier;pattern recognition;mathematics;sequential minimal optimization;relevance vector machine;computational learning theory;active learning;structured support vector machine	ML	21.445599093984217	-38.0267079179275	120520
70a51d3d33a0249231f27571bfa39b06aedcb3c3	stochastic subset selection for learning with kernel machines	quadratic programming;offline data batch processing;kernel;stochastic subset selection;qp problem;quadratic program;support vector machines;regression task;support vector machine svm;kernel expansion;anomaly detection;real time;training;recognition accuracy;data vector;recognition accuracy stochastic subset selection kernel machines machine learning support vector machines classification task regression task anomaly detection task svm quadratic programming qp problem offline data batch processing data vector stochastic indexing technique kernel expansion kernel basis function training algorithm computational efficiency;online learning;set theory;algorithms artificial intelligence computer simulation decision support techniques models statistical pattern recognition automated stochastic processes;support vector;kernel machine;vectors;machine learning;stochastic indexing technique;stochastic processes;indexing;computational complexity;decision support techniques;indexation;classification task;batch process;pattern classification;subset selection;pattern recognition;models statistical;artificial intelligence;algorithms;svm;regression analysis;pattern recognition automated;kernel training noise support vector machines vectors machine learning computational complexity;kernel machines;support vector machine;learning artificial intelligence;computational efficiency;kernel basis function;support vector machine svm kernel machine online learning pattern recognition;computer simulation;security of data;support vector machines indexing learning artificial intelligence pattern classification quadratic programming regression analysis security of data set theory;training algorithm;noise;anomaly detection task	Kernel machines have gained much popularity in applications of machine learning. Support vector machines (SVMs) are a subset of kernel machines and generalize well for classification, regression, and anomaly detection tasks. The training procedure for traditional SVMs involves solving a quadratic programming (QP) problem. The QP problem scales super linearly in computational effort with the number of training samples and is often used for the offline batch processing of data. Kernel machines operate by retaining a subset of observed data during training. The data vectors contained within this subset are referred to as support vectors (SVs). The work presented in this paper introduces a subset selection method for the use of kernel machines in online, changing environments. Our algorithm works by using a stochastic indexing technique when selecting a subset of SVs when computing the kernel expansion. The work described here is novel because it separates the selection of kernel basis functions from the training algorithm used. The subset selection algorithm presented here can be used in conjunction with any online training technique. It is important for online kernel machines to be computationally efficient due to the real-time requirements of online environments. Our algorithm is an important contribution because it scales linearly with the number of training samples and is compatible with current training techniques. Our algorithm outperforms standard techniques in terms of computational efficiency and provides increased recognition accuracy in our experiments. We provide results from experiments using both simulated and real-world data sets to verify our algorithm.	acclimatization;additive white gaussian noise;algorithmic efficiency;anomaly detection;basis function;batch processing;computation (action);computational complexity theory;concept drift;contain (action);experiment;gain;genetic selection;indexes;kernel (operating system);kernel method;machine learning;online and offline;pattern recognition;population parameter;quadratic programming;real-time clock;requirement;run time (program lifecycle phase);selection algorithm;statistical classification;subgroup;support vector machine;utility functions on indivisible goods	Jason P. Rhinelander;Peter Xiaoping Liu	2012	IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)	10.1109/TSMCB.2011.2171680	support vector machine;kernel method;string kernel;kernel embedding of distributions;radial basis function kernel;computer science;machine learning;pattern recognition;data mining;tree kernel;quadratic programming;polynomial kernel	ML	17.3157478259363	-43.93028953477803	120769
b7b359ea8f37777641df6a624e60fb45f33327cd	application of gist svm in cancer detection		In this paper, we study the application of GIST SVM in disease prediction (detection of cancer). Pattern classification problems can be effectively solved by Support vector machines. Here we propose a classifier which can differentiate patients having benign and malignant cancer cells. To improve the accuracy of classification, we propose to determine the optimal size of the training set and perform feature selection. To find the optimal size of the training set, different sizes of training sets are experimented and the one with highest classification rate is selected. The optimal features are selected through their F-Scores.	f1 score;feature selection;gist;machine learning;statistical classification;support vector machine;test set	S. Aruna;S. P. Rajagopalan;L. V. Nandakishore	2011	CoRR		computer science;machine learning;pattern recognition;data mining;one-class classification	ML	13.743480214655369	-44.56843840954999	120815
a43189863ed5f963d07c809cad3c44631b47d2eb	a measure of relatedness for selecting consolidated task knowledge	neural network;empirical study	The selective transfer of task knowledge is studied within the context of multiple task learning (MTL) neural networks. Given a consolidated MTL network of previously learned tasks and a new primary task, T0, a measure of task relatedness is derived. The existing consolidated MTL network representation is fixed and an output for task T0 is connected to the hidden nodes of the network and trained. The cosine similarity between the hidden to output weight vectors for T0 and the weight vectors for each of the previously learned tasks is used as measure of task relatedness. The most related tasks are then used to learn T0 within a new MTL network using the task rehearsal method. Results of an empirical study on two synthetic domains of invariant concept tasks demonstrate the method’s ability to selectively transfer knowledge from the most related tasks so as to develop hypotheses with superior generalization.	artificial neural network;cosine similarity;synthetic intelligence	Daniel L. Silver;Richard Alisch	2005			artificial intelligence;empirical research;machine learning;artificial neural network;cosine similarity;invariant (mathematics);computer science	ML	16.709835444757637	-48.286613634992726	120858
63b4347cd65c00efae69726996752a2b1c869fa5	mixtrain: scalable training of formally robust neural networks		There is an arms race to defend neural networks against adversarial examples. Notably, adversarially robust training and verifiably robust training are the most promising defenses. The adversarially robust training scales well but cannot provide provable robustness guarantee for the absence of attacks. We present an Interval Attack that reveals fundamental problems about the threat model used by adversarially robust training. On the contrary, verifiably robust training achieves sound guarantee, but it is computationally expensive and sacrifices accuracy, which prevents it being applied in practice. In this paper, we propose two novel techniques for verifiably robust training, stochastic output approximation and dynamic mixed training, to solve the aforementioned challenges. They are based on two critical insights: (1) soundness is only needed in a subset of training data; and (2) verifiable robustness and test accuracy are conflicting to achieve after a certain point of verifiably robust training. On both MNIST and CIFAR datasets, we are able to achieve similar test accuracy and estimated robust accuracy against PGD attacks within 14× less training time compared to state-of-theart adversarially robust training techniques. In addition, we have up to 95.2% verified robust accuracy as a bonus. Also, to achieve similar verified robust accuracy, we are able to save up to 5× computation time and offer 9.2% test accuracy improvement compared to current state-of-the-art verifiably robust training techniques.	analysis of algorithms;approximation;coat of arms;computation;convolutional neural network;first-order predicate;formal verification;image resolution;kernel (operating system);mnist database;network topology;neural networks;provable security;rectifier (neural networks);threat model;time complexity	Shiqi Wang;Yizheng Chen;Ahmed Abdou;Suman Jana	2018	CoRR			ML	19.042034679569998	-50.99888836549099	121077
9cde95ab5ded8126cffc8c4e4442ba575fb8c533	tits-fm: transductive incremental takagi-sugeno fuzzy models	online learning;incremental learning;transductive similarity;ts fuzzy models;graph transduction	In this paper we present a novel model originating from Takagi-Sugeno fuzzy models. It is based on a concept of transductive similarity, where unlike a simple inductive similarity, it considers also local neighborhood of a given element. Transductive property of a local space is used in an inference process, what allows the technique to be used also in incremental settings. Since incremental model construction brings new challenges, we are unable to use the offline transductive approach as some of the previous works did. The key idea of our model is to adjust activation properties of each rule, based on cross-rule S-fuzzy models ransductive similarity raph transduction ncremental learning nline learning similarities. Our method is capable of using the transductive property for any metric. Besides the final model, we also present several improvements to the transductive similarity technique itself, where we alternate the similarity metric in several ways to better exploit the influence of local neighborhood in the final metric. At the end, we demonstrate a superior performance of our technique over the state-of-the-art techniques build on TS fuzzy models on several machine learning datasets. © 2014 Elsevier B.V. All rights reserved.	fm broadcasting;machine learning;online and offline;semantic similarity;transduction (machine learning)	Lukas Tencer;Marta Reznáková;Mohamed Cheriet	2015	Appl. Soft Comput.	10.1016/j.asoc.2014.09.024	transduction;machine learning;pattern recognition;data mining;mathematics	AI	21.396554347174483	-41.763200811409675	121447
29dc2463b52e43455476b1daa3a1aaa859f60216	synthetic data generator for classification rules learning		A standard data set is useful to empirically evaluate classification rules learning algorithms. However, there is still no standard data set which is common enough for various situations. Data sets from the real world are limited to specific applications. The sizes of attributes, the rules and samples of the real data are fixed. A data generator is proposed here to produce synthetic data set which can be as big as the experiments demand. The size of attributes, rules, and samples of the synthetic data sets can be easily changed to meet the demands of evaluation on different learning algorithms. In the generator, related attributes are created at first. And then, rules are created based on the attributes. Samples are produced following the rules. Three decision tree algorithms are evaluated used synthetic data sets produced by the proposed data generator.	algorithm;decision tree learning;experiment;machine learning;synthetic data	Runzong Liu;Bin Fang;Yuan Yan Tang;Patrick P. K. Chan	2016	2016 7th International Conference on Cloud Computing and Big Data (CCBD)	10.1109/CCBD.2016.076	computer science;data mining;algorithm design;decision tree;synthetic data;data set;machine learning;artificial intelligence	ML	14.477466623850134	-38.953556857999125	121578
8f7214bafbed6d4dfd397c35315c7275d5608f61	learning invariant representations with local transformations		Learning invariant representations is an important problem in machine learning and pattern recognition. In this paper, we present a novel framework of transformationinvariant feature learning by incorporating linear transformations into the feature learning algorithms. For example, we present the transformation-invariant restricted Boltzmann machine that compactly represents data by its weights and their transformations, which achieves invariance of the feature representation via probabilistic max pooling. In addition, we show that our transformation-invariant feature learning framework can also be extended to other unsupervised learning methods, such as autoencoders or sparse coding. We evaluate our method on several image classification benchmark datasets, such as MNIST variations, CIFAR-10, and STL-10, and show competitive or superior classification performance when compared to the state-of-the-art. Furthermore, our method achieves state-of-theart performance on phone classification tasks with the TIMIT dataset, which demonstrates wide applicability of our proposed algorithms to other domains.	algorithm;benchmark (computing);computer vision;convolutional neural network;experiment;feature learning;mnist database;machine learning;neural coding;pattern recognition;probabilistic automaton;restricted boltzmann machine;sparse matrix;timit;transformation matrix;unsupervised learning	Kihyuk Sohn;Honglak Lee	2012			semi-supervised learning;unsupervised learning;feature learning;multi-task learning;computer vision;instance-based learning;algorithmic learning theory;feature vector;feature;wake-sleep algorithm;feature extraction;online machine learning;machine learning;pattern recognition;mathematics;stability;competitive learning;computational learning theory;active learning;feature;dimensionality reduction;generalization error	ML	23.739795956285327	-47.12604187137747	121834
c67bca5402a57c5c3b26bd99cdd88a0835df3e15	feature selection using markov clustering and maximum spanning tree in high dimensional data	prediction algorithms;data mining;redundancy;feature extraction;clustering algorithms;markov processes;correlation	Feature selection is the most important preprocessing step for classification of high dimensional data. It reduces the load of computational cost and prediction time on classification algorithm by selecting only the salient features from the data set for learning. The main challenges while applying feature selection on high dimensional data (HDD) are: handling the relevancy, redundancy and correlation between features. The proposed algorithm works with the three main steps to overcome these issues. It focuses on filtering strategy for its effectiveness in handling the data sets with large size and high dimensions. Initially to measure the relevancy of features with respect to class, fisher score is calculated for each feature independently. Next, only relevant features are passed to the clustering algorithm to check the redundancy of features. Finally the correlation between features is calculated using maximum spanning tree and the most appropriate features are filtered out. The classification accuracy of the presented approach is validated by using C4.5, IB1 and Naive Bayes classifier. The proposed algorithm gives high classification accuracy when compared against the accuracies given by three different classifiers on the datasets containing features extracted from fisher score method and dataset containing all the features or full-featured dataset.	algorithmic efficiency;c4.5 algorithm;cluster analysis;computation;content-control software;dspace;feature extraction;feature selection;file spanning;fisher information;hard disk drive;markov chain monte carlo;minimum spanning tree;naive bayes classifier;offset binary;preprocessor;relevance;run time (program lifecycle phase);statistical classification	Neha Bisht;Annappa Basava	2016	2016 Ninth International Conference on Contemporary Computing (IC3)	10.1109/IC3.2016.7880208	minimum redundancy feature selection;prediction;feature extraction;computer science;machine learning;pattern recognition;data mining;markov process;cluster analysis;redundancy;correlation;statistics	ML	12.86138580304072	-44.575958902896964	122140
c07f84bd8737c8b5b1c2558e602696512949d8c9	the multiple imputation quantitative noise corrector	quantitative noise correction;data cleaning;multiple imputation;data quality	Relatively little attention has been given in the data mining literature to noise handling procedures that deal specifically with a continuous dependent variable. We present a novel procedure that addresses the problem of detecting and correcting noise when the outcome variable is continuous. Our technique uses a procedure for handling missing data called multiple imputation, a well-known statistical methodology based on sound theoretical principles. We demonstrate the utility of our procedure using a real-world dataset with inherent noise and multiple levels of injected noise in numerous carefully designed controlled experiments. Further, we present a comparison with noise correctors developed using five well-known estimation procedures, providing good coverage of the commonly-used classes of estimation techniques such as linear regression, decision trees and neural networks. The results presented in this work demonstrate conclusively the strong noise detection and correction results of our procedure, which outperforms the five competing noise correctors.	geo-imputation	Taghi M. Khoshgoftaar;Jason Van Hulse;Chris Seiffert;Lili Zhao	2007	Intell. Data Anal.		econometrics;data quality;computer science;noise measurement;data mining;imputation;statistics	AI	15.578222319433582	-38.11947041471434	122358
1b022613fbea96bacf3071b482bcb1eded7c0757	application of weighted support vector machines to network intrusion detection	network intrusion detection;support vector machine	Support Vector Machines(SVMs) have succeeded in many classification fields. Some researchers have tried to apply SVMs to Intrusion Detection recently and got desirable results. By analyzing C-SVM theoretically and experimentally, we found that C-SVM had some properties which showed C-SVM was not most suitable for Network Intrusion Detection. First, C-SVM has different classification error rates on different classes if the sizes of training classes are uneven. Second, C-SVM is over-dependent on every training sample, even if the samples are duplicated. Third, C-SVM does not make a difference between training samples. According to these characteristics of C-SVM and the fact that the size of network normal data is always much larger than that of intrusion data and the fact that the importance of attack data is different from each other, an extended C-SVM, termed weighted C-SVM is proposed in this paper. Weighed C-SVM introduces two parameters, class weights and sample weights. Class weights are used to adjust false negative rate and false positive rate of each intrusion class. And sample weights are used to emphasize importance of some intrusion samples. Experiments showed that Weighted C-SVM was more effective than C-SVM in network intrusion detection systems.	data point;experiment;intrusion detection system;support vector machine;test set;write combining;writing commons	Yinshan Jia;Chuanying Jia;Hongwei Qi	2004			computer science;machine learning;pattern recognition;data mining	ML	13.44460771495159	-40.6685322015629	122406
401aaa901d76ef1a8af1e676d3e3c07e94ed1c6c	multiview learning of weighted majority vote by bregman divergence minimization		We tackle the issue of classifier combinations when observations have multiple views. Our method jointly learns view-specific weighted majority vote classifiers (i.e. for each view) over a set of base voters, and a second weighted majority vote classifier over the set of these view-specific weighted majority vote classifiers. We show that the empirical risk minimization of the final majority vote given a multiview training set can be cast as the minimization of Bregman divergences. This allows us to derive a parallel-update optimization algorithm for learning our multiview model. We empirically study our algorithm with a particular focus on the impact of the training set size on the multiview learning results. The experiments show that our approach is able to overcome the lack of labeled information.	algorithm;bregman divergence;empirical risk minimization;experiment;f1 score;mathematical optimization;semi-supervised learning;semiconductor industry;test set;the matrix	Anil Goyal;Emilie Morvant;Massih-Reza Amini	2018		10.1007/978-3-030-01768-2_11	artificial intelligence;mathematics;machine learning;minification;empirical risk minimization;training set;majority rule;pattern recognition;bregman divergence	ML	20.541974251139553	-41.30425926665506	122459
89fb90b8058da62b9270c49af00f80aff0a6e3df	parameter selection for sub-hyper-sphere support vector machine	support vector machines genetic algorithms pattern classification;multi class classification problem;gaussian kernel parameter;support vector machines;subhyper sphere support vector machine;support vector machines kernel support vector machine classification computer science training data performance analysis data analysis lagrangian functions equations;genetic algorithm subhyper sphere support vector machine multi class classification problem gaussian kernel parameter;parameter selection;multi class classification;gaussian kernel;pattern classification;genetic algorithm;genetic algorithms;support vector machine	Sub-hyper-sphere support vector machines (SVMs) are proposed for solving the classification of the intersections of hyper-spheres when dealing with multi-class classification problem. Since the Gaussian kernel parameter influences the overlap position of the hyper-spheres, the resulting minimum bounding sphere-based classifier must be chosen optimally. This paper presents a new GA-based parameter selection method to get better generalization accuracy. Experimental results show the proposed approach is feasible and efficient.	bounding sphere;computation;hyper-heuristic;hyper-threading;multiclass classification;natural computing;software release life cycle;support vector machine	Peng Chen;Tao Wen	2007	Third International Conference on Natural Computation (ICNC 2007)	10.1109/ICNC.2007.540	margin classifier;support vector machine;least squares support vector machine;kernel method;mathematical optimization;radial basis function kernel;machine learning;linear classifier;pattern recognition;mathematics;relevance vector machine;polynomial kernel;structured support vector machine	Robotics	21.658955406303917	-39.404457134804645	122507
79bbac0c46ebce549eaedaa709dc44eb64fdea14	random shifting for cnn: a solution to reduce information loss in down-sampling layers		Down-sampling is widely adopted in deep convolutional neural networks (DCNN) for reducing the number of network parameters while preserving the transformation invariance. However, it cannot utilize information effectively because it only adopts a fixed stride strategy, which may result in poor generalization ability and information loss. In this paper, we propose a novel random strategy to alleviate these problems by embedding random shifting in the down-sampling layers during the training process. Random shifting can be universally applied to diverse DCNN models to dynamically adjust receptive fields by shifting kernel centers on feature maps in different directions. Thus, it can generate more robust features in networks and further enhance the transformation invariance of downsampling operators. In addition, random shifting cannot only be integrated in all down-sampling layers including strided convolutional layers and pooling layers, but also improve performance of DCNN with negligible additional computational cost. We evaluate our method in different tasks (e.g., image classification and segmentation) with various network architectures (i.e., AlexNet, FCN and DFNMR). Experimental results demonstrate the effectiveness of our proposed method.	algorithmic efficiency;artificial neural network;baseline (configuration management);benchmark (computing);computer vision;convolutional neural network;decimation (signal processing);gibbs sampling;imagenet;map;randomness;sampling (signal processing)	Gangming Zhao;Jingdong Wang;Zhaoxiang Zhang	2017		10.24963/ijcai.2017/486	theoretical computer science;artificial intelligence;machine learning;decimation;computer science	AI	23.6048528901609	-50.5583664360043	122678
defed342ebe5a4676b7f3877f52002990ff25e63	on feature selection for the prediction of phishing websites		with the rise of the big data paradigm, large data sets are being made available for knowledge mining. While this open up possibilities for new insights being gained every day, it also exposes data consumers to an increase in low quality, unreliable, redundant or noisy portions of the data. This would negatively affect the process of harvesting knowledge and recognizing patterns. Therefore, efficient feature selection methods to empower for real-time prediction or classification systems. Feature selection is the process of identifying the most relevant attributes and removing the redundant and irrelevant attributes. In this study, we implemented Kaiser-Meyer-Olkin (KMO) Test as a feature selection method and applied that to a publicly available phishing dataset, namely, the UCI of phishing website. furthermore, we used Logistic Regression and Support Vector Machine as classification methods to validate the feature selection method. Our results show just a slight difference in accuracy between implementation using full dataset features and the proposed much smaller dataset (almost 63% of original features set). This reduction in dimensionality is significant for the realtime systems especially when the accuracy reduction is slight. From there, we present a framework enabling a significant reduction in features. This opens the door for future work under which a wider set of classification algorithms will be tested in order to achieve the dimensionality reduction and an increase in performance accuracy.	algorithm;big data;data mining;dimensionality reduction;feature selection;kaiser window;logistic regression;phishing;programming paradigm;real-time clock;real-time computing;real-time operating system;relevance;statistical classification;support vector machine	Wesam Fadheel;Mohamed Abusharkh;Ikhlas Abdel-Qader	2017	2017 IEEE 15th Intl Conf on Dependable, Autonomic and Secure Computing, 15th Intl Conf on Pervasive Intelligence and Computing, 3rd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)	10.1109/DASC-PICom-DataCom-CyberSciTec.2017.146	support vector machine;big data;feature extraction;feature selection;dimensionality reduction;data set;curse of dimensionality;statistical classification;computer science;pattern recognition;artificial intelligence	ML	15.044040581545472	-47.070013974793376	122706
7adb22cb867856fa528fae7350f508f5ae7a4d54	adversarial data programming: using gans to relax the bottleneck of curated labeled data		Paucity of large curated hand-labeled training data forms a major bottleneck in the deployment of machine learning models in computer vision and other fields. Recent work (Data Programming) has shown how distant supervision signals in the form of labeling functions can be used to obtain labels for given data in near-constant time. In this work, we present Adversarial Data Programming (ADP), which presents an adversarial methodology to generate data as well as a curated aggregated label, given a set of weak labeling functions. We validated our method on the MNIST, Fashion MNIST, CIFAR 10 and SVHN datasets, and it outperformed many state-of-the-art models. We conducted extensive experiments to study its usefulness, as well as showed how the proposed ADP framework can be used for transfer learning as well as multi-task learning, where data from two domains are generated simultaneously using the framework along with the label information. Our future work will involve understanding the theoretical implications of this new framework from a game-theoretic perspective, as well as explore the performance of the method on more complex datasets.		Arghya Pal;Vineeth N. Balasubramanian	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00168	transfer of learning;machine learning;software deployment;adversarial system;labeled data;pattern recognition;artificial intelligence;computer science;mnist database;data modeling;training set;bottleneck	Vision	23.999912709383707	-48.07279311588707	122783
990afe0f11dd899d56d09c82111ce06840ab4705	supervised hashing with adaptive discrete optimization for multimedia retrieval		Hashing techniques show significant advantage in dealing with enormous high-dimensional image and multimedia data. Specifically, learning based hashing methods attract a lot of attention from researchers thanks to its great performance in image retrieval. But discrete constraint problem of learning based hashing methods makes the optimization extremely difficult, which can be shown to be NP hard. Thus, most of learning based hashing methods relax the constraint and get a suboptimal result. Recently, some researchers propose discrete optimization hashing techniques to learn hash bits without any relaxation and achieve promising results. But, discrete optimization hashing method like  Supervised Discrete Hashing (SDH)  roughly renews all binary codes and leads to a time-consuming problem. In this paper, we propose an  adaptive discrete cyclic coordinate descent (ACC)  method to effectively solve discrete optimization problem. The specific objective of our study is to boost the efficiency of discrete hash optimization with equivalent performance. We evaluate the proposed method on image and multimedia databases: CIFAR-10, NUS-WIDE and MIRFLickr-25k and show that our method achieves speed-up over compared the state-of-the-art methods, while having on-par and in some cases even better performance.	discrete optimization;hash function;mathematical optimization	Sixiu Chen;Fumin Shen;Yang Yang;Xing Xu;Jingkuan Song	2017	Neurocomputing	10.1016/j.neucom.2016.10.088	hash table;locality-sensitive hashing;machine learning;theoretical computer science;artificial intelligence;multimedia;extendible hashing;double hashing;hopscotch hashing;feature hashing;dynamic perfect hashing;universal hashing;computer science	Vision	20.233654470358022	-47.17398951280509	122957
3a0594d0d2b0843460014a875db8b3adbd3fc4f2	an optimal feature subset selection method based on distance discriminant and distribution overlapping	distance discriminant;distribution overlapping;feature subset selection;feature ranking;optimal feature selection	The goal of feature selection is to search the optimal feature subset with respect to the evaluation function. Exhaustively searching all possible feature subsets requires high computational cost. The alternative suboptimal methods are more efficient and practical but they cannot promise globally optimal results. We propose a new feature selection algorithm based on distance discriminant and distribution overlapping (HFSDD) for continuous features, which overcomes the drawbacks of the exhaustive search approaches and those of the suboptimal methods. The proposed method is able to find the optimal feature subset without exhaustive search or Branch and Bound algorithm. The most difficult problem for optimal feature selection, the search problem, is converted into a feature ranking problem following rigorous theoretical proof such that the computational complexity can be greatly reduced. Since the distribution of overlapping degrees between every two classes can provide useful information for feature selection, HFSDD also takes them into account by using a new approach to estimate the overlapping degrees. In this sense, HFSDD is a distance discriminant and distribution overlapping based solution. HFSDD was compared with ReliefF and mrmrMID on ten data sets. The experimental results show that HFSDD outperforms the other methods.	linear discriminant analysis	Jianning Liang;Su Yang;Yuanyuan Wang	2009	IJPRAI	10.1142/S0218001409007715	mathematical optimization;minimum redundancy feature selection;machine learning;pattern recognition;mathematics;feature selection;feature	Vision	11.25103421073701	-44.09033743031226	123141
ef01adfb76308f77dca5f9588941b1f564221378	multi-label lagrangian support vector machine with random block coordinate descent method	quadratic programming;multi label classification;kernel function;block coordinate descent method;support vector machine	When all training instances and labels are considered all together in a single optimization problem, multi-label support and core vector machines (i.e., Rank-SVM and Rank-CVM) are formulated as quadratic programming (QP) problems with equality and bounded constraints, whose training procedures have a sub-linear convergence rate. Therefore it is highly desirable to design and implement a novel efficient SVM-type multi-label algorithm. In this paper, through applying pairwise constraints between relevant and irrelevant labels, and defining an approximate ranking loss, we generalize binary Lagrangian support vector machine (LSVM) to construct its multi-label form (Rank-LSVM), resulting into a strictly convex QP problem with non-negative constraints only. Particularly, each training instance is associated with a block of variables and all variables are divided naturally into manageable blocks. Consequently we build an efficient training procedure for Rank-LSVM using random block coordinate descent method with a linear convergence rate. Moreover a heuristic strategy is applied to reduce the number of support vectors. Experimental results on twelve data sets demonstrate that our method works better according to five performance measures, and averagely runs 15 and 107 times faster and has 9 and 15% fewer support vectors, compared with Rank-CVM and Rank-SVM. © 2015 Published by Elsevier Inc.	approximation algorithm;convex function;coordinate descent;heuristic;mathematical optimization;multi-label classification;optimization problem;quadratic programming;rate of convergence;relevance;support vector machine	Jianhua Xu	2016	Inf. Sci.	10.1016/j.ins.2015.09.023	kernel;support vector machine;least squares support vector machine;mathematical optimization;combinatorics;computer science;machine learning;mathematics;relevance vector machine;quadratic programming;structured support vector machine;algorithm	AI	21.86944501179505	-38.58832071727861	123500
7d5152adaf54e0e9fb6d17dc562cbd96c43f50f3	computing diverse boolean networks from phosphoproteomic time series data		Logical modeling has been widely used to understand and expand the knowledge about protein interactions among different pathways. Realizing this, the caspo-ts system has been proposed recently to learn logical models from time series data. It uses Answer Set Programming to enumerate Boolean Networks (BNs) given prior knowledge networks and phosphoproteomic time series data. In the resulting sequence of solutions, similar BNs are typically clustered together. This can be problematic for large scale problems where we cannot explore the whole solution space in reasonable time. Our approach extends the caspo-ts system to cope with the important use case of finding diverse solutions of a problem with a large number of solutions. We first present the algorithm for finding diverse solutions and then we demonstrate the results of the proposed approach on two different benchmark scenarios in systems biology: (1) an artificial dataset to model TCR signaling and (2) the HPN-DREAM challenge dataset to model breast cancer cell lines.	boolean network;time series	Misbah Razzaq;Roland Kaminski;Javier Romero;Torsten Schaub;Jérémie Bourdon;Carito Guziolowski	2018		10.1007/978-3-319-99429-1_4	theoretical computer science;time series;model checking;computer science;systems biology;answer set programming	DB	12.132145802466313	-51.68893418838027	123669
b2f98b1a9496fb3e5398eda54afde5ddfd44c9a9	design and analysis of scalable rule induction systems	ta engineering general civil engineering general	Machine learning has been studied intensively during the past two decades. One motivation has been the desire to automate the process of knowledge acquisition during the construction of expert systems. The recent emergence of data mining as a major application for machine learning algorithms has led to the need for algorithms that can handle very large data sets. In real data mining applications, data sets with millions of training examples, thousands of attributes and hundreds of classes are common. Designing learning algorithms appropriate for such applications has thus become an important research problem. A great deal of research in machine learning has focused on classification learning. Among the various machine learning approaches developed for classification, rule induction is of particular interest for data mining because it generates models in the form o f IF-THEN rules which are more expressive and easier for humans to comprehend. One weakness with rule induction algorithms is that they often scale relatively poorly with large data sets, especially on noisy data. The work reported in this thesis aims to design and develop scalable rule induction algorithms that can process large data sets efficiently while building from them the best possible models. There are two main approaches for rule induction, represented respectively by CN2 and the AQ family of algorithms. These approaches vary in the search strategy employed for examining the space of possible rules, each of which has its own advantages and disadvantages. The first part of this thesis introduces a new rule induction algorithm for learning classification rules, which broadly follows the approach of algorithms represented by CN2. The algorithm presents a new search method which employs several novel search-space pruning rules and rule-evaluation techniques. This results in a highly efficient algorithm with improved induction performance. Real-world data do not only contain nominal attributes but also continuous attributes. The ability to handle continuously valued data is thus crucial to the success of any general purpose learning algorithm. Most current discretisation approaches are developed as preprocesses for learning algorithms. The second part of this thesis proposes a new approach which discretises continuous-valued attributes during the learning process. Incorporating discretisation into the learning process has the advantage of taking into account the bias inherent in the learning system as well as the interactions between the different attributes. This in turn leads to improved performance. Overfitting the training data is a major problem in machine learning, particularly when noise is present. Overfitting increases learning time and reduces both the accuracy and the comprehensibility of the generated rules, making learning from large data sets more difficult. Pruning is a technique widely used for addressing such problems and consequently forms an essential component of practical learning algorithms. The third part of this thesis presents three new pruning techniques for rule induction based on the Minimum Description Length (MDL) principle. The result is an effective learning algorithm that not only produces an accurate and compact rule set, but also significantly accelerates the learning process. RULES-3 Plus is a simple rule induction algorithm developed at the author’s laboratory which follows a similar approach to the AQ family of algorithms. Despite having been successfully applied to many learning problems, it has some drawbacks which adversely affect its performance. The fourth part of this thesis reports on an attempt to overcome these drawbacks by utilising the ideas presented in the first three parts of the thesis. A new version of RULES-3 Plus is reported that is a general and efficient algorithm with a wide range of potential applications. In The Name o f Allah, The Most Gracious, The Most Merciful	algorithm;data mining;discretization;emergence;expert system;interaction;knowledge acquisition;machine learning;mathematical induction;minimum description length;oracle advanced queuing;overfitting;rule 184;rule induction;scalability;signal-to-noise ratio	Ashraf A. Afify	2004			semi-supervised learning;unsupervised learning;multi-task learning;instance-based learning;algorithmic learning theory;weighted majority algorithm;computer science;artificial intelligence;online machine learning;machine learning;data mining;supervised learning;stability;computational learning theory;active learning;generalization error	ML	14.326928646731288	-38.8286763216061	124252
308b06b78aa7560269a1b22eecec3f4d80a2c6d9	when costs are unequal and unknown: a subtree grafting approach for unbalanced data classification	tree induction;unbalanced data classification;and unknown costs	In binary classifications, a decision tree learned from unbalanced data typically creates an important challenge related to the high misclassification rate of the minority class. Assigning different misclassification costs can address this problem, though usually at the cost of accuracy for the majority class. This effect can be particularly hazardous if the costs cannot be specified precisely. When the costs are unknown or difficult to determine, decision makers may prefer a classifier with more balanced accuracy for both classes rather than a standard or cost-sensitively learned one. In the context of learning trees, this research therefore proposes a new tree induction approach called subtree grafting (STG). On the basis of a real bank data set and several other data sets, we test the proposed STG method and find that our proposed approach provides a successful compromise between standard and cost-sensitive trees. [Submitted: June, 24, 2010. Revisions received: December 31, 2010; April 14, 2011. Accepted: April 28, 2011] Subject Areas: Tree Induction, Unbalanced Data Classification, and Unknown Costs.	decision tree;star trek generations;statistical classification;tree (data structure);tree rearrangement;unbalanced circuit	Jong-Seok Lee;Dan Zhu	2011	Decision Sciences	10.1111/j.1540-5915.2011.00332.x	mathematical optimization;computer science;artificial intelligence;machine learning;data mining;algorithm;statistics	AI	14.176067617988396	-39.76257680510811	124457
018e1d130a714d9ac4abdd8d9fd9ff60b5ef7ea2	immigrate: a margin-based feature selection method with interaction terms		By balancing margin-quantity maximization and margin-quality maximization, the proposed IMMIGRATE algorithm considers both local and global information when using margin-based frameworks. We here derive a new mathematical interpretation of marginbased cost function by using the quadratic form distance (QFD) and applying both the large-margin and max-min entropy principles. We also design a new principle for classifying new samples and propose a Bayesian framework to iteratively minimize the cost function. We demonstrate the power of our new method by comparing it with 16 widely used classifiers (e.g. Support Vector Machine, k-nearest neighbors, RELIEF, etc.) including some classifiers that are capable of identifying interaction terms (e.g. SODA, hierNet, etc.) on synthetic dataset, five gene expression datasets, and twenty UCI machine learning datasets. Our method is able to outperform other methods in most cases.		Ruzhang Zhao;Pengyu Hong;Jun S. Liu	2018	CoRR		support vector machine;mathematics;machine learning;artificial intelligence;feature selection;quadratic form;quality function deployment;bayesian probability;maximization	ML	21.32909885392995	-40.18372500692943	124604
79ffd2bc4a9d10ef81282da5ecbf5b08922dd46e	compressive feature learning		This paper addresses the problem of unsupervised feature learning for text data. Our method is grounded in the principle of minimum description length and uses a dictionary-based compression scheme to extract a succinct feature set. Specifically, our method finds a set of word k-grams that minimizes the cost of reconstructing the text losslessly. We formulate document compression as a binary optimization task and show how to solve it approximately via a sequence of reweighted linear programs that are efficient to solve and parallelizable. As our method is unsupervised, features may be extracted once and subsequently used in a variety of tasks. We demonstrate the performance of these features over a range of scenarios including unsupervised exploratory analysis and supervised text categorization. Our compressed feature space is two orders of magnitude smaller than the full k-gram space and matches the text categorization accuracy achieved in the full feature space. This dimensionality reduction not only results in faster training times, but it can also help elucidate structure in unsupervised learning tasks and reduce the amount of training data necessary for supervised learning.	categorization;dictionary;dimensionality reduction;document classification;feature learning;feature vector;grams;linear programming;lossless compression;mathematical optimization;minimum description length;supervised learning;text corpus;unsupervised learning	Hristo S. Paskov;Robert West;John C. Mitchell;Trevor J. Hastie	2013			semi-supervised learning;unsupervised learning;computer science;machine learning;pattern recognition;data mining	ML	23.299507369808435	-44.884159699358406	124697
3fb2daa2c5e5a590cf97d371c9d01596ca9afb40	nonlinear classification via linear svms and multi-task learning	linear svms;classification;multi task learning;nonlinear classification	Kernel SVM is prohibitively expensive when dealing with large nonlinear data. While ensembles of linear classifiers have been proposed to address this inefficiency, these methods are time-consuming or lack robustness. We propose an efficient classifier for nonlinear data using a new iterative learning algorithm, which partitions the data into clusters, and then trains a linear SVM for each cluster. These two steps are combined into a graphical model, with the parameters estimated efficiently using the EM algorithm. During training, clustered multi-task learning is used to capture the relatedness among the multiple linear SVMs and avoid overfitting. Experimental results on benchmark datasets show that our method outperforms state-of-the-art methods. During prediction, it also obtains comparable classification performance to kernel SVM, with much higher efficiency.	benchmark (computing);computer multitasking;expectation–maximization algorithm;generative model;google map maker;graphical model;iterative method;kernel (operating system);linear classifier;linux;monoidal t-norm logic;multi-task learning;nonlinear system;overfitting;support vector machine	Xue Mao;Ou Wu;Weiming Hu;Peter O'Donovan	2014		10.1145/2661829.2662068	multi-task learning;biological classification;computer science;machine learning;linear classifier;pattern recognition;data mining	ML	20.18091870587972	-40.61991787429574	124772
33812e0c6b8fdc5e414c5eae760e574e5a81190a	stochasticnet: forming deep neural networks via stochastic connectivity	neural networks;stochastic processes graph theory learning artificial intelligence neural nets;neural networks stochastic processes random graphs biological neural networks brain modeling data models;random graphs;brain modeling;stochastic processes;biological neural networks;data models;random graph deep convolutional nueral network stochasticnet;svhn stochasticnet deep neural networks stochastic connectivity machine learning neural connectivity formation neurons stochastic synaptic formations neural connections cifar 10 mnist	Deep neural networks are a branch in machine learning that has seen a meteoric rise in popularity due to its powerful abilities to represent and model high-level abstractions in highly complex data. One area in deep neural networks that are ripe for exploration is neural connectivity formation. A pivotal study on the brain tissue of rats found that synaptic formation for specific functional connectivity in neocortical neural microcircuits can be surprisingly well modeled and predicted as a random formation. Motivated by this intriguing finding, we introduce the concept of StochasticNet where deep neural networks are formed via stochastic connectivity between neurons. As a result, any type of deep neural networks can be formed as a StochasticNet by allowing the neuron connectivity to be stochastic. Stochastic synaptic formations in a deep neural network architecture can allow for efficient utilization of neurons for performing specific tasks. To evaluate the feasibility of such a deep neural network architecture, we train a StochasticNet using four different image datasets (CIFAR-10, MNIST, SVHN, and STL-10). Experimental results show that a StochasticNet using less than half the number of neural connections as a conventional deep neural network achieves comparable accuracy and reduces overfitting on the CIFAR-10, MNIST, and SVHN data sets. Interestingly, StochasticNet with less than half the number of neural connections, achieved a higher accuracy (relative improvement in test error rate of ~6% compared to ConvNet) on the STL-10 data set than a conventional deep neural network. Finally, the StochasticNets have faster operational speeds while achieving better or similar accuracy performances.	artificial neural network;benchmark (computing);convolutional neural network;deep learning;high- and low-level;mnist database;machine learning;network architecture;network formation;neural networks;neuron;overfitting;performance;random graph;resting state fmri;synaptic package manager;synaptic weight;ural (computer)	Mohammad Javad Shafiee;Parthipan Siva;Alexander Wong	2016	IEEE Access	10.1109/ACCESS.2016.2551458	winner-take-all;stochastic neural network;nervous system network models;random graph;stochastic process;data modeling;cellular neural network;types of artificial neural networks;computer science;artificial intelligence;recurrent neural network;theoretical computer science;machine learning;physical neural network;time delay neural network;deep learning;convolutional neural network;deep belief network;artificial neural network;statistics;spiking neural network	ML	22.05772135324901	-51.72959267696316	124882
9251c637264cbbad1f0328b41443fc1392fe18b4	acdc: a structured efficient linear layer		The linear layer is one of the most pervasive modules in deep learning representations. However, it requiresO(N) parameters andO(N) operations. These costs can be prohibitive in mobile applications or prevent scaling in many domains. Here, we introduce a deep, differentiable, fully-connected neural network module composed of diagonal matrices of parameters, A and D, and the discrete cosine transform C. The core module, structured as ACDC−1, has O(N) parameters and incurs O(N logN) operations. We present theoretical results showing how deep cascades of ACDC layers approximate linear layers. ACDC is, however, a stand-alone module and can be used in combination with any other types of module. In our experiments, we show that it can indeed be successfully interleaved with ReLU modules in convolutional neural networks for image recognition. Our experiments also study critical factors in the training of these structured modules, including initialization and depth. Finally, this paper also points out avenues for implementing the complex version of ACDC using photonic devices.	approximation algorithm;artificial neural network;byte;computation;computer vision;convolutional neural network;deep learning;discrete cosine transform;experiment;expressive power (computer science);fast fourier transform;graphics processing unit;huffman coding;image scaling;logistic regression;mobile app;pervasive informatics;rectifier (neural networks);recurrent neural network;singular value decomposition;sparse matrix;stochastic gradient descent;triune continuum paradigm	Marcin Moczulski;Misha Denil;Jeremy Appleyard;Nando de Freitas	2015	CoRR		mathematical optimization;theoretical computer science;machine learning;mathematics	ML	20.726050333937422	-48.448842313742794	124884
aa215e9f19a8e455e1aa8c5be9951a5755e1687c	a fast revised simplex method for svm training	quadratic programming;kernel;support vector machines support vector machine classification kernel quadratic programming lagrangian functions optimization methods matrix decomposition convergence risk management computer science;support vector machines;pricing;training;presses;simplex method;support vector machines quadratic programming;decomposition method;active set method;vectors;machine learning;quadratic programming fast revised simplex method svm training support vector machines incremental training active set method;support vector machine	Active set methods for training the support vector machines (SVM) are advantageous since they enable incremental training and, as we show in this research, do not exhibit exponentially increasing training times commonly associated with the decomposition methods as the SVM training parameter, C, is increased or the classification difficulty increases. Previous implementations of the active set method must contend with singularities, especially associated with the linear kernel, and must compute infinite descent directions, which may be inefficient, especially as C is increased. In this research, we propose a revised simplex method for quadratic programming, which has a guarantee of non-singularity for the sub-problem, and show how this can be adapted to SVM training.	active set method;authorization;ieee xplore;iteration;iterative method;quadratic programming;simplex algorithm;support vector machine	Christopher Sentelle;Georgios C. Anagnostopoulos;Michael Georgiopoulos	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761810	support vector machine;least squares support vector machine;mathematical optimization;computer science;machine learning;pattern recognition;mathematics;sequential minimal optimization;relevance vector machine;quadratic programming;structured support vector machine	Vision	20.72699022736264	-38.86741477335695	124891
0421c3afe37ad35ff36b7a9b1d8da3ee68275be3	cluster-based under-sampling approaches for imbalanced data distributions	imbalanced data distribution;data mining;classification;data distribution;real world application;distributed environment;under sampling;sampling methods;classification accuracy	For classification problem, the training data will significantly influence the classification accuracy. However, the data in real-world applications often are imbalanced class distribution, that is, most of the data are in majority class and little data are in minority class. In this case, if all the data are used to be the training data, the classifier tends to predict that most of the incoming data belongs to the majority class. Hence, it is important to select the suitable training data for classification in the imbalanced class distribution problem. In this paper, we propose cluster-based under-sampling approaches for selecting the representative data as training data to improve the classification accuracy for minority class and investigate the effect of under-sampling methods in the imbalanced class distribution environment. The experimental results show that our cluster-based under-sampling approaches outperform the other under-sampling techniques in the previous studies. 2008 Elsevier Ltd. All rights reserved.	artificial neural network;backpropagation;computer cluster;credit card fraud;entity–relationship model;experiment;intrusion detection system;performance;run time (program lifecycle phase);sampling (signal processing);word lists by frequency	Show-Jane Yen;Yue-Shi Lee	2009	Expert Syst. Appl.	10.1016/j.eswa.2008.06.108	sampling;biological classification;computer science;machine learning;pattern recognition;data mining;distributed computing environment	AI	13.470348151043371	-40.98453252973607	124995
cda2f01d90d98b71a49b00e34bdc03dc17d67da4	an entropy-based pruning method for cnn compression		This paper aims to simultaneously accelerate and compress off-the-shelf CNN models via filter pruning strategy. The importance of each filter is evaluated by the proposed entropy-based method first. Then several unimportant filters are discarded to get a smaller CNN model. Finally, finetuning is adopted to recover its generalization ability which is damaged during filter pruning. Our method can reduce the size of intermediate activations, which would dominate most memory footprint during model training stage but is less concerned in previous compression methods. Experiments on the ILSVRC-12 benchmark demonstrate the effectiveness of our method. Compared with previous filter importance evaluation criteria, our entropy-based method obtains better performance. We achieve 3.3× speed-up and 16.64× compression on VGG-16, 1.54× acceleration and 1.47× compression on ResNet-50, both with about 1% top-5 accuracy decrease.	benchmark (computing);deep learning;entropy (information theory);kalman filter;library (computing);memory footprint;object detection	Jian-Hao Luo;Jianxin Wu	2017	CoRR		machine learning;acceleration;pattern recognition;pruning;fold (higher-order function);artificial intelligence;memory footprint;computer science;compression (physics)	ML	21.77121386399503	-49.9447055595391	125021
b49b80c8376c6de2db5c30143fcab5c6b86c1016	efficient intrusion detection using representative instances	intrusion detection;feature selection;data preprocessing;intelligent algorithm;centroid based classification	Because of their feasibility and effectiveness, artificial intelligence-based intrusion detection systems attract considerable interest from researchers. However, when confronted with large-scale data sets, many artificial intelligence-based intrusion detection systems could suffer from a high computational burden, even though the feature selection method can help to reduce the computational complexity. To improve the efficiency, we propose a representative instance selection method to preprocess the original data set before training a classifier, which is independent of the learning algorithm that is used for constructing the intrusion detection system. In this study, a new metric is introduced to measure the representative power of an instance with respect to its class. Based on an implementation of representativeness, we select the most representative instance in each subset divided by a novel centroid-based partitioning strategy, and then, we utilise the result as training data to build various intrusion detection models efficiently. Experimental results on a labelled flow-based data set introduced in 2009 show that ANN, KNN, SVM and Liblinear learning with a largely reduced set of representative instances can not only achieve high efficiency in detecting network attacks but also provide comparable detection performance in terms of the detection rate, precision, F-score and accuracy, as compared with four corresponding classifiers built with the original large data set. a 2013 Elsevier Ltd. All rights reserved.	a library for support vector machines;artificial intelligence;computation;computational complexity theory;f1 score;feature selection;intrusion detection system;k-nearest neighbors algorithm;preprocessor;sensor	Chun Guo;Yajian Zhou;Yuan Ping;Shoushan Luo;Yu-Ping Lai;Zhongkun Zhang	2013	Computers & Security	10.1016/j.cose.2013.08.003	anomaly-based intrusion detection system;intrusion detection system;computer science;machine learning;pattern recognition;data mining;data pre-processing;feature selection;computer security	Security	12.606336723801014	-40.63501731576973	125214
3a15891d6ebdd48edd689b25aa61bab4538fc021	benchmarking of update learning strategies on digit classifier systems	training sample selection;expert systems;database management systems;feedback learning;pattern classification database management systems expert systems learning artificial intelligence;multi expert;pattern classification;cedar database update learning strategy digit classifier system labeled data multiexpert system classifier ensemble svm classifier naive bayes classifier support vector machines handwritten digit database;learning artificial intelligence;erbium feeds training knowledge based systems support vector machines learning systems silicon;training sample selection feedback learning multi expert	Three different strategies in order to re-train classifiers, when new labeled data become available, are presented in a multi-expert scenario. The first method is the use of the entire new dataset. The second one is related to the consideration that each single classifier is able to select new samples starting from those on which it performs a missclassification. Finally, by inspecting the multi expert system behavior, a sample misclassified by an expert, is used to update that classifier only if it produces a miss-classification by the ensemble of classifiers. This paper provides a comparison of three approaches under different conditions on two state of the art classifiers (SVM and Naive Bayes) by taking into account four different combination techniques. Experiments have been performed by considering the CEDAR (handwritten digit) database. It is shown how results depend by the amount of the new training samples, as well as by the specific combination decision schema and by classifiers in the ensemble.	database schema;experiment;expert system;feedback;iteration;iterative method;learning classifier system;mesa;naive bayes classifier;semi-supervised learning;semiconductor industry;supervised learning;test set	Donato Barbuzzi;Donato Impedovo;Giuseppe Pirlo	2012	2012 International Conference on Frontiers in Handwriting Recognition	10.1109/ICFHR.2012.186	random subspace method;margin classifier;naive bayes classifier;cascading classifiers;quadratic classifier;computer science;machine learning;pattern recognition;data mining;expert system	Vision	16.863000374319878	-42.860792067849616	125290
2029094e2280a2beb12027bd86dcbfac58c9bdd3	latent space robust subspace segmentation based on low-rank and locality constraints	low rank representation;robust coding;subspace segmentation;期刊论文;latent space	We propose a new latent space robust subspace segmentation method.We build the connections between the proposed algorithm and robust coding methods.We devise a weighted l1-norm regularizer to capture local structures of data sets. Low-rank representation (LRR) and its extensions have proven to be effective methods to handle different kinds of subspace segmentation applications. In this paper, we propose a new subspace segmentation algorithm, termed latent space robust subspace segmentation based on low-rank and locality constraints (LSRS2). Different from LRR, LSRS2 learns a low-dimensional space and a coefficient matrix for a data set simultaneously. In the obtained latent space, the coefficient matrix can faithfully reveal both the global and local structures for the data set. Furthermore, we build the connections between LSRS2 and robust coding methods, and show LSRS2 can be regarded as a kind of robust LRR method. Therefore, it can be guaranteed in theory that LSRS2 shows good performance. In addition, an efficient optimization method for solving LSRS2 is presented and its convergence is also proven. Extensive experiments show that the proposed algorithm outperforms the related subspace segmentation methods.	locality of reference;low-rank approximation	Lai Wei;Aihua Wu;Jun Yin	2015	Expert Syst. Appl.	10.1016/j.eswa.2015.04.041	mathematical optimization;machine learning;pattern recognition;mathematics;scale-space segmentation	Vision	24.601700396657492	-42.57643349836468	125369
b1a5b0664489d9d0d689c912eae96ebc356f8dfa	fusion of flir automatic target recognition algorithms	algorithmic composition;automatic target recognition;infrared;bayes classifier;winner take all	In this paper, we investigate several fusion techniques for designing a composite classifier to improve the performance (probability of correct classification) of forward-looking infrared (FLIR) automatic target recognition (ATR). The motivation behind the fusion of ATR algorithms is that if each contributing technique in a fusion algorithm (composite classifier) emphasizes on learning at least some features of the targets that are not learned by other contributing techniques for making a classification decision, a fusion of ATR algorithms may improve overall probability of correct classification of the composite classifier. In this research, we propose to use four ATR algorithms for fusion. The individual performance of the four contributing algorithms ranges from 73.5% to about 77% of probability of correct classification on the testing set. The set of correctly classified targets by each contributing algorithm usually has a substantial overlap with the set of correctly identified targets by other algorithms (over 50% for the four algorithms being used in this research). There is also a significant part of the set of correctly identified targets that is not shared by all contributing algorithms. The size of this subset of correctly identified targets generally determines the extent of the potential improvement that may result from the fusion of the ATR algorithms. In this research, we propose to use Bayes classifier, committee of experts, stacked-generalization, winner-takes-all, and ranking-based fusion techniques for designing the composite classifiers. The experimental results show an improvement of more than 6.5% over the best individual performance.	algorithm;automatic target recognition	Syed A. Rizvi;Nasser M. Nasrabadi	2003	Information Fusion	10.1016/S1566-2535(03)00043-5	winner-take-all;bayes classifier;infrared;computer science;machine learning;pattern recognition;data mining;remote sensing;automatic target recognition	Vision	13.4824547183397	-44.1707458930094	125403
c91290f48f496514fd4726a4a82bfd6857a2dbfd	face recognition using an nnsrm classifier in lda subspace	minimisation;reconnaissance visage;minimization;analisis componente principal;image processing;facies;structural risk minimization;vector space;procesamiento imagen;minimizacion;linear discriminate analysis;classification;traitement image;discriminant analysis;analyse discriminante;vecino mas cercano;analisis discriminante;face recognition;principal component analysis pca;principal component analysis;nearest neighbor;analyse composante principale;pattern recognition;plus proche voisin;nearest neighbour;nearest neighbor structure risk minimization nnsrm;espace vectoriel;reconnaissance forme;reconocimiento patron;espacio vectorial;clasificacion;linear discriminant analysis lda	NNSRM is an implementation of the structural risk minimization (SRM) principle using the nearest neighbor (NN) rule, and linear discriminant analysis (LDA) is a dimension-reducing method, which is usually used in classifications. This paper combines the two methods for face recognition. We first project the face images into a PCA subspace, then project the results into a much lower-dimensional LDA subspace, and then use an NNSRM classifier to recognize them in the LDA subspace. Experimental results demonstrate that the combined method can achieve a better performance than NN by selecting different distances and a comparable performance with SVM but costing less computational time.	computation;facial recognition system;linear discriminant analysis;local-density approximation;polynomial;principal component analysis;radial basis function kernel;structural risk minimization;time complexity	Danian Zheng;Meng Na;Jiaxin Wang	2007	Pattern Analysis and Applications	10.1007/s10044-007-0067-9	random subspace method;minimisation;speech recognition;facies;structural risk minimization;vector space;image processing;biological classification;computer science;machine learning;pattern recognition;mathematics;k-nearest neighbors algorithm;principal component analysis	Vision	23.69513246115664	-39.40211953003449	125505
7a10f6a406b664d1159e7c4fefbdd6ac275aee53	fast neighborhood subgraph pairwise distance kernel		We introduce a novel graph kernel called the Neighborhood Subgraph Pairwise Distance Kernel. The kernel decomposes a graph into all pairs of neighborhood subgraphs of small radius at increasing distances. We show that using a fast graph invariant we obtain significant speed-ups in the Gram matrix computation. Finally, we test the novel kernel on a wide range of chemoinformatics tasks, from antiviral to anticarcinogenic to toxicological activity prediction, and observe competitive performance when compared against several recent graph kernel methods.	cheminformatics;computation;gramian matrix;graph (discrete mathematics);graph kernel;graph property;induced subgraph;kernel (operating system);kernel method;numerical linear algebra	Fabrizio Costa;Kurt De Grave	2010			graph power;kernel method;factor-critical graph;combinatorics;string kernel;kernel embedding of distributions;radial basis function kernel;distance-regular graph;machine learning;pattern recognition;graph kernel;graph factorization;mathematics;voltage graph;distance-hereditary graph;butterfly graph;variable kernel density estimation;polynomial kernel;line graph	ML	16.64400193932146	-47.062098977157284	125550
a8ea92c28ba9838055ae3178a6f715f2f1e8477d	incremental hashing with kernels		We propose an incremental strategy for learning hash functions with kernels for large-scale image search. Our method is based on a two-stage classification framework that treats binary codes as intermediate variables between the feature space and the semantic space. In the first stage of classification, binary codes are considered as class labels by a set of binary SVMs; each corresponds to one bit. In the second stage, binary codes become the input space of a multi-class SVM. Hash functions are learned by an efficient algorithm where the NP-hard problem of finding optimal binary codes is solved via cyclic coordinate descent and SVMs are trained in a parallelized incremental manner. For modifications like adding images from an unseen class, we describe an incremental procedure for effective and efficient updates to the previous hash functions. Experiments on three largescale image datasets demonstrate the effectiveness of the proposed hashing method, SVM-based Hashing (SVM-Hash), over the state-of-the-art supervised hashing methods.	algorithm;binary code;coordinate descent;cryptographic hash function;feature vector;image retrieval;incremental compiler;mathematical optimization;np-hardness;parallel computing	Bahadir Ozdemir;Mahyar Najibi;Larry S. Davis	2016	CoRR		feature hashing;hopscotch hashing;hash table;double hashing;hash function;linear hashing;perfect hash function;extendible hashing;dynamic perfect hashing;open addressing;theoretical computer science;machine learning;universal hashing;pattern recognition;mathematics;k-independent hashing;cuckoo hashing;locality preserving hashing;2-choice hashing;locality-sensitive hashing	ML	20.114416282388852	-46.61307217567977	125606
5e215ca0ebd60d5fa6aeff50bc181bd1bbf62194	cohort-based kernel visualisation with scatter matrices	eigenvalues and eigenfunctions;pattern clustering;kernel;cohort based kernel visualisation;cluster index;cohort labelling;kernel functions;classification boundary;indicator function;kernel function;s matrix theory;nonlinear classifier;kernel functions cohort based kernel visualisation scatter matrices medical decision support patient database cohort labelling indicator function cluster index kernel trick class labelling nonlinear classifier classification boundary;class labelling;symmetric matrices;data visualisation;indexes;patient database;matrix decomposition;medical information systems;kernel covariance matrix data visualization matrix decomposition eigenvalues and eigenfunctions indexes symmetric matrices;indexation;decision support systems;data visualization;kernel trick;pattern classification;s matrix theory data visualisation decision support systems medical information systems pattern classification pattern clustering;scatter matrices;medical decision support;covariance matrix	A key question in medical decision support is how best to visualise a patient database, with especial reference to cohort labelling, whether this is an indicator function for classification or a cluster index. We propose the use of the kernel trick to visualise complete patient databases, in low-dimensional projections, with class labelling, given a non-linear classifier of choice. The results show that this method is useful both to see how individual patient cases relate to each other with reference to the classification boundary, and also to obtain a visual indication of the separation that can be obtained with difference choices of kernel functions.	andrey ershov;binary classification;colour banding;database;decision support system;dimensionality reduction;kernel (operating system);kernel method;linear classifier;linear separability;nonlinear system	Enrique Romero;Ana S. Fernandes;Tingting Mu;Paulo J. G. Lisboa	2010	The 2010 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2010.5596679	kernel;computer science;machine learning;pattern recognition;data mining;mathematics;data visualization;statistics	ML	15.979007433160866	-49.706409446567626	125621
629f100fbfbdd9dfa1d8e6f94c5420f03c17b6a4	learning a single tucker decomposition network for lossy image compression with multiple bits-per-pixel rates.		Lossy image compression (LIC), which aims to utilize inexact approximations to represent an image more compactly, is a classical problem in image processing. Recently, deep convolutional neural networks (CNNs) have achieved interesting results in LIC by learning an encoder-quantizer-decoder network from a large amount of data. However, existing CNN-based LIC methods usually can only train a network for a specific bits-perpixel (bpp). Such a “one network per bpp” problem limits the generality and flexibility of CNNs to practical LIC applications. In this paper, we propose to learn a single CNN which can perform LIC at multiple bpp rates. A simple yet effective Tucker Decomposition Network (TDNet) is developed, where there is a novel tucker decomposition layer (TDL) to decompose a latent image representation into a set of projection matrices and a core tensor. By changing the rank of core tensor and its quantization, we can easily adjust the bpp rate of latent image representation within a single CNN. Furthermore, an iterative non-uniform quantization scheme is presented to optimize the quantizer, and a coarse-to-fine training strategy is introduced to reconstruct the decompressed images. Extensive experiments demonstrate the state-of-the-art compression performance of TDNet in terms of both PSNR and MS-SSIM indices.	approximation;artificial neural network;benchmark (computing);color depth;convolutional neural network;encoder;experiment;image compression;image processing;iterative method;latent image;line integral convolution;lossy compression;multi-function printer;peak signal-to-noise ratio;pixel;quantization (signal processing);structural similarity;tucker decomposition	Jianrui Cai;Zisheng Cao	2018	CoRR		computer science;lossy compression;image processing;convolutional neural network;artificial intelligence;pattern recognition;image compression;tensor;machine learning;quantization (signal processing);color depth;tucker decomposition	ML	24.162703197710794	-51.209747580120876	125656
1c3d868bacfd822625ec6bdb851add916484f3c5	using multi-granular fuzzy linguistic modelling methods for supervised classification learning purposes		Classification learning is a very complex process whose success and failure ratio depends on a high amount of elements. One of them is the representation mean used for the data that is employed in the process. Granularity of the data used for classification learning purposes can affect dramatically the success and failure ratio of the obtained classification. In this paper, multi-granular fuzzy linguistic modelling methods are applied over the classification learning data in order to modify their granularity and increase the classification success ratio. Thanks to multi-granular fuzzy linguistic modelling methods, it is possible to automatically modify the data granularity in order to determine which data representation is the one that provides the better classification results in the learning process.	data (computing);machine learning;statistical classification;supervised learning	Juan Antonio Morente-Molinera;József Mezei;Christer Carlsson;Enrique Herrera-Viedma	2017	2017 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2017.8015406	linguistics;artificial intelligence;fuzzy logic;machine learning;computer science;one-class classification;granularity;external data representation;fuzzy classification;pattern recognition	Robotics	10.0910024237539	-38.333700783786696	125766
e74413cb6834dd648d62f4377cb337bdc60e3836	fine-tuning of the somknn classifier	neural nets;data mining;statistical analysis data mining decision making learning artificial intelligence neural nets pattern classification self organising feature maps;statistical analysis;self organising feature maps;fine tuning technique somknn classifier data mining task decision making process artificial neural networks automatic classification self organizing maps ann k nearest neighbor statistical classifier knn statistical classifier neuron relocation som map uci repository classification rate improvement;pattern classification;neurons databases training artificial neural networks quantization signal topology lattices;learning artificial intelligence	Classification is an important data mining task used in decision-making processes. Techniques such as Artificial Neural Networks (ANN) and Statistics are used to help in an automatic classification. In a previous work, we proposed a method for classification problems based on Self-Organizing Maps ANN (SOM) and k Nearest Neighbor (kNN) statistical classifier. The SOMkNN classifier, as we call this combination, is much faster than the traditional kNN and it keeps equivalent rates results. We propose a fine-tuning for this classifier here, which consists of a neuron relocation of the SOM map. The experiments presented compare SOMkNN with and without fine-tuning. Experiments using 8 databases, 6 of which are available in the UCI repository, the fine-tuning results are an improvement classification rate in 7 databases and in the last one the result is the same. The results indicate a trend of classification rate improvement with the application of the fine tuning technique. The gain in rate is approximately 1.2% and experiments were performed in order to correlate the results.	artificial neural network;data mining;database;experiment;neural network software;neuron;relocation (computing);statistical classification	Leandro A. da Silva;Edson C. Kitani;Emilio Del-Moral-Hernandez	2013	The 2013 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2013.6706972	quadratic classifier;computer science;machine learning;linear classifier;pattern recognition;data mining;artificial neural network	ML	10.832737600547903	-39.069637911182525	125881
c2f40d02c86608f7c815ca864ee55f2e1a0c5b06	classification model selection via bilevel programming	model selection;optimal method;convex optimization;optimization problem;machine learning;support vector classification;classification error;feature selection;cross validation;support vector machine;bilevel programming;equilibrium constraint	Support vector machines and related classification models require the solution of convex optimization problems that have one or more regularization hyper-parameters. Typically, the hyper-parameters are selected to minimize cross validated estimates of the out-of-sample classification error of the model. This cross-validation optimization problem can be formulated as a bilevel program in which the outer-level objective minimizes the average number of misclassified points across the cross-validation folds, subject to inner-level constraints such that the classification functions for each fold are (exactly or nearly) optimal for the selected hyper-parameters. Feature selection is included in the bilevel program in the form of bound constraints in the weights. The resulting bilevel problem is converted to a mathematical program with linear equilibrium constraints, which is solved using state-of-the-art optimization methods. This approach is significantly more versatile than commonly used grid search procedures, enabling, in particular, the use of models with many hyper-parameters. Numerical results demonstrate the practicality of this approach for model selection in machine learning.	algorithm;bilevel optimization;computation;computer multitasking;convex optimization;cross-validation (statistics);data point;feature selection;hinge loss;hyper-heuristic;machine learning;mathematical optimization;mathematical programming with equilibrium constraints;matrix regularization;missing data;model selection;multi-task learning;natural language processing;nonlinear programming;nonlinear system;numerical method;optimization problem;scalability;semi-supervised learning;semiconductor industry;solver;supervised learning;support vector machine;traverse;typo3;thinking outside the box;time complexity	Gautam Kunapuli;Kristin P. Bennett;Jing Hu;Jong-Shi Pang	2008	Optimization Methods and Software	10.1080/10556780802102586	optimization problem;support vector machine;mathematical optimization;convex optimization;machine learning;linear classifier;pattern recognition;mathematics;feature selection;bilevel optimization;cross-validation;model selection	ML	21.94261923247103	-38.087219656364674	126112
82e2ba43d5b86ceed110f3e28462c0b52d49ab11	sparse discriminative feature selection for multi-class alzheimer’s disease classification		In neuroimaging studies, high dimensionality and small sam- ple size have been always an issue, and it is common to apply a dimension reduction method to avoid the over-fitting problem. Broadly, there are two different approaches in reducing the feature dimensionality: feature selection and subspace learning. When it comes to the feature inter- pretability, the feature selection approach such as the sparse regularized linear regression method is preferable to the subspace learning meth- ods, especially in Alzheimer's Disease (AD) diagnosis. However, based on recent machine learning researches, the subspace learning methods presented promising results in various applications. To this end, in this work, we propose a novel method for discriminative feature selection by combining two conceptually different methodologies of feature selection and subspace learning in a unified framework. Specifically, we integrate the ideas of Fisher's linear discriminant analysis and locality preserving projection, which consider, respectively, the global and local information inherent in observations, in a regularized least square regression model. With the help of global and local information in data, we select class- discriminative and noise-resistant features that thus help enhance clas- sification performance. Furthermore, unlike the previous methods that mostly considered only a binary classification, in this paper, we consider a multi-class classification problem in AD diagnosis. Our experiments on the Alzheimer's Disease Neuroimaging Initiative dataset showed the efficacy of the proposed method by enhancing the performances in multi- class AD classification.		Xiaofeng Zhu;Heung-Il Suk;Dinggang Shen	2014		10.1007/978-3-319-10581-9_20	computer science;machine learning;linear classifier;pattern recognition;data mining;feature;dimensionality reduction	Vision	22.183032292974605	-43.70919926789174	126129
f3dccb11d168f0b9d8407bc4ba7802b2f2b7ab66	machine learning methods to analyze arabidopsis thaliana plant root growth		AbstractOne of the challenging problems in biology is to classify plants based on their reaction on genetic mutation. Arabidopsis Thaliana is a plant that is so interesting, because its genetic structure has some similarities with that of human beings. Biologists classify the type of this plant to mutated and not mutated (wild) types. Phenotypic analysis of these types is a time-consuming and costly effort by individuals. In this paper, we propose a modified feature extraction step by using velocity and acceleration of root growth. In the second step, for plant classification, we employed different Support Vector Machine (SVM) kernels and two hybrid systems of neural networks. Gated Negative Correlation Learning (GNCL) and Mixture of Negatively Correlated Experts (MNCE) are two ensemble methods based on complementary feature of classical classifiers; Mixture of Expert (ME) and Negative Correlation Learning (NCL). The hybrid systems conserve of advantages and decrease the effects of disadvantages of NCL and ME. Our Experimental shows that MNCE and GNCL improve the efficiency of classical classifiers, however, some SVM kernels function has better performance than classifiers based on neural network ensemble method. Moreover, kernels consume less time to obtain a classification rate.	artificial neural network;ensemble learning;feature extraction;hybrid system;machine learning;nested context language;support vector machine;two-hybrid screening;velocity (software development)	Hamidreza Farhidzadeh	2015	CoRR		bioinformatics;machine learning;pattern recognition	ML	11.32888424212292	-50.74974818798252	126249
278ad92eaeb78760afb4407999e445d6528286ff	redundancy-driven modified tomek-link based undersampling: a solution to class imbalance		Class imbalance can be defined as a span among data mining, machine learning and pattern recognition domains that provides to learn from a data-space having unequal class distribution. Common classifiers when trained by imbalanced data tend to bias towards the class possessing bulk instances causing misclassification of upcoming patterns/instances. The study reveals that presence of redundant borderline instances and outliers in the data-space severely catalyzes the effect of class imbalance. The Condensed Nearest Neighbor and Tomek-link undersampling techniques are used as the baseline systems for the present study, and an improved undersampling algorithm is proposed to be employed in the preprocessing stage by amalgamating aspects of outlier and redundancy detection to the baseline system. The proposed scheme imparts to detect outlier, redundant and noisy instances having least contribution in estimating accurate class labels. Thus, a data-level solution has been offered to the concerned problem with novelty in effective elimination of majority instances without losing valuable information. The proposed scheme is implemented and validated with Back Propagation Neural Network (BPNN), K-NearestNeighbor (K-NN), Support Vector Machine (SVM) and Naive Bayes classifiers for 10 real-life datasets. The experimental results obtained clearly manifest the superiority of the proposed scheme over the baseline schemes. © 2016 Elsevier B.V. All rights reserved.	backpropagation;baseline (configuration management);data mining;k-nearest neighbors algorithm;machine learning;naive bayes classifier;pattern recognition;preprocessor;real life;software propagation;support vector machine;undersampling	Debashree Devi;Saroj K. Biswas;Biswajit Purkayastha	2017	Pattern Recognition Letters	10.1016/j.patrec.2016.10.006	machine learning;pattern recognition;data mining;mathematics;statistics	AI	14.297092655813632	-41.922270078788564	126473
6870d5c568bd852c314c352360cac912adf0fd23	enhancing transparency of black-box soft-margin svm by integrating data-based prior information		The lack of transparency often makes the black-box models difficult to be applied to many practical domains. For this reason, the current work, from the black-box model input port, proposes to incorporate data-based prior information into the black-box softmargin SVM model to enhance its transparency. The concept and incorporation mechanism of data-based prior information are successively developed, based on which the transparent or partly transparent SVM optimization model is designed and then solved through handily rewriting the optimization problem as a nonlinear quadratic programming problem. An algorithm for mining data-based linear prior information from data set is also proposed, which generates a linear expression with respect to two appropriate inputs identified from all inputs of system. At last, the proposed transparency strategy is applied to eight benchmark examples and two real blast furnace examples for effectiveness exhibition.	algorithm;benchmark (computing);black box;commercial software;converge;experiment;mathematical optimization;nonlinear system;optimization problem;quadratic programming;rate of convergence;rewriting;support vector machine	Shaohan Chen;Chuanhou Gao;Ping Zhang	2017	CoRR		support vector machine;black box (phreaking);quadratic programming;nonlinear system;transparency (graphic);artificial intelligence;computer science;rewriting;optimization problem;pattern recognition	ML	21.7869428148955	-38.13971633519065	126496
74eb9f3499afc1360b608064701c11caae7b19b1	an innovative one-class least squares support vector machine model based on continuous cognition		One-class classification is a basic problem in machine learning. Unlike the existing typical one-class classifiers designed from the angle of probability or geometric, this paper attempts to study this problem from the bionics point of view. Using the continuous cognition characteristic as the starting point, we propose a new framework of one-class classifier, named multiple regression model (OC-MR), which can be seen as a natural extension of multiple regression for one-class classification problem. This paper applies least squares support vector machine (LSSVM) as an example to show the modeling process of the proposed method and the corresponding one-class classifier is named one-class least squares support vector machine (OC-LSSVM). Various simulation and real-life datasets are used to test the performance of the proposed OC-LSSVM. The existing popular one-class classification methods including Parzen kernel density estimation, support vector data description and Gaussian mixture model are also applied in order to achieve a comprehensive comparison. The results show that OC-LSSVM has achieved the best performance in most of the simulation and real-life datasets due to its good robustness, which highlights the efficacy of OC-LSSVM.	algorithm;cognition;general-purpose modeling;google map maker;kernel density estimation;least squares support vector machine;machine learning;mixture model;one-class classification;optimistic concurrency control;real life;simulation	Guangzao Huang;Zijiang Yang;Xiaojing Chen;Guoli Ji	2017	Knowl.-Based Syst.	10.1016/j.knosys.2017.02.024	margin classifier;least squares support vector machine;computer science;artificial intelligence;machine learning;linear classifier;pattern recognition;data mining;relevance vector machine;structured support vector machine;statistics	ML	23.95963977908537	-40.66623994603172	126512
30b045834a98b052189ab52cebc82293a50270a8	fusion of neural gas	ensemble method;neural gas;machine fusion;data mining application;computer experiment;machine ensembles;vector quantizer	One of the most important feature of the Neural Gas is its ability to preserve the topology in the projection of highly dimensional input spaces to lower dimensions vector quantizations. For this reason, the Neural Gas has proven to be a valuable tool in data mining applications. In this paper an incremental ensemble method for the combination of various Neural Gas models is proposed. Several models are trained with bootstrap samples of the data, the “codebooks” with similar Voronoi polygons are merged in one fused node and neighborhood relations are established by linking similar fused nodes. The aim of combining the Neural Gas is to improve the quality and robustness of the topological representation of the single model. We have called this model Fusion-NG. Computational experiments show that the Fusion-NG model effectively preserves the topology of the input space and improves the representation of the single Neural Gas model. Furthermore, the Fusion-NG explicitly shows the neighborhood relations of it prototypes. We report the performance results using synthetic and real datasets, the latter obtained from a benchmark site.	benchmark (computing);codebook;computation;data mining;dhrystone;experiment;neural gas;oracle fusion middleware	Sebastián Moreno;Héctor Allende;Rodrigo Salas;Carolina Saavedra	2007		10.1007/978-3-540-73053-8_56	neural gas;computer experiment;computer science;artificial intelligence;machine learning;pattern recognition;data mining	ML	20.34489633459835	-42.68970977965604	126564
30ee6d1e5c51197bc999ebbd335dec05bbb59bb4	reconstructing complex networks with binary-state dynamics	cs si;qc physics;nlin ao;journal article;physics soc ph	The prerequisite for our understanding of many complex networked systems lies in the reconstruction of network structure from measurable data. Although binary-state dynamics occurring in a broad class of complex networked systems in nature and society and has been intensively investigated, a general framework for reconstructing complex networks from binary states, the inverse problem, is lacking. Here we offer a general solution to the reconstruction problem by developing a data-based linearization approach for binary-state dynamics with linear, nonlinear, discrete and stochastic switching functions. The linearization allows us to convert the network reconstruction problem into a sparse signal reconstruction problem that can be resolved efficiently and credibly by convex optimization based on compressed sensing. The completely data-based linearization method and the sparse signal reconstruction constitutes a general framework for reconstructing complex networks without any knowledge of the binary-state dynamics occurring on them in an extremely efficient and robust manner. Our framework has been validated by several different kinds of binary-state dynamics in combination with a large number of artificial and real complex networks. A universal high reconstruction accuracy is achieved in spite of the measurement noise and missing data of partial nodes. Our approach opens a new route to the inverse problem in complex networked systems with binary-state dynamics and improves our ability to fully understand and control their emergent dynamics in a comprehensive way. ∗Electronic address: wenxuwang@bnu.edu.cn	complex network;compressed sensing;convex optimization;emergence;mathematical optimization;missing data;nonlinear system;reconstruction conjecture;signal reconstruction;social network;sparse matrix	Jingwen Li;Wen-Xu Wang;Ying-Cheng Lai;Celso Grebogi	2015	CoRR		computer science;artificial intelligence	ML	14.15025929352812	-51.92486466237202	126766
ec25da04ef7f09396ca00da3f9b5f2d9670cb6fc	methods of combining multiple classifiers and their applications to handwriting recognition	us zipcode database multiple classifier combination bayesian classifiers k nearest neighbour classifiers handwriting recognition distance classifiers unconstrained handwritten numerals;handwriting recognition;handwriting recognition pattern recognition character recognition speech recognition hidden markov models remote sensing classification algorithms brain modeling bayesian methods databases;caracter manuscrito;manuscript character;clasificador;classification;multiple classifiers;classifier;reconnaissance caractere;combining classifier;pattern recognition;classificateur;k nearest neighbor;reconnaissance forme;reconocimiento patron;multiple;caractere manuscrit;character recognition;clasificacion;reconocimiento caracter	Method of combining the classification powers of several classifiers is regarded as a general problem in various application areas of pattern recognition, and a systematic investigation has been made. Possible solutions to the problem can be divided into three categories according to the levels of information available from the various classifiers. Four approaches are proposed based on different methodologies for solving this problem. One is suitable for combining individual classifiers such as Bayesian, k-NN and various distance classifiers. The other three could be used for combining any kind of individual classifiers. On applying these methods to combine several classifiers for recognizing totally unconstrained handwritten numerals, the experimental results show that the performance of individual classifiers could be improved significantly. For example, on the U.S. zipcode database, the result of 98.9% recognition with 0.90% substitution and 0.2% rejection can be obtained, as well as a high reliability with 95% recognition, 0% substitution and 5% rejection. These results compared favorably to other research p u p s in Europe, Asia, and North America.	handwriting recognition;k-nearest neighbors algorithm;pattern recognition;rejection sampling	Lei Xu;Adam Krzyzak;Ching Y. Suen	1992	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.155943	random subspace method;speech recognition;cascading classifiers;classifier;biological classification;computer science;machine learning;pattern recognition;handwriting recognition;k-nearest neighbors algorithm;multiple	Vision	13.422356327005627	-43.9452931276353	126804
519b78e0edab88b23ba7a9a119df7d5cf83842ca	convolutional neural networks for unsupervised anomaly detection in text data		In this paper, we discuss the problem of anomaly detection in text data using convolutional neural network (CNN). Recently CNNs have become one of the most popular and powerful tools for various machine learning tasks. CNN’s main advantage is an ability to extract complicated hidden features from high dimensional data with complex structure. Usually CNNs are applied in supervised learning mode. On the other hand, unsupervised anomaly detection is an important problem in many applications, including computer security, behavioral analytics, etc. Since there is no specified target in unsupervised mode, traditional CNN’s objective functions cannot be used. In this paper, we develop a specific CNN architecture. It consists of one convolutional layer and one subsampling layer, we use RBF activation function and logarithmic loss function on the final layer. Minimization of the corresponding objective function helps us to calculate the location parameter of the features’ weights discovered on the last network layer. We use (l_2)-regularization to avoid degenerate solution. Proposed CNN has been tested on anomalies discovering in a stream of text documents modeled with well-known Enron dataset, where proposed method demonstrates better results in comparison with the traditional outlier detection methods based on one-class SVM and NMF.	anomaly detection;convolutional neural network	Oleg Gorokhov;Mikhail Petrovskiy;I. V. Mashechkin	2017		10.1007/978-3-319-68935-7_54	computer science;machine learning;anomaly detection;behavioral analytics;architecture;artificial intelligence;supervised learning;convolutional neural network;one-class classification;deep learning;pattern recognition;activation function	ML	18.883558321543806	-48.62774954631836	126924
d8de2a6e214def6609760ccd3fc2e83eb2b4186c	unsupervised feature selection with controlled redundancy (ufescor)	redundancy control unsupervised feature selection dimensionality reduction;unsupervised feature selection;topology;unsupervised learning feature selection redundancy;sammon error unsupervised feature selection with controlled redundancy ufescor scheme supervised feature selection technique decision making cost measurement errors objective function data set topology;redundancy logic gates linear programming network topology feature extraction measurement uncertainty unsupervised learning;measurement uncertainty;gradient descent technique;dimensionality reduction;redundancy;logic gates;feature extraction;redundancy control;linear programming;correlation;sammon s error	Features selected by a supervised/ unsupervised technique often include redundant or correlated features. While use of correlated features may result in an increase in the design and decision making cost, removing redundancy completely can make the system vulnerable to measurement errors. Most feature selection schemes do not account for redundancy at all, while a few supervised methods try to discard correlated features. We propose a novel unsupervised feature selection scheme (UFeSCoR), which not only discards irrelevant features, but also selects features with controlled redundancy. Here, the number of selected features can also be directed. Our algorithm optimizes an objective function, which tries to select a specified number of features, with a controlled level of redundancy, such that the topology of the original data set can be maintained in the reduced dimension. Here, we have used Sammon's error as a measure of preservation of topology. We demonstrate the effectiveness of the algorithm in terms of choosing relevant features, controlling redundancy, and selecting a given number of features using several data sets. We make a comparative study with five unsupervised feature selection methods. Our results reveal that the proposed method can select useful features with controlled redundancy.	algorithm;cns;centrality;coefficient;colon classification;controlled grammar;cross-validation (statistics);data redundancy;directed graph;feature selection;loss function;matrix multiplication;mechwarrior: living legends;mutual information;nonlinear system;optimization problem;procedural generation;redundancy (engineering);relevance;sammon mapping;sonar;supervised learning;tinymce;universal disk format;unsupervised learning	Monami Banerjee;Nikhil R. Pal	2015	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2015.2455509	minimum redundancy feature selection;logic gate;feature extraction;computer science;linear programming;machine learning;pattern recognition;data mining;redundancy;correlation;measurement uncertainty;dimensionality reduction	AI	14.287639353527258	-45.719643707727634	126972
26dda18412365d6c59866cf8cbc867a911727141	do better imagenet models transfer better		Transfer learning has become a cornerstone of computer vision with the advent of ImageNet features, yet little work has been done to evaluate the performance of ImageNet architectures across different datasets. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 13 classification models on 12 image classification tasks in three settings: as fixed feature extractors, fine-tuned, and trained from random initialization. We find that, when networks are used as fixed feature extractors, ImageNet accuracy is only weakly predictive of accuracy on other tasks (r = 0.24). In this setting, ResNets consistently outperform networks that achieve higher accuracy on ImageNet. When networks are fine-tuned, we observe a substantially stronger correlation (r = 0.86). We achieve state-of-the-art performance on eight image classification tasks simply by fine-tuning state-of-the-art ImageNet architectures, outperforming previous results based on specialized methods for transfer learning. Finally, we observe that, on three small fine-grained image classification datasets, networks trained from random initialization perform similarly to ImageNet-pretrained networks. Together, our results show that ImageNet architectures generalize well across datasets, with small improvements in ImageNet accuracy producing improvements across other tasks, but ImageNet features are less general than previously suggested.		Simon Kornblith;Jonathon Shlens;Quoc V. Le	2018	CoRR		pattern recognition;transfer of learning;machine learning;artificial intelligence;initialization;contextual image classification;computer science	Vision	22.175456394387783	-51.60604839351107	127119
09afb6a3c432f693964acc76622d0614a18422c3	discriminative supervised hashing for cross-modal similarity search		With the advantage of low storage cost and high retrieval efficiency, hashing techniques have recently been an emerging topic in cross-modal similarity search. As multiple modal data reflect similar semantic content, many researches aim at learning unified binary codes. However, discriminative hashing features learned by these methods are not adequate. This results in lower accuracy and robustness. We propose a novel hashing learning framework which jointly performs classifier learning, subspace learning and matrix factorization to preserve class-specific semantic content, termed Discriminative Supervised Hashing (DSH), to learn the discrimative unified binary codes for multi-modal data. Besides, reducing the loss of information and preserving the non-linear structure of data, DSH non-linearly projects different modalities into the common space in which the similarity among heterogeneous data points can be measured. Extensive experiments conducted on the three publicly available datasets demonstrate that the framework proposed in this paper outperforms several state-of -the-art methods.		Jun Yu;Xiao-Jun Wu;Josef Kittler	2018	CoRR			AI	24.39494966362952	-45.131322524478364	127214
469e0e79c936130b3727d598fac46913c75489f6	compression techniques for deep fisher vectors		This paper investigates the use of efficient compression tec hniques for Fisher vectors derived from deep architectures such as restricted Boltzmann machine (RBM). Fi sher representations have recently created a surge of interest by proving their worth for large scale object rec ognition and retrieval problems due to the intrinsic properties that make them unique from the conventional bag o f visual words (BoW) features, however they suffer from the problem of large dimensionality. This paper rovides empirical evidence along with visualisations to explore which of the feature normalisation and stat e of the art compression techniques is well suited for deep Fisher vectors, making them amenable for large scale visual retrieval with reduced memory footprint. We further show that the compressed Fisher vectors give impr essive classification results even with costless linear classifiers like k-nearest neighbour.	autoencoder;dimensionality reduction;experiment;fisher information;fisher–yates shuffle;graphics;imagenet;k-nearest neighbors algorithm;linear classifier;memory footprint;restricted boltzmann machine;stellar classification;support vector machine;t-distributed stochastic neighbor embedding;titan (supercomputer);video card	Sarah Ahmed;Tayyaba Azim	2017		10.5220/0006205002170224	machine learning;pattern recognition;computer science;artificial intelligence;compression (physics)	ML	22.93553120856781	-51.407266640885304	127215
94c62d39042821be7c0f9cebd955adf06cf68890	learning framework of multimodal gaussian-bernoulli rbm handling real-value input data		Abstract The conventional Gaussian–Bernoulli restricted Boltzmann machine (GBRBM), which is a RBM model for processing real-valued data, presumes single Gaussian distribution for learning real numbers. However, a single distribution is not able to effectively reflect complex data in many cases of real applications. In order to overcome this limitation, Gaussian mixture model (GMM) based RBM is proposed. As a learning mechanism for the proposed model, an energy function handling multi-modal distribution is provided. Then, a memetic algorithm (MA) was applied in order to train the proposed framework more accurately in real-valued input data. In order to show the effectiveness of the proposed framework, the method is applied to image reconstructions. The experiments show that the proposed framework provides more valid results than the other RBM based models in reconstruction error. Through the experiment results, it is concluded that the proposed framework is able to apply real-valued input data extensively and reduce difficulties of learning parameters by capturing the characteristics of real-value input data using GMM.	bernoulli polynomials;multimodal interaction;restricted boltzmann machine	Sanghyun Choo;Hyunsoo Lee	2018	Neurocomputing	10.1016/j.neucom.2017.10.018	machine learning;mixture model;artificial intelligence;real number;gaussian;memetic algorithm;pattern recognition;bernoulli's principle;complex data type;restricted boltzmann machine;computer science	Robotics	17.811190311682353	-44.96886638708851	127217
a83fc8eb1cbf5502099e59b2d0c57782c485dcd1	an empirical analysis of hubness in unsupervised distance-based outlier detection	standards;radiation detectors;training;learning systems;training data;context;conferences	Outlier detection is the task of automatic identification of unknown data not covered by training data (e.g. a previously unknown class in classification). We explore outlier detection in the presence of hubs and anti-hubs, i.e. data objects which appear to be either very close or very far from most other data due to a problem of measuring distances in high dimensions. We compare a classic distance based method to two new approaches, which have been designed to counter the negative effects of hubness, on six high-dimensional data sets. We show that mainly anti-hubs pose a problem for outlier detection and that this can be improved by using a hubness-aware approach based on re-scaling the distance space.	anomaly detection;automatic identification and data capture;computation;image scaling;rejection sampling	Arthur Flexer	2016	2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)	10.1109/ICDMW.2016.0106	training set;computer science;machine learning;pattern recognition;data mining;particle detector;one-class classification	DB	15.27598115371901	-40.87826077995007	127388
79d8dac37f3159fb538dfe00e092b59802f3e822	demystifying relational latent representations		Latent features learned by deep learning approaches have proven to be a powerful tool for machine learning. They serve as a data abstraction that makes learning easier by capturing regularities in dataion that makes learning easier by capturing regularities in data explicitly. Their benefits motivated their adaptation to relational learning context. In our previous work, we introduce an approach that learns relational latent features by means of clustering instances and their relations. The major drawback of latent representations is that they are often black-box and difficult to interpret. This work addresses these issues and shows that (1) latent features created by clustering are interpretable and capture interesting properties of data; (2) they identify local regions of instances that match well with the label, which partially explains their benefit; and (3) although the number of latent features generated by this approach is large, often many of them are highly redundant and can be removed without hurting performance much.	abstraction (software engineering);black box;cluster analysis;deep learning;machine learning;predictive modelling;sparse matrix;statistical classification	Sebastijan Dumancic;Hendrik Blockeel	2017		10.1007/978-3-319-78090-0_5	data mining;machine learning;artificial intelligence;computer science;deep learning;abstraction;statistical relational learning;probabilistic latent semantic analysis;cluster analysis	ML	21.87862432557201	-48.33990704705579	127538
59ce182f117eb51415468980ae5a77fa57172393	low-rank structure preserving for unsupervised feature selection		Abstract Unsupervised feature selection has been widely applied to machine learning and pattern recognition, as it does not require class labels. The majority of the popular unsupervised feature selection methods focus on various forms of reconstruction, and minimize the reconstruction residual by discarding features with low contributions. However, they cannot effectively preserve the data distribution in multiple subspaces, because the sample structure information is not substantially utilized to constrain the selected features. In this paper, we propose a low-rank structure preserving method for unsupervised feature selection (LRPFS) to address this shortcoming. The data matrix consisting selected features is assumed as a dictionary, which is learned by a low-rank constraint to preserve the subspace structure. Meanwhile, we further leverage the sparse penalty to remove the redundancy features, and thus obtain the discriminative features with intrinsic structures. In this way, the sample distribution can be preserved by low-rank constraint more precisely via using discriminative features. In turn, the refined sample structure boosts the selection of more representative features. The effectiveness of our method is supported by both theoretical and experimental results.	feature selection;unsupervised learning	Wei Zheng;Chunyan Xu;Jian Yang;Junbin Gao;Fa Zhu	2018	Neurocomputing	10.1016/j.neucom.2018.06.010	data matrix;artificial intelligence;pattern recognition;residual;redundancy (engineering);mathematics;machine learning;linear subspace;subspace topology;discriminative model;sampling distribution;feature selection	AI	23.930673421633617	-43.1433064059028	127670
63863683ec2b46c475dcf4808b91965fa7ac3349	geometric smote: effective oversampling for imbalanced learning through a geometric extension of smote		Classification of imbalanced datasets is a challenging task for standard algorithms. Although many methods exist to address this problem in different ways, generating artificial data for the minority class is a more general approach compared to algorithmic modifications. SMOTE algorithm and its variations generate synthetic samples along a line segment that joins minority class instances. In this paper we propose Geometric SMOTE (G-SMOTE) as a generalization of the SMOTE data generation mechanism. G-SMOTE generates synthetic samples in a geometric region of the input space, around each selected minority instance. While in the basic configuration this region is a hyper-sphere, G-SMOTE allows its deformation to a hyper-spheroid and finally to a line segment, emulating, in the last case, the SMOTE mechanism. The performance of G-SMOTE is compared against multiple standard oversampling algorithms. We present empirical results that show a significant improvement in the quality of the generated data when G-SMOTE is used as an oversampling algorithm.	algorithm;display resolution;emulator;gradient boosting;hyper-heuristic;instance (computer science);logistic regression;oversampling;synthetic data;synthetic intelligence	Georgios Douzas;Fernando Bação	2017	CoRR		standard algorithms;oversampling;test data generation;machine learning;line segment;mathematics;artificial intelligence;joins;pattern recognition	AI	14.269989840185808	-41.81053945204548	127888
59942bf95109326b0e9ee4257936698dcccabf0c	low-complexity data-parallel earth mover's distance approximations		The Earth Mover’s Distance (EMD) is a state-ofthe art metric for comparing probability distributions. The high distinguishability offered by the EMD comes at a high cost in computational complexity. Therefore, linear-complexity approximation algorithms have been proposed to improve its scalability. However, these algorithms are either limited to vector spaces with only a few dimensions or require the probability distributions to populate the vector space sparsely. We propose novel approximation algorithms that overcome both of these limitations, yet still achieve linear time complexity. All our algorithms are data parallel, and therefore, we can take advantage of massively parallel computing engines, such as Graphics Processing Units (GPUs). The experiments on MNIST images show that the new algorithms can perform a billion distance computations in less than a minute using a single GPU. On the popular text-based 20 Newsgroups dataset, the new algorithms are four orders of magnitude faster than the state-of-the-art FastEMD library and match its search accuracy.		Kubilay Atasu;Thomas Mittelholzer	2018	CoRR			DB	18.157723991336702	-47.0607695650651	127909
308930d847d7b6b8b25f688483f34b9389672524	optimistic active-learning using mutual information		An “active learning system” will sequentially decide which unlabeled instance to label, with the goal of efficiently gathering the information necessary to produce a good classifier. Some such systems greedily select the next instance based only on properties of that instance and the few currently labeled points — e.g., selecting the one closest to the current classification boundary. Unfortunately, these approaches ignore the valuable information contained in the other unlabeled instances, which can help identify a good classifier much faster. For the previous approaches that do exploit this unlabeled data, this information is mostly used in a conservative way. One common property of the approaches in the literature is that the active learner sticks to one single query selection criterion in the whole process. We propose a system, MM+M, that selects the query instance that is able to provide the maximum conditional mutual information about the labels of the unlabeled instances, given the labeled data, in an optimistic way. This approach implicitly exploits the discriminative partition information contained in the unlabeled data. Instead of using one selection criterion, MM+M also employs a simple on-line method that changes its selection rule when it encounters an “unexpected label”. Our empirical results demonstrate that this new approach works effectively.	conditional mutual information;discriminative model;experiment;greedy algorithm;linear separability;maxima and minima;online and offline;selection rule;statistical classification	Yuhong Guo;Russell Greiner	2007				AI	17.018723708664385	-39.593432634254825	127972
f54095ca46b304bb68594aa9df7aa1df39facc9d	reducing overfitting of adaboost by clustering-based pruning of hard examples	overfitting;hard to learn samples;clustering;adaboost	In order to solve the problem of overfitting in AdaBoost, we propose a novel AdaBoost algorithm using K-means clustering. AdaBoost is known as an effective method for improving the performance of base classifiers both theoretically and empirically. However, previous studies have shown that AdaBoost is prone to overfitting in overlapped classes. In order to overcome the overfitting problem of AdaBoost, the proposed method uses K-means clustering to remove hard-to-learn samples that exist on overlapped region. Since the proposed method does not consider hard-to-learn samples, it suffers less from the overfitting problem compared to conventional AdaBoost. Both synthetic and real world data were tested to confirm the validity of the proposed method.	adaboost;algorithm;cluster analysis;effective method;k-means clustering;overfitting;synthetic intelligence	Dae-Sun Kim;Yeul-Min Baek;Whoi-Yul Kim	2013		10.1145/2448556.2448646	adaboost;computer science;machine learning;pattern recognition;data mining;overfitting;cluster analysis;pruning	AI	14.487496225584987	-40.89946033040595	128043
f9b0f0c74add809b25cebcf5415f98f98530704a	an improved isomap algorithm for predicting protein localization	mdm-isomap;neighborhood selection;protein localization	In this paper, a system based on the MDM-Isomap (Minimax Distance Metric-based neighborhood selection algorithm for Isomap) is proposed to improve the performance of protein subcellular localization prediction. First of all, the protein sequences are quantized into a high dimension space using an effective sequence encoding scheme. However, the problems caused by such representation are computation complexity and complicated classifier design. To sort out this problem, a new dimension reduction algorithm, MDM-Isomap, is introduced. It is an improved isomap algorithm, which can acquire a suitable neighborhood size for Isomap. It extracts the essential features from the high dimension feature space. Then, an efficient classifier is employed to recognize the subcellular localization of proteins according to the new features after dimension reduction.	computation;dimensionality reduction;feature vector;isomap;line code;master data management;minimax;peptide sequence;protein subcellular localization prediction;quantization (signal processing);selection algorithm	Tong Wang;WenAn Tan;Hongmei Li	2011		10.1007/978-3-642-23339-5_44	mathematical optimization;machine learning;pattern recognition;mathematics	ML	12.756964560583649	-45.707447178781806	128099
f9d799512a351bba9519cecf8935a0187b57c0db	the impact of linear transformation on svm margin	support vector machines;classification performance;support vector machines support vector machine classification helium statistical learning cybernetics machine learning computational modeling mathematics computer science;transforms pattern classification support vector machines;svm margin;vector space;feature weight adjustment;statistical learning theory;linear vector space;feature weighting;transforms;pattern classification;linear transformation;classification performance linear transformation svm margin support vector machine linear vector space feature weight adjustment;support vector machine;generalization capability	It is well recognized that support vector machines (SVM) would produce better classification performance in terms of generalization power. Based on the statistical learning theory (SLT), the margin scale reflects the generalization capability to a great extent. The bigger the margin scale takes, the better the generalization capability of SVM will have. This paper makes an attempt to investigate the impact of linear transformation on SVM margin. The linear transformation maps a linear vector space into another with dimension change. From the result of experiments, we know that the margin of SVM can be enlarged through the appropriate linear transformation. By specifying a particular linear transformation that is feature weight adjustment, the relative amount of margin between two data sets can be improved significantly.	experiment;machine learning;map;statistical learning theory;support vector machine	Qiang He;Junfen Chen;Ming He;Rui-Xian Zhu	2006	2006 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2006.384489	margin classifier;support vector machine;margin;vector space;computer science;machine learning;pattern recognition;data mining;mathematics	ML	22.7596165283345	-40.50267882579509	128668
c67b4c03ef7b773784a18fcb33f782beef1ed32c	random composite forests		We introduce a broad family of decision trees, Composite Trees, whose leaf classifiers are selected out of a hypothesis set composed of p subfamilies with different complexities. We prove new data-dependent learning guarantees for this family in the multi-class setting. These learning bounds provide a quantitative guidance for the choice of the hypotheses at each leaf. Remarkably, they depend on the Rademacher complexities of the sub-families of the predictors and the fraction of sample points correctly classified at each leaf. We further introduce random composite trees and derive learning guarantees for random composite trees which also apply to Random Forests. Using our theoretical analysis, we devise a new algorithm, RANDOMCOMPOSITEFOREST (RCF), that is based on forming an ensemble of random composite trees. We report the results of experiments demonstrating that RCF yields significant performance improvements over both Random Forests and a variant of RCF in several tasks.	algorithm;attack tree;data dependency;decision tree;experiment;leaf subroutine;multiclass classification;rademacher complexity;radio frequency;random forest;ratchet & clank future: tools of destruction;row echelon form	Giulia DeSalvo;Mehryar Mohri	2016			random forest;machine learning;algorithm;statistics	AI	14.979293809733267	-40.00941196695361	128753
9ac334894eaf4b08b06d91753001a7b5f3b92e95	structural regularized projection twin support vector machine for data classification	twin projections;cluster based structural information;期刊论文;kernel trick;binary classification;support vector machine	Projection twin support vector machine (PTSVM) seeks two projection directions for two classes by solving two smaller-sized quadratic programming problems (QPPs), such that the projected samples of one class are well separated from those of the other one in its respective subspace. However, it only simply considers the prior class-based structural information in the optimization problems. In this paper, a structural regularized PTSVM (SRPTSVM) classifier for binary classification is presented. This proposed SRPTSVM focuses on the cluster-based structural information of the corresponding class in each optimization problem, which is vital for designing a good classifier in different real-world problems. This SRPTSVM is extended to a nonlinear version by the kernel trick. Experimental results demonstrate that SRPTSVM is superior in generalization performance to other classifiers. 2014 Elsevier Inc. All rights reserved.	binary classification;kernel method;mathematical optimization;nonlinear system;optimization problem;quadratic programming;support vector machine	Xinjun Peng;Dong Xu	2014	Inf. Sci.	10.1016/j.ins.2014.03.129	binary classification;support vector machine;kernel method;quadratic classifier;computer science;machine learning;linear classifier;pattern recognition;data mining;mathematics;structured support vector machine	AI	23.505913893477874	-40.81504331955426	128793
28e870903b3acf6e54b3222879fde93c0dcd72b1	multitask generalized eigenvalue program	generalized eigenvalue program;multitask learning	We present a novel multitask learning framework called multitask generalized eigenvalue program (MTGEP), which jointly solves multiple related generalized eigenvalue problems (GEPs). This framework is quite general and can be applied to many eigenvalue problems in machine learning and pattern recognition, ranging from supervised learning to unsupervised learning, such as principal component analysis (PCA), Fisher discriminant analysis (FDA), common spatial pattern (CSP), and so on. The core assumption of our approach is that the leading eigenvectors of related GEPs lie in some subspace that can be approximated by a sparse linear combination of basis vectors. As a result, these GEPs can be jointly solved by a sparse coding approach. Empirical evaluation with both synthetic and benchmark real world datasets validates the efficacy and efficiency of the proposed techniques, especially for grouped multitask GEPs.	approximation algorithm;basis (linear algebra);benchmark (computing);common spatial pattern;computer multitasking;dhrystone;linear discriminant analysis;machine learning;neural coding;pattern recognition;principal component analysis;sparse matrix;spatiotemporal pattern;supervised learning;unsupervised learning	Boyu Wang;Joelle Pineau;Borja Balle	2016			multi-task learning;mathematical optimization;computer science;machine learning;pattern recognition	ML	23.742085507066907	-43.69251301204688	128894
45aa83051d45112a01354139856fb5fcfe592f27	mcp: a multi-component learning machine for prediction of protein secondary structure		The Gene or DNA sequence in every cell does not control genetic properties on its own; Rather, this is done through translation of DNA into protein and formation of a certain 3D structure. The biological function of protein is tightly connected to its specific 3D structure. Prediction of the protein secondary structure is a crucial intermediate step towards elucidating its 3D structure and function. Traditional experimental methods for prediction of protein secondary structure are expensive and time-consuming. Therefore, in the past 45 years, various machine learning approaches have been put forth. Nevertheless, their average accuracy has hardly reached beyond 80%. The possible underlying reasons are abstruse sequence-structure relation, noise in input protein data, class imbalance and high dimensional encoding schemes that are used to represent protein sequences. In this paper, we propose an accurate multi-component prediction machine to overcome the challenges of protein secondary structure prediction. We address the high complexity challenge in sequencestructure relation using a multi-component designation. Furthermore, we utilize a compound string dissimilarity measure to directly interpret protein sequence content and avoid information loss. In order to improve the accuracy, we employ two different classifiers including support vector machine and fuzzy nearest neighbor and collectively aggregate the classification outcomes to infer the final protein secondary structures. We conduct comprehensive experiments to compare our model with the current state-of-the-art methods. The experimental results demonstrate that our method can accurately predict protein structure using the input sequences. However, the effectiveness of our unified model can be further enhanced through component configuration. † School of Computer Engineering, Iran University of Science and Technology, Tehran, Iran § ST Electronics SUTD Cyber Security Laboratory, Singapore University of Technology and Design, Singapore # School of Information Technology and Electrical Engineering, University of Queensland, Brisbane, Australia 1 Leila.Khalatbari@gmail.com 2 kangavari@iust.ac.ir 3 saeid.hosseini@uq.net.au 4 h.yin1@uq.edu.au 5 ngaiman cheung@sutd.edu.sg ar X iv :1 80 6. 06 39 4v 2 [ cs .C E ] 3 0 Ju n 20 18 ii Leila Khalatbari et al.	aggregate data;computer engineering;electrical engineering;experiment;feature extraction;feature vector;function (biology);fuzzy set;k-nearest neighbors algorithm;lz77 and lz78;machine learning;mathematical optimization;n-gram;natural language;numerical analysis;peptide sequence;pool (computer science);preprocessor;protein structure prediction;statistical classification;support vector machine;unified model	Leila Khalatbari;Mohammadreza Kangavari;Saeid Hosseini;Hongzhi Yin;Ngai-Man Cheung	2018	CoRR		data mining;support vector machine;protein secondary structure;protein structure;encoding (memory);artificial intelligence;pattern recognition;protein sequencing;computer science;k-nearest neighbors algorithm;curse of dimensionality;protein structure prediction	Comp.	11.39413565520349	-50.51085874543985	129092
c181251238b3f1b2108ee5fb68a0b4f431ee4282	structure learning of bayesian networks based on vertical segmentation data	structure learning;belief networks;bayesian network;sample selection;distributed learning method;centralized learning method;vertical segmentation data;learning methods;distributed learning;learning artificial intelligence belief networks;bayesian methods data mining learning systems computer architecture distributed computing encoding information technology information management technology management engineering management;learning artificial intelligence;centralized learning method bayesian networks vertical segmentation data distributed learning method;bayesian networks	A distributed approach in learning a Bayesian networks from vertical segmentation data was promoted in the paper. The approach includes four sequential steps: local learning, sample selection, cross learning, and combination of the results. The main improvement of the algorithm brings forward in the second step. The complex sub-structure of local BN is considered that exist a hidden node which contacts with the sub-structure. The hidden node exist in the other local BN. The experiment proved that the distributed learning method can learn almost the same structure as the result obtained by a centralized learning method.	algorithm;bayesian network;centralized computing	Hao Huang;Jianqing Huang	2007	Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)	10.1109/FSKD.2007.533	semi-supervised learning;unsupervised learning;feature learning;multi-task learning;instance-based learning;algorithmic learning theory;wake-sleep algorithm;computer science;artificial intelligence;online machine learning;machine learning;pattern recognition;bayesian network;data mining;inductive transfer;learning classifier system;stability;competitive learning;computational learning theory;active learning;artificial neural network;generalization error	ML	19.036183598822	-40.500989089981346	129182
433c4efb6ab851cf59a5bbc0142ed8a6a1d464c7	optimization of a subset of apple features based on modified particle swarm algorithm	particle swarm;least squares approximations;support vector machines feature extraction least squares approximations particle swarm optimisation;support vector machines;group velocity;feature extraction feature selection particle swarm optimization least squares support vector machine computation complexity;least squares support vector machine lssvm;training;particle swarm optimization pso;modified particle swarm optimization;particle swarm optimizer;computational complexity;particle swarm optimization least squares methods feature extraction support vector machines support vector machine classification convergence pattern recognition information technology information security informatics;feature extraction;computation complexity;particle swarm optimization;classification algorithms;feature selection;optimization;particle swarm optimisation;least squares support vector machine lssvm feature selection particle swarm optimization pso;least squares support vector machine	Reducing dimension processing is needed in feature samples because the repeated and secondary features would reduce the classification ability and increase computation complexity. In this paper, a feature selection method, named MPSO (Modified Particle Swarm Optimization), is proposed. The original group velocity of a particle swarm was changed into two separate and parallel particle swarm velocity, which was effectively and quickly applied to the feature extraction of the optimum samples on the basis of Discrete Binary PSO. Then the least squares support vector machine classifier is used to verify the feasibility of this method. The experimental results show that, compared with the method in the literature, the iteration times in this method are only 17 times in average, while the iteration times in the literature are 23 times; the selected features and the average recognition accuracy after feature selection are slightly better than the ones in the method in the literature. Therefore,the proposed method is feasible and effective.	algorithm;computation;feature extraction;feature selection;fitness function;iteration;least squares support vector machine;mathematical optimization;particle swarm optimization;phase-shift oscillator;shadow volume;velocity (software development)	Weixing Zhu;Dajun Hou;Jin Zhang;Jian Zhang	2010	2010 Third International Symposium on Intelligent Information Technology and Security Informatics	10.1109/IITSI.2010.23	mathematical optimization;multi-swarm optimization;machine learning;pattern recognition;mathematics;particle swarm optimization	EDA	11.033220498565	-42.60144604283473	129207
a5e8680002738df11829e9d0fd1876e7b78c4d61	improving classification accuracy using intra-session classifier training and implementation for a b	gas;brain computer interface;bci;n fold cross validation;parameter selection;electroencephalographs;eeg;genetic algorithms;classification accuracy;flda	Genetic Algorithms (GAs) were used in a previous study to automate parameter selection for an EEG-based P300-driven Brain-Computer Interface (BCI). The GA approach showed marked improvement over data-insensitive parameter selection; however, it required lengthy execution times thereby rendering it infeasible for online implementation. Automated parameter selection is retained in this work; however, it is achieved using the less computationally intensive N-fold cross-validation (NFCV). Additionally, this study sought to improve BCI classification accuracy using a training data collection and application protocol that the authors refer to as ‘Intra-session classifier training and implementation’. Intra-session classifier training and implementation using NFCV-driven automated parameter selection yielded a classification accuracy of 82.94% compared to 45.44% for the inter-session approach using datainsensitive parameters. These findings are significant impact since the intrasession protocol can be applied to any P300-based BCI regardless of its application platform to obtain improved classification accuracy.	annual bci research award;best, worst and average case;brain–computer interface;cross-validation (statistics);electroencephalography;feature extraction;genetic algorithm;marginal model;preprocessor;software release life cycle;while	Chanan S. Syan;Randy E. S. Harnarinesingh;Ramaswamy Palaniappan	2012	IJISTA	10.1504/IJISTA.2012.046542	brain–computer interface;computer science;machine learning;pattern recognition;data mining	HCI	11.58358553657094	-41.93758762756466	129280
5e9c8d74e83c9900bb06255656d45eba2eaeb260	a general framework for transfer sparse subspace learning		In this paper, we propose a general framework for transfer learning, referred to as transfer sparse subspace learning (TSSL). This framework is suitable for different assumptions on the divergence measures of the data distributions, such as maximum mean discrepancy, Bregman divergence, and K–L divergence. We introduce an effective sparse regularization to the proposed transfer subspace learning framework, which can reduce time and space cost obviously, and more importantly, which can avoid or at least reduce over-fitting problem. We give different solutions to the problems based on different distribution distance estimation criteria, and convergence analysis is also given. Comprehensive experiments on the text data sets and the face image data sets demonstrate that TSSL-based methods outperform existing transfer learning methods.	bregman divergence;discrepancy function;experiment;matrix regularization;overfitting;sparse matrix;text corpus	Shizhun Yang;Ming Lin;Chenping Hou;Changshui Zhang;Yi Wu	2012	Neural Computing and Applications	10.1007/s00521-012-1084-1	mathematical optimization;machine learning;pattern recognition;mathematics;statistics	ML	23.644915986431474	-41.73226875489562	129410
78de99d4c208d2242d37117676dea37f3a01f013	training neural networks using features replay		Training a neural network using backpropagation algorithm requires passing error gradients sequentially through the network. The backward locking prevents us from updating network layers in parallel and fully leveraging the computing resources. Recently, there are several works trying to decouple and parallelize the backpropagation algorithm. However, all of them suffer from severe accuracy loss or memory explosion when the neural network is deep. To address these challenging issues, we propose a novel parallel-objective formulation for the objective function of the neural network. After that, we introduce features replay algorithm and prove that it is guaranteed to converge to critical points for the non-convex problem under certain conditions. Finally, we apply our method to training deep convolutional neural networks, and the experimental results show that the proposed method achieves faster convergence, lower memory consumption, and better generalization error than compared methods.	algorithm;backpropagation;computer vision;converge;convex optimization;convolutional neural network;coupling (computer programming);experiment;generalization error;gradient;lock (computer science);loss function;neural networks;optimization problem;two-phase locking	Zhouyuan Huo;Bin Gu;Heng Huang	2018			generalization error;artificial neural network;artificial intelligence;machine learning;critical point (mathematics);convolutional neural network;computer science;convergence (routing);backpropagation	ML	21.587669960148922	-49.48974026906247	129441
9fc8addac99bda488f8318721078f39bdd9c6c73	weighted task regularization for multitask learning	support vector machines;anomaly detection;outlier task;support vector machine weighted task regularization knowledge transfer weighted regularized multitask learning framework statistical metrics kullback leibler divergence regularization process;support vector machines data handling learning artificial intelligence;weighted regularization;outlier task multitask learning anomaly detection weighted regularization svm;svm;data handling;learning artificial intelligence;multitask learning;support vector machines kernel training optimization accuracy equations educational institutions	Multitask Learning has been proven to be more effective than the traditional single task learning on many real-world problems by simultaneously transferring knowledge among different tasks which may suffer from limited labeled data. However, in order to build a reliable multitask learning model, nontrivial effort to construct the relatedness between different tasks is critical. When the number of tasks is not large, the learning outcome may suffer if there exists outlier tasks that inappropriately bias majority. Rather than identifying or discarding such outlier tasks, we present a weighted regularized multitask learning framework based on regularized multitask learning, which uses statistical metrics, such as Kullback-Leibler divergence, to assign weights prior to regularization process that robustly reduces the impact of outlier tasks and results in better learned models for all tasks. We then show that this formulation can be solved using dual form like optimizing a standard support vector machine with varied kernels. We perform experiments using both synthetic dataset and real-world dataset from petroleum industry which shows that our methodology outperforms existing methods.	computer multitasking;experiment;kullback–leibler divergence;support vector machine;synthetic data;synthetic intelligence	Yintao Liu;Anqi Wu;Dong Guo;Ke-Thia Yao;Cauligi S. Raghavendra	2013	2013 IEEE 13th International Conference on Data Mining Workshops	10.1109/ICDMW.2013.158	semi-supervised learning;regularization perspectives on support vector machines;support vector machine;multi-task learning;anomaly detection;computer science;machine learning;pattern recognition;data mining;mathematics	ML	20.869096965747804	-42.45362503815422	129474
ad8642e186c5c81d06934d4e6fc249b7cbca40e8	learning transferable architectures for scalable image recognition		"""Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the """"NASNet search space"""") which enables transferability. In our experiments, we search for the best convolutional layer (or """"cell"""") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a """"NASNet architecture"""". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4% error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset."""		Barret Zoph;Vijay Vasudevan;Jonathon Shlens;Quoc V. Le	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00907	machine learning;word error rate;computer vision;scalability;artificial intelligence;feature (computer vision);artificial neural network;computer science;architecture;object detection;pattern recognition;coco;contextual image classification	Vision	22.663673318361905	-51.49929075464106	129599
01572eda0ace32b89c63bcc23137c23535f9fa14	maximing the margin in the input space	quadratic program;temporal variability;prior knowledge;feature space;support vector;artificial intelligent;hilbert space;support vector machine	We propose a novel criterion for support vector machine learning: maximizing the margin in the input space, not in the feature (Hilbert) space. This criterion is a discriminative version of the principal curve proposed by Hastie et al. The criterion is appropriate in particular when the input space is already a well-designed feature space with rather small dimensionality. The de nition of the margin is generalized in order to represent prior knowledge. The derived algorithm consists of two alternating steps to estimate the dual parameters. Firstly, the parameters are initialized by the original SVM. Then one set of parameters is updated by Newton-like procedure, and the other set is updated by solving a quadratic programming problem. The algorithm converges in a few steps to a local optimum under mild conditions and it preserves the sparsity of support vectors. Although the complexity to calculate temporal variables increases the complexity to solve the quadratic programming problem for each step does not change. It is also shown that the original SVM can be seen as a special case. We further derive a simpli ed algorithm which enables us to use the existing code for the original SVM.	calculus of variations;expect;expectation–maximization algorithm;feature vector;hilbert space;local optimum;machine learning;mathematical optimization;newton;quadratic programming;sparse matrix;support vector machine	Shotaro Akaho	2002	CoRR		support vector machine;mathematical optimization;computer science;machine learning;pattern recognition;mathematics;quadratic programming	ML	24.42911428164628	-39.2913381274386	129604
54a52d1693249dbac6cec9323dafad4977ec940e	using self-organizing maps for regression: the importance of the output function		Self-organizing map (SOM) is a powerful paradigm that is extensively applied for clustering and visualization purpose. It is also used for regression learning, especially in robotics, thanks to its ability to provide a topological projection of high dimensional non linear data. In this case, data extracted from the SOM are usually restricted to the best matching unit (BMU), which is the usual way to use SOM for classification, where class labels are attached to individual neurons. In this article, we investigate the influence of considering more information from the SOM than just the BMU when performing regression. For this purpose, we quantitatively study several output functions for the SOM, when using these data as input of a linear regression, and find that the use of additional activities to the BMU can strongly improve regression performance. Thus, we propose an unified and generic framework that embraces a large spectrum of models from the traditional way to use SOM, with the best matching unit as output, to models related to the radial basis function network paradigm, when using local receptive field as output.		Thomas Hecht;Mathieu Lefort;Alexander Gepperth	2015			computer science;artificial intelligence;machine learning;data mining	ML	18.551993471168707	-49.15965649699709	129649
8ce01ceb2bd5139d0905268ed1a427bc884fa16f	constructing ensembles for better ranking	ensembles construction;rankde;artificial dataset;machine learning;ranking performance;pattern classification;pattern classification learning artificial intelligence;machine learning bagging data mining machine learning algorithms boosting information technology data engineering computer science application software training data;learning artificial intelligence;machine learning ensembles construction rankde artificial dataset ranking performance	We propose a novel algorithm, RankDE, to build an ensemble using an extra artificial dataset. RankDE aims at improving the overall ranking performance, which is crucial in many machine learning applications. This algorithm constructs artificial datasets that are diverse with the current training dataset in terms of ranking. We conduct experiments with real-world data sets to compare RankDE with some traditional and state-of-the-art ensembling algorithms of Bagging, Adaboost, DECORATE and Rankboost in terms of ranking. The experiments show that RankDE outperforms Bagging, DECORATE, Adaboost, and Rankboost when limited data is available. When enough training data is available, it is competitive with DECORATE and Adaboost.	adaboost;algorithm;decision tree;experiment;machine learning;semi-supervised learning;semiconductor industry;supervised learning	Jin Huang;Charles X. Ling	2006	Sixth International Conference on Data Mining (ICDM'06)	10.1109/ICDM.2006.42	computer science;online machine learning;machine learning;pattern recognition;data mining;ranking svm	DB	13.45444937736215	-40.72635939123722	129758
d6a5c54da325d7cd331736595d655e5a11892db6	accurate svm classification using border training patterns	svm classification;border training pattern;support vector machines border training patterns clustering hyperspectral images;support vector machines;and forward;hyperspectral images;support vector machines image classification;hyperspectral images svm classification border training pattern support vector machine classification;training;image classification;support vector;support vector machines support vector machine classification training data hyperspectral imaging hyperspectral sensors kernel image sensors laboratories signal processing robustness;training data;accuracy;border training patterns;clustering;classification algorithms;support vector machine classification;support vector machine;hyperspectral imaging;classification accuracy;hyperspectral image	This paper proposes to use border training patterns in order to improve Support Vector Machine (SVM) classification accuracy of hyperspectral images. In the proposed approach, border training patterns which are close to the separating hyperplane, are obtained in two consecutive steps and considered as final training set. In the first step, clustering is performed to the full initial training data of each class. Then, cluster centers of each class are taken as the reduced size training data and forwarded to the second step. In the second step, this reduced size training data is used in the training of SVM and cluster centers which are obtained as support vectors at this step are regarded to be located close to the hyperplane border. Finally, cluster centers which are found as support vectors and original training samples contained in these clusters only are assigned as border training patterns. Experimental results are presented to show that the proposed approach improves SVM classification accuracy.	cluster analysis;support vector machine;test set	Begüm Demir;Sarp Ertürk	2009	2009 First Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing	10.1109/WHISPERS.2009.5289110	computer science;machine learning;pattern recognition;data mining	ML	15.459844975532564	-44.66072669432134	129829
bb0ecedde7d6e837dc9a5e115302a2aaad1035e1	face verification: strategies for employing deep models		Features extracted with deep learning have now achieved state-of-the-art results in many tasks. However, to reuse a learned deep model, transfer learning with fine-tuning needs to be employed, which requires to re-train the whole model or part of it to extract useful features in the new domain. This step is burdensome and requires heavy computing power. Therefore, this work investigates alternatives in transfer-learning that do not involve performing fine-tuning for a model with the new domain. Namely, we explore the correlation of depth and scale in deep models, and look for the layer/scale that yields the best results for the new domain, we also explore metrics for the verification task, using locally connected convolutions to learn distance metrics. Our experiments use a model pre-trained in face identification and adapt it to the face verification task with different data, but still on the face domain. We achieve 96.65% mean accuracy on the Labeled Faces in the Wild dataset and 93.12% mean accuracy on the Youtube Faces dataset which are in the state-of-the-art.	artificial neural network;benchmark (computing);convolution;deep learning;experiment	Ricardo Barbosa Kloss;Artur Jordao;William Robson Schwartz	2018	2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)	10.1109/FG.2018.00045	transfer of learning;reuse;artificial neural network;deep learning;machine learning;artificial intelligence;computer science	Vision	24.200117857711955	-50.84966834319916	129890
b669537fd8477e805f247d6e7abde6d4e31043ea	tuning diversity in bagged ensembles		In this paper, we investigate how the level of diversity amongst individual neural networks in a bagged ensemble can significantly influence overall ensemble generalization performance. We propose a new technique that tunes this diversity so that ensemble generalization performance is optimized and evaluate its performance on benchmark regression data-sets.	algorithm;artificial neural network;benchmark (computing);ensemble kalman filter;generalization (psychology);neural network simulation;overfitting;population parameter;sampling (signal processing);test set;weight	John Carney;Padraig Cunningham	2000	International journal of neural systems	10.1142/S0129065700000272	artificial intelligence;machine learning;pattern recognition	ML	13.32088794831358	-41.73881141904751	130070
46e0d6abd44aa4cff066881bc0f9f9d9c2ce14f8	feature subset selection and ranking for data dimensionality reduction	measurement space;high efficiency;correlation function;clear physical interpretation;data dimensionality reduction;specified candidate feature subset;good effectiveness;efficient feature subsets;overall feature;feature subset selection;feature selection;new algorithm;unsupervised learning;feature extraction;indexing terms;dimensionality reduction;high dimensional data	A new unsupervised forward orthogonal search (FOS) algorithm is introduced for feature selection and ranking. In the new algorithm, features are selected in a stepwise way, one at a time, by estimating the capability of each specified candidate feature subset to represent the overall features in the measurement space. A squared correlation function is employed as the criterion to measure the dependency between features and this makes the new algorithm easy to implement. The forward orthogonalization strategy, which combines good effectiveness with high efficiency, enables the new algorithm to produce efficient feature subsets with a clear physical interpretation	algorithm;dimensionality reduction;estimated;feature selection;genetic selection;nonlinear system;search engine;stepwise regression;subgroup;unsupervised learning;algorithm;emotional dependency	Hua-Liang Wei;Stephen A. Billings	2007	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2007.250607	unsupervised learning;computer science;machine learning;pattern recognition;data mining;mathematics;feature selection;feature;dimensionality reduction	ML	11.48780777419077	-44.141634460579105	130146
32e5b94632f29db6607601a914977f3261637560	nonlinear optimization and support vector machines		Support Vector Machine (SVM) is one of the most important class of machine learning models and algorithms, and has been successfully applied in various fields. Nonlinear optimization plays a crucial role in SVM methodology, both in defining the machine learning models and in designing convergent and efficient algorithms for large-scale training problems. In this paper we will present the convex programming problems underlying SVM focusing on supervised binary classification. We will analyze the most important and used optimization methods for SVM training problems, and we will discuss how the properties of these problems can be incorporated in designing useful algorithms.	algorithm;binary classification;convex optimization;machine learning;mathematical optimization;nonlinear programming;support vector machine	Veronica Piccialli;Marco Sciandrone	2018	4OR	10.1007/s10288-018-0378-2	nonlinear programming;kernel (statistics);support vector machine;mathematical optimization;statistical learning theory;mathematics;binary classification;convex optimization;management information systems	ML	22.271817900914815	-39.151927487471596	130195
466d3b143420fb826f04b8e548efee1078fa5cde	m. hubert, p. rousseeuw and p. segaert: multivariate functional outlier detection		Firstly we congratulate the authors on a wonderful paper full of new nice ideas, which can be considered as a major breakthrough in the functional outlier detection using visual procedures. These ideas include a taxonomy of outliers, the definition of bag distance and the centrality-stability plots. Indeed, each of the last two ideas is fundamental in the development of each of the two corresponding procedures to detect multivariate functional outliers introduced in the paper. The first procedure consists in constructing a heat map using the functional bag distance based on the Tukey depth while the second in a scatter-plot based on the skew-adjusted projection depth, SPD, named the centrality-stability plot, CSP. Moreover, both procedures complement themselves because the heat maps are good in detecting all kind of outliers excepting the shape outliers, but those are clearly identified with the CSP’s. Our discussion focus, firstly, on shedding light on the behaviour of the proposed procedures when applied to multivariate functional data whose dimension is entitled to be extremely high. Secondly, a simplification of the CSP is proposed. Furthermore, we encourage the authors to comment on the advantages/disadvantages of applying, what they call, the MFSPD versus, what they call, the 1/(1+ FAO), as the difference between them just lies in the reverse order of the integral and inverse functionals.	anomaly detection	Alicia Nieto-Reyes;Juan Antonio Cuesta-Albertos	2015	Statistical Methods and Applications	10.1007/s10260-015-0319-6	anomaly detection;statistics;mathematics;nice;outlier;multivariate statistics;random projection	Theory	15.043937882582542	-48.439765542229566	130343
c108bc503dc54b8c6448ab0bf2e9461dbd82b33c	deep information networks		We describe a novel classifier with a tree structure, designed using information theory concepts. This Information Network is made of information nodes, that compress the input data, and multiplexers, that connect two or more input nodes to an output node. Each information node is trained, independently of the others, to minimize a local cost function that minimizes the mutual information between its input and output with the constraint of keeping a given mutual information between its output and the target (information bottleneck). We show that the system is able to provide good results in terms of accuracy, while it shows many advantages in terms of modularity and reduced complexity.	information theory;input/output;lagrange multiplier;loss function;mathematical optimization;modularity (networks);multiplexer;mutual information;norm (social);tree structure	Giulio Franzese;Monica Visintin	2018	CoRR		multiplexer;information theory;machine learning;fold (higher-order function);mutual information;artificial intelligence;tree structure;input/output;modularity;mathematics;information bottleneck method	ML	20.455509610432344	-47.73028208748383	130492
8ffad1107f0f67398da4c10e074a18f3b916fa39	semi-supervised dictionary learning based on hilbert-schmidt independence criterion		In this paper, a novel semi-supervised dictionary learning and sparse representation (SS-DLSR) is proposed. The proposed method benefits from the supervisory information by learning the dictionary in a space where the dependency between the data and class labels is maximized. This maximization is performed using Hilbert-Schmidt independence criterion (HSIC). On the other hand, the global distribution of the underlying manifolds were learned from the unlabeled data by minimizing the distances between the unlabeled data and the corresponding nearest labeled data in the space of the dictionary learned. The proposed SS-DLSR algorithm has closed-form solutions for both the dictionary and sparse coefficients, and therefore does not have to learn the two iteratively and alternately as is common in the literature of the DLSR. This makes the solution for the proposed algorithm very fast. The experiments confirm the improvement in classification performance on benchmark datasets by including the information from both labeled and unlabeled data, particularly when there are many unlabeled data.	benchmark (computing);coefficient;dictionary;expectation–maximization algorithm;experiment;machine learning;schmidt decomposition;semiconductor industry;sparse approximation;sparse matrix;usb	Mehrdad J. Gangeh;Safaa M. A. Bedawi;Ali Ghodsi;Fakhri Karray	2016		10.1007/978-3-319-41501-7_2	semi-supervised learning;k-svd;computer science;machine learning;pattern recognition;data mining;statistics	ML	23.79181320009027	-42.07259375741126	130672
72a6044a0108e0f8f1e68cd70ada46c81a416324	improved training of generative adversarial networks using representative features		Despite of the success of Generative Adversarial Networks (GANs) for image generation tasks, the trade-off between image diversity and visual quality are an well-known issue. Conventional techniques achieve either visual quality or image diversity; the improvement in one side is often the result of sacrificing the degradation in the other side. In this paper, we aim to achieve both simultaneously by improving the stability of training GANs. A key idea of the proposed approach is to implicitly regularizing the discriminator using a representative feature. For that, this representative feature is extracted from the data distribution, and then transferred to the discriminator for enforcing slow updates of the gradient. Consequently, the entire training process is stabilized because the learning curve of discriminator varies slowly. Based on extensive evaluation, we demonstrate that our approach improves the visual quality and diversity of state-of-the art GANs.	autoencoder;context awareness;convolution;discriminator;encoder;extensibility;generative adversarial networks;glossary of computer graphics;gradient;kl-one;kullback–leibler divergence;radio frequency	Duhyeon Bang;Hyunjung Shim	2018			adversarial system;machine learning;generative grammar;pattern recognition;computer science;discriminator;artificial intelligence;learning curve	ML	23.586547700195062	-50.32210639161532	130727
469f02bede21e9aca2d0a05adaf89da38236bcda	aib2: an abstraction data reduction technique based on ib2	prototypes;k nn classification;data reduction	"""Data reduction improves the efficiency of k-NN classifier on large datasets since it accelerates the classification process and reduces storage requirements for the training data. IB2 is an effective data reduction technique that selects some training items form the initial dataset and uses them as representatives (prototypes). Contrary to many other data reduction techniques, IB2 is a very fast, one-pass method that builds its reduced (condensing) set in an incremental manner. New training data can update the condensing set without the need of the """"old"""" removed items. This paper proposes AIB2, a variation of IB2, which generates new prototypes instead of selecting them. AIB2 attempts to improve the efficiency of IB2 by positioning the prototypes in the center of the data areas they represent. The experimental study shows that AIB2 performs better than IB2."""	experiment;k-nearest neighbors algorithm;requirement;statistical classification	Stefanos Ougiaroglou;Georgios Evangelidis	2013		10.1145/2490257.2490260	data reduction;computer science;machine learning;pattern recognition;data mining;prototype	DB	12.791647537315434	-40.735576621871914	130759
425084176d4f65712c25f9ec854bd3a3147e20d2	feature set search space for fuzzyboost learning	feature set characteristic;branch act;classic stump;linear mapping algorithm;fuzzyboost learning;search space;previous temporal fuzzyboost;linear dimensionality reduction method;fuzzyboost algorithm;linear mapping;fuzzy decision;decision stump	This paper presents a novel approach to the weak classifier selection based on the GentleBoost framework, based on sharing a set of features at each round. We explore the use of linear dimensionality reduction methods to guide the search for features that share some properties, such as correlations and discriminative properties. We add this feature set as a new parameter of the decision stump, which turns the single branch selection of the classic stump into a fuzzy decision that weights the contribution of both branches. The weights of each branch act as a confidence measure based on the feature set characteristics, which increases the accuracy and robustness to data perturbations. We propose an algorithm that consider the similarities between the weights provided by three linear mapping algorithms: PCA, LDA and MMLMNN [14]. We propose to analyze the row vectors of the linear mapping, grouping vector components with very similar values. Then, the created groups are the inputs of the FuzzyBoost algorithm. This search procedure generalizes the previous temporal FuzzyBoost [10] to any type of features. We present results in features with spatial support (images) and spatiotemporal support (videos), showing the generalization properties of the FuzzyBoost algorithm in other scenarios.	algorithm;decision stump;dimensionality reduction;facial recognition system;local-density approximation	Plinio Moreno;Pedro Canotilho Ribeiro;José Santos-Victor	2011		10.1007/978-3-642-21257-4_31	machine learning;pattern recognition;data mining;mathematics;statistics	ML	12.713856538871719	-44.31733142830029	130963
c1b810c3df9f3479ba4b518f8303aa0e448c13ef	when does machine learning fail? generalized transferability for evasion and poisoning attacks		Recent results suggest that attacks against supervised machine learning systems are quite effective, while defenses are easily bypassed by new attacks. However, the specifications for machine learning systems currently lack precise adversary definitions, and the existing attacks make diverse, potentially unrealistic assumptions about the strength of the adversary who launches them. We propose the FAIL attacker model, which describes the adversary’s knowledge and control along four dimensions. Our model allows us to consider a wide range of weaker adversaries who have limited control and incomplete knowledge of the features, learning algorithms and training instances utilized. To evaluate the utility of the FAIL model, we consider the problem of conducting targeted poisoning attacks in a realistic setting: the crafted poison samples must have clean labels, must be individually and collectively inconspicuous, and must exhibit a generalized form of transferability, defined by the FAIL model. By taking these constraints into account, we design StingRay, a targeted poisoning attack that is practical against 4 machine learning applications, which use 3 different learning algorithms, and can bypass 2 existing defenses. Conversely, we show that a prior evasion attack is less effective under generalized transferability. Such attack evaluations, under the FAIL adversary model, may also suggest promising directions for future defenses.	adversary (cryptography);adversary model;algorithm;evasion (network security);machine learning;supervised learning	Octavian Suciu;Radu Marginean;Yigitcan Kaya;Hal Daumé;Tudor Dumitras	2018			transferability;computer science;adversary;machine learning;artificial intelligence	Security	18.65016919478037	-51.17571285867812	131432
6e80366622bfaeb24c858bb358af4cce410b481d	how to learn a learning system - automatic decomposition of a multiclass task with probability estimates	learning system	Multiclass classification is the core issue of many pattern recognition tasks. In some applications, not only the predicted class is important but also the confidence associated to the decision. This paper presents a complete framework for multiclass classification that recovers probability estimates for each class. It focuses on the automatic configuration of the system so that no user-provided tuning is needed. No assumption about the nature of data or the number of classes is done either, resulting in a generic system. A suitable decomposition of the original multiclass problem into several biclass problems is automatically learnt from data. State-of-the-art biclass classifiers are optimized and their reliabilities are assessed and considered in the combination of the biclass predictions. Quantitative evaluations on different datasets show that the automatic decomposition and the reliability assessment of our system improve the classification rate compared to other schemes, as well as it provides probability estimates of each class. Besides, it simplifies considerably the user effort to use the framework in a specific problem, since it adapts automatically.	a library for support vector machines;algorithm;algorithmic efficiency;multiclass classification;pattern recognition	Cristina Garcia Cifuentes;Marc Sturzel	2010			semi-supervised learning;feature learning;error-driven learning;computer science;data science;machine learning;data mining	Vision	16.922232008682133	-40.52711238824833	131559
96da654db0d9d37de373034c1f2393877addc5d9	probabilistic dimensionality reduction via structure learning		We propose an alternative probabilistic dimensionality reduction framework that can naturally integrate the generative model and the locality information of data. Based on this framework, we present a new model, which is able to learn a set of embedding points in a low-dimensional space by retaining the inherent structure from high-dimensional data. The objective function of this new model can be equivalently interpreted as two coupled learning problems, i.e., structure learning and the learning of projection matrix. Inspired by this interesting interpretation, we propose another model, which finds a set of embedding points that can directly form an explicit graph structure. We proved that the model by learning explicit graphs generalizes the reversed graph embedding method, but leads to a natural interpretation from Bayesian perspective. This can greatly facilitate data visualization and scientific discovery in downstream analysis. Extensive experiments are performed that demonstrate that the proposed framework is able to retain the inherent structure of datasets and achieve competitive quantitative results in terms of various performance evaluation criteria.	data visualization;dimensionality reduction;downstream (software development);experiment;generative model;graph - visual representation;graph embedding;locality of reference;loss function;optimization problem;performance evaluation	Li Wang	2019	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2017.2785402	feature learning;machine learning;pattern recognition;data mining;mathematics;dimensionality reduction	ML	22.088621748875447	-43.89426377137128	131591
e4e17a7fa706f4f1c01b7f95b9fe5cf4f4bfbaae	evolution of superfeatures through genetic programming	genetic programming;classification;comparative partner selection;binary string fitness characterization;superfeatures	Abstract: The success of automatic classification is intricately linked with an effective feature selection. Previous studies on the use of genetic programming (GP) to solve classification problems have highlighted its benefits, principally its inherent feature selection (a process that is often performed independent of a learning method). In this paper, the problem of classification is recast as a feature generation problem, where GP is used to evolve programs that allow non-linear combination of features to create superFeatures, from which classification tasks can be achieved fairly easily. In order to generate superFeatures robustly, the binary string fitness characterization along with the comparative partner selection strategy is introduced with the aim of promoting optimal convergence. The techniques introduced are applied to two illustrative problems first and then to the real-world problem of audio source classification, with competitive results.	genetic programming	Peter Day;Asoke K. Nandi	2011	Expert Systems	10.1111/j.1468-0394.2010.00547.x	genetic programming;biological classification;computer science;artificial intelligence;machine learning;data mining	AI	10.952093832681065	-41.82188039114922	131625
1199da169617fc41b8cc104af68da766b06f7deb	a new approach for multi-biometric fusion based on subjective logic		Biometric verification systems have to address many practical requirements, such as performance, presentation attack detection (PAD), large population coverage, demographic diversity, and varied deployment environment. Traditional unimodal biometric systems do not fully meet the aforementioned requirements making them vulnerable and susceptible to different types of attacks. In response to that, modern biometric systems combine multiple biometric modalities at different fusion levels, such as sensor, feature, score and decision level. The fused score is decisive to classify an unknown user as a genuine or impostor. In this paper, we describe a new biometric fusion framework based on Subjective Logic (SL); a type of probabilistic logic that explicitly takes uncertainty and trust into consideration. We principally evaluate our proposed fusion framework using two modalities, namely iris and fingerprint. Furethermore, the individual scores obtained from various comparators are combined at score level by applying four score fusion approaches (minimum score, maximum score, simple sum, and subjective logic) and three score normalization techniques (min-max, z-score, hyperbolic tangent). The experimental results show that the proposed score level fusion approach (subjective logic) gives the best authentication accuracy even when particular biometric classifiers give distinct comparison scores.	authentication;biometrics;capacitive sensing;comparator;deployment environment;enhanced entity–relationship model;fingerprint;iris recognition;logic programming;maxima and minima;multimodal interaction;requirement;sl (complexity);software deployment;verification and validation	Kamer Vishi;Audun Jøsang	2017		10.1145/3109761.3158409	computer network;fingerprint;probabilistic logic;normalization (statistics);computer science;machine learning;biometrics;iris recognition;subjective logic;population;authentication;artificial intelligence	Vision	15.201597475342245	-46.6304099740203	131919
0ff55fe97c67a7a670d25428e8225200129c60d2	semi-supervised clustering with partial background information	fuzzy clustering;empirical study;feature space	Incorporating background knowledge into unsupervised clustering algorithms has been the subject of extensive research in recent years. Nevertheless, existing algorithms implicitly assume that the background information, typically specified in the form of labeled examples or pairwise constraints, has the same feature space as the unlabeled data to be clustered. In this paper, we are concerned with a new problem of incorporating partial background knowledge into clustering, where the labeled examples have moderate overlapping features with the unlabeled data. We formulate this as a constrained optimization problem, and propose two learning algorithms to solve the problem, based on hard and fuzzy clustering methods. An empirical study performed on a variety of real data sets shows that our proposed algorithms improve the quality of clustering results with limited labeled examples.	cluster analysis;constrained optimization;constraint (mathematics);feature vector;fuzzy clustering;k-means clustering;k-nearest neighbors algorithm;machine learning;mathematical optimization;optimization problem;semi-supervised learning;semiconductor industry	Jing Gao;Pang-Ning Tan;Haibin Cheng	2006		10.1137/1.9781611972764.46	computer science;astrophysics;empirical research;constrained optimization;cluster analysis;fuzzy clustering;data mining;pairwise comparison;artificial intelligence;data set;feature vector;pattern recognition	AI	20.941990736547666	-42.162292136014365	131956
1379cc1e4a0fab44ff4ead7a0b065eae78d5f4e3	large scale distributed semi-supervised learning using streaming approximation		Traditional graph-based semi-supervised learning (SSL) approaches are not suited for massive data and large label scenarios since they scale linearly with the number of edges |E| and distinct labels m. To deal with the large label size problem, recent works propose sketch-based methods to approximate the label distribution per node thereby achieving a space reduction from O(m) to O(logm), under certain conditions. In this paper, we present a novel streaming graphbased SSL approximation that effectively captures the sparsity of the label distribution and further reduces the space complexity per node to O(1). We also provide a distributed version of the algorithm that scales well to large data sizes. Experiments on real-world datasets demonstrate that the new method achieves better performance than existing state-of-the-art algorithms with significant reduction in memory footprint. Finally, we propose a robust graph augmentation strategy using unsupervised deep learning architectures that yields further significant quality gains for SSL in natural language applications.	approximation algorithm;dspace;deep learning;display resolution;distributed algorithm;freebase;mad;memory footprint;natural language;semi-supervised learning;semiconductor industry;sparse matrix;streaming algorithm;streaming media;supervised learning;unsupervised learning	Sujith Ravi;Qiming Diao	2016			computer science;theoretical computer science;machine learning;data mining;statistics	ML	19.290685922142057	-43.902015706527784	132020
4fdd812505a362c6e7e6b1857f1d9be699d1b112	3d object recognition with deep belief nets		We introduce a new type of top-level model for Deep Belief Nets and evaluate it on a 3D object recognition task. The top-level model is a third-order Boltzmann machine, trained using a hybrid algorithm that combines both generative and discriminative gradients. Performance is evaluated on the NORB database (normalized-uniform version), which contains stereo-pair images of objects under different lighting conditions and viewpoints. Our model achieves 6.5% error on the test set, which is close to the best published result for NORB (5.9%) using a convolutional neural net that has built-in knowledge of translation invariance. It substantially outperforms shallow models such as SVMs (11.6%). DBNs are especially suited for semi-supervised learning, and to demonstrate this we consider a modified version of the NORB recognition task in which additional unlabeled images are created by applying small translations to the images in the database. With the extra unlabeled data (and the same amount of labeled data as before), our model achieves 5.2% error.	3d single-object recognition;artificial neural network;boltzmann machine;canonical account;discriminative model;generative modelling language;gradient;greedy algorithm;hybrid algorithm;loss function;optimization problem;outline of object recognition;pixel;semi-supervised learning;semiconductor industry;supervised learning;test set;unsupervised learning	Vinod Nair;Geoffrey E. Hinton	2009			computer science;artificial intelligence;machine learning;pattern recognition	ML	24.20693400509895	-49.054210249926086	132027
4593c28b0f19d6a8e4d53e96695d1f47ddc1bec7	multi-merge budget maintenance for stochastic gradient descent svm training		Budgeted Stochastic Gradient Descent (BSGD) is a state-of-the-art technique for training large-scale kernelized support vector machines. The budget constraint is maintained incrementally by merging two points whenever the pre-defined budget is exceeded. The process of finding suitable merge partners is costly; it can account for up to 45% of the total training time. In this paper we investigate computationally more efficient schemes that merge more than two points at once. We obtain significant speed-ups without sacrificing accuracy.	computation;kernel method;mathematical optimization;overhead (computing);simplex algorithm;stochastic gradient descent;support vector machine;time complexity	Sahar Qaadan;Tobias Glasmachers	2018	CoRR		merge (version control);mathematical optimization;mathematics;machine learning;artificial intelligence;support vector machine;budget constraint;stochastic gradient descent	ML	20.023202275531997	-38.13405464634264	132071
2364f1ca82cf19bc105135e1216350318c7b98a7	a fuzzy multiclass novelty detector for data streams		In many real-world applications data arrive continuously, in the form of streams. Such data can be used for the acquisition of knowledge by machine learning methods. In data streams learning, novelty detection is a relevant topic, which aims to identify the emergence of a new concept or a drift in the known concept in real time. Most approaches in the literature that focus on the novelty detection problem, make assumptions that limit the method usefulness. For instance, some methods are designed lying on the supposition that labeled data will be available at some time in the stream, while others restrict the proposed algorithm to one-class problems. Some recent approaches aim to overcome the limitations mentioned, considering multiclass problems and unlabeled datasets. In addition, there are also proposals that explore concepts of fuzzy set theory to add more flexibility to the learning process, although restricted to labeled datasets. In this paper, we propose a fuzzy multiclass novelty detector for data streams called FuzzND, as a fuzzy extension of the MINAS algorithm. Our algorithm generates a model based on fuzzy micro-clusters that provides flexible class boundaries. Allowing the identification of different types of novel information, i.e, novel classes, extension of classes or outliers more efficiently. Experiments show that our approach is promising in dealing with the changes in data streams and presents improvements in comparison to the non-fuzzy version.	algorithm;emergence;experiment;fuzzy set;machine learning;multiclass classification;novelty detection;set theory;stream (computing)	Tiago Pinho da Silva;Veronica Ralls;Priscilla de Abreu Lopes;Heloisa A. Camargo	2018	2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2018.8491545	fuzzy logic;streams;novelty;machine learning;artificial intelligence;cluster analysis;fuzzy set;data stream mining;novelty detection;data modeling;computer science	ML	13.99224862563944	-38.12615763271009	132206
85956f6431543d1bf62bb5d5143de4348f14a95c	detecting adversarial perturbations through spatial behavior in activation spaces		Neural network based classifiers are still prone to manipulation through adversarial perturbations. State of the art attacks can overcome most of the defense or detection mechanisms suggested so far, and adversaries have the upper hand in this arms race. Adversarial examples are designed to resemble the normal input from which they were constructed, while triggering an incorrect classification. This basic design goal leads to a characteristic spatial behavior within the context of Activation Spaces, a term coined by the authors to refer to the hyperspaces formed by the activation values of the network’s layers. Within the output of the first layers of the network, an adversarial example is likely to resemble normal instances of the source class, while in the final layers such examples will diverge towards the adversary’s target class. The steps below enable us to leverage this inherent “shift” from one class to another in order to form a novel adversarial example detector. We construct Euclidian spaces out of the activation values of each of the deep neural network layers. Then, we induce a set of knearest neighbor classifiers (k-NN), one per activation space of each neural network layer, using the non-adversarial examples. We leverage those classifiers to produce a sequence of class labels for each nonperturbed input sample and estimate the a priori probability for a class label change between one activation space and another. Training our detector with only normal input follows the principles of anomaly detection, aiming to make our method future ready for as yet unknown attack methods. During the detection phase we compute a sequence of classification labels for each input using the trained classifiers. We then estimate the likelihood of those classification sequences and show that adversarial sequences are far less likely than normal ones. We evaluated our detection method against the state of the art C&W attack method, using two image classification datasets (MNIST, CIFAR-10) which are commonly used for adversarial evaluation. Our evaluation results show that our detector achieves an AUC of 0.95 for the CIFAR-10 dataset, with only a marginal increase in the computational complexity. Keywords— Adversarial Perturbations, Detector, Activation Spaces		Ziv Katzir;Yuval Elovici	2018	CoRR			AI	20.04808442455228	-51.38587709659871	132221
dff016f508d54790bacf532e080753cf8f35d85d	multi-class second-order cone programming support vector machines	second order cone programming;quadratic programming;support vector machines;convex optimization;multi class classification	Novel multiclass approach that simultaneously constructs all required hyperplanes.Extensions of OvO and OvA multiclass SVM to second-order cone programming SVM.Best classification performance is achieved in experiments on benchmark datasets. This paper presents novel second-order cone programming (SOCP) formulations that determine a linear multi-class predictor using support vector machines (SVMs). We first extend the ideas of OvO (One-versus-One) and OvA (One-versus-All) SVM formulations to SOCP-SVM, providing two interesting alternatives to the standard SVM formulations. Additionally, we propose a novel approach (MC-SOCP) that simultaneously constructs all required hyperplanes for multi-class classification, based on the multi-class SVM formulation (MC-SVM). The use of conic constraints for each pair of training patterns in a single optimization problem provides an adequate framework for a balanced and effective prediction.	conic optimization;second-order cone programming;support vector machine	Julio López Hernandez;Sebastián Maldonado	2016	Inf. Sci.	10.1016/j.ins.2015.10.016	support vector machine;mathematical optimization;convex optimization;second-order cone programming;computer science;machine learning;multiclass classification;pattern recognition;mathematics;quadratic programming	ML	21.75925330641265	-39.18813409738777	132400
ad042fbe72167e973690167f0ddc99b0824b5b32	feature definition in pattern recognition with small sample size	small sample size;pattern recognition	Abstract   The problem of feature definition in the design of a pattern recognition system where the number of available training samples is small but the number of potential features is excessively large has not received adequate attention. Most of the existing feature extraction and feature selection procedures are not feasible due to computational considerations when the number of features exceeds, say, 100, and are not even applicable when the number of features exceeds the number of patterns. The feature definition procedure which we have proposed involves partitioning a large set of highly correlated features into subsets, or clusters, through hierarchical clustering. Almost any feature selection or extraction procedure, including the constrained maximum variance approach introduced here, can then be applied to each subset to obtain a single representative feature. The original set of correlated features is thus reduced to a small set of nearly uncorrelated features. The utility of this procedure has been demonstrated on a speaker-identification data base which consists of 20 subjects, 156 features, and 180 samples.	pattern recognition	Anil K. Jain;Richard C. Dubes	1978	Pattern Recognition	10.1016/0031-3203(78)90016-X	feature vector;feature;computer science;machine learning;pattern recognition;mathematics;feature;statistics	Vision	11.333258977757726	-44.28994580153265	132550
cec225091de8a5ecf723fea5a5a41d0b1fc84d31	generative adversarial mapping networks		Generative Adversarial Networks (GANs) have shown impressive performance in generating photo-realistic images. They fit generative models by minimizing certain distance measure between the real image distribution and the generated data distribution. Several distance measures have been used, such as Jensen-Shannon divergence, f -divergence, and Wasserstein distance, and choosing an appropriate distance measure is very important for training the generative network. In this paper, we choose to use the maximum mean discrepancy (MMD) as the distance metric, which has several nice theoretical guarantees. In fact, generative moment matching network (GMMN) (Li, Swersky, and Zemel 2015) is such a generative model which contains only one generator network G trained by directly minimizing MMD between the real and generated distributions. However, it fails to generate meaningful samples on challenging benchmark datasets, such as CIFAR-10 and LSUN. To improve on GMMN, we propose to add an extra network F , called mapper. F maps both real data distribution and generated data distribution from the original data space to a feature representation space R, and it is trained to maximize MMD between the two mapped distributions inR, while the generator G tries to minimize the MMD. We call the new model generative adversarial mapping networks (GAMNs). We demonstrate that the adversarial mapper F can help G to better capture the underlying data distribution. We also show that GAMN significantly outperforms GMMN, and is also superior to or comparable with other state-of-the-art GAN based methods on MNIST, CIFAR-10 and LSUN-Bedrooms datasets.	ac adapter;benchmark (computing);dataspaces;discrepancy function;generative adversarial networks;generative model;impedance matching;jensen's inequality;mapper;mnist database;map;mikumikudance;shannon (unit)	Jianbo Guo;Guangxiang Zhu	2017	CoRR		generative model;artificial intelligence;metric (mathematics);machine learning;mathematics;distance measures;divergence;nice;generative grammar;mnist database;real image	ML	23.286019034921612	-48.03840110758806	132611
f144afe910586a2fd59c6a2ef256548bed0a85a4	classtering: joint classification and clustering with mixture of factor analysers		In this work we propose a novel parametric Bayesian model for the problem of semi-supervised classification and clustering. Standard approaches of semi-supervised classification can recognize classes but cannot find groups of data. On the other hand, semi-supervised clustering techniques are able to discover groups of data but cannot find the associations between clusters and classes. The proposed model can classify and cluster samples simultaneously, allowing the analysis of data in the presence of an unknown number of classes and/or an arbitrary number of clusters per class. Experiments on synthetic and real world data show that the proposed model compares favourably to state-of-the-art approaches for semi-supervised clustering and that the discovered clusters can help to enhance classification performance, even in cases where the cluster and the low density separation assumptions do not hold. We finally show that when applied to a challenging real-world problem of subgroup discovery in breast cancer, the method is capable of maximally exploiting the limited information available and identifying highly promising subgroups.	bayesian network;cluster analysis;experiment;machine learning;semi-supervised learning;semiconductor industry;supervised learning;synthetic data	Emanuele Sansone;Andrea Passerini;Francesco G. B. De Natale	2016		10.3233/978-1-61499-672-9-1089	machine learning;computer science;cluster analysis;artificial intelligence;pattern recognition	ML	17.458497219993273	-42.50478772553577	132686
fa9e8a98b1cce48bebbef8ddebaa1d2c7109516f	fuzzy support vector machines with the uncertainty of parameter c	fuzzy set;uncertainty;fuzzy support vector machine;pattern recognition;fuzzy support vector machines;gaussian distribution	In typical pattern recognition applications, there are usually only some vague and general knowledge about the situation. An optimal classifier, then, will be definitely hard to develop if the decision function lacks sufficient knowledge. The aim of our experiments was to extract some features by some appropriate transformation of the training set. In the paper, we assumed that the training samples were drawn from a Gaussian distribution. Also, assumed that if the data sets are in an imprecise situation, such as classes overlap, it can be represented by fuzzy sets. Results showed a powerful learning capacity: the fuzzy support vector machines with the uncertainty of parameter C rule (FSVMs-UPC) was proposed. Here it indicated that each data point had individual parameter coefficient been valuable. The experimental results show that the proposed method is a better way to postpone or avoid overfitting, and it also gives us a measure of the quality of the ultimately chosen model. 2008 Published by Elsevier Ltd.	coefficient;data point;experiment;fuzzy set;overfitting;pattern recognition;support vector machine;test set;universal product code;vagueness	Che-Chang Hsu;Ming-Feng Han;Shih-Hsing Chang;Hung-Yuan Chung	2009	Expert Syst. Appl.	10.1016/j.eswa.2008.08.032	normal distribution;uncertainty;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;data mining;mathematics;fuzzy set;fuzzy set operations;statistics	AI	12.518496154971666	-39.444031956575174	132860
2c65792b6b351958c284237578b592824dbabc43	a variance reduction framework for stable feature selection	stability;high dimensional data;feature selection;variance reduction	Besides high accuracy, stability of feature selection has recently attracted strong interest in knowledge discovery from high-dimensional data. In this study, we present a theoretical framework about the relationship between the stability and accuracy of feature selection based on a formal bias-variance decomposition of feature selection error. The framework also suggests a variance reduction approach for improving the stability of feature selection algorithms. Furthermore, we propose an empirical variance reduction framework, margin based instance weighting, which weights training instances according to their influence to the estimation of feature relevance. We also develop an efficient algorithm under this framework. Experiments based on synthetic data and real-world micro array data verify both the theoretical framework and the effectiveness of the proposed algorithm on variance reduction. The proposed algorithm is also shown to be effective at improving subset stability, while maintaining comparable classification accuracy based on selected features.	algorithm;bias–variance tradeoff;feature selection;microarray;relevance;synthetic data;variance reduction	Yue Han;Lei Yu	2010	2010 IEEE International Conference on Data Mining	10.1002/sam.11152	minimum redundancy feature selection;stability;computer science;machine learning;pattern recognition;data mining;mathematics;feature selection;statistics;variance reduction;dimensionality reduction;clustering high-dimensional data	ML	11.924213486089984	-44.64926158981821	132886
06384163c07afffc91a890116d42fb1c1a97dbd9	online domain adaptation of a pre-trained cascade of classifiers	optimisation;selected works;gaussian processes;information regularization online domain adaptation black box classifier optimization criterion gaussian process regression scheme pre trained classifier cascade face detection semisupervised learning;gaussian process regression;image classification;rapidly adapting;semi supervised learning;regression analysis face recognition gaussian processes image classification learning artificial intelligence optimisation;black box classifier;information regularization;face recognition;optimization criterion;bepress;regression analysis;pre trained classifier cascade;learning artificial intelligence;face detection;face face detection detectors ground penetrating radar gaussian processes training data training;online domain adaptation;semisupervised learning;gaussian process regression scheme	Many classifiers are trained with massive training sets only to be applied at test time on data from a different distribution. How can we rapidly and simply adapt a classifier to a new test distribution, even when we do not have access to the original training data? We present an on-line approach for rapidly adapting a “black box” classifier to a new test data set without retraining the classifier or examining the original optimization criterion. Assuming the original classifier outputs a continuous number for which a threshold gives the class, we reclassify points near the original boundary using a Gaussian process regression scheme. We show how this general procedure can be used in the context of a classifier cascade, demonstrating performance that far exceeds state-of-the-art results in face detection on a standard data set. We also draw connections to work in semi-supervised learning, domain adaptation, and information regularization.	algorithm;benchmark (computing);black box;computation;domain adaptation;face detection;jones calculus;kriging;mathematical optimization;matrix regularization;online and offline;semi-supervised learning;semiconductor industry;sensor;statistical classification;supervised learning;test data;test set;viola–jones object detection framework	Vidit Jain;Erik G. Learned-Miller	2011	CVPR 2011	10.1109/CVPR.2011.5995317	semi-supervised learning;facial recognition system;margin classifier;computer vision;contextual image classification;face detection;margin;speech recognition;quadratic classifier;computer science;machine learning;pattern recognition;gaussian process;kriging;regression analysis;statistics	Vision	19.35675794608269	-38.58258378680733	132927
c4b678603a87895ec4610fa97dd5fc818c6232e3	quasi-optimum combination of multilayer perceptrons for adaptive multiclass pattern recognition	classifier combination;data fusion;multilayer perceptron;optimum combining;density estimation;distribution pattern;pattern classification;pattern recognition;error rate;experimental evaluation;real time application;neural network	Standard multiclass pattern recognition requires frequent re-learning stages when the set of categories of interest evolves in time. In order to minimize the computation costs of class incorporation and removal, we divide the global multiclass recognizer into a collection of class pairwise neural dichotomizers. When a new class appears, an adequate set of dichotomizers is created and trained to discriminate the new class from the rest. If a class disappears, its associated dichotomizers are eliminated In both cases previously learned knowledge is not disturbed. The properties of neural recognizers and pairwise modularization allow an analytic quasi-optimum method for combining network outputs to obtain the global multiclass response. An incremental and distributed pattern recognition architecture is presented and its performance experimentally evaluated, obtaining better error rates and learning times than conventional multiclass recognizers using similar resources. The design is highly parallel and asyncronous, adequate for dynamic real time applications.	multilayer perceptron;pattern recognition	Alberto Ruiz;Francisco J. Arcas Túnez	1995		10.1007/3-540-59497-3_266	density estimation;word error rate;computer science;machine learning;pattern recognition;data mining;sensor fusion;multilayer perceptron;artificial neural network	Vision	12.962622320254932	-40.70093228610409	132987
9f0691cbdf9a8d9422fa1521d83a589804e1ec02	instance-based data reduction for improved identification of difficult small classes	imbalanced class distribution;data mining;nearest neighbor method;data pre processing;data reduction	We studied three different methods to improve identification of small classes, which are also difficult to classify, by balancing an imbalanced class distribution with data reduction. The new method, neighborhood cleaning (NCL) rule, outperformed simple random sampling within classes and one-sided selection method in the experiments with ten real world data sets. All reduction methods improved clearly identification of small classes (20--30%) true-positive rates of the three-nearest neighbor method and the C4.5 decision tree generator, but the differences between the methods were insignificant. However, the significant differences in accuracies, true-positive rates, and true-negative rates obtained from the reduced data were in favor of our method. The results suggest that the NCL rule is a useful method for improving modeling of difficult small classes, as well as for building classifiers that identify these classes from the real world data which frequently have an imbalanced class distribution.		Jorma Laurikkala	2002	Intell. Data Anal.	10.3233/ida-2002-6402	nearest neighbour algorithm;data reduction;computer science;machine learning;pattern recognition;data mining;mathematics;data pre-processing	ML	13.630104783804153	-41.04366854086743	133017
4f877422ebf9ac3e129949d590608f9209f991a0	beam search for learning a deep convolutional neural network of 3d shapes	training;computer architecture;computational modeling;shape;three dimensional displays;solid modeling;knowledge transfer	This paper addresses 3D shape recognition. Recent work typically represents a 3D shape as a set of binary variables corresponding to 3D voxels of a uniform 3D grid centered on the shape, and resorts to deep convolutional neural networks (CNNs) for modeling these binary variables. Robust learning of such CNNs is currently limited by the small datasets of 3D shapes available - an order of magnitude smaller than other common datasets in computer vision. Related work typically deals with the small training datasets using a number of ad hoc, hand-tuning strategies. To address this issue, we formulate CNN learning as a beam search aimed at identifying an optimal CNN architecture - namely, the number of layers, nodes, and their connectivity in the network - as well as estimating parameters of such an optimal CNN. Each state of the beam search corresponds to a candidate CNN. Two types of actions are defined to add new convolutional filters or new convolutional layers to a parent CNN, and thus transition to children states. The utility function of each action is efficiently computed by transferring parameter values of the parent CNN to its children, thereby enabling an efficient beam search. Our experimental evaluation on the 3D ModelNet dataset demonstrates that our model pursuit using the beam search yields a CNN with superior performance on 3D shape classification than the state of the art.	artificial neural network;beam search;computer vision;convolutional neural network;deep learning;estimation theory;experiment;heuristic (computer science);hoc (programming language);maximal set;search tree;utility;voxel	Xu Xu;Sinisa Todorovic	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7900177	computer vision;shape;computer science;artificial intelligence;machine learning;pattern recognition;geometry;solid modeling;computational model	Vision	24.5472265196992	-47.662920621149645	133032
e0a877f8952e0719cb0cc81d722646a28d8434ae	on the semi-supervised learning of multi-layered perceptrons	objective function;multi layer perceptron;neural network;indexing terms;semi supervised learning	We present a novel approach for training a multi-layered perceptron (MLP) in a semi-supervised fashion. Our objective function, when optimized, balances training set accuracy with fidelity to a graph-based manifold over all points. Additionally, the objective favors smoothness via an entropy regularizer over classifier outputs as well as straightforward `2 regularization. Our approach also scales well enough to enable large-scale training. The results demonstrate significant improvement on several phone classification tasks over baseline MLPs.	baseline (configuration management);entropy (information theory);loss function;memory-level parallelism;optimization problem;perceptron;semi-supervised learning;semiconductor industry;supervised learning;test set	Jonathan Malkin;Amarnag Subramanya;Jeff A. Bilmes	2009			perceptron;semi-supervised learning;artificial neural network;smoothness;pattern recognition;multilayer perceptron;unsupervised learning;regularization (mathematics);computer science;machine learning;artificial intelligence;graph	Vision	23.081053014829834	-47.47932640260934	133112
1d50d03d9c66f936e426d83c15650e5e19e47f1d	stochastic feature mapping for pac-bayes classification	hybrid generative discriminative classification;pac bayes generalization bound;stochastic feature mapping	Hidden information derived from probabilistic generative models of data distributions can be used to construct features for discriminative classifiers. This observation has motivated the development of approaches that attempt to couple generative and discriminative models together for classification. However, existing approaches typically feed features derived from generative models to discriminative classifiers, and do not refine the generative models or the feature mapping functions based on classification results. In this paper, we propose a coupling mechanism developed under the PAC-Bayes framework that can fine-tune the generative models and the feature mapping functions iteratively to improve the classifier’s performance. In our approach, a stochastic feature mapping, which is a function over the random variables of a generative model, is derived to generate feature vectors for a stochastic classifier. We construct a stochastic classifier over the feature mapping and derive the PAC-Bayes generalization bound for the classifier, for both supervised and semi-supervised learning. This allows us to jointly learn the feature mapping and the classifier by minimizing the bound with an EM-like iterative algorithm using labeled and unlabeled data. The resulting framework integrates the learning of the discriminative classifier and the generative model and allows iterative fine-tuning of the generative models, and the feedforward feature mappings based on task performance feedback. Our experiments show, in three distinct applications, this new framework produces a general classification tool with state-of-the-art performance.	algorithm;discriminative model;experiment;feature vector;feedforward neural network;generative model;iterative method;naive bayes classifier;semi-supervised learning;semiconductor industry;stochastic optimization;supervised learning	Xiong Li;Tai Sing Lee;Yuncai Liu	2015	Machine Learning	10.1007/s10994-015-5525-9	generative topographic map;machine learning;linear classifier;pattern recognition;data mining;mathematics;generative model;discriminative model	ML	23.61286473584058	-46.077963461367	133113
89d967cb778cc99ff9b3f3e26257b0636f522af1	decision tree simplification for classifier ensembles	pruning methods;decision tree;classifier ensemble;ecoc;classifiers;decision trees	The goal of designing an ensemble of simple classifiers is to improve the accuracy of a recognition system. However, the performance of ensemble methods is problem-dependent and the classifier learning algorithm has an important influence on ensemble performance. In particular, base classifiers that are too complex may result in overfitting. In this paper, the performance of Bagging, Boosting and Error-Correcting Output Code (ECOC) is compared for five decision tree pruning methods. A description is given for each of the pruning methods and the ensemble techniques. AdaBoost.OC which is a combination of Boosting and ECOC is compared with the pseudo-loss based version of Boosting, AdaBoost.M2 and the influence of pruning on the performance of the ensembles is studied. Motivated by the result that both pruned and unpruned ensembles made by AdaBoost.OC give similar accuracy, pruned ensembles are compared with ensembles of Decision Stumps. This leads to the hypothesis that ensembles of simple classifiers may give better performance for some problems. Using the application of face recognition, it is shown that an AdaBoost.OC ensemble of Decision Stumps outperforms an ensemble of pruned C4.5 trees for face identification, but is inferior for face verification. The implication is that in some real-world tasks to achieve best accuracy of an ensemble, it may be necessary to select base classifier complexity.		Gholamreza Ardeshir	2002	IJPRAI	10.1142/S021800140400340X	random subspace method;cascading classifiers;computer science;machine learning;decision tree;pattern recognition;alternating decision tree;data mining;ensemble learning;pruning	Vision	13.37291396693456	-39.842602048004906	133145
31e4f1f1e09ccfafbe44c3ecd44e72836e260671	a unifying framework for typical multitask multiple kernel learning problems	kernel;support vector machines;kernel support vector machines vectors closed form solutions optimization algorithm design and analysis learning systems;support vector machines svms machine learning optimization methods pattern recognition supervised learning;closed form solutions;learning systems;vectors;optimization;algorithm design and analysis	Over the past few years, multiple kernel learning (MKL) has received significant attention among data-driven feature selection techniques in the context of kernel-based learning. MKL formulations have been devised and solved for a broad spectrum of machine learning problems, including multitask learning (MTL). Solving different MKL formulations usually involves designing algorithms that are tailored to the problem at hand, which is, typically, a nontrivial accomplishment. In this paper we present a general multitask multiple kernel learning (MT-MKL) framework that subsumes well-known MT-MKL formulations, as well as several important MKL approaches on single-task problems. We then derive a simple algorithm that can solve the unifying framework. To demonstrate the flexibility of the proposed framework, we formulate a new learning problem, namely partially-shared common space MT-MKL, and demonstrate its merits through experimentation.	algorithm;computer multitasking;feature selection;kernel (operating system);machine learning;math kernel library;multiple kernel learning	Cong Li;Michael Georgiopoulos;Georgios C. Anagnostopoulos	2014	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2013.2291772	semi-supervised learning;support vector machine;algorithm design;least squares support vector machine;kernel method;instance-based learning;mathematical optimization;kernel;kernel embedding of distributions;radial basis function kernel;computer science;online machine learning;machine learning;pattern recognition;graph kernel;mathematics;tree kernel;stability;computational learning theory;active learning;polynomial kernel;generalization error	ML	22.71073884855879	-38.85743290398687	133191
0f18e18d436c51868a2cba5c7df3859986d6ba40	self supervised boosting	unsupervised learning;boltzmann distribution;learning problems;synthetic data	Boosting algorithms and successful applications thereof abound for classification and regression learning problems, but not for unsupervised learning. We propose a sequential approach to adding features to a random field model by training them to improve classification performance between the data and an equal-sized sample of “negative examples” generated from the model’s current estimate of the data density. Training in each boosting round proceeds in three stages: first we sample negative examples from the model’s current Boltzmann distribution. Next, a feature is trained to improve classification performance between data and negative examples. Finally, a coefficient is learned which determines the importance of this feature relative to ones already in the pool. Negative examples only need to be generated once to learn each new feature. The validity of the approach is demonstrated on binary digits and continuous synthetic data.	algorithm;areal density (computer storage);boosting (machine learning);coefficient;synthetic data;unsupervised learning	Max Welling;Richard S. Zemel;Geoffrey E. Hinton	2002			unsupervised learning;boltzmann distribution;computer science;machine learning;pattern recognition;data mining;synthetic data	ML	16.279315653930354	-39.596201264808236	133312
0c446937b7a07faf6ba3a5225bc043f6095487e9	performance evaluation of ensemble methods for software fault prediction: an experiment	performance;weka;chidamber and kemerer ck metrics;ensemble methods;measures;classifiers	In object-oriented software development, a plethora of studies have been carried out to present the application of machine learning algorithms for fault prediction. Furthermore, it has been empirically validated that an ensemble method can improve classification performance as compared to a single classifier. But, due to the inherent differences among machine learning and data mining approaches, the classification performance of ensemble methods will be varied. In this study, we investigated and evaluated the performance of different ensemble methods with itself and base-level classifiers, in predicting the faults proneness classes. Subsequently, we used three ensemble methods AdaboostM1, Vote and StackingC with five base-level classifiers namely Naivebayes, Logistic, J48, VotedPerceptron and SMO in Weka tool. In order to evaluate the performance of ensemble methods, we retrieved twelve datasets of open source projects from PROMISE repository. In this experiment, we used k-fold (k=10) cross-validation and ROC analysis for validation. Besides, we used recall, precision, accuracy, F-value measures to evaluate the performance of ensemble methods and base-level Classifiers. Finally, we observed significant performance improvement of applying ensemble methods as compared to its base-level classifier, and among ensemble methods we observed StackingC outperformed other selected ensemble methods for software fault prediction.	algorithm;cross-validation (statistics);data mining;ensemble learning;machine learning;open-source software;performance evaluation;receiver operating characteristic;sequential minimal optimization;software development;weka	Shahid Hussain;Jacky W. Keung;Arif Ali Khan;Kwabena Ebo Bennin	2015		10.1145/2811681.2811699	random subspace method;cascading classifiers;computer science;machine learning;pattern recognition;data mining;ensemble learning	AI	11.717416998815008	-41.18126485637654	133349
93cefc506b6a2179d41f79d5cd0db2f4a7c7fb08	unsupervised feature weighting with multi niche crowding genetic algorithms	unsupervised clustering;crowding genetic algorithm;efficient algorithm;feature space;unsupervised feature weighting;clustering;feature weighting;number of clusters;genetic algorithm;feature selection	This paper is concerned with feature weighting/selection in the context of unsupervised clustering. Since different subspaces of the feature space may lead to different partitions of the data set, an efficient algorithm to tackle multi-modal environments is needed. In this context, the Multi-Niche Crowding Genetic Algorithm is used for searching relevant feature subsets. The proposed method is designed to deal with the inherent biases regarding the number of clusters and the number of features that appear in an unsupervised framework. The first one is eliminated with the aid of a new unsupervised clustering criterion, while the second is tackled with the aid of cross-projection normalization. The method delivers a vector of weights which offers a ranking of features in accordance with their relevance to clustering.	approximation;cluster analysis;crowding;display resolution;feature vector;genetic algorithm;greedy algorithm;modal logic;niche blogging;relevance;unsupervised learning	Mihaela Breaban;Henri Luchian	2009		10.1145/1569901.1570058	correlation clustering;genetic algorithm;feature vector;flame clustering;computer science;canopy clustering algorithm;machine learning;pattern recognition;data mining;mathematics;cluster analysis;feature selection;feature	ML	10.176353950479998	-43.68610075634186	133371
49269375c94bbf407cbcb5fbb855bae258003c3f	stochastic coordinate coding and its application for drosophila gene expression pattern annotation		Drosophila melanogaster has been established as a model organism for investigating the fundamental principles of developmental gene interactions. The gene expression patterns of Drosophila melanogaster can be documented as digital images, which are annotated with anatomical ontology terms to facilitate pattern discovery and comparison. The automated annotation of gene expression pattern images has received increasing attention due to the recent expansion of the image database. The effectiveness of gene expression pattern annotation relies on the quality of feature representation. Previous studies have demonstrated that sparse coding is effective for extracting features from gene expression images. However, solving sparse coding remains a computationally challenging problem, especially when dealing with large-scale data sets and learning large size dictionaries. In this paper, we propose a novel algorithm to solve the sparse coding problem, called Stochastic Coordinate Coding (SCC). The proposed algorithm alternatively updates the sparse codes via just a few steps of coordinate descent and updates the dictionary via second order stochastic gradient descent. The computational cost is further reduced by focusing on the non-zero components of the sparse codes and the corresponding columns of the dictionary only in the updating procedure. Thus, the proposed algorithm significantly improves the efficiency and the scalability, making sparse coding applicable for large-scale data sets and large dictionary sizes. Our experiments on Drosophila gene expression data sets show that the proposed algorithm achieves one or two orders of magnitude speedup compared to the state-of-art sparse coding algorithm.	algorithm;algorithmic efficiency;code;column (database);computation;coordinate descent;dictionary;digital image;experiment;interaction;neural coding;scalability;sparse matrix;speedup;stochastic gradient descent;supervised learning	Binbin Lin;Qingyang Li;Qian Sun;Ming-Jun Lai;Ian Davidson;Wei Fan;Jieping Ye	2014	CoRR		k-svd;computer science;bioinformatics;machine learning;data mining	ML	12.639613512431474	-48.44215080630414	133522
e184711916cac08460c7240de6ebf0678a7139bd	generalized entropy based semi-supervised learning	online algorithm;entropy semisupervised learning supervised learning training accuracy linear programming prediction algorithms;multi classification entropy online algorithm semi supervised learning;probability entropy learning artificial intelligence pattern classification;会议论文;semi supervised learning;entropy;small labeled samples size generalized entropy based semisupervised learning probabilistic supervised learning model semisupervised multiclassification learning unlabeled samples online learning algorithm logarithmic regret linear computational overhead;multi classification	Semi-supervised learning is a class of supervised learning techniques that also make use of unlabeled samples for training, the research aims to provide considerable improvement in learning accuracy with a small amount of labeled samples and affordable computational overhead. In this paper, we extend an probabilistic supervised learning model to semi-supervised multi-classification learning, both labeled and unlabeled samples are unified in our model levering the generalized entropy concept. For optimization, we adopt an efficient online learning algorithm which can achieve logarithmic regret with linear computational overhead in supervised learning situation. Empirical study shows our method obtain prediction accuracy closing to that of supervised learning while using extremely small labeled samples size.	algorithm;closing (morphology);mathematical optimization;overhead (computing);regret (decision theory);semi-supervised learning;semiconductor industry;supervised learning	Taocheng Hu;Jinhui Yu	2015	2015 IEEE/ACIS 14th International Conference on Computer and Information Science (ICIS)	10.1109/ICIS.2015.7166603	semi-supervised learning;unsupervised learning;instance-based learning;entropy;online algorithm;empirical risk minimization;wake-sleep algorithm;computer science;online machine learning;machine learning;pattern recognition;data mining;information fuzzy networks;ensemble learning;learning classifier system;supervised learning;stability;computational learning theory;active learning;learning to rank;generalization error	Vision	17.537738889430933	-38.462038156283526	133648
df7dfe53b2b47d69fed500c26b343b8bece9d5db	distance metric learning with eigenvalue fine tuning		Distance metric learning focuses on learning one global or multiple local distance functions to draw similar instances close to each other and push away dissimilar ones. Most existing work has to do matrix projection to learn distance functions. In this paper, we present a novel distance function learning model which is based on eigenvalue fine tuning. Our model not only is able to learn the global distance function but also can be easily adopted into local metric learning tasks. From the perspective of dimension reduction, the proposed model can measure how much information has been preserved after feature transformation. Moreover, we connect our model with principal components analysis to improve its performance by introducing the label information. Experimental results have demonstrated the effectiveness of the proposed method.	dimensionality reduction;principal component analysis	Wenquan Wang;Ya Zhang;Jinglu Hu	2017	2017 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2017.7965895	artificial intelligence;metric (mathematics);machine learning;dimensionality reduction;eigenvalues and eigenvectors;principal component analysis;mathematical optimization;matrix (mathematics);matrix decomposition;symmetric matrix;distance matrix;mathematics	AI	24.383829123991074	-41.598896167079744	133713
dafd3ef130619c15921f99c97b200361eee75409	optimum nonlinear discriminant analysis and discriminant kernel support vector machine			kernel (operating system);linear discriminant analysis;support vector machine	Akinori Hidaka;Takio Kurita	2016	IEICE Transactions		support vector machine;kernel method;kernel fisher discriminant analysis;radial basis function kernel;kernel principal component analysis;computer science;machine learning;pattern recognition;optimal discriminant analysis;linear discriminant analysis;polynomial kernel;multiple discriminant analysis	ML	23.431519283867107	-39.709437509991915	133740
46723b565144112a9a318f2f332311244f8d6c6b	safe semi-supervised extreme learning machine for eeg signal classification		One major challenge in the current brain–computer interface research is the accurate classification of time-varying electroencephalographic (EEG) signals. The labeled EEG samples are usually scarce, while the unlabeled samples are available in large quantities and easy to collect in real applications. Semi-supervised learning (SSL) methods can utilize both labeled and unlabeled data to improve performance over supervised approaches. However, it has been reported that the unlabeled data may undermine the performance of SSL in some cases. To improve the safety of SSL, we proposed a new safety-control mechanism by analyzing the differences between unlabeled data analysis in supervised and semi-supervised learning. We then develop and implement a safe classification method based on the semi-supervised extreme learning machine (SS-ELM). Following this approach, the Wasserstein distance is used to measure the similarities between the predictions obtained from ELM and SS-ELM algorithms, and a different risk degree is thereby calculated for each unlabeled data instance. A risk-based regularization term is then constructed and embedded into the objective function of the SS-ELM. Extensive experiments were conducted using benchmark and EEG datasets to evaluate the effectiveness of the proposed method. Experimental results show that the performance of the new algorithm is comparable to SS-ELM and superior to ELM on average. It is thereby shown that the proposed method is safe and efficient for the classification of EEG signals.	algorithm;benchmark (computing);brain–computer interface;electroencephalography;elm;embedded system;experiment;loss function;optimization problem;semi-supervised learning;semiconductor industry;statistical classification;supervised learning	Qingshan She;Bo Hu;Haitao Gan;Yingle Fan;Thinh Nguyen;Thomas Potter;Yingchun Zhang	2018	IEEE Access	10.1109/ACCESS.2018.2868713	support vector machine;manifold;brain–computer interface;semi-supervised learning;extreme learning machine;machine learning;distributed computing;regularization (mathematics);statistical classification;electroencephalography;computer science;artificial intelligence	AI	21.410199842016898	-39.523736858364515	133913
8dcb51b59d5b3d9af616ff1354709d06fff9d3c0	building emerging pattern (ep) random forest for recognition	emerging pattern mining;itemsets;random forest classifier;weighted decision rules;trees mathematics;data mining;pattern mining;computer vision;training data;indexes;emerging pattern;weighted decision rules emerging pattern random forest classifier visual recognition multiple tree classifiers;action recognition random forest emerging pattern mining;action recognition;multiple tree classifiers;data mining algorithm;random forest;pattern classification;humans;visual recognition;trees mathematics pattern classification;data mining training data itemsets humans computer vision indexes;decision rule	The Random forest classifier comes to be the working horse for visual recognition community. It predicts the class label of an input data by aggregating the votes of multiple tree classifiers. However, the classification performances of these tree classifiers are different. The random forest classifier ignores the difference by simply assigning them equal weights in voting for the final classification decision. Also, the random forest classifier only casts votes from individual tree classifiers without considering their compositions which would be more accurate. In this paper, we propose to tackle the two points by discovering weighted decision rules from the tree classifiers' output sets on training data. By treating the outputs of the tree classifiers on each data as a digital itemset, we want to find discriminative patterns (either containing the output of a single tree classifier or a set of tree classifiers) from the itemsets of training data. We employ an efficient data mining algorithm, the Emerging Pattern (EP) Mining, to search such discriminative patterns and weight them according to their discriminative powers. A set of decision rules are built from these discovered patterns and the final outputs of the Random Forest are made using these decision rules. We call the proposed classifier Emerging Pattern (EP) Random Forest. Experimental results on action categorization problems confirm that the proposed method really improve the performance of the traditional Random Forest classifier.	algorithm;categorization;data mining;expectation propagation;performance;random forest	Liang Wang;Yizhou Wang;Debin Zhao	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5653902	random subspace method;random forest;computer science;machine learning;pattern recognition;incremental decision tree;data mining	ML	12.468350549742933	-46.069437063300775	134288
b730243081b0838a6f7bab8961437aa81ed880b6	a dynamic wrapper method for feature discretization and selection		In many learning problems, an adequate (sometimes discrete) representation of the data is necessary. For instance, for large number of features and small number of instances, learning algorithms may be confronted with the curse of dimensionality , and need to address it in order to be effective. Feature selection and feature discretization techniques have been used to achieve adequate representations of the data, by selecting an adequate subset of features with a convenient representation. In this paper, we propose static and dynamic methods for feature discretization. The static method is unsupervised and the dynamic method uses a wrapper approach with a quantizer and a classifier, and it can be coupled with any static (unsupervised or supervised) discretization procedure. The proposed methods attain efficient representations that are suitable for learning problems. Moreover, using well-known feature selection methods with the features discretized by our methods leads to better accuracy than with the features discretized by other methods or even with the original features.	algorithm;bitwise operation;competitive analysis (online algorithm);curse of dimensionality;discretization;electronic flight bag;feature selection;machine learning;method (computer programming);naive bayes classifier;preprocessor;quantization (signal processing);test set;unsupervised learning	Artur J. Ferreira;Mário A. T. Figueiredo	2012			machine learning;pattern recognition;discretization of continuous features	ML	17.896384675547708	-39.786545837685296	134369
e1a9e4a4d77fd6bb429ed26cd819f10ea75ea114	multiple classifier method for structured output prediction based on error correcting output codes	classifier ensemble;multiple classifier systems;ensemble of classifiers;complex structure;classifier system;multiple classifiers;error correcting output codes ecoc;structured output prediction;classifier ensembles;error correcting output code	It is proposed in the paper a new method for structured output prediction using ensemble of classifiers composed on the basis of Error-Correcting Output Codes. It was presented that newly presented Multiple Classifier Method for Structured Output Prediction based on Error Correcting Output Codes requires comparable computation time in comparison to other accurate algorithms and simultaneously in the classification of more complex structures it provides much better results in the same computation time.	code (cryptography)	Tomasz Kajdanowicz;Michal Wozniak;Przemyslaw Kazienko	2011		10.1007/978-3-642-20042-7_34	random subspace method;margin classifier;cascading classifiers;computer science;machine learning;pattern recognition;data mining;generalized complex structure;structured support vector machine	ML	10.4836914468104	-42.01916818849818	134430
149285fe048d529735ce12cf8f500571e46b7a2e	robust ensemble feature selection for high dimensional data sets	data mining;pattern classification;reliability factor robust ensemble feature selection high dimensional data sets data preprocessing data mining problem feature subsets aggregation technique filter methods confidence measurement conflict measurement;robustness decision trees support vector machines breast cancer data mining machine learning algorithms;pattern classification data mining	Feature selection is an important and frequently used technique in data preprocessing for performing data mining on large scale data sets. Several feature selection methods exist in the literature, each of them uses a specific feature evaluation criterion and may produce different feature subsets even when applied to the same data set. There is not a better resulting subset than the others but all the obtained subsets are the best subsets among the whole feature space. Thinking of a way to take advantage of different feature selection methods simultaneously is a challenging data mining problem. Recently, ensemble feature selection concept have been introduced to help solve this problem. Multiple feature selections are combined in order to produce more robust feature subsets and better classification results. However, one of the most critical decisions when performing ensemble feature selection is the aggregation technique to use for combining the resulting feature lists from the multiple algorithms into a single decision for each feature. In this paper, we propose a robust feature aggregation technique to combine the results of three different filter methods. Our aggregation technique is based on measuring feature algorithms confidence and conflict with the other ones in order to assign a reliability factor guiding the final feature selection. Experiments on high dimensional data sets show that the proposed approach outperforms the single feature selection algorithms as well as two well known aggregation methods in terms of classification performance.	algorithm;data mining;data pre-processing;experiment;feature selection;feature vector;interpreter (computing);object composition;performance;preprocessor;reliability engineering;robustness (computer science)	Afef Ben Brahim;Mohamed Limam	2013	2013 International Conference on High Performance Computing & Simulation (HPCS)	10.1109/HPCSim.2013.6641406	minimum redundancy feature selection;feature vector;feature;feature extraction;computer science;machine learning;kanade–lucas–tomasi feature tracker;pattern recognition;data mining;feature selection;k-nearest neighbors algorithm;feature;feature scaling;feature model;dimensionality reduction	ML	11.189847719766897	-43.616556296465944	134708
5c7e609d5272eeac02e60bab9ad1d4a86d708e06	optimizing data transformations for classification tasks	data transformations;classification algorithm;learning algorithm;evolutionary computation;covariance matrix adaptation;evolutionary based machine learning;optimization technique;euclidean distance;general euclidean distances;machine learning;data transformation;evolution strategy;local search	Many classification algorithms use the concept of distance or similarity between patterns. Previous work has shown that it is advantageous to optimize general Euclidean distances (GED). In this paper, data transformations are optimized instead. This is equivalent to searching for GEDs, but can be applied to any learning algorithm, even if it does not use distances explicitly. Two optimization techniques have been used: a simple Local Search (LS) and the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). CMA-ES is an advanced evolutionary method for optimization in difficult continuous domains. Both diagonal and complete matrices have been considered. Results show that in general, complete matrices found by CMA-ES either outperform or match both Local Search, and the classifier working on the original untransformed data.	algorithm;cma-es;dimensionality reduction;evolution strategy;k-nearest neighbors algorithm;linear classifier;local search (optimization);mathematical optimization;maxima and minima;nonlinear system;optimizing compiler;synthetic intelligence	José María Valls;Ricardo Aler	2009		10.1007/978-3-642-04394-9_22	mathematical optimization;cma-es;computer science;artificial intelligence;machine learning;pattern recognition;mathematics;data transformation;statistics;evolutionary computation	ML	22.800360563699584	-40.005242679625006	134817
1af50a16e43374619b00776f4b28bb5098e656bd	predicting disease by using data mining based on healthcare information system	belief networks;area measurement bayesian methods niobium accuracy sensitivity humans immune system;niobium;bayesian methods;data mining;approximation theory;accuracy;sensitivity;second order approximation healthcare information system data mining process hypertension prediction patient medical records patient diseases medical database under sampling technique weka naive bayesian j 48 classifiers;medical information systems;immune system;diseases;area measurement;humans;sampling methods;health care;sampling methods approximation theory belief networks data mining diseases health care medical information systems	This paper applies the data mining process to predict hypertension from patient medical records with eight other diseases. A sample with the size of 9862 cases has been studied. The sample was extracted from a real world Healthcare Information System database containing 309383 medical records. We observed that the distribution of patient diseases in the medical database is imbalanced. Under-sampling technique has been applied to generate training data sets, and data mining tool Weka has been used to generate the NaIve Bayesian and J-48 classifiers. In addition, an ensemble of five J-48 classifiers was created trying to improve the prediction performance, and rough set tools were used to reduce the ensemble based on the idea of second-order approximation. Experimental results showed a little improvement of the ensemble approach over pure Na'ive Bayesian and J-48 in accuracy, sensitivity, and F-measure.	application domain;approximation;data mining;experiment;f1 score;information system;rough set;sampling (signal processing);weka	Feixiang Huang;Shengyong Wang;Chien-Chung Chan	2012	2012 IEEE International Conference on Granular Computing	10.1109/GrC.2012.6468691	sampling;niobium;immune system;sensitivity;bayesian probability;computer science;data science;machine learning;data mining;accuracy and precision;health care;statistics;approximation theory	Robotics	13.280461252171055	-42.94545239879018	134850
425d49cc6bec2df907d88e684edb7dc1356de31d	best practices for fine-tuning visual classifiers to new domains		Recent studies have shown that features from deep convolutional neural networks learned using large labeled datasets, like ImageNet, provide effective representations for a variety of visual recognition tasks. They achieve strong performance as generic features and are even more effective when fine-tuned to target datasets. However, details of the fine-tuning procedure across datasets and with different amount of labeled data are not well-studied and choosing the best fine-tuning method is often left to trial and error. In this work we systematically explore the design-space for fine-tuning and give recommendations based on two key characteristics of the target dataset: visual distance from source dataset and the amount of available training data. Through a comprehensive experimental analysis, we conclude, with a few exceptions, that it is best to copy as many layers of a pre-trained network as possible, and then adjust the level of fine-tuning based on the visual distance from source.	artificial neural network;best practice;convolutional neural network;imagenet	Brian Chu;Vashisht Madhavan;Oscar Beijbom;Judy Hoffman;Trevor Darrell	2016		10.1007/978-3-319-49409-8_34	machine learning;pattern recognition;data mining	Vision	21.558906937141543	-51.30251484889075	134927
6322b459c344647a08dfffc4c685b134ebc72964	test set selection using active information acquisition for predictive models		In this paper, we consider active information acquisition when the prediction model is meant to be applied on a targeted subset of the population. The goal is to label a pre-specified fraction of customers in the target or test set by iteratively querying for information from the non-target or training set. The number of queries is limited by an overall budget. Arising in the context of two rather disparate applications- banking and medical diagnosis, we pose the active information acquisition problem as a constrained optimization problem. We propose two greedy iterative algorithms for solving the above problem. We conduct experiments with synthetic data and compare results of our proposed algorithms with few other baseline approaches. The experimental results show that our proposed approaches perform better than the baseline schemes.	baseline (configuration management);constrained optimization;constraint (mathematics);experiment;greedy algorithm;iterative method;mathematical optimization;optimization problem;predictive modelling;synthetic data;test set	Sneha Chaudhari;Pankaj Dayama;Vinayaka Pandit;Indrajit Bhattacharya	2013	CoRR		computer science;artificial intelligence;data science;machine learning;data mining;statistics	ML	19.455257150539882	-42.71449011140711	135001
00e0f9633740443a37bf0e15ae9147d6e2db8438	evaluation of protein-protein interaction predictors with noisy partially labeled data sets		Protein-protein interaction (PPI) prediction is an important problem in machine learning and computational biology. However, there is no data set for training or evaluation purposes, where all the instances are accurately labeled. Instead, what is available are instances of positive class (with possibly noisy labels) and no instances of negative class. The non-availability of negative class data is typically handled with the observation that randomly chosen protein-pairs have a nearly 100% chance of being negative class, as only 1 in 1,500 protein pairs expected is expected to be an interacting pair. In this paper, we focused on the problem that non-availability of accurately labeled testing data sets in the domain of protein-protein interaction (PPI) prediction may lead to biased evaluation results. We first showed that not acknowledging the inherent skew in the interactome (i.e. rare occurrence of positive instances) leads to an over-estimated accuracy of the predictor. Then we show that, with the belief that positive interactions are a rare category, sampling random pairs of proteins excluding known interacting proteins set as the negative testing data set could lead to an under-estimated evaluation result. We formalized those two problems to validate the above claim, and based on the formalization, we proposed a balancing method to cancel out the over-estimation with under-estimation. Finally, our experiments validated the theoretical aspects and showed that this balancing evaluation could evaluate the exact performance without availability of golden standard data sets.	computation;computational biology;experiment;interaction;interactome;kerrison predictor;machine learning;pixel density;randomness;regular expression;sampling (signal processing);test set	Haohan Wang;Madhavi K. Ganapathiraju	2015	CoRR		artificial intelligence;machine learning;data mining;mathematics;statistics	ML	16.004474361441943	-38.75449948919787	135125
090016e17c12eaa97b141dfb292882f05a1c3847	one class svm for yeast regulation prediction	high dimensionality;support vector machines;gene regulation;yeast gene;svm;support vector machine;one class learning	In this paper, we outline the main steps leading to the development of the winning solution for Task 2 of KDD Cup 2002 (Yeast Gene Regulation Prediction). Our unusual solution was a pair of linear classifiers in high dimensional space (∼14,000), developed with just 38 and 84 training examples, respectively, all belonging to the target class only. The classifiers were built using the support vector machine approach outlined in the paper.	linear classifier;support vector machine	Adam Kowalczyk;Bhavani Raskutti	2002	SIGKDD Explorations	10.1145/772862.772878	support vector machine;computer science;machine learning;pattern recognition;data mining	ML	11.524370482151046	-41.851282700870065	135460
bccbd30133b2ca75907bae62453e242aa409599b	domain transfer dimensionality reduction via discriminant kernel learning	discriminant kernel learning;dimensionality reduction;transfer learning	Kernel discriminant analysis (KDA) is a popular technique for discriminative dimensionality reduction in data analysis. But, when a limited number of labeled data is available, it is often hard to extract the required low dimensional representation from a high dimensional feature space. Thus, one expects to improve the performance with the labeled data in other domains. In this paper, we propose a method, referred to as the domain transfer discriminant kernel learning (DTDKL), to find the optimal kernel by using the other labeled data from out-of-domain distribution to carry out discriminant dimensionality reduction. Our method learns a kernel function and discriminative projection by maximizing the Fisher discriminant distance and minimizing the mismatch between the in-domain and out-of-domain distributions simultaneously, by which we may get a better feature space for discriminative dimensionality reduction with cross-domain.	dimensionality reduction;discriminant;kernel (operating system)	Ming Zeng;Jiangtao Ren	2012		10.1007/978-3-642-30220-6_24	kernel fisher discriminant analysis;string kernel;kernel embedding of distributions;radial basis function kernel;transfer of learning;kernel principal component analysis;computer science;machine learning;pattern recognition;optimal discriminant analysis;mathematics;linear discriminant analysis;tree kernel;variable kernel density estimation;polynomial kernel;multiple discriminant analysis;statistics;dimensionality reduction	ML	23.828865991396476	-42.12601962435558	135546
aef3fd1dcfbb46e964dbd6d4e7ff746a3240d5f2	unsupervised geodesic convex combination of shape dissimilarity measures		Abstract Dissimilarity or distance metrics are the cornerstone of shape matching and retrieval algorithms. As there is no unique dissimilarity measure that gives good performances in all possible configurations, these metrics are usually combined to provide reliable results. In this paper we propose to compute the best linear convex, or weighted, combination of any set of measured shape distances to enhance shape matching algorithms. To do so, a database is represented as a graph, where nodes are shapes and the edges carry the convex combination of dissimilarity measures. Weights are chosen to maximize the weighted distances between the query shape and shapes in the database. The optimal weights are solutions of a linear programming problem. This fully unsupervised method improves the outcomes of any set of shape similarity measures as shown in our experimental results performed on several popular 3D shape benchmarks.	geodesic convexity	Bilal Mokhtari;Kamal E. Melkemi;Dominique Michelucci;Sebti Foufou	2017	Pattern Recognition Letters	10.1016/j.patrec.2017.07.012	geodesic;artificial intelligence;pattern recognition;mathematics;regular polygon;mathematical optimization;linear programming;convex combination;graph	Vision	20.834343346680154	-41.357504642855254	135755
a83705120fc20a21bd5b155ac2516c269cadd734	evaluation of a score-informed source separation system		In this work, we investigate a method for score-informed source separation using Probabilistic Latent Component Analysis (PLCA). We present extensive test results that give an indication of the performance of the method, its strengths and weaknesses. For this purpose, we created a test database that has been made available to the public, in order to encourage comparisons with alternative methods.	source separation	Joachim Ganseman;Paul Scheunders;Gautham J. Mysore;Jonathan S. Abel	2010			simulation;data mining	DB	11.915213101168238	-47.75652284599858	135820
2247d101be9e79f248c78a23e358798bc402b714	a multi-class structured dictionary learning method using discriminant atom selection		In the last decade, traditional dictionary learning methods have been successfully applied to various pattern classification tasks. Although these methods produce sparse representations of signals which are robust against distortions and missing data, such representations quite often turn out to be unsuitable if the final objective is signal classification. In order to overcome or at least to attenuate such a weakness, several new methods which incorporate discriminative information into sparseinducing models have emerged in recent years. In particular, methods for discriminative dictionary learning have shown to be more accurate (in terms of signal classification) than the traditional ones, which are only focused on minimizing the total representation error. In this work, we present both a novel multi-class discriminative measure and an innovative dictionary learning method. For a given dictionary, this new measure, which takes into account not only when a particular atom is used for representing signals coming from a certain class and the magnitude of its corresponding representation coefficient, but also the effect that such an atom has in the total representation error, is capable of efficiently quantifying the degree of discriminability of each one of the atoms. On the other hand, the new dictionary construction method yields dictionaries which are highly suitable for multiclass classification tasks. Our method was tested with a widely used database for handwritten digit recognition and compared with three state-of-the-art classification methods. The results show that our method significantly outperforms the other three achieving good recognition rates and additionally, reducing the computational cost of the classifier.		R. E. Rol'on;Leandro E. Di Persia;Ruben D. Spies;Hugo Leonardo Rufiner	2018	CoRR			Vision	17.76645866285442	-43.74372042349636	136167
8324e59131ff99dc0f24f1ca42f55a0d4b247582	shapes of features and a modified measure for linear discriminant analysis	linear discriminant analysis measure;feature shapes;testing;linear discriminate analysis;shape measurement;shape similarity;feature extraction;principal component analysis;shape measurement linear discriminant analysis feature extraction clustering algorithms testing low pass filters steel principal component analysis iris pattern recognition;feature selection feature shapes linear discriminant analysis measure iris data set steel surfaces pattern classification;pattern classification feature extraction;pattern classification;pattern recognition;steel;clustering algorithms;iris data set;feature selection;low pass filters;steel surfaces;iris;linear discriminant analysis	In this paper, the problem of selecting most representat ive features among a feature set is considered. Two new feature selection algorithms are in t oduced and their performances are compared with some well-known feature selection algorith ms. The algorithms are tested with the iris data set, three artificially generated da ta sets and a data set obtained from steel surfaces.	algorithm;feature selection;linear discriminant analysis;performance	Cem Ünsalan;Aytül Erçil	2000		10.1109/ICPR.2000.906099	computer vision;low-pass filter;feature extraction;iris flower data set;computer science;machine learning;pattern recognition;mathematics;software testing;cluster analysis;linear discriminant analysis;feature selection;principal component analysis	ML	15.193718510078964	-45.181273970800866	136180
1272d526614e40ce859e73de7e39a54baffd28cc	a unified approach to learning task-specific bit vector representations for fast nearest neighbor search	learning algorithm;information retrieval;image classification;nearest neighbor method;large scale;hashing;machine learning;nearest neighbor;nearest neighbor search;learning to rank;nearest neighbor classification;approaches to learning	Fast nearest neighbor search is necessary for a variety of large scale web applications such as information retrieval, nearest neighbor classification and nearest neighbor regression. Recently a number of machine learning algorithms have been proposed for representing the data to be searched as (short) bit vectors and then using hashing to do rapid search. These algorithms have been limited in their applicability in that they are suited for only one type of task -- e.g. Spectral Hashing learns bit vector representations for retrieval, but not say, classification. In this paper we present a unified approach to learning bit vector representations for many applications that use nearest neighbor search. The main contribution is a single learning algorithm that can be customized to learn a bit vector representation suited for the task at hand. This broadens the usefulness of bit vector representations to tasks beyond just conventional retrieval. We propose a learning-to-rank formulation to learn the bit vector representation of the data. LambdaRank algorithm is used for learning a function that computes a task-specific bit vector from an input data vector. Our approach outperforms state-of-the-art nearest neighbor methods on a number of real world text and image classification and retrieval datasets. It is scalable and learns a 32-bit representation on 1.46 million training cases in two days.	32-bit;bit array;computer vision;data point;information retrieval;k-nearest neighbors algorithm;learning to rank;machine learning;nearest neighbor search;scalability;search problem;stellar classification;web application	Vinod Nair;Dhruv Kumar Mahajan;Sundararajan Sellamanickam	2012		10.1145/2187836.2187961	nearest-neighbor chain algorithm;large margin nearest neighbor;r-tree;nearest neighbour algorithm;ball tree;nearest neighbor graph;contextual image classification;hash function;best bin first;computer science;machine learning;pattern recognition;data mining;cover tree;nearest neighbor search;fixed-radius near neighbors;k-nearest neighbors algorithm;learning to rank;locality-sensitive hashing	ML	18.331071022617948	-46.187982331705456	136183
fe18675b45e72e03f981262ba108ffbc050356a7	crowdlearning: crowded deep learning with data privacy		Deep Learning has shown promising performance in a variety of pattern recognition tasks owning to large quantities of training data and complex structures of neural networks. However conventional deep neural network (DNN) training involves centrally collecting and storing the training data, and then centrally training the neural network, which raises much privacy concerns for the data producers. In this paper, we study how to enable deep learning without disclosing individual data to the DNN trainer. We analyze the risks in conventional deep learning training, then propose a novel idea - Crowdlearning, which decentralizes the heavy- load training procedure and deploys the training into a crowd of computation-restricted mobile devices who generate the training data. Finally, we propose SliceNet, which ensures mobile devices can afford the computation cost and simultaneously minimize the total communication cost. The combination of Crowdlearning and SliceNet ensures the sensitive data generated by mobile devices never leave the devices, and the training procedure will hardly disclose any inferable contents. We numerically simulate our prototype of SliceNet which crowdlearns an accurate DNN for image classification, and demonstrate the high performance, acceptable calculation and communication cost, satisfiable privacy protection, and preferable convergence rate, on the benchmark DNN structure and dataset.	artificial neural network;benchmark (computing);computation;computer vision;deep learning;information privacy;mobile device;numerical analysis;pattern recognition;prototype;rate of convergence;simulation;test set	Linlin Chen;Taeho Jung;Haohua Du;Jianwei Qian;Jiahui Hou;Xiang-Yang Li	2018	2018 15th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)	10.1109/SAHCN.2018.8397100	distributed computing;artificial neural network;computer science;deep learning;computation;mobile device;information privacy;trainer;server;artificial intelligence;contextual image classification	Robotics	18.136217820346012	-49.665881880683195	136227
dc58da4ebf70aca8364e91245b9e4ddc79c5a773	an integrated class-imbalanced learning scheme for diagnosing bearing defects in induction motors		This paper focuses on the development of an integrated scheme for diagnosing bearing defects in induction motors, under the class-imbalanced condition. This scheme comprises of four main modules: segmentation, feature extraction, feature reduction, and fault classification. Various state-of-the-art techniques have been devised in the feature extraction and reduction modules to extract informative sets of features from a raw vibration signal, filter redundant features, and produce the most distinct features for the following module. The fault classification module adapts various state-of-the-art class-imbalanced learning techniques for diagnosing bearing defects. This module contains a novel imputation-based oversampling technique for class-imbalanced learning. This integrated diagnostic scheme is evaluated on three experimental scenarios with different imbalance ratios. The reasonable diagnostic performances confirm the ability of the proposed novel class-imbalanced learning technique in diagnosing bearing defects, independently from the imbalance ratios.	feature extraction;geo-imputation;information;mathematical induction;oversampling;performance	Roozbeh Razavi-Far;Maryam Farajzadeh-Zanjani;Mehrdad Saif	2017	IEEE Transactions on Industrial Informatics	10.1109/TII.2017.2755064	computer science;oversampling;imputation (statistics);machine learning;feature extraction;induction motor;artificial intelligence;bearing (mechanical);pattern recognition	SE	14.214202693941631	-44.657087411446724	136323
ab02a3a951a1f438900190276fe0ed9958a3cc02	from diversity-based prediction to better ontology & schema matching	cross entropy;schema matching;matching prediction;ontology alignment	Ontology & schema matching predictors assess the quality of matchers in the absence of an exact match. We propose MCD (Match Competitor Deviation), a new diversity-based predictor that compares the strength of a matcher confidence in the correspondence of a concept pair with respect to other correspondences that involve either concept. We also propose to use MCD as a regulator to optimally control a balance between Precision and Recall and use it towards 1:1 matching by combining it with a similarity measure that is based on solving a maximum weight bipartite graph matching (MWBM). Optimizing the combined measure is known to be an NP-Hard problem. Therefore, we propose CEM, an approximation to an optimal match by efficiently scanning multiple possible matches, using rare event estimation. Using a thorough empirical study over several benchmark real-world datasets, we show that MCD outperforms other state-of-the-art predictor and that CEM significantly outperform existing matchers.		Avigdor Gal;Haggai Roitman;Tomer Sagi	2016		10.1145/2872427.2882999	ontology alignment;computer science;3-dimensional matching;machine learning;pattern recognition;data mining;database;cross entropy	Web+IR	13.450465382030144	-48.42712428046126	136452
29df177a65698f689764b43e6dbf2469c944bfbf	graph characterization from entropy component analysis	entropy vectors kernel feature extraction complexity theory histograms laplace equations;pattern clustering bioinformatics entropy graph theory;bioinformatics data entropy component analysis structural complexity measures graph clustering graph classification undirected graphs graph entropy distribution feature space von neumann entropy vertex degree statistics local entropy contribution two dimensional histogram bin contents entropy histograms low dimensional space pattern recognition methods	Structural complexity measures and embedding have both been extensively and separately employed for the problems of graph clustering and classification. In this paper we aim to explore whether entropy component analysis can be used as a means of combining these two fundamental approaches. Specifically we develop a novel method that embeds undirected graphs into a feature space based on the graph entropy distribution. We commence from a recently derived expression for the von Neumann entropy of an undirected graph, which depends on vertex degree statistics. Based on this analysis we identify the local entropy contribution associated with each edge in a graph, and this is related to the reciprocal of the product of the degrees of the two vertices connected by the edge. This suggests a simple entropic characterization of graph structure, based on a two-dimensional histogram in which the bins are indexed by vertex degree and the bin-contents is the total entropy contribution associated with the edges that connect vertices of specific degree. This distribution of entropy with vertex degree can be encoded as a matrix, which captures the structure of the graph in terms of an entropic measure of complexity. The matrix can hence be viewed as a sample of entropy histograms from different graphs. Thus we can extract the entropy components from the sample, and use these to embed populations of graphs into a low dimensional space. We apply this method to the problem of graph classification, and compare the classification results of our new method with some alternative state of the art pattern recognition methods on bioinformatics data.	bioinformatics;bivariate data;blum axioms;cluster analysis;degree (graph theory);degree distribution;encode;effective method;entropy (information theory);experiment;feature vector;graph (discrete mathematics);machine learning;pattern recognition;population;quantum statistical mechanics;statistical classification;structural pattern;syntactic pattern recognition;the matrix;vertex (geometry)	Cheng Ye;Richard C. Wilson;Edwin R. Hancock	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.660	graph power;joint entropy;combinatorics;discrete mathematics;topology;binary entropy function;level structure;transfer entropy;null graph;maximum entropy probability distribution;degree;regular graph;simplex graph;machine learning;comparability graph;vertex;mathematics;voltage graph;graph;joint quantum entropy;butterfly graph;line graph;strength of a graph;circulant graph	Vision	16.51276807125815	-47.12514534841724	136562
e042caca6242c5bb9378e2b9aad002e80c8cc143	support vector machine integrated cca for classification of complex chemical patterns	complex chemical patterns;natural spearmint essence;support vector machine integrated cca;support vector machines;chemistry computing;testing;accuracy;self organising feature maps;principal component analysis;classification algorithms;support vector machines chemistry computing genetic algorithms pattern classification principal component analysis self organising feature maps;self organizing map;complex chemical pattern classification;pattern classification;component analysis;genetic algorithm;genetic algorithms;eugenic genetic algorithms pattern classification complex chemical patterns support vector machine correlative component analysis;eugenic genetic algorithm;support vector machine;correlative component analysis;support vector machines support vector machine classification pattern classification carbon capture and storage chemical engineering biomedical engineering genetic algorithms principal component analysis classification algorithms aerodynamics;self organizing map support vector machine integrated cca complex chemical pattern classification correlative component analysis multicollinearity eugenic genetic algorithm natural spearmint essence;multicollinearity;noise;eugenic genetic algorithms	SVM for classification is sensitive to noise and multicollinearity between attributes. Correlative component analysis (CCA) was used to eliminated multicollinearity and noise of original sample data before classified by SVM. To improve the SVM performance, Eugenic Genetic Algorithm (EGA) was used to optimize the parameters of SVM. Finally, a typical example of two classes natural spearmint essence was employed to verify the effectiveness of the new approach CCA-EGA-SVM. The accuracy is much better than that obtained by SVM alone or self-organizing map (SOM) Integrated with CCA.	enhanced graphics adapter;genetic algorithm;image noise;organizing (structure);self-organization;self-organizing map;support vector machine	Xiaofeng Song;Saman K. Halgamuge;De-zhao Chen;Shang-Xu Hu	2008	2008 Second International Conference on Future Generation Communication and Networking	10.1109/FGCN.2008.93	computer science;machine learning;pattern recognition;data mining;ranking svm	ML	10.101177563722228	-44.7766669296061	136927
7f49ea5f6911493b0d9c8ab897813eb62a41cd44	enhancing symmetry in gan generated fashion images		Generative adversarial networks (GANs) are being used in several fields to produce new images that are similar to those in the input set. We train a GAN to generate images of articles pertaining to fashion that have inherent horizontal symmetry in most cases. Variants of GAN proposed so far do not exploit symmetry and hence may or may not produce fashion designs that are realistic. We propose two methods to exploit symmetry, leading to better designs - (a) Introduce a new loss to check if the flipped version of the generated image is equivalently classified by the discriminator (b) Invert the flipped version of the generated image to reconstruct an image with minimal distortions. We present experimental results to show that imposing the new symmetry loss produces better looking images and also reduces the training time.		Vishnu Vardhan Makkapati;Arun Patro	2017		10.1007/978-3-319-71078-5_34	deep learning;discriminator;exploit;computer vision;physics;artificial intelligence	Robotics	23.777552238314872	-49.63422785736682	136929
39b3bfc36e3c6d2f1a2dd3b7950bac7b88041e6b	spectrum sensing for cognitive radio using kernel-based learning	kernel principal component analysis;statistical machine learning;kernel function;inner product;feature space;internet architecture;cognitive radio;machine learning;kernel pca;spectrum sensing;kernel method;support vector machine;eigenvectors;covariance matrix	Kernel method is a very powerful tool in machine learning. The trick of kernel has been effectively and extensively applied in many areas of machine learning, such as support vector machine (SVM) and kernel principal component analysis (kernel PCA). Kernel trick is to define a kernel function which relies on the inner-product of data in the feature space without knowing these feature space data. In this paper, the kernel trick will be employed to extend the algorithm of spectrum sensing with leading eigenvector under the framework of PCA to a higher dimensional feature space. Namely, the leading eigenvector of the sample covariance matrix in the feature space is used for spectrum sensing without knowing the leading eigenvector explicitly. Spectrum sensing with leading eigenvector under the framework of kernel PCA is proposed with the inner-product as a measure of similarity. A modified kernel GLRT algorithm based on matched subspace model will be the first time applied to spectrum sensing. The experimental results on simulated sinusoidal signal show that spectrum sensing with kernel PCA is about 4 dB better than PCA, besides, kernel GLRT is also better than GLRT. The proposed algorithms are also tested on the measured DTV signal. The simulation results show that kernel methods are 4 dB better than the corresponding linear methods. The leading eigenvector of the sample covariance matrix learned by kernel PCA is more stable than that learned by PCA for different segments of DTV signal.	algorithm;cognitive radio;computation;decibel;experiment;feature vector;kernel (operating system);kernel method;kernel principal component analysis;machine learning;nonlinear system;polynomial kernel;similarity measure;simulation;support vector machine	Shujie Hou;Robert Caiming Qiu	2011	CoRR		kernel;principal component regression;kernel method;kernel fisher discriminant analysis;string kernel;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;computer science;machine learning;pattern recognition;graph kernel;mathematics;tree kernel;kernel;variable kernel density estimation;polynomial kernel;statistics;kernel smoother	ML	24.40347610351062	-40.46292037148621	137188
8c43a21439162f140aaa6ff236c44a286c287f5f	a new classification algorithm wks based on weight		By analyzing the disadvantages of the traditional KNN using lazy learning that directly classify the data based on the K neighboring classes using the majority voting method, a new Sigmoid weighted classification algorithm WKS (Weighted KNN Based On Sigmoid) was proposed. WKS provides a new method for learning and training, since each training data di ∊ D contributes to the correct classification of the classifier-expressed by weight Wi. WKS combined with the idea of AdaBoost algorithm that change the data weight distribution in the training base classifier, using Sigmoid function to achieve the weight update, the neighbors with correct contribution to the classify increasing the sample weight, otherwise decrease the weight. Firstly, The training set was modeled by training, and then use the testing set to do classification. By UCI data sets experiment, the experimental results show that WKS has better classification accuracy than traditional KNN algorithm.	adaboost;k-nearest neighbors algorithm;lazy evaluation;lazy learning;sigmoid function;test set	Min Zhang;Min Qi;Kang Sun;Yujun Niu;Longxiang Shi	2017	2017 IEEE 19th International Conference on e-Health Networking, Applications and Services (Healthcom)	10.1109/HealthCom.2017.8210795	lazy learning;adaboost;computer science;classifier (linguistics);algorithm;k-nearest neighbors algorithm;sigmoid function;sample weight;statistical classification;weight distribution	Robotics	13.002671484196064	-39.91405557043055	137310
6b8a58476ef9b42877e01daae0932f48eeb15687	theoretical and empirical analysis of a spatial ea parallel boosting algorithm	spatial evolutionary algorithms;large margin classifiers;machine learning.;parallel boosting;scalability	Many real-world problems involve massive amounts of data. Under these circumstances learning algorithms often become prohibitively expensive, making scalability a pressing issue to be addressed. A common approach is to perform sampling to reduce the size of the dataset and enable efficient learning. Alternatively, one customizes learning algorithms to achieve scalability. In either case, the key challenge is to obtain algorithmic efficiency without compromising the quality of the results. In this article we discuss a meta-learning algorithm (PSBML) that combines concepts from spatially structured evolutionary algorithms (SSEAs) with concepts from ensemble and boosting methodologies to achieve the desired scalability property. We present both theoretical and empirical analyses which show that PSBML preserves a critical property of boosting, specifically, convergence to a distribution centered around the margin. We then present additional empirical analyses showing that this meta-level algorithm provides a general and effective framework that can be used in combination with a variety of learning classifiers. We perform extensive experiments to investigate the trade-off achieved between scalability and accuracy, and robustness to noise, on both synthetic and real-world data. These empirical results corroborate our theoretical analysis, and demonstrate the potential of PSBML in achieving scalability without sacrificing accuracy.	adaboost;addresses (publication format);algorithmic efficiency;architecture as topic;beowulf cluster;boosting (machine learning);chroma subsampling;convergence (action);each (qualifier value);evolutionary algorithm;exhibits as topic;experiment;image scaling;large;machine learning;mapreduce;parallel computing;sampling (signal processing);scalability;semi-supervised learning;semiconductor industry;silo (dataset);standard ml;supervised learning;synthetic intelligence;unsupervised learning	Uday Kamath;Carlotta Domeniconi;Kenneth A. De Jong	2018	Evolutionary Computation	10.1162/evco_a_00202	mathematical optimization;scalability;computer science;theoretical computer science;machine learning;data mining	ML	18.391829615176118	-38.483233716658646	137382
56b37d73fe29d936667378b8df3af69371562533	comparison of filter approaches based on rvfl classifier	selection model;gray relational analysis;classifier combination;credit scoring;information filtering;rvfl classifier;grey relational analysis;linear discriminate analysis;university of california;data mining;feature space;random vector functional link net;f score rrandom vector functional link net gray relational analysis linear discriminate analysis;hybrid model;accuracy;grey relation analysis;artificial neural networks;pattern classification data mining grey systems information filtering;lda;hybrid classification model;filter approaches comparison;classification algorithms;accuracy data models data mining information filters classification algorithms artificial neural networks;wilcoxon signed rank test filter approaches comparison rvfl classifier hybrid classification model credit scoring data mining random vector functional link net rvfl grey relation analysis gra linear discriminate analysis lda;pattern classification;grey systems;feature selection;rrandom vector functional link net;gra;information filters;wilcoxon signed rank test;f score;rvfl;data models	Hybrid classification model is currently an active research area and successfully solves classification problems in credit scoring. Finding effective classificatory models is important. Classification in credit scoring has been regarded as a critical topic, with its related departments collecting huge amounts of data to avoid making the wrong decision. Filter feature selection model is important in credit scoring and in the field of data mining. This study proposes three filter approaches which combine with Random Vector Functional-Link net (RVFL) classifier, to find the suitable classification models. Filter approach retains sufficient information for classification purposes. Different credit scoring combinations are constructed by selecting features with three approaches. Two credit data sets from University of California, Irvine (UCI) are chosen to evaluate the accuracy of various filter selection models. RVFL classifiers combine with Grey relation analysis (GRA), conventional statistical linear discriminate analysis (LDA), and F-score approaches as preprocessing step to optimize features space. In this research, the procedures are described and then evaluated by their performances. The results are compared by nonparametric Wilcoxon signed rank test and performed to show if there is any significant difference between these filters. Our results suggest that the performances of the F-score approach combined with RVFL classifier are brilliant among the two data sets. The hybrid model is more effective and higher accuracy than the original feature space and is a promising method in the field of data mining.	1:1 pixel mapping;ant colony optimization algorithms;british undergraduate degree classification;computation;data mining;effective method;experiment;f1 score;feature selection;feature vector;genetic algorithm;learning classifier system;linear discriminant analysis;particle swarm optimization;performance;preprocessor;simulated annealing	FengChia Li;TsaiYun Lung;ChiHung Yeh	2010	2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2010.5569333	data modeling;feature vector;computer science;wilcoxon signed-rank test;grey relational analysis;machine learning;pattern recognition;data mining;accuracy and precision;feature selection;f1 score;statistics	ML	11.040257612474337	-39.56025783773522	137424
b7eebfd4e23d57d9d9bc1420e03eaac551f86675	a probabilistic hierarchical model for multi-view and multi-feature classification		Some recent works in classification show that the data obtained from various views with different sensors for an object contributes to achieving a remarkable performance. Actually, in many real-world applications, each view often contains multiple features, which means that this type of data has a hierarchical structure, while most of existing works do not take these features with multi-layer structure into consideration simultaneously. In this paper, a probabilistic hierarchical model is proposed to address this issue and applied for classification. In our model, a latent variable is first learned to fuse the multiple features obtained from a same view, sensor or modality. Particularly, mapping matrices corresponding to a certain view are estimated to project the latent variable from a shared space to the multiple observations. Since this method is designed for the supervised purpose, we assume that the latent variables associated with different views are influenced by their ground-truth label. In order to effectively solve the proposed method, the Expectation-Maximization (EM) algorithm is applied to estimate the parameters and latent variables. Experimental results on the extensive synthetic and two real-world datasets substantiate the effectiveness and superiority of our approach as compared with state-of-the-art.	expectation–maximization algorithm;feature data;ground truth;hierarchical database model;latent variable;layer (electronics);modality (human–computer interaction);sensor;statistical classification;synthetic intelligence	Jinxing Li;Hongwei Yong;Bob Zhang;Mu Li;David Zhang	2018			machine learning;probabilistic logic;artificial intelligence;computer science;hierarchical database model	AI	23.68567703364518	-45.59587585170142	137488
211acd7dbd92eb2d014d736b3588032e364e46ef	graph cuts for supervised binary coding		Learning short binary codes is challenged by the inherent discrete nature of the problem. The graph cuts algorithm is a wellstudied discrete label assignment solution in computer vision, but has not yet been applied to solve the binary coding problems. This is partially because it was unclear how to use it to learn the encoding (hashing) functions for out-of-sample generalization. In this paper, we formulate supervised binary coding as a single optimization problem that involves both the encoding functions and the binary label assignment. Then we apply the graph cuts algorithm to address the discrete optimization problem involved, with no continuous relaxation. This method, named as Graph Cuts Coding (GCC), shows competitive results in various datasets.	algorithm;binary code;binary number;computer vision;cryptographic hash function;cut (graph theory);discrete optimization;linear programming relaxation;mathematical optimization;optimization problem	Tiezheng Ge;Kaiming He;Jian Sun	2014		10.1007/978-3-319-10584-0_17	machine learning;pattern recognition	ML	20.393410193338966	-46.83722369761713	137607
b07540228f39eab334895689eed9f55818fd2c0e	efficient movement representation by embedding dynamic movement primitives in deep autoencoders	decoding;training;neurons feature extraction decoding training noise reduction biological neural networks trajectory;trajectory;humanoid robots;feature extraction;noise reduction;neurons;biological neural networks;human movement data movement representation embedding dynamic movement primitives deep autoencoders predictive modeling humanoid movement dmp autoencoded dynamic movement primitive ae dmp feature layer neurons feature space single hidden neuron data imputation	Predictive modeling of human or humanoid movement becomes increasingly complex as the dimensionality of those movements grows. Dynamic Movement Primitives (DMP) have been shown to be a powerful method of representing such movements, but do not generalize well when used in configuration or task space. To solve this problem we propose a model called autoencoded dynamic movement primitive (AE-DMP) which uses deep autoencoders to find a representation of movement in a latent feature space, in which DMP can optimally generalize. The architecture embeds DMP into such an autoencoder and allows the whole to be trained as a unit. To further improve the model for multiple movements, sparsity is added for the feature layer neurons; therefore, various movements can be observed clearly in the feature space. After training, the model finds a single hidden neuron from the sparsity that can efficiently generate new movements. Our experiments clearly demonstrate the efficiency of missing data imputation using 50-dimensional human movement data.	autoencoder;experiment;feature vector;geo-imputation;missing data;neuron;predictive modelling;sparse matrix	Nutan Chen;Justin Bayer;Sebastian Urban;Patrick van der Smagt	2015	2015 IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids)	10.1109/HUMANOIDS.2015.7363570	computer vision;feature extraction;computer science;humanoid robot;artificial intelligence;trajectory;machine learning;noise reduction	Robotics	21.33826826765541	-48.141753329589	137639
0b3b12ce203d9375038561400cffeb8f6b9d43c7	incremental learning for semantic segmentation of large-scale remote sensing data		In spite of remarkable success of the convolutional neural networks on semantic segmentation, they suffer from catastrophic forgetting: a significant performance drop for the already learned classes when new classes are added on the data, having no annotations for the old classes. We propose an incremental learning methodology, enabling to learn segmenting new classes without hindering dense labeling abilities for the previous classes, although the entire previous data are not accessible. The key points of the proposed approach are adapting the network to learn new as well as old classes on the new training data, and allowing it to remember the previously learned information for the old classes. For adaptation, we keep a frozen copy of the previously trained network, which is used as a memory for the updated network in absence of annotations for the former classes. The updated network minimizes a loss function, which balances the discrepancy between outputs for the previous classes from the memory and updated networks, and the mis-classification rate between outputs for the new classes from the updated network and the new ground-truth. For remembering, we either regularly feed samples from the stored, little fraction of the previous data or use the memory network, depending on whether the new data are collected from completely different geographic areas or from the same city. Our experimental results prove that it is possible to add new classes to the network, while maintaining its performance for the previous classes, despite the whole previous training data are not available.	artificial neural network;catastrophic interference;convolutional neural network;discrepancy function;ground truth;loss function;web feed	Onur Tasar;Yuliya Tarabalka;Pierre Alliez	2018	CoRR			ML	20.77705711649712	-49.47457525731826	137694
5316294922cd600d13104b48ecefeba60623e2f9	a comparison study between different sampling strategies for intrusion detection system of active learning model		Corresponding Author: Mohammad Aref Alshraideh Department of Computer Science, the University of Jordan, Amman, Jordan Email: mshridah@ju.edu.jo Abstract: Active learning aims to train an accurate model with minimum cost by labeling the most informative instances without compromising the model performance. So, choosing an efficient criterion for instance selection is the most important step. Sampling stage is the main issue in active learning for many problems such as intrusion detection system. There are many methods for sampling stage to select the informative instances, but what the method should be used to provide the most accurate to the Intrusion Detection System (IDS). So, we made a comparison between three of these methods, uncertainty sampling, Query By Committee (QBC) and expected model change. The contribution of this study is analyzing and examining three of common strategies that used to select the most informative instances to determine the best one of them. The experimental result showed that the expected model change method achieved the highest accuracy compared with uncertainty sampling and query by committee methods.	computer science;email;information;intrusion detection system;sampling (signal processing)	Ghofran Mohammad Alqaralleh;Mohammad Aref Alshraideh;Ali Alrodan	2018	JCS	10.3844/jcssp.2018.1155.1173	machine learning;artificial intelligence;computer science;sampling (statistics);active learning;intrusion detection system;instance selection	ML	12.534487292197536	-41.65792623101154	137904
4b33998a81442a5b049ecd86373cf7e46449eeda	rapid uncertainty computation with gaussian processes and histogram intersection kernels	rapid uncertainty computation;training example;gaussian process;active learning;classification uncertainty;one-class classification;large-scale datasets;training data;public image datasets;bayesian manner;linear amount;histogram intersection kernel	An important advantage of Gaussian processes is the ability to directly estimate classification uncertainties in a Bayesian manner. In this paper, we develop techniques that allow for estimating these uncertainties with a runtime linear or even constant with respect to the number of training examples. Our approach makes use of all training data without any sparse approximation technique while needing only a linear amount of memory. To incorporate new information over time, we further derive online learning methods leading to significant speed-ups and allowing for hyperparameter optimization on-the-fly. We conduct several experiments on public image datasets for the tasks of one-class classification and active learning, where computing the uncertainty is an essential task. The experimental results highlight that we are able to compute classification uncertainties within microseconds even for largescale datasets with tens of thousands of training examples.	active learning (machine learning);categorization;computation;experiment;gaussian process;kernel (operating system);mathematical optimization;one-class classification;sparse approximation;sparse matrix;synthetic intelligence;time complexity	Alexander Freytag;Erik Rodner;Paul Bodesheim;Joachim Denzler	2012		10.1007/978-3-642-37444-9_40	computer science;machine learning;data mining;statistics	ML	19.132698815941808	-38.178622758376626	138070
256a5a7d482d7cde15d3a9eb3b5921fb2d73e5a2	an adaptive classifier design for accurate speech data classification	data classification	In this paper, an adaptive approach to designing accurate classifiers using Nearest Neighbor (NN) and Linear Discriminant Analysis (LDA) is proposed. A novel NN rule with an adaptive distance measure is proposed to classify input patterns. An iterative learning algorithm is employed to incorporate a local weight to the Euclidean distance measure that attempts to minimize the number of misclassified patterns in the training set. In case of data sets with highly overlapped classes, this may cause the classifier to increase its complexity and overfit. As a solution, LDA is considered as a popular feature extraction technique that aims at creating a feature space that best discriminates the data distributions and reduces overlaps between different classes of data. In this paper, an improved variation of LDA (im-LDA) is investigated which aims to moderate the effect of outlier classes. The proposed classifier design is evaluated by 6 standard data sets from UCI ML repository and eventually by TIMIT data set for framewise classification of speech data. The results show the effectiveness of the designed classifier using im-LDA with the proposed ad-NN method.	algorithm;euclidean distance;feature extraction;feature vector;iterative method;linear discriminant analysis;overfitting;preprocessor;statistical classification;timit;test set	Omid Dehzangi;Ehsan Younessian;Fariborz Hosseini Fard	2009			bayes classifier;speech recognition;quadratic classifier;computer science;linear classifier;pattern recognition;data mining;one-class classification	ML	13.299919241634763	-41.038165040287694	138118
55b9fc72c3d4c41831393e998b07ba6fe1323e3c	kernel combined sparse representation for disease recognition	databases;sparse representation collaborative representation disease recognition;medical image processing diseases image classification image representation matrix algebra;kernel;prototypes;image classification disease recognition correlation structure sparse coefficient high dimensional nonlinear information center based kernel combined sparse representation classifier ckcsr classifier center based kernel matrix center based information exact09 database emphysema ct database minimias database wisconsin breast cancer database hd pectf database;training;collaboration;diseases;kernel training databases diseases correlation collaboration prototypes;correlation	Motivated by the idea that the correlation structure of the entire training set can disclose the relationship between the test sample and the training samples, we propose the combined sparse representation (CSR) classifier for disease recognition. The CSR classifier minimizes the correlation structure of the entire training set multiplied by its transposition and the sparse coefficient together for classification. Including the kernel concept, we propose the kernel combined sparse representation classifier utilizing the high-dimensional nonlinear information instead of the linear information in the CSR classifier. Furthermore, considering the information of the training samples and the class center, we then propose the center-based kernel combined sparse representation (CKCSR) classifier. CKCSR uses the center-based kernel matrix to increase the center-based information that is helpful for classification. The proposed classifiers have been evaluated by extensive experiments on several well-known databases including the EXACT09 database, Emphysema-CT database, mini-MIAS database, Wisconsin breast cancer database, and HD-PECTF database. The experimental results demonstrate that the proposed classifiers achieve better recognition rates than the sparse representation-based classification, collaborative representation based classification, and several state-of-the-art methods.	coefficient;database;experiment;kernel (operating system);nonlinear system;sparse approximation;sparse matrix;test set	Yicong Zhou	2016	IEEE Transactions on Multimedia	10.1109/TMM.2016.2602062	kernel;computer science;machine learning;pattern recognition;data mining;prototype;correlation;collaboration	Vision	16.155729656070353	-49.71335990462195	138367
29fc6824c345c0936d46f55275b8eca9dfc3f80f	a method to generate a reduced training set for faster and better nearest neighbor classification		Classification time and space requirements of nearest neigh bor based classifiers depends directly on the training set size. There exist several ways to reduce the training set size, and also there exist some methods to generate artificial training sets which are aimed at achievi ng better classification accuracy. These are often called bootstrap methods. The paper proposes a method wh ich tries to achieve both of these objectives, namely, improving the performance by generating a bootstra pped training set and reducing the training set size by eliminating some irrelevant training patterns. The proposed method is a faster one than similar recent methods and runs in a linear time of the training set size. The method first will find a clustering in a class of training patterns using the c-means clustering method to derive thec mean patterns, then for each pattern, a new pattern is derived by taking a weighted combination of t he pattern with its mean. This smooths the boundary between classes in the feature space, hence can act s regularization step. Along with this, a threshold distance is set and all patterns that fall within t s distance from a mean pattern are removed from the training set. Since these are mostly the interior patter ns, their removal will not affect the boundary between the classes. Experimentally the proposed method is compare d against recent relevant methods and are shown to be effective and faster than them. The proposed method is a suitable one to work with large data sets like those in data mining.	booting;bootstrapping (statistics);cluster analysis;data mining;existential quantification;experiment;feature vector;horner's method;i/o controller hub;interior point method;relevance;requirement;resampling (statistics);statistical classification;test set;time complexity	P. Viswanath;V. Suresh Babu;T. Naveen Kumar	2010			nearest-neighbor chain algorithm;training set;large margin nearest neighbor;best bin first;k-nearest neighbors algorithm;artificial intelligence;pattern recognition;mathematics	ML	13.44991723603682	-40.4118972037611	139243
58b68a35d32d13bd7ed9efb1f0f278cfae6fc3e0	a new resampling method of imbalanced large data based on class boundary	class boundary large data sets imbalanced data sets classification accuracy;sampling methods pattern classification;large data sets;class boundary;minority class imbalanced large data sets class boundary resampling method large data set compression classification accuracy improvement majority class classification performance improvement;classification accuracy;imbalanced data sets	We propose a new method for the calculation of class boundary. Through the compression of large data sets, the method can remove the samples which are not in the class boundary and have little effect on the classification results. It can also improve the classification accuracy of the traditional algorithm by selecting appropriate threshold. For the imbalanced data sets, this method can remove the samples of majority class on the class boundary and improve the classification performance of the minority class. The experiment has demonstrated that the method is effective.	resampling (statistics)	Sheng Xing;Jun-Hai Zhai;Xiaolan Wang;Ming Yuan	2015		10.1109/ICMLC.2015.7340660	machine learning;pattern recognition;data mining;mathematics;soft independent modelling of class analogies	Robotics	13.72733530572971	-41.16434851722544	139269
af2979c5db513ed756be3b7daeedcadf3d8fc58d	a novel semi-supervised feature extraction method and its application in automotive assembly fault diagnosis based on vision sensor data	automotive assembly;complete kernel fisher discriminant analysis;fault classification;feature extraction;semi-supervised learning	The fault diagnosis of dimensional variation plays an essential role in the production of an automotive body. However, it is difficult to identify faults based on small labeled sample data using traditional supervised learning methods. The present study proposed a novel feature extraction method named, semi-supervised complete kernel Fisher discriminant (SS-CKFDA), and a new fault diagnosis flow for automotive assembly was introduced based on this method. SS-CKFDA is a combination of traditional complete kernel Fisher discriminant (CKFDA) and semi-supervised learning. It adjusts the Fisher criterion with the data global structure extracted from large unlabeled samples. When the number of labeled samples is small, the global structure that exists in the measured data can effectively improve the extraction effects of the projected vector. The experimental results on Tennessee Eastman Process (TEP) data demonstrated that the proposed method can improve diagnostic performance, when compared to other Fisher discriminant algorithms. Finally, the experimental results on the optical coordinate data proves that the method can be applied in the automotive assembly process, and achieve a better performance.	automotive occupations;cross-validation (statistics);experiment;feature extraction;fisher information;image sensor;k-nearest neighbors algorithm;kernel;linear discriminant analysis;miller fisher syndrome;naive bayes classifier;name;population parameter;projections and predictions;semi-supervised learning;semiconductor industry;slow-scan television;supervised learning;surgical revision;sweet syndrome;test set;tracer;algorithm;sensor (device);synovial sarcoma;triangulation	Xuan Zeng;Shibin Yin;Yin Guo;Jiarui Lin;Jigui Zhu	2018		10.3390/s18082545	electronic engineering;computer vision;engineering;feature extraction;artificial intelligence;automotive industry	AI	14.482187755001123	-44.825460967771946	139487
041e7e13829faad3144542b9e52de20f79a95efc	linear classifier combination and selection using group sparse regularization and hinge loss	classifier combination;classifier selection;regularized empirical risk minimization;hinge loss;group sparsity	The main principle of stacked generalization is using a second-level generalizer to combine the outputs of base classifiers in an ensemble. In this paper, after presenting a short survey of the literature on stacked generalization, we propose to use regularized empirical risk minimization (RERM) as a framework for learning the weights of the combiner which generalizes earlier proposals and enables improved learning methods. Our main contribution is using group sparsity for regularization to facilitate classifier selection. In addition, we propose and analyze using the hinge loss instead of the conventional least squares loss. We performed experiments on three different ensemble setups with differing diversities on 13 real-world datasets of various applications. Results show the power of group sparse regularization over the conventional l1 norm regularization. We are able to reduce the number of selected classifiers of the diverse ensemble without sacrificing accuracy. ∗Corresponding authors Email addresses: umutsen@sabanciuniv.edu (Mehmet Umut Sen), haerdogan@sabanciuniv.edu (Hakan Erdogan) Preprint submitted to Pattern Recognition Letters September 28, 2012 With the non-diverse ensembles, we even gain accuracy on average by using group sparse regularization. In addition, we show that the hinge loss outperforms the least squares loss which was used in previous studies of stacked generalization.	diplexer;email;empirical risk minimization;ensemble learning;experiment;hinge loss;least squares;linear classifier;matrix regularization;pattern recognition letters;sparse matrix;taxicab geometry	Mehmet Umut Sen;Hakan Erdogan	2013	Pattern Recognition Letters	10.1016/j.patrec.2012.10.008	regularization perspectives on support vector machines;mathematical optimization;hinge loss;computer science;machine learning;pattern recognition;mathematics;statistics	ML	21.597662386816044	-42.910163824278676	139776
f9fb0ad85f903e3ecf5d4bafdf7c74f647e75f58	lemna: explaining deep learning based security applications		While deep learning has shown a great potential in various domains, the lack of transparency has limited its application in security or safety-critical areas. Existing research has attempted to develop explanation techniques to provide interpretable explanations for each classification decision. Unfortunately, current methods are optimized for non-security tasks ( e.g., image analysis). Their key assumptions are often violated in security applications, leading to a poor explanation fidelity. In this paper, we propose LEMNA, a high-fidelity explanation method dedicated for security applications. Given an input data sample, LEMNA generates a small set of interpretable features to explain how the input sample is classified. The core idea is to approximate a local area of the complex deep learning decision boundary using a simple interpretable model. The local interpretable model is specially designed to (1) handle feature dependency to better work with security applications ( e.g., binary code analysis); and (2) handle nonlinear local boundaries to boost explanation fidelity. We evaluate our system using two popular deep learning applications in security (a malware classifier, and a function start detector for binary reverse-engineering). Extensive evaluations show that LEMNA's explanation has a much higher fidelity level compared to existing methods. In addition, we demonstrate practical use cases of LEMNA to help machine learning developers to validate model behavior, troubleshoot classification errors, and automatically patch the errors of the target models.	approximation algorithm;binary code;decision boundary;deep learning;image analysis;lasso;machine learning;malware;nonlinear system;reverse engineering;static program analysis	Wenbo Guo;Dongliang Mu;Jun Xu;Purui Su;Gang Wang;Xinyu Xing	2018		10.1145/3243734.3243792	computer security;small set;fidelity;troubleshooting;deep learning;malware;decision boundary;machine learning;use case;computer science;transparency (graphic);artificial intelligence	Security	18.68410953787524	-51.5184621132528	139864
a4fa3934f879bbf8295cf61ea320c41e9ca50aac	contrasting undersampled boosting with internal and external feature selection for patient response datasets	patient response;high dimensionality;sampling methods bioinformatics data mining feature selection learning artificial intelligence patient treatment pattern classification;class imbalance;data mining;boosting;pattern classification;patient treatment;class imbalance undersampled boosting patient response datasets hybrid boosting algorithm data sampling iteration method selectrusboost rusboost feature classifier filter based feature selection technique feature subset size bioinformatics;feature selection;bioinformatics boosting class imbalance high dimensionality feature selection patient response;boosting bioinformatics buildings dna stability analysis data models logistics;learning artificial intelligence;sampling methods;bioinformatics	Class imbalance (where one class has many more instances than the other class(es)) and high dimensionality (large number of features per instance) are two prevalent problems that are frequently present in patient response datasets. In addition to these problems, these datasets are notoriously difficult to build effective models from. This paper introduces a new hybrid boosting algorithm named SelectRUSBoost which combines data sampling and feature selection with every iteration of boosting. We test SelectRUSBoost along with RUSBoost combined with external feature selection on a set of five patient response datasets. In addition to the datasets we also utilize two classifiers, three filter-based feature selection techniques, and four feature subset sizes. Our results show that SelectRUSBoost will, with few exceptions, outperform RUSBoost combined with external feature selection. Also, the feature selection technique information gain outperformed the other techniques for all combinations of boosting approach, classifier, and feature subset size, and in addition for this feature selection technique SelectRUSBoost always (without exception) outperformed RUSBoost combined with external selection. Statistical analysis confirmed that SelectRUSBoost gives better performance than RUSBoost combined with external selection. This is the first work which utilizes SelectRUSBoost in a bioinformatics study.	algorithm;bioinformatics;feature selection;information gain in decision trees;iteration;kullback–leibler divergence;logistic regression;multilayer perceptron;sampling (signal processing);statistical classification;undersampling	Taghi M. Khoshgoftaar;David J. Dittman;Randall Wald;Amri Napolitano	2013	2013 12th International Conference on Machine Learning and Applications	10.1109/ICMLA.2013.156	sampling;computer science;machine learning;pattern recognition;data mining;feature selection;feature;boosting	DB	13.328023392091703	-42.504312515863184	139885
a2faeb9307651832390a5111ae13993618a61f2c	an improved code selection algorithm for fault prediction	binary code selection algorithm;cortical learning;fault prediction;sisc (similar inputs map to similar codes);sparse distributed coding	The goal of this study is to present an improved code selection algorithm (BCSA) for fault prediction. The contributions mainly contain three parts. The first part is on the extension of the horizontal input in the code selection algorithm (CSA). We propose that the horizontal input is also the prediction for the next coming event, not only for recalling. Thus, BCSA is able to recall and predict alternately. The second part is on the extension of the generic minicolumnar function. We propose that the function of a minicolumn is to be a k-winner-take-all competitive module (CM) and all active cells (the overall input is 1) should be chosen as winners within a CM. The third part is on the improvement of the competition mechanism. In BCSA, the winners are directly chosen with only one round competition. Thus, computing the input’s similarity G is unnecessary. BCSA is applied to analyze the disaster of the space shuttle Challenger which is a well-known example of fault prediction. Compared to other methods, the result of BCSA is specific, robust and independent of the parameters.	selection algorithm;simulation	Yin Kuang;Zhang Yi;Lei Zhang	2012	Neural Computing and Applications	10.1007/s00521-012-1203-z	simulation;computer science;artificial intelligence;operating system;machine learning	ML	17.464415955420886	-49.46996517599126	139998
e1c36a5e20dae7b9a7eacf6220db34f52128d2c7	joint predictive model and representation learning for visual domain adaptation	unsupervised domain adaptation;image classification;shared subspace;predictive function;representation learning	Traditional learning algorithms cannot perform well in scenarios where training data (source domain data) that are used to learn the model have a different distribution with test data (target domain data). The domain adaptation that intends to compensate this problem is an important capability for an intelligent agent. This paper presents a domain adaptation method which learns to adapt the data distribution of the source domain to that of the target domain where no labeled data of the target domain is available (and just unlabeled data are available for the target domain). Our method jointly learns a low dimensional representation space and an adaptive classifier. In fact, we try to find a representation space and an adaptive classifier on this representation space such that the distribution gap between (the marginal and the conditional distribution of) the two domains is minimized and the risk of the adaptive classifier is also minimized. To evaluate the proposed method, we conduct several experiments on image classification datasets. Experimental results verify the superiority of our method to the existing domain adaptation methods and the proposed method outperforms the other methods with a large margin in some of the domain adaptation problems. These results demonstrate the effectiveness of learning the representation space and the adaptive classifier simultaneously.	adaptive filter;adaptive grammar;algorithm;computer vision;domain adaptation;experiment;feature learning;intelligent agent;machine learning;marginal model;test data	Marzieh Gheisari;Mahdieh Soleymani Baghshah	2017	Eng. Appl. of AI	10.1016/j.engappai.2016.12.004	feature learning;contextual image classification;computer science;machine learning;pattern recognition;data mining	AI	22.75977691026873	-45.778004265522824	140127
c0cce703ba02504974fdd59d0e1e1ea6b822bd6f	feature subset selection for classification of histological images	feature extraction;feature subset selection;classification system;prediction accuracy;feature selection;case based learning;central nervous system	Classification of histological images is considered in this paper. The task is to distinguish different classes of tumours of the central nervous system on the basis of features extracted from microscopic slides. The number of extracted features is relatively high and some of them seem to be irrelevant for classification of the images. Thus, the main objective of this study is to select such a feature subset that improves the predictive accuracy of the classifier. The wrapper approach is chosen to obtain this aim, where a search for the good subset of features is made using a non-parametric case-base classifier. To guide a search process, a forward beam selection algorithm is introduced. It sequentially adds relevant features in a parallel way for the most promising subsets. It is shown that the proposed approach gives good predictive accuracy for the considered histopathological problem.	class;classification;extraction;neoplasms;nervous system structure;relevance;selection algorithm;slide (glass microscope);subgroup	Jacek Jelonek;Jerzy Stefanowski	1997	Artificial intelligence in medicine	10.1016/S0933-3657(96)00375-2	feature extraction;computer science;central nervous system;machine learning;pattern recognition;data mining;feature selection;feature	AI	10.67855329521679	-45.85040358339154	140495
5fde0f7213bbfd4f9eb8fc85164d79c297296cc4	a scalable memetic algorithm for simultaneous instance and feature selection	scaling up memetic algorithms instance selection feature selection;instance selection;scaling up;memetic algorithms;feature selection	Instance selection is becoming increasingly relevant due to the huge amount of data that is constantly produced in many fields of research. At the same time, most of the recent pattern recognition problems involve highly complex datasets with a large number of possible explanatory variables. For many reasons, this abundance of variables significantly harms classification or recognition tasks. There are efficiency issues, too, because the speed of many classification algorithms is largely improved when the complexity of the data is reduced. One of the approaches to address problems that have too many features or instances is feature or instance selection, respectively. Although most methods address instance and feature selection separately, both problems are interwoven, and benefits are expected from facing these two tasks jointly. This paper proposes a new memetic algorithm for dealing with many instances and many features simultaneously by performing joint instance and feature selection. The proposed method performs four different local search procedures with the aim of obtaining the most relevant subsets of instances and features to perform an accurate classification. A new fitness function is also proposed that enforces instance selection but avoids putting too much pressure on removing features. We prove experimentally that this fitness function improves the results in terms of testing error. Regarding the scalability of the method, an extension of the stratification approach is developed for simultaneous instance and feature selection. This extension allows the application of the proposed algorithm to large datasets. An extensive comparison using 55 medium to large datasets from the UCI Machine Learning Repository shows the usefulness of our method. Additionally, the method is applied to 30 large problems, with very good results. The accuracy of the method for class-imbalanced problems in a set of 40 datasets is shown. The usefulness of the method is also tested using decision trees and support vector machines as classification methods.	decision trees;decision tree;evolutionary computation;experiment;feature selection;fitness function;genetic selection;genetic algorithm;hepatitis c, chronic;hereditary diseases;image scaling;instance-based learning;local search (optimization);machine learning;memetic algorithm;memetics;pattern recognition;requirement;scalability;statistical classification;stratification;support vector machine;time complexity;trees (plant);benefit;explanation	Nicolás García-Pedrajas;Aida de Haro-García;Javier Pérez-Rodríguez	2014	Evolutionary Computation	10.1162/EVCO_a_00102	mathematical optimization;computer science;machine learning;pattern recognition;data mining;feature selection;memetic algorithm	AI	10.89400234065181	-43.0342790275056	140561
4e4311a5fd99b17bed31b7006a572d29a58cdcf3	a support vector machine classifier with automatic confidence and its application to gender classification	quadratic program;gender classification;pattern classification;support vector machine;label confidence	In this paper, we propose a support vector machine with automatic confidence (SVMAC) for pattern classification. The main contributions of this work to learning machines are twofold. One is that we develop an algorithm for calculating the label confidence value of each training sample. Thus, the label confidence values of all of the training samples can be considered in training support vector machines. The other one is that we propose a method for incorporating the label confidence value of each training sample into learning and derive the corresponding quadratic programming problems. To demonstrate the effectiveness of the proposed SVMACs, a series of experiments are performed on three benchmarking pattern classification problems and a challenging gender classification problem. Experimental results show that the generalization performance of our SVMACs is superior to that of traditional SVMs. & 2011 Elsevier B.V. All rights reserved.	algorithm;experiment;quadratic programming;statistical classification;support vector machine	Ji Zheng;Bao-Liang Lu	2011	Neurocomputing	10.1016/j.neucom.2010.07.032	support vector machine;quadratic classifier;computer science;machine learning;linear classifier;pattern recognition;data mining;relevance vector machine;quadratic programming;structured support vector machine;one-class classification	AI	18.497577440606005	-41.228386463544574	140725
a758c2cf96d1eb7fe5adb58c5f2fafcdf4e21eab	metareg: towards domain generalization using meta-regularization		Training models that generalize to new domains at test time is a problem of fundamental importance in machine learning. In this work, we encode this notion of domain generalization using a novel regularization function. We pose the problem of finding such a regularization function in a Learning to Learn (or) metalearning framework. The objective of domain generalization is explicitly modeled by learning a regularizer that makes the model trained on one domain to perform well on another domain. Experimental validations on computer vision and natural language datasets indicate that our method can learn regularizers that achieve good cross-domain generalization.	benchmark (computing);computer vision;diabetes mellitus, non-insulin-dependent;encode;experiment;generalization (psychology);machine learning;matrix regularization;natural language;reinforcement learning;scalability;anatomical layer;emotional dependency	Yogesh Balaji;Swami Sankaranarayanan	2018			machine learning;computer science;natural language;artificial intelligence;regularization (mathematics)	ML	23.722081742314003	-46.76112248951906	140782
25c0b5de95e19c74944d7e1b8228090b7af67c65	transformation of discriminative single-task classification into generative multi-task classification in machine learning context		Classification is one of the most popular tasks of machine learning, which has been involved in broad applications in practice, such as decision making, sentiment analysis and pattern recognition. It involves the assignment of a class/label to an instance and is based on the assumption that each instance can only belong to one class. This assumption does not hold, especially for indexing problems (when an item, such as a movie, can belong to more than one category) or for complex items that reflect more than one aspect, e.g. a product review outlining advantages and disadvantages may be at the same time positive and negative. To address this problem, multi-label classification has been increasingly used in recent years, by transforming the data to allow an instance to have more than one label; the nature of learning, however, is the same as traditional learning, i.e. learning to discriminate one class from other classes and the output of a classifier is still single (although the output may contain a set of labels). In this paper we propose a fundamentally different type of classification in which the membership of an instance to all classes(/labels) is judged by a multiple-input-multiple-output classifier through generative multi-task learning. An experimental study is conducted on five UCI data sets to show empirically that an instance can belong to more than one class, by using the theory of fuzzy logic and checking the extent to which an instance belongs to each single class, i.e. the fuzzy membership degree. The paper positions new research directions on multi-task classification in the context of both supervised learning and semi-supervised learning.		Han Liu;Mihaela Cocea;Alaa Mohasseb;Mohamed Bader	2017	2017 Ninth International Conference on Advanced Computational Intelligence (ICACI)	10.1109/ICACI.2017.7974487	instance-based learning;semi-supervised learning;one-class classification;active learning (machine learning);machine learning;linear classifier;stability (learning theory);multi-task learning;multiclass classification;computer science;artificial intelligence;pattern recognition	ML	15.486517450131938	-42.46841956765087	140915
7467b2df8d005333207c5014b568d1692a6457ac	classification with svm in lorentzian space		Classification is one of the most researched issues in Machine Learning. In this study, the Lorentzian Support Vector Machine (LSVM) method is proposed that performs classification in Lorentzian space. This proposed new classifier forms a hyperplane separating the classes based on the Lorentzian metric and maximize margins between nearest points to the hyperplane according to the Lorentzian distance. Thus, samples from different classes are classified in Lorentzian space. Also, for the purpose of increasing the classification accuracy, a pre-preprocessing is applied. Experimental results taken from LSVT, SONAR, TELESCOPE and WISCONSIN data sets validate the proposed LSVM method.	machine learning;preprocessor;sonar;support vector machine	Yerzhan Kerimbekov;Hasan S. Bilge;Hasan Huseyin Ugurlu	2017	2017 25th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2017.7960345	support vector machine;pattern recognition;kernel (linear algebra);hyperplane;sonar;data set;machine learning;mathematics;artificial intelligence	ML	16.618756104351167	-42.66463196646686	140958
36f6f63cc1615d41933933419798457e3ebda3a3	semi-supervised two phase test sample sparse representation classifier		Abstract Two Phase Test Sample Sparse Representation (TPTSSR) classifier was recently proposed as an efficient alternative to the Sparse Representation Classifier (SRC). It aims at classifying data using sparse coding in two phases with l 2 regularization. Although high performances can be obtained by the TPTSSR classifier, since it is a supervised classifier, it is not able to benefit from unlabeled samples which are very often available. In this paper, we introduce a semi-supervised version of the TPTSSR classifier called Semi-supervised Two Phase Test Sample Sparse Representation (STPTSSR). STPTSSR combines the merits of sparse coding, active learning and the two phase collaborative representation classifiers. The proposed framework is able to make any sparse representation based classifier semi-supervised. Extensive experiments carried out on six benchmark image datasets show that the proposed STPTSSR can outperform the classical TPTSSR as well as many state-of-the-art semi-supervised methods.	semi-supervised learning;semiconductor industry;sparse approximation;sparse matrix;statistical classification	Youssof El Traboulsi;Fadi Dornaika;Yassine Ruichek	2018	Knowl.-Based Syst.	10.1016/j.knosys.2018.06.018	artificial intelligence;machine learning;computer science;active learning;regularization (mathematics);classifier (linguistics);neural coding;sparse approximation	ML	21.75964729302626	-42.907360307207895	141159
931fc9fb359c8801f2fe239fb116bfff42e7e422	discriminative embedded unsupervised feature selection		Abstract Unsupervised feature selection is a powerful tool to process high-dimensional data, in which a subset of features are selected out for effective data representation. In this paper, we propose a novel unsupervised feature selection method which discovers and exploits the global information of the data by maximizing distances between samples from different clusters, and preserving the locality of the data by incorporating a Laplacian regularization. Moreover, the proposed method directly ranks the features without any transformation by introducing a simplex-based sparse learning strategy, and enables highly discriminative features to be chosen. Extensive experiments are carried out and the results show effectiveness of the proposed method.	embedded system;feature selection;unsupervised learning	Qi-Hai Zhu;Yu-Bin Yang	2018	Pattern Recognition Letters	10.1016/j.patrec.2018.07.018	locality;artificial intelligence;pattern recognition;mathematics;discriminative model;external data representation;regularization (mathematics);feature selection;laplace operator	Vision	24.216243266713906	-42.22125383431169	141187
b7ac2adce0dcefbb75363d7cd4970a6f722cc820	subspace learning based active learning for image retrieval	image sampling;image retrieval active learning subspace learning;computational geometry;image classification;support vector machines image retrieval labeling training learning systems accuracy kernel;feature extraction;relevance feedback computational geometry feature extraction image classification image retrieval image sampling learning artificial intelligence;learning artificial intelligence;relevance feedback;informative data selection subspace learning novel batch mode active learning method image retrieval relevance feedback human labelling classifier improvement label selection geometrical structure data distribution user labeling information mining feature space dimension classifier instability certainty propagation scheme uncertainty sampling subal;image retrieval	The goal of relevance feedback is to improve the performance of image retrieval by leveraging the labeling of human. It is helpful to introduce active learning method in relevance feedback to alleviate the human burden. In the traditional active learning the samples which can improve the classifier the most if they were labeled are selected for the user's labeling. However, the change of the geometrical structure of the data distribution caused by such expensive labeled samples is not fully exploited. By mining user's labeling information, we can reduce the original feature space dimension to ease the classifier's instability brought by the small sample size. In this paper, we propose a novel batch mode active learning method for informative data selection. The labeled samples are not only used to retrain the classifier, but to learn a subspace which efficiently encodes user's intention as well. Especially, a scheme of certainty propagation on the subspace effectively integrates uncertainty sampling and subspace learning into the proposed Subspace learning based batch mode Active Learning method (SubAL) in relevance feedback. Extensive experiments on publicly available dataset shows that the proposed method is promising.	active learning (machine learning);batch processing;experiment;feature vector;image retrieval;information;instability;relevance feedback;sampling (signal processing);software propagation;statistical classification	Biao Niu;Yifan Zhang;Jinqiao Wang;Jian Cheng;Hanqing Lu	2013	2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)	10.1109/ICMEW.2013.6618268	semi-supervised learning;computer vision;contextual image classification;feature extraction;computational geometry;image retrieval;computer science;machine learning;pattern recognition;learning classifier system;active learning	Vision	19.44465674147531	-44.7750161572018	141236
fbd257eda74229e5a19afc3a6825342b30af6db4	experimental study of unsupervised feature learning for hep-2 cell images clustering	feature extraction training noise reduction vectors neural networks image coding decoding;image coding;neural networks;learning;decoding;training;cell;unsupervised;experimental;hep;unsupervised learning feature extraction image representation medical image processing;vectors;clustering;feature extraction;noise reduction;study;feature;deep learning model unsupervised feature learning hep 2 cell image clustering feature representation bow model;2;images	Automatic identification of HEp-2 cell images has received an increasing research attention. Feature representations play a critical role in achieving good identification performance. Much recent work has focused on supervised feature learning. Typical methods consist of BoW model (based on hand-crafted features) and deep learning model (learning hierarchical features). However, these labels used in supervised feature learning are very labour-intensive and time-consuming. They are commonly manually annotated by specialists and very expensive to obtain. In this paper, we follow this fact and focus on unsupervised feature learning. We have verified and compared the features of these two typical models by clustering. Experimental results show the BoW model generally perform better than deep learning models. Also, we illustrate BoW model and deep learning models have complementarity properties.	automatic identification and data capture;cluster analysis;codebook;complementarity (physics);deep learning;feature learning;supervised learning	Yan Zhao;Zhimin Gao;Lei Wang;Luping Zhou	2014	2014 International Conference on Digital Image Computing: Techniques and Applications (DICTA)	10.1109/DICTA.2014.7008108	semi-supervised learning;unsupervised learning;cell;square root of 2;feature learning;computer vision;feature;feature extraction;computer science;machine learning;pattern recognition;noise reduction;cluster analysis;competitive learning;deep belief network;active learning;feature;artificial neural network;generalization error	AI	22.736843711653442	-51.619888646167965	141251
a953eda91d2404fbf88518dca609cc92c2e07f40	learning by propagability	learning process;supervised learning;probability density function;handwritten digit recognition;optimal feature representation;data mining;semi supervised learning;face recognition;dimensionality reduction;dimensionality reduction learning by propagability optimal feature representation;feature extraction;principal component analysis;optimization;semisupervised learning training data principal component analysis linear discriminant analysis algorithm design and analysis feature extraction face recognition scattering design optimization data mining;learning by propagability;feature extraction face recognition;state of the art methods propagability optimal feature representation harmonic system semi supervised configurations conventional semi supervised learning algorithms face recognition;dimensional reduction;algorithm design and analysis	In this paper, we present a novel feature extraction framework, called learning by propagability. The whole learning process is driven by the philosophy that the data labels and optimal feature representation can constitute a harmonic system, namely, the data labels are invariant with respect to the propagation on the similarity-graph constructed by the optimal feature representation. Based on this philosophy, a unified formulation for learning by propagability is proposed for both supervised and semi-supervised configurations. Specifically, this formulation offers the semi-supervised learning two characteristics: 1) unlike conventional semi-supervised learning algorithms which mostly include at least two parameters, this formulation is parameter-free; and 2) the formulation unifies the label propagation and optimal representation pursuing, and thus the label propagation is enhanced by benefiting from the graph constructed with the derived optimal representation instead of the original representation. Extensive experiments on UCI toy data, handwritten digit recognition, and face recognition all validate the effectiveness of our proposed learning framework compared with the state-of-the-art methods for feature extraction and semi-supervised learning.	algorithm;algorithmic efficiency;cluster analysis;computation;experiment;facial recognition system;feature extraction;machine learning;mathematical optimization;nonlinear programming;nonlinear system;optimization toolbox;schema (genetic algorithms);semi-supervised learning;semiconductor industry;software propagation;supervised learning;unsupervised learning	Bingbing Ni;Shuicheng Yan;Ashraf A. Kassim;Loong Fah Cheong	2008	2008 Eighth IEEE International Conference on Data Mining	10.1109/ICDM.2008.53	semi-supervised learning;unsupervised learning;algorithm design;feature learning;computer vision;probability density function;feature extraction;computer science;machine learning;pattern recognition;supervised learning;stability;active learning;dimensionality reduction;principal component analysis;generalization error	Vision	24.411338506413145	-41.610106697755654	141312
8837caf0f49df3975d99ac7cf4faad125e14d30d	v-svm for transient stability assessment in power systems	minimisation;power system simulation;kernel;2nd order convex programming;neural networks;support vector machines;kernel functions;neural nets;convex programming;structural risk minimization;transient stability assessment;risk management;power systems;kernel function;convergence rate;power system interconnection;transient stability;backpropagation;optimal control;artificial neural networks;minimisation support vector machines power system transient stability neural nets generalisation artificial intelligence convex programming optimal control;radial basis function;power system;power system transients;power system transients power system stability kernel power system analysis computing power system interconnection power system simulation risk management power system modeling artificial neural networks support vector machines;radial basis function v svm transient stability assessment power systems support vector machines neurons generalization capability structural risk minimization neural networks 2nd order convex programming kernel functions backpropagation;power system analysis computing;neurons;generalisation artificial intelligence;support vector machine;v svm;power system transient stability;power system modeling;power system stability;neural network;generalization capability	In this paper, support vector machines (SVMs) are studied in the application of transient stability assessment in power systems. SVMs have the following advantages: automatic determination of the number of hidden neurons, fast convergence rate, good generalization capability, etc. SVMs use the principle of structural risk minimization, and thus reduce the dependency of experience unlike neural networks and have better generalization and classification precision. Furthermore, SVMs are solved by the 2nd order convex programming and the final solution of SVMs is sole and optimal. The performance of SVMs depends on the type of kernel functions and the parameters of kernel functions, which are determined by experience or experiments. So the effects of kernel functions and the parameters of kernel functions are analyzed by experiments in the paper. In addition, Experiments corroborate the superiority of v-SVM applied in TSA in power systems by comparing with BP and RBE.	artificial neural network;convex optimization;experiment;ibm power systems;rate of convergence;structural risk minimization;support vector machine	Xiaohong Wang;Sitao Wu;Qunzhan Li;Xiaoru Wang	2005	Proceedings Autonomous Decentralized Systems, 2005. ISADS 2005.	10.1109/ISADS.2005.1452085	kernel;support vector machine;mathematical optimization;computer science;machine learning;electric power system;artificial neural network	ML	19.83040871982599	-38.91565250259843	141322
8db14dd7572c648849ab24098172dac325ca0c67	multivariate discretization based on evolutionary cut points selection for classification	numerical attributes classification data mining dm data preprocessing discretization evolutionary algorithms eas;pattern classification bayes methods learning artificial intelligence;sociology statistics biological cells genetic algorithms evolutionary computation accuracy proposals;pruning and building integrated in classification classifiers multivariate discretization evolutionary cut points selection data preprocessing numerical attributes learning algorithm multivariate classification problem evolutionary algorithm wrapper fitness function reduction mechanism multivariate approach competitive discretization scheme c4 5 naive bayes part	Discretization is one of the most relevant techniques for data preprocessing. The main goal of discretization is to transform numerical attributes into discrete ones to help the experts to understand the data more easily, and it also provides the possibility to use some learning algorithms which require discrete data as input, such as Bayesian or rule learning. We focus our attention on handling multivariate classification problems, where high interactions among multiple attributes exist. In this paper, we propose the use of evolutionary algorithms to select a subset of cut points that defines the best possible discretization scheme of a data set using a wrapper fitness function. We also incorporate a reduction mechanism to successfully manage the multivariate approach on large data sets. Our method has been compared with the best state-of-the-art discretizers on 45 real datasets. The experiments show that our proposed algorithm overcomes the rest of the methods producing competitive discretization schemes in terms of accuracy, for C4.5, Naive Bayes, PART, and PrUning and BuiLding Integrated in Classification classifiers; and obtained far simpler solutions.	c4.5 algorithm;cut (graph theory);data pre-processing;discrete mathematics;discretization;electromechanical dissociation;evolutionary algorithm;experiment;fitness function;genetic selection;handling (psychology);interaction;large;machine learning;naive bayes classifier;numerical analysis;preprocessor;solutions;statistical classification;subgroup	Sergio Ramírez-Gallego;Salvador García;José Manuel Benítez;Francisco Herrera	2016	IEEE Transactions on Cybernetics	10.1109/TCYB.2015.2410143	machine learning;pattern recognition;data mining;mathematics	Vision	10.258434587758495	-43.14539102299634	141649
791e03c5691d454e1d87c568e81bb48e274dfb6e	ensemble based data stream mining with recalling and forgetting mechanisms	classification algorithms accuracy prediction algorithms training data mining algorithm design and analysis clustering algorithms;ensemble pruning data stream mining ebbinghaus forgetting curve recalling mechanism;memory repository ensemble based data stream mining forgetting mechanisms recalling mechanisms sequential chunks mae algorithm memorizing based adaptive ensemble algorithm chunk based data streams ensemble pruning ebbinghaus forgetting curve;pattern classification data mining	Using ensemble of classifiers on sequential chunks of training instances is a popular strategy for data stream mining. Aiming at the limitations of the existing approaches, we introduce recalling and forgetting mechanisms into ensemble based data stream mining, and put forward a new algorithm MAE (Memorizing based Adaptive Ensemble) to mine chunk-based data streams with concept drifts. Ensemble pruning is used as a recalling mechanism to select useful component classifiers for each incoming data chunk. Ebbinghaus forgetting curve is adopted as a forgetting mechanism to evaluate and replace the component classifiers in the memory repository. Experiments have been performed on datasets with different types of concept drifts. Compared with traditional ensemble approaches, the results show that MAE is a good algorithm with high and stable accuracy, less predicting time and moderate training time.	ace;algorithm;data stream mining;experiment;sensor	Yanhuang Jiang;Qiangli Zhao;Yutong Lu	2014	2014 11th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2014.6980873	computer science;machine learning;pattern recognition;data mining	ML	13.444410133492418	-39.12622188231617	141658
d3796b40df755ed87f5e915508a5b4259cf2cd9d	efficient pairwise classification	computational complexity	Pairwise classification is a class binarization procedure that converts a multi-class problem into a series of two-class problems, one problem for each pair of classes. While it can be shown that for training, this procedure is more efficient than the more commonly used oneagainst-all approach, it still has to evaluate a quadratic number of classifiers when computing the predicted class for a given example. In this paper, we propose a method that allows a faster computation of the predicted class when weighted or unweighted voting are used for combining the predictions of the individual classifiers. While its worst-case complexity is still quadratic in the number of classes, we show that even in the case of completely random base classifiers, our method still outperforms the conventional pairwise classifier. For the more practical case of well-trained base classifiers, its asymptotic computational complexity seems to be almost linear.	algorithm;asymptotic computational complexity;best, worst and average case;binary image;computation;computational complexity theory;naive bayes classifier;worst-case complexity	Sang-Hyeun Park;Johannes Fürnkranz	2007		10.1007/978-3-540-74958-5_65	random subspace method;mathematical optimization;machine learning;pattern recognition;mathematics	AI	20.2939047165654	-39.22721902892805	141919
c4c8c9a5e1001ab237a95a5490edc764b802a659	combination of supervised and unsupervised classification using the theory of belief functions		In this paper, we propose to fuse both clustering and supervised classification approach in order to outperform the results of a classification algorithm. Indeed the results of the learning in supervised classification depend on the method and on the parameters chosen. Moreover the learning process is particularly difficult which few learning data and/or imprecise learning data. Hence, we define a classification approach using the theory of belief functions to fuse the results of one clustering and one supervised classification. This new approach applied on real databases allows good and promising results.	algorithm;cluster analysis;database;machine learning;medical imaging;missing data;sonar (symantec);statistical classification;supervised learning;unsupervised learning	Fatma Karem;Mounir Dhibi;Arnaud Martin	2012		10.1007/978-3-642-29461-7_10	semi-supervised learning;unsupervised learning;machine learning;linear classifier;pattern recognition;data mining;mathematics;one-class classification	ML	17.404844602804996	-42.5341845127673	141959
e073c4b82b5c1e624f682197f85ea80581d1ad3c	weighted support vector machine for classification	support vector machines;uneven training class size support vector machine classification weighting factor;breast cancer diagnosis;weighting factor weighted support vector machine pattern classification classification bias training sample misclassification penalty training class size classification accuracy c svm v svm breast cancer diagnosis;support vector machines support vector machine classification object detection error analysis fault diagnosis industrial control intelligent systems machine intelligence decision making electronic mail;pattern classification;pattern classification support vector machines;support vector machine;classification accuracy	In the standard support vector machines for classification, the use of training sets with uneven class sizes results in classification biases towards the class with the large training size. The main causes lie in that the penalty of misclassification for each training sample is considered equally. Weighted support vector machines for classification are proposed in this paper where penalty of misclassification for each training sample is different. By setting the equal penalty for the training samples belonging to same class, and setting the ratio of penalties for different classes to the inverse ratio of the training class sizes, the obtained weighted support vector machines compensate for the undesirable effects caused by the uneven training class size, and the classification accuracy for the class with small training size is improved. But this improvement is obtained at the cost of the possible decrease of classification accuracy for the class with large training size and the possible decrease of the total classification accuracy. Two weighted support vector machines, namely weighted C-SVM and V-SVM, corresponding to C-SVM and V-SVM are given respectively. Experimental simulations on breast cancer diagnosis show the effectiveness of the proposed methods.	simulation;support vector machine	Shu-Xin Du;Sheng-Tan Chen	2005	2005 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2005.1571749	support vector machine;least squares support vector machine;computer science;machine learning;linear classifier;pattern recognition;data mining;relevance vector machine;structured support vector machine;one-class classification	ML	12.4501568110764	-39.42098801485921	142004
52f184b5e619e3b440f9afefb0ec134aaed5f2f2	cooperative multi-objective evolutionary support vector machines for multiclass problems		In recent years, evolutionary algorithms have been found to be effective and efficient techniques to train support vector machines (SVMs) for binary classification problems while multiclass problems have been neglected. This paper proposes CMOE-SVM: Cooperative Multi-Objective Evolutionary SVMs for multiclass problems. CMOE-SVM enables SVMs to handle multiclass problems via co-evolutionary optimization, by breaking down the original M-class problem into M simpler ones, which are optimized simultaneously in a cooperative manner. Furthermore, CMOE-SVM can explicitly maximize the margin and reduce the training error (the two components of the SVM optimization), by means of multi-objective optimization. Through a comprehensive experimental evaluation using a suite of benchmark datasets, we validate the performance of CMOE-SVM. The experimental results, supported by statistical tests, give evidence of the effectiveness of the proposed approach for solving multiclass classification problems.	benchmark (computing);binary classification;convex optimization;cooperative coevolution;evolutionary algorithm;mathematical optimization;multi-objective optimization;multiclass classification;optimization problem;overfitting;semidefinite programming;solver;support vector machine	Alejandro Rosales-Pérez;Andrés Eduardo Gutiérrez-Rodríguez;Salvador García;Hugo Terashima-Marín;Carlos A. Coello Coello;Francisco Herrera	2018		10.1145/3205455.3205524	artificial intelligence;statistical hypothesis testing;machine learning;support vector machine;multi-objective optimization;binary classification;evolutionary algorithm;computer science;multiclass classification	AI	10.180135860806757	-42.61785795216541	142091
700bb2f8962af01de17df1077be418cb3f751b53	comparison of two topological approaches for dealing with noisy labeling	identification of mislabeled instance;topological learning;separability index;machine learning;relaxation;cut edges weighted;geometrical graphs	This paper focuses on the detection of likely mislabeled instances in a learning dataset. In order to detect potentially mislabeled samples, two solutions are considered which are both based on the same framework of topological graphs. The first is a statistical approach based on Cut Edges Weighted statistics (CEW) in the neighborhood graph. The second solution is a Relaxation Technique (RT) that optimizes a local criterion in the neighborhood graph. The evaluations by ROC curves show good results since almost 90% of the mislabeled instances are retrieved for a cost of less than 20% of false positive. The removal of samples detected as mislabeled by our approaches generally leads to an improvement of the performances of classical machine learning algorithms.	algorithm;machine learning;performance;receiver operating characteristic;relaxation (approximation)	Fabien Rico;Fabrice Muhlenbach;Djamel A. Zighed;Stéphane Lallich	2015	Neurocomputing	10.1016/j.neucom.2014.10.087	combinatorics;computer science;machine learning;relaxation;pattern recognition;mathematics	ML	20.662981653838326	-41.33135387301237	142404
88aa6c64336713c73ac298850f302d4f425332c8	incremental boolean combination of classifiers	boolean combination;training data;incremental boolean combination;boolean fusion function;new learn-and-combine approach;pool size;diversified pool;new data;previously-generated pool;new block	The incremental Boolean combination (incrBC ) technique is a new learn-and-combine approach that is proposed to adapt ensemblebased pattern classification systems over time, in response to new data acquired during operations. When a new block of training data becomes available, this technique generates a diversified pool of base classifiers from the data by varying training hyperparameters and random initializations. The responses of these classifiers are then combined with those of previously-trained classifiers through Boolean combination in the ROC space. Through this process, an ensemble is selected from the pool, where Boolean fusion functions and thresholds are adapted for improved accuracy, while redundant base classifiers are pruned. Results of computer simulations conducted using Hidden Markov Models (HMMs) on synthetic and real-world host-based intrusion detection data indicate that incrBC can sustain a significantly higher level of accuracy than when the parameters of a single best HMM are re-estimated for each new block of data, using reference batch and incremental learning techniques. It also outperforms static fusion techniques such as majority voting for combining the responses of new and previously-generated pools of HMMs. Pruning prevents pool sizes from increasing indefinitely over time, without adversely affecting the overall ensemble performance.	algorithm;boolean algebra;computer simulation;hidden markov model;intrusion detection system;markov chain;operating point;synthetic intelligence	Wael Khreich;Eric Granger;Ali Miri;Robert Sabourin	2011		10.1007/978-3-642-21557-5_36	random subspace method;computer science;machine learning;pattern recognition;data mining	ML	12.912610344032693	-40.86733107958736	142459
14f0960eb6633d85f6d02fd44e0e88fca6ce74e9	interactive multi-task relationship learning	machine learning algorithms;training;covariance matrices;diseases;knowledge transfer;predictive models;data models	Multi-task learning (MTL) is a learning paradigm that provides a principled way to improve the generalization performance of a set of related machine learning tasks by transferring knowledge among the tasks. The past decade has witnessed many successful applications of MTL in different domains. In the center of MTL algorithms is how the relatedness of tasks are modeled and encoded in learning formulations to facilitate knowledge transfer. Among the MTL algorithms, the multi-task relationship learning (MTRL) attracted much attention in the community because it learns task relationship from data to guide knowledge transfer, instead of imposing a prior task relatedness assumption. However, this method heavily depends on the quality of training data. When there is insufficient training data or the data is too noisy, the algorithm could learn an inaccurate task relationship that misleads the learning towards suboptimal models. To address the aforementioned challenge, in this paper we propose a novel interactive multi-task relationship learning (iMTRL) framework that efficiently solicits partial order knowledge of task relationship from human experts, effectively incorporates the knowledge in a proposed knowledge-aware MTRL formulation. We propose an efficient optimization algorithm for kMTRL and comprehensively study query strategies that identify the critical pairs that are most influential to the learning. We present extensive empirical studies on both synthetic and real datasets to demonstrate the effectiveness of proposed framework.	algorithm;computer multitasking;critical pair (logic);machine learning;mathematical optimization;multi-task learning;programming paradigm;synthetic intelligence;test set	Kaixiang Lin;Jiayu Zhou	2016	2016 IEEE 16th International Conference on Data Mining (ICDM)	10.1109/ICDM.2016.0035	semi-supervised learning;unsupervised learning;data modeling;multi-task learning;instance-based learning;error-driven learning;computer science;artificial intelligence;online machine learning;machine learning;data mining;inductive transfer;predictive modelling;stability;active learning;generalization error	AI	21.597923564074936	-43.93391565250566	142481
4536ed6b7ac42a8427ecd17ae8568f1079231790	ransac-gp: dealing with outliers in symbolic regression with genetic programming		Genetic programming (GP) has been shown to be a powerful tool for automatic modeling and program induction. It is often used to solve difficult symbolic regression tasks, with many examples in real-world domains. However, the robustness of GP-based approaches has not been substantially studied. In particular, the present work deals with the issue of outliers, data in the training set that represent severe errors in the measuring process. In general, a datum is considered an outlier when it sharply deviates from the true behavior of the system of interest. GP practitioners know that such data points usually bias the search and produce inaccurate models. Therefore, this work presents a hybrid methodology based on the RAndom SAmpling Consensus (RANSAC) algorithm and GP, which we call RANSAC-GP. RANSAC is an approach to deal with outliers in parameter estimation problems, widely used in computer vision and related fields. On the other hand, this work presents the first application of RANSAC to symbolic regression with GP, with impressive results. The proposed algorithm is able to deal with extreme amounts of contamination in the training set, evolving highly accurate models even when the amount of outliers reaches 90%.	genetic programming;random sample consensus;symbolic regression	Uriel López;Leonardo Trujillo;Yuliana Martínez;Pierrick Legrand;Enrique Naredo;Sara Silva	2017		10.1007/978-3-319-55696-3_8	machine learning;pattern recognition	AI	15.945886549405085	-40.95607256921765	142486
361d30fea9358a1284fca230ffdcb9bebc44a7f1	low-rank multi-view clustering in third-order tensor space		The plenty information from multiple views data as well as the complementary information among different views are usually beneficial to various tasks, e.g., clustering, classification, de-noising. Multi-view subspace clustering is based on the fact that the multi-view data are generated from a latent subspace. To recover the underlying subspace structure, the success of the sparse and/or low-rank subspace clustering has been witnessed recently. Despite some state-of-the-art subspace clustering approaches can numerically handle multi-view data, by simultaneously exploring all possible pairwise correlation within views, the high order statistics is often disregarded which can only be captured by simultaneously utilizing all views. As a consequence, the clustering performance for multi-view data is compromised. To address this issue, in this paper, a novel multi-view clustering method is proposed by using t-product in third-order tensor space. Based on the circular convolution operation, multi-view data can be effectively represented by a tlinear combination with sparse and low-rank penalty using “selfexpressiveness”. Our extensive experimental results on facial, object, digits image and text data demonstrate that the proposed method outperforms the state-of-the-art methods in terms of many criteria.	circular convolution;cluster analysis;clustering high-dimensional data;experiment;free viewpoint television;low-rank approximation;markov chain;numerical analysis;sparse matrix;spectral clustering;statistical classification;text corpus;eric	Ming Yin;Junbin Gao;Shengli Xie;Yi Guo	2016	CoRR		correlation clustering;constrained clustering;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;brown clustering;affinity propagation;statistics;clustering high-dimensional data;conceptual clustering	AI	24.489246147024748	-43.349806320068865	142545
4dd064958db9b9cd87072bd9ddb43ad207060988	time series classification based on multi-codebook important time subsequence approximation algorithm		This paper proposes a multi-codebook important time subsequence approximation (MCITSA) algorithm for time series classification. MCITSA generates a codebook using important time subsequences for each class based on the difference of categories. In this way, each codebook contains the class information itself. To predict the class label of an unseen time series, MCITSA needs to compare the similarities between important time subsequences extracted from the unseen time series and codewords of each class. Experimental results on time series datasets demonstrate that MCITSA is more powerful than PVQA in classifying time series.	approximation algorithm;codebook;time series	Zhiwei Tao;Li Zhang;Bangjun Wang;Fanzhang Li	2016		10.1007/978-3-319-46681-1_69	combinatorics;discrete mathematics;longest increasing subsequence;pattern recognition;longest common subsequence problem;longest alternating subsequence	Theory	15.495600413607825	-47.982189778859954	142716
6aad1530809f48d5f304f6b74e6248e61c5c66e1	towards meaningful physics from generative models		In several physical systems, important properties characterizing the system itself are theoretically related with specific degrees of freedom. Although standard Monte Carlo simulations provide an effective tool to accurately reconstruct the physical configurations of the system, they are unable to isolate the different contributions corresponding to different degrees of freedom. Here we show that unsupervised deep learning can become a valid support to MC simulation, coupling useful insights in the phases detection task with good reconstruction performance. As a testbed we consider the 2D XY model, showing that a deep neural network based on variational autoencoders can detect the continuous Kosterlitz-Thouless (KT) transitions, and that, if endowed with the appropriate constrains, they generate configurations with meaningful physical content.	algorithm;artificial neural network;classical xy model;computational physics;deep learning;generative model;kosterlitz–thouless transition;monte carlo method;simulation;testbed;variational principle	Marco Cristoforetti;Giuseppe Jurman;Andrea I. Nardelli;Cesare Furlanello	2017	CoRR		computational physics;particle physics;physics;generative grammar;monte carlo method;classical xy model;deep learning;artificial neural network;artificial intelligence;testbed;physical system	ML	23.287288714079505	-48.956927565406886	142802
d3fa0254ea57ab890ac53fa85d6ea8883fd4aadb	gradient correlation: are ensemble classifiers more robust against evasion attacks in practical settings?		Pattern recognition is an essential part of modern security systems for malware detection, intrusion detection, and spam filtering. Conventional classifiers widely used in these applications are found vulnerable themselves to adversarial machine learning attacks. Existing studies argued that ensemble classifiers are more robust than a single classifier under evasion attacks due to more uniform weights produced on the basis of training data. In this paper, we investigate the problem in a more practical setting where attackers do not know the classifier details. Instead, attackers may acquire only a portion of the labeled data or a replacement dataset for learning the target decision boundary. In this case, we show that ensemble classifiers are not necessarily more robust under a least effort attack based on gradient descent. Our experiments are conducted with both linear and kernel SVMs on real datasets for spam filtering and malware detection.	evasion (network security);gradient	Fuyong Zhang;Yi Wang;Hua Wang	2018		10.1007/978-3-030-02922-7_7	data mining;support vector machine;labeled data;filter (signal processing);malware;computer science;adversarial machine learning;gradient descent;decision boundary;intrusion detection system;artificial intelligence;pattern recognition	Security	19.126516539201162	-51.11522258906783	143190
12f0a5268975d535761d99819856992f7021780b	transcend: detecting concept drift in malware classification models		Building machine learning models of malware behavior is widely accepted as a panacea towards effective malware classification. A crucial requirement for building sustainable learning models, though, is to train on a wide variety of malware samples. Unfortunately, malware evolves rapidly and it thus becomes hard—if not impossible—to generalize learning models to reflect future, previously-unseen behaviors. Consequently, most malware classifiers become unsustainable in the long run, becoming rapidly antiquated as malware continues to evolve. In this work, we propose Transcend, a framework to identify aging classification models in vivo during deployment, much before the machine learning model’s performance starts to degrade. This is a significant departure from conventional approaches that retrain aging models retrospectively when poor performance is observed. Our approach uses a statistical comparison of samples seen during deployment with those used to train the model, thereby building metrics for prediction quality. We show how Transcend can be used to identify concept drift based on two separate case studies on Android and Windows malware, raising a red flag before the model starts making consistently poor decisions due to out-of-date training.	algorithm;analysis of algorithms;android;barnsley fern;bitwise operation;bridging (networking);concept drift;machine learning;malware;microsoft windows;multiclass classification;sensor;software deployment;statistical classification;video-in video-out	Roberto Jordaney;Kumar Sharad;Santanu Kumar Dash;Zhi Wang;Davide Papini;Ilia Nouretdinov;Lorenzo Cavallaro	2017			concept drift;computer security;software deployment;computer science;malware;android (operating system);panacea (medicine)	ML	14.40151686229585	-46.430061052913956	143341
75fcaff84b0eaa609945a7996822a34da18dacf6	a relief-based modality weighting approach for multimodal information retrieval	relief;information retrieval;computational complexity;feature weighting;feature selection;information fusion;multimodal information fusion	Despite the extensive number of studies for multimodal information fusion, the issue of determining the optimal modalities has not been adequately addressed yet. In this study, a RELIEF-based multimodal feature selection approach (RELIEF-RDR) is proposed. The original RELIEF algorithm is extended for weaknesses in three major issues; multi-labeled data, noise and class-specific feature selection. To overcome these weaknesses, discrimination based weighting mechanism of RELIEF is supported with two additional concepts; representation and reliability capabilities of features, without an increase in computational complexity. These capabilities of features are exploited by using the statistics on dissimilarities of training instances. The experiments conducted on TRECVID 2007 dataset validated the superiority of RELIEF-RDR over RELIEF.	algorithm;computational complexity theory;experiment;feature selection;information retrieval;modality (human–computer interaction);multimodal interaction;restrictive design rules	Turgay Yilmaz;Elvan Gulen;Adnan Yazici;Masaru Kitsuregawa	2012		10.1145/2324796.2324858	computer science;machine learning;pattern recognition;data mining;computational complexity theory;feature selection;information retrieval	AI	12.145294337244877	-44.07826627661859	143387
269e3556271a3a4d239516e980b2edf96b75b895	convolutional neural networks for steganalysis via transfer learning		Recently, a large number of studies have shown that Convolutional Neural Networks are effective for learning features automatically for steganalysis. This paper uses the transfer learning method to...	convolutional neural network;steganalysis	Juan Tian;Yingxiang Li	2019	IJPRAI	10.1142/S0218001419590067	transfer of learning;convolutional neural network;machine learning;mathematics;artificial intelligence;steganalysis	NLP	23.115093071322168	-51.54089880813027	143475
5255385b3422655e6219a008276bd633b962b388	design of a fuzzy min-max hyperbox classifier using a supervised learning method	supervised learning	ABSTRACT This investigation proposes a fuzzy min-max hyperbox classifier to solve M-class classification problems. In the proposed fuzzy min-max hyperbox classifier, a supervised learning method is implemented to generate min-max hyperboxes for the training patterns in each class so that the generated fuzzy min-max hyperbox classifier has a perfect classification rate in the training set. However, the 100% correct classification of the training set generally leads to overfitting. In order to improve this drawback, a procedure is employed to decrease the complexity of the input decision boundaries so that the generated fuzzy hyperbox classifier has a good generalization performance. Finally, two benchmark data sets are considered to demonstrate the good performance of the proposed approach for solving this classification problem.	naive bayes classifier;supervised learning	Chia-Chong Chen	2006	Cybernetics and Systems	10.1080/01969720600626337	fuzzy classification;computer science;machine learning;pattern recognition;data mining;mathematics	NLP	13.020585239129808	-40.15324025485443	143491
c452da900aa4fcdf5e8cf908c9cb9f26955f9c59	a novel finger vein verification system based on two-stream convolutional network learning		Abstract Convolutional neural networks have been proven to have strong feature representation ability; however, they often require large training samples and high computation that are infeasible for real-time finger vein verification. To address this limitation, we propose a lightweight deep-learning framework for finger vein verification. First, we designed a lightweight two-channel network that has only three convolution layers for finger vein verification. Then, we extracted the mini-ROI from the original image to better solve the displacement problem based on the evaluation of the two-channel network. Finally, we present a two-stream network to integrate the original image and the mini-ROI that achieves results superior to the current state of the art on both the MMCBNU and SDUMLA databases.		Yuxun Fang;Qiuxia Wu;Wenxiong Kang	2018	Neurocomputing	10.1016/j.neucom.2018.02.042	convolutional neural network;machine learning;artificial intelligence;computation;convolution;pattern recognition;mathematics	AI	23.381384003716068	-51.84186896437995	143674
516d34c32974766bd14432d3aa2723dd4e7a8d62	a unified feature disentangler for multi-domain image translation and manipulation		We present a novel and unified deep learning framework which is capable of learning domain-invariant representation from data across multiple domains. Realized by adversarial training with additional ability to exploit domain-specific information, the proposed network is able to perform continuous cross-domain image translation and manipulation, and produces desirable output images accordingly. In addition, the resulting feature representation exhibits superior performance of unsupervised domain adaptation, which also verifies the effectiveness of the proposed model in learning disentangled features for describing cross-domain data.	deep learning;domain adaptation;domain-specific language;network architecture;unsupervised learning	Alexander H. Liu;Yen-Cheng Liu;Yu-Ying Yeh;Yu-Chiang Frank Wang	2018			computer science;machine learning;deep learning;domain adaptation;artificial intelligence;pattern recognition;exploit;image translation	ML	24.349851364970416	-49.36944582068173	143719
6dc75daf6416a90eb71c453a96579501bf472e20	a novel swarm based feature selection algorithm in multifunction myoelectric control	myoelectric control;journal article;particle swarm optimization;pattern recognition;mutual information;feature selection	Accurate and computationally efficient myoelectric control strategies have been the focus of a great deal of research in recent years. Although many attempts exist in literature to develop such strategies, deficiencies still exist. One of the major challenges in myoelectric control is finding an optimal feature set that can best discriminate between classes. However, since the myoelectric signal is recorded using multi channels, the feature vector size can become very large. Hence a dimensionality reduction method is needed to identify an informative, yet small size feature set. This paper presents a new feature selection method based on modifying the Particle Swarm Optimization (PSO) algorithm with the inclusion of Mutual Information (MI) measure. The new method, called BPSOMI, is a mixture of filter and wrapper approaches of feature selection. In order to prove its efficiency, the proposed method is tested against other dimensionality reduction techniques proving powerful classification accuracy.	algorithmic efficiency;dimensionality reduction;feature selection;feature vector;information theory;multi-function printer;mutual information;particle swarm optimization;selection algorithm;swarm intelligence	Rami N. Khushaba;Akram AlSukker;Ahmed Al-Ani;Adel Al-Jumaily;Albert Y. Zomaya	2009	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-2009-0426	mathematical optimization;computer science;machine learning;pattern recognition;data mining;mutual information;particle swarm optimization;feature selection;feature;statistics;dimensionality reduction	AI	10.658783795432818	-44.59756608492032	143738
58d548c948edd02d4a555382f813934a8ef0d8fa	unsupervised feature learning classification using an extreme learning machine	unsupervised learning;optimisation;image classification;parallel programming;iterative methods;feature extraction;unsupervised learning feature extraction image classification iterative methods optimisation parallel programming;parallel programming unsupervised feature learning classification extreme learning machine ufl elm feature extraction handcrafting iterated optimization classifier training dataset;training neurons feature extraction vectors testing graphics processing units neural networks	This paper presents a new approach, which we call UFL-ELM, to classification using both unsupervised and supervised learning. Unlike traditional approaches in which features are extracted, hand-crafted, and then trained using time-consuming, iterated optimization, this proposed method leverages unsupervised feature learning to learn features from the data themselves and then train the classifier using an extreme learning machine to reach the analytic solution. The result is therefore widely and quickly applied to universal data. Experiments on a large dataset of images confirm the ease of use and speed of training of this unsupervised feature learning approach. Furthermore, the paper discusses how to speed up training, using massively parallel programming.	elm;encoder;experiment;feature learning;iteration;mathematical optimization;parallel computing;supervised learning;unsupervised learning;usability	Dao Lam;Donald C. Wunsch	2013	The 2013 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2013.6706977	semi-supervised learning;unsupervised learning;feature learning;contextual image classification;wake-sleep algorithm;feature extraction;computer science;machine learning;linear classifier;pattern recognition;data mining;iterative method;stability;competitive learning;k-nearest neighbors algorithm;feature;artificial neural network;dimensionality reduction	AI	20.237825267515927	-38.33681218514329	143915
26f9a81c280cebf10d0d486744b35b194c12df59	towards making unlabeled data never hurt	unlabeled data;biological patents;reliability;biomedical journals;s3vms;pedestrian safety;support vector machines;text mining;poison control;europe pubmed central;injury prevention;citation search;prediction algorithms;safety literature;traffic safety;injury control;citation networks;semi supervised learning;home safety;injury research;safety abstracts;human factors;research articles;abstracts;open access;occupational safety;safety;safe;life sciences;clinical guidelines;s4vms;safety research;accident prevention;particle separators;violence prevention;optimization;bicycle safety;full text;poisoning prevention;falls;ergonomics;rest apis;suicide prevention;orcids;semisupervised learning;europe pmc;biomedical research;data models;bioinformatics;literature search	It is usually expected that learning performance can be improved by exploiting unlabeled data, particularly when the number of labeled data is limited. However, it has been reported that, in some cases existing semi-supervised learning approaches perform even worse than supervised ones which only use labeled data. For this reason, it is desirable to develop safe semi-supervised learning approaches that will not significantly reduce learning performance when unlabeled data are used. This paper focuses on improving the safeness of semi-supervised support vector machines (S3VMs). First, the S3VM-us approach is proposed. It employs a conservative strategy and uses only the unlabeled instances that are very likely to be helpful, while avoiding the use of highly risky ones. This approach improves safeness but its performance improvement using unlabeled data is often much smaller than S3VMs. In order to develop a safe and well-performing approach, we examine the fundamental assumption of S3VMs, i.e., low-density separation. Based on the observation that multiple good candidate low-density separators may be identified from training data, safe semi-supervised support vector machines (S4VMs) are here proposed. This approach uses multiple low-density separators to approximate the ground-truth decision boundary and maximizes the improvement in performance of inductive SVMs for any candidate separator. Under the assumption employed by S3VMs, it is here shown that S4VMs are provably safe and that the performance improvement using unlabeled data can be maximized. An out-of-sample extension of S4VMs is also presented. This extension allows S4VMs to make predictions on unseen instances. Our empirical study on a broad range of data shows that the overall performance of S4VMs is highly competitive with S3VMs, whereas in contrast to S3VMs which hurt performance significantly in many cases, S4VMs rarely perform worse than inductive SVMs.	approximation algorithm;decision boundary;ensemble learning;forty nine;generalization (psychology);graph - visual representation;learning disorders;mandibular right second molar tooth;semi-supervised learning;semiconductor industry;separators;small;supervised learning;support vector machine;support vector machine;tracer;benefit	Yu-Feng Li;Zhi-Hua Zhou	2011	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2014.2299812	semi-supervised learning;data modeling;support vector machine;simulation;prediction;computer science;suicide prevention;human factors and ergonomics;data science;injury prevention;machine learning;data mining;reliability;statistics	ML	16.562877487066967	-40.91725251202837	143973
0e6a859aedd989adf988be26366a4a1ba3b5b07f	hyperspectral image segmentation with markov random fields and a convolutional neural network		This paper presents a new supervised segmentation algorithm for hyperspectral image (HSI) data which integrates both spectral and spatial information in a probabilistic framework. A convolutional neural network (CNN) is first used to learn the posterior class distributions using a patch-wise training strategy to better utilize the spatial information. Then, the spatial information is further considered by using a Markov random field prior, which encourages the neighboring pixels to have the same labels. Finally, a maximum a posteriori segmentation model is efficiently computed by the α-expansion min-cut-based optimization algorithm. The proposed segmentation approach achieves state-of-the-art performance on one synthetic dataset and two benchmark HSI datasets in a number of experimental settings.	algorithm;artificial neural network;benchmark (computing);convolutional neural network;cut (graph theory);dhrystone;horizontal situation indicator;image segmentation;markov chain;markov random field;mathematical optimization;minimum cut;pixel;unified framework	Xiangyong Cao;Feng Zhou;Lin Xu;Deyu Meng;Zongben Xu;John Paisley	2017	CoRR		machine learning;pattern recognition;image segmentation;scale-space segmentation	Vision	24.27181116737923	-47.119108702085114	144120
0aadfde22bac391ee9a8866c20529dda2f98bfe4	learning better data representation using inference-driven metric learning	learning algorithm;data representation	Distance metric learning algorithms [1, 2] learn the Mahalanobis distance, dA(xi, x j), between instances xi and x j. dA(xi, x j) = (xi − x j)A(xi − x j) where A is s d× d positive-definite matrix, and d is the data dimension. This matrix can be written as A = P>P, where P is another matrix of size d × d. dA(xi, x j) = (xi − x j)A(xi − x j) = (xi − x j)PP(xi − x j) = (Pxi − Px j)(Pxi − Px j) = dEuclidean(Pxi, Px j) •Hence, computing Mahalanobis distance in the original space is equivalent to computing Euclidean distance in the space induced by projection matrix P. •Learning can now be performed in the data representation induced by P. Distance Metric Learning Algorithms	algorithm;data (computing);euclidean distance;machine learning;p (complexity)	Paramveer S. Dhillon;Partha Pratim Talukdar;Koby Crammer	2010			semi-supervised learning;unsupervised learning;multi-task learning;instance-based learning;wake-sleep algorithm;computer science;online machine learning;machine learning;pattern recognition;data mining;external data representation;learning classifier system;supervised learning;stability;competitive learning;active learning;generalization error	ML	23.389408363853725	-41.35311203920013	144191
0c91d5305ad34814b631d4a642bb0535a2e066ea	feature selection based on mutual information	well logging feature selection feedforward greedy algorithms hydrocarbon reservoirs learning artificial intelligence;measurement error problems machine learning prediction petroleum reservoir properties machine learning models support vector machine svm artificial neural networks ann reservoir property prediction uncertain data well log dataset data interpretation greedy feedforward method mutual information criterion feature subset selection;t technology general;yttrium uncertainty mutual information reservoirs support vector machines predictive models prediction algorithms;feature selection machine learning mutual information	The application of machine learning models such as support vector machine (SVM) and artificial neural networks (ANN) in predicting reservoir properties has been effective in the recent years when compared with the traditional empirical methods. Despite that the machine learning models suffer a lot in the faces of uncertain data which is common characteristics of well log dataset. The reason for uncertainty in well log dataset includes a missing scale, data interpretation and measurement error problems. Feature Selection aimed at selecting feature subset that is relevant to the predicting property. In this paper a feature selection based on mutual information criterion is proposed, the strong point of this method relies on the choice of threshold based on statistically sound criterion for the typical greedy feedforward method of feature selection. Experimental results indicate that the proposed method is capable of improving the performance of the machine learning models in terms of prediction accuracy and reduction in training time.	artificial neural network;feature selection;feedforward neural network;greedy algorithm;machine learning;mutual information;support vector machine;uncertain data	Muhammad Aliyu Sulaiman;Jane Labadin	2015	2015 9th International Conference on IT in Asia (CITA)	10.1109/CITA.2015.7349827	feature learning;feature vector;computer science;online machine learning;machine learning;pattern recognition;data mining;computational learning theory;feature selection;active learning;feature;dimensionality reduction	ML	12.900527211343565	-39.12083339783009	144344
431f46b08ec9f3983baeed4ea1909fa15cd3685b	effectiveness of hierarchical softmax in large scale classification tasks		Typically, Softmax is used in the final layer of a neural network to get a probability distribution for output classes. But the main problem with Softmax is that it is computationally expensive for large scale data sets with large number of possible outputs. To approximate class probability efficiently on such large scale data sets we can use Hierarchical Softmax. LSHTC datasets were used to study the performance of the Hierarchical Softmax. LSHTC datasets have large number of categories. In this paper we evaluate and report the performance of normal Softmax Vs Hierarchical Softmax on LSHTC datasets. This evaluation used macro f1 score as a performance measure. The observation was that the performance of Hierarchical Softmax degrades as the number of classes increase.		Abdul Arfat Mohammed;Venkatesh Umaashankar	2018	2018 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2018.8554637	probability distribution;computer science;task analysis;machine learning;f1 score;binary tree;artificial neural network;macro;artificial intelligence;data set;softmax function	AI	16.10173544893539	-44.045846609955376	144513
e0c76f2ef4399e154e4b9dc2fa6147313fe74398	fuzzy-rough instance selection combined with effective classifiers in credit scoring	fuzzy-rough instance selection;hybrid classifier;credit scoring	The wrong clusters number or poor starting points of each cluster have negative influence on the classification accuracy in the hybrid classifier based credit scoring system. The paper represents a new hybrid classifier based on fuzzy-rough instance selection, which have the same ability as clustering algorithms, but it can eliminate isolated and inconsistent instances without the need of determining clusters number and starting points of each cluster. The unrepresentative instances that cause conflicts with other instances are completely determined by the fuzzy-rough positive region which is only related to intrinsic data structure of datasets. By removing unrepresentative instances, both the training data quality and classifier training time can be improved. To prevent eliminating more instances than strictly necessary, the k-nearest neighbor algorithm is adopted to check the eliminated instances, and the instance whose predicted class is the same with predefined class is added back. SVM classifier with three different kernel functions are applied to the reduced dataset. The experimental results show that the proposed hybrid classifier has better classification accuracy on two real world datasets.	artificial neural network;cluster analysis;data quality;data structure;k-nearest neighbors algorithm;linear discriminant analysis;logistic regression;statistical classification;support vector machine;test set;while	ZhanFeng Liu;Su Pan	2017	Neural Processing Letters	10.1007/s11063-017-9641-3	support vector machine;kernel (statistics);machine learning;artificial intelligence;fuzzy logic;pattern recognition;cluster analysis;mathematics;classifier (linguistics);instance selection;data structure;margin classifier	AI	13.371596231373026	-40.241196244153976	144572
4f3484a1b08b332479f0cc0197528e9007292a90	stream-based active unusual event detection	unsupervised learning;critical point;active learning;event detection;on the fly;public space	We present a new active learning approach to incorporate human feedback for on-line unusual event detection. In contrast to most existing unsupervised methods that perform passive mining for unusual events, our approach automatically requests supervision for critical points to resolve ambiguities of interest, leading to more robust and accurate detection on subtle unusual events. The active learning strategy is formulated as a stream-based solution, i.e. it makes decision on-the-fly on whether to query for labels. It adaptively combines multiple active learning criteria to achieve (i) quick discovery of unknown event classes and (ii) refinement of classification boundary. Experimental results on busy public space videos show that with minimal human supervision, our approach outperforms existing supervised and unsupervised learning strategies in identifying unusual events. In addition, better performance is achieved by using adaptive multi-criteria approach compared to existing single criterion and multi-criteria active learning strategies.	active learning (machine learning);akaike information criterion;common criteria;decision boundary;online and offline;refinement (computing);supervised learning;unsupervised learning	Chen Change Loy;Tao Xiang;Shaogang Gong	2010		10.1007/978-3-642-19315-6_13	semi-supervised learning;unsupervised learning;computer science;machine learning;pattern recognition;data mining;active learning;critical point;active learning	ML	17.157658541705977	-39.71214922059906	144699
2f7e1eb92c254244c9a0e5a51ad4643b6b6460cb	domain generalization based on transfer component analysis	part of book or chapter of book	This paper investigates domain generalization: How to use knowledge acquired from related domains and apply it to new domains? Transfer Component Analysis (TCA) learns a shared subspace by minimizing the dissimilarities across domains, while maximally preserving the data variance. We propose Multi-TCA, an extension of TCA to multiple domains as well as Multi-SSTCA, which is an extension of TCA for semi-supervised learning. In addition to the original application of TCA for domain adaptation problems, we show that Multi-TCA can also be applied for domain generalization. Multi-TCA and Multi-SSTCA are evaluated on two publicly available datasets with the tasks of landmine detection and Parkinson telemonitoring. Experimental results demonstrate that Multi-TCA can improve predictive performance on previously unseen domains.		Thomas Grubinger;Adriana Birlutiu;Holger Schöner;Thomas Natschläger;Tom Heskes	2015		10.1007/978-3-319-19258-1_28	computer science;artificial intelligence;cognitive science	Vision	21.911389003203034	-45.045665162060665	144702
563962805514512000a2238c582b3dc31f12e931	a cost-sensitive multi-criteria quadratic programming model for imbalanced data		Multiple Criteria Quadratic Programming (MCQP), a mathematical programming-based classification method, has been developed recently and proved to be effective and scalable. However, its performance degraded when learning from imbalanced data. This paper proposes a cost-sensitive MCQP (CS-MCQP) model by introducing the cost of misclassifications to the MCQP model. The empirical tests were designed to compare the proposed model with MCQP and a selection of classifiers on 26 imbalanced datasets from the UCI repositories. The results indicate that the CS-MCQP model not only performs better than the optimization-based models (MCQP and SVM), but also outperforms the selected classifiers, ensemble, preprocessing techniques and hybrid methods on imbalanced datasets in terms of AUC and GeoMean measures. To validate the results statistically, Student’s t test and Wilcoxon signed-rank test were conducted and show that the superiority of CS-MCQP is statistically significant with significance level 0.05. In addition, we analyze the effect of noisy, small disjunct and overlapping data properties on the proposed model and conclude that the CS-MCQP model achieves better performance on imbalanced data with overlapping feature than noisy and small disjunct data.	programming model;quadratic programming	Xiangrui Chao;Yi Peng	2018	JORS	10.1057/s41274-017-0233-4	support vector machine;quadratic programming;computer science;scalability;preprocessor;wilcoxon signed-rank test;machine learning;artificial intelligence;pattern recognition;statistical significance	NLP	12.034100821835692	-41.81401114126512	144732
ac5c2f71b9dc9e715d2f82525c7b8d73f2ba0504	learning deep representations by distributed random samplings		In this paper, we propose an extremely simple deep model for the unsupervised nonlinear dimensionality reduction – deep distributed random samplings. First, its network structure is novel: each layer of the network is a group of mutually independent k-centers clusterings. Second, its learning method is extremely simple: the k centers of each clustering are only k randomly selected examples from the training data; for small-scale data sets, the k centers are further randomly reconstructed by a simple cyclic-shift operation. Experimental results on nonlinear dimensionality reduction show that the proposed method can learn abstract representations on both large-scale and small-scale problems, and meanwhile is much faster than deep neural networks on large-scale problems.	artificial neural network;circular shift;cluster analysis;deep learning;nonlinear dimensionality reduction;nonlinear system;randomness	Xiao-Lei Zhang	2013	CoRR		artificial intelligence;machine learning;data mining;mathematics	ML	21.36343025926538	-45.237290140628396	144868
0420a3ca207083762930705596d130949d6bef95	meta-learning for escherichia coli bacteria patterns classification		In machine learning area, there has been a great interest during the past decade to the theory of combining machine learning algorithms. The approaches proposed and implemented become increasingly interesting at the moment when many challenging real-world problems remain difficult to solve, especially those characterized by imbalanced data. Learning with imbalanced datasets is problematic, since the uneven distribution of data influences the behavior of the majority of machine learning algorithms, which often lead to poor performance. It is within this type of data that our study is placed. In this paper, we investigate a meta-learning approach for classifying proteins into their various cellular locations based on their amino acid sequences, A meta-learner system based on k-Nearest Neighbors (kNN) algorithm as base-classifier, since it has shown good performance in this context as individual classifier and DECORATE as meta-classifier using cross-validation tests for classifying Escherichia Coli bacteria proteins from the amino acid sequence information is evaluated. The paper reports also a comparison against a Decision Tree induction as baseclassifier. The experimental results show that the k-NN-based meta-learning model is more efficient than the Decision Tree-based model and the individual k-NN classifier.	cross-validation (statistics);decision tree;k-nearest neighbors algorithm;machine learning;peptide sequence	Hafida Bouziane;Belhadri Messabih;Abdallah Chouarfia	2012			computer science;machine learning;pattern recognition;data mining	ML	12.573674039342114	-42.83545927480362	144910
8188ad3c08567e1f34b61f3f2a83e5644f8744c7	multiple boosting in the ant colony decision forest meta-classifier	ant colony optimization;bagging;boosting;random forest;meta ensemble;ant colony decision forest	The idea of ensemble methodology is to combine multiple predictive models in order to achieve a better prediction performance. In this task we analyze the self-adaptive methods for improving the performance of Ant Colony Decision Tree and Forest algorithms. Our goal is to present and compare new metaensemble approaches based on Ant Colony Optimization. The proposed meta-classifiers (consisting of homogeneous classifiers) can be characterized by the self-adaptability or the good accommodation with the analyzed data sets and offer appropriate classification accuracy. In this article we provide an overview of ensemble methods in classification tasks and concentrate on the different methodologies, such as Bagging, Boosting and Random Forest. We present all important types of ensemble methods including Boosting and Bagging in context of distributed approach, where agent-ants create better solutions employing adaptive mechanisms. Self adaptive, combining methods and modeling appropriate issues, such as ensembles presented here are discussed in context of the quality of the results. Smaller trees in decision forest without loss of accuracy are achieved during the analysis of different data sets. 2014 Elsevier B.V. All rights reserved.	algorithm;ant colony optimization algorithms;boosting (machine learning);bootstrap aggregating;decision tree;ensemble learning;predictive modelling;random forest	Jan Kozak;Urszula Boryczka	2015	Knowl.-Based Syst.	10.1016/j.knosys.2014.11.027	random forest;ant colony optimization algorithms;bootstrap aggregating;computer science;machine learning;pattern recognition;data mining;boosting	AI	12.053630597627594	-41.058367451722226	144967
dd55b3817a2abef1b68c1a369977ab5c35ae63d1	a comparative evaluation of medium- and large-scale feature selectors for pattern classifiers	feature selection;image processing	Needs of feature selection in medium and large problems increases in many fields including medical and image processing fields. Previous comparative studies of feature selection algorithms are not satisfactory in problem size and in criterion function. In addition, no way has not shown to compare algorithms with different objectives. In this study, we propose a unified way to compare a large variety of algorithms. Our results show that the sequential floating algorithms promises for up to medium problems and genetic algorithms for medium and large problems.	analysis of algorithms;feature selection;genetic algorithm;image processing;loss function	Mineichi Kudo;Jack Sklansky	1998	Kybernetika			AI	12.105368824558484	-43.98749143377125	145077
07a2dbbb888783f3138297efedb99a55c4b2716c	on d-asymptotics for high-dimensional discriminant analysis with different variance-covariance matrices	d asymptotics;misclassification rate;discriminant analysis;high dimensional data			Takanori Ayano;Joe Suzuki	2012	IEICE Transactions	10.1587/transinf.E95.D.3106	econometrics;kernel fisher discriminant analysis;computer science;machine learning;pattern recognition;optimal discriminant analysis;discriminant function analysis;linear discriminant analysis;multiple discriminant analysis;statistics;clustering high-dimensional data	ML	23.52387026980545	-39.40746652353414	145188
4ca8b9cba6a41e65100e338cbafb9e2562c0840c	cascading randomized weighted majority: a new online ensemble learning algorithm		With the increasing volume of data in the world, the best approach for learning from this data is to exploit an online learning algorithm. Online ensemble methods are online algorithms which take advantage of an ensemble of classifiers to predict labels of data. Prediction with expert advice is a well-studied problem in the online ensemble learning literature. The Weighted Majority algorithm and the randomized weighted majority (RWM) are the most well-known solutions to this problem, aiming to converge to the best expert. Since among some expert, The best one does not necessarily have the minimum error in all regions of data space, defining specific regions and converging to the best expert in each of these regions will lead to a better result. In this paper, we aim to resolve this defect of RWM algorithms by proposing a novel online ensemble algorithm to the problem of prediction with expert advice. We propose a cascading version of RWM to achieve not only better experimental results but also a better error bound for sufficiently large datasets.	advice (programming);converge;dataspaces;ensemble learning;experiment;machine learning;online algorithm;randomized algorithm;software bug;time complexity;weighted majority algorithm	Mohammadzaman Zamani;Hamid Beigy;Amirreza Shaban	2016	Intell. Data Anal.	10.3233/IDA-160836	weighted majority algorithm;computer science;randomized weighted majority algorithm;machine learning;pattern recognition;data mining;ensemble learning;statistics	ML	15.26924336267303	-40.0882563498166	145270
d4abe236645e72e3fed16fd10125308f64387ff5	stacking and rotation-based technique for machine learning classification with data reduction		The paper focuses on using stacking and rotation-based technique to improve performance and generalization ability of the machine learning classification with data reduction. The aim of data reduction technique is decreasing the quantity of information required to learn a high quality classifiers, especially when the data are huge. The paper shows that merging both stacking and rotation-based ensemble techniques with machine classification based on data reduction may bring additional benefits with respect to the accuracy of the classification process. The finding that has been confirmed by computational experiments. The paper includes the description of the approach and the discussion of the computational experiment results.	computation;display resolution;experiment;focus stacking;machine learning	Ireneusz Czarnowski;Piotr Jedrzejowicz	2017	2017 IEEE International Conference on INnovations in Intelligent SysTems and Applications (INISTA)	10.1109/INISTA.2017.8001132	stacking;merge (version control);cluster analysis;data reduction;principal component analysis;feature extraction;artificial intelligence;machine learning;statistical classification;computational learning theory;pattern recognition;computer science	HPC	14.122555310072086	-42.58225359649616	145391
561cb7d664ef1a690bed856900b37100927bba70	when does cotraining work in real data?	single view;splitting method;annealing;supervised learning;training;labeled training set semisupervised learning two view cotraining view verification method view splitting method;satisfiability;glass;accuracy;training accuracy entropy book reviews sonar annealing glass;independence assumption;two view cotraining;cotraining;view splitting method;book reviews;entropy;single view semisupervised learning cotraining sufficiency assumption independence assumption view splitting;labeled training set;view splitting;learning artificial intelligence;view verification method;semisupervised learning;sufficiency assumption;sonar	Cotraining, a paradigm of semisupervised learning, is promised to alleviate effectively the shortage of labeled examples in supervised learning. The standard two-view cotraining requires the data set to be described by two views of features, and previous studies have shown that cotraining works well if the two views satisfy the sufficiency and independence assumptions. In practice, however, these two assumptions are often not known or ensured (even when the two views are given). More commonly, most supervised data sets are described by one set of attributes (one view). Thus, they need be split into two views in order to apply the standard two-view cotraining. In this paper, we first propose a novel approach to empirically verify the two assumptions of cotraining given two views. Then, we design several methods to split single view data sets into two views, in order to make cotraining work reliably well. Our empirical results show that, given a whole or a large labeled training set, our view verification and splitting methods are quite effective. Unfortunately, cotraining is called for precisely when the labeled training set is small. However, given small labeled training sets, we show that the two cotraining assumptions are difficult to verify, and view splitting is unreliable. Our conclusions for cotraining's effectiveness are mixed. If two views are given, and known to satisfy the two assumptions, cotraining works well. Otherwise, based on small labeled training sets, verifying the assumptions or splitting single view into two views are unreliable; thus, it is uncertain whether the standard cotraining would work or not.	algorithm;co-training;heuristic;programming paradigm;semi-supervised learning;supervised learning;test set;verification and validation	Jun Du;Charles X. Ling;Zhi-Hua Zhou	2011	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2010.158	entropy;annealing;computer science;artificial intelligence;machine learning;data mining;database;mathematics;accuracy and precision;glass;statistical assumption;supervised learning;algorithm;sonar;statistics;satisfiability	Vision	16.24111030000033	-38.896358217821074	145634
3b3024ca511dcdc114bcc6f02acd87c46224e7e6	an unsupervised, fast correlation-based filter for feature selection for data clustering		Feature selection is an important method to provide both efficiency and effectiveness for high-dimension data clustering. However, most feature selection methods require prior knowledge such as class-label information to train the clustering module, where its performance depends on training data and types of learning machine. This paper presents a feature selection algorithm that does not require supervised feature assessment. We analyze relevance and redundancy among features and effectiveness to each target class to build a correlation-based filter. Compared to feature sets selected by existing methods, the experimental results show that performance of a feature set selected by the proposed method is comparably equal and better when it is tested on the RCV1v2 corpus and Isolet data set, respectively. However, our technique is simpler and faster and it is independent to types of learning machine.	cluster analysis;computer cluster;feature selection;unsupervised learning	Part Pramokchon;Punpiti Piamsa-nga	2013		10.1007/978-981-4585-18-7_10	correlation clustering;canopy clustering algorithm;pattern recognition;cure data clustering algorithm;cluster analysis	ML	12.985910821413825	-41.45975794614858	145654
4441db450e35df8ad5a91be8c3d09f7b5093e1cb	gaussian mixture model using semisupervised learning for probabilistic fault diagnosis under new data categories	unsupervised learning;training;training data;gaussian mixture model;probabilistic logic;semisupervised learning;fault diagnosis	Fault diagnosis has played a vital role in industry to prevent operation hazards and failures. To overcome the limitation of conventional diagnosis approaches, which misclassify new types of faults into existing categories from training, a novel probabilistic diagnosis framework will be proposed in this paper for effective detection on new data categories. Gaussian mixture model (GMM) is applied for the pattern recognition, while its training procedure is improved from conventional unsupervised learning to novel semisupervised learning. Even with unlabeled training data, component number in our GMM can be autoselected instead of predetermined. For online testing, the probabilistic classification results from GMM’s soft assignment assist to improve overall diagnosis framework, which is able to first detect whether new types of faults occur and further categorize them in detail via the GMM update. The effectiveness of our fault diagnosis framework is testified on an industrial fault simulator of rotary machine and the partial discharge measurement of various high-voltage electronic equipment components. Compared with existing approaches, our probabilistic diagnosis framework is able to achieve an average diagnosis accuracy of 97.9% without new data categories and it can also classify new data categories with diagnosis accuracy of at least 86.3% if occurred.	algorithm;categorization;discharger;fault simulator;google map maker;mixture model;pattern recognition;probabilistic turing machine;rotary system;semi-supervised learning;simulation;unsupervised learning	Heng-Chao Yan;Junhong Zhou;Chee Khiang Pang	2017	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2017.2654552	unsupervised learning;training set;computer science;machine learning;pattern recognition;mixture model;data mining;probabilistic logic	AI	14.284140341675528	-44.660700408990394	145789
65688d7a888d975c64878cf9e0e31a8182f05f64	dual training and dual prediction for polarity classification		Bag-of-words (BOW) is now the most popular way to model text in machine learning based sentiment classification. However, the performance of such approach sometimes remains rather limited due to some fundamental deficiencies of the BOW model. In this paper, we focus on the polarity shift problem, and propose a novel approach, called dual training and dual prediction (DTDP), to address it. The basic idea of DTDP is to first generate artificial samples that are polarity-opposite to the original samples by polarity reversion, and then leverage both the original and opposite samples for (dual) training and (dual) prediction. Experimental results on four datasets demonstrate the effectiveness of the proposed approach for polarity classification.	algorithm;bag-of-words model in computer vision;machine learning;reversion (software development)	Rui Xia;Tao Wang;Xuelei Hu;Shoushan Li;Chengqing Zong	2013			artificial intelligence;machine learning	ML	19.22739659300552	-47.981243394735976	145825
86caac65e188a45c5d82fb4cb560b7b471c736ae	resampling strategies for regression	software engineering;artificial intelligence;business and finance	Several real world prediction problems involve forecasting rare values of a target variable. When this variable is nominal, we have a problem of class imbalance that was thoroughly studied within machine learning. For regression tasks, where the target variable is continuous, few works exist addressing this type of problem. Still, important applications involve forecasting rare extreme values of a continuous target variable. This paper describes a contribution to this type of tasks. Namely, we propose to address such tasks by resampling approaches that change the distribution of the given data set to decrease the problem of imbalance between the rare target cases and the most frequent ones. We present two modifications of well-known resampling strategies for classification tasks: the under-sampling and the synthetic minority over-sampling technique (SMOTE) methods. These modifications allow the use of these strategies on regression tasks where the goal is to forecast rare extreme values of the target variable. In an extensive set of experiments, we provide empirical evidence for the superiority of our proposals for these particular regression tasks. The proposed resampling methods can be used with any existing regression algorithm, which means that they are general tools for addressing problems of forecasting rare extreme values of a continuous target variable.	algorithm;experiment;machine learning;oversampling;resampling (statistics);sampling (signal processing);synthetic intelligence	Luís Torgo;Paula Branco;Rita P. Ribeiro;Bernhard Pfahringer	2015	Expert Systems	10.1111/exsy.12081	computer science;artificial intelligence;machine learning;data mining	ML	15.580477447086926	-38.11358425100157	145929
9970742959eb3ee533f42fed0586fdf2fe8cf792	class dependent cluster refinement	pattern clustering;pattern clustering pattern classification;online handwriting data;class dependent cluster refinement;class dependent approach;online handwriting data class dependent cluster refinement unsupervised classification pattern recognition inter class variations class dependent approach clustering problem;pattern recognition handwriting recognition clustering algorithms prototypes testing;pattern classification;pattern recognition;unsupervised classification;matematik;clustering problems;inter class variations;clustering problem	Unsupervised classification is a very common problem in pattern recognition even when the classes are known. In many areas intra-class variations may be greater than the inter-class variations causing a need for a subdivision of the training set of a class into smaller subunits often referred to as clusters. The subdivision or clustering is often performed independently of the relative properties of the other present classes in the recognition task. This paper presents a novel class-dependent approach to the clustering problem. Experiments with online handwriting data show that the novel clustering approach CDCR produces a clustering better suited for the task of pattern recognition. Although only validated for two recognition methods in this paper, the same approach could be applied to other methods as well as to other pattern recognition problems	cluster analysis;pattern recognition;statistical classification;subdivision surface;test set;unsupervised learning	Jakob Sternby	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.364	correlation clustering;feature;fuzzy clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;single-linkage clustering;clustering high-dimensional data;conceptual clustering	Vision	15.27024999030493	-44.78225647750445	146093
b6ed3663dee9eafade8ce669cc3d2dfa543ec05b	an optimization framework for combining ensembles of classifiers and clusterers with applications to nontransductive semisupervised learning and transfer learning	transductive learning;performance;classification;ensembles;clustering;theory;transfer learning;algorithms;design;article;semisupervised learning;classification clustering	Unsupervised models can provide supplementary soft constraints to help classify new “target” data because similar instances in the target set are more likely to share the same class label. Such models can also help detect possible differences between training and target distributions, which is useful in applications where concept drift may take place, as in transfer learning settings. This article describes a general optimization framework that takes as input class membership estimates from existing classifiers learned on previously encountered “source” (or training) data, as well as a similarity matrix from a cluster ensemble operating solely on the target (or test) data to be classified, and yields a consensus labeling of the target data. More precisely, the application settings considered are nontransductive semisupervised and transfer learning scenarios where the training data are used only to build an ensemble of classifiers and are subsequently discarded before classifying the target data. The framework admits a wide range of loss functions and classification/clustering methods. It exploits properties of Bregman divergences in conjunction with Legendre duality to yield a principled and scalable approach. A variety of experiments show that the proposed framework can yield results substantially superior to those provided by naïvely applying classifiers learned on the original task to the target data. In addition, we show that the proposed approach, even not being conceptually transductive, can provide better results compared to some popular transductive learning techniques.	bregman divergence;cluster analysis;concept drift;ensembles of classifiers;experiment;legendre polynomials;loss function;mathematical optimization;scalability;semi-supervised learning;similarity measure;transduction (machine learning);unsupervised learning	Ayan Acharya;Eduardo R. Hruschka;Joydeep Ghosh;Sreangsu Acharyya	2014	TKDD	10.1145/2601435	semi-supervised learning;design;performance;computer science;machine learning;pattern recognition;data mining;theory	ML	16.976336077945398	-40.14664410691767	146247
87950e81830186490756eaf2b6f8e335889da407	machine learning methods for multimedia information retrieval		In this thesis we examined several multimodal feature extraction and learning methods for retrieval and classification purposes. We reread briefly some theoretical results of learning in Section 2 and reviewed several generative and discriminative models in Section 3 while we described the similarity kernel in Section 4. We examined different aspects of the multimodal image retrieval and classification in Section 5 and suggested methods for identifying quality assessments of Web documents in Section 6. In our last problem we proposed similarity kernel for time-series based classification. The experiments were carried over publicly available datasets and source codes for the most essential parts are either open source or released. Since the used similarity graphs (Section 4.2) are greatly constrained for computational purposes, we would like to continue work with more complex, evolving and capable graphs and apply for different problems such as capturing the rapid change in the distribution (e.g. session based recommendation) or complex graphs of the literature work. The similarity kernel with the proper metrics reaches and in many cases improves over the state-of-the-art. Hence we may conclude generative models based on instance similarities with multiple modes is a generally applicable model for classification and regression tasks ranging over various domains, including but not limited to the ones presented in this thesis. More generally, the Fisher kernel is not only unique in many ways but one of the most powerful kernel functions. Therefore we may exploit the Fisher kernel in the future over widely used generative models, such as Boltzmann Machines [Hinton et al., 1984], a particular subset, the Restricted Boltzmann Machines and Deep Belief Networks [Hinton et al., 2006]), Latent Dirichlet Allocation [Blei et al., 2003] or Hidden Markov Models [Baum and Petrie, 1966] to name a few.	information retrieval;machine learning	Bálint Zoltán Daróczy	2017	CoRR		machine learning;pattern recognition;active learning;information retrieval;human–computer information retrieval	NLP	16.99488228433083	-49.07154732678504	146378
9136b7d9f41e837fac4ce2220f97e29bff5dbfe4	a hybrid random subspace classifier fusion approach for protein mass spectra classification	classifier fusion;classifier combination;learning algorithm;decision tree;classifier ensemble;mass spectra;logistic regression;feature space;pattern recognition;multiple model;ovarian cancer	Classifier fusion strategies have shown great potential to enhance the performance of pattern recognition systems. There is an agreement among researchers in classifier combination that the major factor for producing better accuracy is the diversity in the classifier team. Re-sampling based approaches like bagging, boosting and random subspace generate multiple models by training a single learning algorithm on multiple random replicates or sub-samples, in either feature space or the sample domain. In the present study we proposed a hybrid random subspace fusion scheme that simultaneously utilizes both the feature space and the sample domain to improve the diversity of the classifier ensemble. Experimental results using two protein mass spectra datasets of ovarian cancer demonstrate the usefulness of this approach for six learning algorithms (LDA, 1-NN, Decision Tree, Logistic Regression, Linear SVMs and MLP). The results also show that the proposed strategy outperforms three conventional re-sampling based ensemble algorithms on these datasets.	algorithm;bootstrap aggregating;decision tree;dimensionality reduction;ensemble learning;feature vector;linear logic;logistic regression;machine learning;memory-level parallelism;pattern recognition;preprocessor;sampling (signal processing);statistical classification	Amin Assareh;Mohammad Hassan Moradi;L. Gwenn Volkert	2008		10.1007/978-3-540-78757-0_1	random subspace method;margin classifier;mass spectrum;feature vector;quadratic classifier;computer science;machine learning;decision tree;pattern recognition;data mining;logistic regression	ML	13.326374749902255	-41.7909987917452	146469
d43238f5a16a15b4d6fcba9220d8be1d2b151d26	improving the performance of the biohel learning classifier system	learning classifier systems;estimation of distribution algorithms;bioinformatics oriented hierarchical evolutionary learning	The identification of significant attributes is of major importance to the performance of a variety of Learning Classifier Systems including the newly-emerged Bioinformatics-oriented Hierarchical Evolutionary Learning (BioHEL) algorithm. However, the BioHEL fails to deliver on a set of synthetic datasets which are the checkerboard data mixed with Gaussian noises due to the fact the significant attributes were not successfully recognised. To address this issue, a univariate Estimation of Distribution Algorithm (EDA) technique is introduced to BioHEL which primarily builds a probabilistic model upon the outcome of the generalization and specialization operations. The probabilistic model which estimates the significance of each attribute provides guidance for the exploration of the problem space. Experiment evaluations showed that the proposed BioHEL systems achieved comparable performance to the conventional one on a number of real-world small-scale datasets. Research efforts were also made on finding the optimal parameter for the traditional and proposed BioHEL systems.	learning classifier system	Xiao-Lei Xia;Huanlai Xing	2013	Expert Syst. Appl.	10.1016/j.eswa.2013.05.025	estimation of distribution algorithm;computer science;artificial intelligence;machine learning;pattern recognition;data mining;statistics	ML	13.796319599021222	-43.08621458965293	146511
de242423327e26b1fb08774522c98495baa97bf7	compressing deep neural network for facial landmarks detection		State-of-the-art deep neural networks (DNNs) have greatly improved the performance of facial landmarks detection. However, DNN models usually have a large number of parameters, which leads to high computational complexity and memory cost. To address this problem, we propose a method to compress large deep neural networks, which includes three steps. (1) Importance-based neuron pruning: compared with traditional connection pruning, we introduce weights correlations to prune unimportant neurons, which can reduce index storage and inference computation costs. (2) Product quantization: further use of product quantization helps to enforce weights sharing, which stores fewer cluster indexes and codebooks than scalar quantization. (3) Network retraining: to reduce training difficulty and performance degradation, we iteratively retrain the network, compressing one layer at a time. Experiments of compressing a VGG-like model for facial landmarks detection demonstrate that the proposed method achieves 26x compression of the model with 1.5% performance degradation.		Dan Zeng;Fan Zhao;Yixin Bao	2016		10.1007/978-3-319-49685-6_10	scalar (physics);fold (higher-order function);artificial neural network;quantization (signal processing);computational complexity theory;computation;inference;artificial intelligence;computer science;pattern recognition	AI	21.710104097961576	-49.06861216199741	146562
bf7fb7fe93303e8e08a3442d3c3a7d3c609adf55	cost-sensitive boosting algorithms as gradient descent	optimization boosting cost sensitive classification gradient descent;cost sensitive boosting algorithms;gradient descent optimization cost sensitive boosting algorithms;pattern recognition gradient methods;objective function;boosting;gradient descent;cost sensitive classification;gradient descent optimization;pattern recognition;boosting costs laboratories intelligent systems information science automation pattern recognition classification algorithms algorithm design and analysis;gradient methods;optimization	AdaBoost is a well known boosting method for generating strong ensemble of weak base learners. The procedure of AdaBoost can be fitted in a gradient descent optimization framework, which is important for analyzing and devising its procedure. Cost sensitive boosting (CSB) is an emerging subject extending the boosting methods for cost sensitive classification applications. Most CSB methods are performed by directly modifying the original AdaBoost procedure. Unfortunately, the effectiveness of most cost sensitive boosting methods are checked only by experiments. It remains unclear whether these methods can be viewed as gradient descent procedures like AdaBoost. In this paper, we show that several typical CSB methods can also be view as gradient descent for minimizing a unified objective function. We then deduce a general greedy boosting procedure. Experimental results also validate the effectiveness of the proposed procedure.	adaboost;boosting (machine learning);collection of computer science bibliographies;experiment;gradient descent;greedy algorithm;loss function;mathematical optimization;optimization problem	Qu-Tang Cai;Yangqiu Song;Chang-Shui Zhang	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518033	gradient descent;brownboost;mathematical optimization;computer science;machine learning;pattern recognition;gradient boosting;boosting	ML	20.820142764812637	-40.79559303681267	146607
32d741b112125548672379a5c2f40e7255e5be9a	cost-sensitive label embedding for multi-label classification	multi-label classification;cost-sensitive;label embedding	Label embedding (LE) is an important family of multi-label classification algorithms that digest the label information jointly for better performance. Different real-world applications evaluate performance by different cost functions of interest. Current LE algorithms often aim to optimize one specific cost function, but they can suffer from bad performance with respect to other cost functions. In this paper, we resolve the performance issue by proposing a novel cost-sensitive LE algorithm that takes the cost function of interest into account. The proposed algorithm, cost-sensitive label embedding with multidimensional scaling (CLEMS), approximates the cost information with the distances of the embedded vectors by using the classic multidimensional scaling approach for manifold learning. CLEMS is able to deal with both symmetric and asymmetric cost functions, and effectively makes cost-sensitive decisions by nearest-neighbor decoding within the embedded vectors. We derive theoretical results that justify how CLEMS achieves the desired cost-sensitivity. Furthermore, extensive experimental results demonstrate that CLEMS is significantly better than a wide spectrum of existing LE algorithms and state-of-the-art cost-sensitive algorithms across different cost functions.	algorithm;cryptographic hash function;decoding methods;disk mirroring;embedded system;image scaling;loss function;multi-label classification;multidimensional scaling;nonlinear dimensionality reduction	Kuan-Hao Huang;Hsuan-Tien Lin	2017	Machine Learning	10.1007/s10994-017-5659-z	mathematical optimization;machine learning;data mining;mathematics;algorithm	AI	22.803051213987104	-42.30472840179237	146778
f9b44310bb0095e1b291c94d6f9ceb3616b57f23	multiple bayesian discriminant functions for high-dimensional massive data classification	decision boundaries;naive bayes;feature weighting;high-dimensional massive data;class dispersion	The presence of complex distributions of samples concealed in high-dimensional, massive sample-size data challenges all of the current classification methods for data mining. Samples within a class usually do not uniformly fill a certain (sub)space but are individually concentrated in certain regions of diverse feature subspaces, revealing the class dispersion. Current classifiers applied to such complex data inherently suffer from either high complexity or weak classification ability, due to the imbalance between flexibility and generalization ability of the discriminant functions used by these classifiers. To address this concern, we propose a novel representation of discriminant functions in Bayesian inference, which allows multiple Bayesian decision boundaries per class, each in its individual subspace. For this purpose, we design a learning algorithm that incorporates the naive Bayes and feature weighting approaches into structural risk minimization to learn multiple Bayesian discriminant functions for each class, thus combining the simplicity and effectiveness of naive Bayes and the benefits of feature weighting in handling high-dimensional data. The proposed learning scheme affords a recursive algorithm for exploring class density distribution for Bayesian estimation, and an automated approach for selecting powerful discriminant functions while keeping the complexity of the classifier low. Experimental results on real-world data characterized by millions of samples and features demonstrate the promising performance of our approach.	algorithm;bayesian approaches to brain function;data mining;discriminant;naive bayes classifier;recursion (computer science);structural risk minimization	Jianfei Zhang;Shengrui Wang;Lifei Chen;Patrick Gallinari	2016	Data Mining and Knowledge Discovery	10.1007/s10618-016-0481-y	machine learning;pattern recognition;data mining;optimal discriminant analysis;mathematics;multiple discriminant analysis;statistics	ML	16.304719734091492	-39.712437629483865	146870
2564fd51f29784dec75c17926cced146c64aa922	hipad - a hybrid interior-point alternating direction algorithm for knowledge-based svm and feature selection		We consider classification tasks in the regime of scarce labeled training data in high dimensional feature space, where specific expert knowledge is also available. We propose a new hybrid optimization algorithm that solves the elastic-net support vector machine (SVM) through an alternating direction method of multipliers in the first phase, followed by an interior-point method for the classical SVM in the second phase. Both SVM formulations are adapted to knowledge incorporation. Our proposed algorithm addresses the challenges of automatic feature selection, high optimization accuracy, and algorithmic flexibility for taking advantage of prior knowledge. We demonstrate the effectiveness and efficiency of our algorithm and compare it with existing methods on a collection of synthetic and real-world data.	algorithm;augmented lagrangian method;computation;elastic net regularization;experiment;feature selection;feature vector;html5 in mobile devices;interior point method;linear system;mathematical optimization;support vector machine;synthetic intelligence;time complexity;two-phase locking	Zhiwei Qin;Xiaocheng Tang;Ioannis Akrotirianakis;Amit Chakraborty	2014		10.1007/978-3-319-09584-4_28	computer science;machine learning;pattern recognition;data mining	ML	22.40350367229741	-38.50328469810171	146875
21ab12e617fb6be1bbcc3ddf621657f548e55874	pattern recognition by distributed coding: test and analysis of the power space similarity method	offline handwritten numeric patterns power space similarity method distributed coding pattern recognition learning samples;pattern recognition testing pattern analysis space technology spatial databases learning systems neural networks handwriting recognition agriculture distributed databases;pattern recognition;learning artificial intelligence pattern recognition very large databases;very large databases;learning artificial intelligence	This paper considers pattern recognition methods using distributed coding. These methods permit rapid learning from a large number of training samples; their recognition speed is high regardless of the size of the learning samples. This paper presents both basic algorithm and extended algorithms. Experiments with a large database of off-line handwritten numeric patterns are then described using the power space similarity method, being a type of distributed coding. Finally the effectiveness of the technique is considered.	algorithm;database;online and offline;pattern recognition	Takao Kobayashi;Masaki Nakagawa	2004	Ninth International Workshop on Frontiers in Handwriting Recognition	10.1109/IWFHR.2004.83	speech recognition;feature;intelligent character recognition;computer science;machine learning;pattern recognition;data mining	Vision	17.58170392605629	-46.00976085269413	146928
6092fcac12560f2100dc71abedbffd8cb492b183	a new heuristic for feature selection by consistent biclustering	supervised classification;discrete mathematics;optimization problem;science learning;computer experiment;feature selection;heuristic algorithm	Given a set of data, biclustering aims at finding simultaneous partitions in biclusters of its samples and ofthe features which are used for representing the samples. Consi stent biclusterings allow to obtain correct classifications of the samples from the known classification of the features, and vice versa , and they are very useful for performing supervised classificati ons. The problem of finding consistent biclusterings can be seen a s a feature selection problem, where the features that are not r elevant for classification purposes are removed from the set of data, while the total number of features is maximized in order to preserve information. This feature selection problem can be formulated as a linear fractional 0–1 optimization problem. We proposea reformulation of this problem as a bilevel optimization problem, and we present a heuristic algorithm for an efficient solution of the reformulated problem. Computational experiments show that the presented algorithm is able to find better solutionswith respect to the ones obtained by employing previously presen ted heuristic algorithms.	biclustering;bilevel optimization;computation;experiment;feature selection;heuristic (computer science);mathematical optimization;optimization problem;selection algorithm	Antonio Mucherino;Sonia Cafieri	2010	CoRR		computational problem;heuristic;optimization problem;mathematical optimization;computer experiment;computer science;machine learning;pattern recognition;mathematics;feature selection	ML	10.28657930371088	-43.794906266911475	147120
5e0b7436a542725bc98cc83ea4cd4169fa3d965f	eprennid: an evolutionary prototype reduction based ensemble for nearest neighbor classification of imbalanced data	differential evolution;smote;instance selection;imbalanced data;mathematics and statistics;prototype selection;performance;over sampling technique;nearest neighbor;algorithms;classifiers;optimization;data sets;prototype generation	Classification problems with an imbalanced class distribution have received an increased amount of attention within the machine learning community over the last decade. They are encountered in a growing number of real-world situations and pose a challenge to standard machine learning techniques. We propose a new hybrid method specifically tailored to handle class imbalance, called EPRENNID. It performs an evolutionary prototype reduction focused on providing diverse solutions to prevent the method from overfitting the training set. It also allows us to explicitly reduce the underrepresented class, which the most common preprocessing solutions handling class imbalance usually protect. As part of the experimental study, we show that the proposed prototype reduction method outperforms state-of-theart preprocessing techniques. The preprocessing step yields multiple prototype sets that are later used in an ensemble, performing a weighted voting scheme with the nearest neighbor classifier. EPRENNID is experimentally shown to significantly outperform previous proposals. & 2016 Elsevier B.V. All rights reserved.	experiment;heuristic (computer science);machine learning;nearest neighbour algorithm;opaque pointer;overfitting;preprocessor;prototype;sampling (signal processing);test set	Sarah Vluymans;Isaac Triguero;Chris Cornelis;Yvan Saeys	2016	Neurocomputing	10.1016/j.neucom.2016.08.026	differential evolution;performance;computer science;machine learning;pattern recognition;data mining;k-nearest neighbors algorithm;data set	AI	10.978560834787304	-43.15576047846637	147146
7bddda6bc768c6dc23c3b75b7b53d615677eb93b	binary classification models comparison: on the similarity of datasets and confusion matrix for predictive toxicology applications	similarity of predictive models;confusion matrix;similarity of toxicology datasets;classifiers comparison	Nowadays generating predictive models by applying machine learning and model ensembles techniques is a faster task facilitated by development of more user-friendly data mining tools. However, such progress raises the issues related to model management: once developed, many classifiers for example become accessible in collections of models. Choosing the relevant model from the collection can reduce costs of generating new predictive models: calculating the similarity of predictive models is the key to rank them, which may improve model selection or combination. For this aim we introduce a methodology to measure the similarity of classifiers by comparing their datasets, transfer functions and confusion matrices. We propose the Dataset Similarity Coefficient to calculate the similarity of datasets, and the Similarity of Models measure to calculate the similarity between such predictive models. In this paper we focus on toxicology applications of binary classification models. The results show that our methodology performs well in measuring models similarity from a collection of classifiers.	binary classification;confusion matrix	Mokhairi Makhtar;Daniel Neagu;Mick J. Ridley	2011		10.1007/978-3-642-23208-4_11	computer science;machine learning;pattern recognition;data mining	ML	10.943831994413621	-49.32160354924271	147284
c43b5e008331e44270b6548b49223b31dd93a297	efficient local feature encoding for human action recognition with approximate sparse coding		Local spatio-temporal features are popular in the human action recognition task. In practice, they are usually coupled with a feature encoding approach, which helps to obtain the video-level vector representations that can be used in learning and recognition. In this paper, we present an efficient local feature encoding approach, which is called Approximate Sparse Coding (ASC). ASC computes the sparse codes for a large collection of prototype local feature descriptors in the off-line learning phase using Sparse Coding (SC) and look up the nearest prototype’s precomputed sparse code for each to-be-encoded local feature in the encoding phase using Approximate Nearest Neighbour (ANN) search. It shares the low dimensionality of SC and the high speed of ANN, which are both desired properties for a local feature encoding approach. ASC has been excessively evaluated on the KTH dataset and the HMDB51 dataset. We confirmed that it is able to encode large quantity of local video features into discriminative low dimensional representations efficiently. key words: approximate sparse coding, sparse coding, approximate nearest neighbour, local feature encoding, action recognition	approximation algorithm;code;coding theory;encode;nearest neighbor search;neural coding;online and offline;precomputation;prototype;sparse matrix;ti advanced scientific computer	Yu Wang;Jien Kato	2016	IEICE Transactions		computer science;theoretical computer science;machine learning;pattern recognition;sparse approximation;neural coding	ML	18.640321308662074	-46.3787879907064	147800
969174f09f529de4e1eb02f2fcd7d7cba3b1aa13	feature selection for multiple binary classification problems	alternative partitions;curse of dimensionality;data representation;clustering;transpose projection;feature selection;binary classification	The present study proposes an unsupervised method for selection of feature subsets, which retain sucient information for classi®cation purposes. Multiple alternative physically feasible partitions can be dealt with, using the present method. The method is based on an alternative approach to data representation, in which the axes are the data points instead of the features (a transpose projection). Under this representation, coherent features are located in the vicinity of each other, and hence can be clustered, while noisy features are pointed out and eliminated. The method bypasses the ``curse of dimensionality'' and demonstrates good results in particular in small data sets. Ó 1999 Elsevier Science B.V. All rights reserved.	binary classification;coherence (physics);curse of dimensionality;data (computing);data point;feature selection;unsupervised learning	Yair Shapira;Isak Gath	1999	Pattern Recognition Letters	10.1016/S0167-8655(99)00046-X	binary classification;mathematical optimization;curse of dimensionality;computer science;machine learning;pattern recognition;mathematics;external data representation;cluster analysis;feature selection	NLP	23.802270753990506	-41.69819087831522	147826
2cc0691af661746c5ae7685f3f314a744b833877	on generalized multiple-instance learning	drug discovery;multiple instance learning;protein sequence;robot vision;protein sequence identification;winnow;hausdorff metric;content based image retrieval	We describe a generalization of the multiple-instance learning model in which a bag’s label is not based on a single instance’s proximity to a single target point. Rather, a bag is positive if and only if it contains a collection of instances, each near one of a set of target points. We list potential applications of this model (robot vision, content-based image retrieval, protein sequence identification, and drug discovery) and describe target concepts for these applications that cannot be represented in the conventional multiple-instance learning model. We then adapt a learning-theoretic algorithm for learning in this model and present empirical results.	algorithm;approximation;cluster analysis;computational complexity theory;content-based image retrieval;dd (unix);enumerated type;existential quantification;experiment;generalization error;hausdorff dimension;image scaling;markov chain monte carlo;mathematical optimization;monte carlo method;multiple-instance learning;overfitting;pattern recognition;perceptron;polynomial;relevance;requirement;scalability;single-instance storage;speedup;test set;theory;utility functions on indivisible goods;weight function;whole earth 'lectronic link;winnow (algorithm);word lists by frequency	Stephen Scott;Jun Zhang;Joshua Brown	2005	International Journal of Computational Intelligence and Applications	10.1142/S1469026805001453	winnow;hausdorff distance;computer vision;instance-based learning;machine learning;protein sequencing;pattern recognition;drug discovery	ML	18.37644167431999	-41.49911879262477	147991
b3fbee7c4cc4f3f26d286fd07b58ecc73b5d5257	incremental machine learning to reduce biochemistry lab costs in the search for drug discovery	concept drift;supervised learning;drug discovery;reinforcement learning;laboratory tests;supervised machine learning;machine learning	This paper promotes the use of supervised machine learning in laboratory settings where chemists have a large number of samples to test for some property, and are interested in identifying as many positive instances for the least laboratory testing effort. Rather than traditional supervised learning where the chemists would first develop a large training set and then train a classifier, the paper promotes incrementally re-training from each lab test as it completes and then predicting the next best sample to test, as in the field of reinforcement learning. The method outperformed the 2001 KDD Cup thrombin competition winner, partly due to its reduced risk to concept drift from training set to test set.	concept drift;data mining;experiment;machine learning;reinforcement learning;supervised learning;test set	George Forman	2002			semi-supervised learning;unsupervised learning;instance-based learning;error-driven learning;test set;computer science;artificial intelligence;data science;concept drift;online machine learning;machine learning;learning classifier system;supervised learning;stability;computational learning theory;reinforcement learning;active learning;drug discovery;learning to rank;generalization error	AI	14.795807262714355	-38.90040227084395	148055
479de824eaf123a6a594965cab05ea5ddc58671f	minimizer of the reconstruction error for multi-class document categorization	text mining;dimensionality reduction;principal component analysis;document categorization	In the present article we introduce and validate an approach for single-label multi-class document categorization based on text content features. The introduced approach uses the statistical property of Principal Component Analysis, which minimizes the reconstruction error of the training documents used to compute a low-rank category transformation matrix. Such matrix transforms the original set of training documents from a given category to a new low-rank space and then optimally reconstructs them to the original space with a minimum reconstruction error. The proposed method, called Minimizer of the Reconstruction Error (mRE) classifier, uses this property, and extends and applies it to new unseen test documents. Several experiments on four multi-class datasets for text categorization are conducted in order to test the stable and generally better performance of the proposed approach in comparison with other popular classification methods.	approximation algorithm;categorization;cross-validation (statistics);document classification;document-term matrix;existential quantification;experiment;lanczos algorithm;principal component analysis;sparse matrix;taxonomy (general);transformation matrix	Juan-Carlos Gomez;Marie-Francine Moens	2014	Expert Syst. Appl.	10.1016/j.eswa.2013.08.016	text mining;computer science;artificial intelligence;machine learning;pattern recognition;data mining;dimensionality reduction;principal component analysis	ML	23.8827692950153	-42.06759492723454	148283
c051cfabf26e75e4af2edfeec86f633a7d76590f	cross temporal recurrent networks for ranking question answer pairs		Temporal gates play a significant role in modern recurrentbased neural encoders, enabling fine-grained control over recursive compositional operations over time. In recurrent models such as the long short-term memory (LSTM), temporal gates control the amount of information retained or discarded over time, not only playing an important role in influencing the learned representations but also serving as a protection against vanishing gradients. This paper explores the idea of learning temporal gates for sequence pairs (question and answer), jointly influencing the learned representations in a pairwise manner. In our approach, temporal gates are learned via 1D convolutional layers and then subsequently cross applied across question and answer for joint learning. Empirically, we show that this conceptually simple sharing of temporal gates can lead to competitive performance across multiple benchmarks. Intuitively, what our network achieves can be interpreted as learning representations of question and answer pairs that are aware of what each other is remembering or forgetting, i.e., pairwise temporal gating. Via extensive experiments, we show that our proposed model achieves state-of-the-art performance on two community-based QA datasets and competitive performance on one factoid-based QA dataset.	ap computer science principles;benchmark (computing);effective method;encoder;experiment;gradient;interaction;litecoin;long short-term memory;recursion;software quality assurance;temporal logic	Yi Tay;Luu Anh Tuan;Siu Cheung Hui	2018			encoder;computer science;artificial intelligence;machine learning;recursion;factoid;ranking;pairwise comparison;forgetting	AI	21.704745524498197	-48.27815008691672	148577
d25d6f913e2a4e0ea9c406e5df7ff783dfce17fc	enhancing recognition of a weak class - comparative study based on biological population data mining	metalearning;weak class recognition;classification;boosting	This paper presents an overview of several methods that can be used to improve recognition of a weak class in binary classification problem. We illustrated this problem in the context of data mining based on a biological population data. We analyze feasibility of several approaches such as boosting, non-symmetric cost of misclassification events, and combining several weak classifiers (metalearning). We show that metalearning seems counter-productive if the goal is to enhance the recognition of a weak class, and that the method of choice would consist in combining boosting with the non-symmetric cost approach.	data mining	Henryk Maciejewski;Ewa Walkowicz;Olgierd Unold;Pawel Skrobanek	2012		10.1007/978-3-642-29350-4_16	biological classification;computer science;machine learning;pattern recognition;data mining;mathematics;boosting	ML	11.476676557553473	-41.71567115749179	148918
fbb22fe488b1eb973071ef117a0dcad9ede1651a	sparsity regularization label propagation for domain adaptation learning	label propagation;domain adaptation learning;multiple kernel learning;maximum mean discrepancy;sparse representation	Recently, domain adaptation learning (DAL) has shown surprising performance by utilizing labeled samples from the source (or auxiliary) domain to learn a robust classifier for the target domain of the interest which has a few or even no labeled samples. In this paper, by incorporating classical graphbased transductive SSL diagram, a novel DAL method is proposed based on a sparse graph constructed via kernel sparse representation of data in an optimal reproduced kernel Hilbert space (RKHS) recovered by minimizing inter-domain distribution discrepancy. Our method, named as Sparsity regularization Label Propagation for Domain Adaptation Learning (SLPDAL), can propagate the labels of the labeled data from both domains to the unlabeled one in the target domain using their sparsely reconstructed objects with sufficient smoothness by using three steps: (1) an optimal RKHS is first recovered so as to minimize the data distributions of two domains; (2) it then computes the best kernel sparse reconstructed coefficients for each data point in both domains by using l1-norm minimization in the RKHS, thus constructing a sparse graph; and (3) the labels of the labeled data from both domains is finally propagated to the unlabeled points in the target domain over the sparse graph based on our proposed sparsity regularization framework, in which it is assumed that the label of each data point can be sparsely reconstructed by those of other data points from both domains. Furthermore, based on the proposed sparsity regularization framework, an easy way is derived to extend SLPDAL to out-of-sample data. Promising experimental results have been obtained on both a serial of toy datasets and several real-world datasets such as face, visual video and text. & 2014 Elsevier B.V. All rights reserved.	coefficient;data point;diagram;discrepancy function;domain adaptation;hilbert space;inter-domain;kernel (operating system);matrix regularization;software propagation;sparse approximation;sparse matrix;taxicab geometry	Jianwen Tao;Wenjun Hu;Shitong Wang	2014	Neurocomputing	10.1016/j.neucom.2014.02.044	semi-supervised learning;mathematical optimization;computer science;machine learning;pattern recognition;sparse approximation;mathematics	AI	24.164789887940653	-43.23611873761025	149295
b902c069acff1404c930bc8d050636330671be12	overcoming data scarcity with transfer learning		Despite increasing focus on data publication and discovery in materials science and related fields, the global view of materials data is highly sparse. This sparsity encourages training models on the union of multiple datasets, but simple unions can prove problematic as (ostensibly) equivalent properties may be measured or computed differently depending on the data source. These hidden contextual differences introduce irreducible errors into analyses, fundamentally limiting their accuracy. Transfer learning, where information from one dataset is used to inform a model on another, can be an effective tool for bridging sparse data while preserving the contextual differences in the underlying measurements. Here, we describe and compare three techniques for transfer learning: multi-task, difference, and explicit latent variable architectures. We show that difference architectures are most accurate in the multi-fidelity case of mixed DFT and experimental band gaps, while multi-task most improves classification performance of color with band gaps. For activation energies of steps in NO reduction, the explicit latent variable method is not only the most accurate, but also enjoys cancellation of errors in functions that depend on multiple tasks. These results motivate the publication of high quality materials datasets that encode transferable information, independent of industrial or academic interest in the particular labels, and encourage further development and application of transfer learning methods to materials informatics problems.	bridging (networking);computer multitasking;display resolution;encode;informatics;irreducibility;latent variable;sparse matrix	Maxwell L. Hutchinson;Erin Antono;Brenna M. Gibbons;Sean Paradiso;Julia Ling;Bryce Meredig	2017	CoRR		machine learning;transfer of learning;scarcity;mathematics;artificial intelligence;sparse matrix;limiting;materials informatics;band gap;bridging (networking);latent variable	ML	20.27509004789904	-45.00439200608815	149448
27fca2a7c2d565be983b4a786194e92bdc3d6ac7	remote sensing image classification based on stacked denoising autoencoder		Focused on the issue that conventional remote sensing image classification methods have run into the bottlenecks in accuracy, a new remote sensing image classification method inspired by deep learning is proposed, which is based on Stacked Denoising Autoencoder. First, the deep network model is built through the stacked layers of Denoising Autoencoder. Then, with noised input, the unsupervised Greedy layer-wise training algorithm is used to train each layer in turn for more robust expressing, characteristics are obtained in supervised learning by Back Propagation (BP) neural network, and the whole network is optimized by error back propagation. Finally, Gaofen-1 satellite (GF-1) remote sensing data are used for evaluation, and the total accuracy and kappa accuracy reach 95.7% and 0.955, respectively, which are higher than that of the Support Vector Machine and Back Propagation neural network. The experiment results show that the proposed method can effectively improve the accuracy of remote sensing image classification.	artificial neural network;autoencoder;backpropagation;computer vision;deep learning;greedy algorithm;network model;noise reduction;software propagation;supervised learning;support vector machine	Peng Liang;Wenzhong Shi;Xiaokang Zhang	2018	Remote Sensing	10.3390/rs10010016	remote sensing;support vector machine;artificial neural network;supervised learning;computer vision;geology;artificial intelligence;network model;autoencoder;deep learning;contextual image classification;backpropagation	AI	22.47963990934364	-51.147212382191874	149577
24f3a2071fe44cae8bf82c57049daa5b65fcf567	sparse dimensionality reduction based on compressed sensing	sparse matrices compressed sensing convergence handwritten character recognition image classification image representation learning artificial intelligence;classification error rate sparse dimensionality reduction compressed sensing objective function semisupervised learning convergence rate sparse dictionary training dataset omp algorithm k svd algorithm data sparse representation gaussian random matrix measurement matrix data discrimination l 2 norm sdr cs method handwritten digit mnist dataset image l 1 norm;instance based learning compressed sensing dimensionality reduction sparse representation sparse dictionary measurement matrix semi supervised learning;dictionaries transforms vectors sparse matrices databases business optimization	In this paper, we propose a novel approach SDR-CS (Sparse Dimensionality Reduction based on CS) based on compressed sensing to reduce dimensionality. With certain constraint of objective function, our semi-supervised learning method utilizes instance to construct the optimally sparse dictionary in the training dataset, employs K-SVD and OMP algorithms to improve the convergence rate of learning, and then reduces the dimensionality of sparse representation of original data by Gaussian random matrix as measurement matrix, to achieve the purpose of dimensionality reduction. Experimental results demonstrate that our overcomplete sparse dictionary can enhance the major underlying structure characteristics of sparse representation, which are mapped into the regions with continuous dimensionality, not the same dimensionality, and improve the discrimination among data which belong to different classes. Only with the constraint of l2-norm, the proposed SDR-CS method has better performance of dimensionality reduction in the MNIST dataset, and it is superior to other existing methods with constraints of l2/l1-norm, achieving the classification error rate of 0.03.	algorithm;cluster analysis;compressed sensing;computation;dictionary;dimensionality reduction;etsi satellite digital radio;experiment;k-svd;loss function;mnist database;openmp;optimization problem;rate of convergence;semi-supervised learning;semiconductor industry;singular value decomposition;sparse approximation;sparse matrix;supervised learning	Yufang Tang;Xueming Li;Yang Liu;Jizhe Wang;Yan Xu	2014	2014 IEEE Wireless Communications and Networking Conference (WCNC)	10.1109/WCNC.2014.6953119	sparse pca;speech recognition;k-svd;computer science;machine learning;pattern recognition;sparse approximation	AI	23.98154180988797	-42.02915736966945	149607
f70c83756015c4744895329551d62f194347cf3f	online active learning paired ensemble for concept drift and class imbalance		Practical applications often require learning algorithms capable of addressing data streams with concept drift and class imbalance. This paper proposes an online active learning paired ensemble for drifting streams with class imbalance. The paired ensemble consists of a long-term stable classifier and a dynamic classifier to address both sudden concept drift and gradual concept drift. To select the most representative instances for learning, a hybrid labeling strategy which includes an uncertainty strategy and an imbalance strategy is proposed. The uncertainty strategy applies a margin-based uncertainty criterion and a dynamic adjustment threshold. Based on the categorical distribution of the last data block, the imbalance strategy prefers to learn instances of the minority category. In addition, it also incorporates the advantages of the traditional random strategy and helps to capture the drifts away from the decision boundary. Experiments on real datasets and synthetic datasets utilize prequential AUC as an evaluation index, comparing the classification performance of our method with semi-supervised and supervised learning methods. The results show that the proposed methods can obtain higher AUC values at an even lower labeling cost. Moreover, it is noteworthy that the labeling cost can be dynamically allocated according to the concept drift and imbalance ratio.		Hang Zhang;Weike Liu;Jicheng Shan;Qingbao Liu	2018	IEEE Access	10.1109/ACCESS.2018.2882872	supervised learning;streams;data stream mining;active learning;concept drift;computer science;distributed computing;categorical distribution;machine learning;statistical classification;decision boundary;artificial intelligence	AI	14.563621327002345	-39.841749825303474	149621
f040b71bdbd1c1e16351e8db68179d5ea1ba303d	software defect classification with a variant of nsga-ii and simple voting strategies		Software Defect Prediction is based on datasets that are imbalanced and therefore limit the use of machine learning based classification. Ensembles of genetic classifiers indicate good performance and provide a promising solution to this problem. To further examine this solution, we performed additional experiments in that direction. In this paper we report preliminary results obtained by using a Matlab variant of NSGA-II in combination with four simple voting strategies on three subsequent releases of the Eclipse Plug-in Development Environment (PDE) project. Preliminary results indicate that the voting procedure might influence software defect prediction performances.	software bug	Emil Rubinic;Goran Mausa;Tihana Galinac Grbac	2015		10.1007/978-3-319-22183-0_33	computer science;theoretical computer science;machine learning;data mining	AI	10.69624491876146	-42.372697385692085	149712
1ef230a8d20658cad5f267a929afe4733b6f68b2	self-adaptive attribute weighting for naive bayes classification	self adaptive;attribute weighting;naive bayes;journal article;期刊论文;artificial immune systems;evolutionary computing	Naive Bayes (NB) is a popular machine learning tool for classification, due to its simplicity, high computational efficiency, and good classification accuracy, especially for high dimensional data such as texts. In reality, the pronounced advantage of NB is often challenged by the strong conditional independence assumption between attributes, which may deteriorate the classification performance. Accordingly, numerous efforts have been made to improve NB, by using approaches such as structure extension, attribute selection, attribute weighting, instance weighting, local learning and so on. In this paper, we propose a new Artificial Immune System (AIS) based self-adaptive attribute weighting method for Naive Bayes classification. The proposed method, namely AISWNB, uses immunity theory in artificial immune systems to search optimal attribute weight values, where self-adjusted weight values will alleviate the conditional independence assumption and help calculate the conditional probability in an accurate way. One noticeable advantage of AISWNB is that the unique immune system based evolutionary computation process, including initialization, clone, section, and mutation, ensures that AISWNB can adjust itself to the data without explicit specification of functional or distributional forms of the underlying model. As a result, AISWNB can obtain good attribute weight values during the learning process. Experiments and comparisons on 36 machine learning benchmark data sets and six image classification data sets demonstrate that AISWNB significantly outperforms its peers in classification accuracy, class probability estimation, and class ranking performance.	naive bayes classifier;victor pan;zhi-li zhang	Jia Wu;Shirui Pan;Xingquan Zhu;Zhihua Cai;Peng Zhang;Chengqi Zhang	2015	Expert Syst. Appl.	10.1016/j.eswa.2014.09.019	naive bayes classifier;computer science;artificial intelligence;machine learning;pattern recognition;data mining;statistics;evolutionary computation	AI	11.879542163460302	-39.52654652811364	149820
5b2da1ca022c6dcd876cd1c40de67eef5b43afd4	automatic relevance determination with ensembles of bayesian mlps		The problem of controlling model complexity and data complexity are fundamental issues in neural network learning. Some researchers have used Bayesian-learning on neural networks to control the model complexity. The Bayesian-based technique, Automatic Relevance Determination (ARD), can effectively control the complexity of data, and automatically determine the relevance of input features by controlling the distribution of corresponding groups of weights in a network. However, we found that the relevance determination made by a single ARD model is not stable and accurate. Neural network ensemble techniques were used in our research to improve the accuracy of feature relevance determination. The accuracy of the ensemble feature relevance determination was evaluated using two synthetic datasets in which the relevance of each individual input feature was pre-determined. The results showed that ensemble feature relevance determination can effectively separate relevant features, redundancies and irrelevant features from each other, and provide useful suggestions of the boundaries between these relevance levels. Thus, the features selected, based on the ensemble feature relevance determination, benefits not only non-linear models such as neural networks, but also linear models such as the linear regression model, by enabling them to classify the samples in several real-world datasets more accurately than by using all the available input features, extracted principal components and independent components from the datasets. We also found that an ensemble of ARD models is good at selecting group relevance features, but not at ranking the relevance for each individual input feature, because the relevance rank determination of an input feature can be affected by any redundancies in the data which are highly correlated with it.	relevance	Yu Fu	2008			artificial neural network;linear model;principal component analysis;linear regression;bayesian probability;artificial intelligence;computer science;pattern recognition;ranking	NLP	11.257826174479822	-38.652752479425644	149967
58c2d156ec6c8a247543f8ad0cdbbc3879281720	incorporating prior knowledge into svm for image retrieval	new knowledge-based target function;optimisation;learning performance;knowledge based systems;learning (artificial intelligence);knowledge-based target function;svm;prior knowledge;retrieval performance;support vector machine;image retrieval;svm optimization;labelled sample;support vector machines;knowledge;prior;learning artificial intelligence;image;knowledge base	SVM based image retrieval suffers from the scarcity of labelled samples. In this paper, this problem is solved by incorporating prior knowledge into SVM. Firstly, some prior knowledge of image retrieval is discussed and constructed. After that, the knowledge is incorporated into SVM optimization as a constraint, and a new knowledge-based target function is formulated. Based on this, a framework of image retrieval with knowledge based SVM is proposed. Experimental results demonstrate that the proposed method can effectively improve the learning and retrieval performance of SVM, especially when the number of labelled samples is small.	image retrieval;mathematical optimization;support vector machine	Lei Wang;Ping Xue;Kap Luk Chan	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1334423	support vector machine;knowledge base;image retrieval;computer science;machine learning;pattern recognition;data mining;ranking svm	Vision	17.685482433057306	-43.026025270556	149978
10ee2ca6babbec2b3e26ce9e8b9718e74b88d4aa	training cnns with low-rank filters for efficient image classification		I We train CNNs with composite layers of oriented low-rank filters, of which the network learns the most effective linear combination I In effect our networks learn a basis space for filters, based on simpler low-rank filters I We propose an initialization for composite layers of heterogeneous filters, to train such networks from scratch I Our models are faster and use less parameters I With a small number of full filters, our models also generalize better	algorithmic efficiency;approximation algorithm;artificial neural network;computation;computer vision;convolutional neural network	Yani Ioannou;Duncan P. Robertson;Jamie Shotton;Roberto Cipolla;Antonio Criminisi	2015	CoRR		computer science;artificial intelligence;theoretical computer science;machine learning	ML	23.345382742871685	-49.55220730632854	150057
08b4c61724390f0ba52c711b89121c8dcf7de6c5	feature selection for support vector machines	high dimensional input vectors;high dimensionality;neural networks;support vector machines;support vector machines support vector machine classification neural networks computer science computational efficiency lagrangian functions packaging;packaging;learning automata;component ranking;generalization ability;computational complexity;feature pruning feature selection support vector machines svm high dimensional input vectors computational efficiency pattern classification component ranking generalization ability;feature extraction;pattern classification;generalisation artificial intelligence pattern classification feature extraction learning automata computational complexity;support vector machine classification;svm;feature selection;generalisation artificial intelligence;computer science;support vector machine;classification accuracy;computational efficiency;lagrangian functions;feature pruning	In the context of support vector machines (SVM), high dimensional input vectors often reduce the computational efficiency and significantly slow down the classification process. In this paper, we propose a strategy to rank individual components according to their influence on the class assignments. This ranking is used to select an appropriate subset of the features. It replaces the original feature set without significant loss in classification accuracy. Often, the generalization ability of the classifier even increases due to the implicit regularization achieved by feature pruning.	computation;dynamic language runtime;experiment;feature selection;gradient;iterative method;support vector machine	Lothar Hermes;Joachim M. Buhmann	2000		10.1109/ICPR.2000.906174	support vector machine;computer science;machine learning;linear classifier;pattern recognition;data mining;mathematics;feature selection;artificial neural network	ML	19.783081551247104	-39.20565842524793	150192
193869b4607fa16c92b5b9268343ce0e192b21e0	multicategory learning classifiers for character reading	convergence;machine learning pattern recognition convergence character recognition costs performance analysis algorithm design and analysis tellurium pattern classification decision making;training;tellurium;vectors;machine learning;computer experiment;linear discriminant function;classification algorithms;performance analysis;pattern classification;pattern recognition;support vector machine classification;character recognition;algorithm design and analysis	This paper presents properties of several different algorithms However, there exist some difficulties in constructing the suitable for multicategory classification of hand-printed alphanumeric multicategory classifier because of its complexity, its long characters. In the character reader the input patterns are generally training period, etc. If we use a well-known technique, composed of the template characters and their distorted ones. Using the template patterns, a nonparametric procedure is developed for determinthere are two approaches to obtain a nonparametric multiing linear discriminant functions. Furthermore, we propose the mechanism category classifier. One is the Perceptron-like machine [1] which has the ability to recognize even a misprinted character by using which has more than two receptor units. The other is the the information of the preceding character. The algorithms offer the machine in which classification is made by detecting the following advantages: flexibility (cost assignments), simplicity, adaptaoutput corresponding to each category. For the tion, and acceptable performance. Performance of the machines is maximum otu haspon poset rategorygorithe analyzed and convergence proofs of the learning procedures in the latter type Nilsson [2] has proposed training algorithms machines are derived. We also present some results of computer and made an analysis of the learning process. Also, Duda experiments. and Fossum [3] have given the convergence proof in a	algorithm;discriminant;existential quantification;experiment;incidence poset;perceptron;printing;sensor	Masamichi Shimura	1973	IEEE Trans. Systems, Man, and Cybernetics	10.1109/TSMC.1973.5408580	statistical classification;algorithm design;speech recognition;computer experiment;convergence;feature;computer science;artificial intelligence;machine learning;pattern recognition;tellurium	ML	12.497026862151564	-38.1978215126057	150655
fda5c8d32910704cebef1a1672fa0bb185a688c8	turbo-smt: parallel coupled sparse matrix-tensor factorizations and applications	coupled matrix tensor factorization;sparse;fmri data;tensor;neurosemantics;algorithm;parallel;school of automation;speedup;computer science automation formerly	How can we correlate the neural activity in the human brain as it responds to typed words, with properties of these terms (like 'edible', 'fits in hand')? In short, we want to find latent variables, that jointly explain both the brain activity, as well as the behavioral responses. This is one of many settings of the Coupled Matrix-Tensor Factorization (CMTF) problem. Can we enhance any CMTF solver, so that it can operate on potentially very large datasets that may not fit in main memory? We introduce Turbo-SMT, a meta-method capable of doing exactly that: it boosts the performance of any CMTF algorithm, produces sparse and interpretable solutions, and parallelizes any CMTF algorithm, producing sparse and interpretable solutions (up to 65 fold). Additionally, we improve upon ALS, the work-horse algorithm for CMTF, with respect to efficiency and robustness to missing values. We apply Turbo-SMT to BrainQ, a dataset consisting of a (nouns, brain voxels, human subjects) tensor and a (nouns, properties) matrix, with coupling along the nouns dimension. Turbo-SMT is able to find meaningful latent variables, as well as to predict brain activity with competitive accuracy. Finally, we demonstrate the generality of Turbo-SMT, by applying it on a Facebook dataset (users, 'friends', wall-postings); there, Turbo-SMT spots spammer-like anomalies.		Evangelos E. Papalexakis;Tom M. Mitchell;Nikos D. Sidiropoulos;Christos Faloutsos;Partha Pratim Talukdar;Brian Murphy	2016	Statistical analysis and data mining	10.1002/sam.11315	tensor;speedup;computer science;artificial intelligence;theoretical computer science;machine learning;parallel;data mining;mathematics;statistics	ML	19.702170882777825	-45.59223576107898	150704
92b478e90dae4ff83a89d6fd092e84f3fe5c4441	blasso for object categorization and retrieval: towards interpretable visual models	object categorization;interpretability;boosting;sparsity;feature selection;lasso	We propose a new supervised object retrieval method based on the selection of local visual features learned with the BLasso algorithm. BLasso is a boosting-like procedure that efficiently approximates the Lasso path through backward regularization steps. The advantage compared to a classical boosting strategy is that it produces a sparser selection of visual features. This allows us to improve the efficiency of the retrieval and, as discussed in the paper, it facilitates human visual interpretation of the models generated. We carried out our experiments on the Caltech-256 dataset with state-of-the-art local visual features. We show that our method outperforms AdaBoost in effectiveness while significantly reducing the model complexity and the prediction time. We discuss the evaluation of the visual models obtained in terms of human interpretability.		Ahmed Rebai;Alexis Joly;Nozha Boujemaa	2012	Pattern Recognition	10.1016/j.patcog.2011.11.022	computer science;machine learning;lasso;pattern recognition;data mining;sparsity-of-effects principle;feature selection;boosting	Vision	23.349205017313686	-44.232398320235205	150973
31ed4572e083871f02ff047456dfd593522d149b	integrating active learning with supervision for crowdsourcing generalization	pattern classification internet learning artificial intelligence;supervised classification crowdsourcing active learning;mue crowdsourcing generalization data quality improvement active learning strategies uncertainty measures clue;active learning;supervised classification;crowdsourcing uncertainty data models noise measurement estimation training labeling;crowdsourcing	With various online crowdsourcing platforms, it is easy to collect multiple labels for the same examples from the crowd. Consensus integration algorithms can infer the estimated ground truths from the multiple label sets of these crowdsourcing datasets. However, it couldn't be avoided that these integrated (estimated) labels still contain noises. In order to further improve the performance of a model learned from data with these integrated labels, we propose an active learning framework to further improve the data quality, such that to improve the model quality, through acquiring limited true labels from experts (the oracle). We further investigate two active learning strategies in terms of two uncertainty measures (i.e., CLUE and MUE) within the active learning framework. From our experimental results on eight simulation crowdsourcing datasets and four real-world crowdsourcing datasets with three popular consensus integration algorithms, we draw several conclusions as follows. (i) Our active learning framework with the input from the oracle significantly improves the generalization ability of the model learned from crowdsourcing data. (ii) Our two active learning strategies outperform a random active learning strategy.	algorithm;crowdsourcing;data quality;simulation	Zhenyu Shu;Victor S. Sheng;Yonghui Zhang;Dianhong Wang;Jing Zhang;Heng Chen	2015	2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)	10.1109/ICMLA.2015.13	computer science;data science;machine learning;data mining;active learning;crowdsourcing;active learning	ML	16.73617249559995	-40.19422869988675	151088
555cb7b89386d0849014b9af7e3eebb377488a2e	enhanced semi-supervised local fisher discriminant analysis for face recognition	semi supervised local fisher discriminant analysis;small sample size problem;face recognition;dimensionality reduction;enhanced semi supervised local fisher discriminant analysis	An improved manifold learning method, called enhanced semi-supervised local Fisher discriminant analysis (ESELF), for face recognition is proposed. Motivated by the fact that statistically uncorrelated and parameter-free are two desirable and promising characteristics for dimension reduction, a new difference-based optimization objective function with unlabeled samples has been designed. The proposed method preserves the manifold structure of labeled and unlabeled samples in addition to separating labeled samples in different classes from each other. The semi-supervised method has an analytic form of the globally optimal solution and it can be computed based on eigen decomposition. Experiments on synthetic data and AT&T, Yale and CMU PIE face databases are performed to test and evaluate the proposed algorithm. The experimental results and comparisons demonstrate the effectiveness of the proposed method. Crown Copyright© 2010 Published by Elsevier B.V. All rights reserved.	algorithm;crown group;database;eigen (c++ library);experiment;facial recognition system;linear discriminant analysis;mathematical optimization;maxima and minima;nonlinear dimensionality reduction;optimization problem;semi-supervised learning;semiconductor industry;supervised learning;synthetic data	Hong Huang;Jianwei Li;Jiamin Liu	2012	Future Generation Comp. Syst.	10.1016/j.future.2010.11.005	facial recognition system;kernel fisher discriminant analysis;computer science;machine learning;pattern recognition;optimal discriminant analysis;linear discriminant analysis;dimensionality reduction	AI	23.696085028079253	-41.02842163126625	151144
021ac0a872a0af901db55c3c589d75220e06405f	a robust subspace algorithm for principal component analysis	noise robustness;noisy pca;principal component analysis;subspace algorithm;pca	We present a noise robust PCA algorithm which is an extension of the Oja subspace algorithm and allows tuning the noise sensitivity. We derive a loss function which is minimized by this algorithm and interpret it in a noisy PCA setting. Results on the local stability analysis of this algorithm are given and it is shown that the locally stable equilibria are those which minimize the loss function.	algorithm;erkki oja;loss function;oja's rule;principal component analysis	Andreas Weingessel;Kurt Hornik	2003	International journal of neural systems	10.1142/S0129065703001650	computer science;machine learning;pattern recognition;mathematics;statistics;principal component analysis	ML	22.41731589002532	-40.08354024272829	151179
31d29e44c1e5a322e08dc59302aa501d15f94796	are accuracy and robustness correlated	measurement;neural networks;training;error analysis;computational modeling;robustness;agriculture	Machine learning models are vulnerable to adversarial examples formed by applying small carefully chosen perturbations to inputs that cause unexpected classification errors. In this paper, we perform experiments on various adversarial example generation approaches with multiple deep convolutional neural networks including Residual Networks, the best performing models on ImageNet Large-Scale Visual Recognition Challenge 2015. We compare the adversarial example generation techniques with respect to the quality of the produced images, and measure the robustness of the tested machine learning models to adversarial examples. Finally, we conduct large-scale experiments on cross-model adversarial portability. We find that adversarial examples are mostly transferable across similar network topologies, and we demonstrate that better machine learning models are less vulnerable to adversarial examples.	artificial neural network;convolutional neural network;experiment;imagenet;machine learning;network topology	Andras Rozsa;Manuel Günther;Terrance E. Boult	2016	2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)	10.1109/ICMLA.2016.0045	agriculture;computer science;artificial intelligence;machine learning;data mining;artificial neural network;measurement;robustness	ML	19.35516543494313	-51.46686989754052	151217
bd014c13a5acc36392534c12a5dcb5268197cf28	learning using privileged information in prototype based models	information theoretic metric learning itml;generalized matrix learning vector quantization gmlvq;learning using hidden information luhi	In some pattern analysis problems, there exists expert knowledge, in addition to the original data involved in the classification process. Most of existing approaches ignore such auxiliary (privileged) knowledge. Recently a new learning paradigm - Learning Using Hidden Information - was introduced in the SVM+ framework. This approach is formulated for binary classification and, as typical for many kernel based methods, can scale unfavorably with the number of training examples. In this contribution we present a more direct novel methodology, based on a prototype metric learning model, for incorporation of valuable privileged knowledge. This is done by changing the global metric in the input space, based on distance relations revealed by the privileged information. Our method achieves competitive performance against the SVM+ formulations. We also present a successful application of our method to a large scale multi-class real world problem of galaxy morphology classification.	prototype	Shereen Fouad;Peter Tiño;Somak Raychaudhury;Petra Schneider	2012		10.1007/978-3-642-33266-1_40	semi-supervised learning;artificial intelligence;machine learning;pattern recognition;mathematics	NLP	24.089767908764077	-41.61264982218581	151261
0753f8ea09efd37d38558405dd152765a39a701e	manifold regularized experimental design for active learning	databases;g400 computer science;kernel;manifolds;training;g500 information systems;training data;semisupervised learning	Various machine learning and data mining tasks in classification require abundant data samples to be labeled for training. Conventional active learning methods aim at labeling the most informative samples for alleviating the labor of the user. Many previous studies in active learning select one sample after another in a greedy manner. However, this is not very effective, because the classification models have to be retrained for each newly labeled sample. Moreover, many popular active learning approaches utilize the most uncertain samples by leveraging the classification hyperplane of the classifier, which is not appropriate, since the classification hyperplane is inaccurate when the training data are small-sized. The problem of insufficient training data in real-world systems limits the potential applications of these approaches. This paper presents a novel method of active learning called manifold regularized experimental design (MRED), which can label multiple informative samples at one time for training. In addition, MRED gives an explicit geometric explanation for the selected samples to be labeled by the user. Different from existing active learning methods, our method avoids the intrinsic problems caused by insufficiently labeled samples in real-world applications. Various experiments on synthetic data sets, such as the Yale face database and the Corel image database, have been carried out to show how MRED outperforms existing methods.	active learning (machine learning);automatic image annotation;computer vision;data mining;database;effective method;experiment;experimental design;generalization (psychology);greedy algorithm;information;inspiration function;iteration;machine learning;neuritis, autoimmune, experimental;semi-supervised learning;semiconductor industry;supervised learning;synthetic data;synthetic intelligence;tracer;world-system;manifold;one time	Lining Zhang;Hubert P. H. Shum;Ling Shao	2017	IEEE Transactions on Image Processing	10.1109/TIP.2016.2635440	semi-supervised learning;training set;kernel;manifold;computer science;machine learning;pattern recognition;data mining;mathematics;active learning;statistics	ML	20.84810620377099	-42.87061195375394	151377
a28fa5f2e64b14eceea498557a3d09b683c0a759	learning bayesian network parameters with domain knowledge and insufficient data		To improve the learning accuracy of parameters in a Bayesian network (BN) from limited data, domain knowledge is often incorporated into the learning process as parameter constraints. Maximum a posteriori (MAP) based methods that use both data and constraints have been studied extensively. Among those methods, the qualitatively maximum a posteriori (QMAP) method exhibits high learning performance. In the QMAP method, when the data are limited, estimation from the data often fails to satisfy all the parameter constraints, which makes the overall QMAP estimation unreliable. To ensure that the QMAP estimation does not violate any given parameter constraint and further improve the learning accuracy, in this paper, we propose a qualitatively maximum a posteriori correction (QMAP-C) estimation algorithm, which regulates QMAP estimation by replacing the data estimation with a further constrained estimation. Experiments show that the proposed algorithm outperforms most of the existing parameter learning methods when the parameter constraints are correct.	algorithm;bayesian network;estimation theory	Zhigao Guo;Xiaoguang Gao;Ruohai Di	2017			domain knowledge;variable-order bayesian network;machine learning;bayesian network;artificial intelligence;computer science;pattern recognition	AI	19.6567196732214	-41.21316048166528	151409
aec55e5047a7801b372f628d79361056014966b3	feature selection using dynamic weights for classification	information criterion;classification;journal;machine learning;feature selection;filter method	Feature selection aims at finding a feature subset that has the most discriminative information from the original feature set. In this paper, we firstly present a new scheme for feature relevance, interdependence and redundancy analysis using information theoretic criteria. Then, a dynamic weighting-based feature selection algorithm is proposed, which not only selects the most relevant features and eliminates redundant features, but also tries to retain useful intrinsic groups of interdependent features. The primary characteristic of the method is that the feature is weighted according to its interaction with the selected features. And the weight of features will be dynamically updated after each candidate feature has been selected. To verify the effectiveness of our method, experimental comparisons on six UCI data sets and four gene microarray datasets are carried out using three typical classifiers. The results indicate that our proposed method achieves promising improvement on feature selection and classification accuracy. 2012 Elsevier B.V. All rights reserved.	dna microarray;experiment;feature selection;information theory;interdependence;relevance;selection algorithm;statistical classification	Xin Sun;Yanheng Liu;Mantao Xu;Huiling Chen;Jiawei Han;Kunhao Wang	2013	Knowl.-Based Syst.	10.1016/j.knosys.2012.10.001	feature learning;minimum redundancy feature selection;feature vector;feature;feature extraction;biological classification;computer science;machine learning;kanade–lucas–tomasi feature tracker;pattern recognition;data mining;feature selection;k-nearest neighbors algorithm;feature;feature scaling;dimensionality reduction	AI	12.123386518920109	-44.627899966946806	151430
5724d844d31845ca10e3ce52969fd1156be7fda3	kscore: a novel machine learning approach that is not dependent on the data structure of the training set	quantitative structure activity relationship;bayesian classifier;recursive partitioning;structural risk minimization;model generation;machine learning;gaussian process;data structure;neural network	Currently machine learning approaches used in Quantitative Structure Activity Relationship (QSAR) model generation impose restrictions and/or make assumptions on how the training set descriptors correlate with a target activity. kScore has been developed as the first machine learning approach that does not require the training data to conform to a defined kernel, accommodates uneven data point distributions in the descriptor space, and optimizes the weight of each dimension in the descriptor space in order to identify the descriptors most relevant to the target property. The ability of kScore to adapt to virtually any correlation makes it essential that generalization terms be included to inhibit overtraining. The Structural Risk Minimization principle and the linear epsilon-insensitive loss terms have been added to the kScore optimization function. The resulting kScore algorithm has proven to be quite universal across several datasets and either produces results similar to or outperforms the most predictive machine learning algorithms tested, such as SVM, kNN, Recursive Partitioning, Neural Networks, Gaussian Process, and the Bayesian Classifier.		Scott Oloff;Ingo Muegge	2007	Journal of computer-aided molecular design	10.1007/s10822-007-9108-0	naive bayes classifier;data structure;structural risk minimization;machine learning;pattern recognition;data mining;gaussian process;quantitative structure–activity relationship;statistics;recursive partitioning	ML	17.088445778838352	-38.116014315165586	151482
87b6457983c515f6c02921d22d70e1547dfec112	on-line laplacian one-class support vector machines	rkhs;online learning;oneclass svm;semisupervised learning;manifold regularization	We propose a manifold regularization algorithm designed to work in an on–line scenario where data arrive continuously over time and it is not feasible to completely store the data stream for training the classifier in batch mode. The On–line Laplacian One–Class SVM (OLapOCSVM) algorithm exploits both positively labeled and totally unlabeled examples, updating the classifier hypothesis as new data becomes available. The learning procedure is based on conjugate gradient descent in the primal formulation of the SVM. The on–line algorithm uses an efficient buffering technique to deal with the continuous incoming data. In particular, we define a buffering policy that is based on the current estimate of the support of the input data distribution. The experimental results on real–world data show that OLapOCSVM compares favorably with the corresponding batch algorithms, while making it possible to be applied in generic on–line scenarios with limited memory requirements.	algorithm;batch processing;cluster analysis;computer vision;conjugate gradient method;data mining;gradient descent;mnist database;machine learning;manifold regularization;marco dorigo;performance;requirement;semi-supervised learning;semiconductor industry;supervised learning;support vector machine	Salvatore Frandina;Marco Lippi;Marco Maggini;Stefano Melacci	2013		10.1007/978-3-642-40728-4_24	mathematical optimization;machine learning;pattern recognition;reproducing kernel hilbert space;mathematics	ML	20.51236252270911	-38.89912981288026	151486
f6dc3f764c259d384c7b097020b2db387888d9c0	stacked pooling: improving crowd counting by boosting scale invariance		In this work, we explore the cross-scale similarity in crowd counting scenario, in which the regions of different scales often exhibit high visual similarity. This feature is universal both within an image and across different images, indicating the importance of scale invariance of a crowd counting model. Motivated by this, in this paper we propose simple but effective variants of pooling module, i.e., multi-kernel pooling and stacked pooling, to boost the scale invariance of convolutional neural networks (CNNs), benefiting much the crowd density estimation and counting. Specifically, the multi-kernel pooling comprises of pooling kernels with multiple receptive fields to capture the responses at multi-scale local ranges. The stacked pooling is an equivalent form of multi-kernel pooling, while, it reduces considerable computing cost. Our proposed pooling modules do not introduce extra parameters into model and can easily take place of the vanilla pooling layer in implementation. In empirical study on two benchmark crowd counting datasets, the stacked pooling beats the vanilla pooling layer in most cases.	artificial neural network;benchmark (computing);convolutional neural network;kernel (operating system)	Siyu Huang;Xi Li;Zhiqi Cheng;Zhongfei Zhang;Alexander G. Hauptmann	2018	CoRR		boosting (machine learning);convolutional neural network;machine learning;pattern recognition;pooling;artificial intelligence;density estimation;computer science;scale invariance;receptive field	Vision	24.34934788009067	-50.43432212810514	151490
b9ea59d344cea7210540e82e1176adb2c82135c6	matrix profile iv: using weakly labeled time series to predict outcomes		In academic settings over the last decade, there has been significant progress in time series classification. However, much of this work makes assumptions that are simply unrealistic for deployed industrial applications. Examples of these unrealistic assumptions include the following: assuming that data subsequences have a single fixed-length, are precisely extracted from the data, and are correctly labeled according to their membership in a set of equalsize classes. In real-world industrial settings, these patterns can be of different lengths, the class annotations may only belong to a general region of the data, may contain errors, and finally, the class distribution is typically highly skewed. Can we learn from such weakly labeled data? In this work, we introduce SDTS, a scalable algorithm that can learn in such challenging settings. We demonstrate the utility of our ideas by learning from diverse datasets with millions of datapoints. As we shall demonstrate, our domain-agnostic parameter-free algorithm can be competitive with domain-specific algorithms used in neuroscience and entomology, even when those algorithms have been tuned by domain experts to incorporate domain knowledge.	algorithm;domain-specific language;scalability;spatial data transfer standard;time series	Chin-Chia Michael Yeh;Nickolas Kavantzas;Eamonn J. Keogh	2017	PVLDB	10.14778/3137765.3137784	labeled data;data mining;database;domain knowledge;scalability;machine learning;artificial intelligence;computer science;matrix (mathematics)	DB	20.91867824353214	-45.24648612594574	151656
94bd436b5dc2060e1b23c70c16fea26ec56db42a	a new perceptron algorithm for sequence labeling with non-local features	local features	We cannot use non-local features with current major methods of sequence labeling such as CRFs due to concerns about complexity. We propose a new perceptron algorithm that can use non-local features. Our algorithm allows the use of all types of non-local features whose values are determined from the sequence and the labels. The weights of local and non-local features are learned together in the training process with guaranteed convergence. We present experimental results from the CoNLL 2003 named entity recognition (NER) task to demonstrate the performance of the proposed algorithm.	algorithm;maxima and minima;named-entity recognition;perceptron;sequence labeling;social inequality	Jun'ichi Kazama;Kentaro Torisawa	2007			computer science;machine learning;pattern recognition;data mining	NLP	22.55505811842942	-47.41278036394552	151845
b06c1badd507c84e312321049e17a29d6fc132b6	weighting schemes based on em algorithm for lda		Latent Dirichlet allocation (LDA) is a popular probabilistic topic modelling method, which automatically finds latent topics from a corpus. LDA users often encounter two major problems: first, LDA treats each word equally, and common words tend to scatter across almost all topics without reason, thereby leading to bad topic interpretability, consistency, and overlap. Second, an appropriate way to distinguish low-dimensional topic features for better classification performance is lacking. To overcome these two shortcomings, we propose two novel weighting schemes: a word-weighted scheme, which is realised by introducing a weight factor during the iterative process, and a topic-weighted scheme, which is realised by combining the Jenson-Shannon (JS) distance and the entropy of the generated low-dimensional topic features as a weight coefficient, using expectation-maximisation (EM). Experimental results show that the word-weighted scheme can find better topics for improving the clustering performance effective...	expectation–maximization algorithm;local-density approximation	Yaya Ju;Jianfeng Yan;Zhi-Qiang Liu;Lu Yang	2018	IJHPSA	10.1504/IJHPSA.2018.10015209	latent dirichlet allocation;topic model;parallel computing;computer science;data mining;probabilistic logic;expectation–maximization algorithm;cluster analysis;iterative and incremental development;interpretability;weighting	NLP	17.388832862608012	-41.87971980935635	152237
d5cf73f3d4d165f3aa51f1492caefc1d0a52a743	the generalized condensed nearest neighbor rule as a data reduction method	asymptotic bayes risk efficiency;bayes methods;generalized condensed nearest neighbor rule;data reduction bayes methods;bayes risk;nearest neighbor;data reduction;nearest neighbor searches prototypes cellular neural networks clustering algorithms iterative algorithms absorption support vector machines information science employment learning systems;asymptotic bayes risk efficiency generalized condensed nearest neighbor rule data reduction generalized condensed nearest neighbor algorithm;generalized condensed nearest neighbor algorithm	In this paper, we propose a new data reduction algorithm that iteratively selects some samples and ignores others that can be absorbed, or represented, by those selected. This algorithm differs from the condensed nearest neighbor (CNN) rule in its employment of a strong absorption criterion, in contrast to the weak criterion employed by CNN; hence, it is called the generalized CNN (GCNN) algorithm. The new criterion allows GCNN to incorporate CNN as a special case, and can achieve consistency, or asymptotic Bayes-risk efficiency, under certain conditions. GCNN, moreover, can yield significantly better accuracy than other instance-based data reduction methods. We demonstrate the last claim through experiments on five datasets, some of which contain a very large number of samples	algorithm;cluster analysis;eisenstein's criterion;experiment;instance-based learning;resultant;risk aversion	Chien-Hsing Chou;Bo-Han Kuo;Fu Chang	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.1119	nearest-neighbor chain algorithm;large margin nearest neighbor;nearest neighbor graph;data reduction;best bin first;computer science;machine learning;pattern recognition;nearest neighbor search;k-nearest neighbors algorithm;statistics	DB	16.775766990818617	-39.732564276083316	152265
abb09cd924d99e7f0ea6d728e55ed9848d39935b	feature selection based on minimum error minimax probability machine	minimum error minimax probability machine;classification;minimax probability machine;feature selection;support vector machine	Feature selection is an important task in pattern recognition. Support Vector Machine (SVM) and Minimax Probability Machine (MPM) have been successfully used as the classification framework for feature selection. However, these paradigms cannot automatically control the balance between prediction accuracy and the number of selected features. In addition, the selected feature subsets are also not stable in different data partitions. Minimum Error Minimax Probability Machine (MEMPM) has been proposed for classification recently. In this paper, we outline MEMPM to select the optimal feature subset with good stability and automatic balance between prediction accuracy and the size of feature subset. The experiments against feature selection with SVM and MPM show the advantages of the proposed MEMPM formulation in stability and automatic balance between the feature subset size and the prediction accuracy.	feature selection;minimax	Zenglin Xu;Irwin King;Michael R. Lyu	2007	IJPRAI	10.1142/S0218001407005958	support vector machine;minimum redundancy feature selection;feature vector;feature;biological classification;computer science;machine learning;pattern recognition;data mining;mathematics;feature selection;feature	AI	11.964666973899394	-42.3525740396457	152293
a2ece01d5fa8aa295f3c2f576f0431bb1b76d450	research on online learning of radar emitter recognition based on hull vector		In view of the problem that support vector machine classifier learning new knowledge is poor in real time, a new algorithm of radar emitter identification based on hull vector and Parzen window density estimation is studied. The algorithm takes the Parzen window density estimation to eliminate the outliers, and reduces the training time by using the hull vector of the sample set. The identification of radar signal samples is finished by using the trained classifier. The simulation results show that the online learning algorithm based on hull vector and Parzen window density estimation has good timeliness and high recognition rate.	algorithm;kernel density estimation;lambert's cosine law;learning classifier system;radar;requirement;simulation;support vector machine;window function	Weigang Zhu;Meng Li;Chuangzhan Zeng	2017	2017 IEEE Second International Conference on Data Science in Cyberspace (DSC)	10.1109/DSC.2017.30	support vector machine;density estimation;algorithm design;hull;radar;kernel density estimation;outlier;machine learning;statistical classification;artificial intelligence;pattern recognition;computer science	Robotics	16.5340623191693	-42.43350803198638	152297
2dce40f817175b64536ec87adcbd33e35362c429	dataset structure as prior information for parameter-free regularization of extreme learning machines	extreme learning machines;regularization;affinity matrices	This paper proposes a novel regularization approach for Extreme Learning Machines. Regularization is performed using a priori spatial information expressed by an affinity matrix. We show that the use of this type of a priori information is similar to perform Tikhonov regularization. Furthermore, if a parameter free affinity matrix is used, like the cosine similarity matrix, regularization is performed without any need for parameter tuning. Experiments are performed using classification problems to validate the proposed approach. & 2015 Elsevier B.V. All rights reserved.	affinity analysis;cosine similarity;matrix regularization;processor affinity;similarity measure;statistical classification	Leonardo José Silvestre;André Paim Lemos;João Pedro Braga;Antônio de Pádua Braga	2015	Neurocomputing	10.1016/j.neucom.2014.11.080	regularization perspectives on support vector machines;backus–gilbert method;regularization;mathematical optimization;proximal gradient methods for learning;computer science;machine learning;pattern recognition;mathematics;tikhonov regularization	AI	23.14296515478791	-41.56196665051794	152459
8cd9852ee0defbf3a19210afc09f87568dba3086	robust covariate shift regression		In many learning settings, the source data available to train a regression model differs from the target data it encounters when making predictions due to input distribution shift. Appropriately dealing with this situation remains an important challenge. Existing methods attempt to “reweight” the source data samples to better represent the target domain, but this introduces strong inductive biases that are highly extrapolative and can often err greatly in practice. We propose a robust approach for regression under covariate shift that embraces the uncertainty resulting from sample selection bias by producing regression models that are explicitly robust to it. We demonstrate the benefits of our approach on a number of regression tasks.	norm (social);selection bias;source data	Xiangli Chen;Miguel Pozuelo Monfort;Anqi Liu;Brian D. Ziebart	2016			covariate	AI	17.755340929280226	-38.22193917211765	152712
f9252a1e3488deb86f3e7550b40663da6c21fd48	automated parameter tuning in one-class support vector machine: an application for damage detection		Machine learning algorithms have been employed extensively in the area of structural health monitoring to compare new measurements with baselines to detect any structural change. One-class support vector machine (OCSVM) with Gaussian kernel function is a promising machine learning method which can learn only from one-class data and then classify any new query samples. However, generalization performance of OCSVM is profoundly influenced by its Gaussian model parameter $$\sigma $$ σ . This paper proposes a new algorithm named appropriate distance to the enclosing surface (ADES) for tuning the Gaussian model parameter. The semantic idea of this algorithm is based on inspecting the spatial locations of the edge and interior samples, and their distances to the enclosing surface of OCSVM. The algorithm selects the optimal value of $$\sigma $$ σ which generates a hyperplane that is maximally distant from the interior samples but close to the edge samples. The sets of interior and edge samples are identified using a hard margin linear support vector machine. The algorithm was successfully validated using sensing data collected from the Sydney Harbour Bridge and vehicle-mounted sensors for damage detection, in addition to five public data sets. The results obtained by ADES are compared to those of variance–mean, maximum distance and MIES methods. The results of the ADES approach outperform these state-of-the-art methods, especially on the bridge and road data sets. Experiments on these data sets show that an average 3% better accuracy is achieved by ADES over these state-of-the-art methods. The designed ADES algorithm is an appropriate choice to identify the optimal value of $$\sigma $$ σ for OCSVM, especially in high-dimensional data sets.	baseline (configuration management);benchmark (computing);experiment;genetic algorithm;information privacy;loss function;machine learning;mean squared error;optimization problem;sensor;support vector machine	Ali Anaissi;Nguyen Lu Dang Khoa;Yang Wang	2018	International Journal of Data Science and Analytics	10.1007/s41060-018-0151-9	anomaly detection;hyperplane;gaussian function;support vector machine;structural health monitoring;structural change;mathematics;pattern recognition;artificial intelligence;data set	ML	16.88811663438772	-42.13985664963954	152768
f913bb65b62b0a6391ffa8f59b1d5527b7eba948	on improving robustness of lda and srda by using tangent vectors	subspace learning;articulo;lda;dimensionality reduction;tangent vectors;srda	In the area of pattern recognition, it is common for few training samples to be available with respect to the dimensionality of the representation space; this is known as the curse of dimensionality. This problem can be alleviated by using a dimensionality reduction approach, which overcomes the curse relatively well. Moreover, supervised dimensionality reduction techniques generally provide better recognition performance; however, several of these tend to suffer from the curse when applied directly to high-dimensional spaces. We propose to overcome this problem by incorporating additional information to supervised subspace learning techniques using what is known as tangent vectors. This additional information accounts for the possible differences that the sample data can suffer. In fact, this can be seen as a way to model the unseen data and make better use of the scarce training samples. In this paper, methods for incorporating tangent vector information are described for one classical technique (LDA) and one state-of-the-art technique (SRDA). Experimental results confirm that this additional information improves performance and robustness to known transformations.	curse of dimensionality;dimensionality reduction;local-density approximation;pattern recognition;robustness (computer science);supervised learning	Mauricio Villegas;Roberto Paredes	2013	Pattern Recognition Letters	10.1016/j.patrec.2013.03.001	tangent vector;computer science;machine learning;pattern recognition;data mining;mathematics;dimensionality reduction	Vision	23.75260137683058	-40.96242287598116	152945
5120fbb4b42b5f2b9b05b5a6d39206eaa72cffc1	revisiting rcnn: on awakening the classification power of faster rcnn		Recent region-based object detectors are usually built with separate classification and localization branches on top of shared feature extraction networks. In this paper, we analyze failure cases of state-ofthe-art detectors and observe that most hard false positives result from classification instead of localization. We conjecture that: (1) Shared feature representation is not optimal due to the mismatched goals of feature learning for classification and localization; (2) multi-task learning helps, yet optimization of the multi-task loss may result in sub-optimal for individual tasks; (3) large receptive field for different scales leads to redundant context information for small objects. We demonstrate the potential of detector classification power by a simple, effective, and widely-applicable Decoupled Classification Refinement (DCR) network. DCR samples hard false positives from the base classifier in Faster RCNN and trains a RCNN-styled strong classifier. Experiments show new stateof-the-art results on PASCAL VOC and COCO without any bells and whistles.		Bowen Cheng;Yunchao Wei;Honghui Shi;Rogério Schmidt Feris;Jinjun Xiong;Thomas S. Huang	2018		10.1007/978-3-030-01267-0_28	machine learning;artificial intelligence;computer science;object detection;feature extraction;pattern recognition;coco;receptive field;false positive paradox;feature learning	ML	21.38857973644978	-51.80463935043664	153057
1a963ba172d6aca8ebf36f3e0668661f9ec15604	probabilistic confusion entropy for evaluating classifiers	probabilistic confusion entropy;confusion entropy;multi class classification	For evaluating the classification model of an information system, a proper measure is usually needed to determine if the model is appropriate for dealing with the specific domain task. Though many performance measures have been proposed, few measures were specially defined for multi-class problems, which tend to be more complicated than two-class problems, especially in addressing the issue of class discrimination power. Confusion entropy was proposed for evaluating classifiers in the multi-class case. Nevertheless, it makes no use of the probabilities of samples classified into different classes. In this paper, we propose to calculate confusion entropy based on a probabilistic confusion matrix. Besides inheriting the merit of measuring if a classifier can classify with high accuracy and class discrimination power, probabilistic confusion entropy also tends to measure if samples are classified into true classes and separated from others with high probabilities. Analysis and experimental comparisons show the feasibility of the simply improved measure and demonstrate that the measure does not stand or fall over the classifiers on different datasets in comparison with the compared measures.	approximation error;benchmark (computing);bittorrent protocol encryption;confusion matrix;information system;linear discriminant analysis;mean squared error	Xiao-Ning Wang;Jin-Mao Wei;Han Jin;Gang Yu;Hai-Wei Zhang	2013	Entropy	10.3390/e15114969	machine learning;multiclass classification;pattern recognition;mathematics;statistics	ML	11.807512920616597	-38.48564595718269	153194
ecb1cc5d51cd03390134d96944ba017334232ab9	an instance selection algorithm for regression and its application in variance reduction	variance reduction instance selection regression problems genetic fuzzy systems gfss;ad hoc data driven method instance selection algorithm variance reduction machine learning reduced training error genetic fuzzy systems gfs is class conditional instance selection for regression ccisr;fuzzy set theory;genetic algorithms;regression analysis;regression analysis fuzzy set theory genetic algorithms learning artificial intelligence;learning artificial intelligence;training measurement uncertainty accuracy evolutionary computation complexity theory machine learning algorithms genetics	The tradeoff between bias and variance is a well-known problem in machine learning, since algorithms are expected to achieve a reduced training error without going into overfitting. In Genetic Fuzzy Systems (GFSs), overfitting is usually avoided through the control of the number of rules and/or the number of labels. However, in many machine learning approaches, variance is reduced through the use of a validation set. Inspired by this idea, we propose in this paper an Instance Selection (IS) algorithm for regression problems called Class Conditional Instance Selection for Regression (CCISR) which is based on CCIS [1]. The output of CCISR is used in a GFS to obtain Rule Bases with a low variance, as the rules are generated with an ad hoc data driven method guided by the selected instances, but the error is still measured with the full training dataset. The combined system has been tested over 12 publicly available datasets, and results were compared with other GFSs. Our approach is capable of achieving a reduction in the number of rules while maintaining a good accuracy.	fuzzy control system;fuzzy rule;genetic algorithm;genetic fuzzy systems;global forecast system;hoc (programming language);k-means clustering;knowledge base;machine learning;overfitting;preprocessor;selection algorithm;variance reduction	Ismael Rodríguez-Fdez;Manuel Mucientes;Alberto Bugarín	2013	2013 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2013.6622486	instance-based learning;genetic algorithm;test set;computer science;artificial intelligence;machine learning;pattern recognition;data mining;fuzzy set;regression analysis;generalization error	Robotics	11.052297747854738	-40.49522223424658	153635
75003c069da53911f714d8d28b121ed9b29e0911	sort: second-order response transform for visual recognition		In this paper, we reveal the importance and benefits of introducing second-order operations into deep neural networks. We propose a novel approach named Second-Order Response Transform (SORT), which appends element-wise product transform to the linear sum of a two-branch network module. A direct advantage of SORT is to facilitate cross-branch response propagation, so that each branch can update its weights based on the current status of the other branch. Moreover, SORT augments the family of transform operations and increases the nonlinearity of the network, making it possible to learn flexible functions to fit the complicated distribution of feature space. SORT can be applied to a wide range of network architectures, including a branched variant of a chain-styled network and a residual network, with very light-weighted modifications. We observe consistent accuracy gain on both small (CIFAR10, CIFAR100 and SVHN) and big (ILSVRC2012) datasets. In addition, SORT is very efficient, as the extra computation overhead is less than 5%.	artificial neural network;computation;convolutional neural network;deep learning;feature vector;flow network;nonlinear system;overhead (computing);perturbation theory;software propagation	Yan Wang;Lingxi Xie;Chenxi Liu;Ya Zhang;Wenjun Zhang;Alan L. Yuille	2017	2017 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2017.152	machine learning;counting sort;theoretical computer science;artificial intelligence;proxmap sort;tree sort;in-place algorithm;adaptive sort;computer science;sort;selection sort;sorting algorithm	Vision	21.370045653348527	-48.75614872891941	153706
c78fe0423be0c1507d3eab1f76089ffeb080d525	an evolutionary algorithm for making decision graphs for classification problems	evolutionary computation;classification;decision graph;multi root nodes;majority vote	As the exponential increase of data in the world, machine learning, pattern recognition, data mining etc. are attracting more attentions recently. Classification is one of the major research in pattern recognition and a large number of methods have been proposed such as decision trees, neural networks (NNs), support vector machines (SVMs). In order to easily understand and analyze the reason of the classification results, decision trees are useful comparing to NNs and SVMs. In this paper, to enhance the classification ability of decision trees, a new evolutionary algorithm for creating decision graphs is proposed as a superset of decision trees, where multi-root nodes and majority voting mechanism based on Maximum a posteriori are introduced. In the performance evaluation, it is clarified that the proposed method shows better classification ability than decision trees.	artificial neural network;data mining;decision tree;evaluation function;evolutionary algorithm;genetic operator;machine learning;pattern recognition;performance evaluation;simulation;support vector machine;time complexity	Shingo Mabu;Masanao Obayashi;Takashi Kuremoto	2016	JRNAL	10.2991/jrnal.2016.3.1.11	mathematical optimization;interactive evolutionary computation;machine learning;pattern recognition;mathematics	ML	10.90085428155328	-40.483393071775026	153846
dcad214accce927a1e3f889207b2e56f380fc7b9	large-scale maximum margin discriminant analysis using core vector machines	estensibilidad;complejidad espacio;kernel principal component analysis;analisis componente principal;kernel;support vector machines computational complexity feature extraction principal component analysis;approximate algorithm;analisis estadistico;support vector machines;time complexity;cubico;methode noyau;base donnee tres grande;scalability feature extraction support vector machines svms core vector machines;approximation algorithm;extraction forme;maximum margin discriminant analysis;large data sets;algorithms computer simulation discriminant analysis humans models statistical neural networks computer principal component analysis signal processing computer assisted time factors;indexing terms;data mining;support vector;journal article;signal processing computer assisted;discriminant analysis;analyse discriminante;classification a vaste marge;large scale;analisis discriminante;complexite temps;cubique;time factors;statistical analysis;extraccion forma;kernel fisher discriminant analysis;computational complexity;feature extraction;principal component analysis;metodo nucleo;analyse statistique;algoritmo aproximacion;analyse composante principale;space complexity;pattern recognition;large margin methods;component analysis;models statistical;councils;support vector machine classification;algorithms;kernel method;large scale systems support vector machines feature extraction kernel support vector machine classification data mining principal component analysis scalability information analysis councils;humans;core vector machines;extensibilite;scalability;reconnaissance forme;support vector machine;extraction caracteristique;maquina ejemplo soporte;very large databases;vector support machine;complexite espace;reseau neuronal;neural networks computer;reconocimiento patron;vector processor;algorithme approximation;complejidad tiempo;classification accuracy;information analysis;computer simulation;pattern extraction;support vector machines svms;red neuronal;cubics	Large-margin methods, such as support vector machines (SVMs), have been very successful in classification problems. Recently, maximum margin discriminant analysis (MMDA) was proposed that extends the large-margin idea to feature extraction. It often outperforms traditional methods such as kernel principal component analysis (KPCA) and kernel Fisher discriminant analysis (KFD). However, as in the SVM, its time complexity is cubic in the number of training points m, and is thus computationally inefficient on massive data sets. In this paper, we propose an (1 + isin)2-approximation algorithm for obtaining the MMDA features by extending the core vector machine. The resultant time complexity is only linear in m, while its space complexity is independent of m. Extensive comparisons with the original MMDA, KPCA, and KFD on a number of large data sets show that the proposed feature extractor can improve classification accuracy, and is also faster than these kernel-based methods by over an order of magnitude.	algorithm;cubic function;dspace;extractor device component;feature extraction;feature model;kernel principal component analysis;linear discriminant analysis;mmda;randomness extractor;resultant;support vector machine;time complexity	Ivor W. Tsang;András Kocsor;James T. Kwok	2008	IEEE Transactions on Neural Networks	10.1109/TNN.2007.911746	computer simulation;support vector machine;kernel fisher discriminant analysis;computer science;machine learning;pattern recognition;data mining;approximation algorithm;artificial neural network	ML	23.144560778513913	-39.09900166891635	153918
2b484e25e1cee457b3b9ec5a94d7281372ef19c7	generalized functional relevance learning vector quantization		Generalized learning vector quantization (GRLVQ) is a prototype based classification algorithm with metric adaptation weighting each data dimensions according to their relevance for the classification task. We present in this paper an extension for functional data, which are usually very high dimensional. This approach supposes the data vectors have to be functional representations. Taking into account, these information the so-called relevance profile are modeled by superposition of simple basis functions depending on only a few parameters. As a consequence, the resulting functional GRLVQ has drastically reduced number of parameters to be adapted for relevance learning. We demonstrate the ability of the new algorithms for standard functional data sets using different basis functions, namely Gaussians and Lorentzians.	algorithm;basis function;learning vector quantization;mental representation;prototype;relevance	Marika Kaden;Barbara Hammer;Michael Biehl;Thomas Villmann	2011			learning vector quantization;machine learning;pattern recognition;relevance vector machine	ML	16.770705533761145	-44.95387757535261	153987
bfd1ac5dadff18600a4389dd255565d9ebfab506	doubly nested network for resource-efficient inference		We propose a new anytime neural network which allows partial evaluation by subnetworks with different widths as well as depths. Compared to conventional anytime networks only with the depth controllability, the increased architectural diversity leads to higher resource utilization and consequent performance improvement under various and dynamic resource budgets. We highlight architectural features to make our scheme feasible as well as efficient, and show its effectiveness in image classification tasks.	anytime algorithm;artificial neural network;computer vision;partial evaluation	Jaehong Kim;Sungeun Hong;Yongseok Choi;Jiwon Kim	2018	CoRR		software deployment;machine learning;slicing;mathematics;computation;artificial intelligence;convolution;inference;communication channel;sorting;budget constraint	NLP	22.802659568343824	-51.92706525111804	154091
339ab3a06c26c7eb1ed121f6469909a04235a668	weighted margin multi-class core vector machines	support vector machines data handling pattern classification;weighted margin support vector machine;kernel;support vector machines;approximation algorithms;support vector machines support vector machine classification scalability quadratic programming kernel approximation algorithms educational institutions space technology computational intelligence computer security;training;large data sets;prior knowledge;wmmcvm;large data sets handling;large data sets handling weighted margin multiclass core vector machines weighted margin support vector machine wmsvm wmmcvm center constrained meb problem;center constrained meb problem;machine learning;classification algorithms;pattern classification;weighted margin multiclass core vector machines;optimization;support vector machine;data handling;wmsvm	The incorporation of prior knowledge into SVMs for classification is the key element that allows increasing the performance to many applications. Wu proposed weighted margin support vector machine (WMSVM), the scalability aspect of the approach to handle large data sets still needs much of exploration. In this paper, we describe a generalization of weighted margin multi-class core vector machine (WMMCVM) which views the weighted margin multi-class SVM as a center-constrained MEB Problem, so the QP problem is then solved using the CVM technique to achieve scalability to handle large data sets. Experimental results indicate that the proposed WMMCVM technique gives a better performance than the original one.	scalability;support vector machine	Yuan Yuan;Bing Chen;Jiandong Wang;Liming Fang;Tao Xu	2008	2008 International Conference on Computational Intelligence and Security	10.1109/CIS.2008.27	statistical classification;support vector machine;margin;computer science;machine learning;pattern recognition;data mining;approximation algorithm	ML	20.571045380820582	-41.38847065369868	154107
973dc50483dd51bf347646b410973f5303876768	symbolic modeling of genetic regulatory networks	genetic regulatory network;gene networks;symbolic execution;qualitative dynamical models;model checking;temporal properties	Understanding the functioning of genetic regulatory networks supposes a modeling of biological processes in order to simulate behaviors and to reason on the model. Unfortunately, the modeling task is confronted to incomplete knowledge about the system. To deal with this problem we propose a methodology that uses the qualitative approach developed by Thomas. A symbolic transition system can represent the set of all possible models in a concise and symbolic way. We introduce a new method based on model-checking techniques and symbolic execution to extract constraints on parameters leading to dynamics coherent with known behaviors. Our method allows us to efficiently respond to two kinds of questions: is there any model coherent with a certain hypothetic behavior? Are there behaviors common to all selected models? The first question is illustrated with the example of the mucus production in Pseudomonas aeruginosa while the second one is illustrated with the example of immunity control in bacteriophage lambda.	bacteriophages;biological processes;checking (action);coherence (physics);constraint (mathematics);description;epigenetic process;experiment;gene regulatory network;interaction;license;lysogeny;model checking;partial;population parameter;pseudomonas aeruginosa ab:acnc:pt:ser:qn;rem sleep behavior disorder;simulation;symbolic execution;transition system;study of epigenetics	Daniel Mateus;Jean-Pierre Gallois;Jean-Paul Comet;Pascale Le Gall	2007	Journal of bioinformatics and computational biology	10.1142/S0219720007002850	model checking;biology;gene regulatory network;computer science;bioinformatics;artificial intelligence;theoretical computer science;machine learning;mathematics;genetics;algorithm;statistics	Comp.	12.697938494773158	-51.76701015071543	154201
047c99194952a34c094662359236642848b698ac	combining discrete svm and fixed cardinality warping distances for multivariate time series classification	alternative method;evaluation performance;warping distance;discrete support vector machines;performance evaluation;supervised learning;support vector machines;gauchissement;evaluacion prestacion;maquina vector soporte;rectangular shape;time series;etiquetage;etiquetaje;multivariate time series;machine vecteur support;methode alternative;metodo alternativo;signal classification;serie temporelle;torcimiento;serie temporal;labelling;classification signal;support vector machine;apprentissage supervise;time series classification;classification automatique;automatic classification;learning theory;aprendizaje supervisado;clasificacion automatica;warping;forma rectangular;forme rectangulaire	Time series classification is a supervised learning problem aimed at labeling temporally structured multivariate sequences of variable length. The most common approach reduces time series classification to a static problem by suitably transforming the set of multivariate input sequences into a rectangular table composed by a fixed number of columns. Then, one of the alternative efficient methods for classification is applied for predicting the class of new temporal sequences. In this paper, we propose a new classification method, based on a temporal extension of discrete support vector machines, that benefits from the notions of warping distance and softened variable margin. Furthermore, in order to transform a temporal dataset into a rectangular shape, we also develop a new method based on fixed cardinality warping distances. Computational tests performed on both benchmark and real marketing temporal datasets indicate the effectiveness of the proposed method in comparison to other techniques. & 2010 Elsevier Ltd. All rights reserved.	benchmark (computing);column (database);computation;geo warping;mathematical optimization;matrix regularization;nonlinear system;similarity measure;supervised learning;support vector machine;time series;two-phase commit protocol	Carlotta Orsenigo;Carlo Vercellis	2010	Pattern Recognition	10.1016/j.patcog.2010.06.005	support vector machine;speech recognition;computer science;machine learning;pattern recognition;mathematics;supervised learning	ML	23.668138717154516	-39.015948240473556	154410
e81064bbb827a3fc911f1ea2fdbb28f32a8e90a5	a nearest prototype selection algorithm using multi-objective optimization and partition	set theory divide and conquer methods genetic algorithms learning artificial intelligence pattern classification;divide and conquer partition machine learning prototype selection multi objective optimization genetic algorithm;divide and conquer partition;prototype selection;multi objective optimization;set theory;divide and conquer methods;prototypes algorithm design and analysis classification algorithms genetic algorithms partitioning algorithms optimization accuracy;machine learning;pattern classification;divided random subsets nearest prototype selection algorithm multiobjective optimization nearest neighbor classification algorithms genetic algorithms prediction classification divide and conquer partition random subsets;genetic algorithm;genetic algorithms;learning artificial intelligence	Prototype selection aims at reducing the scale of datasets to improve prediction accuracy and operation efficiency by removing noisy or redundant patterns via the nearest neighbor classification algorithms. Genetic algorithms have been used recently for prototype selection and showed good performance, however, they have some drawbacks such as the deteriorated running effect, slow convergence for the large datasets. The goal of designing good prototype selection algorithm is to maximize prediction classification accuracy and minimize the reduction ratio simultaneously. According to this goal, a two objective optimization model is set up for prototype selection problem. To make the model easier to be solved, the model is transformed to a single objective optimization model by the division of the two objectives. To solve the problem efficiently, a new prototype selection algorithm based on genetic algorithm and the divide-and-conquer partition is presented. The divide-and-conquer partition can divide the whole dataset into some random subsets, and thus make the problems more easier. Then, the genetic algorithm is specifically designed on these divided random subsets. The simulation results indicate that the proposed algorithm can obtain smaller reduction ratio and higher classification efficiency, or at least comparable to those of some existing compared algorithms. This illustrates that the proposed algorithm is an expedient method in design nearest neighbor classifiers.	genetic algorithm;mathematical optimization;multi-objective optimization;nearest-neighbor interpolation;prototype;selection algorithm;simulation;time complexity	Juan Li;Yuping Wang	2013	2013 Ninth International Conference on Computational Intelligence and Security	10.1109/CIS.2013.62	quality control and genetic algorithms;mathematical optimization;genetic algorithm;hybrid algorithm;computer science;artificial intelligence;machine learning;pattern recognition;population-based incremental learning	AI	10.815612860449729	-41.140942270122196	154434
5288e1e7e914f73bf65c745f328844907226cd3e	learning deep binary descriptor with multi-quantization		In this paper, we propose an unsupervised feature learning method called deep binary descriptor with multi-quantization (DBD-MQ) for visual analysis. Existing learning-based binary descriptors utilize the rigid sign function for binarization despite of data distributions, which usually suffer from severe quantization loss. To this end, we propose a deep multi-quantization network to learn a data-dependent binarization in an unsupervised manner. More specifically, we design a K-Autoencoders (KAEs) network to jointly learn the parameters of feature extractor and the binarization functions under a deep learning framework, so that discriminative binary descriptors can be obtained with a fine-grained multi-quantization. As DBD-MQ simply allocates the same number of quantizers to each real-valued feature dimension ignoring the elementwise diversity of informativeness, we further propose a deep competitive binary descriptor with multi-quantization (DCBD-MQ) method to learn optimal allocation of bits with the fixed binary length in a competitive manner, where informative dimensions gain more bits for complete representation. Moreover, we present a similarity-aware binary encoding strategy based on the earth moveru0027s distance of Autoencoders, so that elements that are quantized into similar Autoencoders will have smaller Hamming distances. Extensive experimental results on six widely-used datasets show that our DBD-MQ and DCBD-MQ outperform most state-of-the-art unsupervised binary descriptors	autoencoder;binary code;binary image;deep learning;defective by design;feature learning;image registration;image retrieval;unsupervised learning	Yueqi Duan;Jiwen Lu;Ziwei Wang;Jianjiang Feng;Jie Zhou	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.516	deep learning;computer science;pattern recognition;artificial intelligence;quantization (signal processing);discriminative model;feature dimension;binary number;sign function;feature learning;hamming code	Vision	23.702118742101632	-50.19967347364173	154624
2fc859fd8e9cf665eb86f5655730d38065fb2adb	a preliminary study on automatic algorithm selection for short-term traffic forecasting		Despite the broad range of Machine Learning (ML) algorithms, there are no clear baselines to find the best method and its configuration given a Short-Term Traffic Forecasting (STTF) problem. In ML, this is known as the Model Selection Problem (MSP). Although Automatic Algorithm Selection (AAS) has proved success dealing with MSP in other areas, it has hardly been explored in STTF. This paper deepens into the benefits of AAS in this field. To this end, we have used Auto-WEKA, a well-known AAS method, and compared it to the general approach (which consists of selecting the best of a set of algorithms) over a multi-class imbalanced classification STTF problem. Experimental results show AAS as a promising methodology in this area and allow important conclusions to be drawn on how to improve the performance of ASS methods when dealing with STTF.	algorithm selection	Juan S. Angarita-Zapata;Isaac Triguero;Antonio D. Masegosa	2018		10.1007/978-3-319-99626-4_18	model selection;algorithm selection;machine learning;artificial intelligence;computer science	EDA	11.526462334494122	-40.32723074626408	154689
186071ab45581b6896456e5f5961e3b18584ad23	tensor decomposition for compressing recurrent neural network		In the machine learning fields, Recurrent Neural Network (RNN) has become a popular architecture for sequential data modeling. However, behind the impressive performance, RNNs require a large number of parameters for both training and inference. In this paper, we are trying to reduce the number of parameters and maintain the expressive power from RNN simultaneously. We utilize several tensor decompositions method including CANDECOMP/PARAFAC (CP), Tucker decomposition and Tensor Train (TT) to re-parameterize the Gated Recurrent Unit (GRU) RNN. We evaluate all tensor-based RNNs performance on sequence modeling tasks with a various number of parameters. Based on our experiment results, TT-GRU achieved the best results in a various number of parameters compared to other decomposition methods.	data modeling;expressive power (computer science);machine learning;random neural network;recurrent neural network;truth-table reduction;tucker decomposition	Andros Tjandra;Sakriani Sakti;Satoshi Nakamura	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8489213	artificial intelligence;machine learning;tensor;architecture;pattern recognition;tucker decomposition;data modeling;inference;recurrent neural network;stress (mechanics);matrix decomposition;computer science	ML	21.1531581409738	-48.02593459477897	154922
64c66431de01e8a7111dff4898a9791fd4b72f1f	multifocal learning for customer problem analysis	customer service support;multi focal learning;empirical study;classification algorithm;customer service;learning problems;prediction model;support vector machine	In this study, we formalize a multifocal learning problem, where training data are partitioned into several different focal groups and the prediction model will be learned within each focal group. The multifocal learning problem is motivated by numerous real-world learning applications. For instance, for the same type of problems encountered in a customer service center, the problem descriptions from different customers can be quite different. Experienced customers usually give more precise and focused descriptions about the problem. In contrast, inexperienced customers usually provide diverse descriptions. In this case, the examples from the same class in the training data can be naturally in different focal groups. Therefore, it is necessary to identify those natural focal groups and exploit them for learning at different focuses. Along this line, the key development challenge is how to identify those focal groups in the training data. As a case study, we exploit multifocal learning for profiling customer problems. Also, we provide an empirical study about how the performance of multifocal learning is affected by the quality of focal groups. The results on real-world customer problem logs show that multifocal learning can significantly boost the performance of many existing classification algorithms, such as Support Vector Machines (SVMs), for classifying customer problems and there is strong correlation between the quality of focal groups and the learning performance.	algorithm;experience;focal (programming language);problem solving;support vector machine	Yong Ge;Hui Xiong;Wenjun Zhou;Siming Li;Ramendra K. Sahoo	2011	ACM TIST	10.1145/1961189.1961196	support vector machine;instance-based learning;simulation;computer science;machine learning;data mining;predictive modelling;empirical research	ML	15.743990146300884	-42.97390395500028	154953
d7cf832d96a2569c7f660a32ef4ee4703c5b8961	analysis of nutrition data by means of a matrix factorization method	matrix factorization;nutrition data;data analysis;body mass index bmi;feature selection;learning to rank	We present a factorization framework to analyze the data of a regression learning task with two peculiarities. First, inputs can be split into two parts that represent semantically significant entities. Second, the performance of regressors is very low. The basic idea of the approach presented here is to try to learn the ordering relations of the target variable instead of its exact value. Each part of the input is mapped into a common Euclidean space in such a way that the distance in the common space is the representation of the interaction of both parts of the input. The factorization approach obtains reliable models from which it is possible to compute a ranking of the features according to their responsibility in the variation of the target variable. Additionally, the Euclidean representation of data provides a visualization where metric properties have a clear semantics. We illustrate the approach with a case study: the analysis of a dataset about the variations of Body Mass Index for Age of children after a Food Aid Program deployed in poor rural communities in Southern México. In this case, the two parts of inputs are the vectorial representation of children and their diets. In addition to discovering latent information, the mapping of inputs allows us to visualize children and diets in a common metric space.	entity	Jorge Díez;Edna Gamboa;Teresita González de Cossío;Oscar Luaces;Thorsten Joachims;Antonio Bahamonde	2015	Progress in Artificial Intelligence	10.1007/s13748-015-0062-0	mathematical optimization;computer science;theoretical computer science;machine learning;data mining;data analysis;matrix decomposition;feature selection;learning to rank	ML	21.90851306644861	-43.697335132971716	155465
bf7f554ec8f5f44e9c942237effd282a16efecf6	salic: social active learning for image classification	social network services;reliability;training;set theory computational complexity image classification image denoising learning artificial intelligence;noise measurement;sample selection process salic social active learning image classification user tagged images binary classifier set social context human oracle oracle confidence contextual based indication computational complexity reduction training data informativeness agnostic learning approach;training data;user tagged images active learning image classification large scale multi modal fusion social context;training noise measurement context reliability social network services training data data models;context;data models	In this paper, we present SALIC, an active learning method for selecting the most appropriate user tagged images to expand the training set of a binary classifier. The process of active learning can be fully automated in this social context by replacing the human oracle with the images' tags. However, their noisy nature adds further complexity to the sample selection process since, apart from the images' informativeness (i.e., how much they are expected to inform the classifier if we knew their label), our confidence about their actual label should also be maximized (i.e., how certain the oracle is on the images' true contents). The main contribution of this work is in proposing a probabilistic approach for jointly maximizing the two aforementioned quantities. In the examined noisy context, the oracle's confidence is necessary to provide a contextual-based indication of the images' true contents, while the samples' informativeness is required to reduce the computational complexity and minimize the mistakes of the unreliable oracle. To prove this, first, we show that SALIC allows us to select training data as effectively as typical active learning, without the cost of manual annotation. Finally, we argue that the speed-up achieved when learning actively in this social context (where labels can be obtained without the cost of human annotation) is necessary to cope with the continuously growing requirements of large-scale applications. In this respect, we demonstrate that SALIC requires ten times less training data in order to reach the same performance as a straightforward informativeness-agnostic learning approach.	baseline (configuration management);binary classification;computational complexity theory;computer vision;expectation–maximization algorithm;online and offline;online machine learning;requirement;scalability;test set	Elisavet Chatzilari;Spiros Nikolopoulos;Yiannis Kompatsiaris;Josef Kittler	2016	IEEE Transactions on Multimedia	10.1109/TMM.2016.2565440	data modeling;computer vision;training set;computer science;noise measurement;machine learning;pattern recognition;data mining;reliability;world wide web;statistics	Vision	17.453524340843725	-39.293450392697075	155497
4dde8f0ff7c8453e2f2f687f64b0343405e3ff3f	multi-view laplacian twin support vector machines	multi view learning;semi supervised learning;laplacian support vector machines;laplacian twin support vector machines;twin support vector machines	Twin support vector machines are a recently proposed learning method for pattern classification. They learn two hyperplanes rather than one as in usual support vector machines and often bring performance improvements. Semi-supervised learning has attracted great attention in machine learning in the last decade. Laplacian support vector machines and Laplacian twin support vector machines have been proposed in the semi-supervised learning framework. In this paper, inspired by the recent success of multi-view learning we propose multi-view Laplacian twin support vector machines, whose dual optimization problems are quadratic programming problems. We further extend them to kernel multi-view Laplacian twin support vector machines. Experimental results demonstrate that our proposed methods are effective.	duality (optimization);kernel (operating system);machine learning;mathematical optimization;quadratic programming;semi-supervised learning;semiconductor industry;supervised learning;support vector machine	Xijiong Xie;Shiliang Sun	2014	Applied Intelligence	10.1007/s10489-014-0563-8	semi-supervised learning;margin classifier;least squares support vector machine;mathematical optimization;computer science;machine learning;data mining;relevance vector machine;active learning	AI	24.206536091310014	-40.934217788818074	155664
86f46e31036f20093e466bd65d70a501c191cf6c	multi-view spectral clustering via tensor-svd decomposition		Multi-view clustering has attracted considerable attention in recent years, some related approaches always use matrices to represent views, and model by capturing two dimensional structure among views. The critical deficiency of these work is ignoring the space structure information of all views, which results in the mediocre performance of clustering. In this paper, we propose a novel Tensor-SVD decomposition based Multi-view Spectral Clustering algorithm(TMSC) to iron out flaws. Our method firstly puts transition probability matrices of all views into a three-order tensor, which naturally reserves the whole structure information of data. Then it establishes a low multi-rank tensor model based on tensor-SVD decomposition by fully mining the complementary information among multiple views. Another difficulty in this paper is that the optimal objective of TMSC has a low multirank constraint on the transition probability tensor, and a probabilistic simplex constraint on each fiber of the tensor. To tackle this challenging problem, we design an optimization procedure based on the Augmented Lagrangian Multiplier scheme. Experimental results on real word datasets show that TMSC has superior clustering quality over several state-of-theart multi-view clustering approaches.	angular defect;augmented lagrangian method;cluster analysis;lagrange multiplier;markov chain;mathematical optimization;singular value decomposition;spectral clustering	Yan Zhang;Weiwei Yang;Bangtian Liu;Ge-Yang Ke;Yan Pan;Jian Yin	2017	2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI)	10.1109/ICTAI.2017.00081	artificial intelligence;tensor;cluster analysis;probabilistic logic;spectral clustering;computer science;pattern recognition;matrix (mathematics);matrix decomposition;singular value decomposition;augmented lagrangian method	Vision	24.473500086839568	-43.27189000603901	156034
472fef09854838b550e2ae2c1ef92c13160135ef	self-adaptive weighted extreme learning machine for imbalanced classification problems		A self-adaptive weighted extreme learning machine (SawELM) is proposed in this paper to deal with the imbalanced binary-class classification problems. SawELM calculates the output-layer weights based on a newly-designed self-adaptive mechanism which includes the following two modules: one is to gradually reduce the weights of wrongly-classified training instances and the other is to dynamically update the outputs of these wrongly-classified instances. On 50 imbalanced binary-class data sets selected from KEEL repository, we compare the accuracy, G-mean, and F-measure of SawELM with unweighted ELM (UnWELM) and weighted ELM (WELM). The experimental results show that the newly-designed self-adaptive mechanism is effective and SawELM obviously improves the imbalanced classification performance of WELM. SawWLM obtains the significantly higher G-mean and F-measure than UnWELM and WELM. Meanwhile, the accuracy of SawELM is better than WELM and comparable to UnWELM.		Hao Long;Yu-Lin He;Joshua Zhexue Huang;Qiang Wang	2017		10.1007/978-3-319-67274-8_11	extreme learning machine;computer science;data mining;machine learning;data set;artificial intelligence;pattern recognition	AI	12.728300578015117	-40.011115814728846	156112
c8648d2ff2e8ba080399a87fa5008344154c97a6	improved pseudo nearest neighbor classification	local mean based pseudo nearest neighbor rule;k nearest neighbor rule;期刊论文;pattern classification;pseudo nearest neighbor rule;local mean vector	k-Nearest neighbor (KNN) rule is a very simple and powerful classification algorithm. In this article, we propose a new KNN-based classifier, called the local mean-based pseudo nearest neighbor (LMPNN) rule. It is motivated by the local mean-based k-nearest neighbor (LMKNN) rule and the pseudo nearest neighbor (PNN) rule, with the aim of improving the classification performance. In the proposed LMPNN, the k nearest neighbors from each class are searched as the class prototypes, and then the local mean vectors of the neighbors are yielded. Subsequently, we attempt to find the local mean-based pseudo nearest neighbor per class by employing the categorical k local mean vectors, and classify the unknown query patten according to the distances between the query and the pseudo nearest neighbors. To assess the classification performance of the proposed LMPNN, it is compared with the competing classifiers, such as LMKNN and PNN, in terms of the classification error on thirty-two real UCI data sets, four artificial data sets and three image data sets. The comprehensively experimental results suggest that the proposed LMPNN classifier is a promising algorithm in pattern recognition.		Jianping Gou;Yongzhao Zhan;Yunbo Rao;Xiangjun Shen;Xiaoming Wang;Wu He	2014	Knowl.-Based Syst.	10.1016/j.knosys.2014.07.020	nearest-neighbor chain algorithm;large margin nearest neighbor;ball tree;nearest neighbor graph;best bin first;machine learning;pattern recognition;data mining;cover tree;nearest neighbor search;fixed-radius near neighbors;k-nearest neighbors algorithm	Vision	11.645613099056236	-42.85809098935654	156483
49b16d485d19466b34a3191c3368df6e41eea2a1	graph-based isometry invariant representation learning		Learning transformation invariant representations of visual data is an important problem in computer vision. Deep convolutional networks have demonstrated remarkable results for image and video classification tasks. However, they have achieved only limited success in the classification of images that undergo geometric transformations. In this work we present a novel Transformation Invariant Graph-based Network (TIGraNet), which learns graph-based features that are inherently invariant to isometric transformations such as rotation and translation of input images. In particular, images are represented as signals on graphs, which permits to replace classical convolution and pooling layers in deep networks with graph spectral convolution and dynamic graph pooling layers that together contribute to invariance to isometric transformations. Our experiments show high performance on rotated and translated images from the test set compared to classical architectures that are very sensitive to transformations in the data. The inherent invariance properties of our framework provide key advantages, such as increased resiliency to data variability and sustained performance with limited training sets.	algorithm;benchmark (computing);color;computer vision;convolution;convolutional neural network;experiment;feature learning;graph (discrete mathematics);heart rate variability;isometric projection;linear algebra;machine learning;map;rendering (computer graphics);signal processing;spatial variability;test set	Renata Khasanova;Pascal Frossard	2017			computer vision;combinatorics;discrete mathematics;machine learning;pattern recognition;mathematics	ML	24.254080160995976	-51.85952128722045	156794
226327132e1a61ef4e32d55124857f1a44e6ddc2	image-to-class distance metric learning for image classification	gradient descent method;distance function;distance metric learning;naive bayes;image classification;convex optimization;optimization problem;performance improvement;local features;nearest neighbor	Image-To-Class (I2C) distance is first used in Naive-Bayes Nearest-Neighbor (NBNN) classifier for image classification and has successfully handled datasets with large intra-class variances. However, the performance of this distance relies heavily on the large number of local features in the training set and test image, which need heavy computation cost for nearest-neighbor (NN) search in the testing phase. If using small number of local features for accelerating the NN search, the performance will be poor. In this paper, we propose a large margin framework to improve the discrimination of I2C distance especially for small number of local features by learning Per-Class Mahalanobis metrics. Our I2C distance is adaptive to different class by combining with the learned metric for each class. These multiple Per-Class metrics are learned simultaneously by forming a convex optimization problem with the constraints that the I2C distance from each training image to its belonging class should be less than the distance to other classes by a large margin. A gradient descent method is applied to efficiently solve this optimization problem. For efficiency and performance improved, we also adopt the idea of spatial pyramid restriction and learning I2C distance function to improve this I2C distance. We show in experiments that the proposed method can significantly outperform the original NBNN in several prevalent image datasets, and our best results can achieve state-of-the-art performance on most datasets.	caltech 101;computation;computer vision;convex optimization;experiment;gradient descent;mathematical optimization;naive bayes classifier;nearest-neighbor interpolation;optimization problem;standard test image;test set	Zhengxiang Wang;Yiqun Hu;Liang-Tien Chia	2010		10.1007/978-3-642-15549-9_51	gradient descent;optimization problem;mathematical optimization;contextual image classification;convex optimization;naive bayes classifier;metric;computer science;machine learning;pattern recognition;mathematics;distance transform;k-nearest neighbors algorithm	Vision	22.021028712474664	-40.952967839919395	156834
2dc01c711e3b473c85e883ab6a7aef9359cb4dac	on kernel difference-weighted k-nearest neighbor classification	optimisation sous contrainte;constrained optimization;correlacion;information structure;structure information;methode noyau;approximation plus proche voisin;kernel methods;estructura informacion;optimizacion con restriccion;vecino mas cercano;classifier;mathematical programming;metodo nucleo;nearest neighbor;distance weighted knn;pattern classification;pattern recognition;plus proche voisin;nearest neighbour;kernel method;k nearest neighbor;reconnaissance forme;correlation;reconocimiento patron;classification accuracy;constrained optimization problem;programmation mathematique;programacion matematica;nearest neighbor approximation;nearest neighbor distance weighted knn;classification forme	Nearest neighbor (NN) rule is one of the simplest and the most important methods in pattern recognition. In this paper, we propose a kernel difference-weighted k-nearest neighbor (KDF-KNN) method for pattern classification. The proposed method defines the weighted KNN rule as a constrained optimization problem, and we then propose an efficient solution to compute the weights of different nearest neighbors. Unlike traditional distance-weighted KNN which assigns different weights to the nearest neighbors according to the distance to the unclassified sample, difference-weighted KNN weighs the nearest neighbors by using both the correlation of the differences between the unclassified sample and its nearest neighbors. To take into account the effective nonlinear structure information, we further extend difference-weighted KNN to its kernel version KDF-KNN. Our experimental results indicate that KDF-WKNN is much better than the original KNN and the distance-weighted KNN methods, and is comparable to or better than several state-of-the-art methods in terms of classification accuracy.	constrained optimization;constraint (mathematics);direction finding;k-nearest neighbors algorithm;kernel (operating system);key derivation function;mathematical optimization;nonlinear system;optimization problem;p/poly;pattern recognition;polynomial;radial basis function;reconstruction conjecture;rule 184	Wangmeng Zuo;David Zhang;Kuanquan Wang	2007	Pattern Analysis and Applications	10.1007/s10044-007-0100-z	kernel method;constrained optimization;computer science;machine learning;pattern recognition;k-nearest neighbors algorithm;statistics	Vision	23.293733546029312	-39.568948540078985	156900
6a531a763e6a109284ac27a51791c3e8296e1503	domain adaptation: overfitting and small sample statistics	small samples;science learning;data dependence;theoretical analysis;feature selection;similarity function	"""We study the prevalent problem when a test distribution differs from the training distribution. We consider a setting where our training set consists of a small number of sample domains, but where we have many samples in each domain. Our goal is to generalize to a new domain. For example, we may want to learn a similarity function using only certain classes of objects, but we desire that this similarity function be applicable to object classes not present in our training sample (e.g. we might seek to learn that """" dogs are similar to dogs """" even though images of dogs were absent from our training set). Our theoretical analysis shows that we can select many more features than domains while avoiding overfitting by utilizing data-dependent variance properties. We present a greedy feature selection algorithm based on using T-statistics. Our experiments validate this theory showing that our T-statistic based greedy feature selection is more robust at avoiding overfitting than the classical greedy procedure."""	data dependency;domain adaptation;experiment;feature selection;greedy algorithm;overfitting;selection algorithm;similarity measure;test set	Dean P. Foster;Sham M. Kakade;Ruslan Salakhutdinov	2011	CoRR		computer science;machine learning;pattern recognition;data mining;mathematics;feature selection;statistics	ML	17.33135681986927	-39.67678332515508	156973
a8b2c73f7c19f4e6e3783a5c19304025d9b7025f	learning to attack: adversarial transformation networks		With the rapidly increasing popularity of deep neural networks for image recognition tasks, a parallel interest in generating adversarial examples to attack the trained models has arisen. To date, these approaches have involved either directly computing gradients with respect to the image pixels or directly solving an optimization on the image pixels. We generalize this pursuit in a novel direction: can a separate network be trained to efficiently attack another fully trained network? We demonstrate that it is possible, and that the generated attacks yield startling insights into the weaknesses of the target network. We call such a network an Adversarial Transformation Network (ATN). ATNs transform any input into an adversarial attack on the target network, while being minimally perturbing to the original inputs and the target network’s outputs. Further, we show that ATNs are capable of not only causing the target network to make an error, but can be constructed to explicitly control the type of misclassification made. We demonstrate ATNs on both simple MNISTdigit classifiers and state-of-the-art ImageNet classifiers deployed by Google, Inc.: Inception ResNet-v2. With the resurgence of deep neural networks for many real-world classification tasks, there is an increased interest in methods to assess the weaknesses in the trained models. Adversarial examples are small perturbations of the inputs that are carefully crafted to fool the network into producing incorrect outputs. Seminal work by (Szegedy et al. 2013) and (Goodfellow, Shlens, and Szegedy 2014), as well as much recent work, has shown that adversarial examples are abundant, and that there are many ways to discover them. Given a classifier f(x) : x ∈ X → y ∈ Y and original inputs x ∈ X , the problem of generating untargeted adversarial examples can be expressed as the optimization: argminx∗ L(x,x ∗) s.t. f(x∗) = f(x), where L(·) is a distance metric between examples from the input space (e.g., the L2 norm). Similarly, generating a targeted adversarial attack on a classifier can be expressed as argminx∗ L(x,x ∗) s.t. f(x∗) = yt, where yt ∈ Y is some target label chosen by the attacker. Until now, these optimization problems have been solved using three broad approaches: (1) By directly using optimizers like L-BFGS or Adam (Kingma and Ba 2015), as Copyright c © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. proposed in (Szegedy et al. 2013) and (Carlini and Wagner 2016). (2) By approximation with single-step gradient-based techniques like fast gradient sign (Goodfellow, Shlens, and Szegedy 2014) or fast least likely class (Kurakin, Goodfellow, and Bengio 2016). (3) By approximation with iterative variants of gradient-based techniques (Kurakin, Goodfellow, and Bengio 2016; Moosavi-Dezfooli et al. 2016; Moosavi-Dezfooli, Fawzi, and Frossard 2016). These approaches use multiple forward and backward passes through the target network to more carefully move an input towards an adversarial classification. Other approaches assume a black-box model and only having access to the target model’s output (Papernot et al. 2016; Baluja, Covell, and Sukthankar 2015; Tramèr et al. 2016). See (Papernot et al. 2015) for a discussion of threat models. Each of the above approaches solved an optimization problem such that a single set of inputs was perturbed enough to force the target network to make a mistake. We take a fundamentally different approach: given a welltrained target network, can we create a separate, attacknetwork that, with high probability, minimally transforms all inputs into ones that will be misclassified? No per-sample optimization problems should be solved. The attack-network should take as input a clean image and output a minimally modified image that will cause a misclassification in the target network. Further, can we do this while imposing strict constraints on the types and amount of perturbations allowed? We introduce a class of networks, called Adversarial Transformation Networks, to efficiently address this task. Adversarial Transformation Networks In this work, we propose Adversarial Transformation Networks (ATNs). An ATN is a neural network that transforms an input into an adversarial example against a target network or set of networks. ATNs may be untargeted or targeted, and trained in a black-box or white-box manner. In this work, we will focus on targeted, white-box ATNs. Formally, an ATN can be defined as a neural network: gf,θ(x) : x ∈ X → x′ (1) where θ is the parameter vector of g, f is the target network which outputs a probability distribution across class labels, and x′ ∼ x, but argmax f(x) = argmax f(x′). The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)	adversary (cryptography);approximation;artificial intelligence;artificial neural network;augmented transition network;black box;computer vision;deep learning;generative adversarial networks;gradient descent;imagenet;independence day: resurgence;iterative method;limited-memory bfgs;mathematical optimization;optimization problem;optimizing compiler;perturbation theory;pixel;sensor;t-norm;white box (software engineering);with high probability;word lists by frequency	Shumeet Baluja;Ian Fischer	2018			artificial intelligence;machine learning;adversarial system;computer science	AI	19.175012041801516	-50.98721970385828	157005
f5188fb417a0f9e7f5d0de33796380b7a5aaee51	corrigendum to “probing for sparse and fast variable selection with model-based boosting”		[This corrects the article DOI: 10.1155/2017/1421409.].	boosting (machine learning);digital object identifier;feature selection;genetic selection;sparse;corrigendum	Janek Thomas;Tobias Hepp;Andreas Mayr;Bernd Bischl	2018		10.1155/2018/2430438	machine learning;computer science;boosting (machine learning);artificial intelligence;feature selection	ML	11.452191986337791	-47.71808484496686	157307
1308d7604b7afde9f628007f80511b8769d1ed07	efficient exact k-nn and nonparametric classification in high dimensions	high dimensionality;k nearest neighbor;support vector machine;high dimension	This paperis aboutnon-approximateaccelerationof high dimensional nonparametricoperationssuchask nearestneighborclassifiersandthe predictionphaseof SupportVectorMachineclassifiers.We attemptto exploit the fact that even if we want exact answersto nonparametric queries,we usually do not needto explicitly find the datapointsclose to thequery, but merelyneedto askquestionsaboutthepropertiesabout thatsetof datapoints.This offersa smallamountof computationaleeway, and we investigate how much that leeway can be exploited. For clarity, this paperconcentrateson purek-NN classificationandtheprediction phaseof SVMs. We introducenew ball treealgorithmsthat on real-world datasetsgive accelerationsof 2-fold up to 100-foldcompared againsthighly optimizedtraditionalball-tree-basedk-NN. Theseresults includedatasetswith up to 106 dimensionsand105 records,andshow non-trivial speedupswhile giving exactanswers. 1 Intr oduction Nonparametricmodelshavebecomeincreasinglypopularin thestatisticscommunitiesand probabilisticAI communities.They remainhamperedby their computationalcomplexity. Spatialmethodssuchaskd-trees[6, 17], R-trees[9], metrictrees[18, 4] andball trees[15] havebeenproposedandtestedasawayof alleviatingthecomputational costof suchstatistics without resortingto approximateanswers. They have beenusedin many different ways,andwith a varietyof treesearchalgorithmsandwith a varietyof “cachedsufficient statistics”decoratingtheinternalleaves,for examplein [14, 5, 16,8]. The main concernwith suchaccelerationsis the extent to which they can survive high dimensional data.Indeed,therearesomedatasetsin thispaperfor whichahighlyoptimized conventionalk nearestneighborsearchbasedon ball treesis on averagemore expensi ve thanthe nai ve linear searchalgorithm,but extracting the k nearestneighborsis often not needed,even for a k nearestneighborclassifier . This paperis aboutthe consequences of the fact that noneof thesethreequestionshave the sameprecisemeaning: (a) “What are the k nearestneighbors of t?” (b) “How manyof the k nearestneighbors of t are from the positiveclass?” and (c) “Ar e at least q of the k nearest neighbors from the positiveclass?” The computationalgeometrycommunityhasfocusedon question(a), but usesof proximity queriesin statisticsfar morefrequentlyrequire(b) and(c) typesof computations.Further, in additionto traditionalK-NN, thesameinsightappliesto many otherstatisticalcomputationssuchasnonparametricdensityestimation,locally weighted regression,mixturemodels,k-meansandthepredictionphaseof SVM classification. 2 Ball tr ees A ball tree is a binary tree in which each node representsa set of points, called Points(Node) . Givena dataset,the root nodeof a ball treerepresentsthefull setof points in thedataset.A nodecanbeeithera leaf nodeor a non-leafnode. A leaf nodeexplicitly containsa list of thepointsrepresentedby thenode. A non-leafnodedoesnot explicitly containasetof points.It hastwo child nodes:Node.child1 andNode.child2, where Points(Node.child1)∩Points(Node.child2) = φ Points(Node.child1)∪Points(Node.child2) = Points(Node) Pointsareorganizedspatially. Eachnodehasadistinguishedpointcalledapivot. Depending ontheimplementation, thepivot maybeoneof thedatapoints, or it maybethecentroid of Points(Node) . Eachnoderecordsthemaximumdistanceof thepointsit ownsto its pivot. Call this theradiusof thenode Node.Radius= maxx∈Points(Node)| Node.Pivot −x | Balls lower down the tree cover smallervolumes. This is achieved by insisting,at tree constructiontime, that x ∈ Points(Node.child1) ⇒ | x−Node.child1.Pivot | ≤ | x−Node.child2.Pivot | x ∈ Points(Node.child2) ⇒ | x−Node.child2.Pivot | ≤ | x−Node.child1.Pivot | Providedour distancefunctionobeys thetriangleinequality, thisgivestheability to bound the distancefrom a target point t to any point in any ball treenode. If x ∈ Points(Node) thenwecanbesurethat: |x− t| ≥ |t−Node.Pivot|−Node.Radius (1) |x− t| ≤ |t−Node.Pivot|+Node.Radius (2) Ball treesareconstructedtop-down. Thereareseveralwaysto constructthem,andpractical algorithmstradeoff thecostof construction(it wouldbeuselessto beO(R2) givenadataset with R points,for example)againstthe tightnessof theradiusof theballs. [13] describes onefastway of constructinga ball treeappropriatefor computationalstatistics.If a ball treeis balanced,thentheconstructiontime is O(CRlogR), whereC is thecostof a pointpoint distancecomputation(which is O(m) if therearem denseattributes,andO( f m) if therecordsaresparsewith only fraction f of attributestakingnon-zerovalues). 2.1 KNS1: Conventional K nearestneighbor search with ball tr ees In this paper , we call conventionalball-tree-basedsearch[18] KNS1. Let a pointsetPSbe asetof datapoints.Webegin with thefollowing definition: SaythatPSconsistsof thek-NN of t in pointsetV if andonly if ((|V |≥ k) ∧ (PSarethek-NN of t in V)) ∨ ((|V |< k) ∧ (PS= V)) (3) Wenow definearecursi veprocedurecalledBallKNNwith thefollowing inputsandoutput. PS = BallKNN(PS,Node) Let V = set of points searchedso far, on entry. AssumePSin consistsof the k-NN of t in V. This function efficiently ensuresthat on exit, PSout consistsof the k-NN of t in V ∪Points(Node) .	ball tree;binary tree;computation;emoticon;k-nearest neighbors algorithm;tree (data structure)	Ting Liu;Andrew W. Moore;Alexander G. Gray	2003			support vector machine;computer science;machine learning;pattern recognition;data mining;mathematics;k-nearest neighbors algorithm;statistics	ML	17.431544471432524	-39.31127088346458	157318
a9a95dc3af8388c2b9f8b2e55f030490c94725f7	experimental platform for intelligent computing (epic)		This paper presents the architecture and user interface of a novel Experimental Platform for Intelligent Computing (EPIC). Unlike the two most popular platforms (WEKA and KEEL), the proposed EPIC tool has a very friendly user interface, and offers some advantages with respect to existing tools for Intelligent Computing experiments. In particular, EPIC handles mixed and incomplete data directly, without preprocessing, and its architecture supports multi-target supervised classification and regression. It also contains a module for two dimensional dataset visualization, which includes the visualization of the decision frontier for several supervised learning algorithms.		Javier A. Hernández-Castaño;Oscar Camacho Nieto;Yenny Villuendas-Rey;Cornelio Yáñez-Márquez	2018	Computación y Sistemas		natural language processing;artificial intelligence;supervised learning;visualization;architecture;epic;machine learning;computer science;preprocessor;user interface	Robotics	14.397288192681946	-47.03382502617498	157481
b8851e107f0d0fc46a3179eb60db9fe9b9cd2364	a novel low-rank hypergraph feature selection for multi-view classification		In order to select informative features from a high-dimensional multi-view dataset, we have proposed a feature selection method that simultaneously embedding the low-rank constraint, sparse representation, global and local structure learning into a unified framework. Firstly, we utilize the conventional regression function ( i.e.  the least square loss function) to form a novel regression framework by introducing a low-rank constraint and a relaxation term. And then we employ an  l  21 -norm regularization term to filter out the redundant and irrelative features. Furthermore, we utilize a hypergraph based regularization term rather than the simple graph to construct a Laplacian matrix that will be used in enhancing the inherent association of data. Besides, we proposed a novel optimization algorithm to solve the objective function. Finally, we feed the reduced data got by the proposed feature selection method into Support Vector Machines (SVM) in term of classification accuracy. The experimental results showed that the proposed method achieved the best classification performance, compared with the state-of-the-art feature selection methods on real multi-view dataset.	feature selection	Xiaohui Cheng;Yonghua Zhu;Jingkuan Song;Guoqiu Wen;Wei He	2017	Neurocomputing	10.1016/j.neucom.2016.10.089	machine learning;pattern recognition;data mining;mathematics	AI	24.18713981699824	-42.3305497732237	157529
d97c17546770b78baf6ef8dd0aa3cd80dea4ca6b	learning svm with varied example cost: a k nn evaluating approach	classification;loss function;pattern recognition;k nearest neighbor;support vector machine;weight change;learning cost	The paper proposes a model merging a non-parametric  k- nearest-neighbor ( k NN) method into an underlying support vector machine (SVM) to produce an instance-dependent loss function. In this model, a filtering stage of the  k NN searching was employed to collect information from training examples and produced a set of emphasized weights which can be distributed to every example by a class of real-valued class labels. The emphasized weights changed the policy of the equal-valued impacts of the training examples and permitted a more efficient way to utilize the information behind the training examples with various significance levels. Due to the property of estimating density locally, the  k NN method has the advantage to distinguish the heterogeneous examples from the regular examples by merely considering the situation of the examples themselves. The paper shows the model is promising with both the theoretical derivations and consequent experimental results.		Chan-Yun Yang;Che-Chang Hsu;Jr-Syu Yang	2006		10.1007/978-3-540-74377-4_35	support vector machine;biological classification;computer science;artificial intelligence;machine learning;pattern recognition;data mining;k-nearest neighbors algorithm;loss function	ML	16.159052426447573	-38.876502083439895	157627
34625a5ed92548f319c8bdf0f9fd9b87c305e094	multi-label semi-supervised classification through optimum-path forest		Abstract Multi-label classification consists of assigning one or multiple classes to each sample in a given dataset. However, the project of a multi-label classifier is usually limited to a small number of supervised samples as compared to the number of all possible label combinations. This scenario favors semi-supervised learning methods, which can cope with the absence of supervised samples by adding unsupervised ones to the training set. Recently, we proposed a semi-supervised learning method based on optimum connectivity for single-label classification. In this work, we extend it for multi-label classification with considerable effectiveness gain. After a single-label data transformation, the method propagates labels from supervised to unsupervised samples, as in the original approach, by assuming that samples from the same class are more closely connected through sequences of nearby samples than samples from distinct classes. Given that the procedure is more reliable in high-density regions of the feature space, an additional step repropagates labels from the maxima of a probability density function to correct possible labeling errors from the previous step. Finally, the data transformation is reversed to obtain multiple labels per sample. The new approach is experimentally validated on several datasets in comparison with state-of-the-art methods.	machine learning;multi-label classification;semiconductor industry;supervised learning	Willian Paraguassu Amorim;Alexandre X. Falcão;João Paulo Papa	2018	Inf. Sci.	10.1016/j.ins.2018.06.067	artificial intelligence;semi-supervised learning;machine learning;probability density function;mathematics;feature vector;classifier (linguistics);small number;training set;maxima	AI	17.195589930999684	-42.19600353578706	157647
d4a6edcc1f0f5107479eb62be6ce6c22bb93775f	roughly balanced bagging for imbalanced data	class imbalance problem;minority example;negative binomial distribution.;original bagging algorithm;bagging-based method;bagging;sampling;smallest class;balanced bagging;rb bagging;real-world data set;imbalanced data;skewed class distribution;negative binomial distribution;resampling;sampling methods	The class imbalance problem appears in many real-world applications of classification learning. We propose an ensemble algorithm “Roughly Balanced (RB) Bagging” using a novel sampling technique to improve the original bagging algorithm for data sets with skewed class distributions. For this sampling method, the number of samples in the largest and smallest classes are different, but they are effectively balanced when averaged over all of the subsets, which supports the approach of bagging in a more appropriate way. Individual models in RB Bagging tend to show larger diversity, which is one of the keys of ensemble models, compared with existing bagging-based methods for imbalanced data that use exactly the same number of majority and minority examples for every training subset. In addition, the proposed method makes full use of all of the minority examples by under-sampling, which is efficiently done by using negative binomial distributions. Numerical experiments using benchmark and real-world data sets demonstrate that RB Bagging shows better performance than the existing “balanced” methods and other common methods for area under the ROC curve (AUC), which is a widely used metric in the class imbalance problem. Copyright © 2009 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 2: 412-426, 2009		Shohei Hido;Hisashi Kashima;Yutaka Takahashi	2009	Statistical Analysis and Data Mining	10.1002/sam.10061	sampling;bootstrap aggregating;resampling;machine learning;data mining;mathematics;negative binomial distribution;statistics	ML	13.986593337815231	-41.18227244607312	157656
0c412aa790e8d07dd096ef37ae918d59b1844de5	adaptive feature split selection for co-training: application to tire irregular wear classification	ltm tire data co training semi supervised classification feature splits dca;mechanical engineering computing;wear;wear learning artificial intelligence mechanical engineering computing pattern classification principal component analysis;principal component analysis;tires accuracy training ribs feature extraction training data discrete cosine transforms;pattern classification;learning artificial intelligence;adaptive feature split selection classification performance dca dependent component analysis mi based approach mutual information based approach semisupervised learning method tire irregular wear classification co training	Co-training is a practical and powerful semi-supervised learning method. It yields high classification accuracy with a training data set containing only a small set of labeled data. Successful performance in co-training requires two important conditions on the features: diversity and sufficiency. In this paper, we propose a novel mutual information (MI) based approach inspired by the idea of dependent component analysis (DCA) to achieve feature splits that are maximally independent between-subsets (diversity) or within-subsets (sufficiency). We evaluate the relationship between the classification performance and the relative importance of the two conditions. Experimental results on actual tire data indicate that compared to diversity, sufficiency has a more significant impact on their classification accuracy. Further results show that co-training with feature splits obtained by the MI-based approach yields higher accuracy than supervised classification and significantly higher when using a small set of labeled training data.	co-training;mutual information;semi-supervised learning;semiconductor industry;supervised learning;test set	Wei Du;Ronald Phlypo;Tülay Adali	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6638308	speech recognition;computer science;machine learning;linear classifier;pattern recognition;data mining;wear;one-class classification;principal component analysis	Vision	14.276184686042948	-44.09777287579606	157725
10cde25f835be150ded676ac5080ac6425b6b772	neural multi-scale image compression		This study presents a new lossy image compression method that utilizes the multi-scale features of natural images. Our model consists of two networks: multi-scale lossy autoencoder and parallel multi-scale lossless coder. The multi-scale lossy autoencoder extracts the multi-scale image features to quantized variables and the parallel multi-scale lossless coder enables rapid and accurate lossless coding of the quantized variables via encoding/decoding the variables in parallel. Our proposed model achieves comparable performance to the state-of-the-art model on Kodak and RAISE-1k dataset images, and it encodes a PNG image of size 768 × 512 in 70 ms with a single GPU and a single CPU process and decodes it into a high-fidelity image in approximately 200 ms.	autoencoder;central processing unit;graphics processing unit;image compression;lossless compression;lossy compression	Ken Nakanishi;Shin-ichi Maeda;Takeru Miyato;Daisuke Okanohara	2018	CoRR		image processing;autoencoder;mathematics;artificial intelligence;feature (computer vision);lossy compression;machine learning;image compression;deep learning;portable network graphics;computer vision;lossless compression	Vision	23.918295103926823	-51.51700525263663	157869
dc0c30e320f4983821edf286a239ead8fb852d19	improved discriminate analysis for high-dimensional data and its application to face recognition	reconnaissance visage;traitement signal;metodo estadistico;analisis componente principal;singularite;high dimensionality;image processing;learning;small sample size;dimension reduction;biometrie;biometrics;small sample size problem;biometria;dimensional analysis;procesamiento imagen;statistical method;informacion fisher;linear discriminate analysis;traitement image;aprendizaje;discriminant analysis;analyse discriminante;analisis discriminante;apprentissage;automatic recognition;face recognition;methode statistique;feature extraction;signal processing;principal component analysis;analyse dimensionnelle;high dimensional data;analyse composante principale;singularidad;pattern recognition;analisis dimensional;reconnaissance forme;extraction caracteristique;reconocimiento patron;procesamiento senal;linear discriminant analysis;information fisher;reconocimiento automatico;reconnaissance automatique;fisher information;singularity	Many pattern recognition applications involve the treatment of high-dimensional data and the small sample size problem. Principal component analysis (PCA) is a common used dimension reduction technique. Linear discriminate analysis (LDA) is often employed for classification. PCA plus LDA is a famous framework for discriminant analysis in high-dimensional space and singular cases. In this paper, we examine the theory of this framework and find out that even if there is no small sample size problem the PCA dimension reduction cannot guarantee the subsequent successful application of LDA. We thus develop an improved discriminate analysis method by introducing an inverse Fisher criterion and adding a constrain in PCA procedure so that the singularity phenomenon will not occur. Experiment results on face recognition suggest that this new approach works well and can be applied even when the number of training samples is one per class.	facial recognition system	Xiao-Sheng Zhuang;Dao-Qing Dai	2007	Pattern Recognition	10.1016/j.patcog.2006.11.015	singularity;feature extraction;computer science;fisher information;machine learning;dimensional analysis;pattern recognition;mathematics;linear discriminant analysis;biometrics;statistics;dimensionality reduction;principal component analysis;clustering high-dimensional data	Vision	24.053681105020335	-39.13394227525426	157957
78e67151083d101217c6ab96690785ea494b751f	fuzzy clustering algorithm based on factor analysis and its application to mail filtering	fuzzy clustering;fuzzy equivalence relation;spam filtering;index terms—factor analysis;indexing terms;factor analysis;error rate	Aim at the faults of Dynamic Clustering Algorithm based on Fuzzy Equation Matrix, we raise a fuzzy clustering algorithm based on factor analysis, which it combines the technology of reducing dimension using factor analyses method. The algorithm will deal with the sample collections before fuzzy clustering, which enlarge the scale of using dynamic clustering algorithm to resolve practical problems. All these show that the algorithm has a strong capability of concluding and abstracting through being applied to E-mail filtering. At the same time, we also make an experiment in our optional database. The experiment result verifies that the algorithm recall rate is 87.3 % in the mail filtering, which is higher than the SVM’s 80.1%, Naïve Bayes’s 61.7%, and KNN’s 73.2% respectively. The experiments show that the new algorithm has better recall rate and error rate.	bit error rate;cluster analysis;experiment;factor analysis;fuzzy clustering;k-nearest neighbors algorithm;naive bayes classifier;sensitivity and specificity	Jingtao Sun;Qiuyu Zhang;Zhanting Yuan	2009	JSW		correlation clustering;constrained clustering;data stream clustering;index term;k-medians clustering;fuzzy clustering;flame clustering;word error rate;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;factor analysis;dbscan;clustering high-dimensional data	Web+IR	12.843216437251359	-40.81728484465517	158033
d2118330e6b55ff0d6fbc23d18305d1422c91dcc	convolutional neural networks on irregular domains through approximate translations on inferred graphs		We propose a generalization of convolutional neural networks (CNNs) to irregular domains, through the use of a translation operator on a graph structure. In regular settings such as images, convolutional layers are designed by translating a convolutional kernel over all pixels, thus enforcing translation equivariance. In the case of general graphs however, translation is not a well-defined operation, which makes shifting a convolutional kernel not straightforward. In this article, we introduce a methodology to allow the design of convolutional layers that are adapted to signals evolving on irregular topologies, even in the absence of a natural translation. Using the designed layers, we build a CNN that we train using the initial set of signals. Contrary to other approaches that aim at extending CNNs to irregular domains, we incorporate the classical settings of CNNs for 2D signals as a particular case of our approach. Designing convolutional layers in the vertex domain directly implies weight sharing, which in other approaches is generally estimated a posteriori using heuristics.	approximation algorithm;artificial neural network;convolutional neural network;heuristic (computer science);kernel (operating system);pixel;translation operator (quantum mechanics)	Bastien Pasdeloup;Vincent Gripon;Jean-Charles Vialatte;Dominique Pastor	2017	CoRR		kernel (linear algebra);locality;mathematics;discrete mathematics;combinatorics;grid;convolutional neural network;operator (computer programming);vertex (geometry);artificial intelligence;machine learning;convolution;graph	ML	22.927984772911223	-48.82801281699735	158175
d159ea481b057b16616951e8e548d832d4450373	improving svm performance using a linear combination of kernels	real world application;kernel method;genetic algorithm;support vector machine;numerical experiment;convex combination	Standard kernel-based classifiers use only a single kernel, but the real-world applications and the recent developments of various kernel methods have emphasized the need to consider a combination of multiple kernels. We propose an evolutionary approach for finding the optimal weights of a combined kernel used by the Support Vector Machines (SVM) algorithm for solving some particular problems. We use a genetic algorithm (GA) for evolving these weights. The numerical experiments show that the evolved combined kernels (ECKs) perform better than the convex combined kernels (CCKs) for several classification problems.		Laura Diosan;Mihai Oltean;Alexandrina Rogozan;Jean-Pierre Pécuchet	2007		10.1007/978-3-540-71629-7_25	support vector machine;kernel method;mathematical optimization;string kernel;kernel embedding of distributions;genetic algorithm;convex combination;radial basis function kernel;kernel principal component analysis;computer science;machine learning;pattern recognition;mathematics;tree kernel;variable kernel density estimation;polynomial kernel;kernel smoother	Vision	22.08832926247818	-39.28067677646197	158186
0d871c98f3b38a000b3584bf1fe723bdeb7a2a8c	subset feature learning for fine-grained category classification	learning artificial intelligence feature extraction feedforward neural nets image classification;neural networks;training;learning systems;accuracy;birds;feature extraction;birds accuracy training feature extraction australia learning systems neural networks;domain generic feature learning subset feature learning fine grained category classification fine grained categorisation small interclass variation large intraclass variation low training image number deep convolutional neural network feature learning caltech ucsd bird dataset mean accuracy;australia	Fine-grained categorisation has been a challenging problem due to small inter-class variation, large intra-class variation and low number of training images. We propose a learning system which first clusters visually similar classes and then learns deep convolutional neural network features specific to each subset. Experiments on the popular fine-grained Caltech-UCSD bird dataset show that the proposed method outperforms recent fine-grained categorisation methods under the most difficult setting: no bounding boxes are presented at test time. It achieves a mean accuracy of 77.5%, compared to the previous best performance of 73.2%. We also show that progressive transfer learning allows us to first learn domain-generic features (for bird classification) which can then be adapted to specific set of bird classes, yielding improvements in accuracy.	artificial neural network;categorization;computer vision;convolutional neural network;feature learning;outline of object recognition;statistical classification;ucsd pascal/p-system	ZongYuan Ge;Chris McCool;Conrad Sanderson;Peter I. Corke	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2015.7301271	semi-supervised learning;feature learning;feature extraction;computer science;artificial intelligence;machine learning;pattern recognition;accuracy and precision;deep learning;competitive learning;feature;artificial neural network	Vision	24.017167711520017	-50.960205244599436	158187
19ea4acfd489f5051012d65b44b9a4bce08005fe	feature sensitivity on biochemical signaling pathways	biochemistry;basis pursuit regularization;biochemical signal transduction pathways;biochemical signaling pathways;continuous regularization problem;feature sensitivity;model complexity	This paper investigates the solution of the feature selection problem in biochemical signal transduction pathways by examining the sensitivity of the features with respect to the model complexity using basis pursuit regularization (BPR). Feature selection is effectively transformed into a continuous regularization problem with a characteristic 1-norm imposed on the parameter vector to penalize the models complexity. This technique makes possible the design of sparse models for the pathway data and because of the nature of the 1-norm it is possible to analyze the entire solution path (parameter locus) as the regularizer changes from zero to infinity.	basis pursuit;continuous optimization;feature selection;gene regulatory network;interaction;locus;loss function;mathematical optimization;matrix regularization;neural coding;optimization problem;selection algorithm;simpson's paradox;simpson's rule;sparse matrix;transduction (machine learning);vii	George Papadopoulos;Martin Brown	2007	2007 IEEE Symposium on Computational Intelligence and Bioinformatics and Computational Biology		biochemistry;mathematical optimization;computer science;bioinformatics;machine learning;mathematics;feature selection;signal transduction	Comp.	11.338554417011544	-51.94571231816217	158231
68d86e346d6cc0e5280f8d5732c703824f992c35	robust 1-norm soft margin smooth support vector machine	heuristic method;classification;robustness;support vector machine;smooth technique;outlier resistance	Based on studies and experiments on the loss term of SVMs, we argue that 1-norm measurement is better than 2-norm measurement for outlier resistance. Thus, we modify the previous 2-norm soft margin smooth support vector machine (SSVM2) to propose a new 1-norm soft margin smooth support vector machine (SSVM1). Both SSVMs can be solved in primal form without a sophisticated optimization solver. We also propose a heuristic method for outlier filtering which costs little in training process and improves the ability of outlier resistance a lot. The experimental results show that SSVM1 with outlier filtering heuristic performs well not only on the clean, but also the polluted synthetic and benchmark UCI datasets.	support vector machine	Li-Jen Chien;Yuh-Jye Lee;Zhi-Peng Kao;Chih-Cheng Chang	2010		10.1007/978-3-642-15381-5_18	support vector machine;biological classification;computer science;machine learning;pattern recognition;data mining;relevance vector machine;robustness	ML	21.13755052106475	-38.472495737736324	158328
839b2632fb0c73729ad427bb7ebd74fb5c536375	hierarchical maximum-margin clustering		We present a hierarchical maximum-margin clustering method for unsupervised data analysis. Our method extends beyond flat maximummargin clustering, and performs clustering recursively in a top-down manner. We propose an effective greedy splitting criteria for selecting which cluster to split next, and employ regularizers that enforce feature sharing/competition for capturing data semantics. Experimental results obtained on four standard datasets show that our method outperforms flat and hierarchical clustering baselines, while forming clean and semantically meaningful cluster hierarchies.	cluster analysis;greedy algorithm;hierarchical clustering;recursion;resultant;taxonomy (general);top-down and bottom-up design;unsupervised learning	Guang-Tong Zhou;Sung Ju Hwang;Mark W. Schmidt;Leonid Sigal;Greg Mori	2015	CoRR		correlation clustering;constrained clustering;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;biclustering;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	ML	20.750425937995928	-42.09429384025586	158420
baf59085b394c7095f8d5014acd43464154adb2d	a multi-criteria meta-learning method to select under-sampling algorithms for imbalanced datasets		Standard classifiers consider a balanced distribution of examples’ classes in the data, thus, imbalanced datasets may hinder the learning process. Sampling techniques balance the data by adjusting the examples’ classes distribution. However, selecting an appropriate sampling technique and its parameters for a given imbalanced dataset is still an open problem. This work proposes a method that uses Meta-Learning to recommend a technique for an imbalanced dataset considering multiple performance criteria. The experiments revealed that the proposal reached results comparable to those achieved by the brute-force approach, overcame the techniques with their default parameters most of the time, and always surpassed the random search approach.	algorithm;brute-force search;experiment;gibbs sampling;meta learning (computer science);random search;sampling (signal processing);time complexity	Romero F. A. B. de Morais;Péricles B. C. de Miranda;Ricardo Martins	2017				AI	14.622842831826672	-40.81686691144996	158525
595d0fe1c259c02069075d8c687210211908c3ed	a survey on learning to hash		Nearest neighbor search is a problem of finding the data points from the database such that the distances from them to the query point are the smallest. Learning to hash is one of the major solutions to this problem and has been widely studied recently. In this paper, we present a comprehensive survey of the learning to hash algorithms, categorize them according to the manners of preserving the similarities into: pairwise similarity preserving, multiwise similarity preserving, implicit similarity preserving, as well as quantization, and discuss their relations. We separate quantization from pairwise similarity preserving as the objective function is very different though quantization, as we show, can be derived from preserving the pairwise similarities. In addition, we present the evaluation protocols, and the general performance analysis, and point out that the quantization algorithms perform superiorly in terms of search accuracy, search time cost, and space cost. Finally, we introduce a few emerging topics.	algorithm;categorization;data point;distance;hash function;loss function;marijuana abuse;nearest neighbor search;optimization problem;protocols documentation;question (inquiry);solutions	Jingdong Wang;Shaobo Zhang;Jingkuan Song;Nicu Sebe;Heng Tao Shen	2018	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2017.2699960	theoretical computer science;machine learning;data mining;mathematics	Vision	19.29634603469453	-46.15925948168026	158910
fad5aa024bcdecf015edb60313d978147c1c2d88	using bag-of-little bootstraps for efficient ensemble learning		The technique bag-of-little bootstrap provides statistical estimates equivalent to the ones of bootstrap in a tiny fraction of the time required by bootstrap. In this work, we propose to combine bag-of-little bootstrap into an ensemble of classifiers composed of random trees. We show that using this bootstrapping procedure, instead of standard bootstrap samples, as the ones used in random forest, can dramatically reduce the training time of ensembles of classifiers. In addition, the experiments carried out illustrate that, for a wide range of training times, the proposed ensemble method achieves a generalization error smaller than that achieved by random forest.	ensemble learning	Pablo de Viña;Gonzalo Martínez-Muñoz	2018		10.1007/978-3-030-01418-6_53	ensembles of classifiers;pattern recognition;machine learning;artificial intelligence;bootstrapping (electronics);random forest;generalization error;ensemble learning;bootstrapping;computer science	ML	15.347922708583875	-39.977450115659614	158953
84320e897649b9c11058162901a7a8dc783f19e4	logistic component analysis for fast distance metric learning	distance metric learning;input data space logistic component analysis recognition rate classification problem linear discriminant analysis lda discriminating feature extraction method mahalanobis distance metric learning neighborhood component analysis mahalanobis distance metric learning methods stochastic nearest neighbor assignment nca within class coherency objective function minimization computational cost reduction fast distance metric learning method between class distinguishability nearest mean classification standard repository datasets computational time;maharanobis distance;pattern recognition;pattern classification learning artificial intelligence;logistic function;logistic function distance metric learning maharanobis distance	Discriminating feature extraction is important to achieve high recognition rate in a classification problem. Fisher's linear discriminant analysis (LDA) is one of the well-known discriminating feature extraction methods and is closely related to the Mahalanobis distance metric learning. Neighborhood component analysis (NCA) is one of the Mahalanobis distance metric learning methods based on stochastic nearest neighbor assignment. The objective function of NCA can be expressed as a within-class coherency by a simple formula, and NCA extracts discriminating features by minimizing the objective function. Unfortunately, the computational cost of NCA significantly increases as the number of input data increases. For reducing the computational cost, we propose a fast distance metric learning method by taking the between-class distinguish ability into account of nearest mean classification. According to the experimental results using standard repository datasets, the computational time of our method is evaluated as 27 times shorter than that of NCA while keeping or improving the accuracy.	algorithmic efficiency;computation;feature extraction;linear discriminant analysis;loss function;next-generation secure computing base;optimization problem;time complexity;whole earth 'lectronic link	Kenji Watanabe;Toshikazu Wada	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.229	logistic function;computer science;mahalanobis distance;machine learning;pattern recognition;mathematics;k-nearest neighbors algorithm;statistics	Vision	20.96905191634112	-39.820992956334315	159190
2371218f5725196b0248d93ecdab4583c10a8d9f	ensemble learning with active data selection for semi-supervised pattern classification	unlabeled data;engineering;semisupervised pattern classification;pattern classification semisupervised learning humans probability supervised learning degradation boosting face recognition neural networks learning systems;facial expression recognition;ensemble learning;neural nets;computer science artificial intelligence;synthetic data classification;neural network classifier;semi supervised learning;facial expression recognition task;facial expression recognition task ensemble learning active data selection semisupervised pattern classification semisupervised learning discriminative classifier generalization neural network classifier synthetic data classification;face recognition;pattern classification face recognition generalisation artificial intelligence learning artificial intelligence neural nets;active data selection;pattern classification;generalisation artificial intelligence;synthetic data;learning artificial intelligence;generalization;semisupervised learning;generalization capability;discriminative classifier;computer science software	Unlike traditional pattern classification, semi-supervised learning provides a novel technique to make use of both labeled and unlabeled data for improving the performance of classification. In general, there are two critical issues for semi-supervised learning of discriminative classifiers; i.e., how to create an initial classifier of a good generalization capability with the limited labeled data and the how to make an effective use of unlabeled data without degradation of the established classifier. To tackle two aforementioned problems, we propose an ensemble learning approach based on a recent active data selection strategy, where ensemble learning would yield good generalization and active data selection tends to choose the unlabeled data more likely resulting in an improvement during semi-supervised learning. By using an ensemble of K-NN classifiers, we demonstrate the effectiveness of our approach on a synthetic data classification and a facial expression recognition tasks.	co-training;computation;data point;discriminative model;elegant degradation;ensemble learning;k-nearest neighbors algorithm;naive bayes classifier;semi-supervised learning;semiconductor industry;statistical classification;supervised learning;synthetic data	Shihai Wang;Ke Chen	2007	2007 International Joint Conference on Neural Networks	10.1109/IJCNN.2007.4370982	semi-supervised learning;unsupervised learning;generalization;computer science;machine learning;pattern recognition;data mining;ensemble learning;artificial neural network;synthetic data;generalization error	ML	14.424975984573562	-40.747685125627754	159259
be4faea0971ef74096ec9800750648b7601dda65	feature analysis of unsupervised learning for multi-task classification using convolutional neural network	unsupervised learning;convolutional neural networks;multi-task learning;auto-encoder;deep learning	This study analyzes the characteristics of unsupervised feature learning using a convolutional neural network (CNN) to investigate its efficiency for multi-task classification and compare it to supervised learning features. We keep the conventional CNN structure and introduce modifications into the convolutional auto-encoder design to accommodate a subsampling layer and make a fair comparison. Moreover, we introduce non-maximum suppression and dropout for a better feature extraction and to impose sparsity constraints. The experimental results indicate the effectiveness of our sparsity constraints. We also analyze the efficiency of unsupervised learning features using the t-SNE and variance ratio. The experimental results show that the feature representation obtained in unsupervised learning is more advantageous for multi-task learning than that obtained in supervised learning.	artificial neural network;autoencoder;chroma subsampling;computer multitasking;convolutional neural network;dropout (neural networks);encoder;feature extraction;feature learning;multi-task learning;sparse matrix;supervised learning;t-distributed stochastic neighbor embedding;unsupervised learning;zero suppression	Jonghong Kim;Waqas Bukhari;Minho Lee	2017	Neural Processing Letters	10.1007/s11063-017-9724-1	machine learning;supervised learning;semi-supervised learning;deep learning;competitive learning;unsupervised learning;pattern recognition;active learning (machine learning);wake-sleep algorithm;artificial intelligence;computer science;feature learning	AI	22.987484628371252	-46.41243274461787	159310
4939a8eac22b26ede64b68528b48367c5ec50e88	comparing neural network, logistic regression, and discriminant analysis for knowledge representation and classification explanation		Many research studies have proved neural networks as a viable alternative to statistical models for classification tasks. However, compared with statistical models, neural networks have had the drawback of being unable to explain its classification logic until the development of rule extraction algorithms from trained neural networks. This research attempts to compare the results of the rule extraction algorithm GLARE with logistic regression and discriminant analysis in terms of their ability to identify important predictor variables and handle different levels of difficult classification tasks. Our experimental results show that GLARE can precisely identify important predictor variables as its statistical counterparts. In addition, GLARE's extracted rules generate higher correct classification rates than statistical models in a moderately difficult classification task. Most importantly, GLARE can reveal nonlinear relationship between predictor variables and tobe-predicted classes, which many statistical classifiers cannot.	algorithm;artificial neural network;kerrison predictor;knowledge representation and reasoning;linear discriminant analysis;logistic regression;nonlinear system;rule induction;statistical classification;statistical model	Monica S. Lam	2007			machine learning;pattern recognition;optimal discriminant analysis;logistic model tree;linear discriminant analysis;multiple discriminant analysis;multinomial logistic regression;statistics	ML	10.348680559529237	-38.06373813060098	159447
b38e49bc606cf2b52483737ac44fedba42047ccd	pnnu: parallel nearest-neighbor units for learned dictionaries	multi core programming;computer vision;cifar;signal processing;nearest neighbor;matching pursuit;data analytics;nnu;learned dictionary;sparse coding;speedup;pnnu;kth;parallel processing	We present a novel parallel approach, parallel nearest neighbor unit (PNNU), for finding the nearest member in a learned dictionary of high-dimensional features. This is a computation fundamental to machine learning and data analytics algorithms such as sparse coding for feature extraction. PNNU achieves high performance by using three techniques: (1) PNNU employs a novel fast table look up scheme to identify a small number of atoms as candidates from which the nearest neighbor of a query data vector can be found; (2) PNNU reduces computation cost by working with candidate atoms of reduced dimensionality; and (3) PNNU performs computations in parallel over multiple cores with low inter-core communication overheads. Based on efficient computation via techniques (1) and (2), technique (3) attains further speed up via parallel processing. We have implemented PNNU on multi-core machines. We demonstrate its superior performance on three application tasks in signal processing and computer vision. For an action recognition task, PNNU achieves 41x overall performance gains on a 16-core compute server against a conventional serial implementation of nearest neighbor computation. Our PNNU software is available online as open source.	algorithm;computation;computer vision;data dictionary;data point;deep learning;dictionary;feature extraction;graphics processing unit;hardware acceleration;lookup table;machine learning;mobile computing;multi-core processor;nearest-neighbor interpolation;neural coding;noise reduction;open-source software;parallel computing;requirement;server (computing);signal processing;sparse matrix;speedup;supercomputer	H. T. Kung;Bradley McDanel;Surat Teerapittayanon	2015		10.1007/978-3-319-29778-1_14	parallel processing;parallel computing;speedup;k-svd;computer science;theoretical computer science;machine learning;signal processing;pattern recognition;neural coding;data analysis;k-nearest neighbors algorithm;matching pursuit	ML	18.51822730110189	-46.18069076401658	159546
22fe94f4749c3728e3ca73f352524e60fa84fe13	ensemble data mining methods	data mining;machine learning	INTRODUCTION Ensemble Data Mining Methods, also known as Committee Methods or Model Combiners, are machine learning methods that leverage the power of multiple models to achieve better prediction accuracy than any of the individual models could on their own. The basic goal when designing an ensemble is the same as when establishing a committee of people: each member of the committee should be as competent as possible, but the members should be complementary to one another. If the members are not complementary, Le., if they always agree, then the committee is unnecessary---any one member is sufficient. If the members are complementary, then when one or a few members make an error, the probability is high that the remaining members can correct this error. Research in ensemble methods has largely revolved around designing ensembles consisting of competent yet complementary models.	data mining;ensemble forecasting;ensemble learning;machine learning	Nikunj C. Oza	2009			unsupervised learning;decision tree learning;computer science;online machine learning;machine learning;pattern recognition;incremental decision tree;data mining;ensemble learning;generalization error	AI	15.935318473458452	-41.4714895178384	159770
98cc26663f8d8b3009434cd46e390308859f914e	maximum margin clustering made practical	estensibilidad;unsupervised learning;iterative method;pattern clustering;optimisation;non convex programming;learning algorithm;convergence;maximum margin clustering;premature convergence;handwriting recognition;programmation semi definie;laplacian square loss;non convex optimization;optimizacion;supervised learning;support vector machines;nonconvex optimization;semidefinite programs;unsupervised learning supervised learning optimization methods support vector machines machine learning convergence fasteners laplace equations scalability handwriting recognition;metodo penalidad;programmation non convexe;apprentissage non supervise;algorithme apprentissage;fonction perte;funcion perdida;unsupervised learning large margin methods maximum margin clustering mmc scalability;metodo iterativo;optimization problem;classification a vaste marge;laplace equations;programacion no convexa;penalty method;methode penalite;nonconvex optimization problem;laplacian square loss maximum margin clustering supervised learning nonconvex optimization problem semidefinite programs alternating optimization nonconvex problem;machine learning;mathematical programming;methode iterative;loss function;clustering method;large margin methods;analyse non convexe;fasteners;optimization;extensibilite;scalability;apprentissage supervise;non convex analysis;maquina ejemplo soporte;vector support machine;learning artificial intelligence;programacion semi definida;pattern clustering learning artificial intelligence optimisation;aprendizaje supervisado;algoritmo aprendizaje;maximum margin clustering mmc;programmation mathematique;nonconvex problem;programacion matematica;alternating optimization;semidefinite program;semi definite programming;optimization methods;analisis no convexo	Motivated by the success of large margin methods in supervised learning, maximum margin clustering (MMC) is a recent approach that aims at extending large margin methods to unsupervised learning. However, its optimization problem is nonconvex and existing MMC methods all rely on reformulating and relaxing the nonconvex optimization problem as semidefinite programs (SDP). Though SDP is convex and standard solvers are available, they are computationally very expensive and only small data sets can be handled. To make MMC more practical, we avoid SDP relaxations and propose in this paper an efficient approach that performs alternating optimization directly on the original nonconvex problem. A key step to avoid premature convergence in the resultant iterative procedure is to change the loss function from the hinge loss to the Laplacian/square loss so that overconfident predictions are penalized. Experiments on a number of synthetic and real-world data sets demonstrate that the proposed approach is more accurate, much faster (hundreds to tens of thousands of times faster), and can handle data sets that are hundreds of times larger than the largest data set reported in the MMC literature.		Kai Zhang;Ivor W. Tsang;James T. Kwok	2009	IEEE transactions on neural networks	10.1109/TNN.2008.2010620	unsupervised learning;optimization problem;support vector machine;mathematical optimization;scalability;convergence;computer science;machine learning;penalty method;pattern recognition;mathematics;iterative method;handwriting recognition;supervised learning;premature convergence;loss function	ML	22.30968237557631	-38.16476221741926	159791
e6fb0cb06b4e1c028b1faf1f9bb4a3048c707184	transfer learning with neural automl		We reduce the computational cost of Neural AutoML with transfer learning. AutoML relieves human effort by automating the design of ML algorithms. Neural AutoML has become popular for the design of deep learning architectures, however, this method has a high computation cost.To address this we propose Transfer Neural AutoML that uses knowledge from prior tasks to speed up network design. We extend RL-based architecture search methods to support parallel training on multiple tasks and then transfer the search strategy to new tasks. On language and image classification data, Transfer Neural AutoML reduces convergence time over single-task training by over an order of magnitude on many tasks.	algorithm;algorithmic efficiency;architecture as topic;class;computation;computer vision;controllers;convergence (action);deep learning;meta-process modeling;ninl gene;natural language processing;network planning and design;overfitting;rl (complexity);while	Catherine Wong;Neil Houlsby;Yifeng Lu;Andrea Gesmundo	2018			artificial intelligence;machine learning;computer science;transfer of learning;architecture;speedup;deep learning;computation;network planning and design;contextual image classification;convergence (routing)	ML	22.029952715204136	-50.91764370844847	159935
56c448434df9f01de306815122f6517a33235e22	label propagation through sparse neighborhood and its applications	label propagation;linear reconstruction;sparse neighborhood;classification;sparse reconstruction	In this paper, a novel semi-supervised learning approach is proposed. It assumes that, for the ith sample xi , the samples from xi ’s sparse neighborhood have the same label with xi and the label of xi can be linearly reconstructed by the labels of those samples from xi ’s sparse neighborhood. Our algorithm firstly selects the sparse neighborhood for each sample, and then in that sparse neighborhood finds the Different from many existing methods, we construct the adapting graph, simultaneously, give the weight of each edge. What’s more, we highlight the role of those samples in that sparse neighborhood, meanwhile, eliminate the role of those samples out of that sparse neighborhood. The experimental results on face recognition and document classification demonstrate the effectiveness and efficiency of our proposed approach in this paper. & 2012 Elsevier B.V. All rights reserved.	algorithm;document classification;experiment;facial recognition system;list of prokaryotic names with standing in nomenclature;mathematical optimization;optimization problem;semi-supervised learning;semiconductor industry;software propagation;sparse matrix;supervised learning	Fei Zang;Jiang-She Zhang	2012	Neurocomputing	10.1016/j.neucom.2012.03.017	biological classification;machine learning;pattern recognition;sparse approximation;data mining;mathematics	AI	23.765697058916004	-42.1972187059895	159970
79406841a4e88c9b0fbcfb1aa5b6fcc3b4f036a5	mining video associations for efficient database management	association mining;database management;video database	Clustering, classification, and regression, are three major research topics in machine learning. So far, much work has been conducted in solving multiple instance classification and multiple instance regression problems, where supervised training patterns are given as bags and each bag consists of some instances. But the research on unsupervised multiple instance clustering is still limited . This paper formulates a novel Maximum Margin Multiple Instance Clustering (MIC) problem for the multiple instance clustering task. To avoid solving a nonconvex optimization problem directly, MIC is further relaxed, which enables an efficient optimization solution with a combination of Constrained Concave-Convex Procedure (CCCP) and the Cutting Plane method. Furthermore, this paper analyzes some important properties of the proposed method and the relationship between the proposed method and some other related ones. An extensive set of empirical results demonstrate the advantages of the proposed method against existing research for both effectiveness and efficiency.	algorithm;cluster analysis;concave function;convex optimization;cutting-plane method;lagrangian relaxation;mbm (file format);machine learning;mathematical optimization;optimization problem;statistical classification	Dan Zhang;Fei Wang;Luo Si;Tao Li	2003			computer science;data mining;database;world wide web	ML	23.23425154735079	-41.069214425900306	160288
f6e8a1a89d4587ffdaf4ae0bc28fc8f1a3029ef7	transfer learning with manifold regularized convolutional neural network		Deep learning has been recently proposed to learn robust representation for various tasks and deliver state-of-the-art performance in the past few years. Most researchers attribute such success to the substantially increased depth of deep learning models. However, training a deep model is time-consuming and need huge amount of data. Though techniques like fine-tuning can ease those pains, the generalization performance drops significantly in transfer learning setting with little or without target domain data. Since the representation in higher layers must transition from general to specific eventually, generalization performance degrades without integrating sufficient label information of target domain. To address such problem, we propose a transfer learning framework called manifold regularized convolutional neural networks (MRCNN). Specifically, MRCNN fine-tunes a very deep convolutional neural network on source domain, and simultaneously tries to preserve the manifold structure of target domain. Extensive experiments demonstrate the effectiveness of MRCNN compared to several state-of-the-art baselines.	convolutional neural network	Fuzhen Zhuang;Lang Huang;Jia He;Jixin Ma;Qing He	2017		10.1007/978-3-319-63558-3_41	transfer of learning;convolutional neural network;manifold;machine learning;manifold alignment;computer science;deep learning;nonlinear dimensionality reduction;artificial intelligence;pattern recognition	AI	22.323977778667075	-50.01003204943244	160375
a85f63636dfca87222972e63e817f1e0f86a592b	neural approaches to image compression/decompression using pca based learning algorithms	learning algorithm;image compression	Principal Component Analysis is a well-known statistical method for feature extraction, data compression and multivariate data projection. Aiming to obtain a guideline for choosing a proper method for a specific application we developed a series of simulations on some the most currently used PCA algorithms as GHA, Sanger variant of GHA and APEX. The paper reports the conclusions experimentally derived on the convergence rates and their corresponding efficiency for specific image processing tasks.	data compression;experiment;feature extraction;generalized hebbian algorithm;image compression;image processing;principal component analysis;simulation	Luminita State;Catalina Cocianu;Panayiotis M. Vlamos;Doru Constantin	2008			data compression;computer vision;image compression;computer science;machine learning;pattern recognition;texture compression	ML	16.591976115050464	-44.87359369844228	160400
349b28db88c97fc68d897cb0f7d1b5f1b1349f46	lazy bagging for classifying imbalanced data	error reduction;local decision boundary;imbalanced data classification;lazy bagging design;test instance;supervised classification learning;nearest neighbor;pattern classification;bagging testing decision trees decision theory nearest neighbor searches accuracy data mining computer science data engineering design engineering;classification error;pattern classification learning artificial intelligence;learning artificial intelligence;test instance lazy bagging design imbalanced data classification local decision boundary supervised classification learning	In this paper, we propose a lazy bagging (LB) design, which builds bootstrap replicate bags based on the characteristics of the test instances. Upon receiving a test instance Ik, LB will trim bootstrap bags by taking Ik's nearest neighbors in the training set into consideration. Our hypothesis is that an unlabeled instance's nearest neighbors provide valuable information for learners to refine their local decision boundaries for classifying this instance. By taking full advantage of Ik's nearest neighbors, the base learners are able to receive less bias and variance in classifying Ik. This strategy is beneficial for classifying imbalanced data because refining local decision boundaries can help a learner reduce its inherent bias towards the majority class and improve its performance on minority class examples. Our experimental results will confirm that LB outperforms C4.5 and TB in terms of reducing classification error, and most importantly this error reduction is largely contributed from LB's improvement on minority class examples.	booting;c4.5 algorithm;care-of address;entropy (information theory);inverse kinematics;kerrison predictor;lattice boltzmann methods;lazy evaluation;sampling (signal processing);self-replicating machine;terabyte;test set	Xingquan Zhu	2007	Seventh IEEE International Conference on Data Mining (ICDM 2007)	10.1109/ICDM.2007.95	instance-based learning;computer science;artificial intelligence;machine learning;pattern recognition;data mining;k-nearest neighbors algorithm	Robotics	14.816074162980339	-40.375077723817	161299
6c09b69af2feb6526177b8f0bc227e02a9155f7f	handling class imbalance problems via weighted bp algorithm	class imbalance;weight;networked learning;imbalance dataset;neural network;bp algorithm	When using Neural Networks (NN) to handle class imbalance problems, there exists a fact that minority class makes less contribution to the error function than the majority class, so the network learned prefers to recognizing majority class data which we pay less attention to. This paper proposes a novel algorithm WNN (Weighted NN) to solve this problem using a newly defined error function in BP (BP) algorithm. Experimental results executed on 20 UCI datasets show that the approach can effectively enhance the recognition rate of minority class data.	algorithm;backpropagation	Xiaoqin Wang;Huaxiang Zhang	2009		10.1007/978-3-642-03348-3_75	computer science;artificial intelligence;machine learning;data mining;weight;artificial neural network	Theory	13.587401799897	-40.23318609201157	161354
95763cd98e940078020d5bda8858ada3df7bb1db	face verification with balanced thresholds	reconnaissance visage;threshold balance;iterative method;evaluation performance;transformation affine;orthogonal matrix;base donnee;performance evaluation;subspace learning;learning;biometrie;evaluacion prestacion;biometrics;projection method;database;biometria;matriz ortogonal;base dato;matrice diagonale;face verification;algorithms artificial intelligence biometry computer security face humans image enhancement image interpretation computer assisted information storage and retrieval pattern recognition automated subtraction technique;metodo subespacio;matrix algebra;indexing terms;methode sous espace;class specific optimal thresholds;metodo iterativo;algorithme;aprendizaje;feasibility;etat actuel;algorithm;probes matrix decomposition iterative algorithms constraint optimization algorithm design and analysis biometrics image databases us department of defense asia degradation;matrix algebra face recognition;apprentissage;balanced thresholds;automatic recognition;face recognition;dimensionality reduction;methode projection;matriz diagonal;methode iterative;affine transformation;state of the art;metodo proyeccion;matrice orthogonale;pattern recognition;subspace method;estado actual;threshold balance dimensionality reduction face verification subspace learning;reconnaissance forme;reconocimiento patron;affine transformation matrix;dimensional reduction;practicabilidad;faisabilite;diagonal matrix face verification balanced thresholds class specific optimal thresholds dimensionality reduction algorithm affine transformation matrix threshold balanced transformation orthogonal matrix;transformacion afin;reconocimiento automatico;reconnaissance automatique;threshold balanced transformation;dimensionality reduction algorithm;algoritmo;diagonal matrix	The process of face verification is guided by a prelearned global threshold, which, however, is often inconsistent with class-specific optimal thresholds. It is, hence, beneficial to pursue a balance of the class-specific thresholds in the model-learning stage. In this paper, we present a new dimensionality reduction algorithm tailored to the verification task that ensures threshold balance. This is achieved by the following aspects. First, feasibility is guaranteed by employing an affine transformation matrix, instead of the conventional projection matrix, for dimensionality reduction, and, hence, we call the proposed algorithm threshold balanced transformation (TBT). Then, the affine transformation matrix, constrained as the product of an orthogonal matrix and a diagonal matrix, is optimized to improve the threshold balance and classification capability in an iterative manner. Unlike most algorithms for face verification which are directly transplanted from face identification literature, TBT is specifically designed for face verification and clarifies the intrinsic distinction between these two tasks. Experiments on three benchmark face databases demonstrate that TBT significantly outperforms the state-of-the-art subspace techniques for face verification	algorithm;benchmark (computing);database;dimensionality reduction;exhibits as topic;experiment;global illumination;iteration;learning disorders;machine learning;stability (learning theory);transformation matrix;verification of theories	Shuicheng Yan;Dong Xu;Xiaoou Tang	2007	IEEE Transactions on Image Processing	10.1109/TIP.2006.884939	facial recognition system;feasibility study;computer science;artificial intelligence;machine learning;affine transformation;mathematics;algorithm	Vision	23.64331409760869	-39.2023422987297	161572
a4ae3aadc22f2e6684acd71322c337a0958700ba	top-k multi-class svm using multiple features		Abstract Recent studies have demonstrated the advantages of fusing multi-modal features in improving the accuracy of visual object classification. However, regarding a complex classification task with a large number of categories, previous studies on multiple feature fusion are prone to failure resulting from the occurrence of class ambiguity. In this paper, we address this issue by allowing k  ( k  ≥ 2) guesses at the top instead of only considering the one with the largest prediction score in the framework of multi-view learning. This strategy relaxes the penalty for making an error in the top- k predictions, which can mitigate the challenge of class ambiguity to some extent. To fuse multiple features effectively, we introduce an adaptive weight for each view and exploit an efficient alternating optimization algorithm to learn the optimal classifiers and their corresponding weights jointly. Extensive experiments on several benchmark datasets illustrate the effectiveness and superiority of the proposed model over the state-of-the-art approaches.		Caixia Yan;Minnan Luo;Huan Liu;Zhihui Li;Qinghua Zheng	2018	Inf. Sci.	10.1016/j.ins.2017.08.004	machine learning;support vector machine;artificial intelligence;ambiguity;exploit;pattern recognition;computer science	AI	23.66508494508745	-44.082899189089915	161648
390ad7873aeb9445ee441f4bdd2d4ce3495b449b	influence of minority class instance types on smote imbalanced data oversampling		Despite more than two decades of intense research, learning from imbalanced data still remains as one of the major difficulties posed for computational intelligence systems. Among plethora of techniques dedicated to alleviating this problem, preprocessing algorithms are considered among the most efficient ones. They aim at re-balancing the training set by either undersampling of the majority class, or oversampling of the minority one. Here, Synthetic Minority Oversampling Technique, commonly known as SMOTE, stands as the most popular solution that introduces artificial instances on the basis of minority class neighborhood distribution. However, many recent works point out to the fact that the imbalanced ratio itself is not the sole source of learning difficulties in such scenarios. One should take a deeper look into the minority class structure in order to identify which instances influence the performance of classifiers in most significant manner. In this paper, we propose to investigate the role of minority class instance types on the performance of SMOTE. To achieve this, instead of oversampling uniformly the minority class, we preprocess only selected subsets of instances, based on their individual difficulties. Experimental study proves that such a selective oversampling leads to improved classification performance.	approximation algorithm;cluster analysis;computation;computational intelligence;data pre-processing;experiment;instance (computer science);oversampling;preprocessor;synthetic intelligence;test set;undersampling	Przemyslaw Skryjomski;Bartosz Krawczyk	2017			oversampling;artificial intelligence;business;pattern recognition	AI	14.328701221687657	-41.544029994089684	162117
2a9dd1cd5cc72ff546479f9bcb176da3db0b3523	matrix factorization for recommendation with explicit and implicit feedback		Abstract Matrix factorization (MF) methods have proven as efficient and scalable approaches for collaborative filtering problems. Numerous existing MF methods rely heavily on explicit feedback. Typically, these data types may be extremely sparse; therefore, these methods can perform poorly. In order to address these challenges, we propose a latent factor model based on probabilistic MF, by incorporating implicit feedback as complementary information. Specifically, the explicit and implicit feedback matrices are decomposed into a shared subspace simultaneously. Then, the latent factor vectors are jointly optimized using a gradient descent algorithm. The experimental results using the MovieLens datasets demonstrate that the proposed algorithm outperforms the baselines.		Shulong Chen;Yuxing Peng	2018	Knowl.-Based Syst.	10.1016/j.knosys.2018.05.040	data type;machine learning;collaborative filtering;artificial intelligence;probabilistic logic;matrix (mathematics);gradient descent;subspace topology;computer science;matrix decomposition;movielens	DB	22.839618743338693	-43.611431880233106	162199
0c7884037fdfc846e980e9d7294d534ab5db28d1	classifier ensembles for image identification using multi-objective pareto features		In this paper we propose classifier ensembles that use multiple Pareto image features for invariant image identification. Different from traditional ensembles that focus on enhancing diversity by generating diverse base classifiers, the proposed method takes advantage of the diversity inherent in the Pareto features extracted using a multi-objective evolutionary Trace Transform algorithm. Two variants of the proposed approach have been implemented, one using multilayer perceptron neural networks as base classifiers and the other k-Nearest Neighbor. Empirical results on a large number of images from the Fish-94 and COIL-20 datasets show that on average, ensembles using Pareto features perform much better than traditional classifier ensembles using the same features and data randomization. The better classification performance of the proposed ensemble is further supported by diversity analysis using a number of measures, indicating that the proposed ensemble consistently produces a higher degree of diversity than traditional ones. Our experimental results demonstrate that the proposed classifier ensembles are robust to various geometric transformations in images such as rotation, scale and translation, and to additive noise.	additive white gaussian noise;algorithm;artificial neural network;multilayer perceptron;neural ensemble;pareto efficiency;utility functions on indivisible goods	Wissam A. Albukhanajer;Yaochu Jin;Johann A. Briffa	2017	Neurocomputing	10.1016/j.neucom.2017.01.067	machine learning;pattern recognition;data mining;mathematics;statistics	AI	13.147144733381053	-42.66615388648904	162249
9df950fd79304000dfd23c7c497e8912a613027c	transductive propagation network for few-shot learning		Few-shot learning aims to build a learner that quickly generalizes to novel classes even when a limited number of labeled examples (so-called low-data problem) are available. Meta-learning is commonly deployed to mimic the test environment in a training phase for good generalization, where episodes (i.e., learning problems) are manually constructed from the training set. This framework gains a lot of attention to few-shot learning with impressive performance, though the low-data problem is not fully addressed. In this paper, we propose Transductive Propagation Network (TPN), a transductive method that classifies the entire test set at once to alleviate the low-data problem. Specifically, our proposed network explicitly learns an underlying manifold space that is appropriate to propagate labels from few-shot examples, where all parameters of feature embedding, manifold structure, and label propagation are estimated in an end-to-end way on episodes. We evaluate the proposed method on the commonly used miniImageNet and tieredImageNet benchmarks and achieve the state-of-the-art or promising results on these datasets.	computation;cross entropy;deployment environment;end-to-end principle;euclidean distance;ground truth;meta learning (computer science);semi-supervised learning;semiconductor industry;software propagation;test set;transduction (machine learning)	Yanbin Liu;Juho Lee;Minseop Park;Saehoon Kim;Yi Yang	2018	CoRR		manifold;machine learning;embedding;training set;transduction (machine learning);computer science;test set;artificial intelligence	ML	23.090510255131125	-49.26034456640824	162283
45928b349798140bc463baafda3dec3901b033b3	the hidden neurons selection of the wavelet networks using support vector machines and ridge regression	ridge regression;linear program;support vector machine;hidden neurons selection;wavelet network	A 1-norm support vector machine stepwise (SVMS) algorithm is proposed for the hidden neurons selection of wavelet networks (WNs). In this new algorithm, the linear programming support vector machine (LPSVM) is employed to pre-select the hidden neurons, and then a stepwise selection algorithm based on ridge regression is introduced to select hidden neurons from the pre-selection. The main advantages of the new algorithm are that it can get rid of the influence of the ill conditioning of the matrix and deal with the problems that involve a great number of candidate neurons or a large size of samples. Four examples are provided to illustrate the efficiency of the new algorithm. r 2007 Elsevier B.V. All rights reserved.	linear programming;neuron;selection algorithm;stepwise regression;support vector machine;the matrix;wavelet	Min Han;Jia Yin	2008	Neurocomputing	10.1016/j.neucom.2007.12.009	support vector machine;computer science;linear programming;machine learning;pattern recognition;data mining;mathematics;tikhonov regularization	ML	22.48725627251365	-39.65668241849485	162325
0522fdd8efe0e4baa5602d1384047a5705613765	supervised machine learning: a review of classification techniques	learning algorithms;data mining;intelligent data analysis;classifiers	The goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various supervised machine learning classification techniques. Of course, a single chapter cannot be a complete review of all supervised machine learning classification algorithms (also known induction classification algorithms), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.	machine learning;supervised learning	Sotiris B. Kotsiantis	2007			semi-supervised learning;unsupervised learning;confusion matrix;computer science;online machine learning;machine learning;linear classifier;pattern recognition;data mining;supervised learning;stability;one-class classification;generalization error	ML	15.536593349526727	-40.25624514294575	162326
8d2a78a1c233f0da72f616689c7f2aa47baa0c7d	graph regularized non-negative matrix factorization by maximizing correntropy	unsupervised clustering;cbir;image clustering;machine learning;nonnegative matrix factorization;期刊论文;image retrieval	Non-negative matrix factorization (NMF) has proved effective in many clustering and classification task s. The classic ways to measure the errors between the original and the reconstructed matrix are l2 distance or KullbackLeibler (KL) divergence. However, nonlinear cases are not properly handled when we use these error measures. As a consequence, alternative measures based on nonlinear kernels, such as correntropy, are proposed. However, the current correntropy-based NMF only targets on the lowlevel features without considering the intrinsic geometri cal distribution of data. In this paper, we propose a new NMF algorithm that preserves local invariance by adding graph regularization into the process of max-correntropybased matrix factorization. Meanwhile, each feature can learn corresponding kernel from the data. The experiment results of Caltech101 and Caltech256 show the benefits of such combination against other NMF algorithms for the unsupervised image clustering. I. I NTRODUCTION Given a collection of images, the clustering algorithms attempt to group the dataset into multiple clusters such that images in the same cluster are similar to each other in terms of the semantics information. In this process, a good feature extraction method is vital to the clustering performance. Essentially, clustering (e.g. k-means) or classification algorithms (e.g. support vector machines [1]–[4]) map the low-level image features to semantic information. These algorithms have a variety of applications in different areas [5]–[19]. If the extracted image feature s can reflect the latent semantic concepts, we believe it can somehow better boost the clustering/classification performance. Recently, non-negative matrix factorization (NMF) has proven to be a powerful matrix factorization tool for data representation. Matrix factorization decomposes the orig inal matrixX into multiple low-rank matrices, such that their product approximates X . In NMF,X is decomposed into two non-negative matrices H (basis matrix) andW (coefficient matrix). The vectors inH spans a latent semantic space where each basis vector defines a semantic Manuscript submitted to Journal of Computers. topic. By doing so, unlike other matrix factorization methods, such as singular value decomposition, that interpret the data as both additive and subtractive combination of semantics NMF only allows additive relationship. This constraint has proved to be closer to the way how humans perceive and understand the data [20]–[22]. Based on this methodology, we can map the low-level image features into the additive combination of latent semantics for the clustering. Extensive work has been done to investigate the NMF algorithm for different clustering tasks. Generally, the matrix decomposition is done by minimizing the errors between original and reconstructed matrix using l2 distance or KL divergence [23]–[25]. One of the concerns is that they are linear similarity measures, which may not be suitable for data with nonlinear structure, like images [26 ]. A possible solution is to use nonlinear similarity measure to model the error (e.g. kernlized nonlinear mapping [27], [28]). Among these nonlinear methods, we are especially interested in the NMF based on maximizing correntropy criterion (MCC). Correntropy is a generalized nonlinear measure between two variables. MCC-based methods have proved effective in many areas, e.g. cancer clustering [29], face recognition [30] and software defect prediction [31]. Another approach is preserving the geometric structure of data based on the manifold assumption of data distribution. To be more specific, the authors in [32], [33] exploit the local invariance and encode such geometrical information by constructing a nearest neighbor graph. Thus, at least two factors are included in modeling this process: the distance between X andH ∗ W based on l2 distance (or KL divergence) at low-level feature space, and the graph regularization. Furthermore, the authors in [34] incorporate such intrinsic geometric information in multiple manifolds. In this paper, we propose a graph regularized NMF algorithm based on maximizing correntropy criterion for unsupervised image clustering. We can leverage MCC to properly model the errors in low-level feature space. Furthermore, the graph regularization can keep the correct geometric information in our factorization process. To our knowledge, this is the first work that combines MCC and graph regularization in NMF. Meanwhile, our proposed algorithm can self-learn the kernels from the data to evolve for clustering task. The results on two datasets (Caltech101 and Caltech256) shows the supremacy of our proposed algorithm over other variants of NMF algorithms. The paper is organized into six sections: related work is discussed in Section II. In section III, our proposed model and the convergence proof are explained. The experiment settings are mentioned in Section IV. The results and further discussions are shown in Section V. The last section is the conclusion. II. RELATED WORK K-means is perhaps one of the most popular clusteirng algorithms. This algorithm finds the match cluster for each data point by minimizing distance between the data point and existing clusters’ centroids. The clusters’ centroids are updated at each iteration when a new cluster member is introduced. Similar to Naive Bayes and Gaussian mixture model [35], [36] that have their own assumption of data distribution,K-means algorithm assumes that data within the same class should also be close to each other in the feature space. One potential concern is that if the dataset properties don’t follow such assumptions, the accuracy of the algorithm may be at risk. To solve the above issue, some methods (e.g. Latent Semantics Indexing (LSI) [37]) try to map the data into the latent semantics space where each basis axis in such space represents one type of semantic information of the dataset. Thus, we represent each image as a combination of multiple semantics information. Then we can use any clustering or classification algorithm to directly work on these semantic features. In LSI, the coefficients of the combination could be either positive or negative. However, a negative coefficient could be difficult to interpret. Meanwhile, LSI algorithm requires the basis in semantic space to be orthogonal from each other. This property, on one hand, ensures that we will have a unique solution [38]; on the other hand, it indicates that the semantics basis are distinguished from each other, which, however, is not always the case. NMF is similar to LSI algorithm in the way that they both map the dataset into the latent feature space. However, the basis in latent feature space are not necessarily orthogonal from each other in NMF. Meanwhile, each basis now corresponds to one topic of the dataset. The key benefit is that we can easily find the category of the image by simply investigating the largest components in the latent space. Furthermore, every element in the two decomposed low-rank matrices are non-negative. This additive combination makes it easier to interpret an image intuitively. NMF has been investigated heavily for image clustering [23], [24], [39], [40]. The classic NMF mainly targets on minimizing the l2 distance or KL divergence. One issue with it is that it may not be able to handle the nonlinear data. Different algorithms have been proposed to solve this problem. One way is to use nonlinear distance measure, such as correntropy [29], [30], kernelized distance [28], etc. Another approach is preserving the geometric structure of data based on the manifold assumption. For example, the authors in [32] exploit the local invariance and encode such geometrical information by constructing a nearest neighbor graph. In a similar way, the authors in [41], [42] incorporate a portion of true labels into the graph-regularized method (i.e. semi-supervised learning ). The key ideas are to map samples with same ground-truth labels onto the same point in the semantic feature space. Some other solutions can be found in [43]. III. A LGORITHMS A. NMF algorithm Assuming we have a matrix X ∈ R . NMF allows us to factorizeX into two non-negative matrices H ∈ R D×K andW ∈ R , where the product of H ∗W approximates original matrixX . Each column inX is a feature vector of the image with D elements. Thus, X represents the whole dataset with N images. Conventionally, we name H as basis matrix such that each column forms a basis vector of the semantic feature space, and W as coefficient matrix. Hence, an image is further represented as the additive combination of weighted basis vectors in semantic space. l2 norm distance (Equation 1) and Kullback-Leibler (KL) divergence (Equation 2) are two commonly-used measures of the similarity between original matrixX and the product of H andW , wherel2 norm distance is:	akaike information criterion;algorithm;apache axis;basis (linear algebra);cluster analysis;coefficient;data (computing);data point;distance matrix;encode;euclidean distance;facial recognition system;feature (computer vision);feature extraction;feature vector;high- and low-level;iteration;k-means clustering;kl-one;kernel method;kullback–leibler divergence;low-rank approximation;mixture model;naive bayes classifier;non-negative matrix factorization;nonlinear system;section 508 amendment to the rehabilitation act of 1973;semi-supervised learning;semiconductor industry;similarity measure;singular value decomposition;software bug;statistical classification;supervised learning;support vector machine;supremacy: your will be done;the matrix;utility functions on indivisible goods	Le Li;Jianjun Yang;Kaili Zhao;Honggang Zhang;Zhuoyi Fan	2014	JCP	10.4304/jcp.9.11.2570-2579	mathematical optimization;image retrieval;computer science;machine learning;pattern recognition;mathematics;non-negative matrix factorization	ML	24.500010191109375	-42.01324702248728	162405
65442febdb5586ea8e7e0105d33985dc149bb761	discriminative hierarchical k-means tree for large-scale image classification	histograms;training;support vector machine svm hierarchical k means tree hktree image classification large scale naive bayesian nearest neighbor nbnn;testing;accuracy;artificial neural networks;vectors;sun;trees mathematics bayes methods image classification learning artificial intelligence;vectors accuracy histograms training artificial neural networks sun testing;naive bayesian nn based approaches discriminative hierarchical k means tree large scale image classification d hktree scheme classification accuracy nonparametric nearest neighbor based classifiers learning based classifiers hierarchical support vector machines based methods memory requirement	A key challenge in large-scale image classification is how to achieve efficiency in terms of both computation and memory without compromising classification accuracy. The learning-based classifiers achieve the state-of-the-art accuracies, but have been criticized for the computational complexity that grows linearly with the number of classes. The nonparametric nearest neighbor (NN)-based classifiers naturally handle large numbers of categories, but incur prohibitively expensive computation and memory costs. In this brief, we present a novel classification scheme, i.e., discriminative hierarchical K-means tree (D-HKTree), which combines the advantages of both learning-based and NN-based classifiers. The complexity of the D-HKTree only grows sublinearly with the number of categories, which is much better than the recent hierarchical support vector machines-based methods. The memory requirement is the order of magnitude less than the recent Naïve Bayesian NN-based approaches. The proposed D-HKTree classification scheme is evaluated on several challenging benchmark databases and achieves the state-of-the-art accuracies, while with significantly lower computation cost and memory requirement.	benchmark (computing);categories;cellular automaton;class;comparison and contrast of classification schemes in linguistics and metadata;computation (action);computational complexity theory;computer vision;database;k-means clustering;k-nearest neighbors algorithm;multi-level cell;single linkage cluster analysis;support vector machine;replication compartment	Shizhi Chen;Xiaodong Yang;Yingli Tian	2015	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2014.2366476	sun microsystems;computer science;machine learning;pattern recognition;data mining;histogram;mathematics;accuracy and precision;software testing;artificial neural network;statistics	Vision	17.253805561840945	-43.933587052935124	162881
325827d5ca0bb41cd62e4ce9c68a1140961b2674	doping: generative data augmentation for unsupervised anomaly detection with gan		Recently, the introduction of the generative adversarial network (GAN) and its variants has enabled the generation of realistic synthetic samples, which has been used for enlarging training sets. Previous work primarily focused on data augmentation for semi-supervised and supervised tasks. In this paper, we instead focus on unsupervised anomaly detection and propose a novel generative data augmentation framework optimized for this task. In particular, we propose to oversample infrequent normal samples - normal samples that occur with small probability, e.g., rare normal events. We show that these samples are responsible for false positives in anomaly detection. However, oversampling of infrequent normal samples is challenging for real-world high-dimensional data with multimodal distributions. To address this challenge, we propose to use a GAN variant known as the adversarial autoencoder (AAE) to transform the high-dimensional multimodal data distributions into low-dimensional unimodal latent distributions with well-defined tail probability. Then, we systematically oversample at the 'edge' of the latent distributions to increase the density of infrequent normal samples. We show that our oversampling pipeline is a unified one: it is generally applicable to datasets with different complex data distributions. To the best of our knowledge, our method is the first data augmentation technique focused on improving performance in unsupervised anomaly detection. We validate our method by demonstrating consistent improvements across several real-world datasets.		Swee Kiat Lim;Yi Loo;Ngoc-Trung Tran;Ngai-Man Cheung;Gemma Roig;Yuval Elovici	2018	2018 IEEE International Conference on Data Mining (ICDM)	10.1109/ICDM.2018.00146	anomaly detection;machine learning;autoencoder;generative grammar;artificial intelligence;oversampling;decoding methods;false positive paradox;computer science;complex data type	DB	19.911636956593373	-49.33689043035181	162884
c94e29285cbda57f7349870a4838e4d4971e31db	support vector machine pairwise classifiers with error reduction for image classification	empirical study;error reduction;region importance;image classification;region based image retrieval;feedback;support vector machine;classification accuracy	In this paper we study how Support Vector Machines (SVMs) can be applied to image classification. To enhance classification accuracy, we normalize SVM pairwise classification results. From empirical study on a fifteen-category diversified image set, we show that combining pairwise SVMs and error reduction is an effective approach from image classification. This study is a critical step for our on-going effort on the development of a comprehensive approach, closely adapted to SVMs, to image classification.	computer vision;normalization (image processing);support vector machine	Kingshy Goh;Edward Y. Chang;Kwang-Ting Cheng	2001		10.1145/500933.500944	support vector machine;contextual image classification;computer science;machine learning;pattern recognition;data mining;feedback;empirical research;relevance vector machine;structured support vector machine	ML	15.21186644750596	-46.00250784093527	162893
99e86caec3b53b2fb16126ca95bda49d06d37b24	deep unsupervised learning through spatial contrasting		Convolutional networks have marked their place over the last few years as the best performing model for various visual tasks. They are, however, most suited for supervised learning from large amounts of labeled data. Previous attempts have been made to use unlabeled data to improve model performance by applying unsupervised techniques. These attempts require different architectures and training methods. In this work we present a novel approach for unsupervised training of Convolutional networks that is based on contrasting between spatial regions within images. This criterion can be employed within conventional neural networks and trained using standard techniques such as SGD and back-propagation, thus complementing supervised methods.	artificial neural network;backpropagation;convolutional neural network;electronic filter topology;semi-supervised learning;semiconductor industry;software propagation;supervised learning;unsupervised learning	Elad Hoffer;Itay Hubara;Nir Ailon	2016	CoRR		semi-supervised learning;unsupervised learning;computer science;machine learning;pattern recognition;data mining;deep learning	ML	22.268166226176742	-50.83465157552008	163417
02d45827d6b56f8ac81cab1cb4296ec046869bea	variable selection for efficient design of machine learning-based models: efficient approaches for industrial applications		In many real word applications of neural networks and other machine learning approaches, large experimental datasets are available, containing a huge number of variables, whose effect on the considered system or phenomenon is not completely known or not deeply understood. Variable selection procedures identify a small subset from original feature space in order to point out the input variables, which mainly affect the considered target. The identification of such variables leads to very important advantages, such as lower complexity of the model and of the learning algorithm, savings of computational time and improved performance. Moreover, variable selection procedures can help to acquire a deeper knowledge of the considered problem, system or phenomenon by identifying the factors which mostly affect it. This concept is strictly linked to the crucial aspect of the stability of the variable selection, defined as the sensitivity of a machine learning model with respect to variations in the dataset that is exploited in its training phase. In the present review, different categories of variable section procedures are presented and discussed, in order to highlight strengths and weaknesses of each method in relation to the different tasks and to the variables of the considered dataset.	feature selection;machine learning	Silvia Cateni;Valentina Colla	2016		10.1007/978-3-319-44188-7_27	entropy (information theory);artificial neural network;mutual information;binary classification;feature selection;machine learning;strengths and weaknesses;feature vector;artificial intelligence;phenomenon;computer science	EDA	14.976771469072345	-43.2270752286355	163504
712f55956d61d2879b91bef9e06445da90a1310d	exploit more information of the sample for representation based face recognition	biometrics;classification methods;feature space;face recognition	Besides the representation based classification method defined in the original space for face recognition, the method has also been extended to the feature space. As defined in the original space the method uses a linear combination of all the training samples to approximate the test sample in the original space and then exploits the determined linear combination of all the training samples of each class to classify the test sample. As defined in the feature space the method first implicitly transforms the sample into a high-dimensional space, then classify the test sample in the feature space. In this paper, we propose to integrate the original data of the sample and its representation in the feature space for face recognition based on their respective rationales. The proposed method outperforms the representation based classification method defined in the original space.	facial recognition system	Kezong Tang;Xuan Xiao;Zuoyong Li	2015	IJWMC	10.1504/IJWMC.2015.070938	facial recognition system;computer vision;feature vector;computer science;machine learning;pattern recognition;biometrics	Vision	24.01545643445594	-41.44386780681797	163617
0b1c1c32b7ab78106590bf76fd4261aa6ce25599	transfer boosting with synthetic instances for class imbalanced object recognition	boosting training data object recognition training robustness support vector machines convergence;transfer boosting class imbalanced learning object recognition synthetic instances	A challenging problem in object recognition is to train a robust classifier with small and imbalanced data set. In such cases, the learned classifier tends to overfit the training data and has low prediction accuracy on the minority class. In this paper, we address the problem of class imbalanced object recognition by combining synthetic minorities over-sampling technique (SMOTE) and instance-based transfer boosting to rebalance the skewed class distribution. We present ways of generating synthetic instances under the learning framework of transfer Adaboost. A novel weighted SMOTE technique (WSMOTE) is proposed to generate weighted synthetic instances with weighted source and target instances at each boosting round. Based on WSMOTE, we propose a novel class imbalanced transfer boosting algorithm called WSMOTE-TrAdaboost and experimentally demonstrate its effectiveness on four datasets (Office, Caltech256, SUN2012, and VOC2012) for object recognition application. Bag-of-words model with SURF features and histogram of oriented gradient features are separately used to represent an image. We experimentally demonstrated the effectiveness and robustness of our approaches by comparing it with several baseline algorithms in boosting family for class imbalanced learning.	adaboost;algorithm;bag-of-words model;baseline (configuration management);diffusion weighted imaging;experiment;gradient;naive bayes classifier;name;outline of object recognition;overfitting;oversampling;population parameter;sampling (signal processing);self-balancing binary search tree;speeded up robust features;statistical classification;synthetic data;synthetic intelligence	Xuesong Zhang;Yan Zhuang;Wei David Wang;Witold Pedrycz	2018	IEEE Transactions on Cybernetics	10.1109/TCYB.2016.2636370	adaboost;robustness (computer science);machine learning;boosting (machine learning);overfitting;brownboost;cognitive neuroscience of visual object recognition;training set;histogram;computer science;artificial intelligence;pattern recognition	Vision	14.990752127818439	-41.66562013572045	163672
092dd7cb6c9b415eb83afb104fa63d7d4290ac33	kernel subclass support vector description for face and human action recognition	kernel;standards;support vector machines;training;training data;face recognition support vector machines optimization kernel training training data standards;support vector machines face recognition feature extraction image classification optimisation;face recognition;arbitrary dimensionality kernel subclass support vector data description classifier face recognition human action recognition svdd optimization problem feature spaces;optimization	In this paper, we present the Kernel Subclass Support Vector Data Description classifier. We focus on face recognition and human action recognition applications, where we argue that sub-classes are formed within the training class. We modify the standard SVDD optimization problem, so that it exploits subclass information in its optimization process. We extend the proposed method to work in feature spaces of arbitrary dimensionality. We evaluate the proposed method in publicly available face recognition and human action recognition datasets. Experimental results have shown that increased performance can be obtained by employing the proposed method.	facial recognition system;kernel (operating system);mathematical optimization;optimization problem	Vasileios Mygdalis;Alexandros Iosifidis;Anastasios Tefas;Ioannis Pitas	2016	2016 First International Workshop on Sensing, Processing and Learning for Intelligent Machines (SPLINE)	10.1109/SPLIM.2016.7528409	kernel method;machine learning;pattern recognition;data mining;mathematics	Vision	24.558413579537614	-41.704773651089106	163722
1aed7c5df635c2085c5bc607ce4164067aeb87cb	efficient learning on point sets	kernel;complexity theory;approximation algorithms;learning artificial intelligence data mining;training;image classification;image classification point set collective data fast efficient large scale kmeans;data mining;kernel vectors training approximation methods approximation algorithms feature extraction complexity theory;fast;efficient;large scale;collective data;vectors;kmeans;point sets k means condensing algorithm speed algorithm space requirements efficient learning;feature extraction;point set;approximation methods;learning artificial intelligence	Recently several methods have been proposed to learn from data that are represented as sets of multidimensional vectors. Such algorithms usually suffer from the high demand of computational resources, making them impractical on large-scale problems. We propose to solve this problem by condensing i.e. reducing the sizes of the sets while maintaining the learning performance. Three methods are examined and evaluated with a wide spectrum of set learning algorithms on several large-scale image data sets. We discover that k-Means can successfully achieve the goal of condensing. In many cases, k-Means condensing can improve the algorithms' speed, space requirements, and surprisingly, learning performances simultaneously.	algorithm;computation;computational resource;k-means clustering;machine learning;performance;requirement	Liang Xiong;Barnabás Póczos;Jeff G. Schneider	2013	2013 IEEE 13th International Conference on Data Mining	10.1109/ICDM.2013.59	mathematical optimization;contextual image classification;kernel;feature extraction;computer science;machine learning;pattern recognition;data mining;mathematics;approximation algorithm;k-means clustering	Robotics	18.479899481130243	-44.63738785778879	163770
42bf92fc1cc6f18a7ebb27314cdf051b418e0c6b	efficient attributed network embedding via recursive randomized hashing		Attributed network embedding aims to learn a lowdimensional representation for each node of a network, considering both attributes and structure information of the node. However, the learning based methods usually involve substantial cost in time, which makes them impractical without the help of a powerful workhorse. In this paper, we propose a simple yet effective algorithm, named NetHash, to solve this problem only with moderate computing capacity. NetHash employs the randomized hashing technique to encode shallow trees, each of which is rooted at a node of the network. The main idea is to efficiently encode both attributes and structure information of each node by recursively sketching the corresponding rooted tree from bottom (i.e., the predefined highest-order neighboring nodes) to top (i.e., the root node), and particularly, to preserve as much information closer to the root node as possible. Our extensive experimental results show that the proposed algorithm, which does not need learning, runs significantly faster than the state-of-the-art learning-based network embedding methods while achieving competitive or even better performance in accuracy.	big data;computation;encode;hash function;randomized algorithm;recursion (computer science);simulation;time complexity;tree (data structure)	Wei Wu;Bin Li;Ling Chen;Chengqi Zhang	2018		10.24963/ijcai.2018/397	artificial intelligence;hash function;machine learning;recursion;embedding;computer science	AI	20.83717609274249	-47.25644262679616	163857
d0aeffd3b7b9aa7cb76caa1c09bc8597109ed5ef	inferring fuzzy cognitive map models for gene regulatory networks from gene expression data	biology computing;complex networks;ant colony optimisation;learning algorithm;cognitive systems;inference algorithms heuristic algorithms gene expression bayesian methods fuzzy cognitive maps biological system modeling differential equations;ant colony optimization;dream project fuzzy cognitive map models gene regulatory networks gene expression data genetic causal relations cellular function disease mechanism boolean networks bayesian networks differential equations fuzzy cognitive maps fcm grn representation ant colony optimization;genetics;fuzzy logic;gene expression;fuzzy cognitive map;genetics ant colony optimisation biology computing cellular biophysics cognitive systems complex networks fuzzy logic;gene regulatory network;cellular biophysics;learning algorithm fuzzy cognitive map gene regulatory network gene expression ant colony optimization	Gene Regulatory Networks (GRNs) represent the causal relations among the genes and provide insight on the cellular functions and the mechanism of the diseases. GRNs can be inferred from gene expression data by a number of algorithms, e.g. Boolean networks, Bayesian networks, and differential equations. While reliable inference of GRNs is still an open problem, new algorithms need to be developed. Fuzzy Cognitive Maps (FCMs) is used to represent GRNs in this paper. Most of the FCM learning algorithms are able to learn FCMs with less than 40 nodes. A new algorithm that is able to learn FCMs with more than 100 nodes is proposed. The proposed method is based on Ant Colony Optimization (ACO). A decomposed approach is proposed to reduce the dimension of the problem; therefore the FCM learning algorithm is more scalable (the dimension of the problem to be solved in one ACO run equals to the number of nodes or genes). The proposed approach is tested on data from DREAM project. The experiment results suggest the proposed approach outperforms several other algorithms.	algorithm;ant colony optimization algorithms;causal filter;discretization;dynamic bayesian network;fuzzy cognitive map;gene regulatory network;machine learning;nonlinear system;scalability	Ye Chen;Lawrence J. Mazlack;Long J. Lu	2012	2012 IEEE International Conference on Bioinformatics and Biomedicine	10.1109/BIBM.2012.6392627	fuzzy logic;biology;gene regulatory network;ant colony optimization algorithms;gene expression;fuzzy cognitive map;computer science;bioinformatics;artificial intelligence;machine learning;genetics;complex network	Robotics	12.351765067796677	-51.72821884917157	164183
92c45e16306271a66a952be25ee7ce99cf961285	distributed quadratic programming solver for kernel svm using genetic algorithm	quadratic programming;kernel;support vector machines;training;support vector machines training genetic algorithms encoding quadratic programming graphics processing units kernel;graphics processing units;genetic algorithms;computer science;support vector machines distributed programming genetic algorithms graphics processing units matrix multiplication pattern classification quadratic programming;encoding;matrix multiplication distributed quadratic programming solver kernel svm genetic algorithm classification problems geneticsvm evolutionary computing based distributed approach qp kernel support vector machine graphics processing units gpu	Support vector machine (SVM) is a powerful tool for classification and regression problems, however, its time and space complexities make it unsuitable for large datasets. In this paper, we present GeneticSVM, an evolutionary computing based distributed approach to find optimal solution of quadratic programming (QP) for kernel support vector machine. In Ge-neticSVM, novel encoding method and crossover operation help in obtaining the better solution. In order to train a SVM from large datasets, we distribute the training task over the graphics processing units (GPUs) enabled cluster. It leverages the benefit of the GPUs for large matrix multiplication. The experiments show better performance in terms of classification accuracy as well as computational time on standard datasets like GISETTE, ADULT, etc.	computer graphics;evolutionary computation;experiment;genetic algorithm;graphics processing unit;kernel (operating system);matrix multiplication;quadratic programming;sequential consistency;solver;statistical classification;support vector machine;time complexity	Dinesh Singh;C. Krishna Mohan	2016	2016 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2016.7743790	support vector machine;least squares support vector machine;kernel method;mathematical optimization;kernel;genetic algorithm;radial basis function kernel;computer science;theoretical computer science;machine learning;sequential minimal optimization;quadratic programming;polynomial kernel;structured support vector machine;encoding	ML	20.517636270342052	-38.56065887543708	164458
d7b8f7ccd374026da9bf104d06cf3b71f3406264	ordinal feature selection for iris and palmprint recognition	linear programming training iris recognition optimization biomedical imaging databases boosting;palmprint;ordinal measures;polyu databases ordinal feature selection palmprint recognition iris recognition feature representation model feature space optimization formulation misclassification error interclass matching intra matching ordinal feature descriptors sparse representation linear inequality constraints linear programming problem lp formulation mrmr relieff boosting lasso biometric recognition casia;linear programming;feature selection;palmprint recognition image matching image representation iris recognition linear programming;iris	Ordinal measures have been demonstrated as an effective feature representation model for iris and palmprint recognition. However, ordinal measures are a general concept of image analysis and numerous variants with different parameter settings, such as location, scale, orientation, and so on, can be derived to construct a huge feature space. This paper proposes a novel optimization formulation for ordinal feature selection with successful applications to both iris and palmprint recognition. The objective function of the proposed feature selection method has two parts, i.e., misclassification error of intra and interclass matching samples and weighted sparsity of ordinal feature descriptors. Therefore, the feature selection aims to achieve an accurate and sparse representation of ordinal measures. And, the optimization subjects to a number of linear inequality constraints, which require that all intra and interclass matching pairs are well separated with a large margin. Ordinal feature selection is formulated as a linear programming (LP) problem so that a solution can be efficiently obtained even on a large-scale feature pool and training database. Extensive experimental results demonstrate that the proposed LP formulation is advantageous over existing feature selection methods, such as mRMR, ReliefF, Boosting, and Lasso for biometric recognition, reporting state-of-the-art accuracy on CASIA and PolyU databases.	biometrics;database;feature selection;feature vector;fingerprint;genetic selection;image analysis;iris (eye);lasso;linear inequality;linear programming;loss function;matching;mathematical optimization;optimization problem;ordinal position;ordinal data;part dosing unit;population parameter;social inequality;sparse approximation;sparse matrix	Zhenan Sun;Libin Wang;Tieniu Tan	2014	IEEE Transactions on Image Processing	10.1109/TIP.2014.2332396	internet routing in space;computer science;linear programming;machine learning;pattern recognition;data mining;mathematics;feature selection;feature	Vision	22.109219360297164	-41.050806319860556	164540
57f54c4fd7f51d0eefcb4f8880b9452499d46c3b	tangent-normal adversarial regularization for semi-supervised learning		Compared with standard supervised learning, the key difficulty in semi-supervised learning is how to make full use of the unlabeled data. A recently proposed method, virtual adversarial training (VAT), smartly performs adversarial training without label information to impose a local smoothness on the classifier, which is especially beneficial to semi-supervised learning. In this work, we propose tangent-normal adversarial regularization (TNAR) as an extension of VAT by taking the data manifold into consideration. The proposed TNAR is composed by two complementary parts, the tangent adversarial regularization (TAR) and the normal adversarial regularization (NAR). In TAR, VAT is applied along the tangent space of the data manifold, aiming to enforce local invariance of the classifier on the manifold, while in NAR, VAT is performed on the normal space orthogonal to the tangent space, intending to impose robustness on the classifier against the noise causing the observed data deviating from the underlying data manifold. Demonstrated by experiments on both artificial and practical datasets, ∗Equal contributions. †Corresponding author. our proposed TAR and NAR complement with each other, and jointly outperforms other state-of-the-art methods for semi-supervised learning.	experiment;manifold regularization;matrix regularization;nar 2;semi-supervised learning;semiconductor industry;supervised learning	Bing Yu;Jingfeng Wu;Zhanxing Zhu	2018	CoRR		tangent space;machine learning;semi-supervised learning;robustness (computer science);manifold;supervised learning;smoothness;artificial intelligence;tangent;normal space;mathematics	ML	23.69681050765804	-44.82226671352276	164709
8c0ea3a0edd4e72923091419e6fef5b2a539247f	recurrent neural networks for adaptive feature acquisition		We propose to tackle the cost-sensitive learning problem, where each feature is associated to a particular acquisition cost. We propose a new model with the following key properties: (i) it acquires features in an adaptive way, (ii) features can be acquired per block (several at a time) so that this model can deal with high dimensional data, and (iii) it relies on representation-learning ideas. The effectiveness of this approach is demonstrated on several experiments considering a variety of datasets and with different cost settings.	algorithm;artificial neural network;experiment;gradient descent;machine learning;network architecture;recurrent neural network	Gabriella Contardo;Ludovic Denoyer;Thierry Artières	2016		10.1007/978-3-319-46675-0_65	machine learning;pattern recognition;artificial intelligence;clustering high-dimensional data;computer science;types of artificial neural networks;time delay neural network;recurrent neural network	ML	21.42103332589234	-45.11749824090268	164724
2c6ad6fc7d613510d9c05e3b6ddd67ab78cc9e69	multiple kernel learning with hybrid kernel alignment maximization		Two-stage multiple kernel learning (MKL) algorithms have been extensively researched in recent years due to their high efficiency and effectiveness. Previous works have attempted to optimize the combination coefficients by maximizing the centralized kernel alignment between the combined kernel and the ideal kernel. Though demonstrating previous promising performance, we observe that these algorithms may suffer from the approaching in calculating the alignment. In particular, we observe that the local information should be incorporated when computing the kernel alignment, which is beneficial to further improve the classification performance. To this end, we first define the local kernel alignment based on centralized kernel alignment. A new kernel alignment that combines the global and local information of base kernels is then developed. After that, we propose an alternative algorithm with proved convergence to identify the multiple kernel coefficients. Intensive experimental results show that the performance of the proposed algorithm is superior to those of existing MKL algorithms. © 2017 Elsevier Ltd. All rights reserved.	centralized computing;coefficient;expectation–maximization algorithm;hybrid kernel;kernel (operating system);math kernel library;multiple kernel learning	Yueqing Wang;Xinwang Liu;Yong Dou;Qi Lv;Yao Lu	2017	Pattern Recognition	10.1016/j.patcog.2017.05.005	kernel method;mathematical optimization;kernel embedding of distributions;radial basis function kernel;machine learning;pattern recognition;graph kernel;mathematics;tree kernel;polynomial kernel;kernel smoother	AI	23.333117077752693	-41.029949270588126	164732
4ab008c72750cb00a4c26f03aab0bab98edd36a8	an information-theoretic filter method for feature weighting in naive bayes	naive bayes;data mining;classification;feature weighting	In spite of its simplicity, naive Bayesian learning has been widely used in many data mining applications. However, the unrealistic assumption that all features are equally important negatively impacts the performance of naive Bayesian learning. In this paper, we propose a new method that uses a Kullback–Leibler measure to calculate the weights of the features analyzed in naive Bayesian learning. Its performance is compared to that of other state-of-the-art methods over a number of datasets.	naive bayes classifier;theory	Chang-Hwan Lee	2014	IJPRAI	10.1142/S0218001414510070	naive bayes classifier;bayesian programming;biological classification;computer science;machine learning;pattern recognition;data mining	ML	16.589494094231135	-39.67440937869528	164794
606f026faba347b6f822b7feed7dd5ad4c29ac81	imposing higher-level structure in polyphonic music generation using convolutional restricted boltzmann machines and constraints		We introduce a method for imposing higher-level structure on generated, polyphonic music. A Convolutional Restricted Boltzmann Machine (C-RBM) as a generative model is combined with gradient descent constraint optimisation to provide further control over the generation process. Among other things, this allows for the use of a “template” piece, from which some structural properties can be extracted, and transferred as constraints to the newly generated material. The sampling process is guided with Simulated Annealing to avoid local optima, and to find solutions that both satisfy the constraints, and are relatively stable with respect to the C-RBM. Results show that with this approach it is possible to control the higher-level self-similarity structure, the meter, and the tonal properties of the resulting musical piece, while preserving its local musical coherence.	artificial neural network;code;generative model;gibbs sampling;gradient descent;high- and low-level;level structure;local optimum;mathematical optimization;random neural network;restricted boltzmann machine;sampling (signal processing);self-similarity;simulated annealing;simulation;stochastic neural network;time series	Stefan Lattner;Maarten Grachten;Gerhard Widmer	2016	CoRR	10.5920/jcms.2018.01	speech recognition;machine learning;mathematics;algorithm	ML	22.651035269233844	-48.41634633169193	165683
02db742fd95178eef4623b66e2659b9f26937f5e	knowing what doesn't matter: exploiting the omission of irrelevant data	learning algorithm;adversarial noise;decision tree;blocked attributes;dnf;irrelevant values;theory revision;decision trees;diagnosis;learnability	Most learning algorithms work most e ectively when their training data contain completely speci ed labeled samples In many diagnostic tasks however the data will include the values of only some of the attributes we model this as a blocking process that hides the values of those attributes from the learner While blockers that remove the values of critical attributes can handicap a learner this paper instead focuses on blockers that remove only conditionally irrelevant attribute values i e values that are not needed to classify an instance given the values of the other unblocked attributes We rst motivate and formalize this model of super uous value blocking and then demonstrate that these omissions can be useful by proving that certain classes that seem hard to learn in the general PAC model viz decision trees and DNF formulae are trivial to learn in this setting We then extend this model to deal with theory revision i e modifying an existing formula blockers that occasionally include super uous values or exclude required values and other corruptions of the training data	algorithm;blocking (computing);decision tree;machine learning;relevance;viz: the computer game	Russell Greiner;Adam J. Grove;Alexander Kogan	1997	Artif. Intell.	10.1016/S0004-3702(97)00048-9	computer science;artificial intelligence;machine learning;decision tree;data mining;mathematics	ML	15.575986346380892	-38.223928616478005	165700
06c877218dec6e189103b182f2e0b69c220abd3d	editorial for the special issue on graph-based representations in pattern recognition	settore inf 01 informatica;other	Graph-based representations have been used with considerable success in computer vision and pattern recognition for over 40 years for their representational power and flexibility. Indeed, such representations arise naturally in many low-level and highlevel vision tasks and are in general central to several pattern matching and analysis approaches. However, despite their large use, structural representations pose unique problems in machine learning, due to their non-vectorial nature. The goal of this special issue was to publish high-quality papers that provide a clear picture of the state-of-the-art of the use of graphs as a representational tool in computer vision and pattern recognition, and was meant to disseminate the best and most representative papers presented at the Eighth IAPR TC-15 International Workshop on Graph-based representation (GbR 2011). The guest editors selected 11 papers from 34 candidates and had each reviewed at least by two reviewers, with several being reviewed by three. The papers describe a diverse set of methods and applications. The first paper presented in this special issue ‘‘Graph Characterizations From Von Neumann Entropy’’ by Han et al. analyzes the complexity of structural representations by studying the entropy of the spectra of the graph Laplacians. ‘‘Invariants of Distance k-Graphs for Graph Embedding’’ by Czech presents new graph features based on the degrees of distance k-graphs, axillary structures representing extended neighborhoods, while in ‘‘Feature selection on node statistics based embedding of graphs’’, Gibert et al. address the selection of feature to represent structural information using different feature selection algorithms, such as PCA and ranking, based on the node statistics. Zhang and Hancock, in ‘‘Hypergraph based Information-theoretic Feature Selection’’ addresses the problem of feature selection based on high-order feature dependencies with respect to the class labels. These dependencies are captured by means of an information–theoretic criterion referred to as multidimensional interaction information, and are abstracted in terms of a hypergraph. The most informative features are finally sought using a hypergraph clustering approach. Based on recent results generalizing the central limit theorem on graph orbitfolds, Jain presents a method to infer an approximation of the ‘‘mean’’ and ‘‘standard deviation’’ of a probability distribution that represents graphs in ‘‘Maximum Likelihood Method for Parameter Estimation of Bell-Shaped Functions on Graphs.’’	algorithm;approximation;cluster analysis;computer vision;estimation theory;feature selection;graph embedding;han unification;high- and low-level;interaction information;international association for pattern recognition;machine learning;pattern matching;principal component analysis	Andrea Torsello;Xiaoyi Jiang;Miquel Ferrer	2012	Pattern Recognition Letters	10.1016/j.patrec.2012.08.016	computer science;artificial intelligence;algorithm	Vision	16.73492967248058	-49.236178888590494	165810
215ab83e9af071ed87443669b19e919e8545104a	weighted clone selection algorithm based on rough set theory	attribute weight;classification;clone selection;rough set theory	Clone selection is a new artificial intelligence technology, with self-organization, self-learning, self-recognition, self-memory capacity. In the traditional clone selection algorithm for data classification, all the attributes for classification have the same influence, which affects its classification performance to some extent, given an appropriate weight for each attribute value can modify this imbalance. Accordingly this, proposes a weighted clone selection algorithm based on rough set to improve the performance of clone selection. In weighted clone selection algorithm attribute weights obtained directly from the training data using rough set theory, the attribute weights was used to test Data classification. Then verify the validity of the method by the experiments of UCI data sets.	rough set;selection algorithm;set theory	Jia Wu;Zhihua Cai;Xiaolin Chen;Meng Li;Bin Guo	2013	JSW		rough set;biological classification;computer science;machine learning;pattern recognition;data mining	EDA	10.340556955690097	-39.973378169487795	165816
ce8f734f6b0022a72f03c594bef61412ed472e30	clustering-based bayesian multi-net classifier construction with ant colony optimization	belief networks;q335 artificial intelligence;pattern clustering;ant colony optimisation;pattern clustering ant colony optimisation belief networks learning artificial intelligence pattern classification;clustering algorithms bayes methods equations mathematical model ant colony optimization educational institutions buildings;uci dataset clustering based bayesian multinet classifier ant colony optimization bmn dataset partition k modes algorithm case based bayesian network classifier cbbn approach classifier learning ant clust b algorithm aco;pattern classification;learning artificial intelligence	Bayesian Multi-nets (BMNs) are a special kind of Bayesian network (BN) classifiers that consist of several local networks, typically, one for each predictable class, to model an asymmetric set of variable dependencies given each class value. Alternatively, multi-nets can be learnt upon arbitrary partitions of a dataset, in which each partition holds more consistent variable dependencies given the data subset in the partition. This paper proposes two contributions to the approach that clusters the dataset into separate data subsets to build asymmetric local BN classifiers, one for each subset. First, we extend the K-modes algorithm, previously used by the Case-Based Bayesian Network Classifiers (CBBN) approach to create clusters before learning the BN classifiers. Second, we introduce the Ant-Clust-B algorithm that employs Ant Colony Optimization (ACO) to learn clustering-based BMNs. Ant-Clust-B uses ACO in the clustering step before learning the local BN classifiers. Empirical results are obtained from experiments on 18 UCI datasets.	algorithm;ant colony optimization algorithms;bayesian network;cluster analysis;experiment	Khalid M. Salama;Alex Alves Freitas	2013	2013 IEEE Congress on Evolutionary Computation	10.1109/CEC.2013.6557945	computer science;artificial intelligence;machine learning;pattern recognition;data mining	ML	18.448486241661303	-42.03214568996161	166019
4eac1b02155a28133c47ac5ecdc8d99ca4cacbe3	"""neural system identification for large populations separating """"what"""" and """"where"""""""		Neuroscientists classify neurons into different types that perform similar computations at different locations in the visual field. Traditional methods for neural system identification do not capitalize on this separation of “what” and “where”. Learning deep convolutional feature spaces that are shared among many neurons provides an exciting path forward, but the architectural design needs to account for data limitations: While new experimental techniques enable recordings from thousands of neurons, experimental time is limited so that one can sample only a small fraction of each neuron’s response space. Here, we show that a major bottleneck for fitting convolutional neural networks (CNNs) to neural data is the estimation of the individual receptive field locations – a problem that has been scratched only at the surface thus far. We propose a CNN architecture with a sparse readout layer factorizing the spatial (where) and feature (what) dimensions. Our network scales well to thousands of neurons and short recordings and can be trained end-to-end. We evaluate this architecture on ground-truth data to explore the challenges and limitations of CNN-based system identification. Moreover, we show that our network model outperforms current state-of-the art system identification models of mouse primary visual cortex.	artificial neural network;computation;convolutional neural network;design of experiments;end-to-end principle;ground truth;network model;neuron;population;sparse matrix;system identification	David A. Klindt;Alexander S. Ecker;Thomas Euler;Matthias Bethge	2017			architecture;network model;computer science;machine learning;convolutional neural network;artificial intelligence;visual cortex;bottleneck;system identification;neuron;receptive field	ML	21.602015743137567	-50.77702502606512	166160
1dcc7c59ed04c63e5e5fa0f980d703e3888266a2	relaxing from vocabulary: robust weakly-supervised deep learning for vocabulary-free image tagging	machine learning noise measurement training data vocabulary training robustness tagging;relevance feedback approximation theory backpropagation feature extraction image representation image retrieval;imagenet vocabulary free image tagging image categories weakly supervised deep learning model web images human labors feature map affinity representation low rank approximation objective function relevance feedback back propagation semantic based relevance metric wordnet	The development of deep learning has empowered machines with comparable capability of recognizing limited image categories to human beings. However, most existing approaches heavily rely on human-curated training data, which hinders the scalability to large and unlabeled vocabularies in image tagging. In this paper, we propose a weakly-supervised deep learning model which can be trained from the readily available Web images to relax the dependence on human labors and scale up to arbitrary tags (categories). Specifically, based on the assumption that features of true samples in a category tend to be similar and noises tend to be variant, we embed the feature map of the last deep layer into a new affinity representation, and further minimize the discrepancy between the affinity representation and its low-rank approximation. The discrepancy is finally transformed into the objective function to give relevance feedback to back propagation. Experiments show that we can achieve a performance gain of 14.0% in terms of a semantic-based relevance metric in image tagging with 63,043 tags from the WordNet, against the typical deep model trained on the ImageNet 1,000 vocabulary set.	backpropagation;deep learning;discrepancy function;experiment;imagenet;loss function;low-rank approximation;optimization problem;processor affinity;relevance feedback;scalability;software propagation;vocabulary;wordnet	Jianlong Fu;Yue Wu;Tao Mei;Jinqiao Wang;Hanqing Lu;Yong Rui	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.230	computer vision;computer science;machine learning;pattern recognition;information retrieval	Vision	23.267260169719776	-47.46647758716043	166349
11922fedba3c7b4bbe03a36797eeebe3ab011ad2	a new framework for dissimilarity and similarity learning	learning algorithm;learning experience;learning problems;binary classification;similarity function	In this work we propose a novel framework for learning a (dis)similarity function. We cast the learning problem as a binary classification task or a regression task in which the new learning instances are the pairwise absolute differences of the original instances. Under the classification approach the class label we assign to a specific pairwise difference indicates whether the two original instances associated with the difference are members of the same class or not. Under the regression approach we assign positive target values to the pairwise differences of instances from different classes and negative target values to the differences of instances of the same class. The computation of the (dis)similarity of two examples amounts to the computation of prediction scores for classification, or the prediction of a continuous value for regression. The proposed framework is very general as we are free to use any learning algorithm. Moreover, our formulation generally leads to a (dis-)similarity which, depending on the learning algorithm, can be efficient and simple to learn. Experiments performed on a number of classification problems demonstrate the effectiveness of the proposed approach.	algorithm;binary classification;computation;experiment;large margin nearest neighbor;machine learning;similarity learning;similarity measure;statistical classification	Adam Woznica;Alexandros Kalousis	2010		10.1007/978-3-642-13672-6_38	semi-supervised learning;binary classification;instance-based learning;computer science;machine learning;pattern recognition;data mining;mathematics;generalization error	ML	19.53053932263059	-41.763857544823665	166371
fdbe50ca6741ddad66fb289960624627c6869cfc	weakly semi-supervised deep learning for multi-label image annotation	social network services;ranking loss weakly labeled image unlabeled image deep learning;neural networks;unlabeled image;training semantics machine learning visualization big data neural networks social network services;training;semantics;visualization;machine learning;big data;deep learning;visual databases image retrieval image sampling learning artificial intelligence neural nets;unlabeled image leveraging weakly semisupervised deep learning multilabel image annotation weakly labeled image leveraging benchmark datasets large image datasets high quality image triplet sampling social network images cnn training deep convolutional neural network training triplet similarity loss weakly weighted pairwise ranking loss wesed;ranking loss;weakly labeled image	In this paper, we study leveraging both weakly labeled images and unlabeled images for multi-label image annotation. Motivated by the recent advance in deep learning, we propose an approach called weakly semi-supervised deep learning for multi-label image annotation (WeSed). In WeSed, a novel weakly weighted pairwise ranking loss is effectively utilized to handle weakly labeled images, while a triplet similarity loss is employed to harness unlabeled images. WeSed enables us to train deep convolutional neural network (CNN) with images from social networks where images are either only weakly labeled with several labels or unlabeled. We also design an efficient algorithm to sample high-quality image triplets from large image datasets to fine-tune the CNN. WeSed is evaluated on benchmark datasets for multi-label annotation. The experiments demonstrate the effectiveness of our proposed approach and show that the leverage of the weakly labeled images and unlabeled images leads to a significantly better performance.	algorithm;artificial neural network;automatic image annotation;benchmark (computing);convolutional neural network;deep learning;experiment;multi-label classification;sampling (signal processing);semi-supervised learning;semiconductor industry;signal-to-noise ratio;social network;supervised learning;triplet state	Fei Wu;Zhuhao Wang;Zhongfei Zhang;Yi Yang;Jiebo Luo;Wenwu Zhu;Yueting Zhuang	2015	IEEE Transactions on Big Data	10.1109/TBDATA.2015.2497270	semi-supervised learning;big data;visualization;computer science;machine learning;pattern recognition;data mining;semantics;deep learning	Vision	23.225619766005238	-47.37828075774874	166419
12ff131e8f1e07389af926b3b16abce0aba3fc26	support vector regression as a classification problem with a priori knowledge in the form of detractors		In this article, we propose applying a recently proposed technique of reducing Support Vector Classification models for regression problems. The reduced method creates reduced models by removing support vectors and uses a general formulation of Support Vector Classification with a priori knowledge in the form of detractors. We apply this method for regression problems by using a reformulation of ε-insensitive Support Vector Regression as a special case of this general formulation. Indeed, the experiments show that reduced technique can be successfully applied for regression problems. The tests were performed on various regression data sets and on stock price data from public domain repositories.	computer science;experiment;support vector machine	Marcin Orchel	2011		10.1007/978-3-642-23169-8_38	machine learning;pattern recognition;data mining	ML	20.83862746737149	-39.671073230148515	166917
719f1aa3c32f7c4e02bd7c194cd0dda73ceb2895	learning weighted naive bayes with accurate ranking	markov chain monte carlo method weighted naive bayes learning classification algorithm gain ratio method hill climbing method;bayes methods;naive bayes;pattern classification learning artificial intelligence bayes methods markov processes monte carlo methods;markov chain monte carlo methods;pattern classification;chromium computer science classification algorithms machine learning data mining niobium application software bayesian methods error analysis machine learning algorithms;hill climbing;markov processes;learning artificial intelligence;monte carlo methods	Naive Bayes is one of most effective classification algorithms. In many applications, however, a ranking of examples are more desirable than just classification. How to extend naive Bayes to improve its ranking performance is an interesting and useful question in practice. Weighted naive Bayes is an extension of naive Bayes, in which attributes have different weights. This paper investigates how to learn a weighted naive Bayes with accurate ranking from data, or more precisely, how to learn the weights of a weighted naive Bayes to produce accurate ranking. We explore various methods: the gain ratio method, the hill climbing method, and the Markov chain Monte Carlo method, the hill climbing method combined with the gain ratio method, and the Markov chain Monte Carlo method combined with the gain ratio method. Our experiments show that a weighted naive Bayes trained to produce accurate ranking outperforms naive Bayes.	algorithm;decision tree learning;experiment;hill climbing;markov chain monte carlo;monte carlo method;naive bayes classifier	Harry Zhang;Shengli Sheng	2004	Fourth IEEE International Conference on Data Mining (ICDM'04)	10.1109/ICDM.2004.10030	bayes classifier;naive bayes classifier;bayesian programming;computer science;hill climbing;machine learning;pattern recognition;bayes error rate;mathematics;markov process;statistics;monte carlo method	ML	12.061758966299985	-38.65894311640463	166947
a850c899597791bd14ebcfbb44529815a367fe85	rank aggregation algorithm selection meets feature selection		Rank aggregation is the important task in many areas, and different rank aggregation algorithms are created to find optimal rank. Nevertheless, none of these algorithms is the best for all cases. The main goal of this work is to develop a method, which for each rank list defines, which rank aggregation algorithm is the best for this rank list. Canberra distance is used as a metric for determining the optimal ranking. Three approaches are proposed in this paper and one of them has shown promising result. Also we discuss, how this approach can be applied to learn filtering feature selection algorithm ensemble.	algorithm selection;feature selection	Alexey Zabashta;Ivan Smetannikov;Andrey Filchenkov	2016		10.1007/978-3-319-41920-6_56	pattern recognition;feature selection	ML	11.346831833078951	-43.89646760939463	167430
1180ec5c555e28131b21d8b5bb01048463a70a3f	values deletion to improve deep imputation processes		Most machine learning algorithms are based on the assumption that available data are completely known, nevertheless, real world data sets are often incomplete. For this reason, the ability of handling missing values has become a fundamental requirement for statistical pattern recognition. In this article, a new proposal to impute missing values with deep networks is analyzed. Besides the real missing values, the method introduces a percentage of artificial missing (‘deleted values’) using the true values as targets. Empirical results over several UCI repository datasets show that this method is able to improve the final imputed values obtained by other procedures used as pre-imputation.	geo-imputation	Adrián Sánchez-Morales;José-Luis Sancho-Gómez;Aníbal R. Figueiras-Vidal	2017		10.1007/978-3-319-59773-7_25	machine learning;imputation (statistics);missing data;computer science;artificial intelligence;listwise deletion;data set	Logic	14.848413274750492	-38.66842241208597	167554
11e3d133a7d4321832a3a10b0b54f2d2d0eea4dd	a novel algorithm for feature selection using harmony search and its application for non-technical losses detection	ciencias exatas e da terra;ciencia da computacao;artigo	Finding an optimal subset of features that maximizes classification accuracy is still an open problem. In this paper, we exploit the speed of the Harmony Search algorithm and the Optimum-Path Forest classifier in order to propose a new fast and accurate approach for feature selection. Comparisons to some other pattern recognition and feature selection techniques showed that the proposed hybrid algorithm for feature selection outperformed them. The experiments were carried out in the context of identifying non-technical losses in power distribution systems. 2011 Elsevier Ltd. All rights reserved.	algorithmic efficiency;computation;discriminant;experiment;feature extraction;feature selection;hs algorithm;harmony search;hybrid algorithm;k-nearest neighbors algorithm;kernel principal component analysis;mathematical optimization;memory-level parallelism;pattern recognition;phase-shift oscillator;radial basis function network;search algorithm	Caio C. O. Ramos;André N. de Souza;Giovani Chiachia;Alexandre X. Falcão;João Paulo Papa	2011	Computers & Electrical Engineering	10.1016/j.compeleceng.2011.09.013	computer science;machine learning;pattern recognition;data mining	AI	11.165528633906723	-42.38843210579575	167754
4f039f425abf0cced8347bfb7b3d0e4d0bb9cd84	an improvement of adaboost for face detection with random forests	random forest;face detection	When AdaBoost algorithm is used for face detection, it may easily lead to overfitting problem if training samples contain noise or are difficult to classify. In this paper, we focus on designing an algorithm named AdaBoostRF, using Random Forests as weak learners. To obtain a set of effective Random Forests weak learners, firstly, CART algorithm was used to construct the base classifier. Then the weak classifier was obtained by using simple majority voting method. Finally, the required strong classifier was obtained after T cycles. Compared with the existing AdaBoost methods, the AdaBoostRF provides a possible way to handle the overfitting problem in AdaBoost. Experimental results based on MIT-CBCL face database showed that the detection performance of the AdaBoostRF algorithm has been improved, and its overall performance is better than that of the AdaBoostSVM algorithm. Experimental based on unbalanced data sets of MIT+CMU face database showed that the overfitting problem has been improved effectively.	adaboost;face detection;random forest	Jun-Ying Zeng;Xiao-Hua Cao;Jun-Ying Gan	2010		10.1007/978-3-642-14831-6_4	random forest;face detection;computer science;machine learning;pattern recognition	Vision	14.180328826731328	-40.7155009515714	167767
11e4c155dfae0f1ecbcd011ba078ad88fdfec493	an investigation of a deep learning based malware detection system		We investigate a Deep Learning based system for malware detection. In the investigation, we experiment with different combination of Deep Learning architectures including Auto-Encoders, and Deep Neural Networks with varying layers over Malicia malware dataset on which earlier studies have obtained an accuracy of (98%) with an acceptable False Positive Rates (1.07%). But these results were done using extensive man-made custom domain features and investing corresponding feature engineering and design efforts. In our proposed approach, besides improving the previous best results (99.21% accuracy and an False Positive Rate of 0.19%) indicates that Deep Learning based systems could deliver an effective defense against malware. Since it is good in automatically extracting higher conceptual features from the data, Deep Learning based systems could provide an effective, general and scalable mechanism for detection of existing and unknown malware.	deep learning;encoder;feature engineering;malware;scalability	Mohit Sewak;Sanjay K. Sahay;Hemant Rathore	2018		10.1145/3230833.3230835	false positive rate;data mining;deep learning;artificial neural network;malware;scalability;computer science;feature engineering;artificial intelligence	SE	19.6472516910665	-51.283466070023366	167900
afca48085287de4e7c6b07cfb6f0a526960b4fcc	learning multi-kernel distance functions using relative comparisons	mahalanobis distance;distance function;learning algorithm;multi kernel basis;pattern recognition;basis grouping;distance function learning	In this manuscript, a new form of distance function that can model spaces where a Mahalanobis distance cannot be assumed is proposed. Two novel learning algorithms are proposed to allow that distance function to be learnt, assuming only relativecomparisons training examples. This allows a distance function to be learnt in non-linear, discontinuous spaces, avoiding the need for labelled or quantitative information. The first algorithm builds a set of basic distance bases. The second algorithm improves generalisation capability by merging different distance bases together. It is shown how the learning algorithms produce a distance function for clustering multiple disjoint clusters belonging to the same class. Crucially, this is achieved despite the lack of any explicit form of class labelling on the training data. 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.	algorithm;cluster analysis;kernel (operating system);machine learning;nonlinear system;pattern recognition	Eng-Jon Ong;Richard Bowden	2005	Pattern Recognition	10.1016/j.patcog.2005.05.011	earth mover's distance;gilbert–johnson–keerthi distance algorithm;damerau–levenshtein distance;radial basis function;edit distance;distance matrix;metric;artificial intelligence;mahalanobis distance;machine learning;pattern recognition;mathematics;distance transform	Vision	23.783058598938986	-41.508309009686755	167921
41f50d08e4c237d0f192f5c09f78d7e1d09d9cef	scaling up spike-and-slab models for unsupervised feature learning	unsupervised learning;object recognition;unsupervised learning boltzmann machines feature extraction image coding inference mechanisms object recognition;image coding;neural nets;unsupervised feature learning spike and slab model real valued data modeling object recognition spike and slab sparse coding approximate inference algorithm partially directed deep boltzmann machine s3c inference algorithm feature extractor learning procedures object recognition performance;training;inference mechanisms;computer vision;computer vision neural nets pattern recognition;slabs;vectors;feature extraction;pattern recognition;encoding feature extraction data models training approximation methods vectors slabs;approximation methods;encoding;boltzmann machines;data models	We describe the use of two spike-and-slab models for modeling real-valued data, with an emphasis on their applications to object recognition. The first model, which we call spike-and-slab sparse coding (S3C), is a preexisting model for which we introduce a faster approximate inference algorithm. We introduce a deep variant of S3C, which we call the partially directed deep Boltzmann machine (PD-DBM) and extend our S3C inference algorithm for use on this model. We describe learning procedures for each. We demonstrate that our inference procedure for S3C enables scaling the model to unprecedented large problem sizes, and demonstrate that using S3C as a feature extractor results in very good object recognition performance, particularly when the number of labeled examples is low. We show that the PD-DBM generates better samples than its shallow counterpart, and that unlike DBMs or DBNs, the PD-DBM may be trained successfully without greedy layerwise training.	addresses (publication format);approximation algorithm;boltzmann machine;class;computation;computational problem;dbm;extractors;feature learning;greedy algorithm;image scaling;inference;machine learning;neural coding;outline of object recognition;parkinson disease;randomness extractor;semi-supervised learning;semiconductor industry;slab allocation;sparse matrix;supervised learning;test scaling;tracer;variational principle	Ian J. Goodfellow;Aaron C. Courville;Yoshua Bengio	2013	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2012.273	unsupervised learning;data modeling;computer vision;feature extraction;computer science;theoretical computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;encoding	ML	22.7283876521564	-46.61677611811157	168083
318a9342794b9eb220b52c8147150779c6c414d6	feature selection on wide multiclass problems using ova-rfe	feature selection	Feature selection is a pre–processing technique commonly used with high–dimensional datasets. It is aimed at reducing the dimensionality of the input space, discarding useless or redundant variables, in order to increase the performance and interpretability of models. For multiclass classification problems, recent works suggested that decomposing the multiclass problem in a set of binary ones, and doing the feature selection on the binary problems could be a sound strategy. In this work we combined the well–known Recursive Feature Elimination (RFE) algorithm with the simple One–Vs–All (OVA) technique for multiclass problems, to produce the new OVA–RFE selection method. We evaluated OVA–RFE using wide datasets from genomic and mass– spectrometry analysis, and several classifiers. In particular, we compared the new method with the traditional RFE (applied to a direct multiclass classifier) in terms of accuracy and stability. Our results show that OVA– RFE is no better than the traditional method, which is in opposition to previous results on similar methods. The opposite results are related to a different interpretation of the real number of variables in use by both methods.	algorithm;feature selection;multiclass classification;recursion (computer science);whole earth 'lectronic link	Pablo M. Granitto;Andrés Burgos	2009	Inteligencia Artificial, Revista Iberoamericana de Inteligencia Artificial		computer science;machine learning;multiclass classification;pattern recognition;data mining;feature selection	AI	11.197849252820992	-43.04767784268943	168132
1b2ccb8ac47e599618d3c6eef7bfbc2fe761d3b5	multibiometric systems: a comparative study of multi-algorithmic and multimodal approaches	comparative analysis;weighted sums;computer science	In this paper, we present a comparative analysis of Multi-algorithmic and Multimodal approaches. We have used palmprint and face as biometric traits and other popular subspace algorithms (PCA, FLD, and ICA). Subsequently, the different combinations of algorithms are also evaluated in our experiment. The multi-algorithmic approach achieves incremental results where as multimodal approach yields far improved results. Hence Complimentary information available through multimodal approach always performs better than multi-algorithmic approach which mainly builds on supplementary information.	multimodal interaction	Mohammad Imran;Ashok Rao;G. Hemantha Kumar	2010		10.1016/j.procs.2010.11.026	qualitative comparative analysis;speech recognition;computer science;artificial intelligence;machine learning;data mining	Logic	11.940500743115047	-47.62119118718787	168490
5158067d1863fd2a054d11b45182e232daa0d260	base model combination algorithm for resolving tied predictions for k-nearest neighbor ova ensemble models	ova classification;ensemble classification;k nearest neighbor classification;combination algorithm;data mining;predictive data mining;model aggregation	M aggregation is the process of constructing several base models that are then combined into a single model for prediction. Ensemble classification has been studied by many researchers and found to provide significant performance improvements over single models. This paper presents a new base model combination algorithm for K-nearest neighbor (KNN) ensemble models based on One-Versus-All (OVA) classification. The proposed algorithm uses two decision functions to determine the best prediction among the many predictions provided by the base models. It is demonstrated in this paper that tied or conflicting predictions can be effectively resolved when a probabilistic function and a distance function are used by a combination algorithm for OVA KNN base model predictions. The resolution of tied predictions leads to improvements in predictive performance.	data mining;ensemble forecasting;k-nearest neighbors algorithm;software performance testing;the forest	Patricia E. N. Lutu;Andries Petrus Engelbrecht	2013	INFORMS Journal on Computing	10.1287/ijoc.1120.0518	computer science;machine learning;pattern recognition;data mining	ML	11.558383516279886	-40.89441291729296	169075
9868cb06d2cda6d9a882e27644e258f82a0c75ef	mi-ds: multiple-instance learning algorithm	pattern classification knowledge based systems learning artificial intelligence;precision and recall mi ds multiple instance learning algorithm supervised learning bags of instances classification rule based mil algorithm;pattern classification;learning artificial intelligence;supervised learning multiple instance learning mil rule based algorithms;support vector machines classification algorithms prediction algorithms standards training data proteins educational institutions;knowledge based systems	Multiple-instance learning (MIL) is a supervised learning technique that addresses the problem of classifying bags of instances instead of single instances. In this paper, we introduce a rule-based MIL algorithm, called mi-DS, and compare it with 21 existing MIL algorithms on 26 commonly used data sets. The results show that mi-DS performs on par with or better than several well-known algorithms and generates models characterized by balanced values of precision and recall. Importantly, the introduced method provides a framework that can be used for converting other rule-based algorithms into MIL algorithms.	addresses (publication format);algorithm;algorithmic learning theory;bag device component;classification;generic drugs;in-phase and quadrature components;logic programming;low-rank approximation;missing data;multiple-instance learning;numerical analysis;precision and recall;similarity measure;single-instance storage;supervised learning	Dat T. Nguyen;Cao D. Nguyen;Rosalyn Hobson Hargraves;Lukasz A. Kurgan;Krzysztof J. Cios	2013	IEEE Transactions on Cybernetics	10.1109/TSMCB.2012.2201468	semi-supervised learning;instance-based learning;computer science;artificial intelligence;knowledge-based systems;machine learning;pattern recognition;data mining;stability;generalization error	ML	14.486959124393287	-42.91125210374798	169252
0b01075b81f0d1046976eb98ff93b947cb596baa	filter-type variable selection based on information measures for regression tasks	conditional mutual information;info eu repo semantics article;variable selection	This paper presents a supervised variable selection method applied to regression problems. This method selects the variables applying a hierarchical clustering strategy based on information measures. The proposed technique can be applied to single-output regression datasets, and it is extendable to multi-output datasets. For single-output datasets, the method is compared against three other variable selection methods for regression on four datasets. In the multi-output case, it is compared against other state-of-the-art method and tested using two regression datasets. Two different figures of merit are used (for the single and multi-output cases) in order to analyze and compare the performance of the proposed method.	cluster analysis;conditional entropy;emoticon;extensibility;feature selection;hierarchical clustering;information theory;selection (genetic algorithm);type variable	Pedro Latorre Carmona;José Martínez Sotoca;Filiberto Pla	2012	Entropy	10.3390/e14020323	computer science;machine learning;pattern recognition;data mining;mathematics;conditional mutual information;feature selection;statistics	AI	19.314140632653896	-42.062991428036995	169276
6563f662bf8272d050d5f9bd8a6c91a3fe8600d7	a weighted voting framework for classifiers ensembles		We propose a probabilistic framework for classifier combination, which gives rigorous optimality conditions (minimum classification error) for four combination methods: majority vote, weighted majority vote, recall combiner and the naive Bayes combiner. The framework is based on two assumptions: class-conditional independence of the classifier outputs and an assumption about the individual accuracies. The four combiners are derived subsequently from one another, by progressively relaxing and then eliminating the second assumption. In parallel, the number of the trainable parameters increases from one combiner to the next. Simulation studies reveal that if the parameter estimates are accurate and the first assumption is satisfied, the order of preference of the combiners is: naive Bayes, recall, weighted majority and majority. By inducing label noise, we expose a caveat coming from the stability-plasticity dilemma. Experimental results with 73 benchmark data sets reveal that there is no definitive best combiner among the four candidates, giving a slight preference to naive Bayes. This combiner was better for problems with a large number of fairly balanced classes while weighted majority vote was better for problems with a small number of unbalanced classes.	benchmark (computing);diplexer;naive bayes classifier;precision and recall;simulation;unbalanced circuit	Ludmila I. Kuncheva;Juan José Rodríguez Diez	2012	Knowledge and Information Systems	10.1007/s10115-012-0586-6	machine learning;pattern recognition;data mining;mathematics;statistics	AI	16.34962197437012	-39.29552507479234	169370
1c108b02fe1939dcf4bb1f13b89e0555b960ccc6	joint machine learning and human learning design with sequential active learning and outlier detection for linear regression problems	machine learning algorithms;linear regression;training data;regression analysis data handling learning artificial intelligence optimisation;training data labeling data models linear regression machine learning algorithms optimization robustness;robustness;optimization;item response theory joint machine learning human learning design outlier detection linear regression problems data labeling task human labeling errors sequential active learning scheme training data labeling sparse optimization problem sequential active learning procedure nonsparse human errors irt;training machine learning human learning item response theory linear regression active learning;labeling;data models	In this paper, we propose a joint machine learning and human learning design approach to make the training data labeling task in linear regression problems more efficient and robust to noise, modeling mismatch, and human labeling errors. Considering a sequential active learning scheme which relies on human learning to enlarge training data set, we integrate it with sparse outlier detection algorithms to mitigate the inevitable human errors during training data labeling. First, we assume sparse human errors and formulate the outlier detection as a sparse optimization problem within the sequential active learning procedure. Then, for non-sparse human errors, with the IRT (item response theory) to model the distribution of human errors, appropriate data are selected to reconstruct a training data set with sparse human errors. Simulations are conducted to verify the desirable performance of the proposed approach.	active learning (machine learning);algorithm;anomaly detection;computer simulation;item response theory;machine learning;mathematical optimization;optimization problem;sparse approximation;sparse matrix;test set	Xiaohua Li;Jianbo Zheng	2016	2016 Annual Conference on Information Science and Systems (CISS)	10.1109/CISS.2016.7460537	semi-supervised learning;unsupervised learning;data modeling;multi-task learning;instance-based learning;training set;labeling theory;algorithmic learning theory;test set;computer science;linear regression;online machine learning;machine learning;pattern recognition;data mining;ensemble learning;learning classifier system;supervised learning;stability;computational learning theory;active learning;statistics;robustness;generalization error	AI	19.516077753177864	-41.24494466952034	169383
3696cad3ef94b86cd1ab39de33a70fed757beaab	optimization of general statistical accuracy measures for classification based on learning vector quantization		We propose a framework for classification learning based on generalized learning vector quantization using statistical quality measures as cost function. Statistical measures like the F -measure or the Matthews correlation coefficient reflect better the performance for two-class classification problems than the simple accuracy, in particular if the data classes are imbalanced. For this purpose, we introduce soft approximations of those quantities contained in the confusion matrix, which are the basis for the calculation of the quality measures.	approximation;confusion matrix;learning vector quantization;loss function;matthews correlation coefficient	Marika Kaden;Wieland Hermann;Thomas Villmann	2014			machine learning;learning vector quantization;artificial intelligence;linde–buzo–gray algorithm;linear classifier;pattern recognition;computer science;vector quantization;confusion matrix;matthews correlation coefficient	ML	11.95433533829831	-43.46303747611173	169423
c40d628c18270d299d9a0bfda1dbd79c559c3249	prnn: recurrent neural network with persistent memory		Although Recurrent Neural Network (RNN) has been a powerful tool for modeling sequential data, its performance is inadequate when processing sequences with multiple patterns. In this paper, we address this challenge by introducing an external memory and constructing a novel persistent memory augmented RNN (termed as PRNN). The PRNN captures the principle patterns in training sequences and stores them in an external memory. By leveraging the persistent memory, the proposed method can adaptively update states according to the similarities between encoded inputs and memory slots, leading to a stronger capacity in assimilating sequences with multiple patterns. Contentbased addressing is suggested in memory accessing, and gradient descent is utilized for implicitly updating the memory. Our approach can be further extended by combining the prior knowledge of data. Experiments on several datasets demonstrate the effectiveness of the proposed method.	artificial neural network;convolutional neural network;data assimilation;end-to-end principle;experiment;gradient descent;language model;loss function;mixture model;persistent memory;random neural network;recurrent neural network;time series;wii	Kui Zhao;Yuechuan Li;Chi Zhang;Cheng Yang;Shenghuo Zhu	2018	CoRR			AI	20.90251586836982	-48.43853790608844	169429
6b7f27cff688d5305c65fbd90ae18f3c6190f762	generative networks as inverse problems with scattering transforms		Generative Adversarial Nets (GANs) and Variational Auto-Encoders (VAEs) provide impressive image generations from Gaussian white noise, but the underlying mathematics are not well understood. We compute deep convolutional network generators by inverting a fixed embedding operator. Therefore, they do not require to be optimized with a discriminator or an encoder. The embedding is Lipschitz continuous to deformations so that generators transform linear interpolations between input white noise vectors into deformations between output images. This embedding is computed with a wavelet Scattering transform. Numerical experiments demonstrate that the resulting Scattering generators have similar properties as GANs or VAEs, without learning a discriminative network or an encoder.	discriminator;encoder;experiment;interaction;mathematical optimization;network architecture;numerical analysis;sparse matrix;variational principle;wavelet;white noise	Tomás Angles;Stéphane Mallat	2018	CoRR		mathematical optimization;encoder;discriminative model;wavelet;lipschitz continuity;mathematics;inverse problem;embedding;white noise;scattering	ML	22.957943371117068	-48.69151601672157	169470
e0fd957c8b68011399ae92f50e474626e0420d36	probability fuzzy svm for image retrieval	support vector machines probability distribution radio frequency image retrieval training linear programming labeling;support vector machines;relevance feedback support vector machines fuzzy distribution probability distribution;probability distribution;support vector machines content based retrieval fuzzy reasoning image classification image retrieval statistical distributions;relevance feedback numeric value data probability membership probability fuzzy svm pfsvm probability fuzzy support vector machine probability distribution rf scheme sss problem small sample size problem learning cbir system content based image retrieval system;relevance feedback;fuzzy distribution	In the scheme of relevance feedback (RF) of conventional content-based image retrieval (CBIR) systems, only the labeled samples will be trained for learning, which gives rise to the small sample size (sss) problem. To solve the problem, a variety of RF schemes have been developed with exploiting support vector machine (SVM) which concentrates on enhancing classification ability of the schemes. In fact, there is not only classification distribution but also probability distribution in both numeric value data and image data. In fact, probability distribution existing in the images with multi-semantic meaning is more crucial for building a better scheme of RF. For modeling the scheme of RF under the consideration of both classification distribution and probability distribution, a framework of probability fuzzy support vector machine (PFSVM) is developed in this paper. In the framework, every pseudo-label sample is assigned both a fuzzy and a probability membership which are integrated into mechanism of PFSVM for active learning. Experimental results with a database of 1,000 images demonstrate the effectiveness of the proposed method.	content-based image retrieval;database;radio frequency;relevance feedback;support vector machine	Deshan Liu;Deqin Yan;Yonghua Tang;Shenglan Liu	2015	2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2015.7381929	probability distribution;support vector machine;fuzzy classification;computer science;machine learning;pattern recognition;data mining;mathematics;statistics	Vision	17.42117525873265	-42.98299807375381	169678
f0c3617e212d5319410a7bcccfe4cb78af6b8fb0	on the evolutionary weighting of neighbours and features in the k-nearest neighbour rule		Abstract This paper presents an evolutionary method for modifying the behaviour of the k-Nearest-Neighbour classifier (kNN) called Simultaneous Weighting of Attributes and Neighbours (SWAN). Unlike other weighting methods, SWAN presents the ability of adjusting the contribution of the neighbours and the significance of the features of the data. The optimization process focuses on the search of two real-valued vectors. One of them represents the votes of neighbours, and the other one represents the weight of each feature. The synergy between the two sets of weights found in the optimization process helps to improve significantly, the classification accuracy. The results on 35 datasets from the UCI repository suggest that SWAN statistically outperforms the other weighted kNN methods.		Daniel Mateos-García;Jorge García-Gutiérrez;José Cristóbal Riquelme Santos	2019	Neurocomputing	10.1016/j.neucom.2016.08.159	machine learning;artificial intelligence;evolutionary computation;mathematics;pattern recognition;weighting	AI	10.070058621246357	-42.41653911715432	169685
70494381f61794f9d986fdf11df47f333c6de168	data augmentation method of sar image dataset		Large-scale high-quality, standardized, measurable and accurate data is the key to promote the progress of the algorithm in the radar remote sensing. Data scaling is a widespread technology that increases the size of a labeled training set dataset through specific data transformations. Synthetic Aperture Radar (SAR) image simulators based on computer-aided mapping models play an important role in SAR applications such as automatic target recognition and image interpretation, but the accuracy of this simulator is due to geometric errors and simplification of electromagnetic calculations. In order to achieve a SAR image datasets with the known target and azimuth angles, we can generate the desired image directly from a known image database. We can realize the augmentation of SAR image data set through linear synthesis and Generative Adversarial Networks, which can generate SAR images for the specified azimuth.	algorithm;aperture (software);automatic target recognition;generative adversarial networks;image scaling;level of detail;simulation;test set	Mingrui Zhang;Zongyong Cui;Xianyuan Wang;Zongjie Cao	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518825	computer vision;azimuth;scaling;artificial intelligence;radar;synthetic aperture radar;training set;computer science;data transformation (statistics);automatic target recognition	Vision	23.3067793130313	-50.94115215245561	169687
b6df27431358922d43a9b6d67c940d7df8f01e62	learning features for relational data		Feature engineering is one of the most important but tedious tasks in data science projects. This work studies automation of feature learning for relational data. We first theoretically proved that learning relevant features from relational data for a given predictive analytics problem is NP-hard. However, it is possible to empirically show that an efficient rule based approach predefining transformations as a priori based on heuristics can extract very useful features from relational data. Indeed, the proposed approach outperformed the state of the art solutions with a significant margin. We further introduce a deep neural network which automatically learns appropriate transformations of relational data into a representation that predicts the target variable well instead of being predefined as a priori by users. In an extensive experiment with Kaggle competitions, the proposed methods could win late medals. To the best of our knowledge, this is the first time an automation system could win medals in Kaggle competitions with complex relational data. ACM Reference Format: Hoang Thanh Lam, TranNgocMinh,Mathieu Sinn, Beat Buesser, andMartin Wistuba. 2018. Learning Features For Relational Data. In Proceedings of ACM Conference (Conference’17). ACM, New York, NY, USA, 12 pages. https: //doi.org/10.1145/nnnnnnn.nnnnnnn	artificial neural network;data science;deep learning;feature engineering;feature learning;https;heuristic;lam/mpi;np-hardness	Hoang Thanh Lam;Tran Ngoc Minh;Mathieu Sinn;Beat Buesser;Martin Wistuba	2018	CoRR		data mining;artificial neural network;machine learning;artificial intelligence;predictive analytics;computer science;rule-based system;process automation system;heuristics;relational database;feature engineering;feature learning	ML	14.614728749194372	-47.9084051633283	169858
bcdaccf3a02db8899fb09b814b99c170b814e76f	information-theoretic algorithm for feature selection	feature selection;information-theoretic algorithm;feature selection;classification;information-theoretic network;classification;information-theoretic network;information theory;information theory	Feature selection is used to improve efficiency of learning algorithms by finding an optimal subset of features. However, most feature selection techniques can handle only certain types of data. Additional limitations of existing methods include intensive computational requirements and inability to identify redundant variables. In this paper, we are presenting a novel, information-theoretic algorithm for feature selection, which finds an optimal set of attributes by removing both irrelevant and redundant features. The algorithm has a polynomial computational complexity and it is applicable to datasets of mixed nature. The method performance is evaluated on several benchmark datasets by using a standard classifier (C4.5).	anytime algorithm;artificial neural network;benchmark (computing);c4.5 algorithm;computational complexity theory;feature selection;information theory;k-nearest neighbors algorithm;machine learning;polynomial;polynomial-time reduction;quadratic function;relevance;requirement;run time (program lifecycle phase);scalability	Mark Last;Abraham Kandel;Oded Maimon	2001	Pattern Recognition Letters		minimum redundancy feature selection;information theory;biological classification;computer science;machine learning;pattern recognition;data mining;computational complexity theory;feature selection;feature;statistics	ML	11.163245552110753	-43.246683349660955	170189
f9ab8db5a832cc760af84b6fa2ab0f9921f84a8f	an improved statistic for the pooled triangle test against prnu-copy attack		We propose a new statistic to improve the pooled version of the triangle test used to combat the fingerprint-copy counterforensic attack against photoresponse nonuniformity based camera identification [1]. As opposed to the original version of the test, the new statistic exploits the one-tailed nature of the test, weighting differently positive and negative deviations from the expected value of the correlation between the image under analysis and the candidate images, i.e., those image suspected to have been used during the attack. The experimental results confirm the superior performance of the new test, especially when the conditions of the test are challenging ones, that is when the number of images used for the fingerprint-copy attack is large and the size of the image under test is small.	copy attack;fingerprint	Mauro Barni;H&#x00E9;ctor Santoyo-Garc&#x00ED;a;Benedetta Tondi	2018	IEEE Signal Processing Letters	10.1109/LSP.2018.2863045	mathematics;pattern recognition;artificial intelligence;expected value;statistic;exploit;weighting;copy attack	Vision	16.694980366019273	-50.649090238598895	170239
361bd984e353b8aea8ea7b1e909a3f11e3fa5e0c	a sorting optimization curve with quality and yield requirements	human disease;sorting;receiver operator characteristic;threshold;roc;false positive rate;true positive;roc curve;soc;classifier optimization;binary classification	Binary classifiers used for sorting can be compared and optimized using receiver-operating characteristic (ROC) curves which describe the trade-off between the false positive rate and true positive rate of the classifiers. This approach is well suited for the diagnosis of human diseases where individual costs of misclassification are of great concern. While it can be applied to the sorting of merchandise or other materials, the variables described by the ROC curve and its existing alternatives are less relevant for that range of applications and another approach is needed. In this paper, quality and yield factors are introduced into a sorting optimization curve (SOC) for the choice of the operating point of the classifier, associated with the prediction of output quantity and quality. Given examples are the sorting of seeds and apples with specific requirements. In both cases the operating point of the classifier is easily chosen on the SOC, while the output characteristics of the sorted product are accurately predicted.	gnu octave;mathematical optimization;operating point;receiver operating characteristic;requirement;seeds (cellular automaton);sensitivity and specificity;sorting	David Ooms;Rodolphe Palm;Vincent Leemans;Marie-France Destain	2010	Pattern Recognition Letters	10.1016/j.patrec.2009.12.015	econometrics;computer science;machine learning;pattern recognition;mathematics;receiver operating characteristic;statistics	Vision	10.208620903045603	-48.257282246366465	170294
85a32fd0d591ad5bfa742e2b5407bd4a191113d8	gas-sensor drift counteraction with adaptive active learning for an electronic nose	active learning;drift counteraction;electronic nose;online	Gas sensors are the key components of an electronic nose (E-nose) in violated odour analysis. Gas-sensor drift is a kind of physical change on a sensor surface once an E-nose works. The perturbation of gas-sensor responses caused by drift would deteriorate the performance of the E-nose system over time. In this study, we intend to explore a suitable approach to deal with the drift effect in an online situation. Considering that the conventional drift calibration is difficult to implement online, we use active learning (AL) to provide reliable labels for online instances. Common AL learning methods tend to select and label instances with low confidence or massive information. Although this action clarifies the ambiguity near the classification boundary, it is inadequate under the influence of gas-sensor drift. We still need the samples away from the classification plane to represent drift variations comprehensively in the entire data space. Thus, a novel drift counteraction method named AL on adaptive confidence rule (AL-ACR) is proposed to deal with online drift data dynamically. By contrast with conventional AL methods selecting instances near the classification boundary of a certain category, AL-ACR collects instances distributed evenly in different categories. This action implements on an adjustable rule according to the outputs of classifiers. Compared with other reference methods, we adopt two drift databases of E-noses to evaluate the performance of the proposed method. The experimental results indicate that the AL-ACR reaches higher accuracy than references on two E-nose databases, respectively. Furthermore, the impact of the labelling number is discussed to show the trend of performance for the AL-type methods. Additionally, we define the labelling efficiency index (LEI) to assess the contribution of certain labelling numerically. According to the results of LEI, we believe AL-ACR can achieve the best effect with the lowest cost among the AL-type methods in this work.		Tao Liu;Dongqi Li;Jianjun Chen;Yanbing Chen;Tao Yang;Jianhua Cao	2018		10.3390/s18114028		AI	14.155204083725133	-38.660073047310654	170461
501bc80334fd6e429d6fa2da7c81ecb1f03580b6	sgan: an alternative training of generative adversarial networks		"""The Generative Adversarial Networks (GANs) have demonstrated impressive performance for data synthesis, and are now used in a wide range of computer vision tasks. In spite of this success, they gained a reputation for being difficult to train, what results in a time-consuming and human-involved development process to use them. We consider an alternative training process, named SGAN, in which several adversarial """"local"""" pairs of networks are trained independently so that a """"global"""" supervising pair of networks can be trained against them. The goal is to train the global pair with the corresponding ensemble opponent for improved performances in terms of mode coverage. This approach aims at increasing the chances that learning will not stop for the global pair, preventing both to be trapped in an unsatisfactory local minimum, or to face oscillations often observed in practice. To guarantee the latter, the global pair never affects the local ones. The rules of SGAN training are thus as follows: the global generator and discriminator are trained using the local discriminators and generators, respectively, whereas the local networks are trained with their fixed local opponent. Experimental results on both toy and real-world problems demonstrate that this approach outperforms standard training in terms of better mitigating mode collapse, stability while converging and that it surprisingly, increases the convergence speed as well."""		Tatjana Chavdarova;François Fleuret	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00980	adversarial system;machine learning;pattern recognition;generative grammar;discriminator;spite;computer science;adversary;artificial intelligence;reputation;convergence (routing)	Vision	19.42870540731876	-51.481607884135514	170563
c7f5f71376b42c887ad5ed21986cbcb265dc8d7e	an informational energy lvq approach for feature ranking.	supervised classification;ranking and selection;incremental learning	Input feature ranking and selection represent a necessary preprocessing stage in classification, especially when one is required to manage large quantities of data. We introduce a weighted LVQ algorithm, called Energy Relevance LVQ (ERLVQ), based on Onicescu’s informational energy [10]. ERLVQ is an incremental learning algorithm for supervised classification and feature ranking.	algorithm;feature vector;learning vector quantization;machine learning;preprocessor;relevance;supervised learning	Razvan Andonie;Angel Cataron	2004			machine learning;pattern recognition;data mining;ranking svm;learning to rank	NLP	10.696688000102954	-39.33973838961055	170803
f109a683e9a54d9a6bc24cac73ca0a8f7bc09082	sparse kernel pca for outlier detection		In this paper, we propose a new method to perform Sparse Kernel Principal Component Analysis (SKPCA) and also mathematically analyze the validity of SKPCA. We formulate SKPCA as a constrained optimization problem with elastic net regularization in kernel feature space and solve it. We consider outlier detection (where KPCA is employed) as an application for SKPCA, using the RBF kernel. We test it on 5 real world datasets and show that by using just 4% (or even less) of the principal components (PCs), where each PC has on average less than 12% non-zero elements in the worst case among all 5 datasets, we are able to nearly match and in 3 datasets even outperform KPCA. We also compare the performance of our method with a recently proposed method for SKPCA and show that our method performs better in terms of both accuracy and sparsity. We also provide a novel probabilistic proof to justify the existence of sparse solutions for KPCA using the RBF kernel. To the best of our knowledge, this is the first attempt at theoretically analyzing the validity of SKPCA.		Rudrajit Das;Aditya Golatkar;Suyash P. Awate	2018	2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)		anomaly detection;artificial intelligence;kernel (linear algebra);feature vector;probabilistic logic;kernel principal component analysis;computer science;pattern recognition;principal component analysis;elastic net regularization;radial basis function kernel	ML	24.069555047554577	-40.38748342823243	170935
