id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
4f8e9495e2e870cba9dffcc95ac17f932baef1c6	controlling perceptual factors in neural style transfer		Neural Style Transfer has shown very exciting results enabling new forms of image manipulation. Here we extend the existing method to introduce control over spatial location, colour information and across spatial scale. We demonstrate how this enhances the method by allowing high-resolution controlled stylisation and helps to alleviate common failure cases such as applying ground textures to sky regions. Furthermore, by decomposing style into these perceptual factors we enable the combination of style information from multiple sources to generate new, perceptually appealing styles from existing ones. We also describe how these methods can be used to more efficiently produce large size, high-quality stylisation. Finally we show how the introduced control measures can be applied in recent methods for Fast Neural Style Transfer.	image resolution;spatial scale	Leon A. Gatys;Alexander S. Ecker;Matthias Bethge;Aaron Hertzmann;Eli Shechtman	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.397	computer vision;machine learning;artificial intelligence;perception;computer science;spatial ecology	Vision	22.787567665122555	-55.47514435232999	49365
46f83973aaab8574011a6630a84ce9c623c119ac	espace: accelerating convolutional neural networks via eliminating spatial and channel redundancy		Recent years have witnessed an extensive popularity of convolutional neural networks (CNNs) in various computer vision and artificial intelligence applications. However, the performance gains have come at a cost of substantially intensive computation complexity, which prohibits its usage in resource-limited applications like mobile or embedded devices. While increasing attention has been paid to the acceleration of internal network structure, the redundancy of visual input is rarely considered. In this paper, we make the first attempt of reducing spatial and channel redundancy directly from the visual input for CNNs acceleration. The proposed method, termed ESPACE (Elimination of SPAtial and Channel rEdundancy), works by the following three steps: First, the 3D channel redundancy of convolutional layers is reduced by a set of low-rank approximation of convolutional filters. Second, a novel mask based selective processing scheme is proposed, which further speedups the convolution operations via skipping unsalient spatial locations of the visual input. Third, the accelerated network is fine-tuned using the training data via back-propagation. The proposed method is evaluated on ImageNet 2012 with implementations on two widelyadopted CNNs, i.e. AlexNet and GoogLeNet. In comparison to several recent methods of CNN acceleration, the proposed scheme has demonstrated new state-of-the-art acceleration performance by a factor of 5.48× and 4.12× speedup on AlexNet and GoogLeNet, respectively, with a minimal decrease in classification accuracy. Introduction In recent years, convolutional neural networks (CNNs) have demonstrated impressive performance in various computer vision and artificial intelligence applications, such as object recognition (Krizhevsky, Sutskever, and Hinton 2012)(Simonyan and Zisserman 2014)(Lecun et al. 1998)(Szegedy et al. 2015)(He et al. 2015), object detection (Girshick et al. 2014)(Girshick 2015)(Ren et al. 2015), and image retrieval (Gong et al. 2014b). The cutting-edge CNNs are computationally intensive, in which the speed limitation mainly resorts to the convolution operations in the convolutional layers1. For example, an 8-layer AlexNet (Krizhevsky, Copyright c © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. *Corresponding Author. In this paper, we focus on the acceleration of the convolutional layers, as it takes up over 80% running time in most existing CNNs, Sutskever, and Hinton 2012) with about 600,000 nodes costs 240MB storage (including 61M parameters) and requires 729M FLOP2 to classify one image with size 224 × 224. Such cost is further intensified in deeper CNNs, e.g. a 16layer-VGGNet (Simonyan and Zisserman 2014) with 1.5M nodes costs 528MB storage (including 144M parameters) and requires about 15B FLOP to classify one image. Under such circumstance, the existing CNNs cannot be directly deployed to scenarios that require fast processing and compact storage, such as streaming or real-time applications. On one hand, CNNs with million-scale parameters typically tend to be over parameterized and heavily computed (Denil et al. 2013). Therefore, not all parameters and operations (e.g. convolution or non-linear activation) are essentially necessary in producing a discriminative decision. On the other hand, it is quantitatively shown in (Ba and Caruana 2014) that, neither shallow nor simplified CNNs provide comparable performance to deep CNNs with billion-scale online operations. Therefore, to accelerate online CNNs predictions without significantly decreasing the decision accuracy, a natural thought is to discover and discard redundant parameters and operations in deep CNNs. Accelerating CNNs has attracted a few research attention very recently, most of which focus on accelerating the convolutional layer, which is the most time-consuming part of CNNs. In the literature, the related works can be further categorized into four groups, i.e. designing compact convolutional filters, parameters quantization, parameters pruning and tensor decomposition. Designing compact convolutional filters. Using a compact filter for convolution can directly reduce the computation cost. The key idea is to replace the loose and over-parametric filters with compact blocks to improve the speed, which significantly accelerate CNNs like GoogLeNet (Szegedy et al. 2015), ResNet (He et al. 2015) on several benchmarks. Decomposing 3× 3 convolution with two 1 × 1 convolutions was used in (Szegedy, Loffe, and Vanhoucke 2016), which achieved state-of-the-art acceleration performance on object recognition. SqueezeNet (Iandola, Moskewicz, and Ashraf 2016) was proposed to replace 3×3 convolution with 1 × 1 convolution, which created a comi.e. AlexNet, GoogLeNet and VGGNet. FLOP: The number of Floating-point operation to classify one image with CNNs.	applications of artificial intelligence;artificial neural network;backpropagation;benchmark (computing);categorization;computation;computer vision;convolution;convolutional neural network;espace;embedded system;flip-flop (electronics);image retrieval;imagenet;intranet;low-rank approximation;nonlinear system;object detection;outline of object recognition;real-time clock;real-time computing;software propagation;speedup;streaming media;time complexity;word lists by frequency	Shaohui Lin;Rongrong Ji;Chao Chen;Feiyue Huang	2017			espace;machine learning;theoretical computer science;artificial intelligence;redundancy (engineering);convolutional neural network;computer science;communication channel	AI	23.866091203475936	-52.118229732961254	49754
4c2b450fe5054e751bdbab0eb5f01076065c46f9	convolutional neural networks applied to human face classification	support vector machines avatars computer vision face recognition image classification neural nets;support vector machines convolutional neural network model human face classification computer vision applications computer generated avatars human face images icmla 2012 face recognition challenge avatar captcha dataset image classification accuracy;support vector machines;neural nets;image classification;computer vision;captcha deep learning convolutional neural networks face recognition;face recognition;deep learning;avatars;convolutional neural networks;machine learning learning systems avatars support vector machines humans mathematical model face;captcha	Convolutional neural network models have covered a broad scope of computer vision applications, achieving competitive performance with minimal domain knowledge. In this work, we apply such a model to a task designed to deter automated systems. We trained a convolutional neural network to distinguish between images of human faces from computer generated avatars as part of the ICMLA 2012 Face Recognition Challenge. The network achieved a classification accuracy of 99% on the Avatar CAPTCHA dataset. Furthermore, we demonstrated the potential of utilizing support vector machines on the same problem and achieved equally competitive performance.	artificial neural network;brute-force attack;captcha;computer vision;convolutional neural network;facial recognition system;neural networks;support vector machine	Brian Cheung	2012	2012 11th International Conference on Machine Learning and Applications	10.1109/ICMLA.2012.177	support vector machine;computer vision;contextual image classification;computer science;machine learning;pattern recognition;captcha;deep learning;artificial neural network	Vision	22.858090828832594	-53.93665784687322	49800
91d5b4b3c855436850d742b76e669b4bc569d21d	protein-protein binding affinity prediction based on an svr ensemble	machine learning;potential of mean force;protein-protein interaction affinity;two-layer support vector machine	Accurately predicting generic protein-protein binding affinities (PPBA) is essential to analyze the outputs of protein docking and may help infer real status of cellular protein-protein interaction sub-networks. However, accurate PPBA prediction is still extremely challenging. Machine learning methods are promising to address this problem. We propose a two-layer support vector regression (TLSVR) model to implicitly capture binding contributions that are hard to explicitly model. The TLSVR circumvents both the descriptor compatibility problem and the need for problematic modeling assumptions. Input features for TLSVR in first layer are scores of 2209 interacting atom pairs within each distance bin. The base SVRs are combined by the second layer to infer the final affinities. Leave-one-out validation on our heterogeneous data shows that the TLSVR method obtains a very good result of R=0.80 and SD=1.32 with real affinities. Comparison experiment further demonstrates that TLSVR is superior to the previous state-of-art methods in predicting generic PPBA. © 2012 Springer-Verlag.	processor affinity	Xueling Li;Min Zhu;Xiao-Lai Li;Hong-Qiang Wang;Shulin Wang	2012		10.1007/978-3-642-31588-6_19	artificial intelligence;machine learning;bin;pattern recognition;computer science;support vector machine;macromolecular docking;potential of mean force	NLP	10.814291595325924	-56.074909382426185	50073
6af53fc48b634f1840271120811b79335d441ca4	chemical information in 3d space		Progress in recent decades in the representation of chemical structures is briefly outlined. A reasonable model of the three-dimensional structure of any organic compound can nowadays be generated. This opens the door to the study of the relationships between the 3D structure of organic molecules and physical, chemical, or biological properties. A code has been developed that allows the representation of the 3D structure of molecules by a fixed (constant) number of variables and thus is amenable to an analysis by statistical and pattern recognition methods or neural networks. Examples for the investigation of biological data and the simulation of infrared spectra are given. Further work on the relationships between structure and infrared spectra has shown that the 3D structure of an organic molecule can be derived from its infrared spectrum.	artificial neural network;cheminformatics;pattern recognition;simulation	Johann Gasteiger;Jens Sadowski;Jan H. Schuur;Paul Selzer;Larissa Steinhauer;Valentin Steinhauer	1996	Journal of Chemical Information and Computer Sciences	10.1021/ci960343+	computational chemistry;organic compound;infrared;artificial neural network;biological data;molecule;molecular physics;infrared spectroscopy;mathematics	Theory	13.200506583863367	-58.12229327888013	50984
2457716a042a5c31b83fa282d101711479ab1bbb	activity recognition with finite state machines	finite state machine;greedy kind;central problem;generalized representation;multiple sequence alignment;novel instance;activity recognition;essential part;finite state machine representation;unseen instance;high accuracy	This paper shows how to learn general, Finite State Machine representations of activities that function as recognizers of previously unseen instances of activities. The central problem is to tell which differences between instances of activities are unimportant and may be safely ignored for the purpose of learning generalized representations of activities. We develop a novel way to find the “essential parts” of activities by a greedy kind of multiple sequence alignment, and a method to transform the resulting alignments into Finite State Machine that will accept novel instances of activities with high accuracy.	activity recognition;finite-state machine;greedy algorithm;multiple sequence alignment	Wesley Kerr;Anh Tran;Paul R. Cohen	2011		10.5591/978-1-57735-516-8/IJCAI11-228	artificial intelligence;machine learning;mathematics;algorithm	NLP	20.245275810773137	-57.843258523214914	51026
2ec5eb211e7fd42fe144d4786d814d3494da83e8	alchemical free energy calculations and isothermal titration calorimetry measurements of aminoadamantanes bound to the closed state of influenza a/m2tm	artikel	Adamantane derivatives, such as amantadine and rimantadine, have been reported to block the transmembrane domain (TM) of the M2 protein of influenza A virus (A/M2) but their clinical use has been discontinued due to evolved resistance in humans. Although experiments and simulations have provided adequate information about the binding interaction of amantadine or rimantadine to the M2 protein, methods for predicting binding affinities of whole series of M2 inhibitors have so far been scarcely applied. Such methods could assist in the development of novel potent inhibitors that overcome A/M2 resistance. Here we show that alchemical free energy calculations of ligand binding using the Bennett acceptance ratio (BAR) method are valuable for determining the relative binding potency of A/M2 inhibitors of the aminoadamantane type covering a binding affinity range of only ∼2 kcal mol(-1). Their binding affinities measured by isothermal titration calorimetry (ITC) against the A/M2TM tetramer from the Udorn strain in its closed form at pH 8 were used as experimental probes. The binding constants of rimantadine enantiomers against M2TMUdorn were measured for the first time and found to be equal. Two series of alchemical free energy calculations were performed using 1,2-dipalmitoyl-sn-glycero-3-phosphocholine (DPPC) and 1,2-dimyristoyl-sn-glycero-3-phosphocholine (DMPC) lipids to mimic the membrane environment. A fair correlation was found for DPPC that was significantly improved using DMPC, which resembles more closely the DPC lipids used in the ITC experiments. This demonstrates that binding free energy calculations by the BAR approach can be used to predict relative binding affinities of aminoadamantane derivatives toward M2TM with good accuracy.	adamantane;amantadine;calorimetry;deferred procedure call;dimyristoylphosphatidylcholine;experiment;ligands;mqtt;processor affinity;remanence;rimantadine;simulation;tissue membrane;titration method;transmembrane domain;colfosceril palmitate;free energy;kilocalorie	Harris Ioannidis;Antonios Drakopoulos;Christina Tzitzoglaki;Nadine Homeyer;Felix Kolarov;Paraskevi Gkeka;Kathrin Freudenberger;Christos Liolios;Günther Gauglitz;Zoe Cournia;Holger Gohlke;Antonios Kolocouris	2016	Journal of chemical information and modeling	10.1021/acs.jcim.6b00079	crystallography;biochemistry;chemistry	Comp.	10.991567175335227	-62.633707155587324	51468
b60a2d0b4db12a56aee1ffc078efedd2f018aeba	data mining in protein binding cavities	data mining;protein binding	The molecular function of a protein is coupled to the binding of a substrate or an endogenous ligand to a well defined binding cavity. To detect functional relationships among proteins, their binding-site exposed physicochemical characteristics were described by assigning generic pseudocenters to the functional groups of the amino acids flanking a particular active site. These pseudocenters were assembled into small substructures and their spatial similarity with appropriate chemical properties was examined. If two substructures of two binding cavities are found to be similar, they form the basis for an expanded comparison of the complete cavities. Preliminary tests indicate the benefit of this method and motivate further studies.	data mining	Katrin Kupas;Alfred Ultsch	2004		10.1007/3-540-28084-7_40	active site;biophysics;adenylate kinase;amino acid;substrate (chemistry);molecular function;ligand (biochemistry);chemistry;plasma protein binding	Comp.	10.20881133041088	-59.97141643035092	51818
e808179eecb1251a64dc96c15ad342d0c66199ac	rf-based 3d skeletons		This paper introduces RF-Pose3D, the first system that infers 3D human skeletons from RF signals. It requires no sensors on the body, and works with multiple people and across walls and occlusions. Further, it generates dynamic skeletons that follow the people as they move, walk or sit. As such, RF-Pose3D provides a significant leap in RF-based sensing and enables new applications in gaming, healthcare, and smart homes.  RF-Pose3D is based on a novel convolutional neural network (CNN) architecture that performs high-dimensional convolutions by decomposing them into low-dimensional operations. This property allows the network to efficiently condense the spatio-temporal information in RF signals. The network first zooms in on the individuals in the scene, and crops the RF signals reflected off each person. For each individual, it localizes and tracks their body parts - head, shoulders, arms, wrists, hip, knees, and feet. Our evaluation results show that RF-Pose3D tracks each keypoint on the human body with an average error of 4.2 cm, 4.0 cm, and 4.9 cm along the X, Y, and Z axes respectively. It maintains this accuracy even in the presence of multiple people, and in new environments that it has not seen in the training set. Demo videos are available at our website: http://rfpose3d.csail.mit.edu.	artificial neural network;coat of arms;convolution;convolutional neural network;rf modulator;radio frequency;sensor;test set	Mingmin Zhao;Yonglong Tian;Hang Zhao;Mohammad Abu Alsheikh;Tianhong Li;Rumen Hristov;Zachary Kabelac;Dina Katabi;Antonio Torralba	2018		10.1145/3230543.3230579	artificial neural network;architecture;computer science;convolutional neural network;radio frequency;distributed computing;training set;shoulders	Networks	21.800115184015446	-59.520310280667175	51876
7880fd31cfaaeef9f7a7ca6012c6250d6fae2f4d	visual recognition using a combination of shape and color features		We develop and implement a new approach to utilizing color information for object and scene recognition that is inspired by the characteristics of colorand object-selective neurons in the high-level inferotemporal cortex of the primate visual system. In our hierarchical model, we introduce a new dictionary of features representing visual information as quantized color blobs that preserve coarse, relative spatial information. We run this model on several datasets such as Caltech101, Outdoor Scenes and Underwater Images. The combination of our color features with (grayscale) shape features leads to significant increases in performance over shape or color features alone. Using our model, performance is significantly higher than using color naively, i.e. concatenating the channels of various color spaces. This indicates that usage of color information per se is not enough to produce good performance, and that it is specifically our biologically-inspired approach to color that results in significant improvement.	benchmark (computing);cluster analysis;color space;computer vision;concatenation;dictionary;emulator;expanded memory;grayscale;hierarchical database model;high- and low-level;os-tan;quantization (signal processing)	Sepehr Jalali;Cheston Tan;Joo-Hwee Lim;Jo Yew Tham;Sim Heng Ong;Paul J. Seekings;Elizabeth A. Taylor	2013			cognitive psychology;psychology	Vision	21.42024734977314	-66.11954009620041	52550
6fc9ca129803c658014361b98295c2832b9f0db2	colorimetric integrated pcr protocol for rapid detection of vibrio parahaemolyticus	colorimetric;pcr;virulence gene;bacterium detection;primer design	Rapid detection of pathogens is of great significance for food safety and disease diagnosis. A new colorimetric method for rapid and easy detection of Vibrio parahaemolyticus (V. parahaemolyticus or Vp) has been developed in this research. A specific sequence was designed and integrated with the forward primer for molecular detection of Vp. This specific sequence was tested and treated as the horseradish peroxidase (HRP)-mimicking DNAzyme and could be amplified during the polymerase chain reaction (PCR) process. The products of PCR including the sequence of HRP-mimicking DNAzyme could produce the distinguished color in the presence of catalysis substrates. The optical signal of the catalysis reaction, which is in a linear relationship with the initial template of Vp, could be determined with the naked eye or measured with Ultraviolet-visible (UV-vis) for qualitative and quantitative detections, respectively. Based on the optical signal intensity, rapid and easy detection of Vp was successfully achieved with satisfied sensitivity and specificity. Furthermore, the detection of tdh, trh, tlh and toxR virulence genes of two Vp species (Vp 33847 and Vp 17802) were all performed successfully with this developed colorimetric integrated PCR protocol, which demonstrated potential applicability for the rapid detection of other bacteria.	bacteria;catalysis;clinical use template;dna, catalytic;entity name part qualifier - adopted;etoposide;horseradish peroxidase;primer;polymerase chain reaction;screening procedure;sensitivity and specificity;sensor;sequence number;signature block;thyrotropin-releasing hormone;vibrio parahaemolyticus;vibrio sp identified:prid:pt:water:nom:organism specific culture;virulence	Kewen Cheng;Daodong Pan;Jun Teng;Li Yao;Yingwang Ye;Feng Xue;Fan Xia;Wei Chen	2016		10.3390/s16101600	molecular biology;colorimetry;polymerase chain reaction	Vision	10.882103338937222	-64.57587646816067	52553
eabe64d35c0ad421d78d1c687a02058babcf8ce6	statistical pattern recognition - early development and recent progress	statistical pattern recognition;early development;recent progress	The area of statistical pattern recognition has experienced about three decades of continued progress. In this paper, early development of the area is examined by reviewing the activities of the first decade. Recent progress in major topics of statistical pattern recognition including classification rules, feature extraction, contextual analysis and cluster analysis, are then reviewed. Particular notes are made on the important contributions of the late Prof. Fu, which have profound impact on the development of statistical pattern recognition. The future outlook of the area is indeed bright as many applications making use of statistical pattern recognition are explored to seek more reliable recognition performance. Efforts on efficient hardware implementation and less costly recognition software packages as well as the combined statistical-structural approach are also likely to increase significantly.	pattern recognition	C. H. Chen	1987	IJPRAI	10.1142/S0218001487000047	artificial intelligence;data science;operations research	Vision	19.424634615108516	-59.11767485965021	52815
3fdf09f12c7972e4ee42db3ea9dcadebe97b90b8	accurate prediction of hiv-1 drug response from the reverse transcriptase and protease amino acid sequences using sparse models created by convex optimization	reverse transcriptase;antiretroviral therapy;cost function;optimization technique;amino acid sequence;kernel function;human immunodeficiency virus;non linear model;convex optimization;reverse transcriptase inhibitors;genetics;protease inhibitor;loss function;linear model;possibility theory;least absolute shrinkage and selection operator;cross validation;support vector machine;occam s razor;jel classifications f12	MOTIVATION Genotype-phenotype modeling problems are often overcomplete, or ill-posed, since the number of potential predictors-genes, proteins, mutations and their interactions-is large relative to the number of measured outcomes. Such datasets can still be used to train sparse parameter models that generalize accurately, by exerting a principle similar to Occam's Razor: When many possible theories can explain the observations, the most simple is most likely to be correct. We apply this philosophy to modeling the drug response of Type-1 Human Immunodeficiency Virus (HIV-1). Owing to the decreasing expense of genetic sequencing relative to in vitro phenotype testing, a statistical model that reliably predicts viral drug response from genetic data is an important tool in the selection of antiretroviral therapy (ART). The optimization techniques described will have application to many genotype-phenotype modeling problems for the purpose of enhancing clinical decisions.   RESULTS We describe two regression techniques for predicting viral phenotype in response to ART from genetic sequence data. Both techniques employ convex optimization for the continuous subset selection of a sparse set of model parameters. The first technique, the least absolute shrinkage and selection operator, uses the l(1) norm loss function to create a sparse linear model; the second, the support vector machine with radial basis kernel functions, uses the epsilon-insensitive loss function to create a sparse non-linear model. The techniques are applied to predict the response of the HIV-1 virus to 10 reverse transcriptase inhibitor and 7 protease inhibitor drugs. The genetic data are derived from the HIV coding sequences for the reverse transcriptase and protease enzymes. When tested by cross-validation with actual laboratory measurements, these models predict drug response phenotype more accurately than models previously discussed in the literature, and other canonical techniques described here. Key features of the methods that enable this performance are the tendency to generate simple models where many of the parameters are zero, and the convexity of the cost function, which assures that we can find model parameters to globally minimize the cost function for a particular training dataset.   AVAILABILITY Results, tables and figures are available at ftp://ftp.genesecurity.net.   SUPPLEMENTARY INFORMATION An Appendix to accompany this article is available at Bioinformatics online.	algorithm;amino acid sequence;amino acids;antiretroviral therapy;appendix;arabic numeral 0;bioinformatics;biopolymer sequencing;cerebral convexity meningioma;coding theory;condition number;convex function;convex optimization;cross reactions;cross-validation (statistics);data table;decision trees;decision tree;endopeptidases;ephrin type-b receptor 1, human;hiv infections;immunologic deficiency syndromes;interaction;lasso;least squares;linear model;loss function;mathematical optimization;maxima and minima;mutation;nonlinear system;occam's razor;population parameter;protease inhibitors;rna-directed dna polymerase;radial (radio);retroviridae;reverse transcriptase inhibitors;rule (guideline);scientific publication;silo (dataset);sparse language;sparse matrix;statistical model;subgroup;support vector machine;taxicab geometry;well-posed problem;drug response;occam	Matthew Rabinowitz;Lance Myers;Milena Banjevic;Albert Chan;Joshua Sweetkind-Singer;Jessica Haberer;Kelly McCann;Roland Wolkowicz	2006	Bioinformatics	10.1093/bioinformatics/btk011	biology;mathematical optimization;convex optimization;computer science;bioinformatics;machine learning;statistics;loss function	Comp.	11.306237678521567	-52.09445997092394	52973
ebf7206404f21bc1dd151f82b12139086554f62a	guest editorial: special issue on statistics of shapes and textures	general	Nature provides an enormous variety of object shapes and textures, and at the same time these shapes and textures are often severely restricted in their variability by complicated rules. This seemingly self contradiction in naturally restricted variation is manifested in the restricted variety of images of natural objects. We believe that it is these complicated statistics of natural shapes and textures, which complicate tasks such as object recognition, segmentation, and classification. These tasks are often a crucial part of automated image analysis in as diverse areas such as autonomous robot navigation, visual inspection, and medical diagnosis. Statistics of Shapes and Textures may provide direct quantitative answers to scientific problems involving the analysis of images of natural objects as well as crucial model information in automated image analysis. Modeling and estimating the Statistics of Shapes and Textures is thus of major concern in automated image analysis. While a full description of the Statistics of Shapes and Textures may not always be possible or practical, partial descriptions may often be used instead. Such situations arise, when only partial information is available on the studied shapes and textures, and when the statistics of the shapes and textures is too complicated for a full description. Following a workshop on Statistics of Shapes and Textures held at the IT University of Copenhagen August 2000, a call for papers for this special issue of Journal of Mathematical Imaging and Vision was announced in 2001. Of 16 submitted papers 7 high quality papers were selected for publication, the result of which you now hold in your hand. The articles cover important aspects of the modeling of the Statistics of Shape and Textures and solutions to the inverse inference problem from images. The papers cover diverse methods such as functional minimization, sparse coding, deformable templates, granularity measures, and shape features, each with an in depth contribution to the development of the field.	autonomous robot;display resolution;heart rate variability;it university;image analysis;image segmentation;neural coding;outline of object recognition;robotic mapping;sparse matrix;statistical classification;texture mapping;visual inspection	Mads Nielsen;Lars Kai Hansen;Peter Johansen;Jon Sporring	2002	Journal of Mathematical Imaging and Vision	10.1023/A:1020620302003	computer vision;applied mathematics;artificial intelligence;mathematics	Vision	22.26907699830032	-61.124648457005314	53016
3939a2ac1c3be9e2441e891c4ddf1509d64e3b83	toward a computational tool predicting the stereochemical outcome of asymmetric reactions: development of the molecular mechanics-based program ace and application to asymmetric epoxidation reactions	epoxidation;asymmetric catalysis;ace;molecular mechanics;prediction	The development and application of ACE, a program that predicts the stereochemical outcome of asymmetric reactions is presented. As major implementations, ACE includes a genetic algorithm to carry out an efficient global conformational search combined with a conjugate gradient minimization routine for local optimization and a corner flap algorithm to search ring conformations. Further improvements have been made that enable ACE to generate Boltzmann populations of conformations, to investigate highly asynchronous reactions, to compute fluctuating partial atomic charges and solvation energy and to automatically construct reactants and products from libraries of catalysts and substrates. Validation on previously investigated reactions (asymmetric Diels Alder cycloadditions and organocatalyzed aldol reactions) followed by application to a number of alkene epoxidation reactions and a comparative study of DFT-derived and ACE-derived predictions demonstrate the accuracy and usefulness of ACE in the context of asymmetric catalyst design.		Nathanael Weill;Christopher R. Corbeil;Joris W. De Schutter;Nicolas Moitessier	2011	Journal of computational chemistry	10.1002/jcc.21869	stereochemistry;chemistry;prediction;enantioselective synthesis;molecular mechanics;organic chemistry;computational chemistry	Comp.	11.887230782406295	-60.36724983615921	53286
f31b9eb60b25c889e78fff0ffc2902959c9ccf17	predicting caco-2 cell permeation coefficients of organic molecules using membrane-interaction qsar analysis	caco 2 cell	A methodology termed membrane-interaction QSAR (MI-QSAR) analysis has been developed in order to predict the behavior of organic compounds interacting with the phospholipid-rich regions of biological membranes. One important application of MI-QSAR analysis is to estimate ADME properties including the transport of organic solutes through biological membranes as a computational approach to forecasting drug intestinal absorption. A training set of 30 structurally diverse drugs, whose permeability coefficients across the cellular membranes of Caco-2 cells were measured, was used to construct significant MI-QSAR models of Caco-2 cell permeation. Cellular permeation is found to depend primarily upon aqueous solvation free energy (solubility) of the drug, the extent of drug interaction with a model phospholipid (DMPC) monolayer, and the conformational flexibility of the solute within the model membrane. A test set of eight drugs was used to evaluate the predictivity of the MI-QSAR models. The permeation coefficients of the test set compounds were predicted with the same accuracy as the compounds of the training set.	adme study;coefficient;dimyristoylphosphatidylcholine;interaction;intestinal absorption;leukemia, b-cell;organic chemicals;phospholipids;projections and predictions;quantitative structure-activity relationship;quantitative structure–activity relationship;test set;tissue membrane;free energy;solute	Amit Kulkarni;Yi Han;Anton J. Hopfinger	2002	Journal of chemical information and computer sciences	10.1021/ci010108d	chromatography;stereochemistry;chemistry;computer science;organic chemistry	Comp.	11.536959673194733	-58.7192691612748	53374
c9611c730a088e141f9006ae6e99871e4bdff710	a coumarin-based fluorescent probe as a central nervous system disease biomarker	sensitivity and specificity;fluorescent probe;biosensing techniques;mass spectrometry;spectrometry fluorescence;coumarins;homocysteine;fluorescent dyes;reproducibility of results;humans;parkinson disease;neurochemistry;biological markers;q science;fluorescence spectroscopy;chemoassay;methylmalonic acid	Homocysteine and methylmalonic acid are important biomarkers for diseases associated with an impaired central nervous system (CNS). A new chemoassay utilizing coumarin-based fluorescent probe 1 to detect the levels of homocysteine is successfully implemented using Parkinson's disease (PD) patients' blood serum. In addition, a rapid identification of homocysteine and methylmalonic acid levels in blood serum of PD patients was also performed using the liquid chromatography-mass spectrometry (LC-MS). The results obtained from both analyses were in agreement. The new chemoassay utilizing coumarin-based fluorescent probe 1 offers a cost- and time-effective method to identify the biomarkers in CNS patients.	biological markers;cns disorder;effective method;liquid chromatography mass spectrometry;methylmalonic acid;parkinson disease;parkinsonian disorders;patients;coumarin;homocysteine	Ann-Chee Yap;Ummi Affah Mahamad;Shen-Yang Lim;Hae-Jo Kim;Yeun-Mun Choo	2014		10.3390/s141121140	biochemistry;neurochemistry;chemistry;mass spectrometry;fluorescence spectroscopy;analytical chemistry;quantum mechanics	ML	11.401917954435534	-64.62274065780565	53463
6a2fd8b4e2d0e1998069750129232204423f8f18	modeling steric and electronic effects in 3d- and 4d-qsar schemes: predicting benzoic pka values and steroid cbg binding affinities	binding affinity	We conducted a systematic study of the performance of the 3D- and 4D-QSAR schemes in modeling steric and electronic effects. In particular, we compared the CoMFA and Hopfinger's 4D-QSAR schemes, which apply completely different concepts for the generation of the molecular data used for modeling QSAR. Hence, we attempted to predict the pK(a) values of (o-, m-, and p-)benzoic acids which were divided into three subseries in order to simulate different levels of steric and electronic control. The steroids binding to CBG were used as a benchmark series where biological activity is limited by shape factors. Although individual models differ depending upon the individual scheme, generally, both CoMFA and 4D-QSAR appeared to provide comparable results, irrespective of the differences in the coding schemes used for the description. Moreover, a new 4D-QSAR scheme involving a self-organizing neural network was designed. Generally, the SOM scheme that we designed performs comparably to the grid scheme; however, it provides better results for the charge type descriptors, and the robust neuron architecture allows for the decrease of the influence of the molecular superimposition mode.		Jaroslaw Polanski;Andrzej Bak	2003	Journal of chemical information and computer sciences	10.1021/ci034118l	stereochemistry;chemistry;computer science;computational chemistry;ligand		10.480651248675105	-61.891892730969374	53476
98ed968f872dbc8fe2c421574653601cc64f1c96	tandem mass spectrometry protein identification on a pc grid.	tandem mass spectrometry;ms ms;pc grid;proteomics;protein identification	We present a method to grid-enable tandem mass spectrometry protein identification. The implemented parallelization strategy embeds the open-source x!tandem tool in a grid-enabled workflow. This allows rapid analysis of large-scale mass spectrometry experiments on existing heterogeneous hardware. We have explored different data-splitting schemes, considering both splitting spectra datasets and protein databases, and examine the impact of the different schemes on scoring and computation time. While resulting peptide e-values exhibit fluctuation, we show that these variations are small, caused by statistical rather than numerical instability, and are not specific to the grid environment. The correlation coefficient of results obtained on a standalone machine versus the grid environment is found to be better than 0.933 for spectra and 0.984 for protein identification, demonstrating the validity of our approach. Finally, we examine the effect of different splitting schemes of spectra and protein data on CPU time and overall wall clock time, revealing that judicious splitting of both data sets yields best overall performance.	boundary case;cpu (central processing unit of computer system);central processing unit;coefficient;computation;computation (action);database;databases, protein;experiment;genetic heterogeneity;greater;grid computing;inference;instability;large;layer (electronics);mathematical optimization;numerical stability;numerous;open architecture;open-source software;overhead (computing);parallel computing;proteomics;published database;quantum fluctuation;regulatory submission;score;silo (dataset);tandem mass spectrometry;time complexity	Dominique Zosso;Michael Podvinec;M. Müller;Ruedi Aebersold;Manuel C. Peitsch;Torsten Schwede	2007	Studies in health technology and informatics		chromatography;chemistry;bioinformatics;analytical chemistry	Comp.	12.248263374717808	-58.45589096434633	53565
2fb49e3f32bafa699e91396a01442b495d472544	evaluating model reduction under parameter uncertainty	clustering;model reduction;parameter uncertainty;systems biology	The dynamics of biochemical networks can be modelled by systems of ordinary differential equations. However, these networks are typically large and contain many parameters. Therefore model reduction procedures, such as lumping, sensitivity analysis and time-scale separation, are used to simplify models. Although there are many different model reduction procedures, the evaluation of reduced models is difficult and depends on the parameter values of the full model. There is a lack of a criteria for evaluating reduced models when the model parameters are uncertain. We developed a method to compare reduced models and select the model that results in similar dynamics and uncertainty as the original model. We simulated different parameter sets from the assumed parameter distributions. Then, we compared all reduced models for all parameter sets using cluster analysis. The clusters revealed which of the reduced models that were similar to the original model in dynamics and variability. This allowed us to select the smallest reduced model that best approximated the full model. Through examples we showed that when parameter uncertainty was large, the model should be reduced further and when parameter uncertainty was small, models should not be reduced much. A method to compare different models under parameter uncertainty is developed. It can be applied to any model reduction method. We also showed that the amount of parameter uncertainty influences the choice of reduced models.	approximation algorithm;assumed;cluster analysis;lumpers and splitters;mathematical model;population parameter;reduced cost;spatial variability	Håvard G. Frøysa;Shirin Fallahi;Nello Blaser	2018		10.1186/s12918-018-0602-x	mathematical optimization;cluster analysis;ordinary differential equation;systems biology;bioinformatics;biology	AI	13.185186086971218	-53.30862112505801	53582
2de7208ffb82ddc7a99addcdf10d5570dffc48aa	attention modulation using short- and long-term knowledge	short term memory;semantic memory;bottom up;long term memory;attention;proof of concept;visual search;semantic relations;scene representation	A fast and reliable visual search is crucial for representing visual scenes. The modulation of bottom-up attention plays an important role here. The knowledge about target features is often used to bias the bottom-up pathway. In this paper we propose a system which does not only make use of knowledge about the target features, but also uses already acquired knowledge about objects in the current scene to speed up the visual search. Main ingredients are a relational short term memory in combination with a semantic relational long term memory and an adjustable bottom-up saliency. The focus of this work is to investigate mechanisms to use the memory of the system efficiently. We show a proof-of-concept implementation working in a real-world environment and performing visual search tasks. It becomes clear that using the relational semantic memory in combination with spatial and feature modulation of the bottom-up path is beneficial for speeding up such search tasks.	modulation	Sven Rebhan;Florian Röhrbein;Julian Eggert;Edgar Körner	2008		10.1007/978-3-540-79547-6_15	computer vision;long-term memory;attention;visual search;semantic memory;top-down and bottom-up design;short-term memory;proof of concept	ML	22.458329116916243	-64.76310417497248	53773
a1da95978197244b13cd9749deb154432f930b6c	influence of sublethal lead concentrations on glucose, serum enzymes and ion levels in tilapia (oreochromis mossambicus)		In this study, alterations in glucose, blood enzymes (alkaline phosphatase (ALP), lactate dehydrogenase (LDH), alanine transaminase (ALT), aspartate aminotransferase (AST)) and serum ion (P, Mg, Cl, Ca, Fe) levels were investigated in Tilapia (Oreochromis mossambicus), which were semi-statically exposed to different lead concentrations in vivo. The fish were exposed to low (0.5 mg/L), medium (2.5 mg/L) and high (5 mg/L) concentrations of lead during 14 days. At the end of the experiment, biochemical blood parameters such as glucose, ALP, LDH, AST, chloride and magnesium increased (p<0.05). While, LDL and calcium levels decreased (p<0.05); ALT, cholesterol, albumin, iron and phosphor were fluctuated (p<0.05). Consequently, it was found that exposure of O. mossambicus to lead concentrations affected serum biochemical parameters negatively.	field electron emission;phosphor;semiconductor industry;video-in video-out	Hasan Kaya;Mehmet Akbulut;Sevdan Yilmaz	2015			magnesium;albumin;calcium;lactate dehydrogenase;alkaline phosphatase;oreochromis mossambicus;tilapia;internal medicine;endocrinology;chemistry;alanine transaminase	ML	10.556957825610278	-64.6976120112034	54290
1c662d7fde47a738db8e97a1e965ecdd77f2b4be	powerful, transferable representations for molecules through intelligent task selection in deep multitask networks		Chemical representations derived from deep learning are emerging as a powerful tool in areas such as drug discovery and materials innovation. Currently, this methodology has three major limitations the cost of representation generation, risk of inherited bias, and the requirement for large amounts of data. We propose the use of multi-task learning in tandem with transfer learning to address these limitations directly. In order to avoid introducing unknown bias into multi-task learning through the task selection itself, we calculate task similarity through pairwise task affinity, and use this measure to programmatically select tasks. We test this methodology on several real-world data sets to demonstrate its potential for execution in complex and low-data environments. Finally, we utilise the task similarity to further probe the expressiveness of the learned representation through a comparison to a commonly used cheminformatics fingerprint, and show that the deep representation is able to capture more expressive task-	algorithmic efficiency;cheminformatics;computer multitasking;deep learning;end-to-end principle;fingerprint;multi-task learning;processor affinity;while	Clyde Fare;Lukas Turcani;Edward O. Pyzer-Knapp	2018	CoRR		cheminformatics;computational chemistry;transfer of learning;chemistry;machine learning;deep learning;pairwise comparison;expressivity;artificial intelligence	ML	16.93751876894274	-52.25734226510131	54938
04e34e689386604ab37780c48797352321f95102	boxlets: a fast convolution algorithm for signal processing and neural networks	feature extraction;signal processing;pattern recognition;neural network	Signal processing and pattern recognition algorithms make extensive use of convolution. In many cases, computational accuracy is not as important as computational speed. In feature extraction, for instance, the features of interest in a signal are usually quite distorted. This form of noise justi es some level of quantization in order to achieve faster feature extraction. Our approach consists of approximating regions of the signal with low degree polynomials, and then di erentiating the resulting signals in order to obtain impulse functions (or derivatives of impulse functions). With this representation, convolution becomes extremely simple and can be implemented quite e ectively. The true convolution can be recovered by integrating the result of the convolution. This method yields substantial speed up in feature extraction and is applicable to convolutional neural networks.	artificial neural network;buzen's algorithm;cma-es;convolution;convolutional neural network;elegant degradation;feature extraction;pattern matching;pattern recognition;polynomial;signal processing;software propagation	Patrice Y. Simard;Léon Bottou;Patrick Haffner;Yann LeCun	1998			overlap–add method;feature extraction;computer science;theoretical computer science;machine learning;signal processing;pattern recognition;kernel;artificial neural network	ML	19.173185269280424	-54.6965893757917	55462
25a03ef63964b0258646d475baa1389fe7c11eea	rational design of alpha-helical tandem repeat proteins with closed architectures	databases;geociencias medio ambiente;protein design;molecular;bioengineering;ciencias biologicas generalidades;grupo de excelencia;ciencias basicas y experimentales generalidades;protein structure;proteins;ciencias basicas y experimentales;secondary;protein structure predictions;reproducibility of results;geociencias medio ambiente generalidades;ciencias biologicas;de novo protein;grupo a;x ray;protein;crystallography;computer simulation;amino acid motifs;models	Tandem repeat proteins, which are formed by repetition of modular units of protein sequence and structure, play important biological roles as macromolecular binding and scaffolding domains, enzymes, and building blocks for the assembly of fibrous materials. The modular nature of repeat proteins enables the rapid construction and diversification of extended binding surfaces by duplication and recombination of simple building blocks. The overall architecture of tandem repeat protein structures—which is dictated by the internal geometry and local packing of the repeat building blocks—is highly diverse, ranging from extended, super-helical folds that bind peptide, DNA, and RNA partners, to closed and compact conformations with internal cavities suitable for small molecule binding and catalysis. Here we report the development and validation of computational methods for de novo design of tandem repeat protein architectures driven purely by geometric criteria defining the inter-repeat geometry, without reference to the sequences and structures of existing repeat protein families. We have applied these methods to design a series of closed α-solenoid repeat structures (α-toroids) in which the inter-repeat packing geometry is constrained so as to juxtapose the amino (N) and carboxy (C) termini; several of these designed structures have been validated by X-ray crystallography. Unlike previous approaches to tandem repeat protein engineering, our design procedure does not rely on template sequence or structural information taken from natural repeat proteins and hence can produce structures unlike those seen in nature. As an example, we have successfully designed and validated closed α-solenoid repeats with a left-handed helical architecture that—to our knowledge—is not yet present in the protein structure database.	ank repeat domains;amino acid sequence;architecture as topic;carboxyl group;catalysis;clinical use template;computed tomography scanning systems;crystallography;de novo protein structure prediction;diversification (finance);gene duplication abnormality;protein engineering;protein family;protein structure database;rna;set packing;tandem computers;tandem repeat sequences;newton	Lindsey Doyle;Jazmine Hallinan;Jill Bolduc;Fabio Parmeggiani;David Baker;Barry L. Stoddard;Philip Bradley	2015		10.1038/nature16191	computer simulation;crystallography;biology;protein structure;molecule;bioinformatics;protein design	Comp.	11.301665902797485	-60.76235427572079	56234
32f57284f1e9a47175c2c752d48c01092f536d00	saliency prediction for visual regions of interest with applications in advertising		Human visual fixations play a vital role in a plethora of genres, ranging from advertising design to human-computer interaction. Considering saliency in images thus brings significant merits to Computer Vision tasks dealing with human perception. Several classification models have been developed to incorporate various feature levels and estimate free eye-gazes. However, for real-time applications (Here, real-time applications refer to those that are time, and often resource-constrained, requiring speedy results. It does not imply on-line data analysis), the deep convolution neural networks are either difficult to deploy, given current hardware limitations or the proposed classifiers cannot effectively combine image semantics with low-level attributes. In this paper, we propose a novel neural network approach to predict human fixations, specifically aimed at advertisements. Such analysis significantly impacts the brand value and assists in audience measurement. A dataset containing 400 print ads across 21 successful brands was used to successfully evaluate the effectiveness of advertisements and their associated fixations, based on the proposed saliency prediction model.	region of interest	Shailee Jain;S. Sowmya Kamath	2016		10.1007/978-3-319-56687-0_5	support vector machine;brand equity;audience measurement;artificial neural network;salience (neuroscience);perception;ranging;computer science;semantics;advertising	ML	23.52604549352517	-58.164914356466255	56374
6056a3e3b05b328231d41003b77d560c8a05192e	whole image synthesis using a deep encoder-decoder network	t technology general;qa76 computer software	The synthesis of medical images is an intensity transformation of a given modality in a way that represents an acquisition with a different modality (in the context of MRI this represents the synthesis of images originating from different MR sequences). Most methods follow a patchbased approach, which is computationally inefficient during synthesis and requires some sort of ‘fusion’ to synthesize a whole image from patch-level results. In this paper, we present a whole image synthesis approach that relies on deep neural networks. Our architecture resembles those of encoder-decoder networks, which aims to synthesize a source MRI modality to an other target MRI modality. The proposed method is computationally fast, it doesn’t require extensive amounts of memory, and produces comparable results to recent patch-based approaches.	artificial neural network;backpropagation;biobank;central processing unit;deep learning;encoder;experiment;geo-imputation;graphics processing unit;medical imaging;modality (human–computer interaction);patch (computing);rendering (computer graphics);restricted boltzmann machine;software propagation;titan rain	Vasileios Sevetlidis;Mario Valerio Giuffrida;Sotirios A. Tsaftaris	2016		10.1007/978-3-319-46630-9_13	computer vision;computer science;artificial intelligence;communication	ML	23.793965149077522	-52.12689317850548	56429
9967dbbfc43f0249dbbe66cf02762e8b4a753976	compressed sensing dynamic mri reconstruction using gpu-accelerated 3d convolutional sparse coding		In this paper, we introduce a fast alternating method for reconstructing highly undersampled dynamic MRI data using 3D convolutional sparse coding. The proposed solution leverages Fourier Convolution Theorem to accelerate the process of learning a set of 3D filters and iteratively refine the MRI reconstruction based on the sparse codes found subsequently. In contrast to conventional CS methods which exploit the sparsity by applying universal transforms such as wavelet and total variation, our approach extracts and adapts the temporal information directly from the MRI data using compact shift-invariant 3D filters. We provide a highly parallel algorithm with GPU support for efficient computation, and therefore, the reconstruction outperforms CPU implementation of the state-of-the art dictionary learning-based approaches by up to two orders of magnitude.	central processing unit;code;compressed sensing;computation;convolution;dictionary;graphics processing unit;machine learning;neural coding;numerical analysis;parallel algorithm;solver;sparse matrix;time complexity;wavelet	Tran Minh Quan;Won-Ki Jeong	2016		10.1007/978-3-319-46726-9_56	machine learning;pattern recognition	ML	23.488596605893893	-52.14988460453898	56670
3a858640162e4fb85a7b2d848daa3ef3f06ad628	fuzzy tricentric pharmacophore fingerprints, 1. topological fuzzy pharmacophore triplets and adapted molecular similarity scoring schemes	computer programs;computers in chemistry	"""This paper introduces a novel molecular description--topological (2D) fuzzy pharmacophore triplets, 2D-FPT--using the number of interposed bonds as the measure of separation between the atoms representing pharmacophore types (hydrophobic, aromatic, hydrogen-bond donor and acceptor, cation, and anion). 2D-FPT features three key improvements with respect to the state-of-the-art pharmacophore fingerprints: (1) The first key novelty is fuzzy mapping of molecular triplets onto the basis set of pharmacophore triplets: unlike in the binary scheme where an atom triplet is set to highlight the bit of a single, best-matching basis triplet, the herein-defined fuzzy approach allows for gradual mapping of each atom triplet onto several related basis triplets, thus minimizing binary classification artifacts. (2) The second innovation is proteolytic equilibrium dependence, by explicitly considering all of the conjugated acids and bases (microspecies). 2D-FPTs are concentration-weighted (as predicted at pH=7.4) averages of microspecies fingerprints. Therefore, small structural modifications, not affecting the overall pharmacophore pattern (in the sense of classical rule-based assignment), but nevertheless triggering a pKa shift, will have a major impact on 2D-FPT. Pairs of almost identical compounds with significantly differing activities (""""activity cliffs"""" in classical descriptor spaces) were in many cases predictable by 2D-FPT. (3) The third innovation is a new similarity scoring formula, acknowledging that the simultaneous absence of a triplet in two molecules is a less-constraining indicator of similarity than its simultaneous presence. It displays excellent neighborhood behavior, outperforming 2D or 3D two-point pharmacophore descriptors or chemical fingerprints. The 2D-FPT calculator was developed using the chemoinformatics toolkit of ChemAxon (www.chemaxon.com)."""	aromatics;acceptor (semiconductors);anions;atom;base;basis set (chemistry);binary classification;cations;cheminformatics;fingerprint;fuzzy logic;hydrogen;immunostimulating conjugate (antigen);logic programming;matching;morphologic artifacts;parameterized complexity;pharmacophore;score;triplet state	Fanny Bonachéra;Benjamin Parent;Frédérique Barbosa;Nicolas Froloff;Dragos Horvath	2006	Journal of chemical information and modeling	10.1021/ci6002416	chemistry;bioinformatics;computational chemistry	Comp.	10.413883458900736	-60.84692646380522	56812
754876c90790b158aef5de8bbea297539dc81397	perceptually-guided understanding of egocentric video content: recognition of objects to grasp		Incorporating user perception into visual content search and understanding tasks has become one of the major trends in multimedia retrieval. We tackle the problem of object recognition guided by user perception, as indicated by his gaze during visual exploration, in the application domain of assistance to upper-limb amputees. Although selecting the object to be grasped represents a task-driven visual search, human gaze recordings are noisy due to several physiological factors. Hence, since gaze does not always point to the object of interest, we use video-level weak annotations indicating the object to be grasped, and propose a video-level weak loss in classification with Deep CNNs. Our results show that the method achieves notably better performance than other approaches over a complex real-life dataset specifically recorded, with optimal performance for fixation times around 400-800ms, producing a minimal impact on subjects' behavior.	active object;algorithm;application domain;automatic control;coat of arms;object detection;outline of object recognition;real life;recurrent neural network;sensor;video clip	Iván González-Díaz;Jenny Benois-Pineau;Jean-Philippe Domenger;Aymar de Rugy	2018		10.1145/3206025.3206073	application domain;machine learning;gaze;visual search;artificial intelligence;perception;computer science;grasp;cognitive neuroscience of visual object recognition	Vision	23.7892811639508	-58.72573420170468	56936
82d3a49274771b2f151462d310c779ae6d0f9ee8	biologically inspired object categorization in cluttered scenes	object recognition;layout neurons humans neural networks visual system computer science feature extraction gabor filters pattern recognition object recognition;ventral object categorization human visual system;neural nets;object categorization;biologically inspired object categorization;image classification;neural network biologically inspired object categorization cluttered scenes object recognition human visual system;ventral;human visual system;cluttered scenes;object recognition image classification neural nets;neural network	Humans have the ability to recognize objects in a cluttered scene in 100 s of milliseconds. Computer algorithms operate at a much lower performance level compared to humans. Furthermore, it has proven to be particularly difficult to develop algorithms to recognize all objects in a category, such as, all cat faces vs dog faces, because of the large in-class variability. The distinguishing features can vary significantly among different objects in the same class. A similar case can be made for other categories, such as, cars, human faces, etc. In this paper we approach this problem using a model of the human visual system. The human visual system can be divided into two major pathways, commonly called the 'what' and 'where' pathways. The 'what' pathway recognizes an object in a scene, but not its specific location. In this paper we present a biologically inspired hierarchical 'what' neural network that can successfully classify objects into categories.	algorithm;artificial neural network;categorization;feature extraction;gabor wavelet;gene regulatory network;human visual system model;humans;javaserver faces;spatial variability;windows 95	Theparit Peerasathein;Myung Woo;Roger S. Gaborski	2007	36th Applied Imagery Pattern Recognition Workshop (aipr 2007)	10.1109/AIPR.2007.13	computer vision;boosting methods for object categorization;form perception;computer science;machine learning;3d single-object recognition;communication	Vision	23.162566079234814	-63.5381777586071	57018
dd99f017c46b9d06b6ed84745d28bee47c4be21f	protein secondary structure prediction using rule induction from coverings	experimental method;relaxed threshold rule induction;rule induction;molecular configurations;amino acid;protein sequence;rule based;prediction algorithms;computational method;data mining;protein secondary structure;training data;accuracy;protein structure;proteins;protein secondary structure prediction;secondary structure;data mining algorithm;molecular biophysics;prediction accuracy;amino acids;protein motif;genome sequence;protein sequence protein secondary structure prediction rule induction data mining algorithm relaxed threshold rule induction amino acids;proteins data mining molecular biophysics molecular configurations;amino acids protein sequence protein engineering probability genomics bioinformatics induction generators accuracy nuclear magnetic resonance neural networks	With the increase of data from genome sequencing projects comes the need for reliable and efficient methods for the analysis and classification of protein motifs and domains. Experimental methods currently used to determine protein structure are accurate, yet expensive both in terms of time and equipment. Therefore, various computational approaches to solving the problem have been attempted, although their accuracy has rarely exceeded 75%. In this paper, a rule-based method to predict protein secondary structure is presented. This method uses a newly developed data-mining algorithm called RT-RICO (Relaxed Threshold Rule Induction from Coverings), which identifies dependencies between amino acids in a protein sequence, and generates rules that can be used to predict secondary structures. The average prediction accuracy on sample data sets, or Q3 score, using RT-RICO was 80.3%, an improvement over comparable computational methods	algorithm;computation;data mining;logic programming;protein family;protein structure prediction;rule induction;test data;test set;type class;whole genome sequencing;windows rt	Leong Lee;Jennifer L. Leopold;Ronald L. Frank;Anne M. Maglia	2009	2009 IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology	10.1109/CIBCB.2009.4925711	biology;biochemistry;amino acid;computer science;bioinformatics;machine learning;data mining;protein secondary structure;molecular biophysics	Comp.	10.811099540280294	-54.650054184378426	57065
36f7d3ee9c7f483740579c575b51c81eda0ed542	consistent improvement of cross-docking results using binding site ensembles generated with elastic network normal modes	binding site;binding sites;proteins;protein conformation;normal modes;calibration	The representation of protein flexibility is still a challenge for the state-of-the-art flexible ligand docking protocols. In this article, we use a large and diverse benchmark to prove that is possible to improve consistently the cross-docking performance against a single receptor conformation, using an equilibrium ensemble of binding site conformers. The benchmark contained 28 proteins, and our method predicted the top-ranked near native ligand poses 20% more efficiently than using a single receptor. The multiple conformations were derived from the collective variable space defined by all heavy-atom elastic network normal modes, including backbone and side chains. We have found that the binding site displacements for best positioning of the ligand seem rather independent from the global collective motions of the protein. We also found that the number of binding site conformations needed to represent nonredundant flexibility was < 100. The ensemble of receptor conformations can be generated at our Web site at http://abagyan.scripps.edu/MRC.	benchmark (computing);binding sites;boat dock;contain (action);docking (molecular);docking -molecular interaction;equilibrium;internet backbone;ligand binding domain;ligands;motion;normal mode;protocols documentation;reaction coordinate;type iii site-specific deoxyribonuclease;vertebral column	Manuel Rueda;Giovanni Bottegoni;Ruben Abagyan	2009	Journal of chemical information and modeling	10.1021/ci8003732	crystallography;chemistry;searching the conformational space for docking;bioinformatics;binding site;computational chemistry;nuclear magnetic resonance	Comp.	11.296277205222951	-60.74201313678528	57105
42b23956faafa06e65526d062fbdf510fa74624a	filtered circular fingerprints improve either prediction or runtime performance while retaining interpretability	computer applications in chemistry;theoretical and computational chemistry;computational biology bioinformatics;documentation and information in chemistry	BACKGROUND Even though circular fingerprints have been first introduced more than 50 years ago, they are still widely used for building highly predictive, state-of-the-art (Q)SAR models. Historically, these structural fragments were designed to search large molecular databases. Hence, to derive a compact representation, circular fingerprint fragments are often folded to comparatively short bit-strings. However, folding fingerprints introduces bit collisions, and therefore adds noise to the encoded structural information and removes its interpretability. Both representations, folded as well as unprocessed fingerprints, are often used for (Q)SAR modeling.   RESULTS We show that it can be preferable to build (Q)SAR models with circular fingerprint fragments that have been filtered by supervised feature selection, instead of applying folded or all fragments. Compared to folded fingerprints, filtered fingerprints significantly increase predictive performance and remain unambiguous and interpretable. Compared to unprocessed fingerprints, filtered fingerprints reduce the computational effort and are a more compact and less redundant feature representation. Depending on the selected learning algorithm filtering yields about equally predictive (Q)SAR models. We demonstrate the suitability of filtered fingerprints for (Q)SAR modeling by presenting our freely available web service Collision-free Filtered Circular Fingerprints that provides rationales for predictions by highlighting important structural features in the query compound (see http://coffer.informatik.uni-mainz.de).   CONCLUSIONS Circular fingerprints are potent structural features that yield highly predictive models and encode interpretable structural information. However, to not lose interpretability, circular fingerprints should not be folded when building prediction models. Our experiments show that filtering is a suitable option to reduce the high computational effort when working with all fingerprint fragments. Additionally, our experiments suggest that the area under precision recall curve is a more sensible statistic for validating (Q)SAR models for virtual screening than the area under ROC or other measures for early recognition.   GRAPHICAL ABSTRACT	algorithm;computation;database;databases;databases, molecular;encode;experiment;feature selection;filter (signal processing);fingerprint;fingerprints;precision and recall;predictive modelling;question (inquiry);run time (program lifecycle phase);virtual screening;web service;collision	Martin Gütlein;Stefan Kramer	2016		10.1186/s13321-016-0173-z	computer science;bioinformatics;theoretical computer science;data mining	AI	11.103032529763908	-55.78492420445396	57153
34088357ed609dc3691f2bc3ff80ce59bee62c37	on entropy-based molecular descriptors: statistical analysis of real and synthetic chemical structures	statistical analysis;molecular descriptor;chemical structure	This paper presents an analysis of entropy-based molecular descriptors. Specifically, we use real chemical structures, as well as synthetic isomeric structures, and investigate properties of and among descriptors with respect to the used data set by a statistical analysis. Our numerical results provide evidence that synthetic chemical structures are notably different to real chemical structures and, hence, should not be used to investigate molecular descriptors. Instead, an analysis based on real chemical structures is favorable. Further, we find strong hints that molecular descriptors can be partitioned into distinct classes capturing complementary information.	class;molecular descriptor;numerical analysis;synthetic intelligence	Matthias Dehmer;Kurt Varmuza;Stephan Borgert;Frank Emmert-Streib	2009	Journal of chemical information and modeling	10.1021/ci900060x	molecular descriptor;chemistry;toxicology;bioinformatics;organic chemistry;combinatorial chemistry;computational chemistry;chemical structure	Comp.	12.101811060157637	-57.623521525253885	57185
10553f59331132069219449efc3b20c18606f189	fleksy: a flexible approach to induced fit docking	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;computer applications in chemistry;theoretical and computational chemistry;computational biology bioinformatics;uk phd theses thesis;life sciences;uk research reports;medical journals;europe pmc;documentation and information in chemistry;induced fit;biomedical research;bioinformatics	Protein receptor rearrangements upon ligand binding are a major complicating factor in structure-based drug design. An accurate prediction of these so-called induced fit phenomena calls for ligand docking and virtual screening approaches capable of considering receptor flexibility. We present Fleksy [1], a flexible approach aimed at accurately positioning small molecule ligands into a protein receptor, while taking both ligand and receptor flexibility into account. Our method consists of an ensemble docking stage in which the ligand of interest is docked into a structural ensemble of receptor conformations, followed by a complex optimization stage during which both ligand and protein are allowed to move. Pivotal to our method is the use of receptor ensembles to describe protein flexibility. To construct these ensembles we use a backbone dependent rotamer library and implement the concept of interaction sampling. The latter allows for the evaluation of different orientations and, when relevant, different tautomers of ambivalent interaction partners in the binding site such as asparagine, glutamine and histidine side chains. The docking stage comprises an ensemble-based soft-docking experiment using FlexX-Ensemble [2], followed by an effective flexible receptor-ligand complex optimization using Yasara [3]. Ultimately Fleksy results in a set of receptor-ligand complexes ranked using a consensus scoring function which combines both docking scores and force field energies. Figure 1. Averaged over three cross-docking datasets, in total containing 35 different pharmaceutically relevant receptor-ligand complexes, Fleksy reproduces the observed binding mode within 2.0 Å for 78% of the complexes. This compares favorably to the rigid receptor FlexX program [4] which on average reaches a success rate of 44% for these datasets.	docking (molecular);ensemble forecasting;fleksy;force field (chemistry);internet backbone;mathematical optimization;sampling (signal processing);scoring functions for docking;virtual screening	Markus Wagener;Sander B. Nabuurs;Jacob de Vlieg	2010		10.1186/1758-2946-2-S1-O24	biology;simulation;medicine;searching the conformational space for docking;computer science;bioinformatics;enzyme catalysis	Comp.	11.437400657198694	-59.87796537378302	57227
6901b9e210278c7010bd80c0135e6ae02a1e6821	what can i do with this tool? self-supervised learning of tool affordances from their 3-d geometry		The ability to use tools can significantly increase the range of activities that an agent is capable of. Humans start using external objects since an early age to accomplish their goals, learning from interaction and observation the relationship between the objects used, their own actions, and the resulting effects, i.e., the tool affordances. Robots capable of autonomously learning affordances in a similar self-supervised way would be far more versatile and simpler to design than purpose-specific ones. This paper proposes and evaluates an approach to allow robots to learn tool affordances from interaction, and generalize them among similar tools based on their 3-D geometry. A set of actions is performed by the iCub robot with a large number of tools grasped in different poses, and the effects observed. Tool affordances are learned as a regression between tool-pose features and action-effect vector projections on respective self-organizing maps, which enables the system to avoid categorization and keep gradual representations of both elements. Moreover, we propose a set of robot-centric 3-D tool descriptors, and study their suitability for interaction scenarios, comparing also their performance against features derived from deep convolutional neural networks. Results show that the presented methods allow the robot to predict the effect of its tool use actions accurately, even for previously unseen tool and poses, and thereby to select the best action for a particular goal given a tool-pose.	artificial neural network;categorization;convolutional neural network;dimensionality reduction;humans;icub;map;organizing (structure);programming tool;robot;self-organization;simulation;supervised learning	Tanis Mar;Vadim Tikhanoff;Lorenzo Natale	2018	IEEE Transactions on Cognitive and Developmental Systems	10.1109/TCDS.2017.2717041	affordance;supervised learning;machine learning;computer science;convolutional neural network;artificial intelligence;categorization;feature extraction;geometry;robot kinematics	Robotics	21.03446881626913	-56.1385946541986	57273
7aac4d030baba7adf1b62f126558ddb348d4bd62	computational neural networks for resolving nonlinear multicomponent systems based on chemiluminescence methods	neural network	This paper proves that computational neural networks are reliable, effective tools for resolving nonlinear multicomponent systems involving synergistic effects by using chemiluminescence-based methods developed by continuous addition of reagent technique. Computational neural networks (CNNs) were implemented using a preprocessing of data by principal component analysis; the principal components to be used as input to the CNN were selected on the basis of a heuristic method. The leave-one-out method was applied on the basis of theoretical considerations in order to reduce sample size with no detriment to the prediction capacity of the network. The proposed approach was used to resolve trimeprazine/methotrimeprazine mixtures with a classical peroxyoxalate chemiluminescent system, such as the reaction between bis(2,4,6-trichlorophenyl)oxalate and hydrogen peroxide. The optimum network design, 9:5s:2l, allowed the resolution of mixtures of the two analytes in concentration ratios from 1:10 to 10:1 with very s...	artificial neural network;computation	César Hervás-Martínez;Sebastián Ventura;Manuel Silva;Dolores Pérez-Bendito	1998	Journal of Chemical Information and Computer Sciences	10.1021/ci980030+	chromatography;chemistry;computer science;analytical chemistry;machine learning;artificial neural network	ML	12.034893981762927	-56.07262529619354	57435
b4864a4b0c4c8668a57ff6119ef9224a02a6ea58	multi-modal imaging of histological tissue sections	molecular pathology;brightfield imaging;biological tissues;hematoxylin eosin;histological tissue sections;fluorescence;tissue microarray sections multimodal imaging histological tissue sections sequential imaging sequential registration brightfield imaging fluorescent imaging anatomic pathology molecular pathology fluorescent biomarkers h e dyes;fluorescence imaging;h e dyes;anatomic pathology;tissue microarray sections;fluorescent biomarkers;optical imaging fluorescence optical microscopy pathology morphology immune system proteins marine animals genetics image analysis;fluorescence microscopy;molecular biophysics biological tissues biomedical optical imaging diseases fluorescence image registration medical image processing;medical image processing;image registration;multimodal imaging;brightfield microscopy;molecular biophysics;success rate;fluorescent microscopy;diseases;fluorescent imaging;biomedical optical imaging;sequential imaging;sequential registration;fluorescent in situ hybridization;fluorescent microscopy image registration brightfield microscopy	Two common imaging modalities for histological sections are brightfield and fluorescence microscopy imaging. Hematoxylin-Eosin (H&E) based brightfield microscopy has been the traditional imaging technique for imaging morphology, while an epi-fluorescent microscope is used for immunofluorescent staining of specific proteins or fluorescent in situ hybridization (FISH) for genetic based analysis of DNA. Simultaneous imaging of both microscopy modalities has been difficult due to optical and chemical effects of the H&E dyes. We present a novel sequential imaging and registration technique that enables brightfield and fluorescent imaging on the same tissue section, hence combining the traditional anatomic pathology with the newly emerging field of molecular pathology. First the tissue is labeled with fluorescent biomarkers, and imaged through a fluorescence microscope, and then the tissue is re-labeled with H&E dyes, and imaged again with traditional brightfield. Our robust registration algorithms achieve 99.8% registration success rate on tissue micro array (TMA) sections.	algorithm;galaxy morphological classification;microarray;modal logic;tower mounted amplifier	Ali Can;Musodiq O. Bello;Harvey E. Cline;Xiaodong Tao;Fiona Ginty;Anup Sood;Michael J. Gerdes;Michael Montalto	2008	2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2008.4540989	fluorescence microscope;medicine;pathology;molecular pathology;anatomy;molecular biophysics	Visualization	13.632826519804043	-65.86690044549167	57834
d7fc800abdbcba28db3b5adb0d015ad052901dce	combining ligand- and structure-based approaches for the discovery of new inhibitors of the epha2-ephrin-a1 interaction		The EPH receptor A2 (EPHA2) represents an attractive anticancer target. With the aim to identify novel EPHA2 receptor antagonists, a virtual screening campaign, combining shape-similarity and docking calculations, was conducted on a set of commercially available compounds. A combined score, taking into account both ligand- and structure-based results, was then used to identify the most promising candidates. Two compounds, selected among the best-ranked ones, were identified as EPHA2 receptor antagonists with micromolar affinity.	affinity analysis;boat dock;coxsackievirus a1 ab:acnc:pt:ser:qn;docking (molecular);epha2 gene;micromole/liter;virtual screening	Daniele Pala;Riccardo Castelli;Matteo Incerti;Simonetta Russo;Massimiliano Tognolini;Carmine Giorgio;Iftiin Hassan-Mohamed;Ilaria Zanotti;Federica Vacondio;Silvia Rivara;Marco Mor;Alessio Lodola	2014	Journal of chemical information and modeling	10.1021/ci5004619	pharmacology;chemistry;bioinformatics;combinatorial chemistry	Comp.	10.424632709872881	-59.279581990275666	57856
dc3c75da379280322f72a12cc4dd47d357738070	taking a machine's perspective: human deciphering of adversarial images		How similar is the human mind to the sophisticated machine-learning systems that mirror its performance? Models of object categorization based on convolutional neural networks (CNNs) have achieved human-level benchmarks in assigning known labels to novel images (Krizhevsky, Sutskever, & Hinton, 2012; LeCun, Bengio, & Hinton, 2015). These advances support transformative technologies such as autonomous vehicles and machine diagnosis; beyond this, they also serve as candidate models for the visual system itself — not only in their output but perhaps even in their underlying mechanisms and principles (Cichy et al., 2016; Kriegeskorte, 2015; Yamins & DiCarlo, 2016). However, unlike human vision, CNNs can be “fooled” by adversarial examples — carefully crafted images that appear as nonsense patterns to humans but are recognized as familiar objects by machines, or that appear as one object to humans and a different object to machines (Athalye et al., 2017; Karmon, Zoran, & Goldberg, 2018; Nguyen, Yosinski, & Clune, 2015; Papernot et al., 2016). This seemingly extreme divergence between human and machine classification challenges the promise of these new advances, both as applied image-recognition systems and also as models of the human mind. Surprisingly, however, little work has empirically investigated human classification of such adversarial stimuli: Does human and machine performance fundamentally diverge? Or could humans decipher such images and predict the machine’s preferred labels? Here, we show that human and machine classification of adversarial stimuli are robustly related: In seven experiments on five prominent and diverse adversarial imagesets, human subjects reliably identified the machine’s chosen label over relevant foils. This pattern persisted for images with strong antecedent identities, and even for images described as “totally unrecognizable to human eyes”. We suggest that human intuition may be a more reliable guide to machine (mis)classification than has typically been imagined, and we explore the consequences of this result for minds and machines alike.	artificial neural network;autonomous robot;categorization;computer vision;convolutional neural network;experiment;humans;machine learning;mind;minds and machines;word lists by frequency	Zhenglong Zhou;Chaz Firestone	2018	CoRR		convolutional neural network;nonsense;adversarial system;pattern recognition;categorization;artificial intelligence;computer science;machine learning;transformative learning;intuition	AI	20.12374595670995	-52.15422231602216	57942
372fefe66aa693e271ec6298fac1695208f36aee	face deidentification with generative deep neural networks		Face deidentification is an active topic amongst privacy and security researchers. Early deidentification methods relying on image blurring or pixelization were replaced in recent years with techniques based on formal anonymity models that provide privacy guaranties and at the same time aim at retaining certain characteristics of the data even after deidentification. The latter aspect is particularly important, as it allows to exploit the deidentified data in applications for which identity information is irrelevant. In this work we present a novel face deidentification pipeline, which ensures anonymity by synthesizing artificial surrogate faces using generative neural networks (GNNs). The generated faces are used to deidentify subjects in images or video, while preserving non-identity-related aspects of the data and consequently enabling data utilization. Since generative networks are very adaptive and can utilize a diverse set of parameters (pertaining to the appearance of the generated output in terms of facial expressions, gender, race, etc.), they represent a natural choice for the problem of face deidentification. To demonstrate the feasibility of our approach, we perform experiments using automated recognition tools and human annotators. Our results show that the recognition performance on deidentified images is close to chance, suggesting that the deidentification process based on GNNs is highly effective.	artificial neural network;de-identification;deep learning;experiment;privacy;relevance	Blaz Meden;Refik Can Malli;Sebastjan Fabijan;Hazim Kemal Ekenel;Vitomir Struc;Peter Peer	2017	IET Signal Processing	10.1049/iet-spr.2017.0049	generative grammar;mathematics;machine learning;mathematical optimization;anonymity;artificial neural network;facial expression;artificial intelligence;deidentification	AI	21.284037492146002	-55.61032070399608	57962
c7b4a501e0c10ae830b5925431da36397dee2848	recent advances in deep learning for single image super-resolution		Image super-resolution is an important research field in image analysis. The techniques of image super-resolution has been widely used in many computer vision applications. In recent years, the success of deep learning methods in image super-resolution have attracted more and more researchers. This paper gives a brief review of recent deep learning based methods for single image super-resolution (SISR), in terms of network type, network structure, and training methods. The advantages and disadvantages of these methods are analyzed as well.	deep learning;super-resolution imaging	Yungang Zhang;Yu Xiang	2018		10.1007/978-3-030-00563-4_9	convolutional neural network;deep learning;computer vision;superresolution;computer science;artificial intelligence	ML	24.216631309678473	-55.0589860701241	58153
66470a64c91d27e01c3cae49980d417566268481	transferring and generalizing deep-learning-based neural encoding models across subjects	bayesian inference;deep learning;incremental learning;natural vision;neural encoding	Recent studies have shown the value of using deep learning models for mapping and characterizing how the brain represents and organizes information for natural vision. However, modeling the relationship between deep learning models and the brain (or encoding models), requires measuring cortical responses to large and diverse sets of natural visual stimuli from single subjects. This requirement limits prior studies to few subjects, making it difficult to generalize findings across subjects or for a population. In this study, we developed new methods to transfer and generalize encoding models across subjects. To train encoding models specific to a target subject, the models trained for other subjects were used as the prior models and were refined efficiently using Bayesian inference with a limited amount of data from the target subject. To train encoding models for a population, the models were progressively trained and updated with incremental data from different subjects. For the proof of principle, we applied these methods to functional magnetic resonance imaging (fMRI) data from three subjects watching tens of hours of naturalistic videos, while a deep residual neural network driven by image recognition was used to model visual cortical processing. Results demonstrate that the methods developed herein provide an efficient and effective strategy to establish both subject-specific and population-wide predictive models of cortical representations of high-dimensional and hierarchical visual features.	artificial neural network;bayesian network;biological neural networks;computer vision;deep learning;increment;inference;magnetic resonance imaging;neural coding;population;predictive modelling;transcutaneous electric nerve stimulation;fmri	Haiguang Wen;Junxing Shi;Wei Chen;Zhongming Liu	2018	NeuroImage	10.1016/j.neuroimage.2018.04.053	proof of concept;encoding (memory);deep learning;visual perception;machine learning;functional magnetic resonance imaging;artificial intelligence;population;bayesian inference;computer science;generalization	ML	19.965800805823754	-65.96204665857519	58938
6751b00b5f777bfbf71e2fe3cbfea4f37df23d4b	a growth model having the abilities of growth-regulations for simulating visual nature of botanical trees	boraginaceae;spermatophyta;imaginary plant hormone;tree;angiospermae;arbol;simulation;simulacion;heliotropium;apical dominance;heliotropism;withering;change in leadership;geotropism;dormancy breaking;geotropismo;growth regulator;growth mechanism;arbre;mecanisme croissance;dicotyledones;suppresion of lateral shoots;dominance apicale;hormona;hormone;ruptura dormicion;mecanismo crecimiento;levee dormance;dominancia apical;botany;growth model;botanica;botanique;geotropisme	"""-Although the representation methods of botanical trees have been presented by several researchers in recent years, simulating natural tree features acquired in a growth process still remains a challenging problem. Since tree shape is significantly influenced by its growth environment, such as sunlight conditions and random """"'accidental"""" pruning of branches (e.g., caused by storm or a gardener), not only does no tree have a regular shape, but no two trees are identical, even if they are of the same species. In previous papers, we have shown an attractive fact that if we take into account the abilities of several growth regulations, such as heliotropism and dormancy break, we can easily simulate realistic irregular branching patterns. In this paper, we will present the improved growth model taking into account the following growth regulations: (a) withering, (b) heliotropism, (c) geotropism, and (d) apical dominance: (d-1) suppression to lateral shoots, (d-2) dormancy break, and (d-3) change in leadership. These growth regulations are implemented by employing an """"imaginary plant hormone"""" for implementing the ability of the communication between all buds and branches of a tree. This means that any """"central control unit"""" that keeps watch on the condition of the whole tree and issues commands for each bud and branch to control the growth is unnecessary for a tree. This point is one of the interesting features of our growth model."""	control unit;emoticon;error-tolerant design;imaginary time;lateral thinking;population dynamics;simulation;zero suppression	Norishige Chiba;Ken Ohshida;Kazunobu Muraoka;Mamoru Miura;Nobuji Saito	1994	Computers & Graphics	10.1016/0097-8493(94)90059-0	gravitropism;hormone;artificial intelligence;apical dominance;tree;heliotropism	Graphics	15.151209759121423	-64.37696710986083	58964
3e72365bd07862323f189643486410b2f55b1979	texture synthesis through convolutional neural networks and spectrum constraints	art;convolution;feature extraction;correlation;discrete fourier transforms;biological neural networks	This paper presents a significant improvement for the synthesis of texture images using convolutional neural networks (CNNs), making use of constraints on the Fourier spectrum of the results. More precisely, the texture synthesis is regarded as a constrained optimization problem, with constraints conditioning both the Fourier spectrum and statistical features learned by CNNs. In contrast with existing methods, the presented method inherits from previous CNN approaches the ability to depict local structures and fine scale details, and at the same time yields coherent large scale structures, even in the case of quasi-periodic images. This is done at no extra computational cost. Synthesis experiments on various images show a clear improvement compared to a recent state-of-the art method relying on CNN constraints only.	algorithmic efficiency;artificial neural network;coherence (physics);computation;computational complexity theory;constrained optimization;constraint (mathematics);convolution;convolutional neural network;experiment;fourier analysis;loss function;mathematical optimization;optimization problem;quasiperiodicity;texture synthesis	Gang Liu;Yann Gousseau;Gui-Song Xia	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7900133	computer vision;feature extraction;computer science;artificial intelligence;machine learning;geometry;convolution;correlation	Vision	24.27943571025378	-52.635266346701265	58974
99facca6fc50cc30f13b7b6dd49ace24bc94f702	viplfacenet: an open source deep face recognition sdk	deep learning face recognition open source viplfacenet	Robust face representation is imperative to highly accurate face recognition. In this work, we propose an open source face recognition method with deep representation named as VIPLFaceNet, which is a 10-layer deep convolutional neural network with seven convolutional layers and three fully-connected layers. Compared with the well-known AlexNet, our VIPLFaceNet takes only 20% training time and 60% testing time, but achieves 40% drop in error rate on the real-world face recognition benchmark LFW. Our VIPLFaceNet achieves 98.60% mean accuracy on LFW using one single network. An open-source C++ SDK based on VIPLFaceNet is released under BSD license. The SDK takes about 150ms to process one face image in a single thread on an i7 desktop CPU. VIPLFaceNet provides a state-of-the-art start point for both academic and industrial face recognition applications.	artificial neural network;bsd;benchmark (computing);c++;captcha;central processing unit;convolutional neural network;desktop computer;facial recognition system;imperative programming;level of detail;network architecture;open-source software;php;python;rectifier (neural networks);software development kit;whole earth 'lectronic link	Xin Liu;Meina Kan;Wanglong Wu;Shiguang Shan;Xilin Chen	2016	Frontiers of Computer Science	10.1007/s11704-016-6076-3	computer vision;speech recognition;computer science;machine learning	Vision	23.972680776340773	-53.009692412298435	59080
3a6a53d9b390dbf19f8a5eafc864947c52d302c2	accurate prediction of contact numbers for multi-spanning helical membrane proteins		Prediction of the three-dimensional (3D) structures of proteins by computational methods is acknowledged as an unsolved problem. Accurate prediction of important structural characteristics such as contact number is expected to accelerate the otherwise slow progress being made in the prediction of 3D structure of proteins. Here, we present a dropout neural network-based method, TMH-Expo, for predicting the contact number of transmembrane helix (TMH) residues from sequence. Neuronal dropout is a strategy where certain neurons of the network are excluded from back-propagation to prevent co-adaptation of hidden-layer neurons. By using neuronal dropout, overfitting was significantly reduced and performance was noticeably improved. For multi-spanning helical membrane proteins, TMH-Expo achieved a remarkable Pearson correlation coefficient of 0.69 between predicted and experimental values and a mean absolute error of only 1.68. In addition, among those membrane protein-membrane protein interface residues, 76.8% were correctly predicted. Mapping of predicted contact numbers onto structures indicates that contact numbers predicted by TMH-Expo reflect the exposure patterns of TMHs and reveal membrane protein-membrane protein interfaces, reinforcing the potential of predicted contact numbers to be used as restraints for 3D structure prediction and protein-protein docking. TMH-Expo can be accessed via a Web server at www.meilerlab.org .	acclimatization;approximation error;artificial neural network;backpropagation;biological neural networks;boat dock;coefficient;computation;docking (molecular);dropout (neural networks);exclusion;file spanning;helix dna client for android;herlitz disease;macromolecular docking;membrane proteins;neuronal ceroid-lipofuscinoses;numerous;overfitting;server (computer);server (computing);software propagation;web server	Bian Li;Jeffrey L. Mendenhall;Elizabeth Dong Nguyen;Brian E. Weiner;Axel W. Fischer;Jens Meiler	2016	Journal of chemical information and modeling	10.1021/acs.jcim.5b00517	chemistry;bioinformatics;artificial intelligence;machine learning	Comp.	10.5586083915867	-62.8376788907549	59428
c32b66a9badc57e416cf15287c9c8861b264805b	towards closing the gap in weakly supervised semantic segmentation with dcnns: combining local and global models.		Generating training sets for deep convolutional neural networks is a bottleneck for modern real-world applications. This is a demanding tasks for applications where annotating training data is costly, such as in semantic segmentation. In the literature, there is still a gap between the performance achieved by a network trained on full and on weak annotations. In this paper, we establish a strategy to measure this gap and to identify the ingredients necessary to close it. rnOn scribbles, we establish state-of-the-art results: we obtain a gap in mIoU of 2.4% without CRF, and 2.9% with CRF post-processing. Thanks to a systematic study of the different ingredients involved in the weakly supervised scenario and an original experimental strategy, we unravel a counter-intuitive mechanism that is simple and amenable to generalisations to other weakly-supervised scenarios: averaging local predicted annotations with the baseline ones and reuse them for training a DCNN yields new state-of-the-art results. rnFinally, closing the gap was reported only recently for bounding boxes in Khoreva et al. (arXiv:1603.07485v2), by requiring 10x more training images. By simulating varying amounts of pixel-level annotations respecting scribble human annotations statistics, we show that our training strategy reacts to small increases in the amount of annotations and requires only 2-5x more annotated pixels, closing the gap with only 3.1% of all pixels annotated. This work contributes new ideas towards closing the gap in real-world applications.		Christoph Mayer;Radu Timofte;Grégory Paul	2018	CoRR		machine learning;pattern recognition;convolutional neural network;artificial intelligence;reuse;training set;computer science;bottleneck;scribble;segmentation	Web+IR	22.324966136540098	-52.22642575019923	59544
42e12a48eacb102e5713e57141eec1e122af9688	computer graphic study on models of the molybdenum cofactor of xanthine oxidase	molecular modeling;x ray crystallography;three dimensional;model complexity;computer graphic;xanthine oxidase;active site;nitrogen	Within the scope of our molecular modeling studies on xanthine oxidase (XOD) inhibition by purine analogs we were interested to build up a three-dimensional model of the molybdenum active site. Spectroscopic data indicated that a Mo (VI)atom which is coordinated to sulfur, oxygen and/or nitrogen is clearly involved in substrate binding. In the present study, those data and X-ray crystallography data were used to reconstruct molybdenum-organic complexes from models proposed in the literature. The computer graphic-assisted modeling and evaluation of the model complexes show that the description of the molybdenum center needs further refinement.		Gerd Folkers;Michael Krug;Susanne Trumpp-Kallmeyer	1987	Journal of computer-aided molecular design	10.1007/BF01680559	crystallography;three-dimensional space;biochemistry;chemistry;active site;organic chemistry;molecular model;inorganic chemistry;nitrogen;nuclear magnetic resonance;x-ray crystallography	EDA	13.041112227758862	-60.83541493165486	59554
6f51f59f332cb3ee18fd1955b54d733cf94ee206	parameter refinement for molecular docking	molecular docking	Finding the optimal parameter values for any computer program with adjustable parameters can be very time consuming. In this paper, we introduce the use of the Plackett−Burman and the central composite designs with the aid of the partial least squares method to tackle this problem. Using DOCK3.5 as a test case, we also show a four-step procedure for sequential docking utilizing two parameter sets, both effecting a different level of accuracy. The DOCK parameter values were refined for protein kinase C regulatory domain yielding an orientation at the global “energy” minimum, which is in very good agreement with the experimental protein kinase C regulatory domain−phorbol 13-acetate complex. The scheme is now being used for screening molecular databases to find putative protein kinase C inhibitors.	docking (molecular)	Jukka-Pekka Salo;Ari Yliniemelä;Jyrki Taskinen	1998	Journal of Chemical Information and Computer Sciences	10.1021/ci9801825	chemistry;docking;lead finder;searching the conformational space for docking;computer science;docking;protein–ligand docking	Theory	11.525413075303701	-60.2422435550758	60086
410d107f432a16844dbc86578890d44d6668d445	using multi-scale glide zoom window feature extraction approach to predict protein homo-oligomer types	homo oligomer;amino acid;protein sequence;pseudo amino acid composition;amino acid composition;feature vector;protein structure;multi scale glide zoom window;feature extraction;support vector machine;pseudo amino acid compositions	The concept of multi-scale glide zoom window was proposed and a novel approach of multi-scale glide zoom window feature extraction was used for predicting protein homo-oligomers. Based on the concept of multi-scale glide zoom window, we choose two scale glide zoom window: whole protein sequence glide zoom window and kin amino acid glide zoom window, and for every scale glide zoom window, three feature vectors of amino acids distance sum, amino acids mean distance and amino acids distribution, were extracted. A series of feature sets were constructed by combining these feature vectors with amino acids composition to form pseudo amino acid compositions (PseAAC). The support vector machine (SVM) was used as base classifier. The 75.37% total accuracy is arrived in jackknife test in the weighted factor conditions, which is 10.05% higher than that of conventional amino acid composition method in same condition. The results show that multi-scale glide zoom window method of extracting feature vectors from protein sequence is effective and feasible, and the feature vectors of multi-scale glide zoom window may contain more protein structure information.	feature extraction;glide	Qi-Peng Li;Shao-Wu Zhang;Quan Pan	2008		10.1007/978-3-540-88436-1_7	biology;support vector machine;computer vision;protein structure;amino acid;feature vector;feature extraction;computer science;machine learning;protein sequencing;pattern recognition	NLP	11.076050821368698	-54.864369527073976	60087
ba76d69f2b32388e08ad4c9a73c028a7f4f735c7	neural techniques and postal code detection	image segmentation;propagation arriere;bacpropagation;segmentation;backpropagation;segmentation image;pattern recognition;reseau neuronal recurrent;postal code detection;reconnaissance forme;recurrent neural networks;recurrent neural network;reseau neuronal;reconocimiento patron;detection code postal;red neuronal;segmentacion;neural network	Abstract   Feedforward neural networks trained with backpropagation can be used for pattern recognition. Generalization properties are illustrated with simple examples. Cascade networks and recurrent neural networks (RNN) are discussed. Postal code selection (image segmentation) is considered as a recognition problem. The postal code line is detected with a multiresolution observation. Components of the Dutch postal code (4 digits, 2 letters) are detected using a RNN with positive and negative time delays.	postal	W. P. de Waard	1994	Pattern Recognition Letters	10.1016/0167-8655(94)90049-3	speech recognition;computer science;artificial intelligence;recurrent neural network;machine learning;artificial neural network	Vision	23.762482163557763	-61.908253673256276	60128
beaabc761203158499fbf18055e78b45a1ce54e4	a new neural network for b-turn prediction: the effect of site-specific amino acid preference	neural network model;structural bioinformatics;machine learning;neural network;network performance;amino acid	The prediction of β-turn, despite the observation that one out of four residues in protein belongs to this structure element, has attracted considerably less attention comparing to secondary structure predictions. Neural network machine learning is a popular approach to address such a problem of structural bioinformatics. In this paper, we describe a new neural network model for β-turn prediction that accounts for site-specific amino acid preference, a property ignored in previous training models. We showed that the statistics of amino acid preference at specific sites within and around a β-turn is rather significant, and incorporation of this property helps improve the network performance. Furthermore, by contrasting with a previous model, we revealed a deficiency of not incorporating this site-specific property in previous models.	angular defect;artificial neural network;machine learning;network model;network performance;structural bioinformatics	Zhong-Ru Xie;Ming-Jing Hwang	2006			machine learning;protein secondary structure;artificial intelligence;amino acid;structural bioinformatics;artificial neural network;pattern recognition;network performance;biology	ML	10.461704489876992	-55.891728123465604	60252
11ae0d2199dca98f4ddfdff724bf2b14bdcb6278	modeling multiple experiments using regularized optimization: a case study on bacterial glucose utilization dynamics	regularization;particle swarm optimization;identification;biochemical systems;optimization;bacterial metabolism;modeling	The aim of inverse modeling is to capture the systems׳ dynamics through a set of parameterized Ordinary Differential Equations (ODEs). Parameters are often required to fit multiple repeated measurements or different experimental conditions. This typically leads to a multi-objective optimization problem that can be formulated as a non-convex optimization problem. Modeling of glucose utilization of Lactococcus lactis bacteria is considered using in vivo Nuclear Magnetic Resonance (NMR) measurements in perturbation experiments. We propose an ODE model based on a modified time-varying exponential decay that is flexible enough to model several different experimental conditions. The starting point is an over-parameterized non-linear model that will be further simplified through an optimization procedure with regularization penalties. For the parameter estimation, a stochastic global optimization method, particle swarm optimization (PSO) is used. A regularization is introduced to the identification, imposing that parameters should be the same across several experiments in order to identify a general model. On the remaining parameter that varies across the experiments a function is fit in order to be able to predict new experiments for any initial condition. The method is cross-validated by fitting the model to two experiments and validating the third one. Finally, the proposed model is integrated with existing models of glycolysis in order to reconstruct the remaining metabolites. The method was found useful as a general procedure to reduce the number of parameters of unidentifiable and over-parameterized models, thus supporting feature selection methods for parametric models.	convex optimization;differential diagnosis;estimation theory;experiment;feature selection;global optimization;glucose;glycolysis;identification (psychology);initial condition;linear model;magnetic resonance imaging;mathematical optimization;metabolite;multi-objective optimization;nonlinear system;numerous;optimization problem;particle swarm optimization;population parameter;system dynamics;video-in video-out;exponential;modeling	András Hartmann;João Miranda Lemos;Susana Vinga	2015	Computers in biology and medicine	10.1016/j.compbiomed.2014.08.027	identification;regularization;econometrics;mathematical optimization;multi-swarm optimization;systems modeling;computer science;machine learning;mathematics;microbial metabolism;particle swarm optimization;statistics	ML	13.292079376659377	-53.222472288785994	60347
bde0e10d3887be3552afc2c9126c9927984a4fae	a mathematical formula for subjective expression of visual patterns		Abstract#R##N##R##N#In actual image recognition, one does not always view the whole image, but often subjectively selects a certain subregion on the image to obtain some useful information. This paper clarifies theoretically the reason why such an ability possessed by humans is naturally carried out, by using a mathematical analysis regarding the fundamental properties of a visual pattern. A mathematical formula showing multilayer structures of a visual pattern is obtained by the foregoing analysis. The results obtained in this paper may contribute to elucidating the aspect of the basic process of the feature extraction from the pattern.		Taizo Iijima	1988	Systems and Computers in Japan	10.1002/scj.4690190707	computer vision;artificial intelligence;machine learning;mathematics;algorithm	NLP	18.180855152432358	-57.469848253072236	60562
79b19e4d98bb385ce96a720f5888eab7e754cc4b	neuronal nicotinic receptor agonists: a multi-approach development of the pharmacophore	hydrogen bond;nicotinic acetylcholine receptor;nicotinic receptor;molecular interactions;nitrogen	Based on the results obtained with different automated computational approaches as applied to the study of eleven high-affinity agonists of the neuronal nicotine acetylcholine receptor (nAChR), belonging to different chemical classes, new relevant features were detected which complement the existing pharmacophores. Convergent results from DISCO (Distance Comparison), QXP (Quick Explore), Catalyst/HipHop, and MIPSIM (Molecular Interaction Potential Similarity) allowed us to identify and locate, in a well defined spatial arrangement, three geometrically independent key structural features: (i) a positively charged nitrogen atom for ionic or hydrogen bond interactions, (ii) a lone pair of the pyridine nitrogen or a specific lone pair of a carbonyl oxygen, as a hydrogen bond acceptor, and (iii) a centre of a hydrophobic area generally occupied by aliphatic cycles. The pharmacophore presented herein, along with predictive 2D and 3D QSAR models recently developed in our group, could represent valuable computational tools for the design of new nAChR agonists having therapeutical potential.	acceptor (semiconductors);acetylcholine;binding (molecular function);blood urea nitrogen measurement;cholinergic receptors;class;complement system proteins;computation;distance;glandularia araucana;hoc (programming language);hydrogen bonding;interaction;interactome;ionic;ligands;linear algebra;medicinal chemistry;nicotine;nicotinic agonists;nicotinic receptors;numerous;oxygen;pharmacophore;processor affinity;pyridines;quantitative structure-activity relationship;quantitative structure–activity relationship;rationalization	Orazio Nicolotti;Marialuisa Pellegrini-Calace;Antonio Carrieri;Cosimo Altomare;Nuria B. Centeno;Ferran Sanz;Andrea Carotti	2001	Journal of computer-aided molecular design	10.1023/A:1013115717587	biochemistry;stereochemistry;chemistry;organic chemistry;nitrogen;hydrogen bond	Comp.	10.66855298086788	-60.16041174970245	60668
d01f5f3d1971e7906d58dcc8baf9efb406c47fd2	real-valued (medical) time series generation with recurrent conditional gans		Generative Adversarial Networks (GANs) have shown remarkable success as a framework for training models to produce realistic-looking data. In this work, we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data. RGANs make use of recurrent neural networks (RNNs) in the generator and the discriminator. In the case of RCGANs, both of these RNNs are conditioned on auxiliary information. We demonstrate our models in a set of toy datasets, where we show visually and quantitatively (using sample likelihood and maximum mean discrepancy) that they can successfully generate realistic time-series. We also describe novel evaluation methods for GANs, where we generate a synthetic labelled training dataset, and evaluate on a real test set the performance of a model trained on the synthetic data, and vice-versa. We illustrate with these metrics that RCGANs can generate time-series data useful for supervised training, with only minor degradation in performance on real test data. This is demonstrated on digit classification from ‘serialised’ MNIST and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit. We further discuss and analyse the privacy concerns that may arise when using RCGANs to generate realistic synthetic medical time series data, and demonstrate results from differentially private training of the RCGAN.	artificial neural network;data breach;differential privacy;discrepancy function;discriminator;elegant degradation;generative adversarial networks;international components for unicode;mnist database;recurrent neural network;refinement (computing);supervised learning;synthetic data;test data;test set;time series;usability	Cristóbal Esteban;Stephanie L. Hyland;Gunnar Rätsch	2017	CoRR		time series;artificial intelligence;machine learning;test data;discriminator;recurrent neural network;synthetic data;mnist database;test set;computer science	ML	20.01137974467243	-53.597442656655296	60699
114269e94e766a0c587ca37490c1760660c2dae1	corba: gone but (hopefully) not forgotten	service system;service;system;distributed	A novel method for growing pigs by which excess intake of nutritions is prevented while attaining sufficient growth, as well as a feed used therefor, is disclosed. In the method for growing pigs according to the present invention, a first diet having an energy content in terms of TDN of not less than 72%, a crude protein content of 8% by weight to 23% by weight, and a lysine content of 0.3% by weight to 1.5% by weight, and a second diet having an energy content in terms of TDN of not less than 72%, a crude protein content of 13% by weight to 28% by weight, and a lysine content of 1.0% by weight to 2.0% by weight are fed such that the pigs can access to either of said first and second diets ad libitum. The crude protein content in the second diet is higher than that of the first diet and the difference therebetween is not less than 5% by weight, and the lysine content in the second diet is higher than that of the first diet and the difference therebetween is not less than 0.5% by weight. The first and second diets contains 30-90 parts by weight of isoleucine, 31-91 parts by weight of total of methionine and cysteine, 33-98 parts by weight of threonine, and 10-29 parts by weight of tryptophan per 100 parts by weight of lysine.	common object request broker architecture	Terry Coatta	2007	ACM Queue	10.1145/1255421.1388786	service;computer science;system;service system	OS	12.383409871909263	-63.400406857700546	61283
75e1af4fa819b823d6ad5627178254d2f2fbca0a	distinguishing feature selection for fabric defect classification using neural network	machine vision;automated fabric inspection;backpropagation algorithm;feature selection;defect detection;neural network;defect classification	Over the years significant research has been performed for machine vision based fabric inspectio n systems in order to replace manual inspection, whic h is time consuming and not accurate enough. Automated fabric inspection systems mainly involve two challe nging problems: one is defect detection and another is classification, which remains elusive despite consi derable research effort in automated fabric inspection. The research reported to date to solve the defect class ification problem appears to be insufficient, particularly in selecting appropriate set of features. Scene analy sis and feature selection play a very important role in the classification process. Insufficient scene analysis results in an inappropriate set of features. Selection of an inappropriate feature set increases complexities of subsequent steps and makes the classification task harder. Considering this observation, we present a possibly appropriate feature set in order to address the pro blem of fabric defect classification using neural network ( NN). We justify the features from the point of view of distinguishing quality and feature extraction difficulty. W e performed some experiments in order to show the utility of pr oposed features and compare performances with recently reported relevant works. More than 98% classificati on accuracy has been found, which appears to be very promising.	artificial neural network;experiment;feature extraction;feature selection;machine vision;performance;software bug	Md. Tarek Habib;M. Rokonuzzaman	2011	Journal of Multimedia	10.4304/jmm.6.5.416-424	computer vision;machine vision;computer science;backpropagation;machine learning;pattern recognition;feature selection;feature;artificial neural network	NLP	23.46100844472563	-58.08730365477647	61431
692ff644f06e1451510b97ede94a4c15e3960674	incremental learning by message passing in hierarchical temporal memory	deep architectures;backpropagation;incremental learning;htm	Hierarchical Temporal Memory is a biologically-inspired framework that can be used to learn invariant representations of patterns. Classical HTM learning is mainly unsupervised and once training is completed the network structure is frozen, thus making further training quite critical. In this paper we develop a novel technique for HTM (incremental) supervised learning based on error minimization. We prove that error backpropagation can be naturally and elegantly implemented through native HTM message passing based on Belief Propagation. Our experimental results show that a two stage training composed by unsupervised pre-training + supervised refinement is very effective. This is in line with recent findings on other deep architectures.	hierarchical temporal memory;message passing	Davide Maltoni;Erik M. Rehn	2012		10.1007/978-3-642-33212-8_3	computer science;artificial intelligence;backpropagation;theoretical computer science;machine learning;hierarchical temporal memory	Vision	21.88420702396918	-54.224658754749534	61448
192c718f83d9dc5f53503e6e92d68f7546524096	top-down visual attention estimation using spatially localized activation based on linear separability of visual features		Intelligent information systems captivate people’s attention. Examples of such systems include driving support vehicles capable of sensing driver state and communication robots capable of interacting with humans. Modeling how people search visual information is indispensable for designing these kinds of systems. In this paper, we focus on human visual attention, which is closely related to visual search behavior. We propose a computational model to estimate human visual attention while carrying out a visual target search task. Existing models estimate visual attention using the ratio between a representative value of visual feature of a target stimulus and that of distractors or background. The models, however, can not often achieve a better performance for difficult search tasks that require a sequentially spotlighting process. For such tasks, the linear separability effect of a visual feature distribution should be considered. Hence, we introduce this effect to spatially localized activation. Concretely, our top-down model estimates target-specific visual attention using Fisher’s variance ratio between a visual feature distribution of a local region in the field of view and that of a target stimulus. We confirm the effectiveness of our computational model through a visual search experiment. key words: human visual attention, visual search, saliency map, activation map, linear separability	computation;computational model;information system;interaction;linear separability;robot;top-down and bottom-up design;web search engine	Takatsugu Hirayama;Toshiya Ohira;Kenji Mase	2015	IEICE Transactions		computer vision;visual search;computer science;machine learning;mathematics;linear separability;human visual system model;biased competition theory	HCI	22.825232354713968	-65.38936142652094	61526
b9c60a317b1bfd68d808cfc5df22710c247d7e87	recognising complex patterns through a distributed multi-feature approach	silicon;distributed algorithms;complexity theory;pattern recognition accuracy silicon complexity theory feature extraction scalability training;training;distributed computing;single cycle learning distributed pattern recognition algorithm complex pattern recognition distributed multifeature approach multiple feature implementation holistic approach pattern recognition procedure bias effect multifeature pattern recognition techniques distributed computational networks distributed recognition network;holistic approach;accuracy;feature extraction;distribution pattern;pattern recognition;scalability;pattern recognition distributed algorithms	Multiple-feature implementation enables a holistic approach towards pattern recognition procedure that takes into consideration all significant features, which represent a particular set of complex patterns, such as images and sensor readings. This intends to reduce the bias effect of selecting only a single feature for classification/recognition purposes. In this paper we demonstrate the effectiveness of this approach in comparison with some well-known multi-feature pattern recognition techniques. Our approach benefits from having a set of distributed computational networks working together, forming a distributed recognition network that alleviates the issue of scalability against increasing number of features to be considered. In addition, the use of our proposed single-cycle learning distributed pattern recognition algorithm shows a significant reduction in training samples to achieve comparable accuracy.	algorithm;artificial neural network;computation;curse of dimensionality;feature recognition;holism;optical character recognition;pattern recognition;pixel;scalability	Anang Hudaya Muhamad Amin;Asad I. Khan	2011	2011 11th International Conference on Hybrid Intelligent Systems (HIS)	10.1109/HIS.2011.6122139	computer vision;feature;feature extraction;computer science;machine learning;pattern recognition;3d single-object recognition	Vision	22.452693734325358	-54.20744038290442	61798
93d9b96e7474589b9abc8550bc404e07701bdd36	evaluating evolutionary multiobjective algorithms for the in silico optimization of mutant strains	evolutionary multiobjective algorithms;genetic engineering;genomics;optimisation;evolutionary computation;succinic acid evolutionary multiobjective algorithms in silico optimization mutant strains metabolic engineering genetic manipulations e coli;flux balance analysis;metabolic engineering;genetic manipulations;systems biology;systems biology multiobjective evolutionary algorithms metabolic engineering flux balance analysis;e coli;mutant strains;genetics;multiobjective evolutionary algorithm;optimization problem;capacitive sensors production evolutionary computation organisms biomass genetic engineering algorithm design and analysis genomics bioinformatics simulated annealing;system biology;optimisation bioinformatics evolutionary computation genetic engineering genetics genomics;production;succinic acid;multiobjective optimization;optimization;evolutionary algorithm;microorganisms;algorithm design and analysis;biochemistry;multiobjective evolutionary algorithms;in silico optimization;bioinformatics;in silico	In Metabolic Engineering, the identification of genetic manipulations that lead to mutant strains able to produce a given compound of interest is a promising, while still complex process. Evolutionary Algorithms (EAs) have been a successful approach for tackling the underlying in silico optimization problems. The most common task is to solve a bi-level optimization problem, where the strain that maximizes the production of some compound is sought, while trying to keep the organism viable (maximizing biomass). In this work, this task is viewed as a multiobjective optimization problem and an approach based on multiobjective EAs is proposed. The algorithms are validated with a real world case study that uses E. coli to produce succinic acid. The results obtained are quite promising when compared to the available single objective algorithms.	black and burst;evolutionary algorithm;experiment;genetic algorithm;mathematical optimization;multi-objective optimization;optimization problem;pareto efficiency	Paulo Maia;Isabel Rocha;Eugénio C. Ferreira;Miguel Rocha	2008	2008 8th IEEE International Conference on BioInformatics and BioEngineering	10.1109/BIBE.2008.4696733	genetic engineering;optimization problem;biology;algorithm design;biotechnology;computer science;bioinformatics;multi-objective optimization;flux balance analysis;microorganism;systems biology	Robotics	11.903796447725592	-53.04317775899141	62108
a8985256da7adcb0fa77e47566066d127aba3c5e	regression applied to protein binding site prediction and comparison with classification	institutional repositories;protein function;fedora;binding site;binding sites;multilayer perceptron;classification;vital;computational biology bioinformatics;protein structure;proteins;machine learning;structural genomics;protein binding;algorithms;regression analysis;combinatorial libraries;vtls;false positive;protein interaction mapping;computational biology;computer appl in life sciences;leave one out cross validation;ils;physical interaction;databases protein;microarrays;bioinformatics	The structural genomics centers provide hundreds of protein structures of unknown function. Therefore, developing methods enabling the determination of a protein function automatically is imperative. The determination of a protein function can be achieved by studying the network of its physical interactions. In this context, identifying a potential binding site between proteins is of primary interest. In the literature, methods for predicting a potential binding site location generally are based on classification tools. The aim of this paper is to show that regression tools are more efficient than classification tools for patches based binding site predictors. For this purpose, we developed a patches based binding site localization method usable with either regression or classification tools. We compared predictive performances of regression tools with performances of machine learning classifiers. Using leave-one-out cross-validation, we showed that regression tools provide better predictions than classification ones. Among regression tools, Multilayer Perceptron ranked highest in the quality of predictions. We compared also the predictive performance of our patches based method using Multilayer Perceptron with the performance of three other methods usable through a web server. Our method performed similarly to the other methods. Regression is more efficient than classification when applied to our binding site localization method. When it is possible, using regression instead of classification for other existing binding site predictors will probably improve results. Furthermore, the method presented in this work is flexible because the size of the predicted binding site is adjustable. This adaptability is useful when either false positive or negative rates have to be limited.	binding sites;cross reactions;cross-validation (statistics);dna binding site;fundamental interaction;genomics;imperative programming;machine learning;multilayer perceptron;performance;server (computing);staphylococcal protein a;web server	Joachim Giard;Jérôme Ambroise;Jean-Luc Gala;Benoit M. Macq	2009	BMC Bioinformatics	10.1186/1471-2105-10-276	biology;computer science;bioinformatics;binding site;machine learning;data mining	Comp.	10.13924416473947	-56.34060681467479	62145
8e27c73d1a861d726d513513e8f47a11513bcff4	high-order neural networks and kernel methods for peptide-mhc binding prediction		MOTIVATION Effective computational methods for peptide-protein binding prediction can greatly help clinical peptide vaccine search and design. However, previous computational methods fail to capture key nonlinear high-order dependencies between different amino acid positions. As a result, they often produce low-quality rankings of strong binding peptides. To solve this problem, we propose nonlinear high-order machine learning methods including high-order neural networks (HONNs) with possible deep extensions and high-order kernel support vector machines to predict major histocompatibility complex-peptide binding.   RESULTS The proposed high-order methods improve quality of binding predictions over other prediction methods. With the proposed methods, a significant gain of up to 25-40% is observed on the benchmark and reference peptide datasets and tasks. In addition, for the first time, our experiments show that pre-training with high-order semi-restricted Boltzmann machines significantly improves the performance of feed-forward HONNs. Moreover, our experiments show that the proposed shallow HONN outperform the popular pre-trained deep neural network on most tasks, which demonstrates the effectiveness of modelling high-order feature interactions for predicting major histocompatibility complex-peptide binding.   AVAILABILITY AND IMPLEMENTATION There is no associated distributable software.   CONTACT renqiang@nec-labs.com or mark.gerstein@yale.edu   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	amino acids;artificial neural network;benchmark (computing);bioinformatics;biological neural networks;carcinoma, neuroendocrine;deep learning;experiment;interaction;kernel (operating system);kernel method;machine learning;model of hierarchical complexity;neural network simulation;nonlinear system;protein binding;semiconductor industry;support vector machine;vaccines, peptide	Pavel P. Kuksa;Martin Renqiang Min;Rishabh Dugar;Mark Gerstein	2015	Bioinformatics	10.1093/bioinformatics/btv371	machine learning	ML	10.602853930760803	-55.19821883630318	62472
866aee7bc3ba13394a84ed36fb8f8ba543552d5e	unsupervised network pretraining via encoding human design	microprocessors;histograms;neural nets computer vision feature extraction;neural networks;training;feature extraction histograms neural networks visualization microprocessors computer architecture training;computer architecture;visualization;feature extraction;region covariance feature extraction unsupervised network pretraining encoding human design computer vision researchers image features visual object recognition task deep neural networks hand designed feature extraction object specific representations labeled data classification power convolutional neural networks	Over the years, computer vision researchers have spent an immense amount of effort on designing image features for the visual object recognition task. We propose to incorporate this valuable experience to guide the task of training deep neural networks. Our idea is to pretrain the network through the task of replicating the process of hand-designed feature extraction. By learning to replicate the process, the neural network integrates previous research knowledge and learns to model visual objects in a way similar to the hand-designed features. In the succeeding finetuning step, it further learns object-specific representations from labeled data and this boosts its classification power. We pretrain two convolutional neural networks where one replicates the process of histogram of oriented gradients feature extraction, and the other replicates the process of region covariance feature extraction. After finetuning, we achieve substantially better performance than the baseline methods.	artificial neural network;baseline (configuration management);computer vision;convolutional neural network;deep learning;feature extraction;gradient;histogram of oriented gradients;outline of object recognition;pedestrian detection;self-replication;visual objects	Ming-Yu Liu;Arun Mallya;Oncel Tuzel;Xi Chen	2016	2016 IEEE Winter Conference on Applications of Computer Vision (WACV)	10.1109/WACV.2016.7477698	computer vision;visualization;feature extraction;computer science;machine learning;pattern recognition;time delay neural network;histogram;deep learning;feature;artificial neural network	Vision	23.212325872266312	-54.142048401415686	62852
927c1d082fe5a50e39a74e2efdee9b65344db61e	application of asymmetric networks to movement detection and generating independent subspaces		The prominent feature is the nonlinear characteristics as the squaring and rectification functions, which are observed in the retinal and visual cortex networks. Conventional model for motion processing in cortex, uses a symmetric quadratic functions with Gabor filters. This paper proposes a new motion processing model in the asymmetric networks. First, the asymmetric network is analyzed using Wiener kernels. It is shown that the asymmetric network with nonlinearities is effective and general for generating the directional movement compared with the conventional quadratic model. Second, independence maximization of data is an important issue in computational neural networks. To make clear the characteristics of the asymmetric network with Gabor functions, orthogonality is computed, which shows independent characteristics of the asymmetric network without maximizing optimization of independence in the quadratic model. The orthogonal analyses for the independence of the asymmetric networks are applied to the V1 and MT neural networks to generate independent subspaces by using selective Gabor functions.		Naohiro Ishii;Toshinori Deguchi;Masashi Kawaguchi;Hiroshi Sasaki	2017		10.1007/978-3-319-65172-9_23	linear subspace;mathematical optimization;rectification;quadratic function;theoretical computer science;nonlinear system;quadratic equation;artificial neural network;mathematics;visual cortex;orthogonality	ML	23.44829333139763	-61.58681142110534	62927
ae9d452670de822450eb2323b4918099b8ad3fd8	eigen-distortions of hierarchical representations		We develop a method for comparing hierarchical image representations in terms of their ability to explain perceptual sensitivity in humans. Specifically, we utilize Fisher information to establish a model-derived prediction of sensitivity to local perturbations of an image. For a given image, we compute the eigenvectors of the Fisher information matrix with largest and smallest eigenvalues, corresponding to the model-predicted mostand least-noticeable image distortions, respectively. For human subjects, we then measure the amount of each distortion that can be reliably detected when added to the image. We use this method to test the ability of a variety of representations to mimic human perceptual sensitivity. We find that the early layers of VGG16, a deep neural network optimized for object recognition, provide a better match to human perception than later layers, and a better match than a 4-stage convolutional neural network (CNN) trained on a database of human ratings of distorted image quality. On the other hand, we find that simple models of early visual processing, incorporating one or more stages of local gain control, trained on the same database of distortion ratings, provide substantially better predictions of human sensitivity than either the CNN, or any combination of layers of VGG16. Human capabilities for recognizing complex visual patterns are believed to arise through a cascade of transformations, implemented by neurons in successive stages in the visual system. Several recent studies have suggested that representations of deep convolutional neural networks trained for object recognition can predict activity in areas of the primate ventral visual stream better than models constructed explicitly for that purpose (Yamins et al. [2014], Khaligh-Razavi and Kriegeskorte [2014]). These results have inspired exploration of deep networks trained on object recognition as models of human perception, explicitly employing their representations as perceptual distortion metrics or loss functions (Hénaff and Simoncelli [2016], Johnson et al. [2016], Dosovitskiy and Brox [2016]). On the other hand, several other studies have used synthesis techniques to generate images that indicate a profound mismatch between the sensitivity of these networks and that of human observers. Specifically, Szegedy et al. [2013] constructed image distortions, imperceptible to humans, that cause their networks to grossly misclassify objects. Similarly, Nguyen and Clune [2015] optimized randomly initialized images to achieve reliable recognition by a network, but found that the resulting ∗Currently at Google, Inc. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. ‘fooling images’ were uninterpretable by human viewers. Simpler networks, designed for texture classification and constrained to mimic the early visual system, do not exhibit such failures (Portilla and Simoncelli [2000]). These results have prompted efforts to understand why generalization failures of this type are so consistent across deep network architectures, and to develop more robust training methods to defend networks against attacks designed to exploit these weaknesses (Goodfellow et al. [2014]). From the perspective of modeling human perception, these synthesis failures suggest that representational spaces within deep neural networks deviate significantly from those of humans, and that methods for comparing representational similarity, based on fixed object classes and discrete sampling of the representational space, are insufficient to expose these deviations. If we are going to use such networks as models for human perception, we need better methods of comparing model representations to human vision. Recent work has taken the first step in this direction, by analyzing deep networks’ robustness to visual distortions on classification tasks, as well as the similarity of classification errors that humans and deep networks make in the presence of the same kind of distortion (Dodge and Karam [2017]). Here, we aim to accomplish something in the same spirit, but rather than testing on a set of handselected examples, we develop a model-constrained synthesis method for generating targeted test stimuli that can be used to compare the layer-wise representational sensitivity of a model to human perceptual sensitivity. Utilizing Fisher information, we isolate the model-predicted most and least noticeable changes to an image. We test these predictions by determining how well human observers can discriminate these same changes. We apply this method to six layers of VGG16 (Simonyan and Zisserman [2015]), a deep convolutional neural network (CNN) trained to classify objects. We also apply the method to several models explicitly trained to predict human sensitivity to image distortions, including both a 4-stage generic CNN, an optimally-weighted version of VGG16, and a family of highly-structured models explicitly constructed to mimic the physiology of the early human visual system. Example images from the paper, as well as additional examples, are available at http://www.cns.nyu.edu/~lcv/eigendistortions/. 1 Predicting discrimination thresholds Suppose we have a model for human visual representation, defined by conditional density p(~r|~x), where ~x is an N -dimensional vector containing the image pixels, and ~r is an M -dimensional random vector representing responses internal to the visual system (e.g., firing rates of a population of neurons). If the image is modified by the addition of a distortion vector, ~x+ αû, where û is a unit vector, and scalar α controls the amplitude of distortion, the model can be used to predict the threshold at which the distorted image can be reliably distinguished from the original image. Specifically, one can express a lower bound on the discrimination threshold in direction û for any observer or model that bases its judgments on ~r (Seriès et al. [2009]): T (û; ~x) ≥ β √ ûTJ−1[~x]û (1) where β is a scale factor that depends on the noise amplitude of the internal representation (as well as experimental conditions, when measuring discrimination thresholds of human observers), and J [~x] is the Fisher information matrix (FIM; Fisher [1925]), a second-order expansion of the log likelihood: J [~x] = E~r|~x [( ∂ ∂~x log p(~r|~x) )( ∂ ∂~x log p(~r|~x) )T] (2) Here, we restrict ourselves to models that can be expressed as a deterministic (and differentiable) mapping from the input pixels to mean output response vector, f(~x), with additive white Gaussian noise in the response space. The log likelihood in this case reduces to a quadratic form: log p(~r|~x) = − 2 ( [~r − f(~x)] [~r − f(~x)] ) + const. Substituting this into Eq. (2) gives: J [~x] = ∂f ∂~x T ∂f ∂~x Thus, for these models, the Fisher information matrix induces a locally adaptive Euclidean metric on the space of images, as specified by the Jacobian matrix, ∂f/∂~x.	additive white gaussian noise;artificial neural network;const (computer programming);convolutional neural network;deep learning;distortion;eigen (c++ library);euclidean distance;feature learning;fisher information;formation matrix;image quality;information processing;jacobian matrix and determinant;loss function;nips;outline of object recognition;pixel;randomness;sampling (signal processing);utility functions on indivisible goods	Alexander Berardino;Johannes Ballé;Valero Laparra;Eero P. Simoncelli	2017			computer science;convolutional neural network;visual processing;fisher information;machine learning;artificial intelligence;automatic gain control;pattern recognition;artificial neural network;image quality;eigenvalues and eigenvectors;distortion	ML	20.47880269221409	-52.133498693396874	63277
df311975ad76a272cf0ff0749bb7c12e741a6f33	object classification with simple visual attention and a hierarchical neural network for subsymbolic-symbolic coupling	symbol manipulation;object recognition;hierarchical neural networks;learning artificial intelligence computer vision object recognition image classification feature extraction neural nets symbol manipulation vector quantisation image colour analysis;performance evaluation;neural nets;object categorization;data processing;image classification;computer vision;neural networks prototypes histograms information processing feature extraction remotely operated vehicles mobile robots hospitals color data processing;image colour analysis;feature extraction;object classification;learning artificial intelligence;classification accuracy;vector quantisation;visual attention;vector quantisation object classification hierarchical neural network visual attention image color hierarchical classifier symbolic data processing supervised learning object categorization feature extraction confidence values	An object classification system using a simple color based visual attention method, and a prototype based hierarchical classifier is established as a link between sub-symbolic and symbolic data processing. During learning the classifier generates a hierarchy of prototypes. These prototypes constitute a taxonomy of objects. By assigning confidence values to the prototypes a classification request may also return symbols with confidence values. For performance evaluation the classifier was applied to the task of visual object categorization of three data sets, two real-world and one artificial. Orientation histograms on sub-images were utilized as features. With the currently very simple feature extraction method, classification accuracies of about 75% to 90% were attained.	artificial intelligence;artificial neural network	Steffen Simon;Hans A. Kestler;Axel Baune;Friedhelm Schwenker;Günther Palm	1999		10.1109/CIRA.1999.810056	computer vision;contextual image classification;data processing;feature extraction;computer science;cognitive neuroscience of visual object recognition;machine learning;linear classifier;pattern recognition;artificial neural network	AI	23.52073966787665	-63.1014466888068	64456
33d398906776b41d6d77bb73f3a3f4458ca4d64b	latent attention networks		Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned internal mechanisms that contribute to such effective behaviors or, more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network’s inner mechanisms and their power and limitations. Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs. The computed “attention masks” support improved interpretability by highlighting which input attributes are critical in determining output. We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision, natural language processing, and reinforcement learning. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network’s underlying decision-making process irrespective of the data modality.	artificial neural network;computer vision;deep learning;failure cause;modality (human–computer interaction);natural language processing;reinforcement learning	Christopher Grimm;Dilip Arumugam;Siddharth Karamcheti;David Abel;Lawson L. S. Wong;Michael L. Littman	2017	CoRR		visualization;machine learning;computer science;modalities;reinforcement learning;artificial neural network;interpretability;artificial intelligence	ML	21.125714742902968	-53.80150107429218	64724
f369afe8618e3e2c3d1fcf04a2caa5cf0b8b28d1	tabu search based global optimization algorithms for problems in computational chemistry	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;computer applications in chemistry;theoretical and computational chemistry;computational biology bioinformatics;uk phd theses thesis;life sciences;uk research reports;medical journals;europe pmc;documentation and information in chemistry;biomedical research;bioinformatics	Efficient searches for global minima of highly dimensional functions with numerous local minima are central for the solution of many problems in computational chemistry. Well known examples are the identification of the most stable conformer of molecules possessing a high number of freely rotatable bonds [1] or the equilibration phase for QM/MM computations. Mathematically, both represent global optimization problems in which the potential energy function of the molecule is the objective function while the coordinates used for representing the structural arrangement of the system are the variables. Based on an analysis of well-known metaheuristic algorithms, several new global optimization algorithms based on Tabu Search (TS) were developed in our group [2,3], which are using a steepest descent – modest ascent strategy. In a first application the Gradient Only Tabu Search (GOTS) was shown to be applicable to conformational search problems [4]. Further test calculations showed its high efficiency in comparison to other global search algorithms like Molecular Dynamics, Simulated Annealing or Monte Carlo Minimization (MCM). The tests also revealed that the efficiency of GOTS can be enhanced dramatically by combining GOTS (searches the nearest neighbourhood highly efficient) with short MCM simulations (samples the phase space more widely) [5]. The new algorithm is implemented into a program for conformational search, providing several different force fields and sampling algorithms. The investigations also pointed out that a key point is the algorithm for the modest ascent strategy. For providing a smoother scan of the potential energy surface as well as a more accurate description of the minimum energy path between two found minima, the DimerMethod for transition state search was implemented into GOTS-MCM. First tests show that it is much more efficient than previous versions of GOTS. The new algorithm is applied to solvation of biomolecules to provide global optimized solvent shells [6]. In future, the GOTS algorithm will also be used for a proper prediction of reaction pathways by global optimized minimum energy paths and molecular modelling or docking of potential drug molecules to enzymes.	computation;computational chemistry;docking (molecular);force field (chemistry);global optimization;gradient descent;mathematical optimization;maxima and minima;metaheuristic;molecular dynamics;molecular modelling;monte carlo method;multi-chip module;optimization problem;potential energy surface;qm/mm;sampling (signal processing);search algorithm;simulated annealing;simulation;tabu search;times ascent	Christoph Grebner;Johannes Becker;Daniel Weber;Bernd Engels	2012		10.1186/1758-2946-4-S1-P10	biology;medical research;medicine;computer science;bioinformatics;theoretical computer science;algorithm	ML	12.790170580744176	-60.58283318634525	64814
7c775d818cee96429c9ad805a708e608914d7a33	sstegan: self-learning steganography based on generative adversarial networks		Steganography is designed to conceal a secret message within public media. Traditional steganography needs a lot of expert knowledge and complex artificial rules. To solve this problem, we propose a novel self-learning steganographic algorithm based on the generative adversarial network, which we called SSteGAN. This method learns the steganographic algorithm in an unsupervised manner without expert knowledge and directly generates the stego image from the secret message without the cover image. We define a game with four parts: Alice, Bob, Dev and Eve. Alice and Bob attempt to communicate securely. Eve eavesdrops on their conversation and wants to distinguish whether the secret message is embedded in the image. Dev attempts to determine real images from generated images. Experiment results demonstrate that Alice can produce vivid stego images and Bob can successfully decode the secret message with ( 98.8% ) accuracy.		Zihan Wang;Neng Gao;Xin Wang;Xuexin Qu;Linghui Li	2018		10.1007/978-3-030-04179-3_22	conversation;generative grammar;steganography;adversarial system;machine learning;alice and bob;real image;computer science;artificial intelligence	Vision	19.28308616329606	-53.046840324562034	64822
764242ec07e6dbcfbd2650dba52c260818f8a359	a qxp-based multistep docking procedure for accurate prediction of protein-ligand complexes		The two great challenges of the docking process are the prediction of ligand poses in a protein binding site and the scoring of the docked poses. Ligands that are composed of extended chains in their molecular structure display the most difficulties, predominantly because of the torsional flexibility. On the basis of the molecular docking program QXP-Flo+0802, we have developed a procedure particularly for ligands with a high degree of rotational freedom that allows the accurate prediction of the orientation and conformation of ligands in protein binding sites. Starting from an initial full Monte Carlo docking experiment, this was achieved by performing a series of successive multistep docking runs using a local Monte Carlo search with a restricted rotational angle, by which the conformational search space is limited. The method was established by using a highly flexible acetylcholinesterase inhibitor and has been applied to a number of challenging protein-ligand complexes known from the literature.		Laleh Alisaraie;Lars A. Haller;Gregor Fels	2006	Journal of chemical information and modeling	10.1021/ci050343m	docking (dog);bioinformatics;docking (molecular);protein ligand;monte carlo method;searching the conformational space for docking;protein–ligand docking;chemistry;computational chemistry;binding site;ligand	Comp.	11.78089246862009	-60.21158217192391	64851
1474f48ced3eef7d1330a0b963fef89eb84ff282	a toxicity evaluation and predictive system based on neural networks and wavelets.	microarray data;forecasting;calculation methods neural network;neural networks;computational chemistry;computer programs;mode of action;computers in chemistry;toxicity;evaluation;informatics;basic biological sciences;metabolism;neural network	A computational approach has been developed for performing efficient and reasonably accurate toxicity evaluation and prediction. The approach is based on computational neural networks linked to modern computational chemistry and wavelet methods. In this paper, we present details of this approach and results demonstrating its accuracy and flexibility for predicting diverse biological endpoints including metabolic processes, mode of action, and hepato- and neurotoxicity. The approach also can be used for automatic processing of microarray data to predict modes of action.		Pamela L. Piotrowski;Bobby G. Sumpter;Heinrich V. Malling;John S. Wassom;Pinyi Lu;Robin A. Brothers;Gary A. Sega;Sheryl A. Martin;Morey Parang	2007	Journal of chemical information and modeling	10.1021/ci6004788	microarray analysis techniques;chemistry;forecasting;computer science;bioinformatics;artificial intelligence;evaluation;machine learning;toxicity;computational chemistry;informatics;metabolism;artificial neural network;mode of action	Comp.	12.54041598582233	-55.26881010073662	65321
32991f05d7eb62c6bfc2842d36c9c47b2ab2cc76	development of cannabinoid receptor (cb 2 r) ligands for application in pet studies - where to attach the radiolabel?	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;computer applications in chemistry;theoretical and computational chemistry;computational biology bioinformatics;uk phd theses thesis;life sciences;uk research reports;medical journals;europe pmc;documentation and information in chemistry;biomedical research;bioinformatics	The cannabinoid receptors type 2 (CB2R) are involved in many physiological processes but their expression level in healthy and diseased brain has not been unravelled. With positron emission tomography (PET) it is possible to monitor quantitatively very low amounts of compounds labelled with positron emitting isotopes like F in living organisms at high spatial resolution. For application in clinical research, such radiotracers have to show high selectivity and affinity to the target protein. A series of fluorinated N-carbazolyl-oxadiazolyl-propionamides [1] was synthesised and the affinity towards the human CB2R was measured in receptor binding studies. Here, we combine our CB2R receptor model with 3D-QSAR data [2] to support molecular docking studies employing the MOE software (Version 2012.12 Chemical Computing Group Inc. Montreal. http://www.chemcomp. com). The studies revealed that both the primarily investigated compound 2 and the 2-fluoroethyl substituted carbazole derivative 1 (Ki = 3.6 nM) fits well into the binding pocket. Attachment of the fluorine at different positions of the structure does not lead to significantly different poses in accordance with the experimental data. Organ distribution studies on CD1-mice verified our prediction, [3] that [F]1 and [F]2 can cross the blood-brain barrier.	attachments;blood - brain barrier anatomy;ct scan;cannabinoids;docking (molecular);fits;fluorine;isotopes;ligands;moe;nsa product types;organism;physiological processes;polyethylene terephthalate;positron-emission tomography;positrons;processor affinity;quantitative structure-activity relationship;quantitative structure–activity relationship;selectivity (electronic);steroids, fluorinated;cannabinoid receptor;carbazole;carbazolyl triphenylethylene;radiotracer;receptor binding	Robert Günther;Rares Moldovan;Corinna Lueg;Winnie Deuther-Conrad;Bernhard Wünsch;Peter Brust	2014		10.1186/1758-2946-6-S1-O9	biology;medical research;medicine;computer science;bioinformatics	NLP	10.603943351468414	-59.62839346643513	65369
3d9728f1d476b0bba39ee191aeecbe998e96e308	context-aware distance for anomalous human trajectories detection		In this paper, a novel methodology for the representation and distance measurement of trajectories is introduced in order to perform outliers detection tasks. First, a features extraction procedure based on the linear segmentation of trajectories is presented. Next, a configurable context-aware distance is defined. Our representation and distance are significant in that they weigh the relative importance of several relevant features of the trajectories. A clustering method is applied based on the distances matrix and the outliers detection task is performed in any of the clusters. The results of the experiments show the good performance of the method when applied in two different real data sets.		Ignacio San Román;Isaac Martín de Diego;Cristina Conde;Enrique Cabello	2017		10.1007/978-3-319-58838-4_16	artificial intelligence;outlier;cluster analysis;pattern recognition;matrix (mathematics);data set;computer science	Vision	19.428512567636165	-57.73958810967076	65483
b95c7291ba67095f2ff78dc467a522a2c5727d2e	extra precision docking, free energy calculation and molecular dynamics studies on glutamic acid derivatives as murd inhibitors	extra precision docking;induced fit docking;molecular dynamics;murd inhibitors	The binding modes of well known MurD inhibitors have been studied using molecular docking and molecular dynamics (MD) simulations. The docking results of inhibitors 1-30 revealed similar mode of interaction with Escherichia coli-MurD. Further, residues Thr36, Arg37, His183, Lys319, Lys348, Thr321, Ser415 and Phe422 are found to be important for inhibitors and E. coli-MurD interactions. Our docking procedure precisely predicted crystallographic bound inhibitor 7 as evident from root mean square deviation (0.96Å). In addition inhibitors 2 and 3 have been successfully cross-docked within the MurD active site, which was pre-organized for the inhibitor 7. Induced fit best docked poses of 2, 3, 7 and 15/2Y1O complexes were subjected to 10ns MD simulations to determine the stability of the predicted binding conformations. Induce fit derived docked complexes were found to be in a state of near equilibrium as evident by the low root mean square deviations between the starting complex structure and the energy minimized final average MD complex structures. The results of molecular docking and MD simulations described in this study will be useful for the development of new MurD inhibitors with high potency.	abbreviations;acceptor (semiconductors);accessible surface area;alanine transaminase;amino acids;analog;boat dock;crystal structure;duoxa1 gene;diaminopimelic acid;docking (molecular);docking -molecular interaction;glide;glutamic acid;hydrogen bonding;i-frame delay;implicit solvation;keyboard shortcut;kinetics;ligands;ligase;masoprocol;mean squared error;mesoscopic physics;molecular dynamics;molecular mechanics;muscle rigidity;plant roots;polar surface area;protein data bank;protocols documentation;pyschological bonding;qm/mm;score;simulation;thermodynamics;thioctic acid;uniform memory access;uridine diphosphate;uridine monophosphate;free energy;kilocalorie	Mohammed Afzal Azam;Srikanth Jupudi	2017	Computational biology and chemistry	10.1016/j.compbiolchem.2017.05.004	crystallography;stereochemistry;chemistry;searching the conformational space for docking;computational chemistry	Comp.	11.034891660862415	-62.289581950158656	66530
811c4fe7527ecc2a44381dc3bf89be2adcd2a7ec	smbopt: a software package for optimal operation of chromatographic simulated moving bed processes		In the last years, the Simulated Moving Bed (SMB) technique has increasingly been applied to isolate and purify high-valued  pharmaceutical substances or fine chemicals and biotechnology products [13, 12, 9]. SMB-Processes are governed by mixed continuous  and discrete dynamics and exhibit a complex behavior due to non-linear multi-component adsorption. Such processes cannot be  easily designed and operated in the optimal way based solely on experience. In order to reduce separation costs and to shorten  the overall design and development stages, a new integrated software package for the optimal operation of SMB-Processes is  presented in this contribution.  		Abdelaziz Toumi;Sebastian Engell	2003		10.1007/3-540-27170-8_36	parallel computing;real-time computing;computer science;theoretical computer science	Robotics	14.953797170138234	-63.12865443232353	67386
81fc3466f0cc0077c7f6d2f3600f28ea002d63a0	recent advances in the field of foreground detection: an overview		Foreground detection is the classical computer vision task of segmenting out moving object in a particular scene. Many algorithms have been proposed in the past decade for foreground detection. It is often hard to keep track of recent advances in a particular research field with the passage of time. An overview paper is an effective way for the researchers to compare several algorithms according to their strengths and weaknesses. There are several overview papers in the literature; however, they are somewhat obsolete. This overview paper covers the recent algorithms proposed in past 3–5 years except Gaussian Mixture Models (GMM). The aim and contribution of this overview paper is as follows: First, algorithms are classified in three different categories on the basis of choice of picture’s element, feature, and model. Then, each algorithm is summarized concisely. Furthermore, algorithms are compared quantitative and qualitatively using large realistic standard dataset. Paper is concluded with several promising directions for future research.		Ajmal Shahbaz;Laksono Kurnianggoro;Wahyono;Kang-Hyun Jo	2017		10.1007/978-3-319-56660-3_23	mixture model;artificial intelligence;machine learning;foreground detection;computer science;strengths and weaknesses	NLP	24.247184957289576	-56.374949299516935	68010
72a3efcd92ecbba28ed68e537a86605f57535b3f	document summarization using conditional random fields	document summarization;machine learning;conditional random field;con ditional random fields	Desktop users commonly work on multiple tasks. The TaskTracer system provides a convenient, lowcost way for such users to define a hierarchy of tasks and to associate resources with those tasks. With this information, TaskTracer then supports the multi-tasking user by configuring the computer for the current task. To do this, it must detect when the user switches the task and identify the user’s current task at all times. This problem of “task switch detection” is a special case of the general problem of change-point detection. It involves monitoring the behavior of the user and predicting in real time when the user moves from one task to another. We present a framework that analyzes a sequence of observations to detect task switches. First, a classifier is trained discriminatively to predict the current task based only on features extracted from the window in focus. Second, multiple single-window predictions (specifically, the class probability estimates) are combined to obtain more reliable predictions. This paper studies three such combination methods: (a) simple voting, (b) a likelihood ratio test that assesses the variability of the task probabilities over the sequence of windows, and (c) application of the Viterbi algorithm under an assumed task transition cost model. Experimental results show that all three methods improve over the single-window predictions and that the Viterbi approach gives the best results.	active learning (machine learning);adobe swc file;analysis of algorithms;automatic summarization;computer multitasking;conditional random field;context switch;discriminative model;emoticon;information;interrupt;microsoft windows;network switch;real-time computing;spatial variability;theory;viterbi algorithm	Dou Shen;Jian-Tao Sun;Hua Li;Qiang Yang;Zheng Chen	2007			multi-document summarization;computer science;automatic summarization;machine learning;pattern recognition;data mining;conditional random field	AI	21.94363632779975	-57.39099797589178	68468
0f4129d57bc99802221f57dee89211285e55ea20	a hierarchical approach to classify solitary bees based on image analysis	hierarchical approach;classify solitary;image analysis	Solitary bees (Hymenoptera: Apoidea) are endangered by extinction. Furthermore they are highly suited for nature conservancy indication purposes. Nevertheless their appearence in development and ecological planning tasks is quite seldom due to their diicult classii-cation. Our approach to classiication of solitary bees is based on two ideas: (1) to use only the veining of the wings for discrimination, (2) to automate measurement, evaluation, and classiication using image analysis and a hierarchical classiication approach. First results show the functionality and eeciency of our approach even for the classiica-tion of species which are very diicult to separate by classical taxonomy. 1 Motivation There are several reasons for an intensive research interest in solitary bees (Hy-menoptera: Apoidea): (1) The hazard, especially by modern agriculture techniques , for the about 550 Central European species of solitary bees is obvious. (2) Solitary bees are economically and ecologically important for the pollination of many owers and plants. (3) From all pollinating insects the species of solitary bees show the greatest dependencies on territorial conditions ((owers, timbers etc.). Thus solitary bees are very sensitive to agricultural interventions (structural loss, pesticide etc.) and highly suited as bioindicators within ecological planning and monitoring tasks (Drescher 1982, Schwenninger 1992). However there is a lack of suucient classiication results for solitary bees in traditional taxonomy as well as in morphometrical methods which hampers the ecological analysis and the use of these species as bioindicators. 2 Related Work Classical taxonomy reveals several drawbacks: insuucient literature about the determination of zoological species, unsolved relationships between several gen-era and species, and for all an amazing deeciency about the ecology and diversity of species, i.e. worldwide only about 15% of all species are known (Vogt 1994). Computer-assisted approaches to morphometrical analysis (i.e. classiication based on the measurement of characteristic morphological features) can only be found for honey bees and show a lack of eeciency within extensive applications due to an assembly of quite diierent features used for classiication: (1) features of diierent body parts, i.e.	bees algorithm;ecology;gary drescher;image analysis;morphometrics	Volker Steinhage;B. Kastenholz;Stefan Schröder;W. Drescher	1997			taxonomy (biology);extinction;biological classification;endangered species;hymenoptera;apoidea;pattern recognition;artificial intelligence;biology	ML	16.079367435981005	-63.79963751274695	68627
12938a70e29dea8f21661aeb3b8338c3a65b2ba8	energetic analysis of fragment docking and application to structure-based pharmacophore hypothesis generation	hypothesis generation;score function;molecular recognition;docking accuracy;binding site;enrichment;root mean square deviation;fragments;fragment based drug design;virtual screening;tight binding;drug design;crystal structure	We have developed a method that uses energetic analysis of structure-based fragment docking to elucidate key features for molecular recognition. This hybrid ligand- and structure-based methodology uses an atomic breakdown of the energy terms from the Glide XP scoring function to locate key pharmacophoric features from the docked fragments. First, we show that Glide accurately docks fragments, producing a root mean squared deviation (RMSD) of <1.0 A for the top scoring pose to the native crystal structure. We then describe fragment-specific docking settings developed to generate poses that explore every pocket of a binding site while maintaining the docking accuracy of the top scoring pose. Next, we describe how the energy terms from the Glide XP scoring function are mapped onto pharmacophore sites from the docked fragments in order to rank their importance for binding. Using this energetic analysis we show that the most energetically favorable pharmacophore sites are consistent with features from known tight binding compounds. Finally, we describe a method to use the energetically selected sites from fragment docking to develop a pharmacophore hypothesis that can be used in virtual database screening to retrieve diverse compounds. We find that this method produces viable hypotheses that are consistent with known active compounds. In addition to retrieving diverse compounds that are not biased by the co-crystallized ligand, the method is able to recover known active compounds from a database screen, with an average enrichment of 8.1 in the top 1% of the database.		Kathryn Loving;Noeris K. Salam;Woody Sherman	2009	Journal of computer-aided molecular design	10.1007/s10822-009-9268-1	root-mean-square deviation;chemistry;tight binding;virtual screening;bioinformatics;crystal structure;binding site;combinatorial chemistry;computational chemistry;molecular recognition;score;drug design;statistics	Comp.	11.579023003812964	-59.158745338457784	68780
d42b06dec127b108a6b8e5c4c4842c87367a3201	leave-cluster-out cross-validation is appropriate for scoring functions derived from diverse protein data sets	modelizacion;quality assurance;proteine liaison;scoring credit;validacion cruzada;interaction moleculaire;relacion estructura actividad;relation structure activite;molecular interaction;database;base dato;ligand binding;intelligence artificielle;fixation ligand;modelisation;aseguracion calidad;interaccion molecular;relacion estructura propiedad;proteina enlace;validation croisee;base de donnees;valuacion credito;artificial intelligence;modele donnee;cross validation;binding protein;inteligencia artificial;fijacion ligando;modeling;assurance qualite;structure activity relation;relation structure propriete;property structure relationship;data models;methode score	With the emergence of large collections of protein-ligand complexes complemented by binding data, as found in PDBbind or BindingMOAD, new opportunities for parametrizing and evaluating scoring functions have arisen. With huge data collections available, it becomes feasible to fit scoring functions in a QSAR style, i.e., by defining protein-ligand interaction descriptors and analyzing them with modern machine-learning methods. As in each data modeling ansatz, care has to be taken to validate the model carefully. Here, we show that there are large differences measured in R (0.77 vs 0.46) or R² (0.59 vs 0.21) for a relatively simple scoring function depending on whether it is validated against the PDBbind core set or validated in a leave-cluster-out cross-validation. If proteins from the same family are present in both the training and validation set, the estimated prediction quality from standard validation techniques looks too optimistic.		Christian Kramer;Peter Gedeck	2010	Journal of chemical information and modeling	10.1021/ci100264e	quality assurance;data modeling;systems modeling;computer science;artificial intelligence;data mining;mathematics;ligand;binding protein;algorithm;cross-validation	ML	12.2194202175499	-56.177242381956034	68798
c502ddfa48302c4585fa34e5f76454e1995ab3dc	a graphene oxide-based fluorescent method for the detection of human chorionic gonadotropin	fluorescent biosensors;peptide aptamer;human chorionic gonadotropin;antibody free;graphene oxide	Human chorionic gonadotropin (hCG) has been regarded as a biomarker for the diagnosis of pregnancy and some cancers. Because the currently used methods (e.g., disposable Point of Care Testing (POCT) device) for hCG detection require the use of many less stable antibodies, simple and cost-effective methods for the sensitive and selective detection of hCG have always been desired. In this work, we have developed a graphene oxide (GO)-based fluorescent platform for the detection of hCG using a fluorescein isothiocyanate (FITC)-labeled hCG-specific binding peptide aptamer (denoted as FITC-PPLRINRHILTR) as the probe, which can be manufactured cheaply and consistently. Specifically, FITC-PPLRINRHILTR adsorbed onto the surface of GO via electrostatic interaction showed a poor fluorescence signal. The specific binding of hCG to FITC-PPLRINRHILTR resulted in the release of the peptide from the GO surface. As a result, an enhanced fluorescence signal was observed. The fluorescence intensity was directly proportional to the hCG concentration in the range of 0.05-20 IU/mL. The detection limit was found to be 20 mIU/mL. The amenability of the strategy to hCG analysis in biological fluids was demonstrated by assaying hCG in the urine samples.	adrenergic alpha-agonists;biological markers;ca-125 antigen;cmos;chorionic gonadotropin;dec alpha;fetal proteins;fluorescein-5-isothiocyanate;fluorescence;gonadotropins;graphene;immunoassay method;instruction unit;interference (communication);isothiocyanate;liquid substance;malignant neoplasms;mucin-1 antigen;sensor;stand up to cancer;aptamer;disposable;endopeptidase clp;fluorescein sodium	Ning Xia;Xin Wang;Lin Liu	2016		10.3390/s16101699	molecular biology;chemistry;nanotechnology	AI	11.685452528883008	-65.06697402993703	68830
f7b0ed4aa8e4f6277e9a779466bf2fd152feb25d	hierarchical late fusion for concept detection in videos	semantic indexing;hierarchical;late fusion;semantic concepts;video	We deal with the issue of combining dozens of classifiers into a better one, for concept detection in videos. We compare three fusion approaches that share a common structure: they all start with a classifier clustering stage, continue with an intra-cluster fusion and end with an inter-cluster fusion. The main difference between them comes from the first stage. The first approach relies on a priori knowledge about the internals of each classifier (low-level descriptors and classification algorithm) to group the set of available classifiers by similarity. The second and third approaches obtain classifier similarity measures directly from their output and group them using agglomerative clustering for the second approach and community detection for the third one.	algorithm;cluster analysis;computational complexity theory;high- and low-level	Sabin Tiberius Strat;Alexandre Benoit;Hervé Bredin;Georges Quénot;Patrick Lambert	2012		10.1007/978-3-642-33885-4_34	video;computer science;machine learning;pattern recognition;data mining;hierarchy	Vision	18.305338471151217	-55.71982903169196	68880
75b08bbe86fd3bd89ccace79a87bf7cbc447c055	blind prediction of cyclohexane-water distribution coefficients from the sampl5 challenge		In the recent SAMPL5 challenge, participants submitted predictions for cyclohexane/water distribution coefficients for a set of 53 small molecules. Distribution coefficients (log D) replace the hydration free energies that were a central part of the past five SAMPL challenges. A wide variety of computational methods were represented by the 76 submissions from 18 participating groups. Here, we analyze submissions by a variety of error metrics and provide details for a number of reference calculations we performed. As in the SAMPL4 challenge, we assessed the ability of participants to evaluate not just their statistical uncertainty, but their model uncertainty-how well they can predict the magnitude of their model or force field error for specific predictions. Unfortunately, this remains an area where prediction and analysis need improvement. In SAMPL4 the top performing submissions achieved a root-mean-squared error (RMSE) around 1.5 kcal/mol. If we anticipate accuracy in log D predictions to be similar to the hydration free energy predictions in SAMPL4, the expected error here would be around 1.54 log units. Only a few submissions had an RMSE below 2.5 log units in their predicted log D values. However, distribution coefficients introduced complexities not present in past SAMPL challenges, including tautomer enumeration, that are likely to be important in predicting biomolecular properties of interest to drug discovery, therefore some decrease in accuracy would be expected. Overall, the SAMPL5 distribution coefficient challenge provided great insight into the importance of modeling a variety of physical effects. We believe these types of measurements will be a promising source of data for future blind challenges, especially in view of the relatively straightforward nature of the experiments and the level of insight provided.		Caitlin C. Bannan;Kalistyn H. Burley;Michael Chiu;Michael R. Shirts;Michael K. Gilson;David L. Mobley	2016	Journal of computer-aided molecular design	10.1007/s10822-016-9954-8	computer science;data mining;statistics	ML	13.118306357884482	-59.68254217308219	69118
0f79396f0bd33f514de46ef2e72143c69366ac70	generalized method of moments for estimating parameters of stochastic reaction networks	simulation and modeling;systems biology;physiological cellular and medical topics;computational biology bioinformatics;algorithms;bioinformatics	Discrete-state stochastic models have become a well-established approach to describe biochemical reaction networks that are influenced by the inherent randomness of cellular events. In the last years several methods for accurately approximating the statistical moments of such models have become very popular since they allow an efficient analysis of complex networks. We propose a generalized method of moments approach for inferring the parameters of reaction networks based on a sophisticated matching of the statistical moments of the corresponding stochastic model and the sample moments of population snapshot data. The proposed parameter estimation method exploits recently developed moment-based approximations and provides estimators with desirable statistical properties when a large number of samples is available. We demonstrate the usefulness and efficiency of the inference method on two case studies. The generalized method of moments provides accurate and fast estimations of unknown parameters of reaction networks. The accuracy increases when also moments of order higher than two are considered. In addition, the variance of the estimator decreases, when more samples are given or when higher order moments are included.	approximation;artificial life;biochemical reaction;coefficient;complement system proteins;complex network;computation (action);computational resource;estimated;estimation theory;experiment;gaussian (software);global optimization;gm(m);google map maker;gradient;higher-order function;inference;initial condition;matching;manuscripts;mathematical optimization;multimodal interaction;normal statistical distribution;population parameter;randomness;sample variance;snapshot (computer storage);statistical test;stochastic process;weight;interest	Alexander Lück;Verena Wolf	2016		10.1186/s12918-016-0342-8	biology;computer science;bioinformatics;theoretical computer science;machine learning;systems biology	ML	13.064793842335087	-53.27800323209735	69274
cf1067cc23d043e33c2e01a92796628538c09a30	understanding convolutional neural networks for text classification		We present an analysis into the inner workings of Convolutional Neural Networks (CNNs) for processing text. CNNs used for computer vision can be interpreted by projecting filters into image space, but for discrete sequence inputs CNNs remain a mystery. We aim to understand the method by which the networks process and classify text. We examine common hypotheses to this problem: that filters, accompanied by global max-pooling, serve as ngram detectors. We show that filters may capture several different semantic classes of ngrams by using different activation patterns, and that global max-pooling induces behavior which separates important ngrams from the rest. Finally, we show practical use cases derived from our findings in the form of model interpretability (explaining a trained model by deriving a concrete identity for each filter, bridging the gap between visualization tools in vision tasks and NLP) and prediction interpretability (explaining predictions).	bridging (networking);computer vision;convolutional neural network;document classification;n-gram;natural language processing;sensor	Alon Jacovi;Oren Sar Shalom;Yoav Goldberg	2018	CoRR		machine learning;convolutional neural network;artificial intelligence;visualization;computer science;bridging (networking);interpretability;use case	NLP	21.389320878545405	-53.698850232388075	69565
6e3cc01fb244599cee4ae1eaaeb9f77b54e99454	how many labeled license plates are needed?		Training a good deep learning model often requires a lot of annotated data. As a large amount of labeled data is typically difficult to collect and even more difficult to annotate, data augmentation and data generation are widely used in the process of training deep neural networks. However, there is no clear common understanding on how much labeled data is needed to get satisfactory performance. In this paper, we try to address such a question using vehicle license plate character recognition as an example application. We apply computer graphic scripts and Generative Adversarial Networks to generate and augment a large number of annotated, synthesized license plate images with realistic colors, fonts, and character composition from a small number of real, manually labeled license plate images. Generated and augmented data are mixed and used as training data for the license plate recognition network modified from DenseNet. The experimental results show that the model trained from the generated mixed training data has good generalization ability, and the proposed approach achieves a new state-of-the-art accuracy on Dataset-1 and AOLP, even with a very limited number of original real license plates. In addition, the accuracy improvement caused by data generation becomes more significant when the number of labeled images is reduced. Data augmentation also plays a more significant role when the number of labeled images is increased.	artificial neural network;automatic number plate recognition;color;convolutional neural network;deep learning;generative adversarial networks;optical character recognition	Changhao Wu;Shugong Xu;Guocong Song;Shunqing Zhang	2018		10.1007/978-3-030-03341-5_28	machine learning;labeled data;license;deep learning;pattern recognition;artificial neural network;small number;computer science;scripting language;test data generation;training set;artificial intelligence	AI	21.86832686891152	-52.97278291060721	69676
aa7e8ec2c01b4b41d60386eedb433cdbe27e85a7	a machine learning-based method for protein global model quality assessment	support vector regression;quality assessment;model quality assessment;machine learning;protein structure prediction;secondary structure;indexation;solvent accessibility;cross validation	Model quality assessment is an important task in protein structure prediction, which has been recently identified as one of the bottlenecks limiting the quality and usefulness of protein structure prediction methods. A new prediction category that evaluates the quality of protein models has been implemented since CASP7. In this study, a machine learning-based method for protein Global Model Quality Assessment (GMQA) is presented and the web server can be assessed at http://www.iipl.fudan.edu.cn/gmqa/index.html. The GMQA method takes a protein model as input and outputs the predicted MaxSub score that indicates the absolute quality of the protein model. The proposed method extracts the structural features from the 3D coordinates and assigns an absolute quality score to a model by support vector regression. Three types of features are extracted, including secondary structure, relative solvent accessibility and contact. The closed test is performed on the CASP7 dataset by using cross validation. An open testing is performed on the LKF dataset. In both tests, good correlations between the predicted and true scores are observed. Furthermore, our method is able to discriminate the native or near native structures from a set of decoys. We also demonstrate that the GMQA method outperforms two existing methods, i.e. ProQ and Victor/FRST. Our results show that GMQA is a useful tool for model quality assessment and ranking.	accessibility;casp;cross-validation (statistics);machine learning;protein structure prediction;server (computing);support vector machine;victor a. vyssotsky;web server;world wide web	Qiwen Dong;Yufei Chen;Shuigeng Zhou	2011	Int. J. General Systems	10.1080/03081079.2010.544869	support vector machine;computer science;data science;machine learning;protein structure prediction;data mining;cross-validation;protein secondary structure	Comp.	11.04356780500438	-56.360872872509255	69759
6e21d86603a5966d0db635665bec3efab909fab9	using motion planning to map protein folding landscapes and analyze folding kinetics of known native structures	protein folding kinetics;free energy landscape;folding pathway;motion planning;protein folding;folding kinetics;potential energy;kinetics	We present a novel approach for studying the kinetics of protein folding. The framework has evolved from a robotics motion planning technique called probabilistic roadmap methods (prms) that has been applied in many diverse elds with great success. In our previous work, we applied this method to study protein folding pathways of several small proteins and obtained some very encouraging results. In this paper, we describe how we applied our motion planning framework to the study of protein folding kinetics. In particular, we present a re ned version of our motion planning framework and describe how it can be used to produce potential energy landscapes, free energy landscapes, and many folding pathways all from a single roadmap which is computed in a few hours on a desktop PC. Results are presented for 14 proteins. Our ability to produce large sets of unrelated folding pathways may potentially provide crucial insight into some aspects of folding kinetics, such as proteins that exhibit both two-state and three-state kinetics, that are not captured by other theoretical techniques. This research supported in part by NSF CAREER Award CCR-9624315, NSF Grants IIS-9619850, ACI-9872126, EIA-9975018, EIA-0103742, EIA-9805823, ACI-0113971, CCR-0113974, EIA-9810937, EIA-0079874, and by the Texas Higher Education Coordinating Board grant ARP-036327-017. (a) (b) (c) (d) Figure 1: Snapshots of a carton folding. (a) (b) (c) (d) Figure 2: Snapshots of a 10 ALA chain folding.	desktop computer;emoticon;experiment;fm towns marty;gene regulatory network;ibm notes;jean;kinesiology;kinetics internet protocol;motion planning;probabilistic roadmap;robotics;statistical model;three-state logic	Nancy M. Amato;Ken A. Dill;Guang Song	2003	Journal of Computational Biology	10.1089/10665270360688002	protein folding;potential energy;motion planning;downhill folding;kinetics	Comp.	13.24534542932619	-62.446971942117216	70420
5669bb840f16759a57beffff8b8dec58ebe31530	performance enhancement of deep reinforcement learning networks using feature extraction		The combination of Deep Learning and Reinforcement Learning, termed Deep Reinforcement Learning Networks (DRLN), offers the possibility of using a Deep Learning Neural Network to produce an approximate Reinforcement Learning value table that allows extraction of features from neurons in the hidden layers of the network. This paper presents a two stage technique for training a DRLN on features extracted from a DRLN trained on a identical problem, via the implementation of the Q-Learning algorithm, using TensorFlow. The results show that the extraction of features from the hidden layers of the Deep Q-Network improves the learning process of the agent (4.58 times faster and better) and proves the existence of encoded information about the environment which can be used to select the best action. The research contributes preliminary work in an ongoing research project in modeling features extracted from DRLNs.		Joaquin Ollero;Christopher Child	2018		10.1007/978-3-319-92537-0_25	machine learning;artificial neural network;feature extraction;deep learning;reinforcement learning;computer science;pattern recognition;artificial intelligence	AI	19.061834262009597	-53.90965153709671	70946
52b5baaa9d14719dfba0b25ef5f7632137f7e217	optimal experiment selection for parameter estimation in biological differential equation models	mathematical computing;research design;gene regulatory networks;computational biology bioinformatics;models genetic;algorithms;combinatorial libraries;computer appl in life sciences;microarrays;bioinformatics	Parameter estimation in biological models is a common yet challenging problem. In this work we explore the problem for gene regulatory networks modeled by differential equations with unknown parameters, such as decay rates, reaction rates, Michaelis-Menten constants, and Hill coefficients. We explore the question to what extent parameters can be efficiently estimated by appropriate experimental selection. A minimization formulation is used to find the parameter values that best fit the experiment data. When the data is insufficient, the minimization problem often has many local minima that fit the data reasonably well. We show that selecting a new experiment based on the local Fisher Information of one local minimum generates additional data that allows one to successfully discriminate among the many local minima. The parameters can be estimated to high accuracy by iteratively performing minimization and experiment selection. We show that the experiment choices are roughly independent of which local minima is used to calculate the local Fisher Information. We show that by an appropriate choice of experiments, one can, in principle, efficiently and accurately estimate all the parameters of gene regulatory network. In addition, we demonstrate that appropriate experiment selection can also allow one to restrict model predictions without constraining the parameters using many fewer experiments. We suggest that predicting model behaviors and inferring parameters represent two different approaches to model calibration with different requirements on data and experimental cost.	biological models;choice behavior;coefficient;curve fitting;estimation theory;experiment;fisher information;gene regulatory network;maxima and minima;optimal design;population parameter;requirement	Mark K. Transtrum;Peng Qiu	2012		10.1186/1471-2105-13-181	biology;gene regulatory network;dna microarray;computer science;bioinformatics;machine learning	ML	12.97281361082169	-53.21911464437453	71757
7b3e99596bdfa0f31677c2adc9e3b876e050849c	a memetic inference method for gene regulatory networks based on s-systems	dna;biology computing;evolutionary computation;inference mechanisms;genetics;dna gene expression mathematical model biological system modeling proteins bioinformatics semiconductor device measurement probes systems biology biological systems;mathematical model;dna biology computing inference mechanisms evolutionary computation genetics;dna microarray data;evolutionary algorithm;gene regulatory network;non linear system;topology memetic inference method gene regulatory networks s systems dna microarray data mathematical model quantitative model inference problem evolutionary algorithms sparsely connected nonlinear systems multimodal problem optimization strategies	In this paper, we address the problem of finding gene regulatory networks from experimental DNA microarray data. As underlying mathematical model we used S-Systems, a quantitative model, which recently has found increased attention in the literature. Due to the complexity of the inference problem some researchers suggested evolutionary algorithms for this purpose. We introduce enhancements to this optimization process to infer the parameters of sparsely connected non-linear systems given by the observed data more reliably and precisely. Due to the limited number of available data the inferring problem is under-determined and ambiguous. Further on, the problem often is multi-modal and therefore appropriate optimization strategies become necessary. In this paper, we propose a new method, which evolves the topology as well as the parameters of the mathematical model to find the correct network. This method is compared to standard algorithms found in the literature.	dna microarray;evolutionary algorithm;gene regulatory network;linear system;mathematical model;mathematical optimization;memetics;modal logic;nonlinear system	Christian Spieth;Felix Streichert;Nora Speer;Andreas Zell	2004	Proceedings of the 2004 Congress on Evolutionary Computation (IEEE Cat. No.04TH8753)	10.1109/CEC.2004.1330851	gene regulatory network;computer science;bioinformatics;theoretical computer science;machine learning;evolutionary algorithm;mathematical model;dna;memetic algorithm;evolutionary computation	AI	12.595957459385335	-52.67146637509467	72259
47060abe69dcb743629ff9a3151d86fab645a174	comparison of free‐energy methods using a tripeptide‐water model system	gromos;active-site water;free-energy calculations;molecular dynamics simulations	We investigate the ability of several free-energy calculation methods to combine two alchemical changes. We use Bennett acceptance ratio (BAR), thermodynamic integration (TI), extended TI (X-TI), and enveloping distribution sampling (EDS) to perturb a water molecule, which is restrained to an amino acid that is also being perturbed. In addition to these pairwise methods, we present two two-dimensional approaches, EDS-TI and two-dimensional TI (2D-TI). We compare feasibility, efficiency and usability of these methods in regard to our simple model system, which mimics the displacement of a water molecule in the active site of a protein on residue mutation. The correct treatment of structural water has been shown to greatly aid binding affinity calculations in some cases that remained elusive otherwise. This is of broad interest in, for example, drug design, and we conclude that thus far, the pairwise method BAR and also the newer X-TI remain the most suitable methods to treat this problem as long as few end states are involved. © 2018 Wiley Periodicals, Inc.	affinity;amino acids;drug design;ehlers-danlos syndrome;mutation;psychologic displacement;staphylococcal protein a;thermodynamics	Manuela Maurer;Niels Hansen;Chris Oostenbrink	2018		10.1002/jcc.25537		Robotics	10.60548383338236	-62.5713560767271	72762
1b24dd96dd8593e11a3d45a32dbec9472dbf0341	a multi-modal multi-view dataset for human fall analysis and preliminary investigation on modality		Over the last decade, a large number of methods have been proposed for human fall detection. Most existing methods were evaluated based on trimmed datasets. More importantly, these datasets lack variety of falls, subjects, views and modalities. This paper makes two contributions in the topic of automatic human fall detection. Firstly, to address the above issues, we introduce a large continuous multimodal multivew dataset of human fall, namely CMDFALL. Our CMDFALL dataset was built by capturing activities from 50 subjects, with seven overlapped Kinect sensors and two wearable accelerometers. Each subject performs 20 activities including 8 falls of different styles and 12 daily activities. All multi-modal multi-view data (RGB, depth, skeleton, acceleration) are time-synchronized and annotated for evaluating performance of recognition algorithms of human activities or human fall in indoor environment. Secondly, based on the multimodal property of the dataset, we investigate the role of each modality to get the best results in the context of human activity recognition. To this end, we adopt existing baseline techniques which have been shown to be very efficient for each data modality such as C3D convnet on RGB; DMM-KDES on depth; Res-TCN on skeleton and 2D convnet on acceleration data. We analyze to show which modalities and their combination give the best performance.		Thanh-Hai Tran;Thi-Lan Le;Dinh-Tan Pham;Van-Nam Hoang;Van-Minh Khong;Q Tran;Thai-Son Nguyen;Cuong Pham	2018	2018 24th International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2018.8546308	skeleton (computer programming);computer vision;artificial intelligence;accelerometer;feature extraction;wearable computer;activity recognition;pattern recognition;rgb color model;computer science	Vision	24.0182790088528	-59.03642525983787	72899
442319260d82ae8f7dec016849e566fd15cd520f	saliency-based sequential image attention with multiset prediction		Humans process visual scenes selectively and sequentially using attention. Central to models of human visual attention is the saliency map. We propose a hierarchical visual architecture that operates on a saliency map and uses a novel attention mechanism to sequentially focus on salient regions and take additional glimpses within those regions. The architecture is motivated by human visual attention, and is used for multi-label image classification on a novel multiset task, demonstrating that it achieves high precision and recall while localizing objects with its attention. Unlike conventional multi-label image classification models, the model supports multiset prediction due to a reinforcement-learning based training process that allows for arbitrary label permutation and multiple instances per label.	computer vision;deep learning;downstream (software development);humans;multi-label classification;precision and recall;rl (complexity);reinforcement learning	Sean Welleck;Jialin Mao;Kyunghyun Cho;Zheng Zhang	2017			precision and recall;architecture;permutation;pattern recognition;machine learning;artificial intelligence;computer science;salience (neuroscience);multiset;saliency map;contextual image classification	ML	23.56030474262996	-55.517216709390816	73250
368140f4bfac33686227f5519c73b4b4dff907c4	a novel and effective fmri decoding approach based on sliced inverse regression and its application to pain prediction		Abstract Dimension reduction is essential in fMRI decoding, but the complex relationship between fMRI data and class labels is often unknown or not well modeled so that the most effective dimension reduction (e.d.r.) directions can hardly be identified. In the present study, we introduce a novel fMRI decoding approach based on an effective and general dimension reduction method, namely sliced inverse regression (SIR), which can exploit class information for estimating e.d.r. directions even when the relationship between fMRI data and class labels is not explicitly known. We incorporate singular value decomposition (SVD) into SIR to overcome SIRu0027s limitation in dealing with ultra-high-dimensional data, and integrate SVD-SIR into a pattern classifier to enable quantification of the contributions of fMRI voxels to class labels. The resultant new SIR decoding analysis (SIR-DA) approach is capable of decoding behavioral responses and identifying predictive fMRI patterns. Simulation results showed that SIR-DA can more accurately detect e.d.r. directions and achieve higher classification accuracy than decoding approaches based on conventional dimension reduction methods. Further, we applied SIR-DA on real-world pain-evoked fMRI data to decode the level of pain perception and showed that SIR-DA can achieve higher accuracy in pain prediction than conventional methods. These results suggest that SIR-DA is an effective data-driven technique to decode behavioral or cognitive states from fMRI data and to uncover unknown brain patterns associated with behavior or cognitive responses.	sliced inverse regression	Yiheng Tu;Zening Fu;Ao Tan;Gan Huang;Lihong Hu;Yeung Sam Hung;Zhiguo Zhang	2018	Neurocomputing	10.1016/j.neucom.2017.07.045	voxel;machine learning;effective dimension;artificial intelligence;decoding methods;dimensionality reduction;pattern recognition;singular value decomposition;mathematics;cognition;sliced inverse regression	ML	20.39782857668865	-54.627761407371565	73404
a451f2f9bb2016c4ea29803887c208d1bf6d38c3	object classification using a fragment-based representation	object recognition;systeme intelligent;psichologia cognitiva;cognitive psychology;systeme apprentissage;building block;sistema inteligente;reconnaissance objet;computer vision;learning systems;psychologie cognitive;intelligent system;pattern classification;pattern recognition;classification error;mutual information;object classification;reconnaissance forme;visual object recognition;reconocimiento patron;object perception;visual system;classification forme	The tasks of visual object recognition and classification are natural and effortless for biological visual systems, but exceedingly difficult to replicate in computer vision systems. This difficulty arises from the large variability in images of different objects within a class, and variability in viewing conditions. In this paper we describe a fragment-based method for object classification. In this approach objects within a class are represented in terms of common image fragments, that are used as building blocks for representing a large variety of different objects that belong to a common class, such as a face or a car. Optimal fragments are selected from a training set of images based on a criterion of maximizing the mutual information of the fragments and the class they represent. For the purpose of classification the fragments are also organized into types, where each type is a collection of alternative fragments, such as different hairline or eye regions for face classification. During classification, the algorithm detects fragments of the different types, and then combines the evidence for the detected fragments to reach a final decision. The algorithm verifies the proper arrangement of the fragments and the consistency of the viewing conditions primarily by the conjunction of overlapping fragments. The method is different from previous part-based methods in using class-specific overlapping object fragments of varying complexity, and in verifying the consistent arrangement of the fragments primarily by the conjunction of overlapping detected fragments. Experimental results on the detection of face and car views show that the fragment-based approach can generalize well to completely novel image views within a class while maintaining low mis-classification error rates. We briefly discuss relationships between the proposed method and properties of parts of the primate visual system involved in object perception. 1 Classification and the Generalization Problem Object classification is a natural task for our visual system: we effortlessly classify a novel object as a person, dog, car, house, and the like, based on its appearance. Even a three-year old child can easily classify a large variety of images of many natural classes. In contrast, visual classification proved extremely difficult to reproduce in artificial computer vision system. It is therefore natural to study the mechanisms and processes used by biological visual system for object classification, and to examine the applicability of similar methods to computer vision system. Such studies may lead to the development of better artificial systems dealing with natural images, and can also shed light on what appears to be fundamental differences in the processes of 74 S. Ullman and E. Sali visual information by current computer systems on the one hand, and by biological systems on the other. It is interesting to note in this context the difference between general object classification and the specific identification of individual objects. Classification is concerned with the general description of an object as belonging to a natural class of similar objects, such as a face or a dog, whereas identification involves the recognition of a specific individual within a class, such as the face of a particular person, or the make of a particular car. For human vision, the general classification of an object as a car, for example, is usually easier than the identification of the specific make of the car (Rosch et al.1976). In contrast, current computer vision systems can deal more successfully with the task of recognition compared with classification. This may appear surprising, because specific identification requires finer distinctions between objects compared with general classification, and therefore the task appears to be more demanding. The main difficulty faced by a recognition and classification system is the problem of variability, and the need to generalize across variations in the appearance of objects belonging to the same class. Different dog images, for example, can vary widely, because they can represent different kinds of dogs, and for each particular dog, the appearance will change with the imaging conditions, such as the viewing angle, distance, and illumination conditions, with the animal’s posture, and so on. The visual system is therefore constantly faced with views that are different from all other views seen in the past, and it is required to generalize correctly from past experience and classify correctly the novel image. The variability is complex in nature: it is difficult to provide, for instance, a precise definition for all the allowed variations of dog images. The human visual system somehow learns the characteristics of the allowed variability from experience. This makes classification more difficult for artificial system than individual identification. In performing identification of a specific car, say, one can supply the system with a full and exact model of the object, and the expected variations can be described with precision. This is the basis for several approaches to identification, for example, methods that use image combinations (Ullman & Basri 1991) or interpolation (Poggio & Edelman 1990) to predict the appearance of a known object under given viewing conditions. In classification, the range of possible variations is wider, since now, in addition to variations in the viewing condition, one must also contend with variations in shape of different objects within the same class. In this paper we propose an approach to classification that uses a fragment-based representation. In this approach, images of objects within a class are represented in terms of class-specific fragments. These fragments provide common building blocks that can be used, in different combinations, to represent a large variety of different images of objects within the class. In the next section we discuss the problem of selecting a set of fragments that are best suited for representing a class of related objects, given a set of example images. We then illustrate the use of these fragments to perform classification and deal with the variability in shape between different objects of the same class. We also discuss the problem of coping with variability in the viewing conditions, focusing on the problem of position invariance, with possible application to other aspects of object recognition. Finally, we conclude with some comments about similarities between the proposed approach and aspects of the human visual system.	artificial intelligence;biological system;computer vision;courant–friedrichs–lewy condition;fragment (computer graphics);graph isomorphism problem;heart rate variability;human visual system model;image;interpolation;mutual information;outline of object recognition;poor posture;self-replication;sethi–ullman algorithm;spatial variability;statistical classification;test set;tomaso poggio;verification and validation;viewing angle	Shimon Ullman;Erez Sali	2000		10.1007/3-540-45482-9_8	computer vision;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;machine learning;algorithm	Vision	22.305522214222215	-61.3690594726904	73591
d8d489aa70f3846380cd5ce4d670c2bbf6d36d2d	molecular modeling of cytochrome p450 3a4	molecular modeling;drug metabolism;enzyme;site directed mutagenesis;structure function relationship;three dimensional structure;active site	The three-dimensional structure of human cytochrome P450 3A4 was modeled based on crystallographic coordinates of four bacterial P450s; P450 BM-3, P450cam, P450terp, and P450eryF. The P450 3A4 sequence was aligned to those of the known proteins using a structure-based alignment of P450 BM-3, P450cam, P450terp, and P450eryF. The coordinates of the model were then calculated using a consensus strategy, and the final structure was optimized in the presence of water. The P450 3A4 model resembles P450 BM-3 the most, but the B' helix is similar to that of P450eryF, which leads to an enlarged active site when compared with P450 BM-3, P450cam, and P450terp. The 3A4 residues equivalent to known substrate contact residues of the bacterial proteins and key residues of rat P450 2B1 are located in the active site or the substrate access channel. Docking of progesterone into the P450 3A4 model demonstrated that the substrate bound in a 6 beta-orientation can interact with a number of active site residues, such as 114, 119, 301, 304, 305, 309, 370, 373, and 479, through hydrophobic interactions. The active site of the enzyme can also accommodate erythromycin, which, in addition to the residues listed for progesterone, also contacts residues 101, 104, 105, 214, 215, 217, 218, 374, and 478. The majority of 3A4 residues which interact with progesterone and/or erythromycin possess their equivalents in key residues of P450 2B enzymes, except for residues 297, 480 and 482, which do not contact either substrate in P450 3A4. The results from docking of progesterone and erythromycin into the enzyme model make it possible to pinpoint residues which may be important for 3A4 function and to target them for site-directed mutagenesis.	101 mouse;alignment;amino acids;assignment zero;boat dock;camphor 5-monooxygenase;cancer center support grant;cytochrome p450;docking (molecular);enlargement procedure;equivalent weight;erythromycin;experiment;heparin, low-molecular-weight;homologous gene;homology (biology);homology modeling;interaction;manuscripts;molecular diagnostic techniques;molecular modelling;progesterone;review [publication type];science;substrate (electronics);verification of theories;cytochrome p-450 cyp108 ( pseudomonas);eryf protein, saccharopolyspora erythraea;molecular modeling	Grazyna D. Szklarz;James R. Halpert	1997	Journal of computer-aided molecular design	10.1023/A:1007956612081	biochemistry;stereochemistry;enzyme;chemistry;site-directed mutagenesis;active site;organic chemistry;molecular model;drug metabolism	Comp.	10.160825582157486	-61.590792376183636	73704
1f349aef8a9e496939e47010e20a82367293a37c	software for differential systems and applications involving macroscopic kinetics	systems and applications;kinetics	Abstract   A substantial amount of well tested software is readily available to the scientific community at low cost or no cost. Here, we describe some of this software and give some examples of problems which illustrate its use in model problems involving maroscopic kinetics.	kinetics internet protocol	George D. Byrne	1981	Computers & Chemistry	10.1016/0097-8485(81)80102-7	biology;simulation;computer science;theoretical computer science;physics;kinetics	OS	14.04992763826367	-63.41914236073899	73911
39eb3150a35f88f1cb08c5f5036d40ab2c3dce4a	ligand-target prediction by structural network biology using nannolyze	software;drug discovery;ligands;binding sites;info eu repo semantics article;proteins;roc curve;protein binding;humans;computational biology;databases protein	"""Target identification is essential for drug design, drug-drug interaction prediction, dosage adjustment and side effect anticipation. Specifically, the knowledge of structural details is essential for understanding the mode of action of a compound on a target protein. Here, we present nAnnoLyze, a method for target identification that relies on the hypothesis that structurally similar binding sites bind similar ligands. nAnnoLyze integrates structural information into a bipartite network of interactions and similarities to predict structurally detailed compound-protein interactions at proteome scale. The method was benchmarked on a dataset of 6,282 pairs of known interacting ligand-target pairs reaching a 0.96 of area under the Receiver Operating Characteristic curve (AUC) when using the drug names as an input feature for the classifier, and a 0.70 of AUC for """"anonymous"""" compounds or compounds not present in the training set. nAnnoLyze resulted in higher accuracies than its predecessor, AnnoLyze. We applied the method to predict interactions for all the compounds in the DrugBank database with each human protein structure and provide examples of target identification for known drugs against human diseases. The accuracy and applicability of our method to any compound indicate that a comparative docking approach such as nAnnoLyze enables large-scale annotation and analysis of compound-protein interactions and thus may benefit drug development."""	annotation;area under curve;binding sites;boat dock;docking (molecular);drug design;drug interactions;drugbank;interaction design;ligands;name;test set;drug development	Francisco Martínez-Jiménez;Marc A. Martí-Renom	2015		10.1371/journal.pcbi.1004157	biology;plasma protein binding;bioinformatics;binding site;machine learning;data mining;ligand;receiver operating characteristic;drug discovery	Comp.	10.114619725709357	-56.79400769377335	73915
977794ceaf5f2acc10d7880c531a0ad4b07d1a2e	molecular dynamics simulations of the adenosine a2a receptor: structural stability, sampling, and convergence		Molecular dynamics (MD) simulations of membrane-embedded G-protein coupled receptors (GPCRs) have rapidly gained popularity among the molecular simulation community in recent years, a trend which has an obvious link to the tremendous pharmaceutical importance of this group of receptors and the increasing availability of crystal structures. In view of the widespread use of this technique, it is of fundamental importance to ensure the reliability and robustness of the methodologies so they yield valid results and enable sufficiently accurate predictions to be made. In this work, 200 ns simulations of the A2a adenosine receptor (A2a AR) have been produced and evaluated in the light of these requirements. The conformational dynamics of the target protein, as obtained from replicate simulations in both the presence and absence of an inverse agonist ligand (ZM241385), have been investigated and compared using principal component analysis (PCA). Results show that, on this time scale, convergence of the replicates is not readily evident and dependent on the types of the protein motions considered. Thus rates of inter- as opposed to intrahelical relaxation and sampling can be different. When studied individually, we find that helices III and IV have noticeably greater stability than helices I, II, V, VI, and VII in the apo form. The addition of the inverse agonist ligand greatly improves the stability of all helices.	adenosine a2a receptor;computer simulation;crystal structure;embedded system;embedding;ligands;linear programming relaxation;molecular dynamics;motion;principal component analysis;receptors, purinergic p1;requirement;robustness (computer science);sampling (signal processing);sampling - surgical action;self-replicating machine;tissue membrane;vergence;vii	Hui Wen Ng;Charles A. Laughton;Stephen W. Doughty	2013	Journal of chemical information and modeling	10.1021/ci300610w	crystallography;biochemistry;chemistry;bioinformatics;computational chemistry	Metrics	10.518245534320263	-62.010335168043646	73962
9b7b94e8795660e16d98be9f3e12903fda9e0aee	active machine learning for transmembrane helix prediction	high resolution;protein structure secondary;selected works;active learning;supervised machine learning;computational biology bioinformatics;protein structure;machine learning;feature extraction;transmembrane helix;artificial intelligence;bepress;algorithms;protein folding;combinatorial libraries;membrane protein;computer appl in life sciences;protein data bank;sequence analysis protein;databases protein;microarrays;membrane proteins;bioinformatics	About 30% of genes code for membrane proteins, which are involved in a wide variety of crucial biological functions. Despite their importance, experimentally determined structures correspond to only about 1.7% of protein structures deposited in the Protein Data Bank due to the difficulty in crystallizing membrane proteins. Algorithms that can identify proteins whose high-resolution structure can aid in predicting the structure of many previously unresolved proteins are therefore of potentially high value. Active machine learning is a supervised machine learning approach which is suitable for this domain where there are a large number of sequences but only very few have known corresponding structures. In essence, active learning seeks to identify proteins whose structure, if revealed experimentally, is maximally predictive of others. An active learning approach is presented for selection of a minimal set of proteins whose structures can aid in the determination of transmembrane helices for the remaining proteins. TMpro, an algorithm for high accuracy TM helix prediction we previously developed, is coupled with active learning. We show that with a well-designed selection procedure, high accuracy can be achieved with only few proteins. TMpro, trained with a single protein achieved an F-score of 94% on benchmark evaluation and 91% on MPtopo dataset, which correspond to the state-of-the-art accuracies on TM helix prediction that are achieved usually by training with over 100 training proteins. Active learning is suitable for bioinformatics applications, where manually characterized data are not a comprehensive representation of all possible data, and in fact can be a very sparse subset thereof. It aids in selection of data instances which when characterized experimentally can improve the accuracy of computational characterization of remaining raw data. The results presented here also demonstrate that the feature extraction method of TMpro is well designed, achieving a very good separation between TM and non TM segments.	acquired immunodeficiency syndrome;active learning (machine learning);algorithm;algorithm design;amino acid sequence;bmc bioinformatics;benchmark (computing);bioinformatics;cluster analysis;computation;experiment;feature extraction;ibm notes;image resolution;information;jackson;latent semantic analysis;machine learning;manuscripts;membrane potentials;membrane proteins;national institute of biomedical imaging and bioengineering (u.s.);nephrogenic systemic fibrosis;peptide sequence;protein data bank;scientific publication;silo (dataset);sparse matrix;subgroup;supervised learning;biomedical engineering field;contents - htmllinktype;henry;interest;mecarzole;protein protein interaction;statistical cluster	Hatice U. Osmanbeyoglu;Jessica A. Wehner;Jaime G. Carbonell;Madhavi Ganapathiraju	2010		10.1186/1471-2105-11-S1-S58	biology;computer science;bioinformatics;data science;machine learning;membrane protein	ML	10.498312131930682	-56.80591210346777	74123
38e2c84ae3f0d411673c570c60dc5889b2befbdb	toward automatic annotation of in situ mrna expression patterns of drosophila embryos	biology computing;expression pattern;organic compounds;multilayer perceptrons;image database;image classification;berkeley drosophila genome project;embryos;ontologies artificial intelligence;genetics;temporal information;embryo gene expression ontologies pattern classification neural networks multi layer neural network multilayer perceptrons optimization methods image databases image generation;automatic annotation;drosophila melanogaster;pattern classification;macromolecules;image classification biological techniques ontologies artificial intelligence biology computing organic compounds macromolecules genetics multilayer perceptrons;multi layer perceptron;object classification;berkeley drosophila genome project mrna expression pattern image drosophila melanogaster embryogenesis spatial temporal information gene expression image database image ontology annotation process pattern classification template based global matching multiobjective classification based neural network multilayer perceptron;biological techniques;mrna expression;gene expression pattern;neural network	The in situ mRNA hybridization gene expression pattern images of Drosophila melanogaster in the course of embryogenesis provide spatial-temporal information of the expression patterns of a target gene. This information is critical for understanding the roles of genes during the development of embryos. Currently, the annotation of these images is often done by manually assigning a subset of image ontology terms to the images. This approach is time consuming and depends heavily on the consistency of the experts. Alternatively, if the annotation process can be automated or semi-automated, efficiency and consistency are likely to be greatly enhanced. We formulate this task as a pattern classification problem, and present preliminary results. We consider both the template-based global matching and the multi-objective classification based on neural networks (e.g. multi-layer perceptron). We develop a method to combine them to optimize efficiency and accuracy. This method has been applied to the gene expression pattern image database generated by the Berkeley Drosophila Genome Project.	artificial neural network;gene expression programming;layer (electronics);multilayer perceptron;semiconductor industry	Jie Zhou;Hanchuan Peng	2005	2005 IEEE Computational Systems Bioinformatics Conference - Workshops (CSBW'05)	10.1109/CSBW.2005.134	macromolecule;biology;embryo;contextual image classification;computer science;bioinformatics;machine learning;data mining;multilayer perceptron;genetics;artificial neural network	Comp.	10.27376432134226	-55.079085366598875	74134
84bc4ed3eeb1966fda7074a444bf81143a035804	a program for individual and population optimal design for univariate and multivariate response pharmacokinetic-pharmacodynamic models	d optimal design;information content;data analysis;multivariate response;optimal design;pharmacokinetics;parameter estimation;pharmacodynamics	The design of pharmacokinetic and pharmacodynamic experiments concerns a number of issues, among which are the number of observations and the times when they are taken. Often a model is used to describe these data and the pharmacokinetic-pharmacodynamic behavior of a drug. Knowledge of the data analysis model at the design stage is beneficial for collecting patient data for parameter estimation. A number of criteria for model-oriented experiments, which maximize the information content of the data, are available. In this paper we present a program, Popdes, to investigate the D-optimal design of individual and population multivariate response models, such as pharmacokinetic-pharmacodynamic, physiologically based pharmacokinetic, and parent drug and metabolites models. A pre-clinical and clinical pharmacokinetic-pharmacodynamic model describing the concentration-time profile and effect of an oncology compound in development is used for illustration.	estimation theory;experiment;optimal design;pharmacodynamics;population parameter;self-information;stage is;the times	Ivelina Gueorguieva;Kayode Ogungbenro;Gordon Graham;Sophie Glatt;Leon Aarons	2007	Computer methods and programs in biomedicine	10.1016/j.cmpb.2007.01.004	econometrics;computer science;optimal design;data mining;mathematics;statistics	AI	13.684956990246892	-53.9298306284589	74140
09a503095db2d68b439e48d67481399198ed0e5b	recurrent models of visual attention		Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.	analysis of algorithms;artificial neural network;baseline (configuration management);canonical account;computation;computer vision;convolutional neural network;image resolution;network model;pixel;recurrent neural network;reinforcement learning	Volodymyr Mnih;Nicolas Heess;Alex Graves;Koray Kavukcuoglu	2014			computer vision;computer science;artificial intelligence;machine learning;deep learning;convolutional neural network	ML	23.597318677033332	-55.750028969917466	74184
9b2f1db30c4341e1c62e237dd1fa6125462ecc29	an optimal alignment of proteins energy characteristics with crisp and fuzzy similarity awards	environmental influence;biology computing;search engine;optimisation;conformational modifications;protein family;molecular mechanics theory;molecular configurations;energy adapted smith waterman method;fuzzy set theory;energy profile;energy levels;protein structure;proteins;energy adapted smith waterman method optimal alignment protein energy crisp similarity fuzzy similarity awards energy profile protein molecules conformational modifications molecular structures biochemical reactions molecular mechanics theory optimization;molecular biophysics;protein energy;crisp similarity;optimization;biochemical reactions;optimal alignment;molecular mechanics;proteins optimization biology computing molecular biophysics fuzzy set theory;fuzzy similarity awards;biochemistry;molecular structures;protein molecules;molecular structure;proteins biochemistry biology computing fuzzy set theory molecular biophysics molecular configurations optimisation	We discuss the usage of constant and fuzzy similarity awards while establishing an optimal alignment between energy characteristics of two compared protein energy profiles. Single protein energy profile is a set of energy characteristics of various types of energy. The energy profile is determined for a given protein structure. We use these profiles to find protein molecules of the same structural protein family and inspect conformational modifications in their molecular structures as an effect of biochemical reactions or environmental influences. Energy profiles are received in the computational process based on the molecular mechanics theory. Afterwards, these profiles can be stored in the special purpose database (EDB) and used by the search engine to find similar fragments of protein structures on the energy level. To optimize the alignment path we use modified, energy-adapted Smith-Waterman method with one of the tested similarity awards.	bus (computing);computation;energy level;energy profile (chemistry);molecular mechanics;protein family;smith–waterman algorithm;web search engine	Dariusz Mrozek;Bozena Malysiak-Mrozek;Stanislaw Kozielski	2007	2007 IEEE International Fuzzy Systems Conference	10.1109/FUZZY.2007.4295590	protein structure;molecular geometry;molecule;molecular mechanics;bioinformatics;energy level;mathematics;fuzzy set;protein family;search engine;energy profile;molecular biophysics	Comp.	12.759711390083874	-61.40842216287322	74192
d0266786a444a4679f1bd50a609fa495cbb3b2df	support vector machines-based quantitative structure-property relationship for the prediction of heat capacity	heat capacity;support vector machine	The support vector machine (SVM), as a novel type of learning machine, for the first time, was used to develop a Quantitative Structure-Property Relationship (QSPR) model of the heat capacity of a diverse set of 182 compounds based on the molecular descriptors calculated from the structure alone. Multiple linear regression (MLR) and radial basis function networks (RBFNNs) were also utilized to construct quantitative linear and nonlinear models to compare with the results obtained by SVM. The root-mean-square (rms) errors in heat capacity predictions for the whole data set given by MLR, RBFNNs, and SVM were 4.648, 4.337, and 2.931 heat capacity units, respectively. The prediction results are in good agreement with the experimental value of heat capacity; also, the results reveal the superiority of the SVM over MLR and RBFNNs models.	learning disorders;learning to rank;linear iga bullous dermatosis;molecular descriptor;multiple myeloma;nonlinear system;plant roots;quantitative structure–activity relationship;radial (radio);radial basis function;root-finding algorithm;support vector machine	C. X. Xue;Ruisheng Zhang;Huanxiang Liu;Mancang Liu;Zhide Hu;Bo Tao Fan	2004	Journal of chemical information and computer sciences	10.1021/ci049934n		ML	12.737170358247281	-57.15606400653402	74266
0065afd4e72234cb5ef1a3e08e49e8a56d640aab	blind image quality assessment with semi-supervised learning and fuzzy logic	blind image quality assessment;fuzzy logic;natural scene statistics;semi-supervised lle	Blind image quality assessment (BIQA) is a challenging task due to the difficulties in extracting quality-aware features and modeling the relationship between the features and the visual quality. In this pa- per, we propose a semi-supervised and fuzzy (S 2 F) framework for BIQA. First, we formulate the fuzzy process of subjective quality assessment by using fuzzy logic. Secondly, we introduce the semi-supervised local lin- ear embedding (SS-LLE) to learn the mapping from the features to the truth values using both the labeled and unlabeled images. Experimen- tal results on two benchmarking databases demonstrate the effectiveness and promising performance of the proposed S 2 F framework for BIQA.	fuzzy logic;image quality;semi-supervised learning;supervised learning	Ning Mei;Fei Gao;Wen Lu;Xinbo Gao	2013		10.1007/978-3-642-42057-3_24	computer science;artificial intelligence;machine learning;data mining	Vision	22.910811644925055	-58.02892572887333	74326
88b824698a68078417809d1925c2afeae9718c96	effect of low-field high-frequency nspefs on the biological behaviors of human a375 melanoma cells		Objective: To evaluate the effect of low-field high-frequency nanosecond pulsed electric fields (nsPEFs) on multiple biological behaviors of human A375 melanoma cells and to optimize suitable parameters for further study and clinical use. Methods: An nsPEF generator was developed to generate appropriate pulses. Cell apoptosis and the cell cycle were evaluated by flow cytometry. The CCK-8 assay was performed to explore the effect of nsPEFs on the viability of A375 melanoma cells. Cell migration was assessed using a Transwell Boyden Chamber. The proliferation of A375 melanoma cells was determined by the cloning efficacy test. Furthermore, the nude mouse tumorigenicity assay was used to detect the effectiveness of nsPEFs in vivo. Results:  The nsPEFs with our tested parameters failed to induce apoptosis of A375 melanoma cells, though nsPEFs with high pulse duration (500 ns) induced necrosis. However, the viability and migration of A375 melanoma cells were significantly inhibited by nsPEFs. nsPEFs also suppressed the proliferation of A375 melanoma cells by restricting cells in G0/G1 phase. Moreover, animal experiments demonstrated that nsPEFs inhibited the growth of melanoma in vivo. Conclusion: Low-field high-frequency nsPEFs failed to induce apoptosis but effectively inhibited the growth of melanoma via affecting other biological behaviors of melanoma cells, such as cell viability, proliferation, and migration. Significance: This study investigated the influence of low-field high-frequency nsPEFs on melanoma through evaluating their effects on multiple biological behaviors and is helpful in the treatment of melanoma and other tumors.	apoptosis;behavior;cell cycle;cell survival;electrophoresis, gel, pulsed-field;experiment;flow cytometry;mice, nude;necrosis;neoplasms;pulse duration;sincalide;tumorigenicity;video-in video-out;electric field;melanoma;nanosecond	Bingyu Zhang;Dongdong Kuang;Xuefeng Tang;Yan Mi;Qing Qing Luo;G. I. Song	2018	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2017.2784546	electronic engineering;cell migration;necrosis;apoptosis;computer science;viability assay;cell cycle;melanoma;cancer research;nude mouse	Visualization	10.884386537237344	-65.35850980624706	74424
07ca470ed3be3a476b6fc1917bbbf4182846d1db	transforming sensor data to the image domain for deep learning — an application to footstep detection		Convolutional Neural Networks (CNNs) have become the state-of-the-art in various computer vision tasks, but they are still premature for most sensor data, especially in pervasive and wearable computing. A major reason for this is the limited amount of annotated training data. In this paper, we propose the idea of leveraging the discriminative power of pre-trained deep CNNs on 2-dimensional sensor data by transforming the sensor modality to the visual domain. By three proposed strategies, 2D sensor output is converted into pressure distribution imageries. Then we utilize a pre-trained CNN for transfer learning on the converted imagery data. We evaluate our method on a gait dataset of floor surface pressure mapping. We obtain a classification accuracy of 87.66%, which outperforms the conventional machine learning methods by over 10%.	computer vision;convolutional neural network;deep learning;machine learning;modality (human–computer interaction);pervasive informatics;sensor;wearable computer;wearable technology	Monit Shah Singh;Vinaychandran Pondenkandath;Bo Zhou;Paul Lukowicz;Marcus Liwicki	2017	2017 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2017.7966182	artificial intelligence;convolutional neural network;artificial neural network;discriminative model;wearable computer;machine learning;visualization;deep learning;pattern recognition;feature extraction;pressure sensor;computer science;computer vision	AI	24.088381262339635	-59.34941805987189	74678
4962d375ce9b8cb086c60aefd46bc53d75da5f9f	binary formal inference-based recursive modeling using multiple atom and physicochemical property class pair and torsion descriptors as decision criteria	physicochemical properties	Analysis of a large amount of information, typically generated by high-throughput screening, is a very difficult task. To address this problem, we have developed binary formal inference-based recursive modeling using atom and physicochemical property class pair and torsion descriptors. Recursive partitioning is an exploratory technique for identifying structure in data. The implemented algorithm utilizes a statistical hypothesis testing, similar to Hawkins' formal inference-based recursive modeling program, to separate a data set into two homogeneous subsets at each splitting node. This process is repeated recursively until no further separation can occur. Our implementation of recursive partitioning differs from previously reported approaches by employing a method to extract multiple features at each splitting node. The method was examined for its ability to distinguish random and real data sets. The effect of including a single descriptor and multiple descriptors in the splitting descriptor set was also studied. The method was tested using 27 401 National Cancer Institute (NCI) compounds and their pGI50 (-log(GI50)) against the NCI-H23 cell line. The analyses show that partitioning using multiple descriptors is advantageous in analyzing the structure-activity relationship information.		Sung Jin Cho;C. Frank Shen;Mark A. Hermsmeier	2000	Journal of chemical information and computer sciences	10.1021/ci9908190	combinatorics;discrete mathematics;chemistry;computer science;machine learning;mathematics	Comp.	13.229054353654508	-56.28751087919	74816
e9c04ca31b94c2154c0361ce134f0a7da187a1f4	cytotoxicity of synthesized iron oxide nanoparticles: toward novel biomarkers of colon cancer	au iron oxide nanoparticle cytotoxicity biological analysis platform early colon cancer detection magnetic separation magnetized markers microfluidic structure biosensors superparamagnetic iron oxide nanoparticles spio np purified dna aptamer physicochemical results tem xrd hct116 cell line biomarker concentration human cell viability;magnetic resonance imaging nanoparticles cancer magnetic separation colon iron magnetic field measurement;x ray diffraction biomems biosensors cancer cellular biophysics chemical sensors dna gold iron compounds magnetic particles microfluidics nanomedicine nanoparticles patient diagnosis superparamagnetism toxicology transmission electron microscopy	In this paper we present the preliminary results of a novel biological analysis platform for early colon cancer detection using magnetic separation of magnetized markers. The platform consists of a microfluidic structure integrated with biosensors. Super-Paramagnetic Iron Oxide nanoparticles (SPIO-NPs) were functionalized with purified DNA Aptamer and their synthesis is described. In this paper, we also present the physicochemical results of the synthesized SPIO/Au-NPs characterized by TEM and XRD. Toxicity of our synthesized biomarkers on HCT116 cell line is discussed. Based on our findings, a concentration of 1mg/ml of our biomarkers added to 5 × 105 cells per well has no effect the viability of the human cells even after 24 hours.	biological markers;biosensors;cancer detection;colon carcinoma;colon classification;cultured cell line;gold;guilty gear xrd;leukemia, b-cell;microfluidics;spio nanoparticle;ferric oxide	Mojgan Ahmadzadeh-Raji;M. Amara;Ghasem Amoabediny;Parviz Tajik;A. Barin;Sebastian Magierowski;Ebrahim Ghafar-Zadeh	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6945040	magnetic nanoparticles;nanotechnology	Visualization	11.672866769511707	-65.08371331321653	74897
798ae3be38148fa249d3bb77c89a97b1e7e71bb2	homology model directed alignment selection for comparative molecular field analysis: application to photosystem ii inhibitors	protein design;molecular modeling;x ray crystal structure;amino acid sequence;photosystem ii;reaction center;homology modeling	The use of a computational docking protocol in conjunction with a protein homology model to derive molecular alignments for Comparative Molecular Field Analysis (CoMFA) was examined. In particular, the DOCK program and a model of the herbicidal target site, photosystem II (PSII), was used to derive alignments for two PSII inhibitor training sets, a set of benzo- and napthoquinones and a set of butenanilides. The protein design software in the QUANTA molecular modeling package was used to develop a homology model of spinach PSII based on the reported amino acid sequence and the X-ray crystal structure of the purple bacterium reaction center. The model is very similar to other reported PSII protein homology models. DOCK was then used to derive alignments for CoMFA modeling by docking the inhibitors in the PSII binding pocket. The molecular alignments produced from docking yielded highly predictive CoMFA models. As a comparison, the more traditional atom-atom alignments of the same two training sets failed to produce predictive CoMFA models. The general utilities of this application for homology model refinement and as an alternative scoring method are discussed.	1,4-benzoquinone;alignment;amino acid sequence;amino acids;boat dock;coefficient;complement system proteins;crystal structure;docking (molecular);docking -molecular interaction;energy, physics;homologous gene;homology (biology);homology modeling;jaccard index;ligands;photosystem ii;predictive modelling;quanta computer;quantitative structure-activity relationship;quantitative structure–activity relationship;quinones;refinement (computing);score;staphylococcal protein a;test set;algorithm;molecular modeling;spinach allergenic extract 50 mg/ml injectable solution	Mehran Jalaie;Jon A. Erickson	2000	Journal of computer-aided molecular design	10.1023/A:1008198211292	crystallography;biology;homology modeling;botany;chemistry;bioinformatics;photosystem ii;molecular model;peptide sequence;protein design;photosynthetic reaction centre	Comp.	11.417417104810482	-59.308444790809475	75158
342e52da4341c6cc7aeb73a7c2044e0d346d86a6	active scene recognition with vision and language	detectors;object recognition;support vector machines;training;image classification;inference mechanisms;computer vision;accuracy;object recognition computer vision image classification inference mechanisms;cognition;cognition training detectors equations support vector machines accuracy humans;humans;support vector machine;computer vision active scene recognition high level knowledge utilization object detectors classification performance reasoning module sensory module sensing process;dynamic scenes;active vision;activity recognition	This paper presents a novel approach to utilizing high level knowledge for the problem of scene recognition in an active vision framework, which we call active scene recognition. In traditional approaches, high level knowledge is used in the post-processing to combine the outputs of the object detectors to achieve better classification performance. In contrast, the proposed approach employs high level knowledge actively by implementing an interaction between a reasoning module and a sensory module (Figure 1). Following this paradigm, we implemented an active scene recognizer and evaluated it with a dataset of 20 scenes and 100+ objects. We also extended it to the analysis of dynamic scenes for activity recognition with attributes. Experiments demonstrate the effectiveness of the active paradigm in introducing attention and additional constraints into the sensing process.	active vision;activity recognition;experiment;finite-state machine;high-level programming language;information theory;outline of object recognition;programming paradigm;sensor;video post-processing	Xiaodong Yu;Cornelia Fermüller;Ching Lik Teo;Yezhou Yang;Yiannis Aloimonos	2011	2011 International Conference on Computer Vision	10.1109/ICCV.2011.6126320	support vector machine;computer vision;scene statistics;computer science;machine learning;pattern recognition;activity recognition	Vision	22.277447137944034	-62.64612288597704	75419
c9d80fb4716ee9c04d5a7aafd8f0536d2d662acc	large-scale stochastic scene generation and semantic annotation for deep convolutional neural network training in the robocup spl		Object detection and classification are essential tasks for any robotics scenario, where data-driven approaches, specifically deep learning techniques, have been widely adopted in recent years. However, in the context of the RoboCup standard platform league these methods have not yet gained comparable popularity in large part due to the lack of (publicly) available large enough data sets that involve a tedious gathering and error-prone manual annotation process. We propose a framework for stochastic scene generation, rendering and automatic creation of semantically annotated ground truth masks. Used as training data in conjunction with deep convolutional neural networks we demonstrate compelling classification accuracy on real-world data in a multi-class setting. An evaluation on multiple neural network architectures with varying depth and representational capacity, corresponding run-times on current NAO-H25 hardware, and required sampled training data is provided.	algorithm;artificial neural network;cognitive dimensions of notations;coherence (physics);computer vision;convolutional neural network;deep learning;generative model;glossary of computer graphics;ground truth;image segmentation;layout engine;motion estimation;multiclass classification;nao (robot);network architecture;object detection;pixel;robotics;simulation;synthetic intelligence	Timm Hess;Martin Mundt;Tobias Weis;Visvanathan Ramesh	2017		10.1007/978-3-030-00308-1_3	computer vision;rendering (computer graphics);convolutional neural network;object detection;deep learning;artificial neural network;ground truth;machine learning;computer science;data set;annotation;artificial intelligence	Vision	23.516928195111962	-53.316156729527336	75960
7f4efe590561bbde2df3d1728e94682f30e6d42b	improving the protein fold recognition accuracy of a reduced state-space hidden markov model	secondary structure prediction;fold recognition;protein fold recognition;protein function;hidden markov model;low complexity;hidden markov models;computational complexity;secondary structure;state space;protein classification;pharmaceutical industry;secondary structure alphabet;protein data bank;training algorithm	Fold recognition is a challenging field strongly associated with protein function determination, which is crucial for biologists and the pharmaceutical industry. Hidden Markov models (HMMs) have been widely used for this purpose. In this paper we demonstrate how the fold recognition performance of a recently introduced HMM with a reduced state-space topology can be improved. Our method employs an efficient architecture and a low complexity training algorithm based on likelihood maximization. The fold recognition performance of the model is further improved in two steps. In the first step we use a smaller model architecture based on the {E,H,L} alphabet instead of the DSSP secondary structure alphabet. In the second step secondary structure information (predicted or true) is additionally used in scoring the test set sequences. The Protein Data Bank and the annotation of the SCOP database are used for the training and evaluation of the proposed methodology. The results show that the fold recognition accuracy is substantially improved in both steps. Specifically, it is increased by 2.9% in the first step to 22%. In the second step it further increases and reaches up to 30% when predicted secondary structure information is additionally used and it increases even more and reaches up to 34.7% when we use the true secondary structure. The major advantage of the proposed improvements is that the fold recognition performance is substantially increased while the size of the model and the computational complexity of scoring are decreased.	alphabet;anatomy, regional;annotation;computational complexity theory;expectation–maximization algorithm;hidden markov model;markov chain;pharmacy (field);protein data bank;scop;score;setun;small;state space;test set;threading (protein sequence)	Christos Lampros;Costas Papaloukas;Costas P. Exarchos;Dimitrios I. Fotiadis;Dimitrios G. Tsalikakis	2009	Computers in biology and medicine	10.1016/j.compbiomed.2009.07.007	protein data bank;computer science;bioinformatics;state space;machine learning;pattern recognition;computational complexity theory;hidden markov model;protein secondary structure	AI	10.312594662224505	-54.99213972820904	76458
4ed7bb09b013974f5edeeeb5f871e879423e7184	device-free crowd sensing in dense wifi mimo networks: channel features and machine learning tools		The paper addresses the problem of passive crowd sensing in an indoor space by processing baseband radio signals originated from a dense WiFi network. Focusing on unmodified WiFi devices equipped with multi-antenna OFDM physical radio interfaces (IEEE 802.11n/ac), we investigate the selection of statistical features to measure the body-induced alterations of the channel state information (CSI) and we analyze their dependency over the space (antennas) and frequency domains. Different machine learning methods are compared and optimized to discriminate up to 5 people moving inside the smart space. We compare different solutions for classification and target counting based on feed-forward and recurrent neural networks based on long short term memory architecture (LSTM). Experiments with real subjects are conducted to validate the proposed approach. Results confirm that CSI feature selection is crucial to optimize the counting performance and space-frequency diversity needs to be exploited to provide high-accuracy sensing in complex indoor environments		Sanaz Kianoush;Stefano Savazzi;Monica Nicoli	2018	2018 15th Workshop on Positioning, Navigation and Communications (WPNC)	10.1109/WPNC.2018.8555798	electronic engineering;channel state information;long short term memory;mimo;feature selection;recurrent neural network;machine learning;computer science;orthogonal frequency-division multiplexing;communication channel;artificial intelligence;baseband	Mobile	21.369389327844132	-59.43434517755302	76461
c4b90a92abfa0b04ab00a662da442a772f76d8a3	a simple shape characteristic of protein-protein recognition	protein complex;center of mass;low resolution;binding site;protein structure;protein protein docking	MOTIVATION Observation of co-crystallized protein-protein complexes and low-resolution protein-protein docking studies suggest the existence of a binding-related anisotropic shape characteristic of protein-protein complexes.   RESULTS Our study systematically assessed the global shape of proteins in a non-redundant database of co-crystallized protein-protein complexes by measuring the distance of the surface residues to the protein's center of mass. The results show that on average the binding site residues are closer to the center of mass than the non-binding surface residues. Thus, the study directly detects an important and simple binding-related characteristic of protein shapes. The results provide an insight into one of the fundamental properties of protein structure and association.		George Nicola;Ilya A. Vakser	2007	Bioinformatics	10.1093/bioinformatics/btm303	center of mass;biology;protein structure;image resolution;bioinformatics;binding site;multiprotein complex;protein–protein interaction prediction	Comp.	11.485948393192464	-58.99164769810982	76658
5fad74213a0ae31ad9c50c8171a9d4ba79956e34	objects classification by learning-based visual saliency model and convolutional neural network		Humans can easily classify different kinds of objects whereas it is quite difficult for computers. As a hot and difficult problem, objects classification has been receiving extensive interests with broad prospects. Inspired by neuroscience, deep learning concept is proposed. Convolutional neural network (CNN) as one of the methods of deep learning can be used to solve classification problem. But most of deep learning methods, including CNN, all ignore the human visual information processing mechanism when a person is classifying objects. Therefore, in this paper, inspiring the completed processing that humans classify different kinds of objects, we bring forth a new classification method which combines visual attention model and CNN. Firstly, we use the visual attention model to simulate the processing of human visual selection mechanism. Secondly, we use CNN to simulate the processing of how humans select features and extract the local features of those selected areas. Finally, not only does our classification method depend on those local features, but also it adds the human semantic features to classify objects. Our classification method has apparently advantages in biology. Experimental results demonstrated that our method made the efficiency of classification improve significantly.	artificial neural network;behavior;biological neural networks;computer;computers;convolutional neural network;deep learning;deny (action);experiment;eye tracking;feature extraction;gain;high- and low-level;humans;information processing;inspiration function;mental processes;neuroscience discipline;physical object;pixel;statistical classification;interest	Na Li;Xinbo Zhao;Yongjia Yang;Xiaochun Zou	2016		10.1155/2016/7942501	computer vision;computer science;artificial intelligence;machine learning;one-class classification	ML	23.408968628120824	-57.94386504568181	76685
d065f765caaad8debb1526b738ddde4f9e179719	evolving biochemical systems	topology;kinetic theory biological cells mathematical model biochemistry equations biological system modeling substrates;topology biochemistry bioinformatics cellular biophysics genetic algorithms simulated annealing;simulated annealing biological compound interaction biological cells bioinformatics projects biological information topology kinetic rates chemical reactions np hard problem hybrid architecture genetic programming;simulated annealing;optimization biochemical systems genetic programming simulated annealing;genetic algorithms;cellular biophysics;biochemistry;bioinformatics	The interaction of biological compounds in cells has been enforced to a proper understanding by the numerous bioinformatics projects which contributed with a vast amount of biological information. The construction of biochemical systems (systems of chemical reactions) which include both topology and kinetic rates of the chemical reactions is an NP-hard problem. In this paper we propose a hybrid architecture which combines genetic programming and simulated annealing in order to generate and optimize both the topology (the network) and the reaction rates of a biochemical systems. Simulations and analysis of two real models show promising results for the proposed method.	algorithm;approximation;bioinformatics;biological system;computer simulation;genetic programming;mathematical optimization;simulated annealing	Silvia Rausanu;Crina Grosan;Zujian Wu;Ovidiu Pârvu;David R. Gilbert	2013	2013 IEEE Congress on Evolutionary Computation	10.1109/CEC.2013.6557753	computational biology;genetic algorithm;simulated annealing;computer science;bioinformatics;machine learning	Embedded	11.972715314956107	-53.17512186573258	77303
1473e6f2d250307f0421f1e2ea68b6485d3bd481	efficient feature extraction with simultaneous recurrent network for metric learning	unsupervised learning emotion recognition face recognition feature extraction image classification optical character recognition pipeline processing recurrent neural nets;metric learning classification pipeline dimensionality reduction simultaneous recurrent network architecture srn architecture complex classification tasks facial expression character recognition srn feature extraction;classification simultaneous recurrent network srn unsupervised learning feature extraction metric learning;measurement feature extraction classification algorithms computer architecture pipelines approximation algorithms neural networks	Metric learning has been successful in distance based classification tasks. However, metric learning tends to become increasingly complex with the increase of input feature dimensionality. Therefore, application of an efficient feature extraction and dimensionality reduction technique prior to metric learning has been pursued. Conventional feature extraction and dimensionality reduction techniques used for metric learning are usually hand-crafted and may not offer the best overall performance. Contemporary methods such as deep neural networks (DNNs) along with metric learning have been used for improved feature extraction and dimensionality reduction through learning. While DNNs have exhibited excellent performance, such deep structures tend to get cumbersome with increasing complexity of the task. Consequently, this work attempts to introduce an efficient feature extraction and dimensionality reduction technique using a simultaneous recurrent network (SRN) architecture. The proposed SRN architecture with metric learning is tested on solving two complex classification tasks: facial expression and character recognition. Our results show that the proposed SRN feature extraction and metric learning classification pipeline achieves superior performance in comparison to a DNN-based feature reduction and metric learning pipeline. We also demonstrate that the proposed SRN manages to utilize far less trainable parameters than the comparable DNN model such as stacked autoencoders (SAEs) for the same set of tasks.	artificial neural network;autoencoder;deep learning;dimensionality reduction;feature extraction;feature model;optical character recognition;recurrent neural network	Mahbubul Alam;L. Vidyaratne;Khan M. Iftekharuddin	2016	2016 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2016.7727333	speech recognition;feature;feature extraction;computer science;machine learning;pattern recognition;k-nearest neighbors algorithm;feature;dimensionality reduction	ML	22.578487436439936	-53.47387903755857	77326
d5060de36596fe069c7f9b85af794372fe6e8d4d	robust deep gaussian descriptor for texture recognition		Recently, second-order statistical modeling methods with convolutional features have shown impressive potential as image representation for vision tasks. Among them, bilinear convolutional neural network (B-CNN) has attracted a lot of attentions due to its simplicity and effectiveness. It captures the second-order local feature statistics via outer product, which approximately explores the covariance between convolutional features and achieves promising performance for texture recognition. In order to inherit the merits of B-CNN while further improving its performance, we introduce a Gaussian descriptor into B-CNN and propose a novel robust deep Gaussian descriptor (RDGD) method for texture recognition. We first compute Gaussian by using the output of outer product of B-CNN, and then embed it into the space of symmetric positive definite (SPD) matrices. Finally, matrix power normalization operation is employed to obtain more robust Gaussian descriptor. Experimental results on three texture databases demonstrate that RDGD is superior to its baseline B-CNN and the state-of-the-arts.		Jiahua Wang;Jianxin Zhang;Qiule Sun;Bin Liu;Qiang Zhang	2018		10.1007/978-3-030-00776-8_41	convolutional neural network;outer product;artificial intelligence;computer science;normalization (statistics);matrix (mathematics);bilinear interpolation;pattern recognition;gaussian;covariance;statistical model	Vision	24.599704539680186	-52.8118454883322	77818
38c6ef658b4703c015080e4fc76856b9a826fda3	effects from metal ion in tumor endothelial marker 8 and anthrax protective antigen: biolayer interferometry experiment and molecular dynamics simulation study	integrin;biolayer interferometry;cmg2;mm gbsa;tem8	One of the anthrax receptors, tumor endothelial marker 8 (TEM8), is reported to be a potential anticancer target due to its over-expression during tumor angiogenesis. To extend our BioLayer Interferometry study in PA-TEM8 binding, we present a computational approach to reveal the role of an integral metal ion on receptor structure and binding thermodynamics. We estimated the interaction energy between PA and TEM8 using computer simulation. Consistent with experimental study, computational results indicate the metal ion in TEM8 contributes significantly to the binding affinity, and PA-TEM8 binding is more favorable in the presence of Mg2+ than Ca2+ . Further, computational analysis suggests that the differences in PA-TEM8 binding affinity are comparable to the closely related integrin proteins. The conformation change, which linked to changes in activity of integrins, was not found in TEM8. In the present of Mg2+ , TEM8 remains in a conformation analogous to an integrin open (high-affinity) conformation. © 2017 Wiley Periodicals, Inc.		Zhe Jia;Christine Ackroyd;Tingting Han;Vibhor Agrawal;Yinling Liu;Kenneth Christensen;Brian Dominy	2017	Journal of computational chemistry	10.1002/jcc.24768	analytical chemistry;nanotechnology;integrin	Comp.	10.534541480174315	-62.63264760179837	77895
01ff405c6ee33ae590682817b6c295c483d58d45	psyphy: a psychophysics driven evaluation framework for visual recognition		By providing substantial amounts of data and standardized evaluation protocols, datasets in computer vision have helped fuel advances across all areas of visual recognition. But even in light of breakthrough results on recent benchmarks, it is still fair to ask if our recognition algorithms are doing as well as we think they are. The vision sciences at large make use of a very different evaluation regime known as Visual Psychophysics to study visual perception. Psychophysics is the quantitative examination of the relationships between controlled stimuli and the behavioral responses they elicit in experimental test subjects. Instead of using summary statistics to gauge performance, psychophysics directs us to construct item-response curves made up of individual stimulus responses to find perceptual thresholds, thus allowing one to identify the exact point at which a subject can no longer reliably recognize the stimulus class. In this article, we introduce a comprehensive evaluation framework for visual recognition models that is underpinned by this methodology. Over millions of procedurally rendered 3D scenes and 2D images, we compare the performance of well-known convolutional neural networks. Our results bring into question recent claims of human-like performance, and provide a path forward for correcting newly surfaced algorithmic deficiencies.	area striata structure;artificial neural network;benchmark (computing);computer vision;convolutional neural network;neural network simulation;protocols documentation;psychophysics;science;algorithm	Brandon RichardWebster;Samuel E. Anthony;Walter J. Scheirer	2018	IEEE transactions on pattern analysis and machine intelligence	10.1109/TPAMI.2018.2849989	artificial intelligence;convolutional neural network;computer vision;computer science;visual psychophysics;ask price;psychophysics;stimulus (physiology);machine learning;perception;visual perception	Vision	20.58097324640773	-53.53596884662843	78168
c1415e74b18ddaa792326ff5954de684ae3b2c9d	what are the receptive, effective receptive, and projective fields of neurons in convolutional neural networks?		In this work, we explain in detail how receptive fields, effective receptive fields, and projective fields of neurons in different layers, convolution or pooling, of a Convolutional Neural Network (CNN) are calculated. While our focus here is on CNNs, the same operations, but in the reverse order, can be used to calculate these quantities for deconvolutional neural networks. These are important concepts, not only for better understanding and analyzing convolutional and deconvolutional networks, but also for optimizing their performance in real-world applications.	artificial neural network;convolution;convolutional neural network;neural networks;neuron;semantic network	Hung Le;Ali Borji	2017	CoRR		convolutional neural network;machine learning;pattern recognition;artificial intelligence;artificial neural network;pooling;computer science;projective test;convolution;receptive field	ML	24.297885933404565	-53.21153622658035	78545
a84997fd08e2e67ac606dc442532168e4e1baba8	outlier-attenuating summarization for user-generated-video	accelerometers optimization acceleration smart phones encoding sensors feature extraction;acceleration value outlier attenuating summarization user generated video capture keyframe extraction problem smart phone collaborative sparse coding model l 1 2 regularization l 2 1 regularization sensors;video coding feature extraction image capture mobile computing smart phones;ac celerometers user generated video sparse coding	In this paper, the key-frame extraction problem for user-generated-videos which are captured by smart phones is investigated. A collaborative sparse coding model which incorporates the 1/2, 1 and Li, 2 regularization terms are proposed to select few key-frames while attenuating the influences of the outlier frames. Further, the sensors embedded in the smart phone is used to collect the acceleration values, which can be used to improve the performance of outlier-attenuations. Finally, a real dataset is constructed to test the proposed method and the experimental validation shows promising results.	elastic net regularization;embedded system;key frame;neural coding;sensor;smartphone;sparse matrix;user-generated content	Yulong Liu;Huaping Liu;Yunhui Liu;Fuchun Sun	2014	2014 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2014.6890273	computer vision;speech recognition;computer science;multimedia	Robotics	22.671847519456605	-58.98739107702054	78712
bdfb463573f79a623a3ed3fc4b1e209286a77e4f	using artificial bat sonar neural networks for complex pattern recognition: recognizing faces and the speed of a moving target	t technology general;ucl;discovery;theses;conference proceedings;digital web resources;face recognition;ucl discovery;open access;bf psychology;pattern recognition;ucl library;book chapters;open access repository;facial expression;qh301 biology;artificial neural network;neural network;ucl research	Two sets of studies examined the viability of using bat-like sonar input for artificial neural networks in complex pattern recognition tasks. In the first set of studies, a sonar neural network was required to perform two face recognition tasks. In the first task, the network was trained to recognize different faces regardless of facial expressions. Following training, the network was tested on its ability to generalize and correctly recognize faces using echoes of novel facial expressions that were not included in the training set. The neural network was able to recognize novel echoes of faces almost perfectly (above 96% accuracy) when it was required to recognize up to five faces. In the second face recognition task, a sonar neural network was trained to recognize the sex of 16 faces (eight males and eight females). After training, the network was able to correctly recognize novel echoes of those faces as ‘male’ or as ‘female’ faces with accuracy levels of 88%. However, the network was not able to recognize novel faces as ‘male’ or ‘female’ faces. In the second set of studies, a sonar neural network was required to learn to recognize the speed of a target that was moving towards the viewer. During training, the target was presented in a variety of orientations, and the network's performance was evaluated when the target was presented in novel orientations that were not included in the training set. The different orientations dramatically affected the amplitude and the frequency composition of the echoes. The neural network was able to learn and recognize the speed of a moving target, and to generalize to new orientations of the target. However, the network was not able to generalize to new speeds that were not included in the training set. The potential and limitations of using bat-like sonar as input for artifical neural networks are discussed.	artificial neural network;face;facial recognition system;mental orientation;orientation (graph theory);pattern recognition;sonar (symantec);speed (motion);test set	Itiel E. Dror;Faith L. Florer;Damien Rios;Mark Zagaeski	1996	Biological Cybernetics	10.1007/BF00194925	speech recognition;computer science;artificial intelligence;machine learning;communication;facial expression;artificial neural network	ML	23.664383789156844	-63.050864699789855	78900
2cbb769a9e2b1f5a377e4eeba25efa4c7d29f73b	topography of extracellular matrix mediates vascular morphogenesis and migration speeds in angiogenesis	cell movement;rabbits;animals;vascular endothelial growth factor a;systems biology;signal transduction;extracellular matrix;models biological;neoplasm transplantation;blood vessel;limbus corneae;cell shape;neovascularization physiologic;corneal neovascularization;reproducibility of results;wound healing;mathematical model;cell adhesion;computer simulation	The extracellular matrix plays a critical role in orchestrating the events necessary for wound healing, muscle repair, morphogenesis, new blood vessel growth, and cancer invasion. In this study, we investigate the influence of extracellular matrix topography on the coordination of multi-cellular interactions in the context of angiogenesis. To do this, we validate our spatio-temporal mathematical model of angiogenesis against empirical data, and within this framework, we vary the density of the matrix fibers to simulate different tissue environments and to explore the possibility of manipulating the extracellular matrix to achieve pro- and anti-angiogenic effects. The model predicts specific ranges of matrix fiber densities that maximize sprout extension speed, induce branching, or interrupt normal angiogenesis, which are independently confirmed by experiment. We then explore matrix fiber alignment as a key factor contributing to peak sprout velocities and in mediating cell shape and orientation. We also quantify the effects of proteolytic matrix degradation by the tip cell on sprout velocity and demonstrate that degradation promotes sprout growth at high matrix densities, but has an inhibitory effect at lower densities. Our results are discussed in the context of ECM targeted pro- and anti-angiogenic therapies that can be tested empirically.	angiogenic process;blood vessel;cerebrovascular disorders;contribution;elegant degradation;extracellular matrix;interaction;mathematical model;mathematics;morphogenesis;numerous;simulation;sprout (computer);the matrix;tissue fiber;topography;tumor angiogenesis;velocity (software development);density	Amy L. Bauer;Trachette L. Jackson;Yi Jiang	2009		10.1371/journal.pcbi.1000445	computer simulation;biology;extracellular matrix;vascular endothelial growth factor a;mathematical model;genetics;cell adhesion;systems biology;signal transduction;anatomy	ML	10.285770693395124	-65.87150012134225	79429
a3f726d7ba8aad3fa48f8a51c83475ef0b47090e	reducing the negative effect of defective data on driving behavior segmentation via a deep sparse autoencoder	training;maintenance engineering;testing;data mining;acceleration;feature extraction;principal component analysis	Analyzing driving behavior data is essential for developing driver assistance systems. Statistical segmentation is one of the important methods to realize the analysis. Driving behavior data actually include undesirable defects caused by external environment and sensor failures. Defects in the data cause a huge negative effect on the segmentation. In this paper, we showed that a feature extraction method based on a deep sparse autoencoder with fixed point (DSAE-FP) could reduce the negative effect of defective data in a driving behavior segmentation task. In the experiments, we used sticky hierarchical Dirichlet process hidden Markov model to segment the driving behavior. We compared the segmentation results using hidden features extracted by DSAE-FP and other comparative methods. Experimental results showed that segmentation results of non-defective dataset and defective dataset turned out most similar when DSAE-FP was used.	autoencoder;backpropagation;experiment;feature extraction;fixed point (mathematics);hidden markov model;markov chain;noise reduction;principal component analysis;software bug;software propagation;sparse matrix;sticky bit	Hailong Liu;Tadahiro Taniguchi;Kazuhito Takenaka;Yusuke Tanaka;Takashi Bando	2016	2016 IEEE 5th Global Conference on Consumer Electronics	10.1109/GCCE.2016.7800355	engineering;machine learning;pattern recognition;data mining	Robotics	18.06257542618291	-54.17635953790705	79436
e017b7ca32df28dcfad9db43e98045a4e55b48b0	quantitative structure-property relationships (qsprs) for the estimation of vapor pressure: a hierarchical approach using mathematical structural descriptors	quantitative structure property relationship;vapor pressure	A set of 379 molecular descriptors was calculated for use in hierarchical quantitative structure-property relationship (QSPR) modeling of vapor pressure for a structurally diverse database consisting of 469 chemicals. The hierarchical approach utilizes topostructural, topochemical, geometrical, and quantum chemical descriptors in a stepwise fashion to develop QSPR models. In this way, the relative roles of the various levels of descriptors can be examined. The results show that the easily calculated topological descriptors explain the majority of the variance and that the addition of geometrical and quantum chemical descriptors does not result in a significantly improved model.		Subhash C. Basak;Denise R. Mills	2001	Journal of chemical information and computer sciences	10.1021/ci000165r	chemistry;environmental chemistry;physical chemistry;vapor pressure	Graphics	12.396989155032912	-58.661844737748446	79689
9bd897921c562bd49ce2c50e29c9b851bfc36cc9	landscan: a natural language and computer vision system for analyzing aerial images	vision system;image understanding;computer vision;aerial image;image acquisition;natural language;surface model;natural language interface;natural language processing;active control;scene analysis	"""LandScan (LANguage Driven SCene ANalysis) is presented as an integrated vision system which covers most levels of both vision and natural language processing. Computations are both data-driven and query-driven. In the report we focus on the design of the vision and control modules. Future work will investigate in more detail the design of the natural language interface. The data-driven system employs active control of stereo cameras for image acquisition, and dynamically constructs a surface model from multiple aerial views of an urban scene. The query-driven system allows the user's natural language queries to focus analysis to pertinent regions of the scene. This is different than many image understanding systems which present a symbolic description of the entire scene regardless of what portions of that picture are actually of interest. 1 . I n t r o d u c t i o n The aim of our research on LandScan (LANguage Driven SCene ANalysis) is to develop a system capable of dynamically updating and maintaining a model of an urban world over multiple aerial views. The system will have a natural language front end through which users can query the system about a scene, and interactively assist the vision processing by restricting the analysis to those areas of the scene which are of current interest. A unique contribution of the work is that processing is both data-driven (bottom up, determined by sensor data) and query-driven (top clown, determined by user queries). The integration of both methods into one system can help overcome the shortcomings of each method employed independently. For example, if data-driven processing were able to segment a graph of edges derived from the image into several different connected components, querydriven information about what the system should be looking for can help impose structure, and a unique segmentation, upon the otherwise ambiguous data. The data-driven processing starts with stereo aerial images and reconstructs the surfaces in the scene. The query-driven processing constructs a logical representation of the scene using the queries to guide analysis. Highlevel scene analysis is performed using an Augmented Transition Network (ATN). This research was supported by the following grants: ARO DAA6-29-84-k-0061 AfOSR 82-NM-2W NSF MCS-8219190-CER NSF MCS 82-07294 AVRO DAAB07-84-K-F077 NIH 1-ROlHL-29085-01. As an example, suppose the user asks, """"Is there a car on the street?* The output from this query would be: the objects to be recognized, car and street; the relation ON which must hold between them; and an indication that this query is responded to by a yes/no answer with some explanation. The vision system would then be called to find a car and a street in the relation ON. The car and street would then be added to the Scene Model (if not there already) and the system would reply with an affirmative response. This paper will describe some related research, the implementation of the data-driven and query-driven portions of the LandScan system, and our plans for future work. A later paper will detail how natural language queries will interface with LandScan to guide the scene analysis. 2. Related Research A large corpus of research on aerial image understanding per se exists, and many general vision techniques are applicable to the aerial domain. Large aerial projects have been undertaken at USC [Nevatia 83], CMU [Herman 83] and SRI [Fischlcr 83]. However, very few integrated systems have been successfully implemented, and the best system architecture is still an open question. In particular, the problem of providing high-level feedback to the vision system has not been adequately addressed. ATN's have been used primarily in the domain of natural language [Bates 81], [Winograd 83]. A notable exception is the system designed by Tropf and Walter (Tropf 83] which uses an ATN model for the recognition of 3D objects with known geometries. The work of Talmy [Talmy 83] and Herskovits [Herskovits 82] influenced the design of both the topological relations in the models and the choice of linguistic attributes which must be associated with objects in order to ensure a robust and reliable natural language interface. 3 . Da ta -d r i ven System Imp lemen ta t i on a n d Results This section will describe the data-driven vision module?;, which must be effective in an urban world, seen from above. Urban scenes are characterized by an abundance of straight lines and planar surfaces. Under"""	aerial photography;augmented transition network;computation;computer vision;connected component (graph theory);coppersmith–winograd algorithm;high- and low-level;ibm notes;interactivity;natural language processing;natural language user interface;relevance;stereo camera;stereo cameras;systems architecture;top-down and bottom-up design;ven (currency)	Ruzena Bajcsy;Aravind K. Joshi;Eric Krotkov;Amy E. Zwarico	1985			computer vision;image-based modeling and rendering;machine vision;natural language user interface;computer science;natural language;computer graphics (images)	Vision	18.70311733463622	-57.27363748579497	79906
cff16ddd0b68e89f9d08bceb6592a0950dcdb6e2	nbo 6.0: natural bond orbital analysis program	natural bond orbital;wavefunction analysis;chemical bonding interactions	"""We describe principal features of the newly released version, NBO 6.0, of the natural bond orbital analysis program, that provides novel """"link-free"""" interactivity with host electronic structure systems, improved search algorithms and labeling conventions for a broader range of chemical species, and new analysis options that significantly extend the range of chemical applications. We sketch the motivation and implementation of program changes and describe newer analysis options with illustrative applications."""	chronic multifocal osteomyelitis;conferences;electronic structure;endianness;interactivity;molecular orbital;search algorithm	Eric D. Glendening;Clark R. Landis;Frank Weinhold	2013	Journal of computational chemistry	10.1002/jcc.23266	natural bond orbital;chemistry;computational chemistry;nanotechnology;physics	Theory	12.907531149742393	-62.73763760840053	80088
0677dd5377895b3c61cea0e6a143f38b84f1ebd7	multimedia super-resolution via deep learning: a survey		The recent phenomenal interest in convolutional neural networks (CNNs) must have made it inevitable for the super-resolution (SR) community to explore its potential. The response has been immense and in the last three years, since the advent of the pioneering work, there appeared too many works not to warrant a comprehensive survey. This paper surveys the SR literature in the context of deep learning. We focus on the three important aspects of multimedia namely image, video and multi-dimensions, especially depth maps. In each case, first relevant benchmarks are introduced in the form of datasets and state of the art SR methods, excluding deep learning. Next is a detailed analysis of the individual works, each including a short description of the method and a critique of the results with special reference to the benchmarking done. This is followed by minimum overall benchmarking in the form of comparison on some common dataset, while relying on the results reported in various works.	artificial neural network;benchmark (computing);convolutional neural network;deep learning;depth map;emoticon;gradient;image scaling;information;lr parser;loss function;marginal model;neural coding;optimization problem;programming paradigm;sparse matrix;super-resolution imaging;thrust;tweaking;video;viz: the computer game;while	Khizar Hayat	2018	Digital Signal Processing	10.1016/j.dsp.2018.07.005		NLP	24.158295215937503	-55.090136450054345	80122
809b7332421ffd939e09cb15759d734aad8debb9	application of a novel iwo to the design of encoding sequences for dna computing	iwo algorithm;journal;encoding dna sequences;mathematical model;optimal design;selection effect;thermodynamics;dna computing;dna sequence;fitness function	Encoding and processing information in DNA-, RNA- and other biomolecule-based devices is an important requirement for DNA based computing with potentially important applications. To make DNA computing more reliable, much work has focused on designing the good DNA sequences. However, this is a bothersome task as encoding problem is an NP problem. In this paper, a new methodology based on the IWO algorithm is developed to optimize encoding sequences. Firstly, the mathematics models of constrained objective optimization design for encoding problems based on the thermodynamic criteria are set up. Then, a modified IWO method is developed by defining the colonizing behavior of weeds to overcome the obstacles of the original IWO algorithm, which cannot be applied to discrete problems directly. The experimental results show that the proposed method is effective and convenient for the user to design and select effective DNA sequences in silicon for controllable DNA computing.	dna computing	Xuncai Zhang;Yanfeng Wang;Guangzhao Cui;Ying Niu;Jin Xu	2009	Computers & Mathematics with Applications	10.1016/j.camwa.2008.10.038	dna sequencing;selection bias;peptide computing;computer science;bioinformatics;optimal design;theoretical computer science;mathematical model;mathematics;fitness function;dna computing;algorithm;statistics	HPC	12.017554316968582	-53.15177628138825	80262
60c46555519913e0331fd0687b4bec0a7d822748	evolving the topology of large scale deep neural networks		In the recent years Deep Learning has attracted a lot of attention due to its success in difficult tasks such as image recognition and computer vision. Most of the success in these tasks is merit of Convolutional Neural Networks (CNNs), which allow the automatic construction of features. However, designing such networks is not an easy task, which requires expertise and insight. In this paper we introduce DENSER, a novel representation for the evolution of deep neural networks. In concrete we adapt ideas from Genetic Algorithms (GAs) and Grammatical Evolution (GE) to enable the evolution of sequences of layers and their parameters. We test our approach in the well-known image classification CIFAR-10 dataset. The results show that our method: (i) outperforms previous evolutionary approaches to the generations of CNNs; (ii) is able to create CNNs that have state-of-the-art performance while using less prior knowledge (iii) evolves CNNs with novel topologies, unlikely to be designed by hand. For instance, the best performing CNN obtained during evolution has an unexpected structure using six consecutive dense layers. On the CIFAR-10 the best model reports an average error of 5.87% on test data.	artificial neural network;computer vision;convolutional neural network;deep learning;evolutionary algorithm;experiment;genetic algorithm;genetic operator;grammatical evolution;neural network software;test data;whole earth 'lectronic link	Filipe Assunção;Nuno Lourenço;Penousal Machado;Bernardete Ribeiro	2018		10.1007/978-3-319-77553-1_2	machine learning;genetic algorithm;convolutional neural network;computer science;artificial intelligence;artificial neural network;grammatical evolution;deep learning;contextual image classification;network topology;test data	Vision	21.1659555316015	-52.58025423335254	80511
d09d941868a7ab8c1b85c82462e185616053967e	relative von neumann entropy for evaluating amino acid conservation	amino acid;von neumann entropy;sequence analysis;multiple sequence alignment	The Shannon entropy is a common way of measuring conservation of sites in multiple sequence alignments, and has also been extended with the relative Shannon entropy to account for background frequencies. The von Neumann entropy is another extension of the Shannon entropy, adapted from quantum mechanics in order to account for amino acid similarities. However, there is yet no relative von Neumann entropy defined for sequence analysis. We introduce a new definition of the von Neumann entropy for use in sequence analysis, which we found to perform better than the previous definition. We also introduce the relative von Neumann entropy and a way of parametrizing this in order to obtain the Shannon entropy, the relative Shannon entropy and the von Neumann entropy at special parameter values. We performed an exhaustive search of this parameter space and found better predictions of catalytic sites compared to any of the previously used entropies.	amino acids;brute-force search;entropy (information theory);multiple sequence alignment;population parameter;quantum mechanics;quantum statistical mechanics;sequence analysis;shannon (unit)	Fredrik Johansson;Hiroyuki Toh	2010	Journal of bioinformatics and computational biology	10.1142/S021972001000494X	entropy power inequality;biology;shannon's source coding theorem;joint entropy;mathematical analysis;conditional quantum entropy;quantum relative entropy;amino acid;generalized relative entropy;binary entropy function;rényi entropy;transfer entropy;maximum entropy probability distribution;multiple sequence alignment;bioinformatics;principle of maximum entropy;quantum mutual information;sequence analysis;calculus;mathematics;maximum entropy thermodynamics;joint quantum entropy;configuration entropy;entropy in thermodynamics and information theory;entropy rate;min entropy;von neumann entropy;entropy	Comp.	14.575905531061416	-61.31080343092632	80536
1e7953159b674ab15207c35959e205355b60c6ca	comparison and combination of state-of-the-art techniques for handwritten character recognition: topping the mnist benchmark	statistical significance;pattern recognition;error rate;handwritten character recognition	Although the recognition of isolated handwritten digits has been a research topic for many years, it continues to be of interest for the research community and for commercial applications. We show that despite the maturity of the field, different approaches still deliver results that vary enough to allow improvements by using their combination. We do so by choosing four well-motivated state-of-the-art recognition systems for which results on the standard MNIST benchmark are available. When comparing the errors made, we observe that the errors made differ between all four systems, suggesting the use of classifier combination. We then determine the error rate of a hypothetical system that combines the output of the four systems. The result obtained in this manner is an error rate of 0.35% on the MNIST data, the best result published so far. We furthermore discuss the statistical significance of the combined result and of the results of the individual classifiers.	benchmark (computing);bit error rate;capability maturity model;handwriting recognition;mnist database;statistical classification	Daniel Keysers	2006	CoRR		speech recognition;word error rate;computer science;machine learning;pattern recognition;statistical significance	HCI	19.335963523949733	-59.01756040572389	80704
8c18a0389bacc1b9e5539b9bc9c6b3aba5073a7e	region-based activity recognition using conditional gan	activity recognition;deep learning;generative adversarial network;localization	We present a method for activity recognition that first estimates the activity performer's location and uses it with input data for activity recognition. Existing approaches directly take video frames or entire video for feature extraction and recognition, and treat the classifier as a black box. Our method first locates the activities in each input video frame by generating an activity mask using a conditional generative adversarial network (cGAN). The generated mask is appended to color channels of input images and fed into a VGG-LSTM network for activity recognition. To test our system, we produced two datasets with manually created masks, one containing Olympic sports activities and the other containing trauma resuscitation activities. Our system makes activity prediction for each video frame and achieves performance comparable to the state-of-the-art systems while simultaneously outlining the location of the activity. We show how the generated masks facilitate the learning of features that are representative of the activity rather than accidental surrounding information.	1:1 pixel mapping;activity recognition;black box;cascade device component;channel (digital image);error-tolerant design;estimated;evaluation;feature extraction;frame (physical object);giant axonal neuropathy 1;long short-term memory;loss function;mask data preparation;masks;neural network simulation;resuscitation procedure;sports;tracer;triticum turgidum subsp. durum x hordeum chilense;wounds and injuries	Xinyu Li;Yanyi Zhang;Jianyu Zhang;Yueyang Chen;Huangcan Li;Ivan Marsic;Randall S. Burd	2017	Proceedings of the ... ACM International Conference on Multimedia, with co-located Symposium & Workshops. ACM International Conference on Multimedia	10.1145/3123266.3123365	computer vision;feature extraction;deep learning;activity recognition;computer science;artificial intelligence;channel (digital image)	Robotics	22.639134455154668	-56.93426237380774	80975
b00e2ae9035f7be3d417b8dbdb7dc623126ffb88	a constraint-based approach to structure prediction for simplified protein models that outperforms other existing methods	metodo cuadrado menor;hydrophobic compound;methode moindre carre;modelo reticular;proteine;folding;least squares method;monomere;cubico;hidrofobicidad;monomer;conformation;hydrophobicity;monomero;logical programming;pliage;structure proteine;root mean square deviation;compose hydrophobe;protein structure;enrejado;cubique;conformacion;protein conformation;hydrophobicite;programmation logique;treillis;protein structure prediction;structure prediction;compuesto hidrofobo;doblado;face centered cubic;modele reticulaire;protein folding;proteina;protein;programacion logica;lattice model;cubics;lattice	Lattice protein models are used for hierarchical approaches to protein structure prediction, as well as for investigating general principles of protein folding. So far, one has the problem that either the lattice does not model real protein conformations with good quality, or there is no efficient method known for finding native conformations. We present a constraint-based method that largely improves this situation. It outperforms all existing approaches in lattice protein folding on the type of model we have chosen (namely the HP-model by Lau and Dill[34], which models the important aspect of hydrophobicity). It is the only exact method that has been applied to two different lattices. Furthermore, it is the only exact method for the face-centered cubic lattice. This lattice is important since it has been shown [38] that the FCC lattice can model real protein conformations with coordinate root mean square deviation below 2 A. Our method uses a constraint-based approach. It works by first calculating maximally compact sets of points (hydrophobic cores), and then threading the given HP-sequence to the hydrophobic cores such that the core is occupied by H-monomers.		Rolf Backofen;Sebastian Will	2003		10.1007/978-3-540-24599-5_5	protein structure;combinatorics;monomer	NLP	12.189987646765795	-61.47338851085306	81579
46e1d904c4d1679db417b723fd0549b4a0dea830	three-layer perceptron based classifiers for the partial shape classification problem	alignement;forme partielle;image processing;procesamiento imagen;classification;traitement image;perception multicouche;shape classification;alineamiento;pattern recognition;reconnaissance forme;perception;reseau neuronal;reconocimiento patron;clasificacion;red neuronal;alignment;neural network	Abstract   The question of classification robustness in the multi-network neural network based system for the partial shape classification problem is addressed. In order to increase the robustness in classification, an extension of the multi-network system and a new single network system are proposed. The extension increases the robustness by augmenting the training of the three-layer perceptrons in the system. The three-layer perceptron in the single network system is designed to detect the features in all of the pattern classes. In the test mode, the test pattern is hypothesized to belong to the pattern classes and the network response to the test pattern is used to determine the similarity scores for the hypothesized classes. Two partial shape classification experiments are designed to compare the performance of the original multinetwork system, the augmented training approach, and the single network system on exactly the same test set. The results indicate that there is a significant increase in the classification robustness in the proposed augmented training approach and the single network system.	multitier architecture;perceptron	Lalit Gupta;Jiesheng Wang;Alain Charles;Paul Kisatsky	1994	Pattern Recognition	10.1016/0031-3203(94)90019-1	computer vision;image processing;biological classification;computer science;artificial intelligence;machine learning;pattern recognition;perception	Vision	23.863688267354515	-61.888917635180604	81596
45432293a94144f372521a990b3ac87a0dffcbfb	estimating the domain of applicability for machine learning qsar models: a study on aqueous solubility of drug discovery molecules	solubility;aqueous;machine learning;drug discovery;domain of applicability;error bar;error estimation;gaussian process;bayesian modeling;random forest;ensemble;decision tree;support vector machine;ridge regression;distance	We investigate the use of different Machine Learning methods to construct models for aqueous solubility. Models are based on about 4000 compounds, including an in-house set of 632 drug discovery molecules of Bayer Schering Pharma. For each method, we also consider an appropriate method to obtain error bars, in order to estimate the domain of applicability (DOA) for each model. Here, we investigate error bars from a Bayesian model (Gaussian Process (GP)), an ensemble based approach (Random Forest), and approaches based on the Mahalanobis distance to training data (for Support Vector Machine and Ridge Regression models). We evaluate all approaches in terms of their prediction accuracy (in cross-validation, and on an external validation set of 536 molecules) and in how far the individual error bars can faithfully represent the actual prediction error.	bayesian network;cross-validation (statistics);direction of arrival;drug discovery;estimated;gaussian process;machine learning;normal statistical distribution;quantitative structure-activity relationship;quantitative structure–activity relationship;random forest;support vector machine;weatherstar;triangulation	Timon Schroeter;Anton Schwaighofer;Sebastian Mika;Antonius ter Laak;Detlev Sülzle;Ursula Ganzer;Nikolaus Heinrich;Klaus-Robert Müller	2007	Journal of computer-aided molecular design	10.1007/s10822-007-9125-z	support vector machine;computer science;machine learning;pattern recognition;relevance vector machine;statistics	ML	12.839186034413732	-57.07866648092702	82038
3d7ee6267ec218cdacf58dcf5c8d0a5866987634	computational fragment-based screening using rosettaligand: the sampl3 challenge	fragment based drug design;molecular docking;receptor flexibility;partial charges;scoring function	SAMPL3 fragment based virtual screening challenge provides a valuable opportunity for researchers to test their programs, methods and screening protocols in a blind testing environment. We participated in SAMPL3 challenge and evaluated our virtual fragment screening protocol, which involves RosettaLigand as the core component by screening a 500 fragments Maybridge library against bovine pancreatic trypsin. Our study reaffirmed that the real test for any virtual screening approach would be in a blind testing environment. The analyses presented in this paper also showed that virtual screening performance can be improved, if a set of known active compounds is available and parameters and methods that yield better enrichment are selected. Our study also highlighted that to achieve accurate orientation and conformation of ligands within a binding site, selecting an appropriate method to calculate partial charges is important. Another finding is that using multiple receptor ensembles in docking does not always yield better enrichment than individual receptors. On the basis of our results and retrospective analyses from SAMPL3 fragment screening challenge we anticipate that chances of success in a fragment screening process could be increased significantly with careful selection of receptor structures, protein flexibility, sufficient conformational sampling within binding pocket and accurate assignment of ligand and protein partial charges.		Ashutosh Kumar;Kam Y. J. Zhang	2012	Journal of computer-aided molecular design	10.1007/s10822-011-9523-0	simulation;bioinformatics;combinatorial chemistry	Comp.	10.427705327919888	-59.868250336001736	82458
67f605167fd3dfce4acedbd315210a447fa34a94	cognitive memory: human and machine	human memory;humans animals pattern recognition information retrieval pattern matching registers olfactory neural networks usa councils art;animals;object recognition;brain;art;cognitive systems;pediatrics;neural networks;neural nets;object recognition brain cognition cognitive systems content addressable storage neural nets;information retrieval;training;usa councils;visualization;artificial neural networks;registers;pattern matching;cognition;pattern recognition;autoassociative neural networks cognitive memory machine memory human memory pattern recognition;face;humans;neurons;machine memory;olfactory;autoassociative neural networks;content addressable storage;cognitive memory	This paper addresses three questions: how does human memory work? How could I build a memory like that? How could I use it to solve practical problems?	artificial intelligence;cognitive science;computer;control system;pattern recognition;speech recognition	Bernard Widrow;Mozziyar Etemadi	2009	2009 International Joint Conference on Neural Networks	10.1109/IJCNN.2009.5179086	face;cognition;visualization;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;machine learning;pattern matching;olfaction;processor register;memory;artificial neural network	Robotics	19.664338166774456	-65.41178494209043	82558
7a3080a0dc700e3831d97e1457ab302740cfa04c	comparison of atp binding sites using structure-based similarity methods and molecular interaction fields	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;binding site;computer applications in chemistry;theoretical and computational chemistry;computational biology bioinformatics;uk phd theses thesis;life sciences;uk research reports;medical journals;molecular interactions;europe pmc;documentation and information in chemistry;biomedical research;bioinformatics	Protein kinases represent one of the largest group of drug targets in humans. All kinase enzymes share a catalytic domain that binds ATP. In recent years, many small molecule ATP-competitive kinase inhibitors have been developed. However, the evolutionary relatedness and structural conservation of these targets often lead to unforeseen cross reactivity. The goal of this work was to develop structure based ATP binding site descriptors that reflect the pharmacological profile of a predefined set of inhibitor molecules. For this purpose, a 3-step procedure was applied. First, a multiple sequence alignment generated by Buzko and Shokat [1] was adopted and binding site residues were identified. Second, kinase crystal structures were collected from SwissProt searches with EC numbers and superimposed based on the aligned binding site C-alpha atoms. Finally, molecular interaction field (MIF) descriptors were calculated and compared using a fuzzy Dice coefficient modified for 3D Euclidean space. For reasons of comparability with MIF descriptors, sequence descriptors based on the alignment were calculated. Moreover, kinase-ligand interaction fingerprints based on the superimposition of co-crystallized Xâ€‘ray structures were computed and mapped on the alignment. A dataset of 38 kinase inhibitors with experimental data against 90 human kinases was used to assess the performance of the developed descriptors [2]. Performance was measured by ROC enrichments for sampled kinase structures and Neighborhood Behavior criteria [3] for the complete kinase panel. Here, the performance is based on a comparison of the pairwise distances of the binding site descriptors to the pairwise distances of the inhibitor’s pharmacological profiles. Overall, classification based on the sequence of the binding pocket performed best. However, sequencebased methods cannot detect unrelated kinases with similar pharmacological profile. This in turn is the strength of the MIF-based descriptors since these descriptors characterize the geometry of hot spots on the protein and are independent of sequence information. Visualization of binding site hot spots makes this method suitable to rationally optimize the selectivity profile of a compound.	automated theorem proving;coefficient of determination;crystal structure;enzyme commission number;fingerprint;interactome;multiple sequence alignment;receiver operating characteristic;selectivity (electronic);sørensen–dice coefficient;uniprot	Jan Dreher;Knut Baumann	2011		10.1186/1758-2946-3-S1-P34	biology;medicine;bioinformatics;binding site;data mining	Comp.	10.891175661359675	-58.900446490569124	83059
3387f727258d68b1a879086d620c3e75a9411fdd	the µ- and delta-opioid pharmacophore conformations of cyclic beta-casomorphin analogues indicate docking of the phe3 residue to different domains of the opioid receptors	hydrogen bond;low energy;amino acid;binding site;molecular dynamic simulation;electrostatic potential;force field;opioid receptor;transmembrane domain;spatial orientation	Cyclic beta-casomorphin analogues with a D-configured amino acid residue in position 2, such as Tyr-c[-Xaa-Phe-Pro-Gly-] and Tyr-c[-Xaa-Phe-D-Pro-Gly-] (Xaa = D-A2bu, D-Orn, D-Lys) were found to bind to the mu-opioid receptor as well as to the delta-opioid receptor, whereas the corresponding L-Xaa2 derivatives are nearly inactive at both. Low-energy conformers of both active and nearly inactive derivatives have been determined in a systematic conformational search or by molecular dynamics simulations using the TRIPOS force field. The obtained conformations were compared with regard to a model for mu-selective opiates developed by Brandt et al. [Drug Des. Discov., 10 (1993) 257]. Superpositions as well as electrostatic, lipophilic and hydrogen bounding similarities with the delta-opioid receptor pharmacophore conformation of t-Hpp-JOM-13 proposed by Mosberg et al. [J. Med. Chem., 37 (1994) 4371, 4384] were used to establish the probable delta-pharmacophoric cyclic beta-casomorphin conformations. These conformations were also compared with a delta-opioid agonist (SNC 80) and the highly potent antagonist naltrindole. These investigations led to a prediction of the mu- and delta-pharmacophore structures for the cyclic beta-casomorphins. Interestingly, for the inactive compounds such conformations could not be detected. The comparison between the mu- and delta-pharmacophore conformations of the cyclic beta-casomorphins demonstrates not only differences in spatial orientation of both aromatic groups, but also in the backbone conformations of the ring part. In particular, the differences on phi2 and psi2 (mu approximately 70 degrees, -80 degrees; delta approximately 165 degrees, 55 degrees) cause a completely different spatial arrangement of the cyclized peptide rings when all compounds are matched with regard to maximal spatial overlap of the tyrosine residue. Assuming that both the mu- and delta-pharmacophore conformations bind with the tyrosine residue in a similar orientation at the same transmembrane domain X of their receptors, the side chain of Phe3 as a second binding site has to dock with different domains.	aromatics;adrenergic beta-antagonists;adverse reaction to drug;amino acids;dld gene;docking (molecular);docking -molecular interaction;eighty;force field (chemistry);hydrogen;inactive - biochemical activity level;internet backbone;maximal set;molecular dynamics;opioid receptor;pharmacophore;phenylalanine;probability;ring device;sap netweaver;simulation;space perception;tripos;tyrosine;vertebral column;beta thalassemia;gamma-delta t-cell receptor;lipophilicity;naltrindole	Wolfgang Brandt;Matthias Stoldt;Heiko Schinke	1996	Journal of computer-aided molecular design	10.1007/BF00355043	biochemistry;stereochemistry;amino acid;chemistry;transmembrane domain;spatial disorientation;binding site;organic chemistry;force field;hydrogen bond;nuclear magnetic resonance;electric potential	Comp.	10.628527504185795	-62.22038507714359	83629
15a922fd9d865038eef53ad9d7680cf41b240698	relative advantage of touch over vision in the exploration of texture	vision problem;texture;image segmentation;surface texture image segmentation testing layout gabor filters radio frequency computer science image analysis visual perception humans;tactile receptive fields;brain models;texture segmentation;surface texture;image texture;computer vision;surface properties;visualization;brain modeling;computational modeling;touch;radio frequency;thesis;neurophysiological findings;tactile perception;biologically inspired texture segmentation algorithms texture segmentation scene analysis vision problem surface property neurophysiological findings tactile receptive fields;pixel;surface property;receptive field;book;vision;biologically inspired texture segmentation algorithms;image texture computer vision image segmentation;scene analysis	Texture segmentation is an effortless process in scene analysis, yet its mechanisms have not been sufficiently understood. A common assumption in most current approaches is that texture segmentation is a vision problem. However, considering that texture is basically a surface property, this assumption can at times be misleading. One interesting possibility is that texture may be more intimately related with touch than with vision. Recent neurophysiological findings showed that receptive fields for touch resemble that of vision, albeit with some subtle differences. To leverage on this, we tested how such distinct properties in tactile receptive fields can affect texture segmentation performance, as compared to that of visual receptive fields. Our main results suggest that touch has an advantage over vision in texture processing. We expect our findings to shed new light on the role of tactile perception of texture and its interaction with vision, and help develop more powerful, biologically inspired texture segmentation algorithms.	algorithm;computation;computational model;computer engineering;computer science;ecology;electrical engineering;email;experiment;statistical model	Yoon Ho Bai;Choonseog Park;Yoonsuck Choe	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4760961	image texture;surface finish;vision;computer vision;visualization;computer science;image segmentation;texture;computational model;receptive field;radio frequency;pixel;computer graphics (images)	Vision	23.032261951820324	-66.04225898587983	84145
f8f8f76fa8c1bef87e32e98c1ffdbf7daba02ca5	analysis of interhelical angle and sequential distance in proteins	high resolution;statistical potential;probability density;probability density function;sequential distance;satisfiability;protein structure;rigid body motion;statistical analysis;probability distribution;secondary structure;bivariate distribution;helix;protein;interhelical angle	This paper presents the results and discussion of statistical analysis of helix-helix interactions in proteins in terms of interhelical angle and sequential distance. The protein data includes 1290 high-resolution globular protein structures with less than 20 percent homology. We first define possible cases of interactions between helices and identify protein pairs that satisfy the conditions for each case. Two cases of helix-helix interactions are investigated using six parameters that can be obtained by considering the interactions as rigid-body motions. The computational results show that orientational preferences in helix packing are dominated by interactions between helices that are proximal in sequence. This bias is described with a univariate probability density function that depends only on the sequential distance between interacting helices. We also compute bivariate probability densities that depend on both interhelical orientation and sequential distance. The dominance of certain preferred orientations and sequential distance values in this bivariate distribution are also observed. Angle preferences persist even when the bivariate distribution is normalized by the univariate distribution. This statistical analysis provides quantitative probability distributions from which statistical potentials for secondary structure assembly can be derived.	least-angle regression	Sangyoon Lee	2008		10.1007/978-3-540-69848-7_86	probability density function;combinatorics;mathematics;geometry;statistics	ML	13.876134980887434	-58.41745180916005	84148
3e1dcdecd99261e4d9149deb967de01ce1e2de7c	pair-activity classification by bi-trajectories analysis	databases;motion analysis;image classification feature extraction;active database;image classification;testing;event detection;computer vision;distance measurement;weight measurement;feedback;pair activity classification;trajectory;feature extraction;spatial databases;classification algorithms;correlation;information analysis feedback testing spatial databases computer vision trajectory motion analysis power measurement weight measurement event detection;information analysis;granger causality test;computer vision pair activity classification bi trajectories analysis feature normalization procedure weighted correlation;algorithm design and analysis;bi trajectories analysis;weighted correlation;feature normalization procedure;power measurement	In this paper, we address the pair-activity classification problem, which explores the relationship between two active objects based on their motion information. Our contributions are three-fold. First, we design a set of features, e.g., causality ratio and feedback ratio based on the Granger Causality Test (GCT), for describing the pair-activities encoded as trajectory pairs. These features along with conventional velocity and position features are essentially of multi-modalities, and may be greatly different in scale and importance. To make full use of them, we then present a novel feature normalization procedure to learn the coefficients for weighting these features by maximizing the discriminating power measured by weighted correlation. Finally, we collected a pair-activity database of five categories, each of which consists of about 170 instances. The extensive experiments on this database validate the effectiveness of the designed features for pair-activity representation, and also demonstrate that the proposed feature normalization procedure greatly boosts the pair-activity classification accuracy.	causality;coefficient;database normalization;experiment;geometric complexity theory;high- and low-level;multi-touch;velocity (software development)	Yue Zhou;Shuicheng Yan;Thomas S. Huang	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2008.4587816	statistical classification;granger causality;algorithm design;computer vision;contextual image classification;feature extraction;computer science;trajectory;machine learning;pattern recognition;data mining;feedback;mathematics;software testing;data analysis;correlation;statistics	Vision	22.48978765221142	-60.13792277326598	84542
84b0178d1104c90697b58e0c7ea71ce225e33d02	simulation analysis of experimental design strategies for screening random compounds as potential new drugs and agrochemicals	experimental design;simulation analysis	Computer simulations have been performed to investigate the effectiveness of experimental design techniques when applied to pharmaceutical and agrochemical random screening. Two design algorithms have been investigated, a maximum dissimilarity technique (stepwise elimination) and an approach based on sampling clusters. Results show that the former technique tends to reduce the chances of finding active molecules quickly. In contrast, the cluster sampling approach can afford small improvements in the rate at which active molecules are identified. However, use of maximum dissimilarity methods may still be beneficial if a premium is placed on finding active molecules with unusual structures.	algorithm;coupled cluster;design of experiments;sampling (signal processing);simulation;stepwise regression	Robin Taylor	1995	Journal of Chemical Information and Computer Sciences	10.1021/ci00023a009	pharmacology;chemistry;toxicology;design of experiments	Metrics	12.054458737322935	-62.939596104417504	84863
2b19aa9bb9ce10923bb25144e432975e1397b365	combinatorics of nmr and esr spectral simulations		Pharmaceutical nitrone comprise condensates of an N-hydroxylamine and a physiological aldehyde, providing improved delivery and absorption, enhanced stability and reduced toxicity. Preferred physiological aldehydes are subject to endogenous cellular uptake transport, and include pyridoxal, pyridoxal phosphate, and heme-A. Essentially any physiologically compatible and pharmaceutically active hydroxylamine moiety may be incorporated, such as hydroxylamine moieties of prior pharmaceutical nitrones, and known pharmaceutically active hydroxylamines.	simulation	Krishnan Balasubramanian	1992	Journal of Chemical Information and Computer Sciences	10.1021/ci00008a006	combinatorics;discrete mathematics;pure mathematics	Theory	10.461551940091475	-63.40177366135857	84986
433f9ba4487d8beb93b69c0241dba49cbf847e15	a novel approach for trajectory feature representation and anomalous trajectory detection	raw data trajectory feature representation anomalous trajectory detection tracking algorithm malicious event stacked denoising autoencoder sda denoising process;decoding;training;trajectory;shape;trajectory noise reduction feature extraction noise training shape decoding;feature extraction;noise reduction;stacked denoising autoencoder anomalous trajectory detection feature representation;noise;signal denoising	Trajectories obtained from low level tracking algorithm provide an opportunity for us to analyze meaningful behaviors and monitor adverse or malicious events. How to abstract meaningful features from the raw data of trajectories is a challenge due to the high dimensionality and noise. In this paper, a novel approach, stacked denoising autoencoder(SDA) is applied to address this problem. This method can reduce the dimensionality of the trajectories significantly, so that they can be handled easily. More importantly, the denoising process of the SDA can capture the structure of the raw data, so the features they producing generalize well for detecting anomalous trajectories. The results of the numerical experiments prove the validity of the proposed approach.	algorithm;experiment;image noise;malware;noise reduction;numerical analysis;plasma cleaning;sensor	Wenhui Feng;Chongzhao Han	2015	2015 18th International Conference on Information Fusion (Fusion)		computer vision;machine learning;pattern recognition;mathematics	Robotics	18.091028619029277	-54.15989728716972	85387
802bc24e75f617b8dd09eeaa6388439a953df35b	conformational preferences of the potent dopamine reuptake blocker btcp and its analogs and their incorporation into a pharmacophore model	quantum mechanical calculation;biological activity;molecular mechanics;dopamine;structural similarity;x rays	Molecular mechanics calculations using MM3-92 and ab initio quantum mechanical calculations using SPARTAN 5.0 were performed on the structurally similar PCP and BTCP, in which only the latter has a cocaine-like pharmacological profile as a dopamine reuptake blocker. Calculations were also performed on BTCP analogs with a methyl group in various positions of the cyclohexane ring. The results for the cis-2-methyl compound, which retains good pharmacological activity, allowed us to determine that an aryl-axial conformer is the biologically active form for at least some of the compounds in this series. However, an aryl-equatorial conformer presents the identical pharmacophore, as shown by superposition of the two conformers. X-ray crystallographic structures were also obtained for BTCP and related compounds with a 2-methyl group on the cyclohexane ring, with reasonable agreement between the computational and experimental results. Superposition studies were performed with two rigid analogs of cocaine which illustrate the optimal orientations of the ammonium hydrogen for monoamine transporters. There is excellent agreement between a 'back-bridged' cocaine analog that is optimal as a dopamine reuptake blocker and the previously proposed biologically active conformer of methylphenidate. However, BTCP is found to be a better fit to the 'front-bridged' cocaine analog that is optimal for a serotonin reuptake blocker.	ammonium;analog;blood urea nitrogen measurement;clinical use template;cocaine;computation;contract agreement;cyclohexane;dopamine;hydrogen;jamie wilkinson;judy array;mental orientation;methyl group;methylphenidate;molecular mechanics;muscle rigidity;pharmacology;pharmacophore;phencyclidine;quantum mechanics;serotonin;substance-related disorders;ammonium lactate;monoamine;phenyl	Mark Froimowitz;Kuo-Ming Wu;Jason Rodrigo;Clifford George	2000	Journal of computer-aided molecular design	10.1023/A:1008144707255	pharmacology;biochemistry;stereochemistry;dopamine;chemistry;molecular mechanics;structural similarity;organic chemistry;biological activity	Comp.	10.932269727967098	-62.4151434933125	85389
5ef74e22d1ac1e8c3ec8adc32c03a5dad76e4b8a	recurrent attention for deep neural object detection		Recent advances in deep learning have achieved state-of-the-art results for object detection by replacing the traditional detection methodologies with deep convolutional neural network architectures. A contemporary technique that is shown to further improve the performance of these models on tasks ranging from optical character recognition and neural machine translation to object detection is based on incorporating an attention mechanism within the models. The idea behind the attention mechanism and its variations was to improve the information quality extracted for any confronted task by focusing on the most relevant parts of the input. In this paper we propose two novel deep neural architectures for object recognition that incorporate the idea of the attention mechanism in the well-known faster-RCNN object detector. The objective is to develop attention mechanisms that can be used for small objects detection as they appear when using Drones for covering sport events like bicycle races, football matches and rowing races. The proposed approaches include a class agnostic method that applies the same predetermined context for every class, and a class specific method which learns to include context that maximizes the class's precision individually for each class. The proposed methods are evaluated in the VOC2007 dataset, improving the performance of the baseline faster-RCNN architecture.	approximation algorithm;artificial neural network;baseline (configuration management);convolutional neural network;deep learning;experiment;information quality;information retrieval;neural machine translation;object detection;optical character recognition;outline of object recognition;unmanned aerial vehicle;whole earth 'lectronic link	Georgios Symeonidis;Anastasios Tefas	2018		10.1145/3200947.3201024	optical character recognition;information quality;architecture;ranging;computer science;convolutional neural network;machine learning;deep learning;machine translation;artificial intelligence;object detection	ML	24.465417114448424	-54.13861057040416	85506
796cf8d9fb84d6ceac18d6460de6149050127a93	a multiscale approach to sampling nascent peptide chains in the ribosomal exit tunnel	potential of mean force;molecular dynamic;sequence space;monte carlo	We present a new multiscale method that combines all-atom molecular dynamics with coarse-grained sampling, towards the aim of bridging two levels of physiology: the atomic scale of protein side chains and small molecules, and the huge scale of macromolecular complexes like the ribosome. Our approach uses all-atom simulations of peptide (or other ligand) fragments to calculate local 3D spatial potentials of mean force (PMF). The individual fragment PMFs are then used as a potential for a coarse-grained chain representation of the entire molecule. Conformational space and sequence space are sampled efficiently using generalized ensemble Monte Carlo. Here, we apply this method to the study of nascent polypeptides inside the cavity of the ribosome exit tunnel. We show how the method can be used to explore the accessible conformational and sequence space of nascent polypeptide chains near the ribosome peptidyl transfer center (PTC), with the eventual aim of understanding the basis of specificity for co-translational regulation. The method has many potential applications to predicting binding specificity and design, and is sufficiently general to allow even greater separation of scales in future work.	atom;bridging (networking);dna binding site;dental caries;ligands;molecular dynamics;monte carlo method;polypeptides;ribosomes;sampling (signal processing);sampling - surgical action;sensitivity and specificity;simulation;small molecule;translational regulation;physiological aspects	Vincent A. Voelz;P. Petrone;Vijay S. Pande	2009	Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing		small molecule;bioinformatics;molecular dynamics;ribosome;sequence space;potential of mean force;monte carlo method;ribosomal protein;binding selectivity;biology	Comp.	11.125472875648624	-61.66344096320026	85992
3d0379688518cc0e8f896e30815d0b5e8452d4cd	autotagging facebook: social network context improves photo annotation	databases;social network services;web sites face recognition social sciences computing;social context;facebook social network services face recognition context modeling machine vision humans broadcasting data visualization databases system testing;image understanding;contextual information;conditional random field facebook autotagging social network photo annotation contextual information automatic face recognition;social network;face recognition;photo sharing;social sciences computing;web sites;facebook;conditional random field;face;online social network;communities;facebook autotagging;automatic face recognition;labeling;photo annotation	Most personal photos that are shared online are embedded in some form of social network, and these social networks are a potent source of contextual information that can be leveraged for automatic image understanding. In this paper, we investigate the utility of social network context for the task of automatic face recognition in personal photographs. We combine face recognition scores with social context in a conditional random field (CRF) model and apply this model to label faces in photos from the popular online social network Facebook, which is now the top photo-sharing site on the Web with billions of photos in total. We demonstrate that our simple method of enhancing face recognition with social network context substantially increases recognition performance beyond that of a baseline face recognition system.	baseline (configuration management);computer vision;conditional random field;embedded system;facial recognition system;social network;world wide web	Zak Stone;Todd E. Zickler;Trevor Darrell	2008	2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2008.4562956	facial recognition system;face;social environment;labeling theory;computer science;machine learning;geometry;multimedia;internet privacy;world wide web;conditional random field;social network	Vision	20.675644736353753	-60.47317627843333	86027
5f4c193b924eacf5bb514093b296013992b090ec	cnn-based indoor path loss modeling with reconstruction of input images		Convolutional Neural Networks (CNNs) have shown surprisingly good performance in both classification and regression problems. Given a floor plan of a building and indoor measurement data of Wi-Fi received signal strength (RSS) at discrete locations, a CNN can be trained to approximate the underlying functions of path loss. In this article, we propose a novel CNN-based indoor path loss modeling approach. Based on a floor plan and measurement data, input images are generated for training a CNN, which can make predictions of the RSS of 5 GHz Wi-Fi in an indoor usage scenario. Experiment results show that CNNs can be used for indoor path loss modeling with the encouraging performance with the Root Mean Square Error (RMSE) of 3.9404 dBm and good generalization property.	approximation algorithm;convolution;convolutional neural network;dbm;glossary of computer graphics;neuron;numerical analysis;rss;request for proposal	Hong Cheng;Hyukjoon Lee;Shengjie Ma	2018	2018 International Conference on Information and Communication Technology Convergence (ICTC)	10.1109/ICTC.2018.8539415	signal strength;convolutional neural network;feature extraction;path loss;dbm;floor plan;computer science;mean squared error;pattern recognition;artificial intelligence	Robotics	19.60551445427106	-55.73868728649231	86069
dfdc29e513d2a5c9c30e67513b7da34f03a628be	the atom assignment problem in automated de novo drug design. 4. tests for site-directed fragment placement based on molecular complementarity	assignment problem;hydrogen bond;optimal method;drug design;electrostatic potential	Three previous papers in this series have outlined an optimization method for atom assignment in drug design using fragment placement. In this paper the procedure is rigorously tested on a selection of five ligand-protein co-crystals. The algorithm is presented with the molecular graph of the ligand, and the electrostatic/hydrophobic potential of the site, with the aim of creating a placement on the molecular graph which is as electrostatically complementary or hydrophobically similar to the site as possible. Various designer options were tested, including, where appropriate, hydrogen bonding and a restricted number of halogens. In most cases, the placement obtained was at least as good as the native ligand, if not significantly better.	algorithm;assignment problem;atom;complementarity theory;de novo transcriptome assembly;drug design;graph - visual representation;halogens;hydrogen bonding;ligands;mathematical optimization;molecular graph;paper;pyschological bonding	M. T. Barakat;Philip M. Dean	1995	Journal of computer-aided molecular design	10.1007/BF00124002	stereochemistry;chemistry;computational chemistry;assignment problem;hydrogen bond;nuclear magnetic resonance;electric potential;drug design	EDA	10.69139816135378	-60.19304555570621	86585
2a2c412d93d62956e7b2f14086bfe6d07fa22ce9	artwork identification from wearable camera images for enhancing experience of museum audiences		Recommendation systems based on image recognition could prove a vital tool in enhancing the experience of museum audiences. However, for practical systems utilizing wearable cameras, a number of challenges exist which affect the quality of image recognition. In this pilot study, we focus on recognition of museum collections by using a wearable camera in three different museum spaces. We discuss the application of wearable cameras, and the practical and technical challenges in devising a robust system that can recognize artworks viewed by the visitors to create a detailed record of their visit. Specifically, to illustrate the impact of different kinds of museum spaces on image recognition, we collect three training datasets of museum exhibits containing variety of paintings, clocks, and sculptures. Subsequently, we equip selected visitors with wearable cameras to capture artworks viewed by them as they stroll along exhibitions. We use Convolutional Neural Networks (CNN) which are pre-trained on the ImageNet dataset and fine-tuned on each of the training sets for the purpose of artwork identification. In the testing stage, we use CNNs to identify artworks captured by the visitors with a wearable camera. We analyze the accuracy of their recognition and provide an insight into the applicability of such a system to further engage audiences with museum exhibitions.	algorithm;artworks;bag-of-words model in computer vision;convolutional neural network;digital camera;domain adaptation;emoticon;image;imagenet;neural networks;planar graph;recommender system;wearable computer	Rui Zhang;Yusuf Tas;Piotr Koniusz	2018	CoRR		recommender system;convolutional neural network;machine learning;exhibition;artificial intelligence;human–computer interaction;wearable computer;computer science	HCI	24.223937187948874	-58.94678890733069	86780
02cffc2135050ca5ab8a153abf93e3763eb789d4	a cloud-based workflow approach for optimizing molecular docking simulations of fully-flexible receptor models and multiple ligands	drugs;cavity resonators;scientific workflow;biological system modeling;molecular docking simulations;computational modeling;proteins;fully flexible receptor model;cloud computing;data models	The use of conformations achieved from MolecularDynamics (MD) simulations in docking experiments is the mostaccurate approach to simulate the natural interactions betweenreceptor and ligands at molecular environments. However, suchsimulations are computational costly and their overall executionmay become unfeasible due to the large quantities of structuralinformation needed to represent a Fully-Flexible Receptor (FFR) model. The problem is even more challenging when FFR modelsare used to perform docking-based virtual screening in a largedatabase of ligands. This study aims at developing a cloud-basedworkflow to efficiently optimize docking experiments betweena FFR model and multiple ligands in two strategic ways: bydiscarding groups of unpromising MD conformations for specificligands at docking execution time, and by exploiting on-demandresources from the Microsoft Azure cloud platform. The proposedenvironment is built on e-Science Central, which is a powerfulcloud-based workflow enactment system designed to handlescientific high-throughput tasks. As a result, we expect to reducethe number of docking experiments per ligand without affectingthe quality of the produced models and, therefore, considerablydecreasing the time consumed by docking experiments.	algorithmic efficiency;cloud computing;cluster analysis;computation;computer simulation;docking (molecular);e-science;experiment;high-throughput computing;interaction;microsoft azure;molecular dynamics;optimizing compiler;processor affinity;run time (program lifecycle phase);throughput;virtual screening	Renata De Paris;Duncan Dubugras Alcoba Ruiz;Osmar Norberto de Souza	2015	2015 IEEE 7th International Conference on Cloud Computing Technology and Science (CloudCom)	10.1109/CloudCom.2015.43	data modeling;computational science;simulation;lead finder;cloud computing;computer science;bioinformatics;operating system;protein–ligand docking;computational model	HPC	13.160226002834104	-60.423819109253266	87741
fb437a1b7439fbb0800555d70a4f5a4ad6013b84	a study on the antipicornavirus activity of flavonoid compounds (flavones) by using quantum chemical and chemometric methods	computer programs;computers in chemistry	The AM1 semiempirical method is employed to calculate a set of molecular properties (variables) of 45 flavone compounds with antipicornavirus activity, and 9 new flavone molecules are used for an activity prediction study. Principal Component Analysis (PCA), Hierarchical Cluster Analysis (HCA), Stepwise Discriminant Analysis (SDA), and K-Nearest Neighbor (KNN) are employed in order to reduce dimensionality and investigate which subset of variables should be more effective for classifying the flavone compounds according to their degree of antipicornavirus activity. The PCA, HCA, SDA, and KNN methods showed that the variables MR (molar refractivity), B(9) (bond order between C(9) and C(10) atoms), and B(25) (bond order between C(11) and R(7) atoms) are important properties for the separation between active and inactive flavone compounds, and this fact reveals that electronic and steric effects are relevant when one is trying to understand the interaction between flavone compounds with antipicornavirus activity and the biological receptor. In the activity prediction study, using the PCA, HCA, SDA, and KNN methodologies, three of the 9 new flavone compounds studied were classified as potentially active against picornaviruses.	chemometrics;classification;cluster analysis;family picornaviridae;flavones;flavonoids;hepatocellular adenoma;hierarchical clustering;k-nearest neighbors algorithm;linear discriminant analysis;physical inactivity;principal component analysis;semi-empirical quantum chemistry method;stepwise regression;strand displacement amplification;subgroup;flavone	Jaime Souza;Fábio A. Molfetta;Káthia M. Honório;Regina H. A. Santos;Albérico B. F. da Silva	2004	Journal of chemical information and computer sciences	10.1021/ci030384n	stereochemistry;chemistry;machine learning;organic chemistry	ML	13.447027429018998	-56.471869958482436	88260
43fb130648e62807abb1e36163a2516fc67a4d11	automatic action recognition for assistive robots to support mci patients at home		This paper presents a novel approach for automatic human action recognition, focusing on user behaviour monitoring needs of assistive robots that aim to support Mild Cognitive Impairment (MCI) patients at home. Our action recognition method utilizes the humanu0027s skeleton joints information, extracted from a low-cost depth sensor mounted on a service robot. Herein, we extend the state of art EigenJoints descriptor to improve recognition robustness for a series of actions involved in common daily activities. Specifically, we introduce novel features, so as to take into account action specificities such as the jointsu0027 travelled distance and their evolution trend in subsequent frames to the reference one. In addition, we use information related to the useru0027s manipulated objects, taking into account that several actions may be similar, yet performed with different objects, as well as the fact that real, practical applications involve continuous input video streams rather than pre-segmented action sequences. Through experimental evaluation on the MSR Action3D dataset, our approach has been found to outperform the state of art in action recognition performance. Evaluation has also been performed on a custom dataset, providing further promising results for future practical applications of our overall action recognition framework.	assistive technology;microsoft research;range imaging;service robot;streaming media	Georgios Stavropoulos;Dimitrios Giakoumis;Konstantinos Moustakas;Dimitrios Tzovaras	2017		10.1145/3056540.3076185	robustness (computer science);human–computer interaction;simulation;robot;activity recognition;computer science;cognition;service robot	Vision	23.91347921413821	-59.201624368258926	88265
5e4d9f7ad1a44a76d5d181a23816546d6ca33bb3	in-depth question classification using convolutional neural networks		Convolutional neural networks for computer vision are fairly intuitive. In a typical CNN used in image classification, the first layers learn edges, and the following layers learn some filters that can identify an object. But CNNs for Natural Language Processing are not used often and are not completely intuitive. We have a good idea about what the convolution filters learn for the task of text classification, and to that, we propose a neural network structure that will be able to give good results in less time. We will be using convolutional neural networks to predict the primary or broader topic of a question, and then use separate networks for each of these predicted topics to accurately classify their sub-topics.	artificial neural network;computer vision;convolution;convolutional neural network;document classification;natural language processing;neural networks	Prudhvi Raj Dachapally;Srikanth Ramanam	2018	CoRR		machine learning;artificial intelligence;convolutional neural network;artificial neural network;convolution;computer science;contextual image classification	ML	23.098933200211327	-53.33120113465182	88363
48e7f5a2857a42402a0563f4c3ed992264b7206e	transfer learning for brain decoding using deep architectures		Is there a general representation of the information content of human brain, which can be extracted from the functional magnetic resonance imaging (fMRI) data? Is it possible to learn this representation automatically from big data sets by unsupervised learning methods? Is it possible to transfer this representation to learn and decode a set of cognitive states in other fMRI data sets? This study addresses partial answers to the above questions by using transfer learning in deep architectures. First, a hierarchical representation for fMRI data is learned from a large data set in Human Connectome Project (HCP) by a 3-layered stacked denoising autoencoder (SDAE). Then, the learned representations are used to train and recognize the cognitive states recorded by a relatively small data set of one-back repetition detection experiment. Results show that, it is possible to learn a general representation and transfer the learned representation of an fMRI data set to another dataset for brain decoding problem. The learned representation has a better discriminative power compared to the Pearson correlation features. Results also show us that deep neural networks transfer representations better than factor models commonly used in pattern recognition and neuroscience literature.	artificial neural network;autoencoder;big data;deep learning;human connectome project;independent computing architecture;multitier architecture;noise reduction;pattern recognition;principal component analysis;resonance;self-information;unsupervised learning	Burak Velioglu;Fatos T. Yarman-Vural	2017	2017 IEEE 16th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)	10.1109/ICCI-CC.2017.8109731	artificial neural network;autoencoder;transfer of learning;human connectome project;feature extraction;small data;unsupervised learning;data set;machine learning;artificial intelligence;computer science	ML	20.54355140217134	-54.523740469069445	88436
88356a43079eda96b1416bb12b8eca07a73d4f02	comparing neural-network scoring functions and the state of the art: applications to common library screening	software;databases pharmaceutical;roc curve;drug evaluation preclinical;humans;neural networks computer;molecular docking simulation	We compare established docking programs, AutoDock Vina and Schrödinger's Glide, to the recently published NNScore scoring functions. As expected, the best protocol to use in a virtual-screening project is highly dependent on the target receptor being studied. However, the mean screening performance obtained when candidate ligands are docked with Vina and rescored with NNScore 1.0 is not statistically different than the mean performance obtained when docking and scoring with Glide. We further demonstrate that the Vina and NNScore docking scores both correlate with chemical properties like small-molecule size and polarizability. Compensating for these potential biases leads to improvements in virtual screen performance. Composite NNScore-based scoring functions suited to a specific receptor further improve performance. We are hopeful that the current study will prove useful for those interested in computer-aided drug design.	artificial neural network;autodock;boat dock;docking (molecular);drug design;glide;hope (emotion);ligands;polarizability;schrödinger;scientific publication;score;scoring functions for docking;virtual desktop;virtual screening;chemical properties	Jacob D. Durrant;Aaron J. Friedman;Kathleen Roger;James Andrew McCammon	2013		10.1021/ci400042y	simulation;computer science;bioinformatics;machine learning;receiver operating characteristic	Metrics	10.768479753497195	-57.808318505248835	88663
b9a83d9eb5fbcda68023814cb2003202b131efdb	admet evaluation in drug discovery: 15. accurate prediction of rat oral acute toxicity using relevance vector machine and consensus modeling	computer applications in chemistry;theoretical and computational chemistry;computational biology bioinformatics;documentation and information in chemistry	BACKGROUND Determination of acute toxicity, expressed as median lethal dose (LD50), is one of the most important steps in drug discovery pipeline. Because in vivo assays for oral acute toxicity in mammals are time-consuming and costly, there is thus an urgent need to develop in silico prediction models of oral acute toxicity.   RESULTS In this study, based on a comprehensive data set containing 7314 diverse chemicals with rat oral LD50 values, relevance vector machine (RVM) technique was employed to build the regression models for the prediction of oral acute toxicity in rate, which were compared with those built using other six machine learning approaches, including k-nearest-neighbor regression, random forest (RF), support vector machine, local approximate Gaussian process, multilayer perceptron ensemble, and eXtreme gradient boosting. A subset of the original molecular descriptors and structural fingerprints (PubChem or SubFP) was chosen by the Chi squared statistics. The prediction capabilities of individual QSAR models, measured by q ext (2) for the test set containing 2376 molecules, ranged from 0.572 to 0.659.   CONCLUSION Considering the overall prediction accuracy for the test set, RVM with Laplacian kernel and RF were recommended to build in silico models with better predictivity for rat oral acute toxicity. By combining the predictions from individual models, four consensus models were developed, yielding better prediction capabilities for the test set (q ext (2) = 0.669-0.689). Finally, some essential descriptors and substructures relevant to oral acute toxicity were identified and analyzed, and they may be served as property or substructure alerts to avoid toxicity. We believe that the best consensus model with high prediction accuracy can be used as a reliable virtual screening tool to filter out compounds with high rat oral acute toxicity. Graphical abstractWorkflow of combinatorial QSAR modelling to predict rat oral acute toxicity.	adverse reaction to drug;approximation algorithm;chi;consistency model;drug discovery;ext js javascript framework;fingerprint;gaussian process;gradient boosting;k-nearest neighbors algorithm;lethal dose 50;leukemia, myelocytic, acute;machine learning;median graph;molecular descriptor;multilayer perceptron;normal statistical distribution;pubchem;quantitative structure-activity relationship;quantitative structure–activity relationship;radio frequency;random forest;relevance vector machine;subgroup;support vector machine;test set;video-in video-out;virtual screening	Tailong Lei;Youyong Li;Yunlong Song;Dan Li;Huiyong Sun;Tingjun Hou	2016		10.1186/s13321-016-0117-7	pharmacology;biology;toxicology;bioinformatics	ML	10.818222136372025	-56.11125450821432	89643
402df2bc83b9af38fb80489312a25a71188126b8	ssgan: secure steganography based on generative adversarial networks		How to design a secure steganography method is the problem that researchers have always been concerned about. Traditionally, the steganography method is designed in a heuristic way which does not take into account the detection side (steganalysis) fully and automatically. In this paper, we propose a new strategy that generates more suitable and secure covers for steganography with adversarial learning scheme, named SSGAN. The proposed architecture has one generative network called G, and two discriminative networks called D and S, among which the former evaluates the visual quality of the generated images for steganography and the latter assesses their suitableness for information hiding. Different from the existing work, we use WGAN instead of GAN for the sake of faster convergence speed, more stable training, and higher quality images, and also re-design the S net with more sophisticated steganalysis network. The experimental results prove the effectiveness of the proposed method.	algorithm;generative adversarial networks;heuristic;social network;steganalysis;steganography	Haichao Shi;Jing Dong;Wei Wang;Yinlong Qian;Xiaoyu Zhang	2017		10.1007/978-3-319-77380-3_51	information hiding;steganography;machine learning;computer science;robustness (computer science);discriminative model;artificial intelligence;pattern recognition;architecture;steganography tools;steganalysis;convergence (routing)	AI	19.464057015922236	-52.090579674104866	89721
f193d37a8e8ca0639089cf7952f9022954649278	adaptive sparse representation for analyzing artistic style of paintings	discriminative patches;dictionary learning;dct baseline;sparse representation	Inspired by the outstanding performance of sparse representation (SR) in a variety of image/video relevant classification and identification tasks, we propose an adaptive SR method for painting style analysis. Significantly improved over previous SR-based methods, which heavily rely on the comparison of query paintings, our method is able to authenticate or classify a single query painting based on the estimated decision boundary. Specifically, discriminative patches containing the most representative characteristics of the given samples are first extracted via exploiting the statistics of their representations on the discrete cosine transform (DCT) basis. Then, the strategy of adaptive sparsity constraint is enforced to make the dictionary trained on such patches more adaptive to the training samples than via previous SR techniques. Applying the learned dictionary, the query painting can be authenticated if both better denoising performance and higher kurtosis are obtained compared to the baseline estimated via applying the DCT basis; otherwise, it should be denied. Extensive experiments on our dataset comprised of paintings from van Gogh, his contemporaries, the Wacker forgery, and Monet demonstrate the effectiveness of our method.	authentication;baseline (configuration management);decision boundary;dictionary;discrete cosine transform;experiment;noise reduction;patch (computing);sparse approximation;sparse matrix	Zhi Gao;Mo Shan;Qingquan Li	2015	JOCCH	10.1145/2756556	speech recognition;computer science;machine learning;pattern recognition;sparse approximation	Vision	21.270487281917113	-55.691506019183684	90111
aac4963644ae0c8cd4acf448f306ecd3e8d59d9a	bio-inspired learning approach for electronic nose	e-nose;receptors;drift;biological;neural;noise;classification;adaptive;sensors;chemical;68	The high sensitivity, stability, selectivity and adaptivity of mailman olfactory system is a result of a large number of olfactory receptors feeding into extensive layers of neural processing units. Olfactory receptor cells (ORC) contribute significantly in the sense of smells. Bloodhounds have 4 billion ORC making them ideal for tracking while human has about 30 million ORC. E-nose stability, sensitivity and selectivity have been a challenging task. We hypothesize that appropriate signal processing with an increased number of sensory receptors can significantly improve odour recognition in e-nose. Adding physical receptors to e-nose is costly and can increase system complexity. Therefore, we propose an artificial olfactory receptor cells model inspired by neural circuits of the vertebrate olfactory system to improve e-nose performance. Secondly, we introduce and adaptation layer to cope with drift and unknown changes. The major layers in our model are the sensory transduction layer, sensory adaptation layer, artificial olfactory receptors layer and artificial olfactory cortex layer. Each layer in the proposed system is biologically inspired by the mammalian olfactory system. The experiments are executed using chemo-sensory arrays data generated over a 3-year period. The proposed model resulted in a better performance and stability compared to other models. However, e-nose stability, selectivity and sensitivity issues remain unsolved problems. Our paper provides a new approach to improve e-nose pattern recognition over a long period of time.	biological system;chaos theory;code smell;experiment;pattern recognition;selectivity (electronic);signal processing;transduction (machine learning)	Sanad Al-Maskari;Zhuoming Xu;Wenping Guo;Xiaoming Zhao;Xue Li	2018	Computing	10.1007/s00607-018-0604-y	artificial intelligence;machine learning;olfactory system;signal processing;olfactory receptor cell;electronic nose;sensory system;biological neural network;sensory adaptation;computer science;transduction (physiology);biological system	AI	17.76660465422088	-64.4377778628905	90329
44b26ae42bb2444df520949ca52b024edda95579	is pattern recognition a statistical problem?	pattern recognition	Human beings effortlessly perform sophisticated pattern recognitions tasks such as speech recognition, speaker recognition, signature matching, handwriting recognition, face recognition, etc. We have no simple way of dealing with these problems with the existing mathematical models, which are primarily based on the statistical methods. The answer to this dilemma lies in developing new models of computing, which are primarily motivated by our (though limited) understating of the structure and function of the biological neural networks. The objective of this talk is to justify the need for exploring new models of computing for the human pattern recognition tasks, using illustrations from speech and image processing tasks. 2nd Indian International Conference on Artificial Intelligence (IICAI-05)	artificial intelligence;artificial neural network;facial recognition system;handwriting recognition;image processing;mathematical model;pattern recognition;speaker recognition;speech recognition	Bayya Yegnanarayana	2005				Vision	23.113149795682414	-61.02836225608668	90627
333cab65fabf9a2f59b509e225cceaead2647682	automated identification and classification of stereochemistry: chirality and double bond stereoisomerism	2d coordinates;geometric isomerism;stereoisomers;chirality;report;enantiomers;double bond stereoisomerism;diastereomers;chiral center;stereochemistry	Stereoisomers have the same molecular formula and the same atom connectivity and their existence can be related to the presence of different three-dimensional (3D) arrangements. Stereoisomerism is of great importance in many different fields since the molecular properties and biological effects of the stereoisomers are often significantly different. Most drugs for example, are often composed of a single stereoisomer of a compound, and while one of them may have therapeutic effects on the body, another may be toxic. A challenging task is the automatic detection of stereoisomers using line input specifications such as SMILES or InChI since it requires information about group theory (to distinguish stereoisomers using mathematical information about its symmetry), topology and geometry of the molecule. There are several software packages that include modules to handle stereochemistry, especially the ones to name a chemical structure and/or view, edit and generate chemical structure diagrams. However, there is a lack of software capable of automatically analyzing a molecule represented as a graph and generate a classification of the type of isomerism present in a given atom or bond. Considering the importance of stereoisomerism when comparing chemical structures, this report describes a computer program for analyzing and processing steric information contained in a chemical structure represented as a molecular graph and providing as output a binary classification of the isomer type based on the recommended conventions. Due to the complexity of the underlying issue, specification of stereochemical information is currently limited to explicit stereochemistry and to the two most common types of stereochemistry caused by asymmetry around carbon atoms: chiral atom and double bond.	atom;binary classification;chirality (chemistry);computer program;diagram;function (biology);graph (discrete mathematics);inchi;molecular graph;simplified molecular-input line-entry system	Ana L. Teixeira;João Paulo Leal;André O. Falcão	2013	CoRR		cis–trans isomerism;stereoisomerism;enantiomer;diastereomer;chirality	Comp.	11.27900899319694	-58.707249651404254	90702
f998e8164ebe708fb3ea6b29ad080e48180d246b	protein modeling and molecular dynamic studies of two new surfactant proteins	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;computer applications in chemistry;theoretical and computational chemistry;computational biology bioinformatics;uk phd theses thesis;life sciences;uk research reports;medical journals;europe pmc;documentation and information in chemistry;biomedical research;bioinformatics	Surfactant proteins are of major importance for the stability and flexibility of lipid systems like the lung surfactant or the tear film. They can support the adsorption of phospholipids into a layer or specifically influence the surface tension of a lipid surface [1]. To fulfil these functions, they are described to be highly posttranslationally modified. Furthermore, immunological functions were described for some of the already known surfactant proteins SP-A, SP-B, SP-C and SP-D [2]. For that reason, they are of great interest in the investigation of diseases like the “acute respiratory distress syndrome” (ARDS) or the “dry eye syndrome” (DES). Recently sequences of two putative surfactant proteins, called SP-G and SP-H, were identified. To get insights into the function of SP-G and SP-H, protein structure models were generated, including predictions for posttranslational modifications. These models successfully guided the design of antibodies for the localization of SP-G and SP-H in different human tissues, including lung and ocular system. Both protein models were transferred into a lung surfactant model system consisting of a dipalmitoyl¬phosphatidyl¬choline monolayer. MD simulations over 50 ns were performed with GROMACS starting from different orientations of the protein models. During these simulations, it was possible to track the accumulation of the proteins to the lipid layer and observe the interactions between protein surface and lipid head groups on an atomic scale. The results show that SP-G and SP-H indeed are capable of surface-regulatory properties. Furthermore, it is evident that the interaction between proteins and lipid systems is highly dependent on the present posttranslational modifications. In combination with further experimental work, these simulations can help to determine the functions of SP-G and SP-H in vivo and may support the development of new therapies for ARDS and DES.	distress (novel);gromacs;interaction;molecular dynamics;self-assembled monolayer;simulation;tree accumulation;video-in video-out	Felix Rausch;Wolfgang Brandt;Martin Schicht;Lars Bräuer;Friedrich Paulsen	2013		10.1186/1758-2946-5-S1-O2	biology;medical research;medicine;computer science;bioinformatics;immunology	Comp.	10.049299653590127	-64.08549074079822	90745
31e7a00004b7be9af42a9a282ca38a04af4de16a	systems biology toolbox for matlab: a computational platform for research in systems biology	ordinary differential equation;bifurcation analysis;sensitivity analysis;systems biology markup language;system biology;mathematical software;internal model	We present a Systems Biology Toolbox for the widely used general purpose mathematical software MATLAB. The toolbox offers systems biologists an open and extensible environment, in which to explore ideas, prototype and share new algorithms, and build applications for the analysis and simulation of biological and biochemical systems. Additionally it is well suited for educational purposes. The toolbox supports the Systems Biology Markup Language (SBML) by providing an interface for import and export of SBML models. In this way the toolbox connects nicely to other SBML-enabled modelling packages. Models are represented in an internal model format and can be described either by entering ordinary differential equations or, more intuitively, by entering biochemical reaction equations. The toolbox contains a large number of analysis methods, such as deterministic and stochastic simulation, parameter estimation, network identification, parameter sensitivity analysis and bifurcation analysis.	bifurcation theory;biochemical reaction;deterministic algorithm;estimation theory;matlab;markup language;mathematical software;mathematics;population parameter;prototype;sbml;simulation;systems biology	Henning Schmidt;Mats Jirstrand	2006	Bioinformatics	10.1093/bioinformatics/bti799	computational science;ordinary differential equation;sbml;internal model;computer science;bioinformatics;theoretical computer science;sensitivity analysis;systems biology	Comp.	16.238326931233406	-59.975659221369106	90789
58a302b9640ab1142b20a76ff3c018bb14d7b2d3	intervention in gene regulatory networks via greedy control policies based on long-run behavior	software;animals;simulation and modeling;probability;cost function;drug targeting;systems biology;dynamic programming algorithm;gene regulatory networks;physiological cellular and medical topics;dynamic program;computational biology bioinformatics;optimal control;models genetic;probabilistic boolean network;time factors;stochastic processes;computational complexity;algorithms;cell cycle;mean first passage time;transition probability matrix;gene regulatory network;optimal stochastic control;markov chains;steady state;bioinformatics;markov chain	A salient purpose for studying gene regulatory networks is to derive intervention strategies, the goals being to identify potential drug targets and design gene-based therapeutic intervention. Optimal stochastic control based on the transition probability matrix of the underlying Markov chain has been studied extensively for probabilistic Boolean networks. Optimization is based on minimization of a cost function and a key goal of control is to reduce the steady-state probability mass of undesirable network states. Owing to computational complexity, it is difficult to apply optimal control for large networks. In this paper, we propose three new greedy stationary control policies by directly investigating the effects on the network long-run behavior. Similar to the recently proposed mean-first-passage-time (MFPT) control policy, these policies do not depend on minimization of a cost function and avoid the computational burden of dynamic programming. They can be used to design stationary control policies that avoid the need for a user-defined cost function because they are based directly on long-run network behavior; they can be used as an alternative to dynamic programming algorithms when the latter are computationally prohibitive; and they can be used to predict the best control gene with reduced computational complexity, even when one is employing dynamic programming to derive the final control policy. We compare the performance of these three greedy control policies and the MFPT policy using randomly generated probabilistic Boolean networks and give a preliminary example for intervening in a mammalian cell cycle network. The newly proposed control policies have better performance in general than the MFPT policy and, as indicated by the results on the mammalian cell cycle network, they can potentially serve as future gene therapeutic intervention strategies.	cell cycle;computational complexity theory;drug delivery systems;dynamic programming;first-hitting-time model;gene regulatory network;greedy algorithm;interventional procedure;loss function;mammals;markov chain;optimal control;policy;procedural generation;stationary process;steady state;stochastic control;stochastic matrix	Xiaoning Qian;Ivan Ivanov;Noushin Ghaffari;Edward R. Dougherty	2009	BMC Systems Biology	10.1186/1752-0509-3-61	biology;gene regulatory network;markov chain;computer science;bioinformatics;theoretical computer science;machine learning;systems biology	ML	13.194313664725561	-64.17884152288221	90945
f60c04935ac5ead6220946e51ce5b2a7c205d2b9	algorithm for backrub motions in protein design	protein design;rigid body;amino acid sequence;search algorithm;structure activity relationship;proteins;algorithms;dead end elimination;source code;crystal structure;molecular sequence data;interaction model;protein engineering;mutation;atomic resolution;sequence analysis protein	MOTIVATION The Backrub is a small but kinematically efficient side-chain-coupled local backbone motion frequently observed in atomic-resolution crystal structures of proteins. A backrub shifts the C(alpha)-C(beta) orientation of a given side-chain by rigid-body dipeptide rotation plus smaller individual rotations of the two peptides, with virtually no change in the rest of the protein. Backrubs can therefore provide a biophysically realistic model of local backbone flexibility for structure-based protein design. Previously, however, backrub motions were applied via manual interactive model-building, so their incorporation into a protein design algorithm (a simultaneous search over mutation and backbone/side-chain conformation space) was infeasible.   RESULTS We present a combinatorial search algorithm for protein design that incorporates an automated procedure for local backbone flexibility via backrub motions. We further derive a dead-end elimination (DEE)-based criterion for pruning candidate rotamers that, in contrast to previous DEE algorithms, is provably accurate with backrub motions. Our backrub-based algorithm successfully predicts alternate side-chain conformations from < or = 0.9 A resolution structures, confirming the suitability of the automated backrub procedure. Finally, the application of our algorithm to redesign two different proteins is shown to identify a large number of lower-energy conformations and mutation sequences that would have been ignored by a rigid-backbone model.   AVAILABILITY Contact authors for source code.	benchmark (computing);bioinformatics;blu-ray;caprine arthritis encephalitis virus dna:prthr:pt:xxx:ord:page;combinatorial search;computation;crystal structure;dead-end elimination;dipeptides;excretory function;experiment;i204;internet backbone;molecular design software;motion;muscle rigidity;mutation;provable security;richardson number;search algorithm;source code;staphylococcal protein a;vertebral column;benefit;paragraphs;poly(2-hydroxyethyl acrylate);research grants	Ivelin Georgiev;Daniel A. Keedy;Jane S. Richardson;David C. Richardson;Bruce Randall Donald	2008		10.1093/bioinformatics/btn169	mutation;biology;mathematical optimization;structure–activity relationship;rigid body;computer science;bioinformatics;crystal structure;protein engineering;peptide sequence;genetics;protein design;search algorithm;source code	Comp.	12.017033520998396	-60.5161189874875	91009
088afc0c494a061a83f5ee70f861372943fd76df	structure modeling based computer aided t-cell epitope design.	modelowanie;komorka;t cell;struktura;modeling;structure	Progress in computational modeling for structural biology has motivated the use of molecular mechanics calculations for synthetic peptide design as potential T-cell epitopes (peptides inducing immunogenicity). Short antigen peptides from virus/ bacteria/parasite are recognized by host specifi c human leukocyte antigen (HLA) molecules for T-cell sensitive cellular immune response. However, HLA molecules are highly polymorphic at the sequence level among ethnic population (American Indian, Australian aboriginal, Black, Caucasoid, Hispanic, Mixed, Oriental, Pacifi c Islander and Unknown ethnicity). The binding of peptides to host HLA molecules are both specifi c and sensitive. The use of computer aided molecular modeling principles has been shown for the design of T-cell specifi c epitopes as potential vaccine candidates. Application of computational techniques such as molecular dynamics simulation (MDS), self consistent ensemble optimization (SCEO), free energy (FE) estimation, computational combinatorial ligand design (CCLD), 3D quantitative structure activity relationship (3D-QSAR) and structure based virtual pockets (SBVP) in HLA-peptide binding prediction is discussed. The ability of modeling and design to predict peptide binding to a wide array of defi ned HLA alleles fi nds application in proteome wide scanning of bacteria/virus/parasite proteomes towards cocktail peptide vaccines. HLA Immunology & T-cell epitopes Structure of HLA-peptide complexes determined by X-ray crystallography [1] and competitive binding assay data [2] indicates the binding of HLA molecules to short antigenic peptides during T-cellular immune response. It is known that the specifi c binding of peptides to HLA molecules is the crucial step in T-cell mediated immune response. The binding of peptide to HLA alleles during T-cell immunity is illustrated in Figure 1. High polymorphism among HLA molecules in different ethnicity determines specifi city and sensitivity of peptide binding. Sequence polymorphism is frequently concentrated at the peptide binding groove of both class I and II HLA molecules (Figure 2). The current International ImMunoGeneTics information system (IMGT)/human leukocyte antigen (HLA) database (January 2010) contains over 4447 HLA alleles [3]. The binding of these alleles to overlapping short antigen peptides (9-20 residues) is combinatorial in selection and degeneration. The binding of peptide to HLA alleles through position specifi c accommodation of peptide residues within the binding groove is shown in Figure 3. The binding of peptides to allele specifi c HLA alleles is both specifi c and sensitive determined through the physics and chemistry of the pockets accommodating the position specifi c peptide residues (Figure 4). Hence, the prediction of human leukocyte antigen (HLA) peptide binding fi nds application in epitope (antigen peptide with immunogenicity) design for the development of vaccines and diagnostics of diseases associated with T-cellular immunity. Peptide vaccines are cocktail peptides from virus/bacteria/parasite with specifi city to maximum number of known HLA alleles. The use of molecular mechanics calculations and predictions in the design of T-cell epitopes for specifi city to maximum HLA allele coverage is discussed. Molecular mechanics in T-cell Epitope	computational chemistry;computational immunology;dna binding site;information system;mathematical optimization;molecular dynamics;molecular mechanics;name binding;quantitative structure–activity relationship;simulation;synthetic intelligence	Gopichandran Sowmya;Ayyappan Vaishnavi;Pandjassarame Kangueane	2008	Bio-Algorithms and Med-Systems		epitope;t cell;computer science;bioinformatics	Comp.	11.398546338593365	-58.81566086806834	91176
e37b89778f1b83b016ed2ebb20d66549f951d29a	analysis of the calculated physicochemical properties of respiratory drugs: can we design for inhaled drugs yet?	physicochemical properties	From an analysis of calculated physicochemical properties for 81 currently marketed respiratory drugs, compounds administered via the inhaled/intranasal routes have a higher polar surface area, a higher molecular weight, and a trend toward lower lipophilicity, when compared with their orally administered counterparts. Ranges of physicochemical space are described for the 29 drugs administered by the inhaled or intranasal routes.	inspiration function;molecular weight;oral cavity;polar surface area;respiratory insufficiency;respiratory system agents;lipophilicity	Timothy J. Ritchie;Christopher N. Luscombe;Simon J. F. Macdonald	2009	Journal of chemical information and modeling	10.1021/ci800429e	stereochemistry;chemistry;toxicology;medicinal chemistry	Networks	10.671018461417255	-63.40069860722323	91773
2653239cd84e07228e7cdea5c8a6b201abe3104f	using meta-genetic algorithms to tune parameters of genetic algorithms to find lowest energy molecular conformers	long chains;look up table;molecular conformers;molecular receptors;keywords biological process;conference paper;quantitative measurement;simple ga;biological systems;parameter setting;long chain molecules;chain len;evolutionary process;institutional repository research archive oaister;exhaustive search;minimum energy	Determining the electronic structure of long chain molecules is essential to the understanding of many biological processes, notably those involving molecular receptors in cells. Finding minimum energy conformers and thus electronic structure of long-chain molecules by exhaustive search quickly becomes infeasible as the chain length increases. Typically, resources required are proportional to the number of possible conformers (shapes), which scales as O(3^L) where L is the length. An optimized genetic algorithm that can determine the minimum energy conformer of an arbitrary long-chain molecule in a feasible time is described, using the tool, PyEvolve. The method is to first solve a generic problem for a long chain by exhaustive search, then by using the pre-determined results in a look-up table, to make use of a Meta-GA to optimize parameters of a simple GA through an evolutionary process to solve that same problem. By comparing the results using the tuned parameters obtained by this method with the results from exhaustive search on several molecules of comparable chain length we have obtained quantitative measurements of an increase in speed by a factor of three over standard parameter settings, and a factor of ten over exhaustive search.	brute-force search;decade (log scale);electronic structure;genetic algorithm;lookup table;software release life cycle	Zoe E. Brain;Matthew A. Addicoat	2010			lookup table;computer science;bioinformatics;brute-force search;algorithm	Comp.	12.800326363283766	-61.11053834232231	91830
29ba1383ff313458803146157e2e6e8adc24c4ba	probabilistic disease classification of expression-dependent proteomic data from mass spectrometry of human serum	classification algorithm;time of flight;support vector machines;human serum;mass spectra;exact solution;mass spectrometry;spectrum;linear discriminate analysis;probabilistic classification;disease diagnosis;complex protein mixtures;machine learning;principal component analysis;cancer diagnosis;differential expression;classification algorithms;positive predictive value;ovarian cancer;disease classification;exact algorithms;proteomics;biomarkers;linear discriminant analysis;surface enhanced laser desorption ionization;prostate cancer	We have developed an algorithm called Q5 for probabilistic classification of healthy versus disease whole serum samples using mass spectrometry. The algorithm employs principal components analysis (PCA) followed by linear discriminant analysis (LDA) on whole spectrum surface-enhanced laser desorption/ionization time of flight (SELDI-TOF) mass spectrometry (MS) data and is demonstrated on four real datasets from complete, complex SELDI spectra of human blood serum. Q5 is a closed-form, exact solution to the problem of classification of complete mass spectra of a complex protein mixture. Q5 employs a probabilistic classification algorithm built upon a dimension-reduced linear discriminant analysis. Our solution is computationally efficient; it is noniterative and computes the optimal linear discriminant using closed-form equations. The optimal discriminant is computed and verified for datasets of complete, complex SELDI spectra of human blood serum. Replicate experiments of different training/testing splits of each dataset are employed to verify robustness of the algorithm. The probabilistic classification method achieves excellent performance. We achieve sensitivity, specificity, and positive predictive values above 97% on three ovarian cancer datasets and one prostate cancer dataset. The Q5 method outperforms previous full-spectrum complex sample spectral classification techniques and can provide clues as to the molecular identities of differentially expressed proteins and peptides.	algorithm;algorithmic efficiency;anterior descending branch of left coronary artery;experiment;full-spectrum light;linear discriminant analysis;malignant neoplasm of ovary;mass spectrometry;principal component analysis;probabilistic turing machine;prostatic neoplasms;proteomics;self-replication;sense of identity (observable entity);sensitivity and specificity;serum;silo (dataset);stellar classification;ovarian neoplasm;surface enhanced laser desorption ionization	Ryan H. Lilien;Hany Farid;Bruce Randall Donald	2003	Journal of computational biology : a journal of computational molecular cell biology	10.1089/106652703322756159	spectrum;support vector machine;probabilistic classification;time of flight;mass spectrum;mass spectrometry;computer science;bioinformatics;machine learning;optimal discriminant analysis;surface-enhanced laser desorption/ionization;mathematics;proteomics;linear discriminant analysis;biomarker;principal component analysis	Comp.	10.43375233454015	-52.645787951745156	91944
86568ffeff3820c8c9de5c8e90bd9bfb55cc4955	on the modeling of long-term hiv-1 infection dynamics	calcul scientifique;computer aided analysis;virosis;matematicas aplicadas;analyse assistee;infeccion;modele mathematique;analisis sistema;mathematiques appliquees;bifurcation;sida;virus;cd4 t cell;bifurcacion;hombre;dynamic system;human immunodeficiency virus;modelo matematico;hiv dynamics;virus immunodeficience humaine;acute phase;bifurcation analysis;dynamical system;systeme dynamique;computacion cientifica;human;estimacion parametro;model development;mathematical model;system analysis;analisis asistido;aids;37xx;dynamical systems;analyse systeme;parameter estimation;estimation parametre;sistema dinamico;scientific computation;applied mathematics;retroviridae;viral disease;infection;lentivirus;virose;immune response;homme;modeling and analysis	In this paper we propose and study models of long-term Human Immunodeficiency Virus (HIV-1) infection. Our aim is to identify model mechanisms that allow to explain the trends observed in clinical measurements of the number of CD4+ Tcells and virus throughout the long-term HIV-1 infection, from the acute phase until the onset of AIDS. To achieve our goal we apply some standard methods of modeling and analysis of dynamical systems. Among these methods are model development and validation processes such as parameter estimation, as well as Painleve and bifurcation analysis. keywords: HIV dynamics, immune response, parameter estimation, dynamical systems	bifurcation theory;dynamical system;estimation theory;onset (audio)	Marcos A. Capistran;Francisco J. Solis	2009	Mathematical and Computer Modelling	10.1016/j.mcm.2009.05.006	dynamical system;mathematics;operations research;statistics	ML	16.35906004270654	-64.6689851421409	92187
2e536ed571208784474d28bd0714bfcbcd6d7bf7	model based on grid-derived descriptors for estimating cyp3a4 enzyme stability of potential drug candidates	quantitative structure property relationship;virtual library;drug discovery;enzyme activity;partial least square;enzyme;computational method;three dimensional;absorption distribution metabolism and excretion;success rate;three dimensional structure;molecular interactions;in silico;molecular structure	A number of computational approaches are being proposed for an early optimization of ADME (absorption, distribution, metabolism and excretion) properties to increase the success rate in drug discovery. The present study describes the development of an in silico model able to estimate, from the three-dimensional structure of a molecule, the stability of a compound with respect to the human cytochrome P450 (CYP) 3A4 enzyme activity. Stability data were obtained by measuring the amount of unchanged compound remaining after a standardized incubation with human cDNA-expressed CYP3A4. The computational method transforms the three-dimensional molecular interaction fields (MIFs) generated from the molecular structure into descriptors (VolSurf and Almond procedures). The descriptors were correlated to the experimental metabolic stability classes by a partial least squares discriminant procedure. The model was trained using a set of 1800 compounds from the Pharmacia collection and was validated using two test sets: the first one including 825 compounds from the Pharmacia collection and the second one consisting of 20 known drugs. This model correctly predicted 75% of the first and 85% of the second test set and showed a precision above 86% to correctly select metabolically stable compounds. The model appears a valuable tool in the design of virtual libraries to bias the selection toward more stable compounds.		Patrizia Crivori;Ismael Zamora;Bill Speed;Christian Orrenius;Italo Poggesi	2004	Journal of computer-aided molecular design	10.1023/B:JCAM.0000035184.11906.c2	three-dimensional space;biochemistry;stereochemistry;enzyme;enzyme assay;chemistry;molecule;toxicology;bioinformatics;drug discovery	Comp.	11.143510417616925	-58.62878300413677	92211
84a22c72d2c1cf4878cfe11ef3aa5b09aeb372a9	conception et synthese de nouvelles sondes ciblees pour l'imagerie moleculaire. (design and synthesis of new targeted probes for molecular imaging)		Advance in personalized medicine requires probes development for the detection of pathology markers at the earliest possible stage. This thesis describes the synthesis and the evaluation of targeted probes to produce images in MRI or scintigraphy. These probes are constructed by functionalizing a molecular scaffold prepared by a three-component reaction. The resulting propargylamine has the necessary structure to introduce targeting ligands and two chelating agents for imaging. It also has a group in the terminal position to connect it on structures like small polyfunctional molecules, dendrimers or proteins. A high throughput screening technique based on small molecule microarrays was used for the detection of ligands. Molecules spotted on the chip came from either complex Cglycosides prepared according to the concept of diversity-oriented synthesis or from the laboratory chemical library. These microarrays have enable ligand identification for several proteins such as Vpr a protein from the capsid of AIDS virus. A secondary screening by NMR confirmed the interaction between protein and small molecules. Probes carrying a flavonoid for targeting tumor endothelial cells were treated with a gadolinium salt solution and MRI evaluated their properties. Initial results indicated that these compounds does offer of two to four folds contrast than the contrast agents usually used. Their biological evaluation will be conducted soon.		Claire Beauvineau	2011				Comp.	10.590530194458474	-60.49035664003351	92271
61cdfaeede47ef55bbe59f84b81798a8a8ce3df3	how important are the sizes and locations of fixation regions for the bias model?	face category;learning algorithm;bias model;face category bias model bayesian integrate and shift model learning object categories;learning object categories;bayes methods;face recognition bayes methods;face recognition;humans eyes radio frequency mouth face recognition computer vision detectors bayesian methods robustness visual system;learning object;bayesian integrate and shift model	In this work we consider the Bayesian integrate and shift (BIAS) model for learning object categories and investigate its sensitivity to changes in the sizes and locations of fixation regions. We test the model using a face category and show that the learning algorithm is robust to large variations of the regions' sizes and locations. Specifically, we show that the performance is inversely proportional to the sizes of the fixation regions and that the preferred locations are those that are closer to the center of the object.	algorithm;bayesian network;emoticon	Predrag Neskovic;Ian Sherman;Liang Wu;Leon N. Cooper	2007	Third International Conference on Natural Computation (ICNC 2007)	10.1109/ICNC.2007.417	psychology;computer vision;machine learning;pattern recognition	Vision	22.869387480182336	-65.49122539570992	92691
3496508894821d7ef28fe7510a702df88c5d6570	a survey of control-chart pattern-recognition literature (1991-2010) based on a new conceptual classification scheme	control chart;literature review;pattern recognition;conceptual classification scheme	Control Chart Pattern Recognition (CCPR) is a critical task in Statistical Process Control (SPC). Abnormal patterns exhibited in control charts can be associated with certain assignable causes adversely affecting the process stability. Abundant literature treats the detection of different Control Chart Patterns (CCPs). In fact, numerous CCPR studies have been developed according to various objectives and hypotheses. Despite the widespread literature on this topic, efforts to review and analyze research on CCPR are very limited. For this reason, this survey paper proposes a new conceptual classification scheme, based on content analysis method, to classify past and current developments in CCPR research. More than 120 papers published on CCPR studies within 1991–2010 were classified and analyzed. Major findings of this survey include the following. (1) The most popular CCPR studies deal with independently and identically distributed process data. (2) Some recent studies on identification of mean shifts or/and variance shifts of a multivariate process are based on innovative techniques. (3) The percentage of studies that address concurrent pattern identification is increasing. (4) The majority of the reviewed articles use Artificial Neural Network (ANN) approach. Feature-based techniques, in particular wavelet-denoise, are investigated for improving the recognition performance of ANN. For the same reason, there is a general trend followed by many authors who propose hybrid, modular and integrated ANN recognizer designs combined with decision tree learning, particle swarm optimization, etc. (5) There are two main categories of performance criteria used to evaluate CCPR approaches: statistical criteria that are related to two conventional Average Run Length (ARL) measures, and recognition-accuracy criteria, which are not based on these ARL measures. The most applied criteria are recognition-accuracy criteria, mainly for ANN-based approaches. Performance criteria which are related to ARL measures are insufficient and inappropriate in the case of concurrent pattern identification. Finally, this paper briefly discusses some future research directions and our perspectives. 2012 Elsevier Ltd. All rights reserved.	artificial neural network;chart;comparison and contrast of classification schemes in linguistics and metadata;decision tree learning;finite-state machine;mathematical optimization;noise reduction;particle swarm optimization;pattern recognition;run-length encoding;wavelet	Wafik Hachicha;Ahmed Ghorbel	2012	Computers & Industrial Engineering	10.1016/j.cie.2012.03.002	control chart;computer science;engineering;artificial intelligence;machine learning;data mining;management;operations research;statistics	AI	19.314888092418695	-59.174740786854386	93292
41367bca49675d0dd078dcd9a140b92d05379900	survey on emotional body gesture recognition		"""Automatic emotion recognition has become a trending research topic in the past decade. While works based on facial expressions or speech abound recognizing affect from body gestures remains a less explored topic. We present a new comprehensive survey hoping to boost research in the field. We first introduce emotional body gestures as a component of what is commonly known as """"body language"""" and comment general aspects as gender differences and culture dependence. We then define a complete framework for automatic emotional body gesture recognition. We introduce person detection and comment static and dynamic body pose estimation methods both in RGB and 3D. We then comment the recent literature related to representation learning and emotion recognition from images of emotionally expressive gestures. We also discuss multi-modal approaches that combine speech or face with body gestures for improved emotion recognition. While pre-processing methodologies (e.g. human detection and pose estimation) are nowadays mature technologies fully developed for robust large scale analysis, we show that for emotion recognition the quantity of labelled data is scarce, there is no agreement on clearly defined output spaces and the representations are shallow and largely based on naive geometrical representations."""	emotion recognition;feature learning;gesture recognition;machine learning;modal logic;preprocessor	Fatemeh Noroozi;Ciprian Adrian Corneanu;Dorota Kaminska;Tomasz Sapinski;Sergio Escalera;Gholamreza Anbarjafari	2018	CoRR		psychology;machine learning;artificial intelligence;body language;gesture recognition;pose;emotion recognition;scale analysis (statistics);facial expression;gesture;feature learning	HCI	24.387619700797618	-58.46474676489322	93295
6dd3a95bd46e3ab9c3f649a2034bf5ddba19c710	learning deep representations for semantic image parsing: a comprehensive overview	semantic image segmentation;deep learning;convolutional neural networks;image parsing	Semantic image parsing, which refers to the process of decomposing images into semantic regions and constructing the structure representation of the input, has recently aroused widespread interest in the field of computer vision. The recent application of deep representation learning has driven this field into a new stage of development. In this paper, we summarize three aspects of the progress of research on semantic image parsing, i.e., category-level semantic segmentation, instance-level semantic segmentation, and beyond segmentation. Specifically, we first review the general frameworks for each task and introduce the relevant variants. The advantages and limitations of each method are also discussed. Moreover, we present a comprehensive comparison of different benchmark datasets and evaluation metrics. Finally, we explore the future trends and challenges of semantic image parsing.	algorithm;benchmark (computing);computer multitasking;computer vision;evaluation function;feature learning;image processing;machine learning;natural language understanding;parsing;semi-supervised learning;semiconductor industry;unsupervised learning	Lili Huang;Jiefeng Peng;Ruimao Zhang;Guanbin Li;Liang Lin	2018	Frontiers of Computer Science	10.1007/s11704-018-7195-8	convolutional neural network;machine learning;parsing;deep learning;artificial intelligence;feature learning;computer science	Vision	24.491329767077467	-54.30396325659717	93505
48a248b94a2d648c6a8dce774ad816da83e456ab	learning to classify a collection of images and texts	system performance;visual features	A single net system based on Kohonen’s Feature map was trained using a combined vector that contains visual features of an image and its collateral keywords. The performance of the single net was compared with a multinet system, comprising two SOMs, one trained with visual features and the other on keywords, in the presence of a Hebbian network that learns to associate visual features with keywords. The multi-net system performs better than the single net. Similar results were obtained when Grossberg’s ART networks were used instead of SOMs.	hebbian theory;image retrieval;modality (human–computer interaction);multinet;self-organizing map	Panagiotis Saragiotis;Bogdan Vrusias;Khurshid Ahmad	2005			computer vision;computer science;artificial intelligence;machine learning	ML	24.046022270932134	-63.53684003396446	94055
4cf30266b6ad1847bfb1260c0f22a90d49253a21	dynamic relevance: vision-based focus of attention using artificial neural networks. (technical note)	time varying;task specific selective attentions;plasma etching;dynamic relevance;computer vision;artificial neural networks;focus of attention;hand tracking;autonomous navigation;selective attention;temporal coherence;autonomous control;artificial neural network	This paper presents a method for ascertaining the relevance of inputs in vision-based tasks by exploiting temporal coherence and predictability. In contrast to the tasks explored in many previous relevance experiments, the class of tasks examined in this study is one in which relevance is a time-varying function of the previous and current inputs. The method proposed in this paper dynamically allocates relevance to inputs by using expectations of their future values. As a model of the task is learned, the model is simultaneously extended to create task-specific predictions of the future values of inputs. Inputs that are not relevant, and therefore not accounted for in the model, will not be predicted accurately. These inputs can be de-emphasized, and, in turn, a new, improved, model of the task created. The techniques presented in this paper have been successfully applied to the vision-based autonomous control of a land vehicle, vision-based hand tracking in cluttered scenes, and the detection of faults in the plasma-etch step of semiconductor wafers.	artificial neural network;autonomous robot;cmos;coherence (physics);debian;experiment;neural networks;plasma active;plasma etching;relevance;semiconductor	Shumeet Baluja;Dean Pomerleau	1997	Artif. Intell.	10.1016/S0004-3702(97)00065-9	computer vision;simulation;plasma etching;attention;computer science;artificial intelligence;machine learning;artificial neural network	Robotics	18.6211878852168	-65.24100878817869	94192
4aa464a195048380e6102af98906c7e573077fa0	examination of the deep neural networks in classification of distorted signals		Classification of distorted patterns poses real problem for majority of classifiers. In this paper we analyse robustness of deep neural network in classification of such patterns. Using specific convolutional network architecture, an impact of different types of noise on classification accuracy is evaluated. For highly distorted patterns to improve accuracy we propose a preprocessing method of input patterns. Finally, an influence of different types of noise on classification accuracy is also analysed.	deep learning;neural networks	Michal Koziarski;Boguslaw Cyganek	2016		10.1007/978-3-319-39384-1_60	computer science;artificial intelligence;convolutional neural network;machine learning;robustness (computer science);pattern recognition;artificial neural network;network architecture;preprocessor	AI	21.118629780434908	-54.38135027974079	94840
5b0abee4011d1eaa8ced8c16d8b0ed71d89a31d8	application of an expert system based on genetic algorithm-adaptive neuro-fuzzy inference system (ga-anfis) in qsar of cathepsin k inhibitors	cathepsin k inhibitory activity;genetic algorithm;qsar;adaptive neuro fuzzy inference system	One strategy to potentially improve the success of drug design and development is to use chemometrics methods early in the process to propose molecules and scaffolds with ideal binding and to clarify physicochemical features influencing in their activity. Adaptive Neuro-Fuzzy Interference System (ANFIS) was used to construct the nonlinear quantitative structure–activity relationship (QSAR) model. The Genetic Algorithm (GA) was used to select descriptors which are responsible for the cathepsin K inhibitory activity of studied compounds. ANFIS regression is a nonlinear regression technique developed to relate many regressors to one or several response variables. The accuracy of the generated QSAR model (R = 0.916) is described using various evaluation techniques, such as leave-one-out procedure ðRLOO 1⁄4 0:875Þ and validation through an external test set ðRpred 1⁄4 0:932Þ. 2011 Elsevier Ltd. All rights reserved.	adaptive neuro fuzzy inference system;applicability domain;approximation;chemometrics;database;encode;expert system;genetic algorithm;interference (communication);medicinal chemistry;neuro-fuzzy;nonlinear system;quantitative structure–activity relationship;software release life cycle;test set	Mohsen Shahlaei;Armin Madadkar-Sobhani;Lotfollah Saghaie;Afshin Fassihi	2012	Expert Syst. Appl.	10.1016/j.eswa.2011.11.106	genetic algorithm;adaptive neuro fuzzy inference system;computer science;bioinformatics;artificial intelligence;machine learning;quantitative structure–activity relationship	AI	12.179866406261057	-57.171422963069865	95028
e74b3868b91720b689967f4e6131e965170c0250	protein structure prediction aided by geometrical and probabilistic constraints	peptides;touchstone;distance restraints;packing;monte carlo minimization;beta sheets;secondary structure packing;alpha helices;loop backbone probability distribution;protein structure prediction;secondary structure;energy minimization;sequence;article;conformational searches	Database-assisted ab initio protein structure prediction methods have exhibited considerable promise in the recent past, with several implementations being successful in community-wide experiments (CASP). We have employed combinatorial optimization techniques toward solving the protein structure prediction problem. A Monte Carlo minimization algorithm has been employed on a constrained search space to identify minimum energy configurations. The search space is constrained by using radius of gyration cutoffs, the loop backbone dihedral probability distributions, and various secondary structure packing conformations. Simulations have been carried out on several sequences and 1000 conformations have been initially generated. Of these, 50 best candidates have then been selected as probable conformations. The search for the optimum has been simplified by incorporating various geometrical constraints on secondary structural elements using distance restraint potential functions. The advantages of the reported methodology are its simplicity, and modifiability to include other geometric and probabilistic restraints.	4-dichlorobenzene;ab initio quantum chemistry methods;acquired immunodeficiency syndrome;algorithm;casp;combinatorial optimization;computer simulation;experiment;heuristic (computer science);heuristics;internet backbone;mathematical optimization;monte carlo method;multi-chip module;one thousand;physical restraint equipment (device);probability;protein data bank;protein structure prediction;protein, organized by structure;pseudo-torch syndrome;sampling (signal processing);set packing;structural similarity;vertebral column	Gaurav Porwal;Swapnil Jain;S. Dhilly Babu;Deepak Singh;Hemant Nanavati;Santosh B. Noronha	2007	Journal of computational chemistry	10.1002/jcc.20736	crystallography;mathematical optimization;chemistry;computational chemistry;protein structure prediction;sequence;beta sheet;mathematics;energy minimization;protein secondary structure;alpha helix	Comp.	12.949335309243166	-60.70493667095888	95380
825ff50398dff6336bf0ffadfd36dbc1260f237d	optimizing pooling function for pooled steganalysis	detectors payloads kernel art histograms probability distribution training;object detector pooling function pooled steganalysis granularity machine learning optimal function;steganography learning artificial intelligence optimisation	Pooled steganalysis combines evidence from multiple objects to achieve higher accuracy in detecting hidden messages at the expense of granularity, as the decision is provided on the set of objects instead of a single one. Although it has been introduced almost decade ago, very little work has been done since then. This work builds upon recent advances in machine learning to show, how an optimal function combining outputs of a single object detector on a set of objects can be learned. Although experiments demonstrate that learned combining functions are superior to the prior art, more importantly they reveal many interesting phenomenons and points to direction of further research.	experiment;machine learning;optimizing compiler;sensor;steganalysis	Tomás Pevný;Ivan Nikolaev	2015	2015 IEEE International Workshop on Information Forensics and Security (WIFS)	10.1109/WIFS.2015.7368555	computer science;machine learning;pattern recognition;data mining	Vision	23.095021705977747	-54.3780798452425	95702
ea25cc5332416781e6ba0ca45b21d853571356fd	text-based similarity searching for hit- and lead-candidate identification	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;computer applications in chemistry;theoretical and computational chemistry;computational biology bioinformatics;uk phd theses thesis;life sciences;uk research reports;medical journals;europe pmc;documentation and information in chemistry;biomedical research;bioinformatics	The Pharmacophore Alignment Search Tool (PhAST) is a string-based approach to virtual screening. Molecules are represented by linear sequences which describe their respective pattern of interaction possibilities. The problem of molecule linearization is tackled by applying Minimum Volume Embedding in combination with a Diffusion Kernel to the molecular graph [1,2]. Linear representations are compared using global pairwise sequence alignment [3]. PhAST exhibited enrichment capabilities comparable or superior to most common virtual screening approaches. Compound rankings were proven to be dissimilar to those of other virtual screening techniques. It was shown that emphasis on key interactions through the application of position specific weights in the alignment process significantly increases enrichment. Significance of chemical similarity was determined in form of p-values of global alignment scores, calculated in an approach that was adapted from its original application to local sequence alignments of protein sequences utilizing Marcov chain Monte Carlo simulation [4]. Bonferroni correction was used to correct p-values with respect to the size of the screening library [5]. PhAST was employed in two prospective applications: A screening for non-nucleoside analogue inhibitors of bacterial thymidine kinase yielded a hit with a distinct structural framework but only weak activity. Screenings for drugs that are not members of the NSAID (non-steroidal anti-inflammatory drug) class as modulators of gamma secretase resulted in a potent modulator with clear structural distinction from the reference compound.	chemical similarity;gamma correction;gene ontology term enrichment;interaction;modulation;molecular graph;monte carlo method;peptide sequence;pharmacophore;prospective search;sequence alignment;simulation;text-based (computing);virtual screening	Volker Hähnke	2012		10.1186/1758-2946-4-S1-O12	biology;medical research;medicine;computer science;bioinformatics;data science;data mining	Comp.	10.439675935002336	-59.05234932030017	95856
1361db7cea8b8c0f83550a2da8b4a92298225f5f	an integrated approach utilizing artificial neural networks and seldi mass spectrometry for the classification of human tumours and rapid identification of potential biomarkers	integrated approach;expression pattern;expression profile;disease progression;model system;mass spectrometry;chip;molecular characterization;back propagation algorithm;multi layer perceptron;high throughput;cellular material;institutional repository research archive oaister;data acquisition;surface enhanced laser desorption ionization;artificial neural network	MOTIVATION MALDI mass spectrometry is able to elicit macromolecular expression data from cellular material and when used in conjunction with Ciphergen protein chip technology (also referred to as SELDI-Surface Enhanced Laser Desorption/Ionization), it permits a semi-high throughput approach to be taken with respect to sample processing and data acquisition. Due to the large array of data that is generated from a single analysis (8-10000 variables using a mass range of 2-15 kDa-this paper) it is essential to implement the use of algorithms that can detect expression patterns from such large volumes of data correlating to a given biological/pathological phenotype from multiple samples. If successful, the methodology could be extrapolated to larger data sets to enable the identification of validated biomarkers correlating strongly to disease progression. This would not only serve to enable tumours to be classified according to their molecular expression profile but could also focus attention upon a relatively small number of molecules that might warrant further biochemical/molecular characterization to assess their suitability as potential therapeutic targets.   RESULTS Using a multi-layer perceptron Artificial Neural Network (ANN) (Neuroshell 2) with a back propagation algorithm we have developed a prototype approach that uses a model system (comprising five low and seven high-grade human astrocytomas) to identify mass spectral peaks whose relative intensity values correlate strongly to tumour grade. Analyzing data derived from MALDI mass spectrometry in conjunction with Ciphergen protein chip technology we have used relative importance values, determined from the weights of trained ANNs (Balls et al., Water, Air Soil Pollut., 85, 1467-1472, 1996), to identify masses that accurately predict tumour grade. Implementing a three-stage procedure, we have screened a population of approximately 100000-120000 variables and identified two ions (m/z values of 13454 and 13457) whose relative intensity pattern was significantly reduced in high-grade astrocytoma. The data from this initial study suggests that application of ANN-based approaches can identify molecular ion patterns which strongly associate with disease grade and that its application to larger cohorts of patient material could potentially facilitate the rapid identification of validated biomarkers having significant clinical (i.e. diagnostic/prognostic) potential for the field of cancer biology. AVAILIBILITY: Neuroshell 2 is commercially available from ward systems.	algorithm;artificial neural network;assumed;astrocytoma;backpropagation;bio-informatics;bioinformatics;biological markers;classification;color gradient;data acquisition;data point;deployment environment;disk staging;extrapolation;gene expression profiling;integrated circuit;ions;iontophoresis;large;layer (electronics);license;multilayer perceptron;neoplasms;numerous;patients;progressive disease;projection screen;protein microarrays;prototype;responsiveness;semiconductor industry;software propagation;spectrometry;spectrometry, mass, matrix-assisted laser desorption-ionization;throughput;ward (environment);weight;algorithm;surface enhanced laser desorption ionization	Graham R. Ball;Shahid Mian;F. Holding;R. O. Allibone;J. Lowe;Shoukat Ali;G. Li;S. McCardle;Ian O. Ellis;Colin S Creaser;Robert C. Rees	2002	Bioinformatics	10.1093/bioinformatics/18.3.395	chip;high-throughput screening;mass spectrometry;computer science;bioinformatics;surface-enhanced laser desorption/ionization;data acquisition;multilayer perceptron;artificial neural network	Comp.	10.924643791705432	-56.77402463524716	95870
7edddc85ca8dbbb09b44bfdab6c3e85166240214	high-affinity interactions of ligands at recombinant guinea pig 5ht7 receptors	quantitative structure activity relationship;computer aided design;guinea pig;circadian rhythm;model evaluation;serotonin;heterologous expression;cross validation;database search;serotonin receptor;structural similarity;cell line	The serotonin 5HT7 receptor has been implicated in numerous physiological and pathological processes from circadian rhythms to depression and schizophrenia. Clonal cell lines heterologously expressing recombinant receptors offer good models for understanding drug-receptor interactions and development of quantitative structure-activity relationships (QSAR). Comparative Molecular Field Analysis (CoMFA) is an important modern QSAR procedure that relates the steric and electrostatic fields of a set of aligned compounds to affinity. Here, we utilized CoMFA to predict affinity for a number of high-affinity ligands at the recombinant guinea pig 5HT7 receptor. Using R-lisuride as the template, a final CoMFA model was derived using procedures similar to those of our recent papers. The final cross-validated model accounted for >85% of the variance in the compound affinity data, while the final non-cross validated model accounted for >99% of the variance. Model evaluation was done using cross-validation methods with groups of 5 ligands. Twenty cross-validation runs yielded an average predictive r2(q2) of 0.779 +/- 0.015 (range: 0.669-0.867). Furthermore, 3D-chemical database search queries derived from the model yielded hit lists of promising agents with high structural similarity to the template. Together, these results suggest a possible basis for high-affinity drug action at 5HT7 receptors.		R. E. Wilcox;J. E. Ragan;Robert S. Pearlman;M. Y.-K. Brusniak;R. M. Eglen;D. W. Bonhaus;T. E. Tenner;Joshua D Miller	2001	Journal of computer-aided molecular design	10.1023/A:1014319812972	pharmacology;biology;database search engine;toxicology;bioinformatics;computer aided design;structural similarity;machine learning;heterologous expression;5-ht receptor;quantitative structure–activity relationship;cell culture;cross-validation;circadian rhythm	Comp.	10.505216428612961	-59.02249114433294	95980
2b0270df4c826e1bdef66cc0f37172a784b09bc6	qspr designer – employ your own descriptors in the automated qsar modeling process	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;computer applications in chemistry;theoretical and computational chemistry;computational biology bioinformatics;uk phd theses thesis;life sciences;uk research reports;medical journals;europe pmc;documentation and information in chemistry;biomedical research;bioinformatics	The prediction of physical and chemical properties of molecules is a very important step in the drug discovery process. QSAR and QSPR models are strong tools for predicting these properties. The models employ descriptors and statistical approaches to provide an estimation of the desired property. An abundance of descriptors and QSAR/QSPR models were published, but the prediction of some properties (i.e., pKa, logP) is still a challenge [1]. For this reason, researchers are perpetually working on identifying new descriptors and analyzing their performance in newly designed models. The process of design, parameterization and evaluation of a QSAR/QSPR model is relatively complicated. Therefore, several software tools for its automation are currently under development [2,3]. These tools are very useful if we wish to apply some of many descriptors which they implement. But if we need to use other descriptors or test our own, we require a different solution. We introduce a new version of the software QSPR Designer that fulfils the above mentioned requirement. Specifically, the user can easily and quickly add his own module for descriptor calculation into QSPR Designer. Alternatively, descriptors can also be obtained from an input file. When the descriptors are available, QSPR Designer allows the user to include them to the model, to validate this model and to perform further tasks. The functionality of QSPR Designer is demonstrated on three case studies, in which the user gradually tests and improves his ideas of how to predict pKa. The first case study is focused on the question whether pKa has any relation with atomic charges. The second case study tests if we can use charges and a simple similarity-based model to predict pKa. The third case study analyzes several QSPR models predicting pKa from charges.	data descriptor;logp machine;partial charge;quantitative structure–activity relationship	Ondrej Skrehota;Radka Svobodová Vareková;Stanislav Geidl;Michal Kudera;David Sehnal;Crina-Maria Ionescu;Jan Zidek;Jaroslav Koča	2012		10.1186/1758-2946-4-S1-P37	biology;medical research;computer science;bioinformatics;data science;data mining	SE	12.954306711877459	-58.91009365567865	96004
1d84e2ee8764c00bdfedc1291719b0e0a3f67542	generating sequence of eye fixations using decision-theoretic attention model	selection model;focusing;humans layout eyes feature extraction computer vision object detection retina filters iterative algorithms focusing;iterative algorithms;filters;gaze tracking;layout;automatic generation;computer vision;human subjects;eyes;retina;focus of attention;feature extraction;region of interest;decision theoretic;visual features;humans;object detection	"""Human eyes scan images with serial eye fixations. We proposed a novel attention selectivity model for the automatic generation of eye fixations on 2D static scenes. An activation map was first computed by extracting primary visual features and detecting meaningful objects from the scene. An adaptable retinal filter was applied on this map to generate """"Regions of Interest"""" (ROIs), whose locations corresponded to those of activation peaks and whose sizes were estimated by an iterative adjustment algorithm. The focus of attention was moved serially over the detected ROIs by a decision-theoretic mechanism. The generated sequence of eye fixations was determined from the perceptual bene?t function based on perceptual costs and rewards, while the time distribution of different ROIs was estimated by a memory learning and decaying model. Finally, to demonstrate the effectiveness of the proposed attention model, the gaze tracking results of different human subjects and the simulated eye fixation shifting were compared."""	algorithm;computer animation;computer vision;experiment;eye tracking;human–computer interaction;iterative method;pattern recognition;region of interest;robot;selectivity (electronic);sensor;theory;top-down and bottom-up design;virtual reality	Erdan Gu;Jingbin Wang;Norman I. Badler	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Workshops	10.1109/CVPR.2005.464	layout;computer vision;simulation;feature extraction;computer science;region of interest	Vision	22.946198065072267	-65.56609374263911	96026
3c427246df741cbcbc91b037a9ed6e10102eae57	a comparative study of ligand-receptor complex binding affinity prediction methods based on glycogen phosphorylase inhibitors	prediction method;quantitative structure activity relationship;score function;glycogen phosphorylase;genetics;biological activity;general methods;computer aided molecular design;surface model;structure based drug design;crystal structure;binding affinity;neural network model;3d structure;binding energy;physical properties;scoring system;neural network	Finding an accurate method for estimating the affinity of protein ligands activity is the most challenging task in computer-aided molecular design. In this study we investigate and compare seven different prediction methods for a set of 30 glycogen phosphorylase (GP) inhibitors with known crystal structures. Five of the methods involve quantitative structure-activity relationships (QSAR) based on the 2D or 3D structures of the GP ligands alone. They are hologram QSAR (HQSAR), receptor surface model (RSM), comparative molecular field analysis (CoMFA), and applications of genetic neural network to similarity matrix (SM/GNN) or conventional descriptors (C2GNN). All five QSAR-based models have good predictivity and yield q2 values ranging from 0.60 to 0.82. The other two methods, LUDI and a structure-based binding energy predictor (SBEP) system, make use of the structures of the ligand-receptor complexes. The weak correlation between biological activities and the LUDI scores of this set of inhibitors suggests that the LUDI scoring function, by itself, may not be a general method for reliable ranking of a congeneric series of compounds. The SBEP system is derived from a set of physical properties that characterizes ligand-receptor interactions. The final neural network model, which yields a q2 value of 0.60, employs four descriptors. A jury method that combines the predictions of the five QSAR-based models leads to an increase in predictivity. A multi-layer scoring system that utilizes all seven prediction methods is proposed for the evaluation of novel GP ligands.	artificial neural network;crystal structure;estimated;glycogen phosphorylase;interaction;kerrison predictor;layer (electronics);ligands;network model;neural network simulation;numerous;physical phenomenon or property;processor affinity;quantitative structure-activity relationship;response surface methodology;score;scoring functions for docking;similarity measure;streptomycin;receptor complex	Sung-Sau So;Martin Karplus	1999	Journal of computer-aided molecular design	10.1023/A:1008073215919	biology;glycogen phosphorylase;chemistry;toxicology;bioinformatics;crystal structure;artificial intelligence;machine learning;biological activity;ligand;score;binding energy;quantitative structure–activity relationship;physical property;artificial neural network	Comp.	11.937653080040468	-57.54769183232582	96198
16a7859544f7db6e30caaad070e0cfc6a995a54e	statistical analysis of terminal extensions of protein β-strand pairs	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;uk phd theses thesis;life sciences;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	The long-range interactions, required to the accurate predictions of tertiary structures of β-sheet-containing proteins, are still difficult to simulate. To remedy this problem and to facilitate β-sheet structure predictions, many efforts have been made by computational methods. However, known efforts on β-sheets mainly focus on interresidue contacts or amino acid partners. In this study, to go one step further, we studied β-sheets on the strand level, in which a statistical analysis was made on the terminal extensions of paired β-strands. In most cases, the two paired β-strands have different lengths, and terminal extensions exist. The terminal extensions are the extended part of the paired strands besides the common paired part. However, we found that the best pairing required a terminal alignment, and β-strands tend to pair to make bigger common parts. As a result, 96.97%  of β-strand pairs have a ratio of 25% of the paired common part to the whole length. Also 94.26% and 95.98%  of β-strand pairs have a ratio of 40% of the paired common part to the length of the two β-strands, respectively. Interstrand register predictions by searching interacting β-strands from several alternative offsets should comply with this rule to reduce the computational searching space to improve the performances of algorithms.	amino acids;antiparallel (electronics);cations;expressed sequence tags;greater;information theory;interaction;naruto shippuden: clash of ninja revolution 3;performance;rule (guideline);simulation;strand (programming language);algorithm;interest;protein folding;tertiary	Ning Zhang;Shan Gao;Jishou Ruan;Tao Zhang	2013		10.1155/2013/909436	biology;medical research;medicine;computer science;bioinformatics;data mining;operations research	Comp.	11.031123845123107	-62.3836698052765	96256
9e7125a8cda367624c8872f98cf27fdb62640050	graph-theoretic techniques for macromolecular docking		We propose a solution to the problem of docking two macromolecules. We represent each of two proteins as a set of potential hydrogen bond donors and acceptors and use a clique-detection algorithm to find maximally complementary sets of donor/acceptor pairs. Preliminary results are presented which demonstrate the feasibility of the method.		Eleanor J. Gardiner;Peter Willett;Peter J. Artymiuk	2000	Journal of chemical information and computer sciences	10.1021/ci990262o	docking;protein–ligand docking	Vision	10.34671293350814	-60.01274999349105	96342
da4031caa520f0d50fb1eb9452fb9b8b3df9d216	a consensus procedure improving solvent accessibility prediction	consensus procedure;prediction accuracy;solvent accessibility;correlation coefficient;exposure category	Prediction methods of structural features in 1D represent a useful tool for the understanding of folding, classification, and function of proteins, and, in particular, for 3D structure prediction. Among the structural aspects characterizing a protein, solvent accessibility has received great attention in recent years. The available methods proposed for predicting accessibility have never considered the combination of the results deriving from different methods to construct a consensus prediction able to provide more reliable results. A consensus approach that increases prediction accuracy using three high-performance methods is described. The results of our method for three different protein data sets show that up to 3.0% improvement in prediction accuracy of solvent accessibility may be obtained by a consensus approach. The improvement also extends to the correlation coefficient. Application of our consensus approach to the accessibility prediction using only three prediction methods gives results better than single methods combined for consensus formation. Currently, the scarce availability of predictors with similar parameters defining solvent accessibility hinders the testing of other methods in our consensus procedure.	accessibility;coefficient;staphylococcal protein a	Giulio Gianese;Stefano Pascarella	2006	Journal of computational chemistry	10.1002/jcc.20370	data mining	Comp.	10.376323198279168	-56.117792576155836	96406
34ffc9c35d318195e5c6643e4177bcc803d4e350	lost in folding space? comparing four variants of the thermodynamic model for rna secondary structure prediction	probability;rna folding;computational biology bioinformatics;rna;thermodynamics;algorithms;sequence analysis rna;combinatorial libraries;base sequence;computational biology;computer appl in life sciences;microarrays;bioinformatics	"""Many bioinformatics tools for RNA secondary structure analysis are based on a thermodynamic model of RNA folding. They predict a single, """"optimal"""" structure by free energy minimization, they enumerate near-optimal structures, they compute base pair probabilities and dot plots, representative structures of different abstract shapes, or Boltzmann probabilities of structures and shapes. Although all programs refer to the same physical model, they implement it with considerable variation for different tasks, and little is known about the effects of heuristic assumptions and model simplifications used by the programs on the outcome of the analysis. We extract four different models of the thermodynamic folding space which underlie the programs RNAFOLD, RNASHAPES, and RNASUBOPT. Their differences lie within the details of the energy model and the granularity of the folding space. We implement probabilistic shape analysis for all models, and introduce the shape probability shift as a robust measure of model similarity. Using four data sets derived from experimentally solved structures, we provide a quantitative evaluation of the model differences. We find that search space granularity affects the computed shape probabilities less than the over- or underapproximation of free energy by a simplified energy model. Still, the approximations perform similar enough to implementations of the full model to justify their continued use in settings where computational constraints call for simpler algorithms. On the side, we observe that the rarely used level 2 shapes, which predict the complete arrangement of helices, multiloops, internal loops and bulges, include the """"true"""" shape in a rather small number of predicted high probability shapes. This calls for an investigation of new strategies to extract high probability members from the (very large) level 2 shape space of an RNA sequence. We provide implementations of all four models, written in a declarative style that makes them easy to be modified. Based on our study, future work on thermodynamic RNA folding may make a choice of model based on our empirical data. It can take our implementations as a starting point for further program development."""	algorithm;approximation;base pairing;bioinformatics;cpu cache;class;compiler;computation;concentrate dosage form;declarative programming;dynamic programming;electronic supplementary materials;energy minimization;energy, physics;enumerated type;experiment;extraction;heuristic;less than;machine learning;machine translation;nut hypersensitivity;out there;partition function (mathematics);probability;program development;protein structure prediction;rna folding;shape analysis (digital geometry);shape table;source code;thermodynamics;free energy	Stefan Janssen;Christian Schudoma;Gerhard Steger;Robert Giegerich	2011		10.1186/1471-2105-12-429	biology;rna;dna microarray;computer science;bioinformatics;theoretical computer science;probability;structural bioinformatics;genetics	Comp.	13.47576196876845	-61.28311358165724	96776
b2b019d0ef9d54deef0f0b2511e5568714d296b1	classification of toxicity effects of biotransformed hepatic drugs using whale optimized support vector machines	imbalanced dataset;random sampling;whale optimization algorithm woa;toxic effects;support vector machines svm;synthetic minority over sampling technique smote	Measuring toxicity is an important step in drug development. Nevertheless, the current experimental methods used to estimate the drug toxicity are expensive and time-consuming, indicating that they are not suitable for large-scale evaluation of drug toxicity in the early stage of drug development. Hence, there is a high demand to develop computational models that can predict the drug toxicity risks. In this study, we used a dataset that consists of 553 drugs that biotransformed in liver. The toxic effects were calculated for the current data, namely, mutagenic, tumorigenic, irritant and reproductive effect. Each drug is represented by 31 chemical descriptors (features). The proposed model consists of three phases. In the first phase, the most discriminative subset of features is selected using rough set-based methods to reduce the classification time while improving the classification performance. In the second phase, different sampling methods such as Random Under-Sampling, Random Over-Sampling and Synthetic Minority Oversampling Technique (SMOTE), BorderLine SMOTE and Safe Level SMOTE are used to solve the problem of imbalanced dataset. In the third phase, the Support Vector Machines (SVM) classifier is used to classify an unknown drug into toxic or non-toxic. SVM parameters such as the penalty parameter and kernel parameter have a great impact on the classification accuracy of the model. In this paper, Whale Optimization Algorithm (WOA) has been proposed to optimize the parameters of SVM, so that the classification error can be reduced. The experimental results proved that the proposed model achieved high sensitivity to all toxic effects. Overall, the high sensitivity of the WOA+SVM model indicates that it could be used for the prediction of drug toxicity in the early stage of drug development.	adverse reaction to drug;algorithm;balaena mysticetus;chemical database;computational model;drug toxicity;irritants;kernel;limited stage (cancer stage);oversampling;population parameter;reproduction;rough set;sampling (signal processing);silo (dataset);subgroup;support vector machine;web-oriented architecture;drug development	Alaa Tharwat;Yasmine S. Moemen;Aboul Ella Hassanien	2017	Journal of biomedical informatics	10.1016/j.jbi.2017.03.002	sampling;pathology;machine learning;pattern recognition;data mining;statistics	AI	10.691620754507868	-52.32646690115262	96935
daf44665bb5235f8f1be5f3223a02cdbb4f3200e	automation of yeast pedigree analysis	genomics;pedigree analysis;yeast;image processing;microvalves;fiber optic;biomedical optical imaging microorganisms biological techniques image processing cellular biophysics microfluidics microvalves;imaging;microfluidics;automation fungi performance analysis genomics bioinformatics cells biology cancer microfluidics biology optical fiber devices;cell division;150 hour automation yeast pedigree analysis mitotic cell divisions biologists microfluidic device fiber optic imaging image processing software budding microfluidic valves agar plate yeast cell isolation yeast daughter cell colonies 90 min;biological techniques;biomedical optical imaging;microorganisms;cellular biophysics;automation	Yeast pedigree analysis-isolation and characterization of the products of mitotic cell divisions throughout the lifespan of an individual cell-is a manually intensive process that requires a biologist to manipulate single yeast cells every 90 minutes for as long as 150 hours. Progress toward the development of a system for automating yeast pedigree analysis is presented. Yeast cells are trapped in a microfluidic device, then observed using a fiber-optic imaging bundle. Image processing software classifies each cell as either budding or not budding a newly formed daughter cell. The system recognizes when a cell has a bud, and microfluidic valves direct newly formed daughter cells to an agar plate. Finally, biologists analyze the colonies formed by each yeast daughter cell on the agar plate.	automation;genetic genealogy;image processing;optical fiber;photographic plate	John Koschwanez;Mark Holl;Michael A McMurray;Daniel E Gottschling;Deirdre R. Meldrum	2004	IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004	10.1109/ROBOT.2004.1308032	genomics;microfluidics;image processing;computer science;bioinformatics;optical fiber;automation;microorganism;cell division	Robotics	14.292261062074564	-65.51751949070027	97107
582892cb911e4856868a2357bd9c89d136260061	computer automated structure evaluation (case) of the teratogenicity of retinoids with the aid of a novel geometry index	quantitative structure activity relationship;indexation	The CASE (Computer Automated Structure Evaluation) program, with the aid of a geometry index for discriminating cis and trans isomers, has been used to study a set of retinoids tested for teratogenicity in hamsters. CASE identified 8 fragments, the most important representing the non-polar terminus of a retinoid with an additional ring system which introduces some rigidity in the side chain. The geometry index helped to identify relevant fragments with an all-trans configuration and to distinguish them from irrelevant fragments with other configurations.		Gilles Klopman;Mario Dimayuga	1990	Journal of computer-aided molecular design	10.1007/BF00125314	stereochemistry;chemistry;machine learning;computational chemistry;quantitative structure–activity relationship	Theory	11.650330487848157	-59.03374888806448	97262
b312d6796012ff857613af460c2d7ed163c7cf44	replica exchange monte-carlo simulations of helix bundle membrane proteins: rotational parameters of helices	molecular dynamics simulations;x ray diffraction;molecular dynamics simulation;helix bundle membrane proteins;models molecular;monte carlo method;structure prediction;algorithms;protein folding;monte carlo simulations;membrane proteins	We propose a united-residue model of membrane proteins to investigate the structures of helix bundle membrane proteins (HBMPs) using coarse-grained (CG) replica exchange Monte-Carlo (REMC) simulations. To demonstrate the method, it is used to identify the ground state of HBMPs in a CG model, including bacteriorhodopsin (BR), halorhodopsin (HR), and their subdomains. The rotational parameters of transmembrane helices (TMHs) are extracted directly from the simulations, which can be compared with their experimental measurements from site-directed dichroism. In particular, the effects of amphiphilic interaction among the surfaces of TMHs on the rotational angles of helices are discussed. The proposed CG model gives a reasonably good structure prediction of HBMPs, as well as a clear physical picture for the packing, tilting, orientation, and rotation of TMHs. The root mean square deviation (RMSD) in coordinates of C(α) atoms of the ground state CG structure from the X-ray structure is 5.03 Å for BR and 6.70 Å for HR. The final structure of HBMPs is obtained from the all-atom molecular dynamics simulations by refining the predicted CG structure, whose RMSD is 4.38 Å for BR and 5.70 Å for HR.	amino acids;angularjs;atom;bacteriorhodopsins;cardiomyopathies;dichroism;experiment;extraction;fundamental interaction;ground state;halorhodopsins;ion-exchange chromatography procedure;mathematical model;mean squared error;membrane proteins;molecular dynamics;monte carlo method;parallel tempering;plant roots;protein data bank;set packing;simulation;small;tracer;x-ray (amazon kindle)	H.-H. Wu;C.-C. Chen;C.-M. Chen	2012	Journal of computer-aided molecular design	10.1007/s10822-012-9562-1	crystallography;biochemistry;molecular dynamics;chemistry;computational chemistry;nuclear magnetic resonance;x-ray crystallography;statistics;monte carlo method	Comp.	12.292615427808292	-60.34634319863311	97272
75224988fb7130be4680ecfe740effd1a490467b	modeling of top-down influences on object-based visual attention for robots	object representation;robot vision feature extraction object detection;bottom up;top down;long term memory;training;visualization;computational modeling;robot vision;object detection top down influences object based visual attention robots top down biasing object representations long term memory task relevant feature;feature extraction;robots;robots mediation layout active shape model computational modeling integrated circuit modeling biomimetics object detection orbital robotics visual perception;integrated circuit modeling;visual attention;encoding;object detection	The selectivity of visual attention mechanism is influenced by bottom-up competition and top-down biasing. This paper presents an object-based visual attention model which simulates top-down influences. Five components of top-down influences are modeled: learning of object representations stored in long-term memory (LTM), deduction of task-relevant feature(s), estimation of top-down biases, mediation between bottom-up and top-down fashions, and object completion processing. This model has been applied into the robotic task of object detection. Experimental results in natural and cluttered scenes are shown to validate this model.	biasing;bottom-up proteomics;natural deduction;object detection;object-based language;robot;selectivity (electronic);top-down and bottom-up design	Yuanlong Yu;George K. I. Mann;Ray G. Gosine	2009	2009 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2009.5420737	computer vision;object model;computer science;artificial intelligence;machine learning;top-down and bottom-up design;biased competition theory	Robotics	22.744153507337714	-65.16397460524013	97465
58e65330a70de4948a7bdab5913cd0905246d5f5	a novel biosensor for evaluation of apoptotic or necrotic effects of nitrogen dioxide during acute pancreatitis in rat	apoptosis;animals;flow injection analysis;rats;biosensing techniques;male;acute disease;equipment failure analysis;acute pancreatitis;equipment design;nitrogen dioxide;rats wistar;pancreatitis;l arginine;necrosis;evaluation of apoptotic or necrotic effects;levels of inflammatory mediators;cr iii	The direct and accurate estimation of nitric dioxide levels is an extremely laborious and technically demanding procedure in the molecular diagnostics of inflammatory processes. The aim of this work is to demonstrate that a stop-flow technique utilizing a specific spectroscopic biosensor can be used for detection of nanomolar quantities of NO(2) in biological milieu. The use of novel compound cis-[Cr(C(2)O(4))(AaraNH(2))(OH(2))(2)](+) increases NO(2) estimation accuracy by slowing down the rate of NO(2) uptake. In this study, an animal model of pancreatitis, where nitrosative stress is induced by either 3g/kg bw or 1.5 g/kg bw dose of L-arginine, was used. Biochemical parameters and morphological characteristics of acute pancreatitis were monitored, specifically assessing pancreatic acinar cell death mode, NO(2) generation and cellular glutathione level. The severity of the process correlated positively with NO(2) levels in pancreatic acinar cell cytosol samples, and negatively with cellular glutathione levels.	acinar cell;acute pancreatitis;arginine;cell death;cytoplasmic matrix;glutathione;milieu intérieur;necrosis;nitrogen dioxide;quantity	Dagmara Jacewicz;Aleksandra Dabrowska;Dariusz Wyrzykowski;Joanna Pranczk;Michal Wozniak;Jolanta Kubasik-Juraniec;Narcyz Knap;Kamila Siedlecka;Alexander J. Neuwelt;Lech Chmurzynski	2010		10.3390/s100100280	necrosis;chromium;chemistry;apoptosis;flow injection analysis	HCI	11.424501301537093	-65.25923380198432	97544
0d9d2117a78ea0498e46c3004911e5853552b693	biobayesnet: a web server for feature extraction and bayesian network modeling of biological sequence data	bayesian network;amino acid sequence;bayes theorem;feature vector;models molecular;structure activity relationship;internet;proteins;protein conformation;feature extraction;graphical representation;algorithms;protein folding;feature selection;pattern recognition automated;sequence analysis;molecular sequence data;quality measures;biological data;network structure;computational biology;computer simulation;information storage and retrieval;sequence analysis protein	BioBayesNet is a new web application that allows the easy modeling and classification of biological data using Bayesian networks. To learn Bayesian networks the user can either upload a set of annotated FASTA sequences or a set of pre-computed feature vectors. In case of FASTA sequences, the server is able to generate a wide range of sequence and structural features from the sequences. These features are used to learn Bayesian networks. An automatic feature selection procedure assists in selecting discriminative features, providing an (locally) optimal set of features. The output includes several quality measures of the overall network and individual features as well as a graphical representation of the network structure, which allows to explore dependencies between features. Finally, the learned Bayesian network or another uploaded network can be used to classify new data. BioBayesNet facilitates the use of Bayesian networks in biological sequences analysis and is flexible to support modeling and classification applications in various scientific fields. The BioBayesNet server is available at http://biwww3.informatik.uni-freiburg.de:8080/BioBayesNet/.	arterial pulse quality:type:pt:xxx:nom:palpation;bayesian network;fasta;feature extraction;feature selection;feature vector;graphical user interface;precomputation;server (computer);server (computing);upload;web application;web server	Swetlana Nikolajewa;Rainer Pudimat;Michael Hiller;Matthias Platzer;Rolf Backofen	2007		10.1093/nar/gkm292	computer simulation;protein folding;protein structure;structure–activity relationship;the internet;feature vector;variable-order bayesian network;biological data;feature extraction;bioinformatics;sequence analysis;bayesian network;peptide sequence;bayes' theorem;feature selection	ML	10.156064025773341	-56.877272501628404	97577
b14cd0ba5ab7d707cc17ebab860978b1cd3042c7	mapreduce based big data framework for content searching of surveillance system videos	surveillance system;sifts;mapreduce;icbir	CBIR systems go through sets of stages starting from acquiring the new images, representing these images by extracting the image features, describing the key features and eventually computing the similarity distances to get the most relevant results responding to the query image. In this paper, ICBIR an integrated CBIR Hadoop-MapReduce based framework which is split into both offline and online phases is introduced. Visual statements are built using the extracted interest points SIFTs. Later on, these visual statements are used to estimate the similarity distances which in turn are used to create the image dataset clusters. A huge vocabulary of SIFTs describing the interest points of the image is constructed. In this paper, the authors are interested in routing protocols based on clusters that aim to reduce congestion in order to have reliable data transmission and a reduced loss rate. This is achieved by balancing the traffic load, which results into a balanced energy consumption within the network.	big data;mapreduce	Zheng Xu;Zhiguo Yan;Huan Du	2015	IJSSCI	10.4018/IJSSCI.2015070104	computer science;machine learning;data mining;database;world wide web	Robotics	18.268918119231607	-55.89661855796501	98013
95c2c172412dd50be878fe923a1bbc4e4b7f2fa3	development and validation of a novel protein-ligand fingerprint to mine chemogenomic space: application to g protein-coupled receptors and their ligands	g protein coupled receptor;space application	The present study introduces a novel low-dimensionality fingerprint encoding both ligand and target properties which is suitable to mine protein-ligand chemogenomic space. Whereas ligand properties have been represented by standard descriptors, protein cavities are encoded by a fixed length bit string describing pharmacophoric properties of a definite number of binding site residues. In order to simplify the cavity fingerprint, the concept was applied here to a unique family of targets (G protein-coupled receptors) with a homogeneous cavity description. Particular attention was given to set up data sets of really diverse protein-ligand pairs covering as exhaustively as possible both ligand and target spaces. Several machine learning classification algorithms were trained on two sets of roughly 200000 receptor-ligand fingerprints with a different definition of inactive decoys. Cross-validated models show excellent precision (>0.9) in distinguishing true from false pairs with a particular preference for support vector machine classifiers. When applied to two external test sets of GPCR ligands, the most predictive models were not those performing the best in the previous cross-validation. The ability to recover true GPCR ligands (ligand prediction mode) or true GPCRs (receptor prediction mode) depends on multiple parameters: the molecular complexity of the ligands, the chemical space from which ligand decoys are selected to generate false protein-ligand pairs, and the target space under consideration. In most cases, predicting ligands is easier than predicting receptors. Although receptor profiling is possible, it probably requires a more detailed description of the ligand-binding site. Noteworthy, protein-ligand fingerprints outperform the corresponding ligand fingerprints in mining the GPCR-ligand space. Since they can be applied to a much larger number of receptors than ligand-based fingerprints, protein-ligand fingerprints represent a novel and promising way to directly screen protein-ligand pairs in chemogenomic applications.	algorithm;bit array;chemical space;cross reactions;cross-validation (statistics);dental caries;false precision;fingerprint;greater;ligands;machine learning;physical inactivity;predictive modelling;preparation;protein structure prediction;support vector machine;receptor	Nathanael Weill;Didier Rognan	2009	Journal of chemical information and modeling	10.1021/ci800447g	chemistry;bioinformatics;g protein-coupled receptor;artificial intelligence;machine learning;mathematics	ML	10.05054951642698	-57.49773564231988	98359
d464a0a748a2e79050dff80f080edeb46c40a04a	application of the genetic algorithm joint with the powell method to nonlinear least-squares fitting of powder epr spectra	nonlinear least squares;genetic algorithm	The application of the stochastic genetic algorithm (GA) in conjunction with the deterministic Powell search to analysis of the multicomponent powder EPR spectra based on computer simulation is described. This approach allows for automated extraction of the magnetic parameters and relative abundances of the component signals, from the nonlinear least-squares fitting of experimental spectra, with minimum outside intervention. The efficiency and robustness of GA alone and its hybrid variant with the Powell method was demonstrated using complex simulated and real EPR data sets. The unique capacity of the genetic algorithm for locating global minima, subsequently refined by the Powell method, allowed for successful fitting of the spectra. The influence of the population size, mutation, and crossover rates on the performance of GA was also investigated.		Tomasz Spalek;Piotr Pietrzyk;Zbigniew Sojka	2005	Journal of chemical information and modeling	10.1021/ci049863s	econometrics;mathematical optimization;genetic algorithm;chemistry;machine learning;non-linear least squares;statistics	Robotics	13.939579867060665	-53.39886326151574	98378
6587e48c3ae7137c3b116f32bb2f9a2cb24009b8	identification of hotspots in protein-protein interactions based on recursive feature elimination		The study of protein-protein interactions and protein structure through computational methods is critical to understand protein function. Hot spot residues play an important role in bioinformatics to reveal life activities. However, conventional hot spots prediction methods may face great challenges. This paper proposes a hot spot prediction method based on feature selection method SVM-RFE to improve the training performance. SMOTE based oversampling is used to adds new samples to avoid an overfitting classifier. SVM-RFE is then invoked to obtained optimal feature subset. Finally, a feature-based SVM is created to predict the hot spots. Experimental results indicate that the performance of hot spots prediction has been significantly improved compared with the previous methods.	recursion (computer science)	Xiaoli Lin;Xiaolong Zhang;Fengli Zhou	2018		10.1007/978-3-319-95930-6_56	pattern recognition;machine learning;protein structure;artificial intelligence;computer science;overfitting;support vector machine;protein–protein interaction;oversampling;recursion;feature selection	AI	10.058346303310763	-53.88928814419842	98455
2a3da663a10742852a06d1065428e3ece4178c9d	generic construction of scale-invariantly coarse grained memory		Encoding temporal information from the recent past as spatially distributed memory is essential to be able to instantly access the entire past. Any biological or synthetic agent that relies on the past to predict or plan the future, would be endowed with such a spatially distributed temporal memory. In natural settings where relevant signals show scale free temporal fluctuations, it is advantageous to have a memory system that scale invariantly coarse grains the past information while spatially distributing it. Here we examine the generic mechanism to encode the past in one spatial dimension with a specific emphasis on scale invariant coarse graining. We show that any mechanistic construction of such a memory system in the continuum limit is equivalent to encoding linear combinations of the Laplace transformed past and their approximated inverses. This mechanism is readily amenable to a discretized implementation in networks that maintain information from exponentially long past timescales within linearly increasing memory resources.	approximation algorithm;discretization;distributed memory;encode;synthetic data;triune continuum paradigm	Karthik H. Shankar	2015		10.1007/978-3-319-14803-8_14	simulation;computer science;artificial intelligence;communication	Vision	16.9570828669069	-65.43257541618789	98998
1ad74ecce51b55bdc006beb075b56bb8c300408e	accounting for intraligand interactions in flexible ligand docking with a pmf-based scoring function		We analyzed the frequency with which intraligand contacts occurred in a set of 1300 protein-ligand complexes [ Plewczynski et al. J. Comput. Chem. 2011 , 32 , 742 - 755 .]. Our analysis showed that flexible ligands often form intraligand hydrophobic contacts, while intraligand hydrogen bonds are rare. The test set was also thoroughly investigated and classified. We suggest a universal method for enhancement of a scoring function based on a potential of mean force (PMF-based score) by adding a term accounting for intraligand interactions. The method was implemented via in-house developed program, utilizing an Algo_score scoring function [ Ramensky et al. Proteins: Struct., Funct., Genet. 2007 , 69 , 349 - 357 .] based on the Tarasov-Muryshev PMF [ Muryshev et al. J. Comput.-Aided Mol. Des. 2003 , 17 , 597 - 605 .]. The enhancement of the scoring function was shown to significantly improve the docking and scoring quality for flexible ligands in the test set of 1300 protein-ligand complexes [ Plewczynski et al. J. Comput. Chem. 2011 , 32 , 742 - 755 .]. We then investigated the correlation of the docking results with two parameters of intraligand interactions estimation. These parameters are the weight of intraligand interactions and the minimum number of bonds between the ligand atoms required to take their interaction into account.		A. Y. Lizunov;A. L. Gonchar;Natalia Zaitseva;Viktor Zosimov	2015	Journal of chemical information and modeling	10.1021/acs.jcim.5b00158	simulation;chemistry;computational chemistry	Logic	11.647493283780973	-59.15341326616716	99102
136a58f16d7689b94bc1e3c108b1d96a02389cf0	recognition and restoration of periodic patterns with recurrent neural network	periodic patterns pattern recognition pattern restoration recurrent neural network temporal supervised learning algorithm;neural networks;supervised learning;neural nets;information science;nervous system;noise generators;temporal supervised learning algorithm;computerised pattern recognition;learning systems;recurrent network;periodic patterns;pattern restoration;pattern recognition recurrent neural networks supervised learning noise generators neural networks information science laboratories humans nervous system equations;pattern recognition;computerised picture processing;humans;recurrent neural networks;recurrent neural network;neural nets computerised pattern recognition computerised picture processing learning systems	This paper is concerned with the recognition and restomtion of periodic patterns by the recurrent neural network. Using the f i l l y recurrent network with fhe temporal supervised learning algorithm developed by Williamr and Zipser, we performed several ezperiments aiming at recognizing and restoring periodic patterns. The results of our ezperiments can be summarized by the following three points. First, the recurrent network could recognize complez and multiple patterns simultaneously, i f appmpn'ate number of hidden units are given. Second, the network could regenerate infinitely the patterns with appropriate precision. Third, the recurrent network could recognize and generate infinitely the patterns in spite of the ezistence of a large number of noises in the course of learning.	algorithm;artificial neural network;circuit restoration;recurrent neural network;supervised learning	Ryotaro Kamimura	1990		10.1109/SPDP.1990.143579	computer science;artificial intelligence;recurrent neural network;machine learning;pattern recognition;artificial neural network	AI	19.57745558260794	-65.49383146900479	99175
507e6cb981572224c511f7a30ceea749aba38cd8	qspr treatment of solvent scales	quantitative structure property relationship;predictive value	The results of the quantitative structure -property relationship (QSPR) analysis of 45 different solvent scales and 350 solvents using the CODESSA program are presented. The QSPR models for each of the scales are constructed using only theoretical descriptors. The high quality of the models (32 of the 45 give R2 > 0.90, only two haveR2 < 0.82) enables direct calculation of predicted values for any scale for any previously unmeasured solvent. The descriptors involved are shown to be in good agreement with the physical concepts invoked by the original authors of the scales.	display resolution;quantitative structure–activity relationship	Alan R. Katritzky;Tarmo Tamm;Yilin Wang;Sulev Sild;Mati Karelson	1999	Journal of Chemical Information and Computer Sciences	10.1021/ci980225h	econometrics;chemistry;organic chemistry	HPC	12.79517061891053	-58.51578767538841	99243
146c0c3db0a31d518cd85dbb75db53b84aad3db9	virtual screening discovery of new acetylcholinesterase inhibitors issued from cermn chemical library	inhibicion;chimie informatique;criblage virtuel;etude experimentale;toxicidad;toxicite;computational chemistry;virtual screening;amyloid;depistage;descubrimiento;amiloide;quimica informatica;toxicity;medical screening;inhibition;acetylcholinesterase;amyloide;estudio experimental;in vitro;cribado virtual	In our quest to find new inhibitors able to inhibit acetylcholinesterase (AChE) and, at the same time, to protect neurons from beta amyloid toxicity, i.e., inhibitors interacting with the catalytic anionic subsite as well as with the peripherical anionic site of AChE, a virtual screening of the Centre d'Etudes et de Recherche sur le Medicament de Normandie (CERMN) chemical library was carried out. Two complementary approaches were applied, i.e., a ligand- and a structure-based screening. Each screening led to the selection of different compounds, but only two were present in both screening results. In vitro tests on AChE showed that one of those compounds presented a very good inhibition activity, of the same order as Donepezil. This result shows the real complementary of both methods for the discovery of new ligands.		Jana Sopkova-de Oliveira Santos;Aurélien Lesnard;Jean-Hugues Agondanou;Nathalie Dupont;Anne-Marie Godard;Silvia Stiebing;Christophe Rochais;Frédéric Fabis;Patrick Dallemagne;Ronan Bureau;Sylvain Rault	2010	Journal of chemical information and modeling	10.1021/ci900491t	stereochemistry;chemistry;virtual screening;toxicity;combinatorial chemistry;computational chemistry;amyloid;nanotechnology;in vitro	Comp.	10.15250143947729	-61.58459230606423	100196
9ea16aeed4cb7664f7b962e52275f31a3bbf846e	deep multi-task attribute-driven ranking for fine-grained sketch-based image retrieval		With touch-screen devices becoming ever more ubiquitous, sketch holds great promise as an intuitive and efficient mode of input compared to classic alternatives. This has motivated a major revival of interest in vision-based analysis of sketches, notably in sketch-based image retrieval (SBIR). Superior to classic SBIR methods, finegrained SBIR (FG-SBIR) methods [1] are proposed to make fine-grained retrieval in categorylevel.	image retrieval;job control (unix);sketch;touchscreen	Jifei Song;Yi-Zhe Song;Tao Xiang;Timothy M. Hospedales;Xiang Ruan	2016			computer vision;visual word;computer science;pattern recognition;artificial intelligence;information retrieval;image retrieval;sketch;ranking	Vision	23.929675889639977	-57.4818870256475	100420
b8c0cfb28ae96218d3e15b1db227cf457c929659	interactive discriminative mining of chemical fragments	efficiency;drug design;graphical mining	Structural activity prediction is one of the most important tasks in chemoinformatics. The goal is to predict a property of interest given structural data on a set of small compounds or drugs. Ideally, systems that address this task should not just be accurate, they should also be able to identify an interpretable discriminative structure which describes the most discriminant structural elements with respect to some target. The application of ILP in an interactive software for discriminative mining of chemical fragments is presented in this paper. In particular, it is described the coupling of an ILP system with a molecular visualisation software that allows a chemist to graphically control the search for interesting patterns in chemical fragments. Furthermore, we show how structural information, such as rings, functional groups like carboxyl, amine, methyl, ester, etc are integrated and exploited in the search.	cheminformatics;discriminant	Nuno A. Fonseca;Max Pereira;Vítor Santos Costa;Rui Camacho	2010		10.1007/978-3-642-21295-6_10	bioinformatics;machine learning;data mining;efficiency;drug design	SE	10.896432081148168	-57.347460748843766	100526
848252faf2156c12c7ce1a8ada0b5cafdb380710	an interactive node-link visualization of convolutional neural networks		Convolutional neural networks are at the core of state-ofthe-art approaches to a variety of computer vision tasks. Visualizations of neural networks typically take the form of static diagrams, or interactive toy-sized networks, which fail to illustrate the networks’ scale and complexity, and furthermore do not enable meaningful experimentation. Motivated by this observation, this paper presents a new interactive visualization of neural networks trained on handwritten digit recognition, with the intent of showing the actual behavior of the network given user-provided input. The user can interact with the network through a drawing pad, and watch the activation patterns of the network respond in real-time. The visualization is available at http://scs.ryerson.ca/ ∼aharley/vis/.	activation function;artificial neural network;computer vision;convolutional neural network;diagram;interactive visualization;interactivity;neural networks;real-time clock	Adam W. Harley	2015		10.1007/978-3-319-27857-5_77	computer vision;computer science;artificial intelligence;theoretical computer science;machine learning;deep learning	ML	22.419945708482526	-53.490396295174385	101199
344a38c4c943182eb2487bfe73571b698910e060	similarity-based image retrieval from plural key images by self-organizing map with refractoriness	computers;image features;image feature similarity based image retrieval plural key images self organizing map map layer;image feature;plural key images;spectrum;artificial neural networks;map layer;self organising feature maps;computer experiment;organizing;image color analysis;feature extraction;self organizing map;similarity based image retrieval;self organized map;neurons;self organising feature maps image retrieval;neurons image color analysis image retrieval artificial neural networks feature extraction organizing computers;image retrieval	In this research, we propose a similarity-based image retrieval from plural key images by self-organizing map with refractoriness. In the self-organizing map with refractoriness, the plural neurons in the map layer corresponding to the input can fire sequentially because of the refractoriness. The proposed image retrieval system from plural key images using the self-organizing map with refractoriness makes use of this property in order to retrieve plural similar images. In this image retrieval system, as the image feature, not only color information but also spectrum, impression words and keywords are employed. In the proposed system, the similarity-based image retrieval from plural key images can be realized. We carried out a series of computer experiments and confirmed that the effectiveness of the proposed system.	computer experiment;feature (computer vision);image retrieval;neuron;organizing (structure);self-organization;self-organizing map	Hiroyuki Tanirnizu;Yuko Osana	2008	2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)	10.1109/IJCNN.2008.4633890	spectrum;computer vision;visual word;computer experiment;self-organizing map;feature extraction;image retrieval;computer science;machine learning;feature;information retrieval;artificial neural network	Vision	24.141029622385474	-63.61470948377297	101204
1b5fac8a99723e6a742ae74b48a81aa2b932fe10	3d mesh segmentation via multi-branch 1d convolutional neural networks		There is an increasing interest in applying deep learning to 3D mesh segmentation. We observe that 1) existing feature-based techniques are often slow or sensitive to feature resizing, 2) there are minimal comparative studies and 3) techniques often suffer from reproducibility issue. This study contributes in two ways. First, we propose a novel convolutional neural network (CNN) for mesh segmentation. It uses 1D data, filters and a multi-branch architecture for separate training of multi-scale features. Together with a novel way of computing conformal factor (CF), our technique clearly out-performs existing work. Secondly, we publicly provide implementations of several deep learning techniques, namely, neural networks (NNs), autoencoders (AEs) and CNNs, whose architectures are at least two layers deep. The significance of this study is that it proposes a robust form of CF, offers a novel and accurate CNN technique, and a comprehensive study of several deep learning techniques for baseline comparison.	2d filters;artificial neural network;autoencoder;baseline (configuration management);benchmark (computing);convolutional neural network;deep learning;downstream (software development);feature vector;gabor filter;ground truth;network architecture;pacific symposium on biocomputing;radio frequency;sampling (signal processing);supervised learning	David George;Xianghua Xie;Gary K. L. Tam	2018	Graphical Models	10.1016/j.gmod.2018.01.001	convolutional neural network	SE	24.383916586170855	-52.61440258057657	101443
c3d3ede57efa78a003afaea64f17610041b52f60	deep neural networks for learning spatio-temporal features from tomography sensors		We demonstrate accurate spatio-temporal gait data classification from raw tomography sensor data without the need to reconstruct images. This is based on a simple yet efficient machine learning methodology based on a convolutional neural network architecture for learning spatio-temporal features, automatically end-to-end from raw sensor data. In a case study on a floor pressure tomography sensor, experimental results show an effective gait pattern classification F-score performance of 97.88 $\pm$  1.70%. It is shown that the automatic extraction of classification features from raw data leads to a substantially better performance, compared to features derived by shallow machine learning models that use the reconstructed images as input, implying that for the purpose of automatic decision-making it is possible to eliminate the image reconstruction step. This approach is portable across a range of industrial tasks that involve tomography sensors. The proposed learning architecture is computationally efficient, has a low number of parameters and is able to achieve reliable classification F-score performance from a limited set of experimental samples. We also introduce a floor sensor dataset of 892 samples, encompassing experiments of 10 manners of walking and 3 cognitive-oriented tasks to yield a total of 13 types of gait patterns.	algorithmic efficiency;artificial neural network;convolutional neural network;deep learning;end-to-end principle;experiment;f1 score;iterative reconstruction;machine learning;network architecture;sensor;tomography	Omar Costilla-Reyes;Patricia Scully;Krikor B. Ozanyan	2018	IEEE Transactions on Industrial Electronics	10.1109/TIE.2017.2716907	convolutional neural network;raw data;iterative reconstruction;data classification;artificial neural network;deep learning;engineering;image sensor;feature extraction;computer vision;artificial intelligence	ML	23.11882530813027	-56.95883286022783	101488
94591a7c3b713b1093163240c40c5487d67889d5	molecular docking with ligand attached water molecules	fisica molecular;proteine liaison;hydrogen;hydrogene;interaction moleculaire;molecular interaction;biologia molecular;eau;ligand binding;fixation ligand;physique moleculaire;interaccion molecular;molecular biology;proteina enlace;binding protein;agua;molecular physics;fijacion ligando;hidrogeno;water;biologie moleculaire	A novel approach to incorporate water molecules in protein-ligand docking is proposed. In this method, the water molecules display the same flexibility during the docking simulation as the ligand. The method solvates the ligand with the maximum number of water molecules, and these are then retained or displaced depending on energy contributions during the docking simulation. Instead of being a static part of the receptor, each water molecule is a flexible on/off part of the ligand and is treated with the same flexibility as the ligand itself. To favor exclusion of the water molecules, a constant entropy penalty is added for each included water molecule. The method was evaluated using 12 structurally diverse protein-ligand complexes from the PDB, where several water molecules bridge the ligand and the protein. A considerable improvement in successful docking simulations was found when including flexible water molecules solvating hydrogen bonding groups of the ligand. The method has been implemented in the docking program Molegro Virtual Docker (MVD).	4-dichlorobenzene;boat dock;computed tomographic colonography;docker;docking (molecular);exclusion;hydrogen bonding;ligands;protein data bank;protein structure prediction;protein–ligand docking;pyschological bonding;simulation	Mette Alstrup Lie;René Thomsen;Christian N. S. Pedersen;Birgit Schiøtt;Mikael H. Christensen	2011	Journal of chemical information and modeling	10.1021/ci100510m	stereochemistry;water;hydrogen;chemistry;searching the conformational space for docking;computational chemistry;ligand;binding protein	Robotics	11.279195807452485	-60.8213693682577	101499
fff32bd594b45f6bd38131c8502f5a77e642c95f	mysirna: improving sirna efficacy prediction using a machine-learning model combining multi-tools and whole stacking energy (δg)	whole stacking energy;sirna functionality prediction;δg;gibbs energy;δ g;sirna efficiency prediction;artificial neural network	The investigation of small interfering RNA (siRNA) and its posttranscriptional gene-regulation has become an extremely important research topic, both for fundamental reasons and for potential longer-term therapeutic benefits. Several factors affect the functionality of siRNA including positional preferences, target accessibility and other thermodynamic features. State of the art tools aim to optimize the selection of target siRNAs by identifying those that may have high experimental inhibition. Such tools implement artificial neural network models as Biopredsi and ThermoComposition21, and linear regression models as DSIR, i-Score and Scales, among others. However, all these models have limitations in performance. In this work, a neural-network trained new siRNA scoring/efficacy prediction model was developed based on combining two existing scoring algorithms (ThermoComposition21 and i-Score), together with the whole stacking energy (ΔG), in a multi-layer artificial neural network. These three parameters were chosen after a comparative combinatorial study between five well known tools. Our developed model, 'MysiRNA' was trained on 2431 siRNA records and tested using three further datasets. MysiRNA was compared with 11 alternative existing scoring tools in an evaluation study to assess the predicted and experimental siRNA efficiency where it achieved the highest performance both in terms of correlation coefficient (R(2)=0.600) and receiver operating characteristics analysis (AUC=0.808), improving the prediction accuracy by up to 18% with respect to sensitivity and specificity of the best available tools. MysiRNA is a novel, freely accessible model capable of predicting siRNA inhibition efficiency with improved specificity and sensitivity. This multiclassifier approach could help improve the performance of prediction in several bioinformatics areas. MysiRNA model, part of MysiRNA-Designer package [1], is expected to play a key role in siRNA selection and evaluation.		Mohamed Mysara;Mahmoud ElHefnawi;Jonathan M. Garibaldi	2012	Journal of biomedical informatics	10.1016/j.jbi.2012.02.005	gibbs free energy;computer science;bioinformatics;machine learning;data mining;artificial neural network	HCI	10.649001644809017	-56.46416473790944	101549
d64b29538bc21b5f110f241dcf8029aded0659e2	saliency detection using joint temporal and spatial decorrelation		This article presents a scene-driven (i.e. bottom-up) visual saliency detection technique for videos. The proposed method utilizes non-negative matrix factorization (NMF) to replicate neural responses of primary visual cortex neurons in spatial domain. In temporal domain, principal component analysis (PCA) was applied to imitate the effect of stimulus change experience during neural adaptation phenomena. We apply the proposed saliency model to background subtraction problem. The proposed method does not rely on any background model and is purely unsupervised. In experimental results, it will be shown that the proposed method competes well with some of the state-of-the-art back- ground subtraction techniques especially in dynamic scenes.	decorrelation	Hamed Rezazadegan Tavakoli;Esa Rahtu;Janne Heikkilä	2013		10.1007/978-3-642-38886-6_66	computer vision;computer science;machine learning	Vision	22.720109069213628	-55.750932970424536	101951
f3017c63860104ad6c53368180ddcd8c31ab9577	paccmann: prediction of anticancer compound sensitivity with multi-modal attention-based neural networks		We present a novel approach for the prediction of anticancer compound sensitivity by means of multi-modal attention-based neural networks (PaccMann). In our approach, we integrate three key pillars of drug sensitivity, namely, the molecular structure of compounds, transcriptomic profiles of cancer cells as well as prior knowledge about interactions among proteins within cells. Our models ingest a drug-cell pair consisting of SMILES encoding of a compound and the gene expression profile of a cancer cell and predicts an IC50 sensitivity value. Gene expression profiles are encoded using an attention-based encoding mechanism that assigns high weights to the most informative genes. We present and study three encoders for SMILES string of compounds: 1) bidirectional recurrent 2) convolutional 3) attention-based encoders. We compare our devised models against a baseline model that ingests engineered fingerprints to represent the molecular structure. We demonstrate that using our attention-based encoders, we can surpass the baseline model. The use of attention-based encoders enhance interpretability and enable us to identify genes, bonds and atoms that were used by the network to make a prediction.	artificial neural network;baseline (configuration management);encoder;fingerprint;gene expression profiling;information;interaction;modal logic;simplified molecular-input line-entry system	Ali Oskooei;Jannis Born;Matteo Manica;Vigneshwari Subramanian;Julio S'aez-Rodr'iguez;Mar'ia Rodr'iguez Mart'inez	2018	CoRR			Comp.	10.646411824028288	-56.238080246912695	101968
3c361c35b3691c005576bf0187f1fc84a49f3d7c	toward the discovery of functional transthyretin amyloid inhibitors: application of virtual screening methods	hachage;distributed system;image tridimensionnelle;fisica molecular;inhibicion;chimie informatique;criblage virtuel;systeme reparti;proteine;relacion estructura actividad;relation structure activite;estudio comparativo;momentum;stabilization;solubility;computational chemistry;etude comparative;physique moleculaire;vecino mas cercano;quantite mouvement;virtual screening;cantidad movimiento;hashing;sistema repartido;estabilizacion;blood;comparative study;amyloid;solubilite;amiloide;plasma;tridimensional image;quimica informatica;plus proche voisin;nearest neighbour;proteina;stabilisation;inhibition;protein;molecular physics;amyloide;sangre;sang;structure activity relation;solubilidad;in vitro;imagen tridimensional;cribado virtual	Inhibition of amyloid fibril formation by stabilization of the native form of the protein transthyretin (TTR) is a viable approach for the treatment of familial amyloid polyneuropathy that has been gaining momentum in the field of amyloid research. The TTR stabilizer molecules discovered to date have shown efficacy at inhibiting fibrilization in vitro but display impairing issues of solubility, affinity for TTR in the blood plasma and/or adverse effects. In this study we present a benchmark of four protein- and ligand-based virtual screening (VS) methods for identifying novel TTR stabilizers: (i) two-dimensional (2D) similarity searches with chemical hashed, pharmacophore, and UNITY fingerprints, (ii) 3D searches based on shape, chemical, and electrostatic similarity, (iii) LigMatch, a new ligand-based method which uses multiple templates and combines 3D geometric hashing with a 2D preselection process, and (iv) molecular docking to consensus X-ray crystal structures of TTR. We illustrate the potential of the best-performing VS protocols to retrieve promising new leads by ranking a tailored library of 2.3 million commercially available compounds. Our predictions show that the top-scoring molecules possess distinctive features from the known TTR binders, holding better solubility, fraction of halogen atoms, and binding affinity profiles. To the best of our knowledge, this is the first attempt to rationalize the utilization of a large battery of in silico screening techniques toward the identification of a new generation of TTR amyloid inhibitors.	amyloid neuropathies;amyloid neuropathies, familial;benchmark (computing);clinical use template;crystal structure;docking (molecular);docking -molecular interaction;fibril - cell component;fingerprint;geometric hashing;halogens;inhibition;ligands;plasma active;polyneuropathy;processor affinity;protocols documentation;score;virtual screening;amyloid fibril formation	Carlos J. V. Simões;Trishna Mukherjee;Rui M. M. Brito;Richard M. Jackson	2010	Journal of chemical information and modeling	10.1021/ci100250z	plasma;hash function;chemistry;virtual screening;comparative research;computational chemistry;amyloid;nanotechnology;solubility;in vitro;momentum;quantum mechanics	Comp.	10.351798067031094	-61.618159026952064	102000
033b88f1274adddb9ded4cd23f79d447881c22e6	towards social robots: automatic evaluation of human-robot interaction by face detection and expression classification	time scale;video streaming;ucsd;automatic assessment;real time;real time processing;human robot interaction;automatic evaluation;automatic detection;feature integration;feature selection;support vector machine;facial expression;computer animation;face detection;face to face;social robot	Computer animated agents and robots bring a social dimension to human computer interaction and force us to think in new ways about how computers could be used in daily life. Face to face communication is a real-time process operating at a time scale of less than a second. In this paper we present progress on a perceptual primitive to automatically detect frontal faces in the video stream and code them with respect to 7 dimensions in real time: neutral, anger, disgust, fear, joy, sadness, surprise. The face finder employs a cascade of feature detectors trained with boosting techniques [13, 2]. The expression recognizer employs a novel combination of Adaboost and SVM’s. The generalization performance to new subjects for a 7-way forced choice was 93.3% and 97% correct on two publicly available datasets. The outputs of the classifier change smoothly as a function of time, providing a potentially valuable representation to code facial expression dynamics in a fully automatic and unobtrusive manner. The system was deployed and evaluated for measuring spontaneous facial expressions in the field in an application for automatic assessment of human-robot interaction.	adaboost;computer animation;face detection;finite-state machine;human computer;human–computer interaction;human–robot interaction;real-time computing;real-time locating system;sadness;sensor;smoothing;social robot;spontaneous order;streaming media	Gwen Littlewort;Marian Stewart Bartlett;Ian R. Fasel;Joel Chenu;Takayuki Kanda;Hiroshi Ishiguro;Javier R. Movellan	2003			support vector machine;computer vision;face detection;speech recognition;computer science;social robot;machine learning;computer animation;feature selection;facial expression	HCI	24.606352314251833	-60.849879212889405	102634
bcb7614d63ad1a549e1bd6d971f2352fe699b281	graph kernels for chemical compounds using topological and three-dimensional local atom pair environments	shortest path;kernel function;support vector regression;three dimensional;search trees;structure activity relationship;cheminformatics;biological activity;machine learning;support vector classification;lookup table;support vector machine;molecular graph;distance matrix;graph kernel;structural properties	Approaches that can predict the biological activity or properties of a chemical compound are an important application of machine learning. In this paper, we introduce a new kernel function for measuring the similarity between chemical compounds and for learning their related properties and activities. The method is based on local atom pair environments which can be rapidly computed by using the topological all-shortest paths matrix and the geometrical distance matrix of a molecular graph as lookup tables. The local atom pair environments are stored in prefix search trees, so called tries, for an efficient comparison. The kernel can be either computed as an optimal assignment kernel or as a corresponding convolution kernel over all local atom similarities. We implemented the Tanimoto kernel, min kernel, minmax kernel and the dot product kernel as local kernels, which are computed recursively by traversing the tries. We tested the approach on eight structure-activity and structure-property molecule benchmark data sets from the literature. The models were trained with @e- support vector regression and support vector classification. The local atom pair kernels showed to be at least competitive to state-of-the-art kernels in seven out of eight cases in a direct comparison. A comparison against literature results using similar experimental setups as in the original works confirmed these findings. The method is easy to implement and has robust default parameters.	atom;graph kernel	Georg Hinselmann;Nikolas Fechner;Andreas Jahn;Matthias Eckert;Andreas Zell	2010	Neurocomputing	10.1016/j.neucom.2010.03.008	support vector machine;kernel method;combinatorics;string kernel;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;computer science;theoretical computer science;cheminformatics;machine learning;graph kernel;mathematics;tree kernel;variable kernel density estimation;polynomial kernel;kernel smoother	Theory	14.18256808625222	-58.00990926296341	102722
d833507052d6e68b7f2098096c6f2909321f746c	evaluation of extended parameter sets for the 3d-qsar technique map: implications for interpretability and model quality exemplified by antimalarially active naphthylisoquinoline alkaloids	factorial design;difference operator;model complexity;surface properties;consistency model;biological activity;molecular surface	The 3D-QSAR technique MaP (Mapping Property distributions of molecular surfaces) characterises biologically active compounds in terms of the distribution of their surface properties (H-bond donor, H-bond acceptor, hydrophilic, weakly hydrophobic, strongly hydrophobic). The MaP descriptor is alignment-independent and yields chemically intuitive models. In this study, the impact of different operational parameters on the interpretability and model quality was investigated. Based on a set of antimalarially active naphthylisoquinoline alkaloids the effect of hydrophobicity assignment as well as the differentiation of H-bond propensity was evaluated according to a full factorial design. It turns out, that including different categories for H-bond donor strength significantly improved interpretability, reduced model complexity, and made possible the derivation of a novel pharmacophore hypothesis for this dataset. Further analysis of the factorial design reveals, that MaP models are robust to parameter changes and generate consistent models for different parameter settings.	acceptor (semiconductors);alignment;categories;map;methamphetamine;pharmacophore;population parameter;quantitative structure-activity relationship;quantitative structure–activity relationship;secologanin tryptamine alkaloids;silo (dataset)	Nikolaus Stiefl;Gerhard Bringmann;Christian Rummey;Knut Baumann	2003	Journal of computer-aided molecular design	10.1023/A:1026125706388	factorial experiment;consistency model;biological activity;statistics	EDA	11.863420037040967	-58.70540865064449	102913
2fb56807a330243c73199b2cb4599bbc9a9c0aa6	recurrent semi-supervised classification and constrained adversarial generation with motion capture data		We explore recurrent encoder multi-decoder neural network architectures for semi-supervised sequence classification and reconstruction. We find that the use of multiple reconstruction modules helps models generalize in a classification task when only a small amount of labeled data is available, which is often the case in practice. Such models provide useful high-level representations of motions allowing clustering, searching and faster labeling of new sequences. We also propose a new, realistic partitioning of a well-known, high quality motion-capture dataset for better evaluations. We further explore a novel formulation for future-predicting decoders based on conditional recurrent generative adversarial networks, for which we propose both soft and hard constraints for transition generation derived from desired physical properties of synthesized future movements and desired animation goals. We find that using such constraints allow to stabilize the training of recurrent adversarial architectures for animation generation.	artificial neural network;cluster analysis;display resolution;encoder;generative adversarial networks;high- and low-level;machine learning;motion capture;recurrent neural network;semi-supervised learning;semiconductor industry;supervised learning	Félix G. Harvey;Julien Roy;David Kanaa;Christopher Joseph Pal	2018	Image Vision Comput.	10.1016/j.imavis.2018.07.001	encoder;artificial neural network;labeled data;artificial intelligence;animation;adversarial system;generative grammar;pattern recognition;mathematics;cluster analysis;motion capture	ML	19.511069044286938	-53.13878180160185	103044
ce57b65dd27860f73527f0e95f4d4edd967af9bc	real time measurement of cellular oxygen uptake rates (our) by a fiber optic sensor	oxygen uptake rats;biological tissues;optical variables measurement;diagnostic tool;liver;fluorescence;hepg2 cells oxygen uptake rats cell encapsulation oxygen sensor alginate;optical fiber sensors;real time;hepg2 cells;oxygen;hepg2 cell;oxygen sensor;fibre optic sensors;shape measurement;hydrogels biological tissues fibre optic sensors;biochemical parameter;liver cells;glass;fiber optic;fiber optic sensor;oxygen uptake rate;alginate;monitoring;real time measurement;aminoacids;cell behavior monitoring;hydrogel matrix;cell proliferation;cellular oxygen uptake rates;noninvasive diagnostic tool;cell encapsulation;optical sensors;sugar;hydrogels;regeneration engineering;fiber optic sensing technology;optical microscopy;oxygen uptake;biochemical parameters;cells biology;oxygen optical fiber sensors shape measurement monitoring optical sensors regeneration engineering sugar glass optical microscopy fluorescence;real time systems;cell behavior monitoring real time measurement cellular oxygen uptake rates fiber optic sensor biochemical parameters aminoacids liver cells oxygen uptake rate hydrogel matrix fiber optic sensing technology noninvasive diagnostic tool	Advancements in fiber optic sensors have made it possible for measuring biochemical parameters such as oxygen, glucose, and aminoacids necessary for viable engineered tissue growth. In this study, we have devised an experimental protocol to measure in real-time, the Oxygen Uptake Rate (OUR) values for HEPG2 liver cells when grown a) on cover glass slides; and b) encapsulated within alginate based hydrogel matrices. For both cases, the oxygen uptake rates of HEPG2 cells at selected time points varied in close co-relation with cell proliferation and metabolic activity during the 7-day culture period. Microscopy studies have also been conducted to assess the viability of the encapsulated cells within the alginate matrices. This investigation concludes that OUR can be used as an indicative parameter to assess the metabolic activity of cells encapsulated within the hydrogel matrix. The study also presents a fiber-optic sensing technology as a non-invasive diagnostic tool to monitor cell behavior and activity.	emoticon;fiber optic sensor;optical fiber;real-time clock	Binil Starly;Shih Feng Lan	2009	2009 IEEE International Conference on Virtual Environments, Human-Computer Interfaces and Measurements Systems	10.1109/VECIMS.2009.5068878	hep g2;cell encapsulation;fluorescence;computer science;optical fiber;fiber optic sensor;oxygen sensor;optical microscope;self-healing hydrogels;glass;oxygen;cell growth	Visualization	12.715402382180724	-65.96642500309908	103454
8ceeea730e2ceb02c3ab362f89ab6ed3fb35f077	computational feasibility of an exhaustive search of side-chain conformations in protein-protein docking	combinatorial geometry;conformational search;flexible docking;rotamers	Protein-protein docking procedures typically perform the global scan of the proteins relative positions, followed by the local refinement of the putative matches. Because of the size of the search space, the global scan is usually implemented as rigid-body search, using computationally inexpensive intermolecular energy approximations. An adequate refinement has to take into account structural flexibility. Since the refinement performs conformational search of the interacting proteins, it is extremely computationally challenging, given the enormous amount of the internal degrees of freedom. Different approaches limit the search space by restricting the search to the side chains, rotameric states, coarse-grained structure representation, principal normal modes, and so on. Still, even with the approximations, the refinement presents an extreme computational challenge due to the very large number of the remaining degrees of freedom. Given the complexity of the search space, the advantage of the exhaustive search is obvious. The obstacle to such search is computational feasibility. However, the growing computational power of modern computers, especially due to the increasing utilization of Graphics Processing Unit (GPU) with large amount of specialized computing cores, extends the ranges of applicability of the brute-force search methods. This proof-of-concept study demonstrates computational feasibility of an exhaustive search of side-chain conformations in protein pocking. The procedure, implemented on the GPU architecture, was used to generate the optimal conformations in a large representative set of protein-protein complexes. © 2018 Wiley Periodicals, Inc.		Taras Dauzhenka;Petras J. Kundrotas;Ilya A. Vakser	2018	Journal of computational chemistry	10.1002/jcc.25381	combinatorial chemistry;side chain;computational chemistry;chemistry;macromolecular docking;brute-force search	Comp.	12.677686217696653	-60.99339723671068	103830
26953847d5967f9db67dab41b5530d7f0cab1447	sigma-rf: prediction of the variability of spatial restraints in template-based modeling by random forest	computational biology bioinformatics;models molecular;structural homology protein;models statistical;artificial intelligence;algorithms;sequence alignment;combinatorial libraries;computer appl in life sciences;microarrays;bioinformatics	In template-based modeling when using a single template, inter-atomic distances of an unknown protein structure are assumed to be distributed by Gaussian probability density functions, whose center peaks are located at the distances between corresponding atoms in the template structure. The width of the Gaussian distribution, the variability of a spatial restraint, is closely related to the reliability of the restraint information extracted from a template, and it should be accurately estimated for successful template-based protein structure modeling. To predict the variability of the spatial restraints in template-based modeling, we have devised a prediction model, Sigma-RF, by using the random forest (RF) algorithm. The benchmark results on 22 CASP9 targets show that the variability values from Sigma-RF are of higher correlations with the true distance deviation than those from Modeller. We assessed the effect of new sigma values by performing the single-domain homology modeling of 22 CASP9 targets and 24 CASP10 targets. For most of the targets tested, we could obtain more accurate 3D models from the identical alignments by using the Sigma-RF results than by using Modeller ones. We find that the average alignment quality of residues located between and at two aligned residues, quasi-local information, is the most contributing factor, by investigating the importance of input features used in the RF machine learning. This average alignment quality is shown to be more important than the previously identified quantity of a local information: the product of alignment qualities at two aligned residues.	3d modeling;alignment;assumed;benchmark (computing);casp10 gene;clinical use template;contribution;distance;extraction;homology (biology);homology modeling;modeller;machine learning;normal statistical distribution;physical restraint equipment (device);probability density;protein, organized by structure;rf modulator;radio frequency;random forest;spatial variability;algorithm;caspase-9;width	Juyong Lee;Kiho Lee;InSuk Joung;Keehyoung Joo;Bernard R. Brooks;Jooyoung Lee	2015		10.1186/s12859-015-0526-z	biology;dna microarray;computer science;bioinformatics;theoretical computer science;machine learning;sequence alignment	Comp.	11.575457317195987	-58.4370902898784	103856
7db0604e1abce99c458a530d35ec9adb0f899af3	modélisation de la stéréochimie : une application à la chémoinformatique. (encoding of stereochimistry applied to cheminformatics)		In the framework of prediction of molecule’s properties, graph kernels allow to combine a natural encoding of a molecule by a graph with classical statistical tools. Unfortunately some molecules encoded by a same graph and differing only by the three dimensional orientations of their atoms in space have different properties. Such molecules are called stereoisomers. This work aims to take into account stereoisomerism into graph kernels method. In this document we first present the main methods of prediction of molecule’s properties, and we focus on methods based on graph kernels. Based on this state of the art, we present stereoisomers and a state of the art of methods which take into account those molecules. Then we propose to encode stereoisomers by ordered graphs. We define minimal stereo subgraphs, which are subgraphs that locally characterizes the stereochemistry. Those subgraphs are used to define a kernel that take into account the stereochemistry. Finally we propose three extensions for this kernel. Those extensions allow to consider the neighbourhoods of minimal stereo subgraphs and to compare different minimal stereo subgraphs. RAMEAU Index : Kernel functions, Cheminformatics, Pattern recognition systems, Stereochemistry, Machine learning Discipline : Informatique et applications. Université de Caen Basse-Normandie, ENSICAEN, CNRS GREYC équipe image 6 Boulevard Maréchal Juin, 14050 Caen cedex, France		Pierre-Anthony Grenier	2015				ML	14.456970278877131	-58.464572175232114	104012
b4782a2e64602c620ef553939ceff0fca9b15a3b	a chip for estrogen receptor action: detection of biomarkers released by mcf-7 cells through estrogenic and anti-estrogenic effects	bisphenol a;cell proliferation;endocrine-disrupting chemicals;protein microarray	The fluorescence-based multi-analyte chip platform for the analysis of estrogenic and anti-estrogenic substances is a new in vitro tool for the high throughput screening of environmental samples. In contrast to existing tools, the chip investigates the complex action of xenoestrogens in a human cell model by characterizing protein expression. It allows for the quantification of 10 proteins secreted by MCF-7 cells, representing various biological and pathological endpoints of endocrine action and distinguishing between estrogen- and anti-estrogen-dependent secretion of proteins. Distinct protein secretion patterns of the cancer cell line after exposure to known estrogen receptor agonists ß-estradiol, bisphenol A, genistein, and nonylphenol as well as antagonists fulvestrant and tamoxifen demonstrate the potential of the chip. Stimulation of cells with Interleukin-1ß shifts concentrations of low abundant biomarkers towards the working range of the chip. In the non-stimulated cell culture, Matrix Metalloproteinase 9 (MMP-9) and Vascular Endothelial Growth Factor (VEGF) show differences upon treatment with antagonists and agonists of the estrogen receptor. In stimulated MCF-7 cells challenged with receptor agonists secretion of Monocyte Chemoattractant Protein (MCP-1), Interleukin-6 (IL-6), Rantes, and Interleukin-8 (IL-8) significantly decreases. In parallel, the proliferating effect of endocrine-disrupting substances in MCF-7 cells is assessed in a proliferation assay based on resazurin. Using ethanol as a solvent for test substances increases the background of proliferation and secretion experiments, while using dimethyl sulfoxide (DMSO) does not show any adverse effects. The role of the selected biomarkers in different physiological processes such as cell development, reproduction, cancer, and metabolic syndrome makes the chip an excellent tool for either indicating endocrine-disrupting effects in food and environmental samples, or for screening the effect of xenoestrogens on a cellular and molecular level.	biological markers;cell culture techniques;cell development;cell secretion;chemotactic factors;dimethyl sulfoxide;estradiol;estrogen receptors;estrogens;ethanol;experiment;fluorescence;genistein;interleukin-8;metabolic syndrome x;numerous;physiological processes;protein secretion;quantitation;sulfoxides;tamoxifen;throughput;analyte;bisphenol a;cancer cell;estrogen receptor alpha, human;fulvestrant;nonylphenol;protein expression;resazurin;substance	Konstanze Gier;Claudia Preininger;Ursula Sauer	2017		10.3390/s17081760	cell biology;genistein;cancer cell;estrogen;analytical chemistry;secretion;fulvestrant;estrogen receptor;endocrinology;cell growth;biology;internal medicine;receptor	Comp.	10.588335868841742	-64.13825335846683	104282
d4a1fb7e76c7b15aac7e708f33736cd61b12c502	learning a peripersonal space representation as a visuo-tactile prediction task		The space immediately surrounding our body, or peripersonal space, is crucial for interaction with the environment. In primate brains, specific neural circuitry is responsible for its encoding. An important component is a safety margin around the body that draws on visuo-tactile interactions: approaching stimuli are registered by vision and processed, producing anticipation or prediction of contact in the tactile modality. The mechanisms of this representation and its development are not understood. We propose a computational model that addresses this: a neural network composed of a Restricted Boltzmann Machine and a feedforward neural network. The former learns in an unsupervised manner to represent position and velocity features of the stimulus. The latter is trained in a supervised way to predict the position of touch (contact). Unique to this model, it considers: (i) stimulus position and velocity, (ii) uncertainty of all variables, and (iii) not only multisensory integration but also prediction.	artificial neural network;computational model;electronic circuit;feedforward neural network;interaction;modality (human–computer interaction);restricted boltzmann machine;supervised learning;unsupervised learning;velocity (software development)	Zdenek Straka;Matej Hoffmann	2017		10.1007/978-3-319-68600-4_13	artificial intelligence;pattern recognition;multisensory integration;machine learning;anticipation;feedforward neural network;encoding (memory);stimulus (physiology);artificial neural network;computer science;biological neural network;restricted boltzmann machine	ML	18.97312844264641	-65.94208270842655	104514
4e2b800eb1f8b29c6c8f25c7bb697d874bc13549	generalized workflow for generating highly predictive in silico off-target activity models		Chemical structure data and corresponding measured bioactivities of compounds are nowadays easily available from public and commercial databases. However, these databases contain heterogeneous data from different laboratories determined under different protocols and, in addition, sometimes even erroneous entries. In this study, we evaluated the use of data from bioactivity databases for the generation of high quality in silico models for off-target mediated toxicity as a decision support in early drug discovery and crop-protection research. We chose human acetylcholinesterase (hAChE) inhibition as an exemplary end point for our case study. A standardized and thorough quality management routine for input data consisting of more than 2,200 chemical entities from bioactivity databases was established. This procedure finally enables the development of predictive QSAR models based on heterogeneous in vitro data from multiple laboratories. An extended applicability domain approach was used, and regression results were refined by an error estimation routine. Subsequent classification augmented by special consideration of borderline candidates leads to high accuracies in external validation achieving correct predictive classification of 96%. The standardized process described herein is implemented as a (semi)automated workflow and thus easily transferable to other off-targets and assay readouts.		Lennart T. Anger;Antje Wolf;Klaus-Jürgen Schleifer;Dieter Schrenk;Sebastian G. Rohrer	2014	Journal of chemical information and modeling	10.1021/ci500342q	bioinformatics;data science;data mining	ML	13.210627477979708	-58.83786220309621	104709
03a20e9e430b3c54360e26df27d8a157509b9c82	ifacewat: the interfacial water-implemented re-ranking algorithm to improve the discrimination of near native structures for protein rigid docking	antigen antibody complex;software;journal article;computational biology bioinformatics;hydrogen bonding;models molecular;proteins;protein conformation;computer science and engineering;protein binding;algorithms;humans;combinatorial libraries;computer appl in life sciences;computer simulation;water;microarrays;bioinformatics	Protein-protein docking is an in silico method to predict the formation of protein complexes. Due to limited computational resources, the protein-protein docking approach has been developed under the assumption of rigid docking, in which one of the two protein partners remains rigid during the protein associations and water contribution is ignored or implicitly presented. Despite obtaining a number of acceptable complex predictions, it seems to-date that most initial rigid docking algorithms still find it difficult or even fail to discriminate successfully the correct predictions from the other incorrect or false positive ones. To improve the rigid docking results, re-ranking is one of the effective methods that help re-locate the correct predictions in top high ranks, discriminating them from the other incorrect ones. In this paper, we propose a new re-ranking technique using a new energy-based scoring function, namely IFACEwat - a combined Interface Atomic Contact Energy (IFACE) and water effect. The IFACEwat aims to further improve the discrimination of the near-native structures of the initial rigid docking algorithm ZDOCK3.0.2. Unlike other re-ranking techniques, the IFACEwat explicitly implements interfacial water into the protein interfaces to account for the water-mediated contacts during the protein interactions. Our results showed that the IFACEwat increased both the numbers of the near-native structures and improved their ranks as compared to the initial rigid docking ZDOCK3.0.2. In fact, the IFACEwat achieved a success rate of 83.8% for Antigen/Antibody complexes, which is 10% better than ZDOCK3.0.2. As compared to another re-ranking technique ZRANK, the IFACEwat obtains success rates of 92.3% (8% better) and 90% (5% better) respectively for medium and difficult cases. When comparing with the latest published re-ranking method F2Dock, the IFACEwat performed equivalently well or even better for several Antigen/Antibody complexes. With the inclusion of interfacial water, the IFACEwat improves mostly results of the initial rigid docking, especially for Antigen/Antibody complexes. The improvement is achieved by explicitly taking into account the contribution of water during the protein interactions, which was ignored or not fully presented by the initial rigid docking and other re-ranking techniques. In addition, the IFACEwat maintains sufficient computational efficiency of the initial docking algorithm, yet improves the ranks as well as the number of the near native structures found. As our implementation so far targeted to improve the results of ZDOCK3.0.2, and particularly for the Antigen/Antibody complexes, it is expected in the near future that more implementations will be conducted to be applicable for other initial rigid docking algorithms.	boat dock;computational resource;docking (molecular);docking -molecular interaction;macromolecular docking;mental association;muscle rigidity;scientific publication;score;scoring functions for docking;algorithm	Chinh Tran To Su;Thuy-Diem Nguyen;Jie Zheng;Chee Keong Kwoh	2014		10.1186/1471-2105-15-S16-S9	computer simulation;biology;protein structure;water;plasma protein binding;simulation;dna microarray;computer science;bioinformatics;hydrogen bond	Comp.	10.705418181404664	-59.11213425574211	104863
5c22995dcefea56a854fdba8162c618a7763318d	reinforcement learning based visual attention with application to face detection	detectors;learning;training;optimal control;visualization;face recognition;principal component analysis;face;search problems;visual perception problems reinforcement learning based visual attention face detection cognitive process mainstream approach focal visual attention modelling saliency detection search process visual search adaptive learning process approximate optimal control framework;optimal control face recognition image retrieval learning artificial intelligence;learning artificial intelligence;visualization face training principal component analysis learning search problems detectors;image retrieval	Visual attention is the cognitive process of directing our gaze on one aspect of the visual field while ignoring others. The mainstream approach to modeling focal visual attention involves identifying saliencies in the image and applying a search process to the salient regions. However, such inference schemes commonly fail to accurately capture perceptual attractors, require massive computational effort and, generally speaking, are not biologically plausible. This paper introduces a novel approach to the problem of visual search by framing it as an adaptive learning process. In particular, we devise an approximate optimal control framework, based on reinforcement learning, for actively searching a visual field. We apply the method to the problem of face detection and demonstrate that the technique is both accurate and scalable. Moreover, the foundations proposed here pave the way for extending the approach to other large-scale visual perception problems.	approximation algorithm;artificial neural network;cognition;color vision;computation;focal (programming language);face detection;framing (world wide web);machine vision;optimal control;reinforcement learning;rendering (computer graphics);scalability;top-down and bottom-up design	Benjamin Goodrich;Itamar Arel	2012	2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2012.6239177	facial recognition system;face;computer vision;detector;face detection;visualization;optimal control;image retrieval;computer science;artificial intelligence;machine learning;geometry;principal component analysis	Vision	23.114701457726262	-54.759116185338286	105523
acc2672d23fcfd0bc876ee072619f1b2d597dc22	systematic search for pairwise dependencies of torsion angles	biological patents;biomedical journals;text mining;europe pubmed central;citation search;computer applications in chemistry;citation networks;theoretical and computational chemistry;computational biology bioinformatics;research articles;abstracts;open access;life sciences;clinical guidelines;full text;rest apis;orcids;europe pmc;documentation and information in chemistry;biomedical research;bioinformatics;literature search	Most available tools for conformer generation, like OMEGA [1], ROTATE [2], and MIMUMBA [3], divide the conformational space into quantized degrees of freedom, i.e. torsion angles, which are treated independently. The independence of torsions is however not valid for all fragments [4]. There are pairs of mutually dependent degrees of freedom e.g. two consecutive torsion angles in aryl-X-aryl systems. The fact that two torsions are dependent implies that if one of the torsions is set to a specific angle, the set of possible angles for the other torsion is limited. This knowledge could be used to significantly narrow down the conformational space in deterministic rule-based conformation generators. For our systematic search for pairwise dependent torsion angles, we assembled a set of about 200 chemical patterns, each describing a torsion angle and part of its environment. The patterns range from very general descriptions like ‘rotatable bond between two sp3 hybridized atoms’ to patterns describing a more specific molecular environment. As a first approach we tried to replicate the examples given by Bramelt et al. using suitable chemical patterns and a CSD [5] subset of about 73,000 molecules as a database. We then performed a pairwise analysis of all our chemical patterns, including each pattern with itself, using again the CSD subset of about 73,000 molecules. We used two different search scenarios. In our first search the torsion angles had to be directly next to each other while in our second search they had to be exactly one bond apart. Using our systematic search approach we found many additional examples for dependent torsion angles, confirming the findings of Bramelt et al. and supporting their advice to search for pairs of mutually dependent conformation variables.	cambridge structural database;logic programming;self-replicating machine;torsion (gastropod)	Christin Schärfer;Tanja Schulz-Gasch;Matthias Rarey	2012		10.1186/1758-2946-4-S1-P36	text mining;medical research;computer science;bioinformatics;theoretical computer science;data mining	Comp.	14.05873652605172	-59.57060533354803	105579
ffea8705adeef90e2ffee018921e9ebce6255b24	automation of a procedure to find the polynomial which best fits (k, c1, c2, t) data of electrolyte solutions by non-linear regression analysis using mathematica® software	non linear regression;statistical significance;comparative modeling;conductivity;electrolytes;thermodynamic properties;regression analysis;process analytical chemistry;analytical chemistry	A MATHEMATICA package, 'CONDU.M', has been developed to find the polynomial in concentration and temperature which best fits conductimetric data of the type (kappa, c, T) or (kappa, c1, c2, T) of electrolyte solutions (kappa: specific conductivity; ci: concentration of component i; T: temperature). In addition, an interface, 'TKONDU', has been written in the TCL/Tk language to facilitate the use of CONDU.M by an operator not familiarised with MATHEMATICA. All this software is available on line (UPV/EHU, 2001). 'CONDU.M' has been programmed to: (i) select the optimum grade in c1 and/or c2; (ii) compare models with linear or quadratic terms in temperature; (iii) calculate the set of adjustable parameters which best fits data; (iv) simplify the model by elimination of 'a priori' included adjustable parameters which after the regression analysis result in low statistical significance; (v) facilitate the location of outlier data by graphical analysis of the residuals; and (vi) provide quantitative statistical information on the quality of the fit, allowing a critical comparison among different models. Due to the multiple options offered the software allows testing different conductivity models in a short time, even if a large set of conductivity data is being considered simultaneously. Then, the user can choose the best model making use of the graphical and statistical information provided in the output file. Although the program has been initially designed to treat conductimetric data, it can be also applied for processing data with similar structure, e.g. (P, c, T) or (P, c1, c2, T), being P any appropriate transport, physical or thermodynamic property.	automation;coefficient of determination;electrolytes;emoticon;excretory function;fits;graphical user interface;greater than;models, statistical;outlier;polynomial;population parameter;property (philosophy);r squared;regression analysis;sodium chloride;solutions;solvents;tcl;water;weight;wolfram mathematica;potassium bromide	E. Cortazar;A. Usobiaga;Luis A. Fernández;A. De Diego;Juan Manuel Madariaga	2002	Computers & chemistry	10.1016/S0097-8485(01)00115-2	econometrics;homology modeling;chemistry;computer science;bioinformatics;conductivity;electrolyte;machine learning;mathematics;statistical significance;process analytical chemistry;algorithm;regression analysis;nonlinear regression;statistics	ML	15.711652271692408	-59.593647540669274	105798
466012a9482ed87ae48265ac154ea69d2b77306c	abnormal work cycle detection based on dissimilarity measurement of trajectories	motion analysis;neural networks;assisted living;activity level recognition;technology and engineering;video based monitoring;activities of daily living	This paper proposes a method for detecting the abnormalities of the executed work cycles for the factory workers using their tracks obtained in a multi-camera network. The method allows analyzing both spatial and temporal dissimilarity between the pairwise tracks. The main novelty of the methods is calculating spatial dissimilarity between pair-wise tracks by aligning them using Dynamic Time Warping (DTW) based on coordinate distance, and specially the velocity and dwell time dissimilarity using a different track alignment based on velocity difference. These dissimilarity measurements are used to cluster the executed work cycles and detect abnormalities. The experimental results show that our algorithm outperforms other methods on clustering the tracks because of the use of temporal dissimilarity.	algorithm;cluster analysis;cycle detection;dynamic time warping;sensor;velocity (software development)	Xingzhe Xie;Dimitri Van Cauwelaert;Maarten Slembrouck;Karel Bauters;Johannes Cottyn;Dirk Van Haerenborgh;Hamid K. Aghajan;Peter Veelaert;Wilfried Philips	2015		10.1145/2789116.2789142	computer vision;simulation;activities of daily living;computer science;machine learning;artificial neural network	Vision	19.472795601151176	-57.803252347468835	105889
ead3f5fb56d35c67122a7ca4247dc41d750947b5	shot boundary detection for h.264/avc bitstreams with frames containing multiple types of slices	h 264 avc;frames;technology and engineering;automatic detection;shot boundary dectection;compressed video;shot boundary detection	A Novel Active Learning Approach for SVM Based Web Image Retrieval p. 40 The AVS China National Standard Technology, Applications and Products An End-to-End Application System of AVS: AVS-IPTV in China p. 50 An AVS Based IPTV System Network Trial p. 58 Usage of MPEG-2 to AVS Transcoder in IPTV System p. 65 The Audio and Video Synchronization Method Used in AVS System p. 71 Human Face and Action Recognition Expression-Invariant Face Recognition with Accurate Optical Flow p. 78 Real-Time Facial Feature Point Extraction p. 88 Local Dual Closed Loop Model Based Bayesian Face Tracking p. 98 View-Independent Human Action Recognition by Action Hypersphere in Nonlinear Subspace p. 108		Sarah De Bruyne;Wesley De Neve;Davy De Schrijver;Peter Lambert;Piet Verhoeve;Rik Van de Walle	2007		10.1007/978-3-540-77255-2_20	reference frame;computer vision;computer science;artificial intelligence;theoretical computer science;frame;multimedia	Vision	20.410296627064888	-58.57156810440357	106929
577a675e619c0889af9acf8b7b5678955f13984c	improvement on higher-order neural networks for invariant object recognition	object recognition;invariance echelle;neural model;invarianza;reseau neuronal ordre eleve;invarianza escala;higher order;invariance;distortion;invariant pattern recognition;rotation invariance;distorsion;pattern recognition;numeral;higher order neural network;rotacion;reconnaissance forme;reseau neuronal;chiffre;reconocimiento patron;cifra;rotation;red neuronal;scale invariance;neural network	The higher order neural network(HONN) was proved to be able to realize invariant object recognition. By taking the relationship between input units into account, HONN's are superior to other neural models in invariant pattern recognition. However, there are two main problems preventing HONN's from practical applications. One is the combinatorial increase of weight number, that is, as input size increases, the number of weights in a HONN increases exponentially. The other problem is sensitivity to distortion and noise. In this paper, we described a method, in which by modifying the constraints imposed on the weights in HONN's, the performance of a HONN with respect to distortion can be improved considerably.	distortion;image noise;information;neural networks;outline of object recognition;pattern recognition;sensitivity and specificity	Zhengquan He;Mohammed Yakoob Siyal	1999	Neural Processing Letters	10.1023/A:1018610829733	computer vision;distortion;computer science;artificial intelligence;machine learning;mathematics;artificial neural network	ML	23.500273843048294	-61.706762258244275	107509
ae61bfbd5ef7aabab75e3ef28cd042d6c0b209b1	sfcscorerf: a random forest-based scoring function for improved affinity prediction of protein-ligand complexes		A major shortcoming of empirical scoring functions for protein-ligand complexes is the low degree of correlation between predicted and experimental binding affinities, as frequently observed not only for large and diverse data sets but also for SAR series of individual targets. Improvements can be envisaged by developing new descriptors, employing larger training sets of higher quality, and resorting to more sophisticated regression methods. Herein, we describe the use of SFCscore descriptors to develop an improved scoring function by means of a PDBbind training set of 1005 complexes in combination with random forest for regression. This provided SFCscore(RF) as a new scoring function with significantly improved performance on the PDBbind and CSAR-NRC HiQ benchmarks in comparison to previously developed SFCscore functions. A leave-cluster-out cross-validation and performance in the CSAR 2012 scoring exercise point out remaining limitations but also directions for further improvements of SFCscore(RF) and empirical scoring functions in general.		David Zilian;Christoph A. Sotriffer	2013	Journal of chemical information and modeling	10.1021/ci400120b	computer science;machine learning;pattern recognition;data mining	Vision	11.127616897099532	-56.161307578709554	107526
2b6de8970d61078b6ce0af03d71a74f16b3c761b	comparison of neural network algorithms for face recognition	face recognition;neural network	In the last couple of decades, engineers, neuroscientists and psychologists have turned their attention to face recognition by humans and computer vision systems. Images of different complexities have been tested with a variety of methods. The goals of each research vary, as vary the applications. We present a neural method of recognizing faces using features obtained from compression of these faces with different methods. The extracted fea ti tres are used as inputs to a feedforward neural network. The neural network is trained	algorithm;artificial neural network;computer vision;facial recognition system;feedforward neural network	Evangelia Micheli-Tzanakou;E. Uyeda;Rajarshi Ray;A. Sharma;R. Ramanujan;J. Dong	1995	Simulation	10.1177/003754979506500105	computer science;time delay neural network;neocognitron;artificial neural network	AI	22.053975166174766	-63.93348488351471	107592
df210506d1603c1dfba45a8831b8c0d599b7ac46	a novel biosensor to detect micrornas rapidly	microrna	δ-free F0F1-ATPase within chromatophore was constructed as a novel biosensor to detect miRNA targets. Specific miRNA probes were linked to each rotary β subunits of F0F1-ATPase. Detection of miRNAs was based on the proton flux change induced by lightdriven rotation of δ-free F0F1-ATPase. The hybridization reaction was indicated by changes in the fluorescent intensity of pHsensitive CdTe quantum dots. Our results showed that the assay was attomole sensitivities (1.2× 10−18 mol) to target miRNAs and capable of distinguishing among miRNA family members. Moreover, the method could be used to monitor real-time hybridization without any complicated fabrication before hybridization. Thus, the rotary biosensor is not only sensitive and specific to detect miRNA target but also easy to perform. The δ-free F0F1-ATPase-based rotary biosensor may be a promising tool for the basic research and clinical application of miRNAs.		Jie-Ying Liao;James Q. Yin;Jia-Chang Yue	2009	J. Sensors	10.1155/2009/671896	molecular biology;computer science;bioinformatics;nanotechnology;microrna	HCI	10.441414089427305	-64.06718356669981	107619
2d7361dd9b0eb18cc6d1693ac917473cdb04162a	a review on video-based human activity recognition	healthcare monitoring;segmentation;security surveillance;feature representation;human activity recognition;human computer interface	This review article surveys extensively the current progresses made toward video-based human activity recognition. Three aspects for human activity recognition are addressed including core technology, human activity recognition systems, and applications from low-level to high-level representation. In the core technology, three critical processing stages are thoroughly discussed mainly: human object segmentation, feature extraction and representation, activity detection and classification algorithms. In the human activity recognition systems, three main types are mentioned, including single person activity recognition, multiple people interaction and crowd behavior, and abnormal activity recognition. Finally the domains of applications are discussed in detail, specifically, on surveillance environments, entertainment environments and healthcare systems. Our survey, which aims to provide a comprehensive state-of-the-art review of the field, also addresses several challenges associated with these systems and applications. Moreover, in this survey, various applications are discussed in great detail, specifically, a survey on the applications in healthcare monitoring systems. OPEN ACCESS Computers 2013, 2 89	activity recognition;algorithm;background subtraction;coefficient;enigma machine;feature extraction;hidden markov model;hidden surface determination;high- and low-level;information;mel-frequency cepstrum;software deployment;speech recognition	Shian-Ru Ke;Le Uyen Thuc Hoang;Yong-Jin Lee;Jenq-Neng Hwang;Jang-Hee Yoo;Kyoung-Ho Choi	2013	Computers	10.3390/computers2020088	computer vision;engineering;data mining;communication	HCI	24.58246965295099	-58.72805482750535	107649
13d6f4c143297480925d766112089da2008c9ed6	predicting electron paths		Chemical reactions can be described as the stepwise redistribution of electrons in molecules. As such, reactions are often depicted using “arrow-pushing” diagrams which show this movement as a sequence of arrows. We propose an electron path prediction model (ELECTRO) to learn these sequences directly from raw reaction data. Instead of predicting product molecules directly from reactant molecules in one shot, learning a model of electron movement has the benefits of (a) being easy for chemists to interpret, (b) incorporating constraints of chemistry, such as balanced atom counts before and after the reaction, and (c) naturally encoding the sparsity of chemical reactions, which usually involve changes in only a small number of atoms in the reactants. We design a method to extract approximate reaction paths from any dataset of atom-mapped reaction SMILES strings. Our model achieves state-of-the-art results on a subset of the UPSTO reaction dataset. Furthermore, we show that our model recovers a basic knowledge of chemistry without being explicitly trained to do so.	approximation algorithm;atom;diagram;electron;emoticon;simplified molecular-input line-entry system;sparse matrix;stepwise regression	John Bradshaw;Matt J. Kusner;Brooks Paige;Marwin H. S. Segler;José Miguel Hernández-Lobato	2018	CoRR		electron;atomic physics;statistical physics;molecule;chemical reaction;encoding (memory);atom;small number;chemistry	NLP	14.423194252314586	-59.465071142425565	107872
1109540c91666f4c60c75e93c02493a779df7963	stem cell enrichment with selectin receptors: mimicking the ph environment of trauma	hydrogen ion concentration;cd34 hematopoietic stem cells;wounds and injuries;up regulation;l selectin;equipment failure analysis;cell separation;equipment design;cells cultured;humans;hematopoietic stem cells;biomimetic;acidic ph	The isolation of hematopoietic stem and progenitor cells (HSPCs) is critical for transplantation therapy and HSPC research, however current isolation techniques can be prohibitively expensive, time-consuming, and produce variable results. Selectin-coated microtubes have shown promise in rapidly isolating HSPCs from human bone marrow, but further purification of HSPCs remains a challenge. Herein, a biomimetic device for HSPC isolation is presented to mimic the acidic vascular microenvironment during trauma, which can enhance the binding frequency between L-selectin and its counter-receptor PSGL-1 and HSPCs. Under acidic pH conditions, L-selectin coated microtubes enhanced CD34+ HSPC adhesion, as evidenced by decreased cell rolling velocity and increased rolling flux. Dynamic light scattering was utilized as a novel sensor to confirm an L-selectin conformational change under acidic conditions, as previously predicted by molecular dynamics. These results suggest that mimicking the acidic conditions of trauma can induce a conformational extension of L-selectin, which can be utilized for flow-based, clinical isolation of HSPCs.	acids;biomimetic devices;biomimetics;bone marrow;cd34 antigens;flux limiter;gene ontology term enrichment;habitat;isolate compound;l-selectin;ligands;molecular dynamics;psma7 gene;patients;photon correlation spectroscopy;processor affinity;purification of quantum state;selectins;stem cells;velocity (software development);wounds and injuries;receptor	Thong M. Cao;Michael J. Mitchell;Jane L Liesveld;Michael R. King	2013		10.3390/s130912516	biomimetics;biochemistry;downregulation and upregulation;nanotechnology		10.776351800567914	-64.57593528536701	108102
52d57e682983495dd965c0f5e6e5eac9eea24175	distributed automated docking of flexible ligands to proteins: parallel applications of autodock 2.4	degree of freedom;simulated annealing;root mean square deviation;drug design;force field;molecular docking;parallel applications;heterogeneous network	AutoDock 2.4 predicts the bound conformations of a small, flexible ligand to a nonflexible macromolecular target of known structure. The technique combines simulated annealing for conformation searching with a rapid grid-based method of energy evaluation based on the AMBER force field. AutoDock has been optimized in performance without sacrificing accuracy; it incorporates many enhancements and additions, including an intuitive interface. We have developed a set of tools for launching and analyzing many independent docking jobs in parallel on a heterogeneous network of UNIX-based workstations. This paper describes the current release, and the results of a suite of diverse test systems. We also present the results of a systematic investigation into the effects of varying simulated-annealing parameters on molecular docking. We show that even for ligands with a large number of degrees of freedom, root-mean-square deviations of less than 1 A from the crystallographic conformation are obtained for the lowest-energy dockings, although fewer dockings find the crystallographic conformation when there are more degrees of freedom.		Garrett M. Morris;David S. Goodsell;Ruth Huey;Arthur J. Olson	1996	Journal of computer-aided molecular design	10.1007/BF00124499	root-mean-square deviation;simulation;chemistry;heterogeneous network;docking;simulated annealing;searching the conformational space for docking;computer science;bioinformatics;machine learning;force field;computational chemistry;degrees of freedom;drug design	EDA	12.090328958489406	-60.27819287404858	108803
3b2bb6b3b1dfc8e036106ad73a58ee9b7bc8f887	an efficient end-to-end neural model for handwritten text recognition.		Offline handwritten text recognition from images is an important problem for enterprises attempting to digitize large volumes of handmarked scanned documents/reports. Deep recurrent models such as Multi-dimensional LSTMs [10] have been shown to yield superior performance over traditional Hidden Markov Model based approaches that suffer from the Markov assumption and therefore lack the representational power of RNNs. In this paper we introduce a novel approach that combines a deep convolutional network with a recurrent Encoder-Decoder network to map an image to a sequence of characters corresponding to the text present in the image. The entire model is trained end-to-end using Focal Loss [18], an improvement over the standard Cross-Entropy loss that addresses the class imbalance problem, inherent to text recognition. To enhance the decoding capacity of the model, Beam Search algorithm is employed which searches for the best sequence out of a set of hypotheses based on a joint distribution of individual characters. Our model takes as input a downsampled version of the original image thereby making it both computationally and memory efficient. The experimental results were benchmarked against two publicly available datasets, IAM and RIMES. We surpass the state-of-the-art word level accuracy on the evaluation set of both datasets by 3.5% & 1.1%, respectively.	artificial neural network;beam search;benchmark (computing);correctness (computer science);cross entropy;decimation (signal processing);deep learning;encoder;end-to-end principle;focal (programming language);hidden markov model;identity management;image quality;language model;markov chain;online and offline;optical character recognition;search algorithm	Arindam Chowdhury;Lovekesh Vig	2018			computer science;machine learning;pattern recognition;artificial intelligence;markov property;hidden markov model;joint probability distribution;decoding methods;end-to-end principle;beam search	Vision	21.81915992500679	-55.138393126320864	108893
e6bcdfd6833db87c34fdedece7aec7cdde67e860	assessment of solvated interaction energy function for ranking antibody-antigen binding affinities		Affinity modulation of antibodies and antibody fragments of therapeutic value is often required in order to improve their clinical efficacies. Virtual affinity maturation has the potential to quickly focus on the critical hotspot residues without the combinatorial explosion problem of conventional display and library approaches. However, this requires a binding affinity scoring function that is capable of ranking single-point mutations of a starting antibody. We focus here on assessing the solvated interaction energy (SIE) function that was originally developed for and is widely applied to scoring of protein-ligand binding affinities. To this end, we assembled a structure-function data set called Single-Point Mutant Antibody Binding (SiPMAB) comprising several antibody-antigen systems suitable for this assessment, i.e., based on high-resolution crystal structures for the parent antibodies and coupled with high-quality binding affinity measurements for sets of single-point antibody mutants in each system. Using this data set, we tested the SIE function with several mutation protocols based on the popular methods SCWRL, Rosetta, and FoldX. We found that the SIE function coupled with a protocol limited to sampling only the mutated side chain can reasonably predict relative binding affinities with a Spearman rank-order correlation coefficient of about 0.6, outperforming more aggressive sampling protocols. Importantly, this performance is maintained for each of the seven system-specific component subsets as well as for other relevant subsets including non-alanine and charge-altering mutations. The transferability and enrichment in affinity-improving mutants can be further enhanced using consensus ranking over multiple methods, including the SIE, Talaris, and FOLDEF energy functions. The knowledge gained from this study can lead to successful prospective applications of virtual affinity maturation.		Traian Sulea;Victor Vivcharuk;Christopher R. Corbeil;Christophe Deprez;Enrico O. Purisima	2016	Journal of chemical information and modeling	10.1021/acs.jcim.6b00043	crystallography;chemistry;bioinformatics;combinatorial chemistry	Comp.	10.685772529182016	-59.36094723854726	109866
1b3faf93846601c2f1f4529d2badef3d41b418fb	matching organic libraries with protein-substructures	molecular mimicry;three dimensional;virtual screening;active site	We present a general approach which allows automatic identification of sub-structures in proteins that resemble given three-dimensional templates. This paper documents its success with non-peptide templates such as beta-turn mimetics. We considered well-tested turn-mimetics such as the bicyclic turned dipeptide (BTD), spiro lactam (Spiro) and the 2,5-disubstituded tetrahydrofuran (THF), a new furan-derivative which was recently developed and characterized. The detected geometric similarity between the templates and the protein patches corresponds to r.m.s.-values of 0.3 A for more than 80% of the constituting atoms, which is typical for active site comparisons of homologous proteins. This fast automatic procedure might be of biomedical value for finding special mimicking leads for particular protein sub-structures as well as for template-assembled synthetic protein (TASP) design.		Robert Preissner;Andrean Goede;Kristian Rother;F. Osterkamp;Ulrich Koert;Cornelius Frömmel	2001	Journal of computer-aided molecular design	10.1023/A:1013158818807	three-dimensional space;biochemistry;stereochemistry;chemistry;virtual screening;active site;organic chemistry;combinatorial chemistry;computational chemistry	Comp.	11.46576373747257	-59.390063799641396	110185
bf06518afc7df11f2f7b3b8ac85f1166c763ac91	a proposal for the molecular basis of μ and δ opiate receptor differentiation based on modeling of two types of cyclic enkephalins and a narcotic alkaloid	molecular modeling;low energy;amino acid;vector space;satisfiability;dihedral angle;opioid peptide	The molecular basis underlying the divergent receptor selectivity of two cyclic opioid peptides Tyr-c[N delta-D-Orn2-Gly-Phe-Leu-] (c-ORN) and [D-Pen2,L-Cys5]-enkephalinamide (c-PEN) was investigated using a molecular modeling approach. Ring closure and conformational searching procedures were used to determine low-energy cyclic backbone conformers. Following reinsertion of amino acid side chains, the narcotic alkaloid 7 alpha-[(1R)-1-methyl-1-hydroxy-3-phenylpropyl]-6-14-endoethenotetrahy dro oripavine (PEO) was used as a flexible template for bimolecular superpositions with each of the determined peptide ring conformers using the coplanarity and cocentricity of the phenolic rings as the minimum constraint. A vector space of PEO, accounting for all possible orientations for the C21-aromatic ring of PEO served as a geometrical locus for the aromatic ring of the Phe4 residue in the opioid peptides. Although a vast number of polypeptide conformations satisfied the criteria of the opiate pharmacophore, they could be grouped into three classes differing in magnitude and sign of the torsional angle values of the tyrosyl side chain. Only class III conformers for both c-ORN and c-PEN, having tyramine dihedral angles chi 1 = 150 degrees +/- 30 degrees and chi 2 = -155 degrees +/- 20 degrees, had significant structural and conformational properties that were mutually compatible while respecting the PEO vector space. Comparison of these properties in the context of the divergent receptor selectivity of the studied opioid peptides suggests that the increased distortion of the peptide backbone in the closure region of c-PEN together with the pendant beta,beta-dimethyl group, combine to generate a steric volume which is absent in c-ORN and that may be incompatible with a restrictive topography of the mu receptor. The nature and stereochemistry of substituents adjacent to the closure region of the peptides could also modulate receptor selection by interacting with a charged (delta) or neutral (mu) subsite.	aromatics;alkaloids;amino acids;analgesics, non-narcotic;cell differentiation process;charge (electrical);class;clinical use template;closure;cyclic amp;cyclic neutropenia;enkephalins;isoquinoline alkaloid biosynthesis pathway;mental orientation;narcotics;opioid peptide;opioid receptor;oripavine derivatives analgesics;pendant dosage form;phenylalanine;polyethylene glycols;polypeptides;receptors, opioid, delta;receptors, opioid, mu;ring device;stereochemistry (discipline);surgical replantation;tyramine;tyrosine;vertebral column;beta thalassemia;gamma-delta t-cell receptor;molecular modeling	André Michel;Gérald Villeneuve;John DiMaio	1991	Journal of computer-aided molecular design	10.1007/BF00135314	biochemistry;stereochemistry;amino acid;chemistry;vector space;opioid peptide;organic chemistry;molecular model;dihedral angle;satisfiability	Comp.	10.634106219396175	-62.18648667637695	111062
4c40fae6ad838105f56c1af1e74517a62f926bf1	comfa and docking study of novel estrogen receptor subtype selective ligands	ligand binding domain;score function;cross validation;binding affinity;estrogen receptor	We present the results from a Comparative Molecular Field Analysis (CoMFA) and docking study of a diverse set of 36 estrogen receptor ligands whose relative binding affinities (RBA) with respect to 17beta-Estradiol were available in both isoforms of the nuclear estrogen receptors (ER alpha, ER beta). Initial CoMFA models exhibited a correlation between the experimental relative binding affinities and the molecular steric and electrostatic fields; ER alpha: r2 = 0.79, q2 = 0.44 ER beta: r2 = 0.93, q2 = 0.63. Addition of the solvation energy of the isolated ligand improved the predictive nature of the ER beta model initially; r2 = 0.96, q2 = 0.70 but upon rescrambling of the data-set and reselecting the training set at random, inclusion of the ligand solvation energy was found to have little effect on the predictive nature of the CoMFA models. The ligands were then docked inside the ligand binding domain (LBD) of both ER alpha and ER beta utilizing the docking program Gold, after-which the program CScore was used to rank the resulting poses. Inclusion of both the Gold and CScore scoring parameters failed to improve the predictive ability of the original CoMFA models. The subtype selectivity expressed as RBA(ER alpha/ER beta) of the test sets was predicted using the most predictive CoMFA models, as illustrated by the cross-validated r2. In each case the most selective ligands were ranked correctly illustrating the utility of this method as a prescreening tool in the development of novel estrogen receptor subtype selective ligands.	boat dock;class;docking (molecular);docking -molecular interaction;entry sequenced data set;erdős–rényi model;estradiol;estrogen receptors;estrogens;globulins;gonadal steroid hormones;lafora disease;ligand binding domain;processor affinity;protein isoforms;pyrazoles;shbg gene;score;selectivity (electronic);sensitivity and specificity;sex factors;staphylococcal protein a;test set;beta thalassemia;biomedical engineering field;estrogen receptor alpha, human;ligands activity;non-t, non-b childhood acute lymphoblastic leukemia	Peter Wolohan;David E. Reichert	2003	Journal of computer-aided molecular design	10.1023/A:1026104924132	stereochemistry;chemistry;bioinformatics;estrogen receptor;computational chemistry;ligand;score;genetics;cross-validation;statistics	Comp.	10.12022242940439	-59.39405564813815	111278
e298fc8cb47ceaaeac092fde42991f12d5304f22	spikenet: real-time visual processing with one spike per neuron	real time visualization;shunting inhibition;spiking neurons;temporal coding;visual processing	SpikeNet is an image-processing system that uses very large-scale networks of asynchronously -ring neurons. The latest version allows very e0cient object identi-cation in real-time using a video input, and although this speci-c implementation is designed to run on standard computer hardware, there are a number of clear implications for computational neuroscience. Speci-cally, SpikeNet demonstrates the plausibility of visual processing based on a single feed-forward pass and very sparse levels of -ring. Above all, it is one of the very few models compatible with the severe temporal constraints imposed by experimental data on processing speed in the visual system. c © 2004 Elsevier B.V. All rights reserved.	computation;computational neuroscience;computer hardware;image processing;neuron;plausibility structure;real-time clock;real-time transcription;sparse matrix	Simon J. Thorpe;Rudy Guyonneau;Nicolas Guilbaud;Jong-Mo Allegraud;Rufin van Rullen	2004	Neurocomputing	10.1016/j.neucom.2004.01.138	computer vision;computer science;theoretical computer science;machine learning	Graphics	18.646186777149133	-65.08663330270588	111290
2b2716e2e6bf8dff56da5048b1bc66566a3e18cf	stereo saliency map considering affective factors in a dynamic environment	bottom up;saliency map;bottom up selective attention;independent component analysis;dynamic environment;binocular vision;visual search;integrated saliency map;affective attention;selective attention;stereo saliency map;visual attention;affective computing;neural network	We propose a new integrated saliency map model, which reflects more human-like visual attention mechanism. The proposed model considers not only the binocular stereopsis to construct a final attention area so that the closer attention area can be easily made to pop-out as in human binocular vision, based on the single eye alignment hypothesis, but also both static and dynamic features of an input scene. Moreover, the proposed saliency map model includes an affective computing process to skip an unwanted area and/or to pay attention to a desired area, mimicking the pulvinar's function in the human preference and refusal mechanism in subsequent visual search processes. In addition, we show the effectiveness of using the symmetry feature implemented by a neural network and independent component analysis (ICA) filter to construct more object preferable attention model. The experimental results show that the proposed model can generate more plausible scan paths for natural input scenes.		Young-Min Jang;Sang-Woo Ban;Minho Lee	2007		10.1007/978-3-540-69162-4_110	binocular vision;independent component analysis;computer vision;attention;visual search;computer science;machine learning;top-down and bottom-up design;affective computing;artificial neural network	Robotics	22.866076830996267	-65.03673636215574	111321
17bb295b231c72ecf787990097b6ed94755a3810	recursive partitioning analysis of a large structure-activity data set using three-dimensional descriptors1	recursive partitioning;three dimensional		recursion (computer science)	Xin Chen;Andrew Rusinko;S. Stanley Young	1998	Journal of Chemical Information and Computer Sciences	10.1021/ci980089g	three-dimensional space;chemistry;computer science;machine learning;recursive partitioning	Theory	13.779844498752574	-58.11187180797696	111426
a2dd87a46c1fa581b57ff0eb669730655de0915d	noncontiguous atom matching structural similarity function		Measuring similarity between molecules is a fundamental problem in cheminformatics. Given that similar molecules tend to have similar physical, chemical, and biological properties, the notion of molecular similarity plays an important role in the exploration of molecular data sets, query-retrieval in molecular databases, and in structure-property/activity modeling. Various methods to define structural similarity between molecules are available in the literature, but so far none has been used with consistent and reliable results for all situations. We propose a new similarity method based on atom alignment for the analysis of structural similarity between molecules. This method is based on the comparison of the bonding profiles of atoms on comparable molecules, including features that are seldom found in other structural or graph matching approaches like chirality or double bond stereoisomerism. The similarity measure is then defined on the annotated molecular graph, based on an iterative directed graph similarity procedure and optimal atom alignment between atoms using a pairwise matching algorithm. With the proposed approach the similarities detected are more intuitively understood because similar atoms in the molecules are explicitly shown. This noncontiguous atom matching structural similarity method (NAMS) was tested and compared with one of the most widely used similarity methods (fingerprint-based similarity) using three difficult data sets with different characteristics. Despite having a higher computational cost, the method performed well being able to distinguish either different or very similar hydrocarbons that were indistinguishable using a fingerprint-based approach. NAMS also verified the similarity principle using a data set of structurally similar steroids with differences in the binding affinity to the corticosteroid binding globulin receptor by showing that pairs of steroids with a high degree of similarity (>80%) tend to have smaller differences in the absolute value of binding activity. Using a highly diverse set of compounds with information about the monoamine oxidase inhibition level, the method was also able to recover a significantly higher average fraction of active compounds when the seed is active for different cutoff threshold values of similarity. Particularly, for the cutoff threshold values of 86%, 93%, and 96.5%, NAMS was able to recover a fraction of actives of 0.57, 0.63, and 0.83, respectively, while the fingerprint-based approach was able to recover a fraction of actives of 0.41, 0.40, and 0.39, respectively. NAMS is made available freely for the whole community in a simple Web based tool as well as the Python source code at http://nams.lasige.di.fc.ul.pt/.		Ana L. Teixeira;André O. Falcão	2013	Journal of chemical information and modeling	10.1021/ci400324u	chemical similarity;bioinformatics;theoretical computer science;computational chemistry;mathematics	Comp.	10.369521359856538	-58.89676253240307	111668
512c9fe93cef2dbc30e212cdcb7a10839deb376e	a comparison of classifiers for solar energetic events		We compare the results of using a Random Forest Classifier with the results of using Nonparametric Discriminant Analysis to classify whether a filament channel (in the case of a filament eruption) or an active region (in the case of a flare) is about to produce an event. A large number of descriptors are considered in each case, but it is found that only a small number are needed in order to get most of the improvement in performance over always predicting the majority class. There is little difference in performance between the two classifiers, and neither results in substantial improvements over simply predicting the majority class.	british undergraduate degree classification;linear discriminant analysis;random forest	Graham Barnes;Nicole Schanche;K. D. Leka;Ashna Aggarwal;Kathy Reeves	2016		10.1017/S1743921316012758	physics	Vision	15.047129988196675	-55.50193757447387	111897
9986899f5725d37a5bac81471353e1286bf7dfb7	satr-dl: improving surgical skill assessment and task recognition in robot-assisted surgery with deep neural networks		Purpose: This paper focuses on an automated analysis of surgical motion profiles for objective skill assessment and task recognition in robot-assisted surgery. Existing techniques heavily rely on conventional statistic measures or shallow modelings based on hand-engineered features and gesture segmentation. Such developments require significant expert knowledge, are prone to errors, and are less efficient in online adaptive training systems. Methods: In this work, we present an efficient analytic framework with a parallel deep learning architecture, SATR-DL, to assess trainee expertise and recognize surgical training activity. Through an end-to-end learning technique, abstract information of spatial representations and temporal dynamics is jointly obtained directly from raw motion sequences. Results: By leveraging a shared highlevel representation learning, the resulting model is successful in the recognition of trainee skills and surgical tasks, suturing, needle-passing, and knot-tying. Meanwhile, we explore the use of ensemble in classification at the trial level, where the SATR-DL outperforms state-of-the-art performance by achieving accuracies of 0.960 and 1.000 in skill assessment and task recognition, respectively. Conclusion: This study highlights the potential of SATR-DL to provide improvements for an efficient data-driven assessment in intelligent robotic surgery.	deep learning;end-to-end principle;feature learning;gesture recognition;learning disorders;machine learning;neural tube defects;neural network software;robot;surgical sutures	Ziheng Wang;Ann Majewicz Fey	2018	2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2018.8512575	machine learning;task analysis;artificial neural network;artificial intelligence;deep learning;hidden markov model;segmentation;robotic surgery;surgery;computer science;feature learning;gesture	Robotics	22.782894705537245	-56.9436942505767	111966
52588c7c0bcbc7e0f2efcb37ac6783764cb9d8c1	stepwise assembling of polypeptide chain energy distributions	conformational analysis;high density;density of state;prion protein;local energy minima;energy distribution;source code;fortran;density of states;energy value;potential energy;potential energy landscape;energy landscape;matrix algorithm	The principles and application of conformational analysis software that makes use of a new algorithm are described. It is known that the existence of a local energy minimum in the energy landscape is in general related to the clustering of polypeptide chain conformations near that energy value or, in other words, to a high density of states. A criterion based on this principle is part of an algorithm employed to select subsets of polypeptide chain conformations in broad energy ranges. Chain fragments belonging to these subsets are then combined to build larger polypeptide chains and the corresponding energy distributions. The functionality of the various operations employed in the process is described and the FORTRAN 77 source code that defines the algorithm is listed. The methodology is illustrated with a calculation involving three chain fragments belonging to the cellular prion protein (PrP(C)).	cluster analysis;fortran;hl7publishingsubsection <operations>;large;polypeptides;source code;top-down and bottom-up design;algorithm;statistical cluster	Saul G. Jacchieri	2001	Computers & chemistry	10.1016/S0097-8485(00)00076-0	crystallography;bioinformatics;energy landscape;computational chemistry;mathematics;density of states;physics;quantum mechanics	Logic	14.073158898124506	-61.31944762834963	112073
d1b796ff0c1895426de11a1eaafc5443be29645d	extended-connectivity fingerprints		Extended-connectivity fingerprints (ECFPs) are a novel class of topological fingerprints for molecular characterization. Historically, topological fingerprints were developed for substructure and similarity searching. ECFPs were developed specifically for structure-activity modeling. ECFPs are circular fingerprints with a number of useful qualities: they can be very rapidly calculated; they are not predefined and can represent an essentially infinite number of different molecular features (including stereochemical information); their features represent the presence of particular substructures, allowing easier interpretation of analysis results; and the ECFP algorithm can be tailored to generate different types of circular fingerprints, optimized for different uses. While the use of ECFPs has been widely adopted and validated, a description of their implementation has not previously been presented in the literature.	algorithm;entity name part qualifier - adopted;fingerprint;topological index	David Rogers;Mathew Hahn	2010	Journal of chemical information and modeling	10.1021/ci100050t	bioinformatics;mathematics;algorithm	Comp.	11.792404765874386	-57.078881190968644	112645
57f0b17a48953687237eb61d5f507f1373fa5182	multi-resolution neural networks for tracking seismic horizons from few training images		Detecting a specific horizon in seismic images is a valuable tool for geological interpretation. Because hand-picking the locations of the horizon is a time-consuming process, automated computational methods were developed starting three decades ago. Older techniques for such picking include interpolation of control points however, in recent years neural networks have been used for this task. Until now, most networks trained on small patches from larger images. This limits the networks ability to learn from large-scale geologic structures. Moreover, currently available networks and training strategies require label patches that have full and continuous annotations, which are also time-consuming to generate. We propose a projected loss-function for training convolutional networks with a multi-resolution structure, including variants of the U-net. Our networks learn from a small number of large seismic images without creating patches. The projected loss-function enables training on labels with just a few annotated pixels and has no issue with the other unknown label pixels. Training uses all data without reserving some for validation. Only the labels are split into training/testing. Contrary to other work on horizon tracking, we train the network to perform non-linear regression, and not classification. As such, we propose labels as the convolution of a Gaussian kernel and the known horizon locations that indicate uncertainty in the labels. The network output is the probability of the horizon location. We demonstrate the proposed computational ingredients on two different datasets, for horizon extrapolation and interpolation. We show that the predictions of our methodology are accurate even in areas far from known horizon locations because our learning strategy exploits all data in large seismic images.		Bas Peters;Justin Granek;Eldad Haber	2018	CoRR			ML	21.52045074670593	-52.1338135593211	112894
06e38e0a677ad771f43b60d499050c0b2b3f01a1	correlating protein hot spot surface analysis using probis with simulated free energies of protein-protein interfacial residues	software;amino acid substitution;binding sites;molecular dynamics simulation;hydrogen bonding;proteins;protein conformation;structural homology protein;protein binding;thermodynamics;amino acids;algorithms;entropy;protein interaction mapping;databases protein	A protocol was developed for the computational determination of the contribution of interfacial amino acid residues to the free energy of protein-protein binding. Thermodynamic integration, based on molecular dynamics simulation in CHARMM, was used to determine the free energy associated with single point mutations to glycine in a protein-protein interface. The hot spot amino acids found in this way were then correlated to structural similarity scores detected by the ProBiS algorithm for local structural alignment. We find that amino acids with high structural similarity scores contribute on average -3.19 kcal/mol to the free energy of protein-protein binding and are thus correlated with hot spot residues, while residues with low similarity scores contribute on average only -0.43 kcal/mol. This suggests that the local structural alignment method provides a good approximation of the contribution of a residue to the free energy of binding and is particularly useful for detection of hot spots in proteins with known structures but undetermined protein-protein complexes.	alignment;amino acids;approximation;charmm;duoxa1 gene;energy, physics;exanthema;glycine;molecular dynamics;point mutation;probis;protein binding;simulation;staphylococcal protein a;structural similarity;thermodynamic integration;thermodynamics;algorithm;free energy;kilocalorie	Nejc Carl;Milan Hodošček;Blaz Vehar;Janez Konc;Bernard R. Brooks;Dusanka Janezic	2012	Journal of chemical information and modeling	10.1021/ci3003254	crystallography;biochemistry;protein structure;entropy;plasma protein binding;molecular dynamics;chemistry;bioinformatics;binding site;hydrogen bond;nuclear magnetic resonance	Comp.	11.004190237486485	-60.02105676782857	113148
ac3e372824fa1c46c6daeefba3521773044ae190	statistical modeling of a ligand knowledge base	tolman electronic parameter tep;statistical models;partial least squares;ligand knowledge base lkb;statistical model;journal article;computer programs;mathematical models;molecular str;principal component analysis;computers in chemistry;regression analysis;parameter estimation;knowledge based systems;keywords data reduction;knowledge base	A range of different statistical models has been fitted to experimental data for the Tolman electronic parameter (TEP) based on a large set of calculated descriptors in a prototype ligand knowledge base (LKB) of phosphorus(III) donor ligands. The models have been fitted by ordinary least squares using subsets of descriptors, principal component regression, and partial least squares which use variables derived from the complete set of descriptors, least angle regression, and the least absolute shrinkage and selection operator. None of these methods is robust against outliers, so we also applied a robust estimation procedure to the linear regression model. Criteria for model evaluation and comparison have been discussed, highlighting the importance of resampling methods for assessing the robustness of models and the scope for making predictions in chemically intuitive models. For the ligands covered by this LKB, ordinary least squares models of descriptor subsets provide a good representation of the data, while partial least squares, principal component regression, and least angle regression models are less suitable for our dual aims of prediction and interpretation. A linear regression model with robustly fitted parameters achieves the best model performance over all classes of models fitted to TEP data, and the weightings assigned to ligands during the robust estimation procedure are chemically intuitive. The increased model complexity when compared to the ordinary least squares linear model is justified by the reduced influence of individual ligands on the model parameters and predictions of new ligands. Robust linear regression models therefore represent the best compromise for achieving statistical robustness in simple, chemically meaningful models.		Ralph A. Mansson;Alan H. Welsh;Natalie Fey;A. Guy Orpen	2006	Journal of chemical information and modeling	10.1021/ci600212t	generalized least squares;principal component regression;total least squares;statistical model;simple linear regression;econometrics;knowledge base;least trimmed squares;computer science;linear regression;machine learning;linear model;polynomial regression;errors-in-variables models;regression diagnostic;mathematics;linear probability model;partial least squares regression;explained sum of squares;non-linear least squares;robust regression;regression analysis;linear least squares;nonlinear regression;statistics	Vision	12.983677206920571	-58.18144297037584	113185
cd1ace113ead55fb8382a76bdacd8660494e16f1	modeling loop backbone flexibility in receptor-ligand docking simulations	kinase inhibitor;in silico screening;allosteric effect;induced fit;receptor flexibility	The relevance of receptor conformational change during ligand binding is well documented for many pharmaceutically relevant receptors, but is still not fully accounted for in in silico docking methods. While there has been significant progress in treatment of receptor side chain flexibility sampling of backbone flexibility remains challenging because the conformational space expands dramatically and the scoring function must balance protein-protein and protein-ligand contributions. Here, we investigate an efficient multistage backbone reconstruction algorithm for large loop regions in the receptor and demonstrate that treatment of backbone receptor flexibility significantly improves binding mode prediction starting from apo structures and in cross docking simulations. For three different kinase receptors in which large flexible loops reconstruct upon ligand binding, we demonstrate that treatment of backbone flexibility results in accurate models of the complexes in simulations starting from the apo structure. At the example of the DFG-motif in the p38 kinase, we also show how loop reconstruction can be used to model allosteric binding. Our approach thus paves the way to treat the complex process of receptor reconstruction upon ligand binding in docking simulations and may help to design new ligands with high specificity by exploitation of allosteric mechanisms.		Johannes Flick;Frank Tristram;Wolfgang Wenzel	2012	Journal of computational chemistry	10.1002/jcc.23087	biochemistry;stereochemistry;chemistry;enzyme catalysis	Comp.	11.441996712382036	-61.69753067050878	114461
73d5c9823d085f01cdc97ae85ca3ecee02047ace	classifying digestive organs in wireless capsule endoscopy images based on deep convolutional neural network	parameter selection wireless capsule endoscopy digestive organs classification deep convolutional neural network;object recognition biological organs biomedical optical imaging brightness endoscopes feature extraction feedforward neural nets image classification learning artificial intelligence medical image processing;deep convolutional neural network;feature extraction endoscopes intestines training accuracy convolution wireless communication;parameter selection;wireless capsule endoscopy;luminance change digestive organ classification problem wireless capsule endoscopy images deep convolutional neural network layer wise hierarchy models training data human biological visual systems semantic image feature recognition deep cnn based wce classification system dcnn wce cs complex digestive tract circumstance;digestive organs classification	This paper studies the classification problem of the digestive organs in wireless capsule endoscopy (WCE) images based on deep convolutional neural network (DCNN) framework. Essentially, DCNN proves having powerful ability to learn layer-wise hierarchy models with huge training data, which works similar to human biological visual systems. Classifying digestive organs in WCE images intuitively means to recognize higher semantic image features. To achieve this, an effective deep CNN-based WCE classification system has been constructed (DCNN-WCE-CS). With about 1 million real WCE images, intensive experiments are conducted to evaluate its performance by setting different network parameters. Results illustrate its superior performance compared to traditional classification methods, where about 95% classification accuracy can be achieved in average. Moreover, it is observed that the DCNN-WCE-CS is robust to the large variations of the WCE images due to the individuals and complex digestive tract circumstance, including the rotation, the luminance change of the WCE images.	artificial neural network;convolutional neural network;experiment;tract (literature)	Yuexian Zou;Lei Li;Yi Wang;Jiasheng Yu;Yi Li;W. J. Deng	2015	2015 IEEE International Conference on Digital Signal Processing (DSP)	10.1109/ICDSP.2015.7252086	computer vision;computer science;machine learning;communication	Robotics	24.265103823592135	-53.47086100789214	114940
53a0ee6b3ac6757db7468b3edf48aca9fe3ee851	biomimetic control based on a model of chemotaxis in escherichia coli	system engineering;escherichia coli;control algorithm;bacterial chemotaxis;mobile robot control;bacterial motor control;mobile robot;computer model;mobile robot control bacterial chemotaxis bacterial motor control computer simulation biomimetic control;biomimetic control;intelligent control;genetics;control problem;molecular biology;information processing;system biology;autonomous control;computer simulation;organizational structure;motor control	In the field of molecular biology, extending now to the more comprehensive area of systems biology, the development of computer models for synthetic cell simulation has accelerated extensively and has begun to be used for various purposes, such as biochemical analysis. These models, describing the highly efficient environmental searching mechanisms and adaptability of living organisms, can be used as machine-control algorithms in the field of systems engineering. To realize this biomimetic intelligent control, we require a stripped-down model that expresses a series of information-processing tasks from stimulation input to movement. Here we selected the bacterium Escherichia coli as a target organism because it has a relatively simple molecular and organizational structure, which can be characterized using biochemical and genetic analyses. We particularly focused on a motility response known as chemotaxis and developed a computer model that includes not only intracellular information processing but also motor control. After confirming the effectiveness and validity of the proposed model by a series of computer simulations, we applied it to a mobile robot control problem. This is probably the first study showing that a bacterial model can be used as an autonomous control algorithm. Our results suggest that many excellent models proposed thus far for biochemical purposes can be applied to problems in other fields.	algorithm;artificial cells;autonomous robot;biomimetics;cell (microprocessor);chemotaxis;computer simulation;ethanol 0.62 ml/ml topical gel;information processing;intelligent control;mobile robot;molecular biology;organism;robot control;synthetic intelligence;systems biology;systems engineering	Toshio Tsuji;Michiyo Suzuki;Noboru Takiguchi;Hisao Ohtake	2010	Artificial Life	10.1162/artl.2010.16.2.16203	computer simulation;organizational structure;mobile robot;motor control;biology;simulation;computer science;artificial intelligence;escherichia coli;genetics	ML	14.023134384026752	-63.84249111931083	115160
0c241e4fe3d00a0c87badab6d46e4b08aa049587	prediction of supercritical carbon dioxide solubility of organic compounds from molecular structure	supercritical carbon dioxide;organic compound;molecular structure	A diverse data set of 58 compounds taken from the literature was used to create models for the prediction of the solubility of organic compounds in supercritical carbon dioxide. Descriptors encoding information about the topological, geometric, and electronic properties of each compound in the data set were calculated from the molecular structures. A multiple linear regression model containing seven descriptors was generated. Several new descriptors, which were not present in the original pool, were calculated. One of the new descriptors was used to create the final seven descriptor linear model, which had a better root mean square (rms) error than the original model. The seven descriptors that appeared in the final model were used to make a neural network model which had a significantly better rms error than the linear model.		Heidi L. Engelhardt;Peter C. Jurs	1997	Journal of Chemical Information and Computer Sciences	10.1021/ci960085g	supercritical carbon dioxide;chemistry;molecule;organic chemistry;nanotechnology;quantum mechanics	Theory	12.745044556614811	-57.625124181964495	115214
33bd23fde147daef8c6629a83dcfec2b3192bc98	interfacial chemistry and the design of solid-phase nucleic acid hybridization assays using immobilized quantum dots as donors in fluorescence resonance energy transfer	dna;signal enhancement;animals;immobilization;active area;biosensing techniques;ligands;regeneration;nucleic acids;quantum dots;materials testing;biosensor;signal processing computer assisted;nucleic acid hybridization;equipment design;oligonucleotides;cattle;fluorescent dyes;chemistry;fluorescence resonance energy transfer;biotinylation	The use of quantum dots (QDs) as donors in fluorescence resonance energy transfer (FRET) offer several advantages for the development of multiplexed solid-phase QD-FRET nucleic acid hybridization assays. Designs for multiplexing have been demonstrated, but important challenges remain in the optimization of these systems. In this work, we identify several strategies based on the design of interfacial chemistry for improving sensitivity, obtaining lower limits of detection (LOD) and enabling the regeneration and reuse of solid-phase QD-FRET hybridization assays. FRET-sensitized emission from acceptor dyes associated with hybridization events at immobilized QD donors provides the analytical signal in these assays. The minimization of active sensing area reduces background from QD donor PL and allows the resolution of smaller amounts of acceptor emission, thus lowering the LOD. The association of multiple acceptor dyes with each hybridization event can enhance FRET efficiency, thereby improving sensitivity. Many previous studies have used interfacial protein layers to generate selectivity; however, transient destabilization of these layers is shown to prevent efficient regeneration. To this end, we report a protein-free interfacial chemistry and demonstrate the specific detection of as little as 2 pmol of target, as well as an improved capacity for regeneration.	acceptor (semiconductors);base pair mismatch;biosensors;cell nucleus;contribution;dyes;fluorescence resonance energy transfer;limit of detection;lithium;mathematical optimization;multiplexing;natural science disciplines;natural regeneration;nucleic acid hybridization;nucleic acids;offset binary;oligonucleotide probes;quantum dots;quantum dot;reuse (action);selectivity (electronic);sensor;small;sodium;staphylococcal protein a;throughput;tissue fiber;tracer;warren abstract machine;anatomical layer;orders - hl7publishingdomain;picomole (pm)	W. Russ Algar;Ulrich J. Krull	2011		10.3390/s110606214	biotinylation;nucleic acid;molecular biology;chemistry;analytical chemistry;förster resonance energy transfer;regeneration;nanotechnology;quantum dot;ligand;dna;oligonucleotide;biosensor;nucleic acid thermodynamics	Arch	11.318405151096062	-65.09499888867104	115666
3b32a0d927ef7a6f525fea21620ed8521c1cdf3a	comparison of nr and uniclust databases for protein secondary structure prediction		Three-dimensional structure prediction is one of the important problems in bioinformatics and theoretical chemistry. One of the most important steps in the three-dimensional structure prediction is the estimation of secondary structure. Improving the accuracy rate in protein secondary structure prediction depends on computed attributes as well as the classification algorithms. In multiple alignment methods, which are often used to extract an attribute, the calculated values differ according to the database used for the alignment. For this reason, it is important to use a suitable database against which the target proteins are aligned to compute profile feature vectors. In this study, 5 different datasets are generated for the CB513 benchmark with the aid of two different alignment methods and three different databases. The profile features are fed as input to a two-stage hybrid classifier. According to the experimental results, the highest accuracy rate is obtained when UniClust database is used at the first stage of HHBlits alignment to calculate PSSM values and NR database is used at the first stage of HHBlits alignment to calculate structural profile matrices.	algorithm;benchmark (computing);bioinformatics;database;feature vector;hh-suite;multiple sequence alignment;noise reduction;numerical recipes;position weight matrix;protein structure prediction	Zafer Aydin;Oguz Kaynar;Yasin Gormez	2018	2018 26th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2018.8404285	multiple sequence alignment;pattern recognition;artificial intelligence;protein secondary structure;hidden markov model;computer science;statistical classification;database;feature vector;markov process;matrix (mathematics)	Comp.	10.120300265873944	-54.01224653290992	115820
56839add85be3e09873ceb38e7fadf91a20aa876	representation of protein secondary structure using bond-orientational order parameters	machine learning;bond orientational order;secondary structure;structural alphabet	Structural studies of proteins for motif mining and other pattern recognition techniques require the abstraction of the structure into simpler elements for robust matching. In this study, we propose the use of bond-orientational order parameters, a well-established metric usually employed to compare atom packing in crystals and liquids. Creating a vector of orientational order parameters of residue centers in a sliding window fashion provides us with a descriptor of local structure and connectivity around each residue that is easy to calculate and compare. To test whether this representation is feasible and applicable to protein structures, we tried to predict the secondary structure of protein segments from those descriptors, resulting in 0.99 AUC (area under the ROC curve). Clustering those descriptors to 6 clusters also yield 0.93 AUC, showing that these descriptors can be used to capture and distinguish local structural information.		Cem Meydan;Osman Ugur Sezerman	2012		10.1007/978-3-642-34123-6_17	combinatorics;computer science;machine learning;mathematics;protein secondary structure	Robotics	11.787788005843106	-56.89679057666395	116141
31ff2cf6c374eccc85b3c188d96b547f05e19183	i-tasser server for protein 3d structure prediction	software;score function;standard deviation;amino acid sequence;false negative;structural biology;models chemical;computational biology bioinformatics;full length;protein structure;large scale;models molecular;internet;proteins;protein conformation;protein structure prediction;structure prediction;algorithms;3 dimensional;molecular sequence data;sequence alignment;combinatorial libraries;false positive;correlation coefficient;computer appl in life sciences;3d structure;article;computer simulation;structural similarity;sequence analysis protein;scoring system;microarrays;bioinformatics	Prediction of 3-dimensional protein structures from amino acid sequences represents one of the most important problems in computational structural biology. The community-wide Critical Assessment of Structure Prediction (CASP) experiments have been designed to obtain an objective assessment of the state-of-the-art of the field, where I-TASSER was ranked as the best method in the server section of the recent 7th CASP experiment. Our laboratory has since then received numerous requests about the public availability of the I-TASSER algorithm and the usage of the I-TASSER predictions. An on-line version of I-TASSER is developed at the KU Center for Bioinformatics which has generated protein structure predictions for thousands of modeling requests from more than 35 countries. A scoring function (C-score) based on the relative clustering structural density and the consensus significance score of multiple threading templates is introduced to estimate the accuracy of the I-TASSER predictions. A large-scale benchmark test demonstrates a strong correlation between the C-score and the TM-score (a structural similarity measurement with values in [0, 1]) of the first models with a correlation coefficient of 0.91. Using a C-score cutoff > -1.5 for the models of correct topology, both false positive and false negative rates are below 0.1. Combining C-score and protein length, the accuracy of the I-TASSER models can be predicted with an average error of 0.08 for TM-score and 2 Å for RMSD. The I-TASSER server has been developed to generate automated full-length 3D protein structural predictions where the benchmarked scoring system helps users to obtain quantitative assessments of the I-TASSER models. The output of the I-TASSER server for each query includes up to five full-length models, the confidence score, the estimated TM-score and RMSD, and the standard deviation of the estimations. The I-TASSER server is freely available to the academic community at http://zhang.bioinformatics.ku.edu/I-TASSER .	algorithm;amino acid sequence;amino acids;anatomy, regional;benchmark (computing);bioinformatics;casp;clinical use template;cluster analysis;coefficient;evaluation procedure;experiment;f1 score;i-tasser;online and offline;scoring functions for docking;server (computer);server (computing);standard deviation;structural similarity;thread (computing);statistical cluster	Yonghui Zhang	2007	BMC Bioinformatics	10.1186/1471-2105-9-40	computer simulation;biology;protein structure;computer science;bioinformatics;data science;data mining;structural biology;casp	Comp.	11.009148703127419	-56.47435863317074	116449
27ce155749f0c9903f96097ddd9f9dcc3a920189	book review | new books	physicochemical properties;hydrogen bond;score function;binding sites;models chemical;journal article;computational biology bioinformatics;high throughput screening;bleep;complexes;hsp90 heat shock proteins;protein ligand interactions;target;virtual screening;models molecular;drug design;protein conformation;heat shock protein;molecular weight;chemical databases;protein binding;algorithms;validation;binding affinity;qd chemistry;combinatorial libraries;protein interaction mapping;computer appl in life sciences;computer simulation;mean force;molecular docking;inhibitors;sequence analysis protein;drug development;microarrays;bioinformatics;in silico	The need for fast and accurate scoring functions has been driven by the increased use of in silico virtual screening twinned with high-throughput screening as a method to rapidly identify potential candidates in the early stages of drug development. We examine the ability of some the most common scoring functions (GOLD, ChemScore, DOCK, PMF, BLEEP and Consensus) to discriminate correctly and efficiently between active and non-active compounds among a library of ~3,600 diverse decoy compounds in a virtual screening experiment against heat shock protein 90 (Hsp90). Firstly, we investigated two ranking methodologies, GOLDrank and BestScorerank. GOLD rank is based on ranks generated using GOLD. The various scoring functions, GOLD, ChemScore, DOCK, PMF, BLEEP and Consensus, are applied to the pose ranked number one by GOLD for that ligand. BestScore rank uses multiple poses for each ligand and independently chooses the best ranked pose of the ligand according to each different scoring function. Secondly, we considered the effect of introducing the Thr184 hydrogen bond tether to guide the docking process towards a particular solution, and its effect on enrichment. Thirdly, we considered normalisation to account for the known bias of scoring functions to select larger molecules. All the scoring functions gave fairly similar enrichments, with the exception of PMF which was consistently the poorest performer. In most cases, GOLD was marginally the best performing individual function; the Consensus score usually performed similarly to the best single scoring function. Our best results were obtained using the Thr184 tether in combination with the BestScorerank protocol and normalisation for molecular weight. For that particular combination, DOCK was the best individual function; DOCK recovered 90% of the actives in the top 10% of the ranked list; Consensus similarly recovered 89% of the actives in its top 10%. Overall, we demonstrate the validity of virtual screening as a method for identifying new leads from a pool of ligands with similar physicochemical properties and we believe that the outcome of this study provides useful insight into the setting up of a suitable docking and scoring protocol, resulting in enrichment of 'target active' compounds.	bleep censor;boat dock;book;consensus (computer science);docking (molecular);eighty nine;f1 score;gene ontology term enrichment;hsp90 heat-shock proteins;heat-shock response;high-throughput computing;hydrogen bonding;large;ligands;molecular weight;prb1 gene;scoring functions for docking;shock;throughput;virtual screening;drug development;gold	Chrysi Konstantinou-Kirtay;John B. O. Mitchell;James A. Lumley	2003	BMC Bioinformatics	10.1186/1471-2105-8-27	computer simulation;high-throughput screening;protein structure;plasma protein binding;molecular biology;heat shock protein;dna microarray;docking;virtual screening;bioinformatics;binding site;drug development;ligand;score;hydrogen bond;molecular mass;chemical database;drug design	Comp.	10.53716125614703	-58.86836514982878	116522
233b9d098ee2e33cefa43ba85968b5fbf66c57f4	advances in optimizing recurrent networks	optimisation;long term dependencies recurrent networks deep learning representation learning;training optimization recurrent neural networks biological neural networks computational modeling entropy;recurrent neural networks optimization test error music data credit assignment symmetry breaking sparser gradients output probability models advanced momentum clipping gradients long term dependencies deep learning;recurrent neural nets;learning artificial intelligence;recurrent neural nets learning artificial intelligence optimisation	After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modeling sequences, their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error.	artificial neural network;clipping (computer graphics);deep learning;experiment;file spanning;gradient;list of code lyoko episodes;mathematical optimization;recurrent neural network;symmetry breaking;test case	Yoshua Bengio;Nicolas Boulanger-Lewandowski;Razvan Pascanu	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6639349	types of artificial neural networks;computer science;artificial intelligence;recurrent neural network;machine learning;data mining;deep learning;artificial neural network	ML	18.151135971683875	-53.04510014265632	116637
a9bff710b6af55e8bab2acc0eac3957c3bacf565	maintaining solvent accessible surface area under rotamer substitution for protein design	protein design;protein stability;computational protein design;solvent accessible surface area;surface area;solvent accessible	Although quantities derived from solvent accessible surface areas (SASA) are useful in many applications in protein design and structural biology, the computational cost of accurate SASA calculation makes SASA-based scores difficult to integrate into commonly used protein design methodologies. We demonstrate a method for maintaining accurate SASA during a Monte Carlo search of sequence and rotamer space for a fixed protein backbone. We extend the fast Le Grand and Merz algorithm (Le Grand and Merz, J Comput Chem, 14, 349), which discretizes the solvent accessible surface for each atom by placing dots on a sphere and combines Boolean masks to determine which dots are exposed. By replacing semigroup operations with group operations (from Boolean logic to counting dot coverage) we support SASA updates. Our algorithm takes time proportional to the number of atoms affected by rotamer substitution, rather than the number of atoms in the protein. For design simulations with a one hundred residue protein our approach is approximately 145 times faster than performing a Le Grand and Merz SASA calculation from scratch following each rotamer substitution. To demonstrate practical effectiveness, we optimize a SASA-based measure of protein packing in the complete redesign of a large set of proteins and protein-protein interfaces.	accessible surface area;algorithm;algorithmic efficiency;atom;boolean algebra;darpa grand challenge;hl7publishingsubsection <operations>;internet backbone;logical connective;masks;molecular design software;monte carlo method;q-chem;quantity;set packing;simulation;vertebral column	Andrew Leaver-Fay;Glenn L. Butterfoss;Jack Snoeyink;Brian Kuhlman	2007	Journal of computational chemistry	10.1002/jcc.20626	chemistry;surface area;computational chemistry;protein design	Comp.	12.471160094155364	-60.172474228264385	117387
266dc578ffcdc0d04e005a6ee7f2cbb7cd845e71	doodle master: a doodle beautification system based on auto-encoding generative adversarial networks		For those people without artistic talent, they can only draw rough or even awful doodles to express their ideas. We propose a doodle beautification system named Doodle Master, which can transfer a rough doodle to a plausible image and also keep the semantic concepts of the drawings. The Doodle Master applies the VAE/GAN model to decode and generate the beautified result from a constrained latent space. To achieve better performance for sketch data which is more like discrete distribution, a shared-weight method is proposed to improve the learnt features of the discriminator with the aid of the encoder. Furthermore, we design an interface for the user to draw with basic drawing tools and adjust the number of reconstruction times. The experiments show that the proposed Doodle Master system can successfully beautify the rough doodle or sketch in real-time.	adversary (cryptography);discriminator;encoder;experiment;generative adversarial networks;iteration;real-time clock	Chien-Wen Chen;Wen-Cheng Chen;Min-Chun Hu	2018		10.1145/3209693.3209695	encoder;beautification;adversarial system;generative grammar;machine learning;discriminator;encoding (memory);technical drawing tools;sketch;artificial intelligence;computer science	AI	19.3718561398667	-53.040292839519005	117402
75f9a3723a7e1c2df8fc378965c16f6fda800ffd	design and characterization of libraries of molecular fragments for use in nmr screening against protein targets	computer programs;protein targeting;computers in chemistry	We have designed four generations of a low molecular weight fragment library for use in NMR-based screening against protein targets. The library initially contained 723 fragments which were selected manually from the Available Chemicals Directory. A series of in silico filters and property calculations were developed to automate the selection process, allowing a larger database of 1.79 M available compounds to be searched for a further 357 compounds that were added to the library. A kinase binding pharmacophore was then derived to select 174 kinase-focused fragments. Finally, an additional 61 fragments were selected to increase the number of different pharmacophores represented within the library. All of the fragments added to the library passed quality checks to ensure they were suitable for the screening protocol, with appropriate solubility, purity, chemical stability, and unambiguous NMR spectrum. The successive generations of libraries have been characterized through analysis of structural properties (molecular weight, lipophilicity, polar surface area, number of rotatable bonds, and hydrogen-bonding potential) and by analyzing their pharmacophoric complexity. These calculations have been used to compare the fragment libraries with a drug-like reference set of compounds and a set of molecules that bind to protein active sites. In addition, an analysis of the overall results of screening the library against the ATP binding site of two protein targets (HSP90 and CDK2) reveals different patterns of fragment binding, demonstrating that the approach can find selective compounds that discriminate between related binding sites.		Nicolas Baurin;Fareed Aboul-Ela;Xavier Barril;Ben Davis;Martin Drysdale;Brian W Dymock;Harry Finch;Christophe Fromont;Christine M. Richardson;Heather Simmonite;Roderick E. Hubbard	2004	Journal of chemical information and computer sciences	10.1021/ci049806z	chemistry;bioinformatics;combinatorial chemistry;nanotechnology;protein targeting	Comp.	10.333986312458135	-60.11759474025741	117508
521065410349ebbb0d6142b62ebea43b3f9d92a1	top-down object color biased attention using growing fuzzy topology art	bottom up attention;bottom up;top down;model performance;growing fuzzy topology art;incremental learning;top down object color biased attention;visual attention;natural scenes	In this paper, we propose a top-down object biased attention model which is based on human visual attention mechanism integrating feature based bottom-up attention and goal based top-down attention. The proposed model can guide attention to focus on a given target colored object over other objects or feature based salient areas by considering the object color biased attention mechanism. We proposed a growing fuzzy topology ART that plays important roles for object color biased attention, one of which is to incrementally learn and memorize features of arbitrary objects and the other one is to generate top-down bias signal by competing memorized features of a given target object with features of an arbitrary object. Experimental results show that the proposed model performs well in successfully focusing on given target objects, as well as incrementally perceiving arbitrary objects in natural scenes.	fuzzy mathematics	Byungku Hwang;Sang-Woo Ban;Minho Lee	2008		10.1007/978-3-540-88906-9_13	computer vision;computer science;artificial intelligence;top-down and bottom-up design;biased competition theory	Vision	22.864422997771687	-65.13611212880933	117758
a9a0e1d0b143c8e8f8a56b8519910e4946940590	serious games application for memory training using egocentric images		Mild cognitive impairment is the early stage of several neurodegenerative diseases, such as Alzheimer’s. In this work, we address the use of lifelogging as a tool to obtain pictures from a patient’s daily life from an egocentric point of view. We propose to use them in combination with serious games as a way to provide a non-pharmacological treatment to improve their quality of life. To do so, we introduce a novel computer vision technique that classifies rich and non rich egocentric images and uses them in serious games. We present results over a dataset composed by 10,997 images, recorded by 7 different users, achieving 79% of F1-score. Our model presents the first method used for automatic egocentric images selection applicable to serious games.	computer vision;f1 score;graphics processing unit;lifelog;personalization;samsung sgr-a1;titan;wearable computer	Gabriel Oliveira-Barra;Marc Bolaños;Estefanía Talavera;Adrián Dueñas;Olga Gelonch;Maite Garolera	2017		10.1007/978-3-319-70742-6_11	computer vision;lifelog;computer science;artificial intelligence;multimedia;memory training;cognition	HCI	24.158565322838395	-59.4995294987436	117768
f5ca2a4a5cba57fb77c92c1fd2af4efeff189c09	the first electrochemical mip sensor for tamoxifen	sensitivity and specificity;electropolymerisation;molecular imprinting;biosensing techniques;molecularly imprinted polymers;equipment failure analysis;tamoxifen;equipment design;electrodes;selective estrogen receptor modulators;reproducibility of results;conductometry;anticancer drug;polymers	"""We present an electrochemical MIP sensor for tamoxifen (TAM)-a nonsteroidal anti-estrogen-which is based on the electropolymerisation of an O-phenylenediamine‒resorcinol mixture directly on the electrode surface in the presence of the template molecule. Up to now only """"bulk"""" MIPs for TAM have been described in literature, which are applied for separation in chromatography columns. Electro-polymerisation of the monomers in the presence of TAM generated a film which completely suppressed the reduction of ferricyanide. Removal of the template gave a markedly increased ferricyanide signal, which was again suppressed after rebinding as expected for filling of the cavities by target binding. The decrease of the ferricyanide peak of the MIP electrode depended linearly on the TAM concentration between 1 and 100 nM. The TAM-imprinted electrode showed a 2.3 times higher recognition of the template molecule itself as compared to its metabolite 4-hydroxytamoxifen and no cross-reactivity with the anticancer drug doxorubucin was found. Measurements at +1.1 V caused a fouling of the electrode surface, whilst pretreatment of TAM with peroxide in presence of HRP generated an oxidation product which was reducible at 0 mV, thus circumventing the polymer formation and electrochemical interferences."""	clinical use template;column (database);estrogens;ferricyanides;horseradish peroxidase;ibm tivoli access manager;imprinting (psychology);peroxides;polymer;tamoxifen;while;afimoxifene;monomer;oxidation	Aysu Yarman;Frieder Scheller	2014		10.3390/s140507647	molecular imprinting;stereochemistry;chemistry;molecularly imprinted polymer;electrode;analytical chemistry;organic chemistry;conductometry	HCI	11.168162295111474	-62.920423536242524	117796
315c992b95a0a1cdfd6a8a9cdde0ff163f50ae4a	quantitative structure-retention relationship (qsrr) study of oxygen-containing organic compounds based on gene expression programming (gep)	organic compounds;artificial neural networks;biological cells;mathematical model;predictive models;programming	Gene Expression Programming (GEP) is an orginal and an effective searching method. Multiple Linear Regression (MLR), Artificial Neural Network (ANN) and GEP techniques were used to investigate the correlation between retention indices (RI) and descriptors for 91 oxygen-containing organic compounds. The correlation coefficient on OV-1 column is 0.9891(for ANN), 0.9919(for GEP), and 0.9911(for MLR), on SE-54 column is 0.9892(for ANN), 0.9955(for GEP), and 0.9917(for MLR). As shown from the results, the predicted values by GEP and the experimental values are in good agreement, Also, the QSRR model by GEP is better than QSRR models by ANN and MLR methods.	artificial neural network;coefficient;gene expression programming;learning to rank	Xiaotong Zhang;Ting Sun;Lihua Shi;Ling Ding;Zhaolin Sun;Lijuan Song	2016	2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2016.7603163	programming;computer science;artificial intelligence;machine learning;mathematical model;predictive modelling;artificial neural network;algorithm	ML	12.752791164822918	-57.14074118423244	117850
224b3c08918a550eb2766969ff8712961279ad2a	determination of urine saturation with computer program equil 2 as a method for estimation of the risk of urolithiasis	computer program	To investigate the risk for the development of urolithiasis in 30 children with urolithiasis, 36 children with isolated hematuria, and 15 healthy control children, 24-h urinary excretion of calcium, sodium, oxalate, citrate, sulfate, phosphate, magnesium, urate, chloride, ammonium, and glycosaminoglycans was determined and urine saturation for calcium oxalate was calculated with the computer program EQUIL 2. Compared with controls, children with urolithiasis had significantly increased calcium excretion, oxalate excretion, and urine saturation, whereas children with isolated hematuria had significantly increased calcium excretion only. The best estimation of the relative risk of urolithiasis can be made after urine saturation, using logistic regression. The percentage of patients correctly classified after urine saturation is 85.41% in comparison with 80.95% and 73.81% when the estimation was done by calcium excretion and oxalate excretion, respectively. Using the breakpoint value of 4.29 for urine saturation, it was possible to separate children with increased risk of urolithiasis development from the group of children with isolated hematuria.	ammonium chloride;breakpoint;calcium oxalate;classification;computer program;hematuria;logistic regression;magnesium;oxalates;patients;sodium, dietary;uric acid;urolithiasis;inorganic phosphate;sodium citrate	Danko Milosevic;Danica Batinic;Nenad Blau;Pasko Konjevoda;Nikola Stambuk;Ana Votava-Raic;Vesna Barbaric;Ksenija Fumic;Vlatko Rumenjak;Ana Stavljenic-Rukavina;Ljiljana Nizic;Kristina Vrljicak	1998	Journal of chemical information and computer sciences	10.1021/ci9701087	chemistry;computer science;analytical chemistry	HCI	11.731525549286069	-64.17089237291177	117917
d9a52feb71e4d9be08208ace8867c5b08077efe7	quantitative structure-activity relationships of the synthetic substrates for elastase enzyme using nonlinear partial least squares regression	quantitative structure activity relationship;partial least square regression;enzyme	Eighty-nine synthetic substrates for elastase enzyme and its activities (log 1/Km, log kcat, and log Kcat/Km) are treated using partial least squares (PLS) and quadratic partial least squares (QPLS). Chemical features of synthetic substrates are described using principal properties (PPs). By using the QPLS method, we obtain the nonlinear model equations for three properties (log 1/Km, log kcat, and log kcat/km) with the correlation coefficient 0.736, 0.918, and 0.868, respectively. Also, the predictive correlation coefficients for these model equations are 0.640, 0.865, and 0.793, respectively. By this study, it becomes clear that the z2 value of the amino acid residue on position A and the size of the side chain for amino acid residues on position B are related to the properties of the synthetic substrates.		Toshiro Kimura;Yoshikatsu Miyashita;Kimito Funatsu;Shin-ichi Sasaki	1996	Journal of chemical information and computer sciences	10.1021/ci9501103	chromatography;stereochemistry;enzyme;chemistry;computer science;machine learning;partial least squares regression;quantitative structure–activity relationship;statistics	Metrics	12.614368878309959	-57.8453200773068	118497
2d2127532fbad4d5558ead3e5617e3d3004dd5cf	deepfakes: a new threat to face recognition? assessment and detection		It is becoming increasingly easy to automatically replace a face of one person in a video with the face of another person by using a pre-trained generative adversarial network (GAN). Recent public scandals, e.g., the faces of celebrities being swapped onto pornographic videos, call for automated ways to detect these Deepfake videos. To help developing such methods, in this paper, we present the first publicly available set of Deepfake videos generated from videos of VidTIMIT database. We used open source software based on GANs to create the Deepfakes, and we emphasize that training and blending parameters can significantly impact the quality of the resulted videos. To demonstrate this impact, we generated videos with low and high visual quality (320 videos each) using differently tuned parameter sets. We showed that the state of the art face recognition systems based on VGG and Facenet neural networks are vulnerable to Deepfake videos, with 85.62% and 95.00% false acceptance rates respectively, which means methods for detecting Deepfake videos are necessary. By considering several baseline approaches, we found that audio-visual approach based on lip-sync inconsistency detection was not able to distinguish Deepfake videos. The best performing method, which is based on visual quality metrics and is often used in presentation attack detection domain, resulted in 8.97% equal error rate on high quality Deepfakes. Our experiments demonstrate that GAN-generated Deepfake videos are challenging for both face recognition systems and existing detection methods, and the further development of face swapping technology will make it even more so.		Pavel Korshunov;Sébastien Marcel	2018	CoRR		machine learning;artificial intelligence;pattern recognition;computer science;artificial neural network;word error rate;facial recognition system;swap (computer programming);software	Vision	21.198835760116424	-55.16491247301621	118950
1556427100e8ef8176e27aeec396ce0d958a6257	automated quantification of zebrafish tail deformation for high-throughput drug screening	dna;health research;uk clinical guidelines;drugs;biological patents;europe pubmed central;diseases biomedical imaging drugs microscopy biological system modeling cancer;citation search;biomechanics;aquaculture;medicinsk bildbehandling;deformation;uk phd theses thesis;zebrafish danio rerio curvature extraction high throughput screening quantitative microscopy;feature extraction;medical image processing;life sciences;uk research reports;medical journals;zebrafish tail embryo deformation automated quantification feature extraction classification accuracy multifish microplate wells early phase drug discovery automated image based quantification dna repair chemical agents translucent body biomedical research vertebrate model organism danio rerio high throughput drug screening;cellular biophysics;europe pmc;biomedical research;medical image processing aquaculture biomechanics cellular biophysics deformation dna drugs feature extraction;bioinformatics	Zebrafish (Danio rerio) is an important vertebrate model organism in biomedical research thanks to its ease of handling and translucent body, enabling in vivo imaging. Zebrafish embryos undergo spinal deformation upon exposure to chemical agents that inhibit DNA repair. Automated image-based quantification of spine deformation is therefore attractive for whole-organism based assays for use in early-phase drug discovery. We propose an automated method for accurate high-throughput measurement of tail deformations in multi-fish micro-plate wells. The method generates refined medial representations of partial tail-segments. Subsequently, these disjoint segments are analyzed and fused to generate complete tails. Based on estimated tail curvatures we reach a classification accuracy of 91% on individual animals as compared to known control treatment. This accuracy is increased to 95% when combining scores for fish in the same well.	dna repair;drug discovery;embryo;handling (psychology);high-throughput computing;in vivo imaging;medial graph;numerous;quantitation;tail;throughput;video-in video-out;water wells;zebrafish proteins	Omer Ishaq;Joseph Negri;Mark-Anthony Bray;Alexandra Pacureanu;Randall T. Peterson;Carolina Wählby	2013	2013 IEEE 10th International Symposium on Biomedical Imaging	10.1109/ISBI.2013.6556621	aquaculture;feature extraction;bioinformatics;biomechanics;nanotechnology;deformation;dna	Embedded	13.04413947921989	-65.85320050706417	119150
4fcda3271e79e40b0a5382a0c3df89b905fc5994	boosting multiclass learning with repeating codes and weak detectors for protein subcellular localization	image recognition;learning algorithm;system approach;protein function;proteine;learning;hela cell;localization;bioinformatique;green fluorescent protein;localizacion;proceso adquisicion;acquisition process;public domain;computer vision;protein localization;aprendizaje;apprentissage;performance improvement;localisation;machine learning;fusion protein;protein subcellular location;protein subcellular localization;image analysis;proteina;protein expression;bioinformatica;protein;processus acquisition;error correcting output code;bioinformatics	MOTIVATION Determining locations of protein expression is essential to understand protein function. Advances in green fluorescence protein (GFP) fusion proteins and automated fluorescence microscopy allow for rapid acquisition of large collections of protein localization images. Recognition of these cell images requires an automated image analysis system. Approaches taken by previous work concentrated on designing a set of optimal features and then applying standard machine-learning algorithms. In fact, trends of recent advances in machine learning and computer vision can be applied to improve the performance. One trend is the advances in multiclass learning with error-correcting output codes (ECOC). Another trend is the use of a large number of weak detectors with boosting for detecting objects in images of real-world scenes.   RESULTS We take advantage of these advances to propose a new learning algorithm, AdaBoost.ERC, coupled with weak and strong detectors, to improve the performance of automatic recognition of protein subcellular locations in cell images. We prepared two image data sets of CHO and Vero cells and downloaded a HeLa cell image data set in the public domain to evaluate our new method. We show that AdaBoost.ERC outperforms other AdaBoost extensions. We demonstrate the benefit of weak detectors by showing significant performance improvements over classifiers using only strong detectors. We also empirically test our method's capability of generalizing to heterogeneous image collections. Compared with previous work, our method performs reasonably well for the HeLa cell images.   AVAILABILITY CHO and Vero cell images, their corresponding feature sets (SSLF and WSLF), our new learning algorithm, AdaBoost.ERC, and Supplementary Material are available at http://aiia.iis.sinica.edu.tw/	adaboost;bioinformatics;biological science disciplines;boosting (machine learning);cellular phone;cellular material:mcnt:pt:calculus:qn:estimated;code;collections (publication);computer vision;concentrate dosage form;detectors;endoscopic retrograde cholangiography;error detection and correction;genetic heterogeneity;green fluorescent proteins;hela cells;image analysis;learning with errors;machine learning;microscopy, fluorescence;multiclass classification;national supercomputer centre in sweden;physical object;robustness (computer science);algorithm;intracellular protein transport;recycling endosome	Chung-Chih Lin;Yuh-Show Tsai;Yu-Shi Lin;Tai-Yu Chiu;Chia-Cheng Hsiung;May-I Lee;Jeremy C. Simpson;Chun-Nan Hsu	2007	Bioinformatics	10.1093/bioinformatics/btm497	biology;public domain;image analysis;internationalization and localization;green fluorescent protein;computer science;bioinformatics;artificial intelligence;machine learning;fusion protein;protein subcellular localization prediction;protein expression;genetics	ML	10.670824987816435	-54.54893107333258	119391
2a6d869d4238cd7661384adc8c7162f7bc3b491b	classifying expressions by cascade-correlation neural network	mimica;correlacion;mimique;success rate;correlation;facial expression;reseau neuronal;red neuronal;neural network	The classification of facial expressions by cascade-correlation neural networks [1] is described. A success rate of 100% over the training data for each of six categories of emotion —happiness, sadness, anger, surprise, fear and disgust — and of up to 87.5% over the same categories for the test data, has been achieved. By using single emotion nets for each category, together with a Net for Resolution, the results represent a 12.5% success rate beyond what was achieved by a single net classifying over all six emotion categories. Face data in the form of 10 hand measurements made on 94 well validated full face photographs [2] provided the input data after normalisation. These measures, among others, had previously been shown to discriminate between emotions [3].	artificial neural network;convolutional neural network;sadness;test data	Jun Zhao;Garrett Kearney;Alan K Soper	1995	Neural Computing & Applications	10.1007/BF01421962	computer science;artificial intelligence;machine learning;facial expression;correlation;artificial neural network	NLP	24.062029066903545	-61.316438169779275	119883
8921be5339e1b3b62bd15368cbb2c3203a20ee3c	invisible steganography via generative adversarial network		Steganography and steganalysis are main content of information hiding, they always make constant progress in confrontation. There is large consent that measures based on deep learning have outperformed conventional approaches in steganalysis, which have shown that deep learning is very promising for the information hiding area. And in the last two years, there are also several works using deep learning to achieve the procedure of steganography. While these works still have problems in capacity, invisibility and security. We proposed a new steganography model based on generative adversarial network named as ISGAN. Our model can conceal a secret gray image into a color cover image, and can reveal the secret image successfully. To improve the invisibility, we select a new concealing position and get excellent results. And with the help of GAN’s adversarial training, our model can improve the security. In addition, we use a new loss function which is more appropriate to steganography, can boost the training speed and generate better stego images and revealed secret images. Experiment results show that ISGAN can achieve start-of-art performance on LFW, PASCALVOC12 and ImageNet datasets. ∗This work was supported by the National key Research and Development Program of China(No.2016YFB0800404) and the NSF of China(U1636112,U1636212).	deep learning;ibm notes;imagenet;loss function;steganalysis;steganography	Shiqi Dong;Ru Zhang;Jianyi Liu	2018	CoRR		steganography;computer science;information hiding;convolutional neural network;deep learning;architecture;artificial intelligence;pattern recognition;steganalysis;communication source;human visual system model	Vision	19.70576970401662	-52.27637853300422	119893
d90a48fe2343705a3f15ea2eb2765bb219e5844a	docking studies of tau protein	tau protein	Alzheimer’s disease (AD)[1] is a fatal brain disorder and alone in United States approximately 4.5 millions Americans are suffering from this disease which is expected by 2050 to range between 11.3 million to 16 million[2] Alzheimer’s disease is a form of dementia, in which nerve cells in memory areas of brain and eventually other areas begin to die at accelerated rate resulting in serious deterioration in several mental functions, such as loss in memory, language, orientation and judgment [3] AD is characterized by the formation of senile plaques (made of β amyloid, a toxic protein that comes from normal protein) and neurofibrillary tangles (followed by changes in tau protein) resulting in neuronal destructions. Currently available drugs against AD target the acetylcholine cycle thus stopping the abnormal breakdown of acetylcholine. The modern docking programs/software packages e.g. MOE, AcSite, Spdbv, and Rastop etc can be used to find the active site of the tau protein [4]. The ligand against this active binding site can be found by MOE. The exact confirmation and configuration of the ligand can be calculated to find the best molecule with minimum binding energy [5] and it can be used to develop potential drug molecules against the disease. This work is an attempt to find out the amino acid sequences responsible for biologically active structure which should enable us to design a lead molecule against the disease. In this work we have carried out the docking analysis of Tau protein responsible for AD. 50 structures after the docking were saved in the form of a database. The best five structures in terms of energy were taken, and the amino acids residues of the ligand and receptor molecule which bind to give the best biologically active conformation, were analyzed.	alzheimer's disease neuroimaging initiative;database;docking (molecular);moe	Harkewal Singh;Soma S. Marla;Manas Agarwal	2006			computer science	Comp.	10.119428481126514	-63.31368365795888	119939
df2ff0deab53ee8fc42d83429258e160caa421b7	on the representativeness of convolutional neural networks layers	unsupervised learning;visual embeddings;convolutional neural networks;part of book or chapter of book	Convolutional Neural Networks (CNN) are the most popular of deep network models due to their applicability and success in image processing. Although plenty of effort has been made in designing and training better discriminative CNNs, little is yet known about the internal features these models learn. Questions like, what specific knowledge is coded within CNN layers, and how can it be used for other purposes besides discrimination, remain to be answered. To advance in the resolution of these questions, in this work we extract features from CNN layers, building vector representations from CNN activations. The resultant vector embedding is used to represent first images and then known image classes. On those representations we perform an unsupervised clustering process, with the goal of studying the hidden semantics captured in the embedding space. Several abstract entities untaught to the network emerge in this process, effectively defining a taxonomy of knowledge as perceived by the CNN. We evaluate and interpret these sets using WordNet, while studying the different behaviours exhibited by the layers of a CNN model according to their depth. Our results indicate that, while top (i.e., deeper) layers provide the most representative space, low layers also define descriptive dimensions.	cluster analysis;convolutional neural network;entity;image processing;layers (digital image editing);neural networks;resultant;taxonomy (general);wordnet	Dario Garcia-Gasulla;Jonathan Moreno;Raúl Ramos-Pollán;Romel Casadiegos Barrios;Javier Béjar;Ulises Cortés;Eduard Ayguadé;Jesús Labarta;Toyotaro Suzumura	2016		10.3233/978-1-61499-696-5-29	computer science;artificial intelligence;theoretical computer science;machine learning	AI	23.110205335712028	-53.35728277376916	120408
563606122653c26f4c300771ba2b6294783b3b50	active sensorimotor object recognition in three-dimensional space		Spatial interaction of biological agents with their environment is based on the cognitive processing of sensory as well as motor information. There are many models for sole sensory processing but only a few for integrating sensory and motor information into a unifying sensorimotor approach. Additionally, neither the relations shaping the integration are yet clear nor how the integrated information can be used in an underlying representation. Therefore, we propose a probabilistic model for integrated processing of sensory and motor information by combining bottom-up feature extraction and top-down action selection embedded in a Bayesian inference approach. The integration of sensory perceptions and motor information brings about two main advantages: (i) Their statistical dependencies can be exploited by representing the spatial relationships of the sensor information in the underlying joint probability distribution and (ii) a top-down process can compute the next most informative region according to an information gain strategy. We evaluated our system in two different object recognition tasks. We found that the integration of sensory and motor information significantly improves active object recognition, in particular when these movements have been chosen by an information gain strategy.	outline of object recognition	David Nakath;Tobias Kluth;Thomas Reineking;Christoph Zetzsche;Kerstin Schill	2014		10.1007/978-3-319-11215-2_22	sensory processing;feature extraction;computer science;perception;sensory system;active object;artificial intelligence;pattern recognition;cognition;bayesian inference;action selection	Robotics	20.570298079807475	-64.93390694501562	120598
7aa958ceeadf77abbe4a4a016197e94002835d36	design of stapled dna-minor-groove-binding molecules with a mutable atom simulated annealing method	hydrogen bond;simulated annealing;rational drug design	We report the design of optimal linker geometries for the synthesis of stapled DNA-minor-groove-binding molecules. Netropsin, distamycin, and lexitropsins bind side-by-side to mixed-sequence DNA and offer an opportunity for the design of sequence-reading molecules. Stapled molecules, with two molecules covalently linked side-by-side, provide entropic gains and restrain the position of one molecule relative to its neighbor. Using a free-atom simulated annealing technique combined with a discrete mutable atom definition, optimal lengths and atomic composition for covalent linkages are determined, and a novel hydrogen bond 'zipper' is proposed to phase two molecules accurately side-by-side.		Wynn L. Walker;Mary L. Kopka;Richard E. Dickerson;David S. Goodsell	1997	Journal of computer-aided molecular design	10.1023/A:1007985019866	stereochemistry;chemistry;simulated annealing;nanotechnology;hydrogen bond;nuclear magnetic resonance;drug design	EDA	11.514819902619932	-61.18792955836121	121240
34a2b9bda97fd660eb4d574667c0a1510e30e0f1	protein secondary structure prediction using deep convolutional neural fields	biological patents;biomedical journals;text mining;europe pubmed central;citation search;citation networks;research articles;abstracts;open access;life sciences;clinical guidelines;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Protein secondary structure (SS) prediction is important for studying protein structure and function. When only the sequence (profile) information is used as input feature, currently the best predictors can obtain ~80% Q3 accuracy, which has not been improved in the past decade. Here we present DeepCNF (Deep Convolutional Neural Fields) for protein SS prediction. DeepCNF is a Deep Learning extension of Conditional Neural Fields (CNF), which is an integration of Conditional Random Fields (CRF) and shallow neural networks. DeepCNF can model not only complex sequence-structure relationship by a deep hierarchical architecture, but also interdependency between adjacent SS labels, so it is much more powerful than CNF. Experimental results show that DeepCNF can obtain ~84% Q3 accuracy, ~85% SOV score, and ~72% Q8 accuracy, respectively, on the CASP and CAMEO test proteins, greatly outperforming currently popular predictors. As a general framework, DeepCNF can be used to predict other protein structure properties such as contact number, disorder regions, and solvent accessibility.	accessibility;artificial neural network;casp;conditional random field;conjunctive normal form;deep learning;interdependence;neural network simulation;neural tube defects;protein, organized by structure;negative regulation of secondary cell wall biogenesis	Sheng Wang;Jian Peng;Jianzhu Ma;Jinbo Xu	2016		10.1038/srep18962	natural language processing;text mining;medical research;computer science;bioinformatics;data science;data mining	ML	11.774159696535506	-54.62523095743906	121886
764e96396726f52501066516c184165f75918e43	qsar modeling of genotoxicity on non-congeneric sets of organic compounds	forward selection;multi linear regression;structure factor;mutagenicity;linear regression;ames test;molecular descriptor;molecular descriptors;qsar;quantum chemical descriptors;artificial neural network;neural network;organic compound;molecular structure	A multi-linear (ML) and artificial neural network (ANN) approaches have been used to derive quantitativestructure-activity relationships (QSAR) between the genotoxicity (mutagenicity) and molecular structure of compounds by using large initial pools of descriptors. All derived models involve descriptors that describe possible structural factors influencing the mutagenicbehavior of organic compounds. Different quantum chemical characteristics of compounds have been successfully used together with conventional molecular descriptors. The connection between descriptors represented in the models and the mutagenic behavior ofcompounds is also discussed.	artificial neural network;hydrogen;molecular descriptor;nonlinear system;quantitative structure–activity relationship;stepwise regression	Uko Maran;Sulev Sild	2003	Artificial Intelligence Review	10.1023/A:1026084514236	molecular descriptor;computer science;machine learning;artificial neural network	ML	12.575756114368348	-57.38635920030604	122041
54d184b62691b336148f83d6097ca166fd156f67	towards understanding learning representations: to what extent do different neural networks learn the same representation		It is widely believed that learning good representations is one of the main reasons for the success of deep neural networks. Although highly intuitive, there is a lack of theory and systematic approach quantitatively characterizing what representations do deep neural networks learn. In this work, we move a tiny step towards a theory and better understanding of the representations. Specifically, we study a simpler problem: How similar are the representations learned by two networks with identical architecture but trained from different initializations. We develop a rigorous theory based on the neuron activation subspace match model. The theory gives a complete characterization of the structure of neuron activation subspace matches, where the core concepts are maximum match and simple match which describe the overall and the finest similarity between sets of neurons in two networks respectively. We also propose efficient algorithms to find the maximum match and simple matches. Finally, we conduct extensive experiments using our algorithms. Experimental results suggest that, surprisingly, representations learned by the same convolutional layers of networks trained from different initializations are not as similar as prevalently expected, at least in terms of subspace match.	activation function;algorithm;artificial neural network;deep learning;experiment;microsoft research;neural network software;neuron	Liwei Wang;Lunjia Hu;Jiayuan Gu;Yue Wu;Zhiqiang Hu;Kun He;John E. Hopcroft	2018			architecture;machine learning;artificial intelligence;artificial neural network;computer science;subspace topology	ML	22.09974512852377	-52.6310699174277	122138
0e724480880fe4270f026db0bc1dbcdf96e23a1c	comparative receptor surface analysis of octopaminergic antagonists for the locust neuronal octopamine receptor	quantitative structure activity relationship;comparative receptor surface analysis;drug discovery;cerius2;three dimensional;locusta migratoria;antagonist for octopamine receptor;surface model;three dimensional structure;receptor surface model;active site;surface analysis	In drug discovery, it is common to have measured activity data for a set of compounds acting upon a particular protein but not to have knowledge of the three-dimensional structure of the protein active site. In the absence of such three-dimensional information, one can attempt to build a hypothetical model of the receptor site that can provide insight about receptor site characteristics. Such a model is known as a comparative receptor surface analysis (CoRSA) model, which provides compact and quantitative descriptors which capture three-dimensional information about a putative receptor site. The quantitative structure-activity relationship (QSAR) of a set of 20 antagonists for octopamine (OA) receptor 3 in locust nervous tissue, was analyzed using CoRSA. Three-dimensional energetics descriptors were calculated from receptor surface model (RSM)-ligand interaction and these three-dimensional descriptors were used in QSAR analysis. The predictive character of the QSAR was further assessed using 24 agonists for OA receptor as test molecules. An RSM was generated using some subset of the most active structures and the results provided useful information in the characterization and differentiation of OA receptor.		Akinori Hirashima;Eiichi Kuwano;Morifusa Eto	2003	Computational biology and chemistry	10.1016/j.compbiolchem.2003.07.001	biology;three-dimensional space;biochemistry;stereochemistry;chemistry;toxicology;bioinformatics;active site;surface weather analysis;quantitative structure–activity relationship;drug discovery	Comp.	11.165098214005074	-59.04010860723437	122464
2c8ebc48cd74a0007b4198685f057b5d9177d7a5	application of 3d zernike descriptors to shape-based ligand similarity searching	health research;uk clinical guidelines;biological patents;statistical moment;molecular recognition;europe pubmed central;citation search;shape recognition;computer applications in chemistry;graph matching;three dimensional;theoretical and computational chemistry;computational biology bioinformatics;virtual screening;evaluation metric;drug design;uk phd theses thesis;life sciences;uk research reports;medical journals;similarity search;europe pmc;documentation and information in chemistry;biomedical research;bioinformatics	BACKGROUND The identification of promising drug leads from a large database of compounds is an important step in the preliminary stages of drug design. Although shape is known to play a key role in the molecular recognition process, its application to virtual screening poses significant hurdles both in terms of the encoding scheme and speed.   RESULTS In this study, we have examined the efficacy of the alignment independent three-dimensional Zernike descriptor (3DZD) for fast shape based similarity searching. Performance of this approach was compared with several other methods including the statistical moments based ultrafast shape recognition scheme (USR) and SIMCOMP, a graph matching algorithm that compares atom environments. Three benchmark datasets are used to thoroughly test the methods in terms of their ability for molecular classification, retrieval rate, and performance under the situation that simulates actual virtual screening tasks over a large pharmaceutical database. The 3DZD performed better than or comparable to the other methods examined, depending on the datasets and evaluation metrics used. Reasons for the success and the failure of the shape based methods for specific cases are investigated. Based on the results for the three datasets, general conclusions are drawn with regard to their efficiency and applicability.   CONCLUSION The 3DZD has unique ability for fast comparison of three-dimensional shape of compounds. Examples analyzed illustrate the advantages and the room for improvements for the 3DZD.	algorithm;benchmark (computing);chemical database;dosage forms;evaluation function;graph - visual representation;ligands;line code;matching (graph theory);virtual screening	Vishwesh Venkatraman;Padmasini Chakravarthy;Daisuke Kihara	2009		10.1186/1758-2946-1-19	three-dimensional space;medicine;virtual screening;computer science;bioinformatics;data science;data mining;molecular recognition;drug design;matching	Vision	10.761356401644123	-58.95045930851758	122500
cc57f04555f37ed13338b23b41eb3be19238cd93	surface comparisons of some odour molecules: conformational calculations on sandalwood odour v	van der waals;biological activity;molecular surface	Molecular surface comparison seems to be a very suitable tool for the investigation of small differences between biologically active and inactive compounds of the same structural type. A fast method for such comparisons, based on volume matching followed by the estimation of comparable surface dots, is presented and applied on a few selected sandalwood odour molecules.		Gerhard Buchbauer;Susanne Winiwarter;Peter Wolschann	1992	Journal of computer-aided molecular design	10.1007/BF00126216	van der waals force;stereochemistry;chemistry;bioinformatics;biological activity;computational chemistry	EDA	11.742920104675207	-59.47421861843029	122657
a9214d35f54a9f12bafe8869d28772cb447c0f43	a fourier fingerprint-based method for protein surface representation	surface representation	A crucial enabling technology for structural genomics is the development of algorithms that can predict the putative function of novel protein structures: the proposed functions can subsequently be experimentally tested by functional studies. Testable assignments of function can be made if it is possible to attribute a putative, or indeed probable, function on the basis of the shapes of the binding sites on the surface of a protein structure. However the comparison of the surfaces of 3D protein structures is a computationally demanding task. Here we present four surface representations that can be used locally to describe the global shape of specifically bounded local region models. The most successful of these representations is obtained by a Fourier analysis of the distribution of surface curvature on concentric spheres around a surface point and summarizes a 24 A diameter spherically clipped region of protein surface by a fingerprint of 18 Fourier amplitude values. Searching experiments using these fingerprints on a set of 366 proteins demonstrate that this provides an effective and an efficient technique for the matching of protein surfaces.		Martin J. Bayley;Eleanor J. Gardiner;Peter Willett;Peter J. Artymiuk	2005	Journal of chemical information and modeling	10.1021/ci049647j	combinatorics;chemistry;topology;mathematics;geometry	Graphics	13.15040278538976	-58.612295533055274	122685
d20a222d94fc18f2e4b807b97d5efecd791af254	a bioequivalence test by the direct comparison of concentration-versus-time curves using local polynomial smoothers		In order to test if two chemically or pharmaceutically equivalent products have the same efficacy and/or toxicity, a bioequivalence (BE) study is conducted. The 80%/125% rule is the most commonly used criteria for BE and states that BE cannot be claimed unless the 90% CIs for the ratio of selected pharmacokinetics (PK) parameters of the tested to the reference drug are within 0.8 to 1.25. Considering that estimates of these PK parameters are derived from the concentration-versus-time curves, a direct comparison between these curves motivates an alternative and more flexible approach to test BE. Here, we propose to frame the BE test in terms of an equivalence of concentration-versus-time curves which are constructed using local polynomial smoother (LPS). A metric is presented to quantify the distance between the curves and its 90% CIs are calculated via bootstrapping. Then, we applied the proposed procedures to data from an animal study and found that BE between a generic drug and its brand name cannot be concluded, which was consistent with the results by applying the 80%/125% rule. However, the proposed procedure has the advantage of testing only on a single metric, instead of all PK parameters.	adverse reaction to drug;bootstrapping (compilers);computation;confidence limit;drug kinetics;estimated;lightweight portable security;list of statistical packages;plasma active;polynomial;proprietary name;public-key cryptography;therapeutic equivalency study;turing completeness;united states food and drug administration;interest;little p little k ab:prthr:pt:ser/plas:ord	Suyan Tian;Howard H. Chang;Dana Orange;Jingkai Gu;Mayte Suárez-Fariñas	2016		10.1155/2016/4680642	econometrics;mathematical optimization;mathematics;statistics	ML	15.430016162333464	-58.12274616694913	123015
b9a5bd7c62d880d6074d14454ae30ecf3401eb21	noise tolerance of the universal similarity metric applied to protein contact map comparison in two dimensions	two dimensions	Comparing protein structures based on their contact maps is an important problem in structural proteomics. Building a system for reconstructing protein tertiary structures from their contact maps motivates devising novel contact map comparison algorithms. Several methods that address the contact map comparison problem have been designed. However, they suggest scoring schemes that do not satisfy the two characteristics of “metricity” and “universality”. In this paper we propose a method to apply the Universal Similarity Metric (USM) to contact map comparison in two dimensions. Previous works were based on the conversion of twodimensional data structures (contact maps) to one-dimensional data structures (strings), which generally causes loss of structural information. Our method, which circumvents this dimension reduction, has the advantage of being noise tolerant. Our method is considerably more size-independent than the stringbased method. These features are vital to applications in protein structure reconstruction.		Sara Rahmati;Janice I. Glasgow	2008			protein contact map;intrinsic metric;universality (philosophy);dimensionality reduction;mathematics;data structure;topology	Comp.	15.055537379464756	-54.73271076378795	123166
3401274a162558fe4ecf6081ff160bef41a72d1d	from secondary structure to three-dimensional structure: improved dihedral angle probability distribution function for use with energy searches for native structures of polypeptides and proteins	secondary structure;probability distribution function;dihedral angle			Betty Cheng;Akbar Nayeem;Harold A. Scheraga	1996	Journal of Computational Chemistry	10.1002/(SICI)1096-987X(199609)17:12%3C1453::AID-JCC6%3E3.0.CO;2-J	crystallography;probability density function;combinatorics;mathematics;geometry;dihedral angle;graphical models for protein structure;protein secondary structure	Comp.	13.881586945963507	-58.63580895533876	123592
4b0c3d1ad99c94dc8d68c81082897fefed6dbea4	sensing with artificial tactile sensors: an investigation of spatio-temporal inference	hierarchical temporal memory;bayesian inference;cortical computation;tactile perception;pattern recognition	The ease and efficiency with which biological systems deal with several real world problems, that have been persistently challenging to implement in artificial systems, is a key motivation in biomimetic robotics. In interacting with its environment, the first challenge any agent faces is to extract meaningful patterns in the inputs from its sensors. This problem of pattern recognition has been characterized as an inference problem in cortical computation. The work presented here implements the hierarchical temporal memory (HTM) model of cortical computation using inputs from an array of artificial tactile sensors to recognize simple Braille patterns. Although the current work has been implemented using a small array of robot whiskers, the architecture can be extended to larger arrays of sensors of any arbitrary modality.	biological system;biomimetics;computation;hierarchical temporal memory;interaction;modality (human–computer interaction);pattern recognition;robotics;sensor	Asma Motiwala;Charles W. Fox;Nathan F. Lepora;Tony J. Prescott	2011		10.1007/978-3-642-23232-9_23	computer vision;computer science;artificial intelligence;machine learning	Robotics	20.73332004813552	-65.16954869321714	123654
deeb11437191aff254c69d3581e6dccfe882b53d	modelling and prediction of toxicity of environmental pollutants	high dimensionality;molecular descriptor;environmental pollutant	This paper describes the problem of modelling toxicity of environmental pollutants using molecular descriptors from a systems theoretical viewpoint. It is shown that current toxicity modelling problems systematically incorporate very high levels of noise a priori. By means of a set of individual and combined models self-organised by KnowledgeMiner from a high-dimensional molecular descriptor data set calculated within the DEMETRA project we suggest a way how results interpretation and final decision making can effectively take into account the huge uncertainty of toxicity models.	group method of data handling;level of detail;molecular descriptor;nonlinear system;quantitative structure–activity relationship;runtime system;self-organization;signal-to-noise ratio	Frank Lemke;Johann-Adolf Müller;Emilio Benfenati	2004		10.1007/978-3-540-30478-4_19	environmental science;environmental engineering;toxicology;data mining	ML	13.270223489620706	-55.04020867073837	124191
6c49bd4ca7350a074392376d1dbe332fda017e4f	on the performance of googlenet and alexnet applied to sketches	image classification;deep neural network;sketch classification	This work provides a study on how Convolutional Neural Networks, trained to identify objects primarily in photos, perform when applied to more abstract representations of the same objects. Our main goal is to better understand the generalization abilities of these networks and their learned inner representations. We show that both GoogLeNet and AlexNet networks are largely unable to recognize abstract sketches that are easily recognizable by humans. Moreover, we show that the measured efficacy vary considerably across different classes and we discuss possible reasons for this.	convolutional neural network;neural networks	Pedro Ballester;Ricardo Matsumura de Araújo	2016			computer vision;contextual image classification;computer science;artificial intelligence;machine learning	NLP	22.96538881714926	-53.3003254013826	124244
8db6053c94858581f30c7a6abb4663ab435eae81	4d-qsar analysis of a series of antifungal p450 inhibitors and 3d-pharmacophore comparisons as a function of alignment		"""A training set of 55 antifungal p450 analogue inhibitors was used to construct receptor-independent four-dimensional quantitative structure-activity relationship (RI 4D-QSAR) models. Ten different alignments were used to build the models, and one alignment yields a significantly better model than the other alignments. Two different methodologies were used to measure the similarity of the best 4D-QSAR models of each alignment. One method compares the residual of fit between pairs of models using the cross-correlation coefficient of their residuals of fit as a similarity measure. The other method compares the spatial distributions of the IPE types (3D-pharmacophores) of pairs of 4D-QSAR models from different alignments. Optimum models from several different alignments have nearly the same correlation coefficients, r(2), and cross-validation correlation coefficients, xv-r(2), yet the 3D-pharmacophores of these models are very different from one another. The highest 3D-pharmacophore similarity correlation coefficient between any pair of 4D-QSAR models from the 10 alignments considered is only 0.216. However, the best 4D-QSAR models of each alignment do contain some proximate common pharmacorphore sites. A test set of 10 compounds was used to validate the predictivity of the best 4D-QSAR models of each alignment. The """"best"""" model from the 10 alignments has the highest predictivity. The inferred active sites mapped out by the 4D-QSAR models suggest that hydrogen bond interactions are not prevalent when this class of P450 analogue inhibitors binds to the receptor active site. This feature of the 4D-QSAR models is in agreement with the crystal structure results that indicate no ligand-receptor hydrogen bonds are formed."""	alignment;analog;antifungal agents;coefficient;cross-correlation;cross-validation (statistics);crystal structure;hydrogen bonding;inference;interaction;ligands;pharmacophore;quantitative structure-activity relationship;similarity measure;test set;triangulation	Jianzhong Liu;Dahua Pan;Yufeng J. Tseng;Anton J. Hopfinger	2003	Journal of chemical information and computer sciences	10.1021/ci034142z	econometrics;bioinformatics;mathematics;statistics	Comp.	11.900541443085455	-58.39759510250301	124609
e6b14d05ca4edfdf9b057244d29068e9f2e3a52f	a probabilistic model for discovering high level brain activities from fmri	fmri;conditional random fields;conference proceeding	Functional magnetic resonance imaging (fMRI) has provided an invaluable method of investing real time neuron activities. Statistical tools have been developed to recognise the mental state from a batch of fMRI observations over a period.#R##N##R##N#However, an interesting question is whether it is possible to estimate the real time mental states at each moment during the fMRI observation. In this paper, we address this problem by building a probabilistic model of the brain activity. We model the tempo-spatial relations among the hidden high-level mental states and observable low-level neuron activities. We verify our model by experiments on practical fMRI data. The model also implies interesting clues on the task-responsible regions in the brain.	statistical model	Jun Li;Dacheng Tao	2011		10.1007/978-3-642-24955-6_40	computer science;artificial intelligence;machine learning;conditional random field	ML	20.076267770240957	-63.09809172574805	125128
2dccd5db1ca2350b92ceacc77597086afb7da9ff	universal j-coupling prediction		A data driven approach for small molecule J-coupling prediction is presented. The method is targeted for use as part of an automatic spectrum analysis, therefore emphasizing prediction coverage, maintainability, and speed in the design. The database search involves encoding the coupling path atom types into hash codes, which are used to retrieve the matching coupling constant entries from the database. The fast hash dictionary search is followed by a k Nearest Neighbors regression to resolve the substituent and conformational dependencies, parametrized with atomic charges, torsion angles, and steric bulk.		Juuso Lehtivarjo;Matthias Niemitz;Samuli-Petrus Korhonen	2014	Journal of chemical information and modeling	10.1021/ci500057f	steric effects;j-coupling;computational chemistry;hash function;parametrization;coupling;database search engine;k-nearest neighbors algorithm;coupling constant;computer science	DB	11.503250602378268	-58.1866415531484	125380
2af1c101bd447cdc04599dd81e71ce56c0d65c8b	refined method for droplet microfluidics-enabled detection of plasmodium falciparum encoded topoisomerase i in blood from malaria patients	topoisomerase;malaria;diagnostics;point of care;droplet microfluidics	Rapid and reliable diagnosis is essential in the fight against malaria, which remains one of the most deadly infectious diseases in the world. In the present study we take advantage of a droplet microfluidics platform combined with a novel and user-friendly biosensor for revealing the main malaria-causing agent, the Plasmodium falciparum (P. falciparum) parasite. Detection of the parasite is achieved through detection of the activity of a parasite-produced DNA-modifying enzyme, topoisomerase I (pfTopoI), in the blood from malaria patients. The assay presented has three steps: (1) droplet microfluidics-enabled extraction of active pfTopoI from a patient blood sample; (2) pfTopoI-mediated modification of a specialized DNA biosensor; (3) readout. The setup is quantitative and specific for the detection of Plasmodium topoisomerase I. The procedure is a considerable improvement of the previously published Rolling Circle Enhanced Enzyme Activity Detection (REEAD) due to the advantages of involving no signal amplification steps combined with a user-friendly readout. In combination these alterations represent an important step towards exploiting enzyme activity detection in point-of-care diagnostics of malaria. Micromachines 2015, 6, 1505–1513; doi:10.3390/mi6101432 www.mdpi.com/journal/micromachines Micromachines 2015, 6, 1505–1513	exploit (computer security);the fight: lights out;usability	Marianne Smedegaard Hede;Patricia Nkem Okorie;Signe Kirk Fruekilde;Søren Fjelstrup;Jonas Thomsen;Oskar Franch;Cinzia Tesauro;Magnus Tobias Bugge;Mette Christiansen;Stéphane Picot;Felix Lötsch;Ghyslain Mombo-Ngoma;Johannes Mischlinger;Ayôla A. Adegnika;Finn Skou Pedersen;Yi-Ping Ho;Eskild Petersen	2015	Micromachines	10.3390/mi6101432	molecular biology;point of care;topoisomerase	Vision	10.94103611446765	-64.62490416801008	125517
52ed6241e156b07abc1ea468c9fa27b681ae8bb4	parameter estimation with bio-inspired meta-heuristic optimization: modeling the dynamics of endocytosis	simulation and modeling;systems biology;physiological cellular and medical topics;models biological;computational biology bioinformatics;endocytosis;nonlinear dynamics;algorithms;computer simulation;bioinformatics	We address the task of parameter estimation in models of the dynamics of biological systems based on ordinary differential equations (ODEs) from measured data, where the models are typically non-linear and have many parameters, the measurements are imperfect due to noise, and the studied system can often be only partially observed. A representative task is to estimate the parameters in a model of the dynamics of endocytosis, i.e., endosome maturation, reflected in a cut-out switch transition between the Rab5 and Rab7 domain protein concentrations, from experimental measurements of these concentrations. The general parameter estimation task and the specific instance considered here are challenging optimization problems, calling for the use of advanced meta-heuristic optimization methods, such as evolutionary or swarm-based methods. We apply three global-search meta-heuristic algorithms for numerical optimization, i.e., differential ant-stigmergy algorithm (DASA), particle-swarm optimization (PSO), and differential evolution (DE), as well as a local-search derivative-based algorithm 717 (A717) to the task of estimating parameters in ODEs. We evaluate their performance on the considered representative task along a number of metrics, including the quality of reconstructing the system output and the complete dynamics, as well as the speed of convergence, both on real-experimental data and on artificial pseudo-experimental data with varying amounts of noise. We compare the four optimization methods under a range of observation scenarios, where data of different completeness and accuracy of interpretation are given as input. Overall, the global meta-heuristic methods (DASA, PSO, and DE) clearly and significantly outperform the local derivative-based method (A717). Among the three meta-heuristics, differential evolution (DE) performs best in terms of the objective function, i.e., reconstructing the output, and in terms of convergence. These results hold for both real and artificial data, for all observability scenarios considered, and for all amounts of noise added to the artificial data. In sum, the meta-heuristic methods considered are suitable for estimating the parameters in the ODE model of the dynamics of endocytosis under a range of conditions: With the model and conditions being representative of parameter estimation tasks in ODE models of biochemical systems, our results clearly highlight the promise of bio-inspired meta-heuristic methods for parameter estimation in dynamic system models within system biology.	algorithm;ambient occlusion;axysta cesta;biologic development;biological system;british informatics olympiad;convergence (action);differential diagnosis;differential evolution;dynamical system;endocytosis;endosomes;estimated;estimation theory;inspiration function;leucaena pulverulenta;manuscripts;mathematical optimization;monte carlo method;nonlinear system;numerical analysis;optimization problem;particle swarm optimization;physiological sexual disorders;population parameter;pseudo brand of pseudoephedrine;public-key cryptography;rate of convergence;stigmergy;systems biology;tom conte	Katerina Tashkova;Peter Korosec;Jurij Silc;Ljupco Todorovski;Saso Dzeroski	2011		10.1186/1752-0509-5-159	computer simulation;biology;endocytosis;simulation;computer science;bioinformatics;theoretical computer science;systems biology	ML	13.094038978311222	-53.13380000728014	125570
cadad7c90c5dbda4b210cf64d112eb5ab67bad10	vigorous exercise increases brain lactate and glx (glutamate<ce:hsp sp=0.12></ce:hsp>+<ce:hsp sp=0.12></ce:hsp>glutamine): a dynamic 1h-mrs study	amino acid;glucose uptake;heart rate;proton magnetic resonance spectroscopy;major depression;indexation;panic disorder;glutamate;visual cortex;energy metabolism;mr technique;oxygen uptake	Vigorous exercise increases lactate and glucose uptake by the brain in excess of the increase in brain oxygen uptake. The metabolic fate of this non-oxidized carbohydrate entering the brain is poorly understood, but accumulation of lactate in the brain and/or increased net synthesis of amino acid neurotransmitters are possible explanations. Previous proton magnetic resonance spectroscopy (1H-MRS) studies using conventional pulse sequences have not detected changes in brain lactate following exercise. This contrasts with 1H-MRS studies showing increased brain lactate when blood lactate levels are raised by an intravenous infusion of sodium lactate. Using a J-editing 1H-MRS technique for measuring lactate, we demonstrated a significant 19% increase in lactate in the visual cortex following graded exercise to approximately 85% of predicted maximum heart rate. However, the magnitude of the increase was insufficient to account for more than a small fraction of the non-oxidized carbohydrate entering the brain with exercise. We also report a significant 18% increase in Glx (combined signal from glutamate and glutamine) in visual cortex following exercise, which may represent an activity-dependent increase in glutamate. Future studies will be necessary to test the hypothesis that non-oxidized carbohydrate entering the brain during vigorous exercise is directed, in part, toward increased net synthesis of amino acid neurotransmitters. The possible relevance of these findings to panic disorder and major depression is discussed.	amino acids;carbohydrates;cerebral cortex;ephrin type-b receptor 1, human;glx;glutamine;greater than;heat shock proteins;how true feel vigorous right now;lactic acid;magnetic resonance spectroscopy;major depressive disorder;metabolic process, cellular;minimal recursion semantics;offset binary;oxygen;panic disorder;protons;relevance;sodium lactate;tree accumulation;explanation;glucose uptake	Richard J. Maddock;Gretchen A. Casazza;Michael H. Buonocore;Costin Tanase	2011	NeuroImage	10.1016/j.neuroimage.2011.05.048	psychology;endocrinology;neuroscience;amino acid;glutamate receptor;diabetes mellitus	ML	11.7445824243842	-64.34330819268705	126111
3b7f147681e58e28ef77a726f5ccfd7f26e5e1c0	the path integral for dendritic trees	fonction green;path integral;neurone;integration nerveuse;time dependent;dendrita;integracion nerviosa;modele mathematique;funcion green;neural integration;computational techniques;brownian motion;forma descarga;dendrite;electrophysiology;modelo matematico;integral recorrido;websearch;hep;integrale parcours;neurona;cognition;mathematical model;cognicion;electrofisiologia;reseau neuronal;electrophysiologie;red neuronal;neuron;green function;discharge pattern;neural network;mode decharge	We construct the path integral for determining the potential on any dendritic tree described by a linear cable equation. This is done by generalizing Brownian motion from a line to a tree. We also construct the path integral for dendritic structures with spatially-varying and/or time-dependent membrane conductivities due, for example, to synaptic inputs. The path integral allows novel computational techniques to be applied to cable problems. Our anlaysis leads ultimately to an exact expression for the Green's function on a dendritic tree of arbitrary geometry expressed in terms of a set of simple diagrammatic rules. These rules providing a fast and efficient method for solving complex cable problems.	brownian motion;cable theory;citrus aurantium;diagram;nut hypersensitivity;path integral formulation;rule (guideline);synaptic package manager;tissue membrane;trees (plant)	L. F. Abbott;Edward Farhi;Sam Gutmann	1991	Biological Cybernetics	10.1007/BF00196452	psychology;electrophysiology;combinatorics;dendrite;path integral formulation;cognition;calculus;mathematical model;brownian motion;mathematics;geometry;green's function;artificial neural network;physics;quantum mechanics	Theory	16.386954454710775	-65.98828812981682	126412
70a58208f652941cc031df08d5b27de68dce007d	detecting local clusters in the data on disease vectors influenced by linear features	local malaria vector clusters;disease vector;local malaria vector clusters detecting local clusters disease vectors linear features spatial scan statistic disease vector studies contextual information malaria vectors;disease vector spatial scan statistics linear feature;contextual information;spatial scan statistic;medical computing;linear features;distance measurement;shape diseases vectors distance measurement maximum likelihood detection feature extraction pattern analysis;geophysics computing;shape;statistical analysis;vectors;malaria vectors;feature extraction;maximum likelihood detection;diseases;disease vectors;pattern analysis;detecting local clusters;disease vector studies;spatial scan statistics;linear feature;statistical analysis diseases geophysics computing medical computing	Spatial scan statistic has been applied in many disease vector studies. However it rarely takes into account some relevant contextual information. As a result, the interpretation of the test results has been challenging and some interpretations could be misleading. In this study, a new technique to apply spatial scan statistic for the detection of local clusters in disease vectors is proposed. This new technique takes into account relevant contextual information. In particular, it considers the influences of linear features on the distribution of disease vectors. A case study on malaria vectors was conducted to elucidate this new technique. The results of the case study indicate that the proposed approach can provide a more meaningful identification and interpretation of local malaria vector clusters than the original spatial scan statistic.	sensor	Li Li	2010	2010 18th International Conference on Geoinformatics	10.1109/GEOINFORMATICS.2010.5567801	pattern recognition;data mining;mathematics;statistics	EDA	18.439353236914723	-61.765241754661844	126500
8b0a16b2475798ed1b5f5cffed62a47e1ef30c7d	local interpretable model-agnostic explanations for music content analysis		The interpretability of a machine learning model is essential for gaining insight into model behaviour. While some machine learning models (e.g., decision trees) are transparent, the majority of models used today are still black-boxes. Recent work in machine learning aims to analyse these models by explaining the basis of their decisions. In this work, we extend one such technique, called local interpretable model-agnostic explanations, to music content analysis. We propose three versions of explanations: one version is based on temporal segmentation, and the other two are based on frequency and time-frequency segmentation. These explanations provide meaningful ways to understand the factors that influence the classification of specific input data. We apply our proposed methods to three singing voice detection systems: the first two are designed using decision tree and random forest classifiers, respectively; the third system is based on convolutional neural network. The explanations we generate provide insights into the model behaviour. We use these insights to demonstrate that despite achieving 71.4% classification accuracy, the decision tree model fails to generalise. We also demonstrate that the model-agnostic explanations for the neural network model agree in many cases with the model-dependent saliency maps. The experimental code and results are available online. 1	artificial neural network;black box;convolutional neural network;decision tree model;machine learning;map;network model;random forest	Saumitra Mishra;Bob L. Sturm;Simon Dixon	2017			machine learning;artificial intelligence;computer science;content analysis	ML	21.10492201178515	-54.167495797219466	126537
9605f3ec84b3616458a7718320f6501735da9ac2	"""r-group template comfa combines benefits of """"ad hoc"""" and topomer alignments using 3d-qsar for lead optimization"""	3d-qsar;topomers;selwood;template comfa;ligand alignment	Template CoMFA methodologies extend topomer CoMFA by allowing user-designated templates, for example the experimental receptor-bound conformation of a prototypical ligand, to help determine the alignment of training and test set structures for 3D-QSAR. The algorithms that generate its new structural modality, template-constrained topomers, are described. Template CoMFA's resolution of certain topomer CoMFA concerns, by providing user control of topological consistency and structural acceptability, is demonstrated for sixteen 3D-QSAR training sets, in particular the Selwood dataset.		Richard D. Cramer	2012	Journal of computer-aided molecular design	10.1007/s10822-012-9583-9	bioinformatics;machine learning;data mining	EDA	10.55129840697751	-58.926626544872946	126697
748db2b932a84894e4613dab61b6cedfd9bbed6f	factors affecting accuracy in image translation based on generative adversarial network		With the development of deep learning, image translation has made it possible to output more realistic and highly accurate images. Especially, with the advent of Generative Adversarial Network (GAN), it became possible to perform general purpose learning in various image translation tasks such as “drawings to paintings”, “male to female” and “day to night”. In recent works, several models have been proposed that can do unsupervised learning which does not require an explicit pair of source domain image and target domain image, which is conventionally required for image translation. Two models called “CycleGAN” and “DiscoGAN” have appeared as state-of-the-art models in unsupervised learning-based image translation and succeeded in creating more realistic and highly accurate images. These models share the same network architecture, although there are differences in detailed parameter settings and learning algorithms. (in this paper we will collectively refer to them as “learning techniques”) Both models can do similar translation tasks, but it turned out that there is a large difference in translation accuracy between particular image domains. In this study, we analyzed differences in learning techniques of these models and investigated which learning techniques affect translation accuracy. As a result, it was found that the difference in the size of the feature map, which is the input for the image creation, affects the accuracy.	algorithm;deep learning;feature model;glossary of computer graphics;machine learning;network architecture;unsupervised learning	Fumiya Yamashita;Ryohei Orihara;Yuichi Sei;Yasuyuki Tahara;Akihiko Ohsuga	2018		10.5220/0006591204460453	adversarial system;machine learning;artificial intelligence;computer science;generative grammar;image translation	Vision	22.73489930434234	-53.229240880748925	126728
55fd2d967945624a5457f204b9cecbbc1ed54ad2	combining brain computer interfaces with vision for object categorization	information resources;image recognition;kernel;human computer interaction;performance evaluation;brain computer interface;electroencephalograph device;object categorization;image classification;classification;computer vision;discriminative visual category recognition system;cognitive process;visualization;human aided computing;user interfaces computer vision electroencephalography human computer interaction image classification image recognition sensor fusion;feature extraction;fast convex kernel alignment algorithm;information fusion method brain computer interfaces human aided computing computer vision based processing object categorization systems electroencephalograph device subconscious cognitive processing discriminative visual category recognition system pyramid match kernel fast convex kernel alignment algorithm classification;classification algorithms;stability analysis;information fusion method;humans;information fusion;brain computer interfaces;computer vision based processing;electroencephalography;sensor fusion;classification accuracy;subconscious cognitive processing;object categorization systems;information analysis;user interfaces;human brain;labeling;pyramid match kernel;brain computer interfaces humans electroencephalography kernel performance evaluation computer vision image recognition information resources information analysis stability analysis	Human-aided computing proposes using information measured directly from the human brain in order to perform useful tasks. In this paper, we extend this idea by fusing computer vision-based processing and processing done by the human brain in order to build more effective object categorization systems. Specifically, we use an electroencephalograph (EEG) device to measure the subconscious cognitive processing that occurs in the brain as users see images, even when they are not trying to explicitly classify them. We present a novel framework that combines a discriminative visual category recognition system based on the pyramid match kernel (PMK) with information derived from EEG measurements as users view images. We propose a fast convex kernel alignment algorithm to effectively combine the two sources of information. Our approach is validated with experiments using real-world data, where we show significant gains in classification accuracy. We analyze the properties of this information fusion method by examining the relative contributions of the two modalities, the errors arising from each source, and the stability of the combination in repeated experiments.	algorithm;categorization;cognition;computer vision;electroencephalography;experiment;ieee 802.11i-2004;kernel (operating system)	Ashish Kapoor;Pradeep Shenoy;Desney S. Tan	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2008.4587618	statistical classification;brain–computer interface;computer vision;computer science;machine learning;pattern recognition	Vision	22.92492904160104	-60.25589114418215	126874
13a1cff164736fc6fccaca6da423781ce0efc0bc	optimizing support vector machine based classification and retrieval of semantic video events with genetic algorithms	kernel;multimedia classification tasks;support vector machines;training;video retrieval operating system kernels search problems support vector machines;semantics;search method;video retrieval;optimization event classification support vector machine genetic algorithms;sports video;video event classification;video indexing;event classification;heuristic based techniques;feature extraction;feature subset selection;semantic video events retrieval;genetic algorithm;genetic algorithms;optimization;search problems;support vector machine;operating system kernels;kernel based methods;heuristic based techniques support vector machine semantic video events retrieval genetic algorithms video event classification video indexing video retrieval kernel based methods multimedia classification tasks feature subset selection;support vector machines kernel training semantics gallium optimization feature extraction;gallium	Building accurate models for video event classification is an important research issue since they are essential components for effective video indexing and retrieval. Recently kernel-based methods, particularly support vector machines, have become popular in multimedia classification tasks. However, in order to use them effectively, several factors that hinder accurate classification results, such as feature subset selection and selection of the SVM kernel parameters, must be addressed through the use of heuristic-based techniques. We present a new approach to enhance the performance of SVM for video events classification based on a search method. The latter relies on the simultaneous optimization of the feature and instance subset and SVM kernel parameters, with genetic algorithms. Classification results on sport videos show the significant improvement over conventional SVM.	content-based image retrieval;feature selection;genetic algorithm;heuristic;kernel (operating system);mathematical optimization;optimizing compiler;search engine optimization;software release life cycle;support vector machine	Bashar Tahayna;Mohammed Belkhatir;Saadat M. Alhashmi;Thomas O'Daniel	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5653724	support vector machine;genetic algorithm;computer science;machine learning;pattern recognition;data mining;semantics	Vision	18.064100120918773	-56.65015497712853	126912
2ab96405dbfd4059a9e105769cd8b06f5f06a609	dna computing model of the integer linear programming problem based on molecular beacon	molecular beacon;informing science;chip;dna computing;integer linear program;np complete problem	Biological chip technology and DNA computing are new research areas in biology science and information science separately. The essential characteristic of both is the massive parallel of obtaining and managing information. The integer linear programming problem is an important problem in opsearch and it is an NP-complete problem. But up to now, there does not exist any good algorithm yet. A new DNA computing model is provided to solve a integer linear programming problem based on Molecular Beacon chip. In the method, the integer linear programming problem is solved with molecular beacon by fluorescing upon hybridization to their complementary DNA targets. The method has some significant advantages such as simple encoding, excellent sensitivity, high selectivity, low cost, low error, short operating time, reusable surface and simple experimental steps. The result suggest s the potential of Molecular Beacon used as a DNA computer chip.	dna computing;linear programming	Zhixiang Yin;Jianzhong Cui;Jin Yang;Jin Xu	2006		10.1007/11816102_26	chip;np-complete;integer programming;molecular beacon;computer science;bioinformatics;theoretical computer science;dna computing;algorithm	EDA	12.099730484284128	-53.49114994147136	127507
6e173ad91b288418c290aa8891193873933423b3	can you tell where in india i am from? comparing humans and computers on fine-grained race face classification		We make a rich variety of judgments on faces but the underlying features are poorly understood. While coarse categories such as race or gender are appealing to study, they produce large changes across many features, making it difficult to identify the underlying features used by humans. Moreover, the high accuracy of both humans and machines on these tasks rules out any systematic error analysis. Here we propose, demonstrate and benchmark a novel dataset for understanding human face recognition that overcomes these limitations. The dataset consists of 1647 diverse faces from India labeled with their fine-grained race (North vs South India) as well as classification performance of 129 human subjects on these faces. Our main finding is that, while many machine algorithms achieved an overall performance comparable to humans (64%), their error patterns across faces were qualitatively different and remained so even when explicitly trained to predict human performance. To elucidate the features used by humans, we trained linear classifiers on overcomplete sets of features derived from each face part. This indicated that mouth shape to be the most discriminative part compared to eyes, nose or the external contour. To confirm this prediction, we performed an additional behavioral experiment on humans by occluding various shape parts. Occluding the mouth impaired race classification in humans the most compared to occluding any other face part. Taken together, our results show that studying hard classification tasks can lead to useful insights into both machine and human vision.	algorithm;benchmark (computing);computer;error analysis (mathematics);facial recognition system;human reliability;linear classifier	Harish Katti;Sripati P Arun	2017	CoRR		computer vision;artificial intelligence;machine learning;pattern recognition	ML	22.480233954541536	-53.66551979251344	127859
89aa81d0ab74b5543c6549a5c571d30142fc4b9a	analyzing the topology of active sites: on the prediction of pockets and subpockets	processus gauss;topology;evaluation performance;proteine;performance evaluation;image processing;analyse fonctionnelle;evaluacion prestacion;topologie;coverage rate;procesamiento imagen;bioinformatique;grado recubrimiento;traitement image;topologia;large scale;lugar activo;functional analysis;proteina;gaussian process;bioinformatica;escala grande;proceso gauss;protein;site actif;degre recouvrement;active site;bioinformatics;echelle grande;analisis funcional	Automated prediction of protein active sites is essential for large-scale protein function prediction, classification, and druggability estimates. In this work, we present DoGSite, a new structure-based method to predict active sites in proteins based on a Difference of Gaussian (DoG) approach which originates from image processing. In contrast to existing methods, DoGSite splits predicted pockets into subpockets, revealing a refined description of the topology of active sites. DoGSite correctly predicts binding pockets for over 92% of the PDBBind and the scPDB data set, being in line with the best-performing methods available. In 63% of the PDBBind data set the detected pockets can be subdivided into smaller subpockets. The cocrystallized ligand is contained in exactly one subpocket in 87% of the predictions. Furthermore, we introduce a more precise prediction performance measure by taking the pairwise ligand and pocket coverage into account. In 90% of the cases DoGSite predicts a pocket that contains at least half of the ligand. In 70% of the cases additionally more than a quarter of the respective pocket itself is covered by the cocrystallized ligand. Consideration of subpockets produces an increase in coverage yielding a success rate of 83% for the latter measure.		Andrea Volkamer;Axel Griewel;Thomas Grombacher;Matthias Rarey	2010	Journal of chemical information and modeling	10.1021/ci100241y	functional analysis;simulation;image processing;bioinformatics;artificial intelligence;active site;gaussian process	ML	11.711252919373567	-55.9739552312203	128058
ed5d7d2c0a23cc7f47253033154b7cd260fa9d6a	a randomized kinematics-based approach to pharmacophore-constrained conformational search and database screening	pharmacophore;drug design;protein ligand docking	Computational tools have greatly expedited the pharmaceutical drug design process in recent years. One common task in this process is the search of a large library for small molecules that can achieve both a low-energy conformation and a prescribed pharmacophore. The pharmacophore expresses constraints on the 3D structure of the molecule by specifying relative atom positions that should be maintained to increase the likelihood that the molecule will bind with the receptor site. This article presents a pharmacophore-based database screening system that has been designed, implemented, and tested on a molecular database. The primary focus of this article is on a simple, randomized conformational search technique that attempts to simultaneously reduce energy and maintain pharmacophore constraints. This enables the identification of molecules in a database that are likely to dock with a given protein, which can serve as a powerful aid in the search for better drug candidates. c © 2000 John Wiley & Sons, Inc. J Comput Chem 21: 731–747, 2000 Correspondence to: Steven M. LaValle; e-mail: lavalle@cs. iastate.edu ∗Current address: Prolifix Ltd., 91 Milton Park, Abingdon, Oxfordshire, UK Contract/grant sponsor: NSF CAREER Award (to S.L.); contract/grant number: IRI-9875304 Contract/grant sponsor: NSF CAREER Award (to L.K.); contract/grant number: IRI-970228, SA1728-21122NM, and ATP 003604-0120-1999 Contract/grant sponsor: ARO MURI (to J.-C.L.); contract/ grant number: DAAH04-96-1-007 Journal of Computational Chemistry, Vol. 21, No. 9, 731–747 (2000) c © 2000 John Wiley & Sons, Inc.	database;email;ibm notes;john d. wiley;journal of computational chemistry;pharmacophore;randomized algorithm;randomized rounding	Steven M. LaValle;Paul W. Finn;Lydia E. Kavraki;Jean-Claude Latombe	2000	Journal of Computational Chemistry	10.1002/(SICI)1096-987X(20000715)21:9%3C731::AID-JCC3%3E3.0.CO;2-R	stereochemistry;chemistry;pharmacophore;virtual screening;combinatorial chemistry;protein–ligand docking;drug design;ligandscout	Comp.	12.407981853829842	-60.704885618231145	128702
e5c57aa934dbccb8977b3e0e6395baa364c953ad	weakly supervised convolutional lstm approach for tool tracking in laparoscopic videos		Purpose: Real-time surgical tool tracking is a core component of the future intelligent operating room (OR), because it is highly instrumental to analyze and understand the surgical activities. Current methods for surgical tool tracking in videos need to be trained on data in which the spatial position of the tools is manually annotated. Generating such training data is difficult and time-consuming. Instead, we propose to use solely binary presence annotations to train a tool tracker for laparoscopic videos. Methods: The proposed approach is composed of a CNN + Convolutional LSTM (ConvLSTM ) neural network trained end-to-end, but weakly supervised on tool binary presence labels only. We use the ConvLSTM to model the temporal dependencies in the motion of the surgical tools and leverage its spatio-temporal ability to smooth the class peak activations in the localization heat maps (Lh-maps). Results: We build a baseline tracker on top of the CNN model and demonstrate that our approach based on the ConvLSTM outperforms the baseline in tool presence detection, spatial localization, and motion tracking by over 5.0%, 13.9%, and 12.6%, respectively. Conclusions: In this paper, we demonstrate that binary presence labels are sufficient for training a deep learning tracking model using our proposed method. We also show that the ConvLSTM can leverage the spatio-temporal coherence of consecutive image frames across a surgical video to improve tool presence detection, spatial localization, and motion tracking.		Chinedu Innocent Nwoye;Didier Mutter;Jacques Marescaux;Nicolas Padoy	2018	CoRR			AI	22.620892833721545	-57.03071634084489	128772
a74c9cb461e3a8580e04beaf27aca62608622536	when clutter reduction meets machine learning for people counting using ir-uwb radar		People counting provides key information in sensing applications. Impulse radio ultra-wideband (IR-UWB) radar, which has strong penetration and high-range resolution, has been extensively applied to detect and count people. Current signal processing methods that rely on IR-UWB radar require to establish an environment-dependent threshold manually. Due to the high sensitivity of the IR-UWB radar, the wide diversity of scattered waveforms would bring false alarms. Clutter reduction serves a vital role in signal processing steps to obtain the signal reflected only from the target, while it may also eliminate significant information. In this paper, data-driven solutions based on two machine learning algorithms, the random forest and convolutional neural network (CNN), are proposed to address the challenge of counting people with complex changing scatters. These data-driven methods learn from selected features from radar signals or directly obtain features from radar data and analyze them to automatically produce results. A series of experiments are conducted in the Orange and Caffe platform, and the results indicate that: (i) In data-driven solutions, clutter reduction methods are harmful rather than beneficial for data analysis, verified by discussing four representative clutter reduction methods. (ii) Random forest classification for selected time-domain features in radar signals before complex clutter reduction reaches 91.5% accuracy in testing environment. (iii) CNN provides an automatic counting solution learning directly from radar data.	algorithm;artificial neural network;clutter;convolutional neural network;experiment;long short-term memory;machine learning;network model;orange;people counter;radar;random forest;recurrent neural network;signal processing;ultra-wideband	Xiuzhu Yang;Lin Zhang	2017		10.1007/978-3-319-65482-9_52	convolutional neural network;computer science;random forest;clutter;radar;signal processing;waveform;computer vision;machine learning;artificial intelligence	AI	21.273129314957767	-59.253811490585136	129137
0cf0d55536d38592fab815acf0af327c9181a542	accurmsd: a machine learning approach to predicting structure similarity of docked protein complexes	neural networks;machine learning;scoring functions;protein docking and refinement;rmsd prediction	Protein-protein docking methods aim to compute the correct bound form of two or more proteins. One of the major challenges for docking methods is to accurately discriminate native-like structures. The protein docking community agrees on the existence of a relationship between various favorable intermolecular interactions (e.g. Van der Waals, electrostatic, desolvation forces, etc.) and the similarity of a conformation to its native structure. Different docking algorithms often formulate this relationship as a weighted sum of selected terms and calibrate their weights against a specific training data to evaluate and rank candidate structures. However, the exact form of this relationship is unknown and the accuracy of such methods is impaired by the pervasiveness of false positives.  Unlike the conventional scoring functions, we propose a novel machine learning approach that not only ranks the candidate structures relative to each other but also indicates how similar each candidate is to the native conformation. We trained the AccuRMSD neural network with an extensive dataset using the back-propagation learning algorithm and achieved RMSD prediction accuracy with less than 1Å error margin on 19,600 test samples.	algorithm;artificial neural network;backpropagation;docking (molecular);interaction;machine learning;macromolecular docking;protein structure prediction;scoring functions for docking;software propagation;weight function	Bahar Akbal-Delibas;Marc Pomplun;Nurit Haspel	2014		10.1145/2649387.2649392	computer science;bioinformatics;machine learning;data mining;artificial neural network	ML	10.4480473620608	-56.36746927410609	130058
1b0f5791124672a246fd4374592c362864c8bbf7	is endothelial nitric oxide synthase a moonlighting protein whose day job is cholesterol sulfate synthesis? implications for cholesterol transport, diabetes and cardiovascular disease	glycosaminoglycans;heparan sulfate;diabetes;autophagy;cardiovascular disease;lysosomes;hydrogen sulfide;nitric oxide;article;cholesterol sulfate;endothelial nitric oxide synthase	Theoretical inferences, based on biophysical, biochemical, and biosemiotic considerations, are related here to the pathogenesis of cardiovascular disease, diabetes, and other degenerative conditions. We suggest that the “daytime” job of endothelial nitric oxide synthase (eNOS), when sunlight is available, is to catalyze sulfate production. There is a striking alignment between cell types that produce either cholesterol sulfate or sulfated polysaccharides and those that contain eNOS. The signaling gas, nitric oxide, a wellknown product of eNOS, produces pathological effects not shared by hydrogen sulfide, a sulfur-based signaling gas. We propose that sulfate plays an essential role in HDL-A1 cholesterol trafficking and in sulfation of heparan sulfate proteoglycans (HSPGs), both critical to lysosomal recycling (or disposal) of cellular debris. HSPGs are also crucial in glucose metabolism, protecting against diabetes, and in maintaining blood colloidal suspension and capillary flow, through systems dependent on water-structuring properties of sulfate, an anionic kosmotrope. When sunlight exposure is insufficient, lipids accumulate in the atheroma in order to supply cholesterol and sulfate to the heart, using a OPEN ACCESS Entropy 2012, 14 2493 process that depends upon inflammation. The inevitable conclusion is that dietary sulfur and adequate sunlight can help prevent heart disease, diabetes, and other disease conditions.	angular defect;cmos;hydrogen;resource leak	Stephanie Seneff;Ann Lauritzen;Robert M. Davidson;Laurie Lentz-Marino	2012	Entropy	10.3390/e14122492	nitric oxide;autophagy;glycosaminoglycan	ML	10.572543552765616	-64.67395835216965	130646
1e1a67a78badc619b2f9938e4a03922dcbee0fb6	food/non-food image classification and food categorization using pre-trained googlenet model	food non food classification;googlenet;food recognition;convolutional neural network cnn;deep learning;caffe	Recent past has seen a lot of developments in the field of image-based dietary assessment. Food image classification and recognition are crucial steps for dietary assessment. In the last couple of years, advancements in the deep learning and convolutional neural networks proved to be a boon for the image classification and recognition tasks, specifically for food recognition because of the wide variety of food items. In this paper, we report experiments on food/non-food classification and food recognition using a GoogLeNet model based on deep convolutional neural network. The experiments were conducted on two image datasets created by our own, where the images were collected from existing image datasets, social media, and imaging devices such as smart phone and wearable cameras. Experimental results show a high accuracy of 99.2% on the food/non-food classification and 83.6% on the food category recognition.	artificial neural network;categorization;computer vision;convolutional neural network;deep learning;experiment;smartphone;social media;wearable computer	Ashutosh Singla;Lin Yuan;Touradj Ebrahimi	2016		10.1145/2986035.2986039	computer science;artificial intelligence;machine learning;communication	AI	24.366153970706204	-59.167501234271946	130651
19cab312a27bf508d94c02197c9d4919d93469ba	predictive qsar modeling of substituted phenylpyrazinones as corticotropin-releasing factor-1 (crf1) receptor antagonists: computational approach		A QSAR study has been performed on a series of Phenylpyrazinones derivatives with potent corticotropin-releasing factor-1 (CRF1) receptor antagonists. Structural features responsible for the activity of the compounds were characterized by using physicochemical, topological, and electrotopological descriptors, calculated from the Molecular Design Suite software. The statistically significant 2D-QSAR model having r 2 = 0.8141 and q 2 = 0.7391 with pred_r 2 = 0.7827 was developed by partial least squares method. Results reveal that the 2D-QSAR studies signify positive contribution of SsOHcount and SsCH3 count toward the biological activity, whereas negative contribution of 1PathCoun will be in favor of higher CRF1 activity. The QSAR model indicated that the T_2_F_1, T_C_Cl_1 and SaasCE-index played an important role in determining CRF1 receptor antagonists. Their corticotropin-releasing factor 1 capacity can be increased by number and position of the chlorine group. These correlations will be helpful in the development of Phenylpyrazinones as CRF1 receptor activities with a much more enhanced potency.	cheminformatics;linear least squares (mathematics);partial least squares regression;quantitative structure–activity relationship;requirement	Mukesh C. Sharma	2015	Network Modeling Analysis in Health Informatics and Bioinformatics	10.1007/s13721-015-0100-7	pharmacology;toxicology	Comp.	12.16736505191972	-58.203802612701715	130749
f374055a0b388d07702abd4277a5010c3932c45c	boosting protein threading accuracy	health research;uk clinical guidelines;fold recognition;biological patents;score function;regression tree;europe pubmed central;dynamic programming algorithm;citation search;conditional random fields;protein threading;probabilistic graphical model;uk phd theses thesis;gradient tree boosting;protein structure prediction;life sciences;conditional random field;nonlinear scoring function;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	Protein threading is one of the most successful protein structure prediction methods. Most protein threading methods use a scoring function linearly combining sequence and structure features to measure the quality of a sequence-template alignment so that a dynamic programming algorithm can be used to optimize the scoring function. However, a linear scoring function cannot fully exploit interdependency among features and thus, limits alignment accuracy.This paper presents a nonlinear scoring function for protein threading, which not only can model interactions among different protein features, but also can be efficiently optimized using a dynamic programming algorithm. We achieve this by modeling the threading problem using a probabilistic graphical model Conditional Random Fields (CRF) and training the model using the gradient tree boosting algorithm. The resultant model is a nonlinear scoring function consisting of a collection of regression trees. Each regression tree models a type of nonlinear relationship among sequence and structure features. Experimental results indicate that this new threading model can effectively leverage weak biological signals and improve both alignment accuracy and fold recognition rate greatly.	algorithm;alignment;clinical use template;conditional random field;decision tree learning;dynamic programming;gradient boosting;graphical model;interaction;interdependence;linear iga bullous dermatosis;nonlinear system;protein structure prediction;resultant;score;scoring functions for docking;thread (computing);threaded code;threading (protein sequence);trees (plant)	Jian Peng;Jinbo Xu	2009	Research in computational molecular biology : ... Annual International Conference, RECOMB ... : proceedings. RECOMB	10.1007/978-3-642-02008-7_3	computer science;bioinformatics;data science;machine learning;data mining;conditional random field;statistics	Comp.	10.328306525095506	-55.019467665883525	130848
4b7dd1de66d52ddc7748b22698fdac4543bcaf15	statistical framework for uncertainty quantification in computational molecular modeling	uncertainty quantification;molecular modeling;sampling	Computational molecular modeling often involves noisy data including uncertainties in model parameters, computational approximations etc., all of which propagates to uncertainties in all computed quantities of interest (QOI). This is a fundamental problem that is often left ignored or treated without sufficient rigor. In this article, we introduce a statistical framework for modeling such uncertainties and providing certificates of accuracy for several QOI. Our framework treats sources of uncertainty as random variables with known distributions, and provides both a theoretical and an empirical technique for propagating those uncertainties to the QOI, also modeled as a random variable. Moreover, the framework also enables one to model uncertainties in a multi-step pipeline, where the outcome of one step cascades into the next. While there are many sources of uncertainty, in this article we have applied our framework to only positional uncertainties of atoms in high resolution models, and in the form of B-factors and their effect in computed molecular properties. The empirical approach requires sufficiently sampling over the joint space of the random variables. We show that using novel pseudo-random number generation techniques, it is possible to achieve the required coverage using very few samples. We have also developed intuitive visualization models to analyze uncertainties at different stages of molecular modeling. We strongly believe this framework would be immensely valuable in evaluating predicted computational models, and provide statistical guarantees on their accuracy.	approximation;certificate (record artifact);computation;computational model;image resolution;numerous;pseudo brand of pseudoephedrine;pseudorandomness;quantitation;quantity;random number generation;rigor - temperature-associated observation;sampling (signal processing);signal-to-noise ratio;temporomandibular joint disorders;uncertainty quantification;molecular modeling	Muhibur Rasheed;Nathan Clement;Abhishek Bhowmick;Chandrajit L. Bajaj	2016	ACM-BCB ... ... : the ... ACM Conference on Bioinformatics, Computational Biology and Biomedicine. ACM Conference on Bioinformatics, Computational Biology and Biomedicine	10.1145/2975167.2975182	sampling;econometrics;uncertainty quantification;computer science;molecular model;data mining;statistics	Comp.	15.83312950863713	-61.31741694033089	130978
eb2fd1b41482fbdd6ac257f5a4af34fa35945435	evaluating ideal combinations of necktie and y-shirt by self-organization map for the coordination system	image processing;self organization map;psychology;image data;humans psychology conferences proposals decision support systems data visualization internet;internet;self organising feature maps;visual databases image processing self organising feature maps;decision support systems;necktie y shirt combinations;data visualization;self organized map;humans;coordination contrast;proposals;coordination system;conferences;coordinate system;coordination contrast necktie y shirt combinations self organization map coordination system image data;visual databases	We investigated the impression of externals that Y shirt and the necktie gave the person. The data set to compose the self-organizing map of the result has been extracted. In this paper, we tested the proposing the coordination system that considers relativity about the image data and impressions to the coordination contrast.	organizing (structure);self-organization;self-organizing map	Yukinobu Hoshino	2010	International Conference on Fuzzy Systems	10.1109/FUZZY.2010.5584740	computer vision;the internet;image processing;computer science;artificial intelligence;coordinate system;machine learning;data mining;data visualization	Robotics	24.11807297214176	-63.7386963412601	131009
45619c131ebbf137c957a2e645da09e6aa2313da	fipsdock: a new molecular docking technique driven by fully informed swarm optimization algorithm	cross docking;conformational sampling;docking accuracy;protein ligand binding;flexible docking;article	The accurate prediction of protein-ligand binding is of great importance for rational drug design. We present herein a novel docking algorithm called as FIPSDock, which implements a variant of the Fully Informed Particle Swarm (FIPS) optimization method and adopts the newly developed energy function of AutoDock 4.20 suite for solving flexible protein-ligand docking problems. The search ability and docking accuracy of FIPSDock were first evaluated by multiple cognate docking experiments. In a benchmarking test for 77 protein/ligand complex structures derived from GOLD benchmark set, FIPSDock has obtained a successful predicting rate of 93.5% and outperformed a few docking programs including particle swarm optimization (PSO)@AutoDock, SODOCK, AutoDock, DOCK, Glide, GOLD, FlexX, Surflex, and MolDock. More importantly, FIPSDock was evaluated against PSO@AutoDock, SODOCK, and AutoDock 4.20 suite by cross-docking experiments of 74 protein-ligand complexes among eight protein targets (CDK2, ESR1, F2, MAPK14, MMP8, MMP13, PDE4B, and PDE5A) derived from Sutherland-crossdock-set. Remarkably, FIPSDock is superior to PSO@AutoDock, SODOCK, and AutoDock in seven out of eight cross-docking experiments. The results reveal that FIPS algorithm might be more suitable than the conventional genetic algorithm-based algorithms in dealing with highly flexible docking problems.		Yu Liu;Lei Zhao;Wentao Li;Dongyu Zhao;Miao Song;Yongliang Yang	2013	Journal of computational chemistry	10.1002/jcc.23108	lead finder;searching the conformational space for docking;docking;protein–ligand docking	Comp.	11.698901314961288	-60.24007499757836	131031
2ab4a929320e96b45542085b14f2b8f92e28db57	place cells and spatial navigation based on 2d visual feature extraction, path integration, and reinforcement learning	path integral;hebbian learning;goal orientation;mobile robot;reinforcement learning;gabor filter;place cell;visual features;head direction cell;spatial navigation	We model hippocampal place cells and head-direction cells by combining allothetic (visual) and idiothetic (proprioceptive) stimuli. Visual input, provided by a video camera on a miniature robot, is preprocessed by a set of Gabor filters on 31 nodes of a log-polar retinotopic graph. Unsupervised Hebbian learning is employed to incrementally build a population of localized overlapping place fields. Place cells serve as basis functions for reinforcement learning. Experimental results for goal-oriented navigation of a mobile robot are presented.	basis function;feature extraction;gabor filter;hebbian theory;microbotics;mobile robot;reinforcement learning;spatial navigation	Angelo Arleo;Fabrizio Smeraldi;Stéphane Hug;Wulfram Gerstner	2000			mobile robot;robot learning;spatial memory;computer vision;path integral formulation;hebbian theory;computer science;machine learning;goal orientation;competitive learning;reinforcement learning	ML	21.083617065129776	-65.39141317838423	131233
ab6d1b74a70b08681558dd3ab151479128fb83d3	robust classification of subcellular location patterns in fluorescence microscope images	biology computing;fluorescence;image resolution;neural nets;images interpretation subcellular location patterns fluorescence microscope images robust classification cell type protein location determination improved numeric features major subcellular patterns subcellular location trees genome wide determination neural networks training systematic framework;image classification;robustness fluorescence microscopy proteins biotechnology genomics bioinformatics spatial resolution neural networks pattern recognition;proteins;fluorescence microscopy;neural nets image classification fluorescence optical microscopy cellular biophysics biological techniques biology computing proteins image resolution;protein subcellular location;biological techniques;cellular biophysics;optical microscopy;neural network;spatial resolution	The ongoing biotechnology revolution promises a complete understanding of the mechanisms by which cells and tissues carry out their functions. Central to that goal is the determination of the function of each protein that is present in a given cell type, and determining a protein's location within cells is critical to understanding its function. As large amounts of data become available from genome-wide determination of protein subcellular location, automated approaches to categorizing and comparing location patterns are urgently needed. Since subcellular location is most often determined using fluorescence microscopy, we have developed automated systems for interpreting the resulting images. We report here improved numeric features for describing such images that are fairly robust to image intensity binning and spatial resolution. We validate these features by using them to train neural networks that accurately recognize all major subcellular patterns with an accuracy higher than previously reported. Having validated the features by using them for classification, we also demonstrate using them to create Subcellular Location Trees that group similar proteins and provide a systematic framework for describing subcellular location.	artificial neural network;categorization;feature extraction;product binning	Robert F. Murphy;Meel Velliste;Gregory Porreca	2002		10.1109/NNSP.2002.1030018	biology;cell biology;bioinformatics;analytical chemistry	ML	10.377938207925292	-55.14842434230252	131268
193ff8bddb04a75e02286ab3ec4d376bbef912e1	abstraction and generalization of 3d structure for recognition in large intra-class variation	canonical model;cognitive science;rule based system;shape matching;classification accuracy;3d structure	ion and Generalization of 3D structure for recognition in large intra-class variation Gowri Somanath and Chandra Kambhamettu Video/Image Modeling and Synthesis (VIMS) Lab, Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA. http://vims.cis.udel.edu Abstract. Humans have abstract models for object classes which helps recognize previously unseen instances, despite large intra-class variations. Also objects are grouped into classes based on their purpose. Studies in cognitive science show that humans maintain abstractions and certain specific features from the instances they observe. In this paper, we address the challenging task of creating a system which can learn such canonical models in a uniform manner for different classes. Using just a few examples the system creates a canonical model (COMPAS) per class, which is used to recognize classes with large intra-class variation (chairs, benches, sofas all belong to sitting class). We propose a robust representation and automatic scheme for abstraction and generalization. We quantitatively demonstrate improved recognition and classification accuracy over state-of-art 3D shape matching/classification method and discuss advantages over rule based systems. Humans have abstract models for object classes which helps recognize previously unseen instances, despite large intra-class variations. Also objects are grouped into classes based on their purpose. Studies in cognitive science show that humans maintain abstractions and certain specific features from the instances they observe. In this paper, we address the challenging task of creating a system which can learn such canonical models in a uniform manner for different classes. Using just a few examples the system creates a canonical model (COMPAS) per class, which is used to recognize classes with large intra-class variation (chairs, benches, sofas all belong to sitting class). We propose a robust representation and automatic scheme for abstraction and generalization. We quantitatively demonstrate improved recognition and classification accuracy over state-of-art 3D shape matching/classification method and discuss advantages over rule based systems.	canonical model;cognitive science;humans;information and computer science	Gowri Somanath;Chandra Kambhamettu	2010		10.1007/978-3-642-19318-7_38	rule-based system;computer vision;computer science;artificial intelligence;machine learning;canonical model;mathematics;algorithm;statistics	Vision	22.268525670610998	-61.36309140024347	131800
62f8dd36638c1becdac2ba9f746082db06239346	composition-induced structural transitions in mixed lennard-jones clusters: global reparametrization and optimization	evolutionary computation;mixed lennard jones clusters;lennard jones;genetic algorithms;reparametrization;global cluster structure optimization	As extended benchmarks to global cluster structure optimization methods, we provide a first systematic point of entry into the world of strongly mixed rare gas clusters. A new set of generalized Lennard-Jones pair potentials is generated for this purpose, by fitting them to high-end ab initio reference data. Employing these potentials in our genetic algorithm-based global structure optimization framework, we examined various systems from binary to quinary mixtures of atom types. A central result from this study is that the famous fcc structure for 38 atoms can survive for certain binary mixtures but appears to be prone to collapsing into the dominating icosahedral structure, which we observed upon introduction of one single atom of a ternary type.		Johannes M. Dieterich;Bernd Hartke	2011	Journal of computational chemistry	10.1002/jcc.21721	combinatorics;genetic algorithm;chemistry;lennard-jones potential;computational chemistry;mathematics;quantum mechanics;evolutionary computation	Theory	15.698914159092174	-55.615918814296776	132124
4f17b0a6794c5cb241545ad316ad4d00a2d6c2b6	thoughtviz: visualizing human thoughts using generative adversarial network		Studying human brain signals has always gathered great attention from the scientific community. In Brain Computer Interface (BCI) research, for example, changes of brain signals in relation to specific tasks (e.g., thinking something) are detected and used to control machines. While extracting spatio-temporal cues from brain signals for classifying state of human mind is an explored path, decoding and visualizing brain states is new and futuristic. Following this latter direction, in this paper, we propose an approach that is able not only to read the mind, but also to decode and visualize human thoughts. More specifically, we analyze brain activity, recorded by an ElectroEncephaloGram (EEG), of a subject while thinking about a digit, character or an object and synthesize visually the thought item. To accomplish this, we leverage the recent progress of adversarial learning by devising a conditional Generative Adversarial Network (GAN), which takes, as input, encoded EEG signals and generates corresponding images. In addition, since collecting large EEG signals in not trivial, our GAN model allows for learning distributions with limited training data. Performance analysis carried out on three different datasets -- brain signals of multiple subjects thinking digits, characters, and objects -- show that our approach is able to effectively generate images from thoughts of a person. They also demonstrate that EEG signals encode explicitly cues from thoughts which can be effectively used for generating semantically relevant visualizations.	brain–computer interface;encode;electroencephalography;experiment;information;mind;streaming media	E Morere;Yogesh Singh Rawat;Concetto Spampinato;Mubarak Shah	2018		10.1145/3240508.3240641	speech recognition;encode;computer vision;brain–computer interface;generative grammar;brain activity and meditation;decoding methods;artificial intelligence;adversarial system;human brain;electroencephalography;computer science	ML	19.921760780548812	-54.45858142781634	132494
25e3f72b4438a2a130d33c5af743f9e896eff79f	spectrophores as one-dimensional descriptors calculated from three-dimensional atomic properties: applications ranging from scaffold hopping to multi-target virtual screening	acetylcholinesterase;affinity fingerprinting;artificial cage;compound similarity;descriptor;fingerprint;pharmacophore;qsar;scaffold hopping;spectrophore;tanimoto;thrombin;virtual screening	Spectrophores are novel descriptors that are calculated from the three-dimensional atomic properties of molecules. In our current implementation, the atomic properties that were used to calculate spectrophores include atomic partial charges, atomic lipophilicity indices, atomic shape deviations and atomic softness properties. This approach can easily be widened to also include additional atomic properties. Our novel methodology finds its roots in the experimental affinity fingerprinting technology developed in the 1990's by Terrapin Technologies. Here we have translated it into a purely virtual approach using artificial affinity cages and a simplified metric to calculate the interaction between these cages and the atomic properties. A typical spectrophore consists of a vector of 48 real numbers. This makes it highly suitable for the calculation of a wide range of similarity measures for use in virtual screening and for the investigation of quantitative structure-activity relationships in combination with advanced statistical approaches such as self-organizing maps, support vector machines and neural networks. In our present report we demonstrate the applicability of our novel methodology for scaffold hopping as well as virtual screening.	artificial neural network;fingerprint (computing);frequency-hopping spread spectrum;neural network simulation;numerous;organizing (structure);partial charge;plant roots;processor affinity;quantitative structure-activity relationship;self-organization;self-organizing map;support vector machine;terrapins;virtual screening;lipophilicity	Rafaela Gladysz;Fábio Mendes dos Santos;Wilfried Langenaeker;Gert Thijs;Koen Augustyns;Hans De Winter	2018		10.1186/s13321-018-0268-9	computational chemistry;support vector machine;virtual screening;bioinformatics;computer science;quantitative structure–activity relationship;scaffold;partial charge;ranging;pharmacophore	Comp.	12.806259446532538	-58.443262495844934	132645
637708443a2257d2b6fb85f8c0cc040278f1fb8d	auto coder-decoder (codec) model based sparse representation for image super resolution		In our daily life, the high quality image is widely used in varieties of fields, but sometimes we cannot capture the image with idea resolution due to some influences. For solving the resolution limitation of imaging sensors, the image super resolution (SR) representation technology is widely researched. Considering the advantage of sparse representation, the dictionary learning based methods is widely studied. However, landmark atoms cannot provide the representations of images, since the general feature extractors is universally applicable in feature extraction. To overcome the drawbacks, an auto coder-decoder (CODEC) model is proposed to extract representative features from low resolution (LR) images. The experimental results indicate the proposed method can obtain better effect than other methods.	codec;dictionary;display resolution;feature extraction;image resolution;lr parser;machine learning;sensor;sparse approximation;sparse matrix;super-resolution imaging	Qieshi Zhang;Liyan Gu;Jun Cheng;Xiaojun Wu;Sei-ichiro Kamata	2017	2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)	10.1109/CISP-BMEI.2017.8301950	iterative reconstruction;computer vision;artificial intelligence;pattern recognition;codec;feature extraction;sparse approximation;superresolution;computer science;image resolution	Robotics	24.594032865665223	-54.638983630808035	132659
46f850f85af50cf9638f27a23e3c61602b767814	classification artificial neural systems for genome research	biology computing;chemistry computing;information retrieval;neural nets;genome research;modular databases;molecular sequence databases;neural network classification;protein/nucleic acid classification;search/organization	A neural network classification method has been developed as an alternative approach to the searchl organization problem of large molecular databases. Two artificial neural systems have been implemented on a Cray for rapid protein lnucleic acid classification of unknown sequences. The system employs a n-gram hashing function for sequence encoding and modular back-propagation networks for classification. The protein system has achieved a 82 to 1009o sensitivity at a speed that is about an order of magnitude faster than other search methods. With the rapid accumulation of sequences, the saving in time will become increasingly significant. The pilot nucleic acid system showed a 969Z0 classification accuracy. The software tool would be valuable to the organization of molecular sequence databases and is generally applicable to any databases that are organized according to family relationships.	artificial neural network;backpropagation;hash function;n-gram;programming tool;sequence database;software propagation;tree accumulation	Cathy H. Wu;George M. Whitson;Chun-Tse Hsiao;Cheng-Fu Huang	1992			nucleic acid;hash function;computer science;bioinformatics;machine learning;data mining;artificial neural network	ML	11.088055498873386	-53.65229871098009	132793
c4c3eda89d2161260334204e938beeab7ec5408c	structure-reactivity modeling using mixture-based representation of chemical reactions	chemical reactions;condensed graph of reaction;mixtures;rate constant prediction;reaction fingerprints;simplex representation of molecular structure	"""We describe a novel approach of reaction representation as a combination of two mixtures: a mixture of reactants and a mixture of products. In turn, each mixture can be encoded using an earlier reported approach involving simplex descriptors (SiRMS). The feature vector representing these two mixtures results from either concatenated product and reactant descriptors or the difference between descriptors of products and reactants. This reaction representation doesn't need an explicit labeling of a reaction center. The rigorous """"product-out"""" cross-validation (CV) strategy has been suggested. Unlike the naïve """"reaction-out"""" CV approach based on a random selection of items, the proposed one provides with more realistic estimation of prediction accuracy for reactions resulting in novel products. The new methodology has been applied to model rate constants of E2 reactions. It has been demonstrated that the use of the fragment control domain applicability approach significantly increases prediction accuracy of the models. The models obtained with new """"mixture"""" approach performed better than those required either explicit (Condensed Graph of Reaction) or implicit (reaction fingerprints) reaction center labeling."""		Pavel G. Polishchuk;Timur I. Madzhidov;Timur Gimadiev;Andrey Bodrov;Ramil I. Nugmanov;Alexandre Varnek	2017	Journal of computer-aided molecular design	10.1007/s10822-017-0044-3		Vision	10.925430257518647	-57.28061126098625	132961
81696e80a5b8cb042cff41667519eda8d91d115a	raman spectroscopy analysis based on fourier transform for abo blood group identification		ABO blood type research is not only used for transfusion medicine, but also for the study of some diseases. In this study, principal component analysis is used to take the ABO blood group Raman spectrum of the Fourier transform, in order to improve the ABO blood typing sample recognition rate. When the principal component analysis is performed directly on the Raman spectrum, the differences between the ABO three blood samples can not be well recognized by the score data of the second and the twentieth main components. The Raman spectra of the fluorescence background is extracted from the Raman spectra by Fourier transform, so the imaginary part of the Raman spectrum is obtained. Then, the principal component of the imaginary part signal is analyzed, and fractional graphs of the second principal component and the twentieth principal component are used. In this way, the blood samples of type A, type B and O type are distinguished. The experimental results show that the principal component analysis is carried out by Fourier transform to improve the clustering effect of spectral data, making ABO three blood groups easier to be distinguished. The reason for the enhancement of the clustering effect is that the internal difference of the same kind of spectral data will be reduced and that the difference of the spectral data of different classes will be increased by Fourier transform.	cluster analysis;graph (discrete mathematics);imaginary time;principal component analysis;raman scattering	Haihong Lin;Haotian Yu;Jichun Li;Encai Zhang;Guannan Chen	2017	2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)	10.1109/CISP-BMEI.2017.8302232	raman scattering;raman spectroscopy;fourier transform;blood type;artificial intelligence;principal component analysis;cluster analysis;pattern recognition;analytical chemistry;computer science;abo blood group system;graph	Vision	16.109383336989673	-56.96130682398001	133234
d0125a09070fe6a5232148fe5a4d2a11bf60e436	numerical approach to spatial deterministic-stochastic models arising in cell biology	probability density;calcium channels;monte carlo method;macromolecules;membrane receptor signaling;algorithms;cell membranes;membrane proteins	Hybrid deterministic-stochastic methods provide an efficient alternative to a fully stochastic treatment of models which include components with disparate levels of stochasticity. However, general-purpose hybrid solvers for spatially resolved simulations of reaction-diffusion systems are not widely available. Here we describe fundamentals of a general-purpose spatial hybrid method. The method generates realizations of a spatially inhomogeneous hybrid system by appropriately integrating capabilities of a deterministic partial differential equation solver with a popular particle-based stochastic simulator, Smoldyn. Rigorous validation of the algorithm is detailed, using a simple model of calcium 'sparks' as a testbed. The solver is then applied to a deterministic-stochastic model of spontaneous emergence of cell polarity. The approach is general enough to be implemented within biologist-friendly software frameworks such as Virtual Cell.	algorithm;calcium;emergence;general-purpose modeling;hybrid system;list of minor characters in the matrix series;norm (social);simulation;software framework;solver;spontaneous order;stochastic optimization;stochastic process;testbed;virtual cell	James C. Schaff;Fei Gao;Li Ye;Igor L. Novak;Boris M. Slepchenko	2016		10.1371/journal.pcbi.1005236	macromolecule;biochemistry;mathematical optimization;probability density function;computer science;bioinformatics;theoretical computer science;voltage-dependent calcium channel;membrane protein;statistics;monte carlo method	Comp.	16.37781560993267	-60.352599820210834	133275
40a74eea514b389b480d6fe8b359cb6ad31b644a	discrete deep feature extraction: a theory and new architectures		First steps towards a mathematical theory of deep convolutional neural networks for feature extraction were made—for the continuous-time case— in Mallat, 2012, and Wiatowski and Bölcskei, 2015. This paper considers the discrete case, introduces new convolutional neural network architectures, and proposes a mathematical framework for their analysis. Specifically, we establish deformation and translation sensitivity results of local and global nature, and we investigate how certain structural properties of the input signal are reflected in the corresponding feature vectors. Our theory applies to general filters and general Lipschitz-continuous non-linearities and pooling operators. Experiments on handwritten digit classification and facial landmark detection—including feature importance evaluation—complement the theoretical findings.	artificial neural network;convolutional neural network;feature extraction;feature vector	Thomas Wiatowski;Michael Tschannen;Aleksandar K Stanic;Philipp Grohs;Helmut Bölcskei	2016			computer science;theoretical computer science;machine learning;pattern recognition;mathematics;statistics	ML	24.381073209589847	-52.97250907768041	133314
aeda8902a943c0a4b559127d321de0625febd439	an investigation of molecular dynamics simulation and molecular docking: interaction of citrus flavonoids and bovine β-lactoglobulin in focus	bovine β lactoglobulin;molecular dynamics simulation;citrus flavonoids;binding affinity;molecular docking	Citrus flavonoids are natural compounds with important health benefits. The study of their interaction with a transport protein, such as bovine β-lactoglobulin (BLG), at the atomic level could be a valuable factor to control their transport to biological sites. In the present study, molecular docking and molecular dynamics simulation methods were used to investigate the interaction of hesperetin, naringenin, nobiletin and tangeretin as citrus flavonoids and BLG as transport protein. The molecular docking results revealed that these flavonoids bind in the internal cavity of BLG and the BLG affinity for binding the flavonoids follows naringenin>hesperetin>tangeretin>nobiletin. The docking results also indicated that the BLG-flavonoid complexes are stabilized through hydrophobic interactions, hydrogen bond interactions and π-π stacking interactions. The analysis of molecular dynamics (MD) simulation trajectories showed that the root mean square deviation (RMSD) of various systems reaches equilibrium and fluctuates around the mean value at various times. Time evolution of the radius of gyration, total solvent accessible surface of the protein and the second structure of protein showed as well that BLG and BLG-flavonoid complexes were stable around 2500ps, and there was not any conformational change as for BLG-flavonoid complexes. Further, the profiles of atomic fluctuations indicated the rigidity of the ligand binding site during the simulation.	accessible surface area;boat dock;body cavities;bone structure of radius;bovine metabolome database;carrier proteins;cattle;citrus plant;citrus sinensis ab.igg:acnc:pt:ser:qn;docking (molecular);enzootic bovine leukosis;flavonoids;heparin, low-molecular-weight;hydrogen bonding;interaction;ligands;mean squared error;molecular dynamics;muscle rigidity;plant roots;processor affinity;simulation;stacking;benefit;hesperetin;naringenin;nobiletin;tangeretin	M. Sahihi;Y. Ghayeb	2014	Computers in biology and medicine	10.1016/j.compbiomed.2014.04.022	molecular dynamics;docking;ligand	Comp.	10.239137202797775	-63.76117087835819	133389
a81e7f28661e75c37c744f19848c0071c48faa00	neuronal theories and technical systems for face recognition	feature extraction;face recognition	I present various systems for the recognition of human faces. They consist of three steps: feature extraction, solving the correspondence problem, and the actual comparison with stored faces. Two of them are implemented in the Dynamic Link Architecture and are, therefore, close to biological hardware, the others are more technical in nature but also have some biological plausibility. At the end, I will brieey discuss the coherence with the results of psychophysical experiments on human face recognition.	correspondence problem;experiment;facial recognition system;feature extraction;plausibility structure;theory	Rolf P. Würtz	1997			machine learning;feature extraction;facial recognition system;pattern recognition;computer science;computer vision;artificial intelligence	AI	22.28427603479137	-64.09082077581957	133455
e61872836a0b862ba020d7d4b42d933800c55d8d	the development of models to predict melting and pyrolysis point data associated with several hundred thousand compounds mined from patents	computer applications in chemistry;theoretical and computational chemistry;computational biology bioinformatics;documentation and information in chemistry	BACKGROUND Melting point (MP) is an important property in regards to the solubility of chemical compounds. Its prediction from chemical structure remains a highly challenging task for quantitative structure-activity relationship studies. Success in this area of research critically depends on the availability of high quality MP data as well as accurate chemical structure representations in order to develop models. Currently, available datasets for MP predictions have been limited to around 50k molecules while lots more data are routinely generated following the synthesis of novel materials. Significant amounts of MP data are freely available within the patent literature and, if it were available in the appropriate form, could potentially be used to develop predictive models.   RESULTS We have developed a pipeline for the automated extraction and annotation of chemical data from published PATENTS. Almost 300,000 data points have been collected and used to develop models to predict melting and pyrolysis (decomposition) points using tools available on the OCHEM modeling platform (http://ochem.eu). A number of technical challenges were simultaneously solved to develop models based on these data. These included the handing of sparse data matrices with >200,000,000,000 entries and parallel calculations using 32 × 6 cores per task using 13 descriptor sets totaling more than 700,000 descriptors. We showed that models developed using data collected from PATENTS had similar or better prediction accuracy compared to the highly curated data used in previous publications. The separation of data for chemicals that decomposed rather than melting, from compounds that did undergo a normal melting transition, was performed and models for both pyrolysis and MPs were developed. The accuracy of the consensus MP models for molecules from the drug-like region of chemical space was similar to their estimated experimental accuracy, 32 °C. Last but not least, important structural features related to the pyrolysis of chemicals were identified, and a model to predict whether a compound will decompose instead of melting was developed.   CONCLUSIONS We have shown that automated tools for the analysis of chemical information have reached a mature stage allowing for the extraction and collection of high quality data to enable the development of structure-activity relationship models. The developed models and data are publicly available at http://ochem.eu/article/99826.	annotation;chemical structure;chemical space;chemicals;cheminformatics;data point;display resolution;greater than;melting point;mined;pipeline (computing);predictive modelling;quantitative structure–activity relationship;scientific publication;software patent;sparse matrix	Igor V. Tetko;Daniel M. Lowe;Antony J. Williams	2016	Journal of cheminformatics	10.1186/s13321-016-0113-y	computer science;bioinformatics	Comp.	10.394718543110727	-57.78631944142435	133983
c40fa225057d30e232871016ecd9757533e993f0	joint neural network interpretation of infrared and mass spectra	mass spectra;infrared;neural network	Combining gas phase infrared (IR) spectra with mass spectral (MS) data, a neural network has been developed to predict 26 different molecular substructures from multispectral information. The back-propagation procedure has been used for training, including its previously published modification, the flashcard algorithm. Present functional groups have been detected correctly in 86.4% of all cases, compared with 88.4% using only IR and 78.2% using only MS data for training and prediction. For only 8 out of the 26 functionalities does the joint utilization of infrared and mass spectra yield better prediction results, with the greatest improvement being for halogen bond predictions. The prediction of functional group absence results in accuracy of about 95.5% for both IR and IR/MS networks but only 87.1% for a stand alone MS network. Insights have been gained into the suitability of both data sets for neural network training by presenting just IR or MS data to a jointly trained neural network, revealing the am...		Christoph Klawun;Charles L. Wilkins	1996	Journal of Chemical Information and Computer Sciences	10.1021/ci9501002	chemistry;mass spectrum;infrared;computer science;analytical chemistry;machine learning;nuclear magnetic resonance;artificial neural network	Vision	11.438209690350636	-56.22496871949024	134175
5f0647af599d731782482490dbd7657e5cb8f750	ranking targets in structure-based virtual screening of three-dimensional protein libraries: methods and problems	score function;protein ligand interaction;binding site;three dimensional;virtual screening;statistical analysis;retrospective study;experimental evaluation;false positive;molecular interactions;molecular docking	Structure-based virtual screening is a promising tool to identify putative targets for a specific ligand. Instead of docking multiple ligands into a single protein cavity, a single ligand is docked in a collection of binding sites. In inverse screening, hits are in fact targets which have been prioritized within the pool of best ranked proteins. The target rate depends on specificity and promiscuity in protein-ligand interactions and, to a considerable extent, on the effectiveness of the scoring function, which still is the Achilles' heel of molecular docking. In the present retrospective study, virtual screening of the sc-PDB target library by GOLD docking was carried out for four compounds (biotin, 4-hydroxy-tamoxifen, 6-hydroxy-1,6-dihydropurine ribonucleoside, and methotrexate) of known sc-PDB targets and, several ranking protocols based on GOLD fitness score and topological molecular interaction fingerprint (IFP) comparison were evaluated. For the four investigated ligands, the fusion of GOLD fitness and two IFP scores allowed the recovery of most targets, including the rare proteins which are not readily suitable for statistical analysis, while significantly filtering out most false positive entries. The current survey suggests that selecting a small number of targets (<20) for experimental evaluation is achievable with a pure structure-based approach.	4-dichlorobenzene;binding sites;boat dock;dental caries;docking (molecular);docking -molecular interaction;fingerprint;interactome;intermediate filament proteins;libraries;ligands;methotrexate;protein data bank;protocols documentation;ribonucleosides;score;scoring functions for docking;sensitivity and specificity;tamoxifen;virtual screening	Esther Kellenberger;Nicolas Foata;Didier Rognan	2008	Journal of chemical information and modeling	10.1021/ci800023x	biology;three-dimensional space;chemistry;docking;type i and type ii errors;virtual screening;bioinformatics;binding site;retrospective cohort study;combinatorial chemistry;nanotechnology;protein–ligand docking;score;statistics	Comp.	10.497841200490704	-59.22122495707115	134220
f43796644974c5d523667ecea59ee65dc64e93b4	histopathological image recognition with domain knowledge based deep features		Automatic recognition of histopathological image plays an important role in building computer-aid diagnosis system. Traditionally hand-craft features are widely used for representing histopathological images when building recognition models. Currently with the development of deep learning algorithms, deep features obtained directly from pre-trained networks at less costs show that they perform better than traditional ones. However, most recent work adopts common pre-trained networks for feature extraction and train a classifier with domain knowledge which generates a gap between the common extracted features and the application domain. To fill the gap and improve the performance of the recognition model, in this paper we propose a deep model for histopathological image feature representation in a supervised-learning manner. The proposed model is constructed based on some pre-trained convolutional neural networks. After supervised learning, the feature learning network captures most domain knowledge. The proposed model is evaluated on two histopathological image datasets and the results show that the proposed model is superior to current state-of-the-art models.	computer vision	Gang Zhang;Ming Xiao;Yong-Hui Huang	2018		10.1007/978-3-319-95957-3_38	machine learning;convolutional neural network;computer science;deep learning;application domain;supervised learning;pattern recognition;domain knowledge;feature extraction;ensemble learning;artificial intelligence;feature learning	Vision	23.866889916060618	-53.514832162880026	134368
b70379685b2d1d6115cdf0aea4c5135ff0bede8a	analysis of microarray data using inductive logic programming and ontological background information	doctoral thesis	The use of formal logic in chemometric analysis has been sparse. However, there are some problems which are well suited for analysis with inductive logic programming (ILP), a class of machine learning methods based on first-order predicate logic. The principal strength of ILP is that input, background knowledge and output can contain first-order logic statements. Two results of this is that ILP in contrast to other common methods is capable of learning common substructures in a set of graphs, and that it has the ability to use graph-structured background knowledge in analysis. Particularly the growing set of biomedical ontologies is an interesting research area in this respect. This thesis first contain an introduction to ILP together with a review of chemical and biochemical problems where the method have been applied until now. Most papers on the topic have been published in the machine learning literature, and this paper serves as an overview to the parts of the literature that are most relevant for chemistry. The most well-studied problem is structure–activity relationships (SAR) for diverse sets of molecules, because the relational atom–bond structure is well suited for relational representation and hence also ILP learning. The next part of the thesis is a further enhancement of using ILP for SAR problems. A method is implemented which use descriptors from quantum topology instead of atoms and bonds as the molecular representation. This method performs equally well to the previous atom–bond representation, but have the advantage of being rooted in quantum theory while keeping the representational simplicity of the concepts of atoms and bonds. Two articles are devoted to the main aim of this thesis; the use of the gene ontology as background information in microarray analysis. The first article introduces the method, presents the setup and compare it to traditional analysis methods. ILP performs similar to other methods in terms of predictive performance. The advantage of ILP is in the possibility of a classification theory to include both ontological information and expression information for single genes. After asserting that ILP can be used for such analysis, the next article studies how the variable selection performed on the microarray data before running ILP will affect the result. Three different variable selection methods are tested, and the result was that ILP is not very much affected by variable selection performed on the dataset before analysis, as long as enough relevant information is included	inductive logic programming;microarray	Einar Ryeng	2011			computer science;machine learning;data mining;algorithm	ML	14.452063661273481	-54.68532589631601	134742
3149a891eccf47218e686931985d03c66873717f	supervised approaches for protein function prediction by topological data analysis		Topological Data Analysis is a novel approach, useful whenever data can be described by topological structures such as graphs. The aim of this paper is to investigate whether such tool can be used in order to define a set of descriptors useful for pattern recognition and machine learning tasks.Specifically, we consider a supervised learning problem with the final goal of predicting proteins' physiological function starting from their respective residue contact network. Indeed, folded proteins can effectively be described by graphs, making them a useful case-study for assessing Topological Data Analysis effectiveness concerning pattern recognition tasks.Experiments conducted on a subset of the Escherichia coli proteome using two different classification systems show that descriptors derived from Topological Data Analysis - namely, the Betti numbers sequence - lead to classification performances comparable with descriptors derived from widely-known centrality measures, as concerns the protein function prediction problem. Further benchmarking tests suggest the presence of some information despite the heavy compression intrinsic to the protein-to-Betti numbers casting.	betti number;centrality;experiment;machine learning;pattern recognition;performance;protein function prediction;supervised learning;topological data analysis	Alessio Martino;Antonello Rizzi;Fabio Massimo Frattale Mascioli	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8489307	task analysis;support vector machine;machine learning;supervised learning;feature extraction;artificial intelligence;centrality;pattern recognition;topological data analysis;protein function prediction;betti number;computer science	ML	10.382405355889674	-53.00444736823676	134995
e32d295f2980f9f74b28c30abc2b9c999133adcd	conformational analysis of oligoarabinonucleotides. an nmr and cd study	conformational analysis;sugars;aortic arch;arachidonic acid;magnetic resonance spectroscopy;arabinonucleotides;molecular conformation;phosphorus;dimers;ascending aorta;structure activity relationship;circular dichroism;oligonucleotides;cold temperature;arabinose;nucleic acid conformation;amino acids;computer simulation;alcoholics anonymous;protons	A 500 and 300 MHz proton NMR study of the series of oligoarabinonucleotides 5'aAMP, 3'aAMP, aA-aA, (aA-)2aA and (aA-)3aA is presented. In addition, circular dichroism is used to study the stacking behaviour of aA-aA. The complete 1H-NMR spectral assignment of the compounds (except the tetramer) is given. Proton-proton and proton-phosphorus coupling constants, obtained by computer simulation of the high-field region of the spectra, yield information on the conformation of the arabinose rings (N- or S-type) and on the intramolecular stacking properties of the dimer and the trimer. The monomers 5'aAMP and 3'aAMP exhibit a preference for N- and S-type sugar conformation, respectively. It is shown that the dimer aA-aA at low temperature prefers a mixed stacked state of the type aA(S)-aA(N). In the trimer the aA(2)-aA(3) fragment exhibits a conformation similar to that found in the dimer, whereas the aA(1) residue prefers to adopt S-type sugar and has some tendency to stack upon residue aA(2).	amiga advanced graphics architecture;amino acids;arabinose;circular dichroism;cold temperature;computer simulation;coupling constant;domino tiling;efimov state;exhibits as topic;megahertz;phosphorus;protons;ring device;stacking;sugars;monomer	J. Doornbos;J. L. Barascut;H. Lazrek;J. L. Imbach;Jeroen Van Westrenen;G. M. Visser;Jacques H. van Boom;Cornelis Altona	1983	Nucleic acids research	10.1093/nar/11.13.4583	computer simulation;circular dichroism;biochemistry;structure–activity relationship;phosphorus;l-arabinose operon;genetics;proton;oligonucleotide	Logic	10.317228847404866	-63.59390001850927	135220
67753956f7257e23f51275f0e569e6a4b77cb590	optical detection of carotenoids in living tissue as a measure of fruit and vegetable intake	biological patents;biomedical journals;text mining;europe pubmed central;citation search;skin raman scattering image color analysis cancer spectroscopy retina reflection;citation networks;research articles;abstracts;open access;life sciences;clinical guidelines;biomedical measurement biological tissues;full text;rest apis;orcids;europe pmc;fruit intake measurement vegetable intake assessment fruit intake assessment human tissue carotenoid optical detection method blood carotenoid circulation vegetable intake measurement;biomedical research;bioinformatics;literature search	Circulating blood carotenoids have long been accepted as the most accurate biomarker of total fruit and vegetable intake. Recent technological developments have led to a novel optical detection method of carotenoids in living human tissue, allowing for a non-invasive and rapid method to assess fruit and vegetable intake. Future technological advances to increase sensitivity and molecular specificity, reduce cost, and detect other biomedically important compounds would expand use of this methodology.	biological markers;carotenoids;sensitivity and specificity	Leah D. Whigham;Alisha H. Redelfs	2015	2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2015.7320297	food science;text mining;botany;medical research;medicine;biotechnology;computer science	Visualization	12.16614053691117	-65.49715323567338	135410
6be461dd5869d00fc09975a8f8e31eb5f86be402	real time face detection and facial expression recognition: development and applications to human computer interaction.	detectors;time scale;facial expression recognition;human computer interaction;video streaming;face real time systems robots pixel training support vector machines gabor filters;support vector machines;application software;uncertainty;real time;training;real time processing;gabor filters;human robot interaction;face recognition;automatic detection;streaming media;pixel;robots;facial animation;face;facial expression;computer animation;face detection;face to face;real time systems	Computer animated agents and robots bring a social dimension to human computer interaction and force us to think in new ways about how computers could be used in daily life. Face to face communication is a real-time process operating at a a time scale in the order of 40 milliseconds. The level of uncertainty at this time scale is considerable, making it necessary for humans and machines to rely on sensory rich perceptual primitives rather than slow symbolic inference processes. In this paper we present progress on one such perceptual primitive. The system automatically detects frontal faces in the video stream and codes them with respect to 7 dimensions in real time: neutral, anger, disgust, fear, joy, sadness, surprise. The face finder employs a cascade of feature detectors trained with boosting techniques [15, 2]. The expression recognizer receives image patches located by the face detector. A Gabor representation of the patch is formed and then processed by a bank of SVM classifiers. A novel combination of Adaboost and SVM's enhances performance. The system was tested on the Cohn-Kanade dataset of posed facial expressions [6]. The generalization performance to new subjects for a 7- way forced choice correct. Most interestingly the outputs of the classifier change smoothly as a function of time, providing a potentially valuable representation to code facial expression dynamics in a fully automatic and unobtrusive manner. The system has been deployed on a wide variety of platforms including Sony's Aibo pet robot, ATR's RoboVie, and CU animator, and is currently being evaluated for applications including automatic reading tutors, assessment of human-robot interaction.	aibo;adaboost;answer to reset;code;computer animation;face detection;finite-state machine;human computer;human–computer interaction;human–robot interaction;real-time computing;real-time locating system;robot;sadness;sensor;smoothing;streaming media;support vector machine;tip (unix utility)	Marian Stewart Bartlett;Gwen Littlewort;Ian R. Fasel;Javier R. Movellan	2003	2003 Conference on Computer Vision and Pattern Recognition Workshop	10.1109/CVPRW.2003.10057	facial recognition system;human–robot interaction;robot;face;support vector machine;computer vision;detector;face detection;application software;speech recognition;computer facial animation;uncertainty;computer science;machine learning;computer animation;facial expression;pixel	Vision	24.60025474594184	-60.842727563594686	135487
b2bf54bc93a91f9c08cb4f797b2e69fddc7d8793	a formulation of receptive field type input layer for tam network using gabor function	neural nets;gabor filters filtering brain modeling humans frequency convolution informatics missiles information science electronic mail;human visual cortex receptive field type input layer tam network gabor function topographic attentive mapping network biologically motivated neural network;receptive field;visual cortex;neural network	The TAM (topographic attentive mapping) network is a biologically-motivated neural network. Gabor function is a model for receptive field. We formulate a new input layer using Gabor function for TAM network to realize receptive field of human visual cortex.	artificial neural network;gabor atom;ibm tivoli access manager;topography	Isao Hayashi;Hiromasa Maeda;James R. Williamson	2004	2004 IEEE International Conference on Fuzzy Systems (IEEE Cat. No.04CH37542)	10.1109/FUZZY.2004.1375764	computer vision;computer science;artificial intelligence;machine learning;receptive field;artificial neural network	Robotics	21.330288141973714	-63.009384356210916	135714
4a242d3ab697435ac8e7df131c5c5de49c20e59c	tcsvm - a cascade approach with transductive inference to predicting protein translation initiation site		Correctly predicting Protein Translation Initiation Site is a problem of great interest in molecular biology. The lack of knowledge of conservative Ribonucleic Acid (mRNA) “ribbons” makes the problem not trivial, and machine learning methods have obtained good results in this context. The purpose of the present work is to develop a cascade strategy with the Transductive Support Vector Machine (SVM) method to predict Protein Translation Initiation Site (TIS). The experimental results showed a significant reduction in the computational cost of the method, indicating that the proposed approach is promising, as it achieved an arguably good performance in comparison to the state of the art methods of TIS prediction.	algorithmic efficiency;bioinformatics;computation;machine learning;protein function prediction;statistical classification;support vector machine;transduction (machine learning)	Wallison W. Guimarães;Cristiano Lacerda Nunes Pinto;Cristiane Neri Nobre;Luis E. Zárate	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8488989	machine learning;support vector machine;microsoft windows;cascade;pattern recognition;artificial intelligence;protein translation;transduction (machine learning);computer science	ML	10.418144871580932	-54.483476249701575	135874
30fe51794e66cad2688159e0725c40fb7b1bbcf0	vldp web server: a powerful geometric tool for analysing protein structures in their environment	delaunay;software;nuclear magnetic resonance;protein accessibility;solvents;voronoi;protein structure;models molecular;protein structures;precision;internet;proteins;protein conformation;atom;tessellations;amino acids;crystallography x ray;protein volume;direct visualization	Protein structures are an ensemble of atoms determined experimentally mostly by X-ray crystallography or Nuclear Magnetic Resonance. Studying 3D protein structures is a key point for better understanding protein function at a molecular level. We propose a set of accurate tools, for analysing protein structures, based on the reliable method of Voronoi-Laguerre tessellations. The Voronoi Laguerre Delaunay Protein web server (VLDPws) computes the Laguerre tessellation on a whole given system first embedded in solvent. Through this fine description, VLDPws gives the following data: (i) Amino acid volumes evaluated with high precision, as confirmed by good correlations with experimental data. (ii) A novel definition of inter-residue contacts within the given protein. (iii) A measure of the residue exposure to solvent that significantly improves the standard notion of accessibility in some cases. At present, no equivalent web server is available. VLDPws provides output in two complementary forms: direct visualization of the Laguerre tessellation, mostly its polygonal molecular surfaces; files of volumes; and areas, contacts and similar data for each residue and each atom. These files are available for download for further analysis. VLDPws can be accessed at http://www.dsimb.inserm.fr/dsimb_tools/vldp.	access network;accessibility;amino acid [epc];choose (action);computed tomography scanning systems;crystallography;delaunay triangulation;display resolution;download;elfacos ow 100;embedded system;embedding;experiment;extraction;graph (abstract data type);graph - visual representation;hematological disease;nuclear magnetic resonance;server (computer);server (computing);set packing;topological graph;voronoi diagram;web server;anatomical layer;funding grant	Jeremy Esque;Sylvain Léonard;Alexandre G. de Brevern;Christophe Oguey	2013		10.1093/nar/gkt509	protein structure;bioinformatics	Comp.	12.440459661093046	-59.511375276291176	136189
10ff3f0ffcf8f31d1cb68dc0b3e43784289096ae	prediction of hplc retention index using artificial neural networks and igroup e-state indices	artificial neural networks;retention index;hplc retention index;e state indices;artificial neural network	A back-propagation artificial neural network (ANN) was used to create a 10-fold leave-10%-out cross-validated ensemble model of high performance liquid chromatography retention index (HPLC-RI) for a data set of 498 diverse druglike compounds. A 10-fold multiple linear regression (MLR) ensemble model of the same data was developed for comparison. Molecular structure was described using IGroup E-state indices, a novel set of structure-information representation (SIR) descriptors, along with molecular connectivity chi and kappa indices and other SIR descriptors previously reported. The same input descriptors were used to develop models by both learning algorithms. The MLR model yielded marginally acceptable statistics with training correlation r(2) = 0.65, mean absolute error (MAE) = 83 RI units. External validation of 104 compounds not used for model development yielded validation v(2) = 0.49 and MAE = 73 RI units. The distribution of residuals for the fit and validate data sets suggest a nonlinear relationship between retention index and molecular structure as described by the SIR indices. Not surprisingly, the ANN model was significantly more accurate for both training and validation with training set r(2) = 0.93, MAE = 30 RI units and validation v(2) = 0.84, MAE = 41 RI units. For the ANN model, a total of 91% of validation predictions were within 100 RI units of the experimental value.	algorithm;approximation error;artificial neural network;backpropagation;high pressure liquid chromatography procedure;kappa calculus;learning to rank;machine learning;mathematical model;molecular descriptor;multiple myeloma;nonlinear system;rs-232;regression analysis;software propagation;test set;unit	Daniel R. Albaugh;L. Mark Hall;Dennis W. Hill;Tzipporah M. Kertesz;Marc Parham;Lowell H. Hall;David F. Grant	2009	Journal of chemical information and modeling	10.1021/ci9000162	computer science;artificial intelligence;machine learning;data mining;kovats retention index;artificial neural network	ML	12.78487744686176	-57.03882694104672	136367
bfa545c5a2120e42a2263ff0ad68b75501ce8db9	a biological inspired visual landmark recognition architecture	image recognition;image processing;visual landmark recognition;image recognition art neural nets feature extraction;visual landmarks;computer architecture;visualization;image processing visual landmark recognition neural network;shape;feature extraction;region of interest;pixel;matched filters;autonomous navigation;sarrt artificial neural networks biological inspired visual landmark recognition architecture real time image processing memory feedback modulation mechanism feature extraction;art neural nets;navigation image processing feedback feature extraction;real time image processing;neural network	An architecture that is inspired by a human’s capability to autonomously navigate an environment based on visual landmark recognition is presented. It consists of pre-attentive and attentive stages that allow visual landmarks to be recognized reliably under both clean and cluttered backgrounds. The pre-attentive stage provides an efficient means for real-time image processing by selectively focusing on regions of interest within input images. The attentive stage has a memory feedback modulation mechanism that allows visual knowledge of landmarks in the memory to interact and guide different stages in the architecture for efficient feature extraction and landmark recognition. The results show that the architecture is able to reliably recognise both occluded and non-occluded visual landmarks in complex backgrounds.	feature extraction;image processing;modulation;real-time clock;region of interest	Quoc Do;Lakhmi C. Jain	2009	2009 Digital Image Computing: Techniques and Applications	10.1109/DICTA.2009.61	computer vision;visualization;image processing;feature extraction;shape;computer science;machine learning;matched filter;pixel;region of interest	Vision	21.71800190041015	-65.38488105922262	136841
8543fbe7a733df2c3845db8b4ae8b59bd7b61a55	integrating scattering feature maps with convolutional neural networks for malayalam handwritten character recognition	handwritten recognition;convolutional neural networks;deep learning models;scattering convolutional network;malayalam character recognition;scattering transform	Convolutional neural network (CNN)-based deep learning architectures are the state-of-the-art in image-based pattern recognition applications. The receptive filter fields in convolutional layers are learned from training data patterns automatically during classifier learning. There are number of well-defined, well-studied and proven filters in the literature that can extract informative content from the input patterns. This paper focuses on utilizing scattering transform-based wavelet filters as the first-layer convolutional filters in CNN architecture. The scattering networks are generated by a series of scattering transform operations. The scattering coefficients generated in first few layers are effective in capturing the dominant energy contained in the input data patterns. The present work aims at replacing the first-layer convolutional feature maps in CNN architecture with scattering feature maps. This architecture is equivalent to utilizing scattering wavelet filters as the first-layer receptive fields in CNN architecture. The proposed hybrid CNN architecture experiments the Malayalam handwritten character recognition which is one of the challenging multi-class classification problems. The initial studies confirm that the proposed hybrid CNN architecture based on scattering feature maps could perform better than the equivalent self-learning architecture of CNN on handwritten character recognition problems.	artificial neural network;benchmark (computing);coefficient;computer vision;convolutional neural network;database;deep learning;experiment;filter bank;handwriting recognition;information;map;modulus of continuity;multiclass classification;network architecture;optical character recognition;particle filter;pattern recognition;wavelet	K. Manjusha;M. Anand Kumar;K. P. Soman	2018	International Journal on Document Analysis and Recognition (IJDAR)	10.1007/s10032-018-0308-z	wavelet;convolutional neural network;machine learning;deep learning;pattern recognition;architecture;artificial intelligence;computer science;receptive field;training set;scattering	Vision	24.55204953650753	-53.048416012617025	136899
bff3b0f115f81f5635eb703cd93b3a28fedcaaab	molecular mechanisms orchestrating the stem cell response to translational scaffolds	molecular mechanism bioinstructive nanofiber msc lineage commitment adherens junction membrane associated vesicle trafficking protein membrane associated vesicle stabilizing protein focal adhesion nanofiber based scaffold multiple musculoskeletal lineages msc differentiation mesenchymal stem cell musculoskeletal tissue revolutionized scaffold based approach specific tissue engineering application cell biology cell phenotype fiber morphology translational scaffold stem cell response;tissue engineering biological tissues biomedical materials cellular biophysics molecular biophysics nanofibres nanomedicine proteins;stem cells adhesives geometry sensors proteins junctions computer architecture	A 2013 Perspective in Science titled “Deconstructing Dimensionality” noted the importance of fiber morphology on cell phenotype, concluding with the statement “Identifying the mechanisms by which cells assess the nature of their environment will advance basic cell biology and facilitate the development of synthetic matrices for specific tissue engineering applications.” Nanofibers have revolutionized scaffold-based approaches for musculoskeletal tissues; demonstrating surprising efficacy over promoting mesenchymal stem cell, MSC, differentiation down multiple musculoskeletal lineages. Understanding the fundamental mechanisms involved will allow the future design of nanofiber-based scaffolds to target a lineage with specificity. This article focuses on how three geometry sensors: focal adhesions, membrane associated vesicle stabilizing and trafficking proteins, and adherens junctions; potentially regulate MSC lineage commitment in response to bio-instructive nanofibers.	adherens junction;body tissue;british informatics olympiad;cultured cell line;diameter (qualifier value);focal (programming language);focal adhesions;galaxy morphological classification;genetic translation process;lineage (evolution);mesenchymal stem cells;microsoft cluster server;nanofibers;natural regeneration;optical fiber;promotion (action);sensitivity and specificity;sensor;synthetic intelligence;tissue adhesions;tissue engineering;tissue fiber;tissue membrane;vesicle (morphologic abnormality);tissue regeneration	Tugba Ozdemir;Andrew M. Higgins;Justin L. Brown	2015	2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2015.7318716	molecular biology;cell biology;nanotechnology	Visualization	10.051346488927422	-65.90487882927656	137230
f61d1c03b6ddb78f384646eae35fbffff2e3222c	kinetic modelling of the role of the aldehyde dehydrogenase enzyme and the breast cancer resistance protein in drug resistance and transport	dna;breast cancer cells;tpt h;topotecan;sd;nwd;high performance liquid chromatography;standard deviation;glycoprotein;enzyme;rm therapeutics pharmacology;topotecan lactone form;cpt;multidrug resistance protein 1;deoxyribonucleic acid;hplc;sdln;p gp;efflux pump;drug kinetics;not well determined;dsdna;p glycoprotein;liquid chromatography;double stranded dna;breast cancer resistance protein;aldehyde dehydrogenase;topotecan hydroxy acid form;compartmental model;efflux pumps;standard deviation of the natural logarithm;tpt;aldh;structural identifiability;mprp1;camptothecin;parameter estimation;drug resistance;rc0254 neoplasms tumors oncology including cancer;kinetics;tpt l;brcp;ta engineering general civil engineering general	A compartmental model for the in vitro uptake kinetics of the anti-cancer agent topotecan (TPT) has been extended from a previously published model. The extended model describes the drug activity and delivery of the pharmacologically active form to the DNA target as well as the catalysis of the aldehyde dehydrogenase (ALDH) enzyme and the elimination of drug from the cytoplasm via the efflux pump. Verification of the proposed model is achieved using scanning-laser microscopy data from live human breast cancer cells. Before estimating the unknown model parameters from the experimental in vitro data it is essential to determine parameter uniqueness (or otherwise) from this imposed output structure. This is formally performed as a structural identifiability analysis, which demonstrates that all of the unknown model parameters are uniquely determined by the output structure corresponding to the experiment.	abcg2 gene;acetaldehyde;aldehyde dehydrogenase (nad+);arabic numeral 0;atari;breast cancer cell;catalysis;computation;computer algebra system;drug design;estimated;estimation theory;excretory function;fax;high-level programming language;in vitro [publication type];kinesiology;kinetics (discipline);kinetics internet protocol;mammary neoplasms;multi-compartment model;norm (social);numerous;population parameter;pumping (computer systems);robustness (computer science);scientific publication;sensitivity and specificity;simulation;topotecan;verification of theories;efflux pump complex;pump (device);regulatory proteins	M. I. Atari;Michael J. Chappell;Rachel J. Errington;Paul J. Smith;Neil D. Evans	2011	Computer methods and programs in biomedicine	10.1016/j.cmpb.2010.06.008	aldehyde dehydrogenase;high-performance liquid chromatography;dna;efflux	Comp.	10.699563060658518	-62.77084837337044	137537
f814392e57031daba72999ca8097cf3a6b3c2ded	optimization of erk activity biosensors for both ratiometric and lifetime fret measurements	fret;erk;biosensing techniques;phosphorylation;signal transduction;genetically encoded biosensor;fluorescent dyes;fluorescence resonance energy transfer	Among biosensors, genetically-encoded FRET-based biosensors are widely used to localize and measure enzymatic activities. Kinases activities are of particular interest as their spatiotemporal regulation has become crucial for the deep understanding of cell fate decisions. This is especially the case for ERK, whose activity is a key node in signal transduction pathways and can direct the cell into various processes. There is a constant need for better tools to analyze kinases in vivo, and to detect even the slightest variations of their activities. Here we report the optimization of the previous ERK activity reporters, EKAR and EKAREV. Those tools are constituted by two fluorophores adapted for FRET experiments, which are flanking a specific substrate of ERK, and a domain able to recognize and bind this substrate when phosphorylated. The latter phosphorylation allows a conformational change of the biosensor and thus a FRET signal. We improved those biosensors with modifications of: (i) fluorophores and (ii) linkers between substrate and binding domain, resulting in new versions that exhibit broader dynamic ranges upon EGF stimulation when FRET experiments are carried out by fluorescence lifetime and ratiometric measurements. Herein, we characterize those new biosensors and discuss their observed differences that depend on their fluorescence properties.	anatomic node;biosensors;experiment;flank (surface region);fluorescence resonance energy transfer;linker (computing);mathematical optimization;transduction (machine learning);version;video-in video-out	Pauline Vandame;Corentin Spriet;Franck Riquet;Dave Trinel;Katia Cailliau-Maggio;Jean-François Bodart	2014		10.3390/s140101140	biochemistry;molecular biology;chemistry;analytical chemistry;förster resonance energy transfer	SE	11.292868203611752	-65.5020149173539	137875
79bf27e25babf7fb5785a37a6baa035ceed6003f	learning convolutional neural networks for data-flow graph mapping on spatial programmable architectures (abstract only)	reconfigurable architecture;dfg;mapping;convolutional neural network	Data flow graph (DFG) mapping is critical for the compiling of spatial programmable architecture, where compilation time is a key factor for both time-to-market requirement and mapping successful rate. Inspired from the great progress made in tree search game using deep neural network, we proposed a framework for learning convolutional neural networks for mapping DFGs onto spatial programmable architectures. Considering that mapping is a process from source to target, we present a dual-input neural network capturing features from both DFGs in applications and Process Element Array (PEA) in spatial programmable architectures. In order to train the neural network, algorithms are designed to automatically generate a data set from PEA intermediate states of preprocessed DFG. Finally, we demonstrate that the trained neural network can get high identifying accuracy of mapping quality and our proposed mapping approach are competitive with state-of-the-art DFG mapping algorithms in performance while the compilation time is greatly reduced.	algorithm;artificial neural network;compiler;convolutional neural network;dataflow architecture;deep learning;search game	Shouyi Yin;Dajiang Liu;Lifeng Sun;Xinhan Lin;Leibo Liu;Shaojun Wei	2017		10.1145/3020078.3021801	embedded system;parallel computing;computer science;artificial intelligence;theoretical computer science;machine learning;time delay neural network;convolutional neural network	AI	18.859039371911393	-54.69504533072714	138266
899db02ba28ef2479b5bea3e51627685be5b3865	deep recurrent neural networks for human activity recognition	deep learning;human activity recognition;recurrent neural networks	Adopting deep learning methods for human activity recognition has been effective in extracting discriminative features from raw input sequences acquired from body-worn sensors. Although human movements are encoded in a sequence of successive samples in time, typical machine learning methods perform recognition tasks without exploiting the temporal correlations between input data samples. Convolutional neural networks (CNNs) address this issue by using convolutions across a one-dimensional temporal sequence to capture dependencies among input data. However, the size of convolutional kernels restricts the captured range of dependencies between data samples. As a result, typical models are unadaptable to a wide range of activity-recognition configurations and require fixed-length input windows. In this paper, we propose the use of deep recurrent neural networks (DRNNs) for building recognition models that are capable of capturing long-range dependencies in variable-length input sequences. We present unidirectional, bidirectional, and cascaded architectures based on long short-term memory (LSTM) DRNNs and evaluate their effectiveness on miscellaneous benchmark datasets. Experimental results show that our proposed models outperform methods employing conventional machine learning, such as support vector machine (SVM) and k-nearest neighbors (KNN). Additionally, the proposed models yield better performance than other deep learning techniques, such as deep believe networks (DBNs) and CNNs.	activity recognition;architecture as topic;artificial neural network;behavior;benchmark (computing);conflict (psychology);convolution;convolutional neural network;deep learning;end-to-end principle;experiment;human activities;k-nearest neighbors algorithm;long short-term memory;low-power broadcasting;machine learning;microsoft windows;movement;neural tube defects;neural network software;power semiconductor device;recurrent neural network;support vector machine;anatomical layer;sensor (device)	Abdulmajid Murad;Jae-Young Pyun	2017		10.3390/s17112556	convolutional neural network;discriminative model;support vector machine;deep learning;activity recognition;recurrent neural network;machine learning;artificial intelligence;pattern recognition;computer science	ML	22.713502983717547	-56.839924440269805	138350
313b8df0c3e5bf2b08d6b36eff86b50a3478b07a	image phylogeny by minimal spanning trees	watermarking;duplicate detection;history;minimal spanning tree;image processing;phylogeny;edge detection;near duplicates kinship image dependencies image phylogeny image phylogeny tree image s ancestry relationships near duplicate detection and recognition;near duplicates kinship;trees mathematics;ipt reconstruction minimal spanning trees digital content internet identical copies image phylogeny tree biological systems;vegetation;visualization;near duplicate detection and recognition;phylogeny internet watermarking history vegetation visualization image edge detection;internet;image dependencies;image edge detection;image phylogeny tree;image phylogeny;trees mathematics image processing internet;image s ancestry relationships	Nowadays, digital content is widespread and also easily redistributable, either lawfully or unlawfully. Images and other digital content can also mutate as they spread out. For example, after images are posted on the Internet, other users can copy, resize and/or re-encode them and then repost their versions, thereby generating similar but not identical copies. While it is straightforward to detect exact image duplicates, this is not the case for slightly modified versions. In the last decade, some researchers have successfully focused on the design and deployment of near-duplicate detection and recognition systems to identify the cohabiting versions of a given document in the wild. Those efforts notwithstanding, only recently have there been the first attempts to go beyond the detection of near-duplicates to find the structure of evolution within a set of images. In this paper, we tackle and formally define the problem of identifying these image relationships within a set of near-duplicate images, what we call Image Phylogeny Tree (IPT), due to its natural analogy with biological systems. The mechanism of building IPTs aims at finding the structure of transformations and their parameters if necessary, among a near-duplicate image set, and has immediate applications in security and law-enforcement, forensics, copyright enforcement, and news tracking services. We devise a method for calculating an asymmetric dissimilarity matrix from a set of near-duplicate images and formally introduce an efficient algorithm to build IPTs from such a matrix. We validate our approach with more than 625000 test cases, including both synthetic and real data, and show that when using an appropriate dissimilarity function we can obtain good IPT reconstruction even when some pieces of information are missing. We also evaluate our solution when there are more than one near-duplicate sets in the pool of analysis and compare to other recent related approaches in the literature.	algorithm;biological system;computer forensics;digital recording;distance matrix;encode;file spanning;freely redistributable software;information processes and technology;internet;kruskal's algorithm;phylogenetic tree;phylogenetics;pixel;resampling (statistics);software deployment;synthetic intelligence;test case;uncontrolled format string	Zanoni Dias;Anderson Rocha;Siome Goldenstein	2012	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2011.2169959	computer vision;the internet;visualization;edge detection;image processing;digital watermarking;computer science;bioinformatics;theoretical computer science;minimum spanning tree;machine learning;data mining;mathematics;algorithm;vegetation	ML	17.71322356928801	-58.58519487880105	138467
4923c6bc3ab501651763f6814a8384745f3d9a4d	visual attribute transfer through deep image analogy		"""We propose a new technique for visual attribute transfer across images that may have very different appearance but have perceptually similar semantic structure. By visual attribute transfer, we mean transfer of visual information (such as color, tone, texture, and style) from one image to another. For example, one image could be that of a painting or a sketch while the other is a photo of a real scene, and both depict the same type of scene.  Our technique finds semantically-meaningful dense correspondences between two input images. To accomplish this, it adapts the notion of """"image analogy"""" [Hertzmann et al. 2001] with features extracted from a Deep Convolutional Neutral Network for matching; we call our technique deep image analogy. A coarse-to-fine strategy is used to compute the nearest-neighbor field for generating the results. We validate the effectiveness of our proposed method in a variety of cases, including style/texture transfer, color/style swap, sketch/painting to photo, and time lapse."""	image analogy;texture mapping	Jing Liao;Yuan Yao;Lu Yuan;Gang Hua;Sing Bing Kang	2017	ACM Trans. Graph.	10.1145/3072959.3073683	computer vision;painting;sketch;machine learning;analogy;artificial intelligence;computer science	Graphics	22.914705460471584	-55.25109821947487	138638
09fa3b7cf3ce42e9d0537c139ec99b9b4f0cefdc	sequence recognition with spatio-temporal long-term memory organization	hierarchical memory architecture;hand sign language interpretation;neural nets;brain models;drntu engineering computer science and engineering;conference paper;australian sign language dataset sequence recognition spatio temporal long term memory organization connectionist memory structure spatio temporal sequence learning human cortex symbolic data real valued multidimensional data stream spatio temporal learning error tolerance memory forgetting mechanism hand sign language interpretation;hand sign language interpretation hierarchical memory architecture spatio temporal neural networks;computer architecture microprocessors neurons vectors training organizations estimation;learning artificial intelligence;neural nets brain models learning artificial intelligence natural language processing;natural language processing;spatio temporal neural networks	In this work, we propose a connectionist memory structure for spatio-temporal sequence learning and recognition inspired by the Long-Term Memory structure of human cortex. Besides symbolic data, our framework is able to continuously process real-valued multi-dimensional data stream. This capability is made possible by addressing three critical problems in spatio-temporal learning, namely error tolerance, significance of sequence's elements and memory forgetting mechanism. We demonstrate the potential of the framework with a synthetic example and a real world example, namely the task of hand-sign language interpretation with the Australian Sign Language dataset.	artificial intelligence;autonomous system (internet);connectionism;content-addressable memory;error-tolerant design;learning organization;memory organisation;organizing (structure);robot;robotic mapping;sequence analysis;speech recognition;synthetic intelligence;temporal logic;the australian	Vu Anh Nguyen;Janusz A. Starzyk;Wooi-Boon Goh	2012	The 2012 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2012.6252682	natural language processing;semantic memory;computer science;artificial intelligence;machine learning;artificial neural network	ML	19.706867456848908	-65.91343745308389	138844
f4aaf11031211846681aaa0ad9ef189afbc767fc	combogan: unrestrained scalability for image domain translation		The past year alone has seen unprecedented leaps in the area of learning-based image translation, namely Cycle-GAN, by Zhu et al. But experiments so far have been tailored to merely two domains at a time, and scaling them to more would require an quadratic number of models to be trained. And with two-domain models taking days to train on current hardware, the number of domains quickly becomes limited by the time and resources required to process them. In this paper, we propose a multi-component image translation model and training scheme which scales linearly - both in resource consumption and time required - with the number of domains. We demonstrate its capabilities on a dataset of paintings by 14 different artists and on images of the four different seasons in the Alps. Note that 14 data groups would need (14 choose 2) = 91 different CycleGAN models: a total of 182 generator/discriminator pairs; whereas our model requires only 14 generator/discriminator pairs.		Asha Anoosheh;Eirikur Agustsson;Radu Timofte;Luc Van Gool	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2018.00122	task analysis;artificial intelligence;machine learning;leaps;scaling;data modeling;computer science;discriminator;scalability;image translation;decoding methods	Vision	22.531236896542975	-53.23473869337732	139145
5bd60f8db720d76068919bd44a072571577766f3	on predicting secondary structure transition	transitions;approximate algorithm;decision tree;amino acid sequences;secondary structure transition;amino acid sequence;model validation;np hard problem;protein structure;proteins;machine learning;predictability;secondary structure;random forest;prediction accuracy;optimal prediction;optimal algorithm;bioinformatics	A function of a protein is dependent on its structure; therefore, predicting a protein structure from an amino acid sequence is an active area of research. To improve the accuracy of validation of structures, we are studying the predictability of secondary structure transitions using the following machine learning algorithms: naive Bayes, C4.5 decision tree, and random forest. The annotated data sets from PDB that have agreement with DSSP and STRIDE are used for training and testing. We have demonstrated that predicting structure transition with high degree of certainty is possible and we were able to get as high as 97.5% of prediction accuracy.	amino acid sequence;amino acids;c4.5 algorithm;decision tree;machine learning;naive bayes classifier;protein data bank;protein structure;random forest;setun;staphylococcal protein a;stride scheduling	Rasiah Loganantharaj;Vivek M. Philip	2007	International journal of bioinformatics research and applications	10.1504/IJBRA.2007.015413	random forest;protein structure;predictability;computer science;bioinformatics;machine learning;decision tree;pattern recognition;np-hard;regression model validation;peptide sequence;protein secondary structure	ML	10.768006003475689	-55.30425071731784	139153
fff34334f4a1a7afe8ddbb27911af52decbac987	counterfactuals uncover the modular structure of deep generative models		Deep generative models such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) are important tools to capture and investigate the properties of complex empirical data. However, the complexity of their inner elements makes their functioning challenging to assess and modify. In this respect, these architectures behave as black box models. In order to better understand the function of such networks, we analyze their modularity based on the counterfactual manipulation of their internal variables. Experiments with face images support that modularity between groups of channels is achieved to some degree within convolutional layers of vanilla VAE and GAN generators. This helps understand the functional organization of these systems and allows designing meaningful transformations of the generated images without further training.		Michel Besserve;R'emy Sun;Bernhard Scholkopf	2018	CoRR			ML	21.45908163316724	-53.22577791079982	139179
956b470589eaed541a53e06422a8c398563389ea	grid formalism for the comparative molecular surface analysis: application to the comfa benchmark steroids, azo dyes, and hept derivatives	computer programs;computers in chemistry;azo dye;molecular surface	Shape analysis is a powerful tool in chemistry and drug design, and molecular surface defines shape in the molecular scale. In the current publication we presented a novel formalism for the comparative molecular surface analysis (s-CoMSA). The method enables both quantitative modeling of 3D-QSAR and finding possible pharmacophoric sites. The method provides very predictive models for the CBG activity of the benchmark steroid series, tinctorial properties of the heterocyclic azo dyes and anti-HIV activity of the HEPT series.	benchmark (computing);dyes;predictive modelling;quantitative structure-activity relationship;quantitative structure–activity relationship;semantics (computer science);shape analysis (digital geometry);steroids	Jaroslaw Polanski;Rafal Gieleciak;Tomasz Magdziarz;Andrzej Bak	2004	Journal of chemical information and computer sciences	10.1021/ci049960l	stereochemistry;chemistry;organic chemistry;computational chemistry	Graphics	12.624541915101414	-58.54186519430563	139297
c0dac2f419ef9f8bc4ca5b0d4ffdf4c7fb25fa91	alignment of 3d-structures by the method of 2d-projections	3d structure	The three-dimensional shape of a molecule can be analyzed by considering a series of two-dimensional projections. By comparing projections of two molecules a mechanism for the alignment of three-dimensional structures is derived. The efficiency of this two-dimensional comparison permits a sequential search over alignment space thereby eliminating misconvergence problems that plague many existing automated alignment procedures. Examples of molecular alignment by this method are illustrated together with several variations on the basic procedure. These variations permit a wide variety of structures to be considered. The existence of a reliable and efficient automated alignment methodology has important ramifications for QSAR studies and general chemical informatics.	2d to 3d conversion	Daniel D. Robinson;Paul D. Lyne;W. Graham Richards	1999	Journal of Chemical Information and Computer Sciences	10.1021/ci9803379	chemistry;computer science	Theory	13.465684448860737	-59.546655057125804	139459
6f91188eedcf1fa7ec884ffb10c25fbe4e00b1b6	on evaluating molecular-docking methods for pose prediction and enrichment factors. [j. chem. inf. model. 46, 401-415 (2006)]	molecular docking;enrichment factor		docking (molecular);gene ontology term enrichment;q-chem	Timothy Lovell;Hongming Chen;Paul D. Lyne;Fabrizio Giordanetto;Jin Li	2008	Journal of Chemical Information and Modeling	10.1021/ci7003169	simulation;chemistry;docking;bioinformatics;environmental chemistry;computational chemistry	NLP	10.88962596726518	-59.35141196996667	139999
aa6d5e0e85c52bbd7ec0c2433326b1aaae510938	brain inspired approach to computational face recognition	doctorate;face memory organization;computer vision;computational neuroscience;low level features;face recognition;hierarchical processing;face detection;primary visual cortex modeling		facial recognition system	Joao Paulo da Silva Gomes	2015			psychology;computer vision;face detection;speech recognition;communication	Vision	21.61891695616448	-65.64873870472647	141102
225acf605afc6d9419dedd483d9d1a98455c9e40	identification of protein interaction partners from shape complementarity molecular cross-docking		There is a growing interest in using efficient shape complementarity docking algorithms to analyze protein-protein interactions at a large scale. We have realized complete cross-docking of several tens of enzyme/inhibitors proteins. On the one hand, we demonstrate that docking score distributions for the known complexes are not distinguishable from those for the non-interacting pairs. On the other hand, we show that the knowledge of the experimental interfaces applied to the docking conformations permits to retrieve true interaction partners with high accuracy. We further identify the determinants of the molecular recognition between true interactors compared to non-interacting proteins.	algorithm;complementarity (physics);complementarity theory;docking (molecular);interaction	Elodie Laine;Alessandra Carbone	2013		10.1007/978-3-642-41190-8_34	bioinformatics	Comp.	10.345644071225149	-59.614113201694416	143392
506ab1c82b0a550e5ad3d9e90bc0db8cb505e92a	saccadic predictive vision model with a fovea		We propose a model that emulates saccades, the rapid movements of the eye, called the Error Saccade Model, based on the prediction error of the Predictive Vision Model (PVM). The Error Saccade Model carries out movements of the model's field of view to regions with the highest prediction error. Comparisons of the Error Saccade Model on Predictive Vision Models with and without a fovea show that a fovea-like structure in the input level of the PVM improves the Error Saccade Model's ability to pursue detailed objects in its view. We hypothesize that the improvement is due to poorer resolution in the periphery causing higher prediction error when an object passes, triggering a saccade to the next location.	bayesian search theory;emulator;modified huffman coding;parallel virtual machine;smoothing;unsupervised learning	Michael Hazoglou;Todd Hylton	2018	CoRR	10.1145/3229884.3229886	artificial intelligence;machine learning;saccade;computer science;saccadic masking;mean squared prediction error;radiation;neuromorphic engineering;field of view	Vision	18.813527572632957	-64.94444889051323	143762
bc12e3aca965881db2aceb4e946be7736c346f04	from fold recognition to homology modeling: an analysis of protein modeling challenges at different levels of prediction complexity	fold recognition;protein sequence;protein structure prediction;homology modeling;casp3	An analysis of different approaches to protein structure prediction is presented based solely on the range of models submitted to the third Critical Assessment of Protein Structure Prediction (CASP3) conference. CASP conferences evaluate the current state of the art of protein structure prediction by comparing blind prediction efforts of many groups for the same set of target sequences. Target sequences may be highly similar to those with known structure or can be totally (at least superficially) sequentially dissimilar. Techniques applied to those blind predictions (over 40 targets) ranges from a detailed homology prediction to the detection of remote homologues well below a twilight zone of protein sequence similarity. For the CASP3 conference, we have submitted predictions, totaling 35, with various levels of difficulty and complexity. For ten submitted homology targets, eight of them were determined by experiment so far. The RMSD of C-alpha atoms are 1.2-1.7, 2.3, and 4.6-17.9 A for the three easy targets, two hard targets, and three very hard homology targets, respectively. Out of 18-fold recognition predictions available for analysis, we got six correct predictions, five near misses, three tough near misses and four far misses. Here we analyze successes and failures of those predictions in an attempt to identify common problems and common achievements.	amino acid sequence;aspartate transaminase;casp;gene prediction;homologous gene;homology (biology);homology modeling;pierre robin syndrome;protein structure prediction;protein, organized by structure;sequence alignment;threading (protein sequence)	Krzysztof A. Olszewski;Lisa Yan;David Edwards;Tina Yeh	2000	Computers & chemistry	10.1016/S0097-8485(99)00078-9	biology;homology modeling;chemistry;computer science;bioinformatics;machine learning;protein sequencing;protein structure prediction;data mining	Comp.	10.603433409605723	-57.05783706744128	144335
6f1d6a2c8a3181c0330001e5db97750063cb98f6	application of shape similarity in pose selection and virtual screening in csardock2014 exercise	biological patents;biomedical journals;text mining;europe pubmed central;citation search;citation networks;research articles;abstracts;open access;life sciences;clinical guidelines;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	To evaluate the applicability of shape similarity in docking-based pose selection and virtual screening, we participated in the CSARdock2014 benchmark exercise for identifying the correct docking pose of inhibitors targeting factor XA, spleen tyrosine kinase, and tRNA methyltransferase. This exercise provides a valuable opportunity for researchers to test their docking programs, methods, and protocols in a blind testing environment. In the CSARdock2014 benchmark exercise, we have implemented an approach that uses ligand 3D shape similarity to facilitate docking-based pose selection and virtual screening. We showed here that ligand 3D shape similarity between bound poses could be used to identify the native-like pose from an ensemble of docking-generated poses. Our method correctly identified the native pose as the top-ranking pose for 73% of test cases in a blind testing environment. Moreover, the pose selection results also revealed an excellent correlation between ligand 3D shape similarity scores and RMSD to X-ray crystal structure ligand. In the virtual screening exercise, the average RMSD for our pose prediction was found to be 1.02 Å, and it was one of the top performances achieved in CSARdock2014 benchmark exercise. Furthermore, the inclusion of shape similarity improved virtual screening performance of docking-based scoring and ranking. The coefficient of determination (r(2)) between experimental activities and docking scores for 276 spleen tyrosine kinase inhibitors was found to be 0.365 but reached 0.614 when the ligand 3D shape similarity was included.	benchmark (computing);boat dock;coefficient of determination;crystal structure;docking (molecular);genetic selection;ligands;performance;protein d-aspartate-l-isoaspartate methyltransferase;protein tyrosine kinase;protocols documentation;score;test case;virtual screening	Ashutosh Kumar;Kam Y. J. Zhang	2016	Journal of chemical information and modeling	10.1021/acs.jcim.5b00279	text mining;medical research;computer science;bioinformatics;data science;machine learning;data mining	ML	10.698507356176458	-58.41101942506506	144349
1c67ba70278750876c678004d3f1fdad85cffe40	simplified models of protein folding exploiting the lagrange radius of gyration of the hydrophobic component	lagrange;low energy;high energy;protein structure;trypsin inhibitor;parallel computer;protein folding;funnel;hydrophobic;long range;radius of gyration;compact model	Studies are performed using an early proposed, but relatively little investigated, model that eciently emulates a hydrophobic funneling eect in protein folding. Its simple form, introduced as a further interaction term going as the square of the separation distance, is suitable for initial searches of conformational space by parallel computation and special processors which use polynomial representation of pair-wise interactions. Use of such a term implies calculation of the square of the Lagrange radius of gyration, but weighted by hydrophobicity rather than the masses of the constituent particles. The unusual choice is justi®ed by the observation that experimental protein structures have forms consistent with this Lagrange formalism for hydrophobic residues, and so compact model structures have appropriate density. However, since the long-range and square-power form strains open structures and leads to rapid generation of compact structures, such that for most of the simulation chain the movements result in intra-chain clashes, a rapid rejection algorithm is employed that prunes out similar but high energy structures. The studies also explore the choice of the simplest possible models which might be used to explore folding. Hence pancreatic trypsin inhibitor is modeled as a Ôstring-of-beadsÕ, where the beads represent residues of diering hydrophobicity. This model has only limited success, and because there are no identi®able common centers of interaction between the Ôstring-of-beadsÕ model and all-atom protein representations, it encounters the diculties: (a) of comparing such highly simpli®ed models with observed structures, and (b) of using such models as a starting point for conversion to all-atom models. The conclusion is that this solvent treatment is best applied to all-atom simulations from the outset. Nonetheless, low energy predictions obtained in this simple study can be considered as having promising features, and provide interesting insight into protein folding and the funneling contribution. Ó 2000 Elsevier Science B.V. All rights reserved. www.elsevier.com/locate/parco Parallel Computing 26 (2000) 977±998 E-mail address: robsonb@us.ibm.com (B. Robson). 0167-8191/00/$ see front matter Ó 2000 Elsevier Science B.V. All rights reserved. PII: S 0 1 6 7 8 1 9 1 ( 0 0 ) 0 0 0 2 2 3	algorithm;atom;central processing unit;computation;emoticon;emulator;formal system;intel turbo memory;interaction;parallel computing;personally identifiable information;polynomial;rejection sampling;simulation	Barry Robson	2000	Parallel Computing	10.1016/S0167-8191(00)00022-3	protein folding;protein structure;combinatorics;funnel;hydrophobe;radius of gyration	Comp.	11.791448097902604	-61.96145450858391	144549
b8f55c06d3bc75f64388ded433c1bfbbd232ba1f	multifunctionalized reduced graphene oxide biosensors for simultaneous monitoring of structural changes in amyloid-β 40	alzheimer’s disease;amyloid beta;biosensor;reduced graphene oxide(rgo)	Determination of the conformation (monomer, oligomer, or fibril) of amyloid peptide aggregates in the human brain is essential for the diagnosis and treatment of Alzheimer's disease (AD). Accordingly, systematic investigation of amyloid conformation using analytical tools is essential for precisely quantifying the relative amounts of the three conformations of amyloid peptide. Here, we developed a reduced graphene oxide (rGO) based multiplexing biosensor that could be used to monitor the relative amounts of the three conformations of various amyloid-β 40 (Aβ40) fluids. The electrical rGO biosensor was composed of a multichannel sensor array capable of individual detection of monomers, oligomers, and fibrils in a single amyloid fluid sample. From the performance test of each sensor, we showed that this method had good analytical sensitivity (1 pg/mL) and a fairly wide dynamic range (1 pg/mL to 10 ng/mL) for each conformation of Aβ40. To verify whether the rGO biosensor could be used to evaluate the relative amounts of the three conformations, various amyloid solutions (monomeric Aβ40, aggregated Aβ40, and disaggregated Aβ40 solutions) were employed. Notably, different trends in the relative amounts of the three conformations were observed in each amyloid solution, indicating that this information could serve as an important parameter in the clinical setting. Accordingly, our analytical tool could precisely detect the relative amounts of the three conformations of Aβ40 and may have potential applications as a diagnostic system for AD.	alzheimer's disease neuroimaging initiative;biosensors;dynamic range;fibril - cell component;graphene;liquid substance;monomeric gtp-binding proteins;multiplexing;population parameter;solutions;monomer	Dahye Jeong;Jinsik Kim;Myung-Sic Chae;Wonseok Lee;Seung-Hoon Yang;YoungSoo Kim;Seung Min Kim;Jin San Lee;Jeong Hoon Lee;Jungkyu Choi;Dae Sung Yoon;Kyo Seon Hwang	2018		10.3390/s18061738	analytical chemistry;oxide;amyloid beta;engineering;graphene;amyloid;biosensor;inorganic chemistry	HCI	11.756504384275582	-65.19426443171419	145670
fbeaf54377c977a0cc36c7721b30411a6d34b9ed	binding affinities of factor xa inhibitors estimated by thermodynamic integration and mm/gbsa	energie libre;proteine;termodinamica;biologia molecular;conformation;anneau;high precision;pharmaceutical technology;solvatation;tecnologia farmaceutica;farmacologia;conformacion;teoretisk kemi;molecular biology;pharmacology;precision elevee;technologie pharmaceutique;precision elevada;thermodynamique;thermodynamics;pharmacologie;proteina;energia libre;ring;protein;free energy;solvation;anillo;solvatacion;biologie moleculaire	We present free energy estimates of nine 3-amidinobenzyl-1H-indole-2-carboxamide inhibitors of factor Xa. Using alchemical thermodynamic integration (TI) calculations, we estimate the difference in binding free energies with high accuracy and precision, except for mutations involving one of the amidinobenzyl rings. Crystal studies show that the inhibitors may bind in two distinct conformations, and using TI, we show that the two conformations give a similar binding affinity. Furthermore, we show that we can reduce the computational demand, while still retaining a high accuracy and precision, by using fewer integration points and shorter protein-ligand simulations. Finally, we have compared the TI results to those obtained with the simpler MM/GBSA method (molecular-mechanics with generalized Born surface-area solvation). MM/GBSA gives better results for the mutations that involve a change of net charge, but if a precision similar to that of the TI method is required, the MM/GBSA method is actually slightly more expensive. Thus, we have shown that TI could be a valuable tool in drug design.		Samuel Genheden;Ingemar Nilsson;Ulf Ryde	2011	Journal of chemical information and modeling	10.1021/ci100458f	biochemistry;stereochemistry;chemistry;computational chemistry;solvation;ring	Comp.	12.043817011788065	-61.3178641953346	146047
1e62120192ef9aa395ac52cca19f22a4c9935dd2	cell sorting with combined optical tweezers and microfluidic chip technologies	laser power;optical tweezer;continuous flow environment;digital image processing;fluorescence;image processing;charge carrier processes;sorting;cell maximal escape velocity;microfluidic chip technologies;microfluidic chip cell sorting optical tweezers;integrated optics;sorting biomedical optical imaging biomems cellular biophysics medical image processing microfluidics radiation pressure;force;jamming;chip;optical tweezers;digital image processing technique;biotechnological research;optical imaging;radiation pressure;continuous flow;automatic cell sorting;clustering;clinical medicine;medical image processing;cell sorting;region of interest;microfluidics;tissue;optical imaging microfluidics sorting biomedical optical imaging charge carrier processes force fluorescence;multitrap parallel processing;cell cell interaction;cell cell interactions;biomedical optical imaging;high throughput;cellular biophysics;parallel processing;microfluidic chip;biomems;tissue optical tweezers microfluidic chip technologies biotechnological research clinical medicine automatic cell sorting continuous flow environment digital image processing technique cell cell interactions clustering jamming laser power cell maximal escape velocity multitrap parallel processing	Sorting of specific target cells is an important process in biotechnological research and clinical medicine. This paper proposes a methodology that integrates optical tweezers and microfluidic chip technologies to realize the automatic cell sorting in a continuous flow environment. In the proposed system, cells are driven through the region of interest, and the digital image processing technique is utilized to recognize the target cells. The optical tweezers are used to move the cells selected by image processing to the desired area. In order to move the target cells to the collection reservoir more quickly and reduce the cell-cell interactions (e.g., clustering or jamming), the motion of optical tweezers is further investigated. The relationship between the laser power and the cell maximal escape velocity has been studied to achieve the robust cell sorting with lower power. The idea of multi-trap parallel processing is proposed to achieve high throughput without losing the purity. Utilizing the proposed cell sorter, we can collect rare cells from a sample of primary tissue without expansion, with less damage and higher accuracy.	bitonic sorter;cell (microprocessor);cluster analysis;digital image processing;effective method;interaction;maximal set;parallel computing;pure function;radio jamming;region of interest;sorting;throughput;velocity (software development)	Xiaolin Wang;Zuankai Wang;Dong Sun	2010	2010 11th International Conference on Control Automation Robotics & Vision	10.1109/ICARCV.2010.5707308	parallel processing;optical tweezers;electronic engineering;image processing;computer science;engineering;digital image processing	Robotics	14.468352168857018	-65.55491431764192	146814
97895af56ae85c7534453b8a2fcc4ff510b6cbee	automatic tailoring and transplanting: a practical method that makes virtual screening more useful	ligando;industria farmaceutica;chimie informatique;preuve programme;program proof;criblage virtuel;test programa;proteine;scratch;transferases;enzyme;enzima;pharmaceutical technology;tecnologia farmaceutica;computational chemistry;industrie pharmaceutique;farmacologia;lead compound;feasibility;virtual screening;biblioteca electronica;pharmacology;ligand;rayage;zarpazo;prueba programa;technologie pharmaceutique;quimica informatica;pharmacologie;electronic library;proteina;pharmaceutical industry;test programme;protein;program test;plomo compuesto;practicabilidad;faisabilite;bibliotheque electronique;cribado virtual;compose du plomb	Docking-based virtual screening of large compound libraries has been widely applied to lead discovery in structure-based drug design. However, subsequent lead optimizations often rely on other types of computational methods, such as de novo design methods. We have developed an automatic method, namely automatic tailoring and transplanting (AutoT&T), which can effectively utilize the outcomes of virtual screening in lead optimization. This method detects suitable fragments on virtual screening hits and then transplants them onto a lead compound to generate new ligand molecules. Binding affinities, synthetic feasibilities, and drug-likeness properties are considered in the selection of final designs. In this study, our AutoT&T program was tested on three different target proteins, including p38 MAP kinase, PPAR-α, and Mcl-1. In the first two cases, AutoT&T was able to produce molecules identical or similar to known inhibitors with better potency than the given lead compound. In the third case, we demonstrated how to apply AutoT&T to design novel ligand molecules from scratch. Compared to the solutions generated by other two de novo design methods, i.e., LUDI and EA-Inventor, the solutions generated by AutoT&T were structurally more diverse and more promising in terms of binding scores in all three cases. AutoT&T also completed the assigned jobs more efficiently than LUDI and EA-Inventor by several folds. Our AutoT&T method has certain technical advantages over de novo design methods. Importantly, it expands the application of virtual screening from lead discovery to lead optimization and thus may serve as a valuable tool for many researchers.	chemical library;de novo transcriptome assembly;drug design;each (qualifier value);lead compound;lead poisoning;libraries;ligands;mathematical optimization;occupations;personnameuse - assigned;solutions;synthetic intelligence;transplantation;transplanted tissue;virtual screening	Yan Li;Yuan Zhao;Zhihai Liu;Renxiao Wang	2011	Journal of chemical information and modeling	10.1021/ci200036m	feasibility study;enzyme;chemistry;virtual screening;computational chemistry;nanotechnology;ligand	Comp.	10.790269370652275	-60.61376789205833	147081
d50c948d14df07935e945b0bb8d02712568b5cb2	rapid and accurate protein side chain prediction with local backbone information	protein design;structure refinement;energy function;local backbone features;multiclass support vector machines;protein structure;side chain prediction	High-accuracy protein structure modeling demands accurate and very fast side chain prediction since such a procedure must be repeatedly called at each step of structure refinement. Many known side chain prediction programs, such as SCWRL and TreePack, depend on the philosophy that global information and pairwise energy function must be used to achieve high accuracy. These programs are too slow to be used in the case when side chain packing has to be used thousands of times, such as protein structure refinement and protein design. We present an unexpected study that local backbone information can determine side chain conformations accurately. LocalPack, our side chain packing program which is based on only local information, achieves equal accuracy as SCWRL and TreePack, yet runs 4-14 times faster, hence providing a key missing piece in our efforts to high-accuracy protein structure modeling.	internet backbone;mathematical optimization;refinement (computing);set packing	Jing Zhang;Xin Gao;Jinbo Xu;Ming Li	2008		10.1007/978-3-540-78839-3_25	protein structure;bioinformatics;protein design	Comp.	11.08235931278645	-55.44229731665623	147275
0c633920f9641ddfdb3f36e248ca0ea7b45cc8bd	rapid process refinement for the development of complex solution copolymers	time scale;equation based dynamic modelling;molecular weight distribution;scaling up;free radical polymerization;process parameters;heat transfer;batch process;dynamic modelling;thermodynamics;resource availability;sparse matrix;batch processes;solution polymerization modelling;scale up;heat release rate;free radical;knowledge base;reaction kinetics;time constraint	The industrial manufacture of specialty free radical solution copolymers (FRPs) requires use of a variety of solvents and co-monomers based on the desired properties and end-use. The polymer composition, polymerization solvent, and process parameters have a significant impact on polymer properties such as copolymer composition and polymer molecular weight distribution (MWD). The scale-up and trouble-shooting of copolymerization processes is a significant challenge for the specialty polymers industry due to short project time-scales, limited resources available for each project, and the use of newly developed proprietary monomers. We present approaches that combine process systems and experimental approaches to support data and knowledge-based decisions in the context of live projects under commercial time constraints with only limited data available. These approaches are used for the evaluation and improvement of a batch process operating under reflux conditions that exhibits a large change in solvent boiling point due to the use of high boiling monomers. The approaches allow rapid process refinement and include thermodynamics and heat transfer considerations decoupled from the complexity of reaction kinetics and chemistry with assumptions on heat release rates. Controllability analysis can be carried out at different stages of the batch, sensitivity to the selected solvent tested, and recommendations made on solvent use and process conditions. In addition, we also present an innovative, sparse matrix-based representation of chain length dependent rates that has potential to deliver rapid solutions without loss of detail of MWD.	refinement (computing)	Yadunandan L. Dar;Tahir I. Malik	2009	Computers & Chemical Engineering	10.1016/j.compchemeng.2009.04.015	knowledge base;molar mass distribution;sparse matrix;computer science;free-radical theory of aging;radical polymerization;thermodynamics;heat transfer;polymer chemistry;batch processing	Robotics	14.491108481363161	-62.98441551060694	147294
6f6ad73d741a2f0d806cde69c1fba1c6e9d71679	an autonomous visual perception model for robots using object-based attention mechanism	image recognition;bottom up;image segmentation;saliency map;training;visual perception robotics and automation robot sensing systems layout object detection orbital robotics sun biomimetics psychology physiology;robot vision image recognition object detection;computational modeling;robot vision;automatic detection;image color analysis;feature extraction;robots;object detection autonomous visual perception model robots pre attentive segmentation post attentive recognition object based bottom up attention mechanism discrete protoobjects location based saliency map;object based attention;visual perception;visual field;object detection	The object-based attention theory has shown that perception processes only select one object of interest from the world at a time which is then represented for action. This paper therefore presents an autonomous visual perception model for robots by simulating the object-based bottom-up attention mechanism. Using this model visual perception of robots starts from attentional selection over the scene followed by high-level analysis only on the attended object. The proposed model involves three components: pre-attentive segmentation, bottom-up attentional selection and post-attentive recognition and learning of the attended object. The model pre-attentively segments the visual field into discrete proto-objects at first. Automatic bottom-up competition is then performed to yield a location-based saliency map. By combination of location-based salience within each proto-object, the proto-object based salience is evaluated. The most salient proto-object is selected for recognition and learning. This model has been applied into the robotic task of automatical detection of objects. Experimental results in natural and cluttered scenes are shown to validate this model.	algorithm;autonomous robot;bottom-up proteomics;color vision;decimation (signal processing);high- and low-level;internationalization and localization;location-based service;object-based language;simulation	Yuanlong Yu;George K. I. Mann;Ray G. Gosine	2009	2009 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2009.5420945	robot;computer vision;simulation;object model;visual perception;feature extraction;computer science;top-down and bottom-up design;image segmentation;computational model	Robotics	22.83314245279474	-65.15322314502443	147303
7f808ab080a7efb71d91d306001ec5793594589a	bond-based global and local (bond, group and bond-type) quadratic indices and their applications to computer-aided molecular design, 1. qspr studies of diverse sets of organic chemicals	physicochemical properties;quantitative structure property relationship;stochastic matrix;physicochemical property;tomocomd cardd software;statistical significance;edge adjacency matrix;octane isomers;rate constant;drug design;computer aided molecular design;molecular descriptor;alkyl alcohol;2 furylethylene;qspr model;cross validation;non stochastic and stochastic bond based quadratic indices;adjacency matrix;leave one out;qspr study	"""The concept of atom-based quadratic indices is extended to a series of molecular descriptors (MDs) (both total and local) based on adjacency between edges. The kth edge-adjacency matrix (E ( k )) denotes the matrix of bond-based quadratic indices (non-stochastic) with respect to the canonical basis set. The kth """"stochastic"""" edge-adjacency matrix, ES ( k ), is here proposed as a new molecular representation easily calculated from E ( k ). Then, the kth stochastic bond-based quadratic indices are calculated using ES ( k ) as operators of quadratic transformations. The study of six representative physicochemical properties of octane isomers was used to compare the ability of both series of MDs to produce significant quantitative structure-property relationship (QSPR) models. Moreover, the general performance of the new MDs in this QSPR study has been evaluated with respect to other 2D/3D well-known sets of indices and the obtained results shown a quite satisfactory behavior of the present method. The novel bond-level MDs were also used for the description and prediction of the boiling point of 28 alkyl-alcohols and to the modeling of the specific rate constant (log k) of 34 derivatives of 2-furylethylenes. These models were statistically significant and showed very good stability to data variation in leave-one-out (LOO) cross-validation experiment. The comparison with other approaches (edge- and vertices-based connectivity indices, total and local spectral moments, and quantum chemical descriptors as well as E-state/biomolecular encounter parameters) expose a good behavior of our method in this QSPR studies. The approach described in this report appears to be a very promising structural invariant, useful for QSPR/QSAR studies, similarity/diversity analysis, and computer-aided """"rational"""" molecular (drug) design."""	adjacency matrix;alcohols;bond;basis set (chemistry);cross reactions;cross-validation (statistics);isomerism;molecular descriptor;organic chemicals;quantitative structure-activity relationship;quantitative structure–activity relationship;sgi octane;the matrix;cell transformation;perfluorooctane	Yovani Marrero-Ponce;Francisco Torrens;Ysaias J. Alvarado;Richard Rotondo	2006	Journal of computer-aided molecular design	10.1007/s10822-006-9089-4	molecular descriptor;chemistry;computational chemistry;stochastic matrix;statistical significance;reaction rate constant;drug design;cross-validation;adjacency matrix;statistics	ML	13.02106729067784	-58.10329119181195	147960
d3f5b4b9fa195f2fab5eb21b59cdd59fc9bd99ae	combinatorial qsar modeling of p-glycoprotein substrates	computer programs;p glycoprotein;computers in chemistry	"""Quantitative structure-activity (property) relationship (QSAR/QSPR) models are typically generated with a single modeling technique using one type of molecular descriptors. Recently, we have begun to explore a combinatorial QSAR approach which employs various combinations of optimization methods and descriptor types and includes rigorous and consistent model validation (Kovatcheva, A.; Golbraikh, A.; Oloff, S.; Xiao, Y.; Zheng, W.; Wolschann, P.; Buchbauer, G.; Tropsha, A. Combinatorial QSAR of Ambergris Fragrance Compounds. J. Chem. Inf. Comput. Sci. 2004, 44, 582-95). Herein, we have applied this approach to a data set of 195 diverse substrates and nonsubstrates of P-glycoprotein (P-gp) that plays a crucial role in drug resistance. Modeling methods included k-nearest neighbors classification, decision tree, binary QSAR, and support vector machines (SVM). Descriptor sets included molecular connectivity indices, atom pair (AP) descriptors, VolSurf descriptors, and molecular operation environment descriptors. Each descriptor type was used with every QSAR modeling technique; so, in total, 16 combinations of techniques and descriptor types have been considered. Although all combinations resulted in models with a high correct classification rate for the training set (CCR(train)), not all of them had high classification accuracy for the test set (CCR(test)). Thus, predictive models have been generated only for some combinations of the methods and descriptor types, and the best models were obtained using SVM classification with either AP or VolSurf descriptors; they were characterized by CCR(train) = 0.94 and 0.88 and CCR(test) = 0.81 and 0.81, respectively. The combinatorial QSAR approach identified models with higher predictive accuracy than those reported previously for the same data set. We suggest that, in the absence of any universally applicable """"one-for-all"""" QSAR methodology, the combinatorial QSAR approach should become the standard practice in QSPR/QSAR modeling."""		Patricia de Cerqueira Lima;Alexander Golbraikh;Scott Oloff;Yunde Xiao;Alexander Tropsha	2006	Journal of chemical information and modeling	10.1021/ci0504317	chemistry;bioinformatics;machine learning;combinatorial chemistry	Comp.	11.745870017538483	-56.97325194285546	148178
3a1129c17e617beb9695a112bd5bd22a40c6a5c2	selecting an e-learning solution, part 2: avoiding common rfp mistakes	e-learning solution;common rfp mistake	The reticulocyte content present in a specimen of red blood cells is quantitatively measured based upon the selective immunoreactivity of the reticulocyte portion of the specimen with a reticulocyte-specific antibody which is immunoreactive with proteinaceous material associated with reticulocytes but not associated with mature red blood cells. Such immunoreactive proteinaceous material may be transferrin, transferrin receptor, transcobalamin II, or transcobalamin II receptor. Various procedures are described for quantitating such selective immunoreactivity, including fluorescent and radioactive detection techniques employing direct or indirect fluorescent or radioactive labeling of the reticulocyte-specific antibody.		Karl M. Kapp	2004	eLearn Magazine	10.1145/1037183.1037185	operations management;operations research	NLP	11.723467572568973	-63.26087767713238	148222
6cef7de100ad1df8b16326750c094c2d0f212e2e	icga-pso-elm approach for accurate multiclass cancer classification resulting in reduced gene sets in which genes encoding secreted proteins are highly represented	icga pso elm approach;neural nets biology and genetics classifier design and evaluation feature evaluation and selection;tumor survival;cancer;neural nets;neural network based extreme learning machine gene selection;systems biology;tumor growth icga pso elm approach multiclass cancer classification reduced gene sets secreted proteins encoding integer coded genetic algorithm particle swarm optimization neural network based extreme learning machine gene selection systems biology cell signaling cell proliferation tumor survival;tumours;tumor growth;genetics;feature evaluation and selection;particle swarm optimizer;neural net;tumours bioinformatics cancer genetic algorithms genetics learning artificial intelligence neural nets particle swarm optimisation pattern classification proteins;proteins;extreme learning machine;multiclass cancer classification;biology and genetics;particle swarm optimization;classifier design and evaluation;classification algorithms;system biology;pattern classification;tumors;reduced gene sets;integer coded genetic algorithm;cell proliferation;biological information theory;genetic algorithm;genetic algorithms;sparse data;algorithms artificial intelligence gene expression profiling gene regulatory networks humans neoplasm proteins neoplasms pattern recognition automated;neurons;cancer classification;learning artificial intelligence;gene selection;cancer encoding proteins neoplasms cells biology biological information theory genetic algorithms particle swarm optimization neural networks machine learning;particle swarm optimisation;algorithm design and analysis;cell signaling;secreted proteins encoding;neural network;bioinformatics	A combination of Integer-Coded Genetic Algorithm (ICGA) and Particle Swarm Optimization (PSO), coupled with the neural-network-based Extreme Learning Machine (ELM), is used for gene selection and cancer classification. ICGA is used with PSO-ELM to select an optimal set of genes, which is then used to build a classifier to develop an algorithm (ICGA_PSO_ELM) that can handle sparse data and sample imbalance. We evaluate the performance of ICGA-PSO-ELM and compare our results with existing methods in the literature. An investigation into the functions of the selected genes, using a systems biology approach, revealed that many of the identified genes are involved in cell signaling and proliferation. An analysis of these gene sets shows a larger representation of genes that encode secreted proteins than found in randomly selected gene sets. Secreted proteins constitute a major means by which cells interact with their surroundings. Mounting biological evidence has identified the tumor microenvironment as a critical factor that determines tumor survival and growth. Thus, the genes identified by this study that encode secreted proteins might provide important insights to the nature of the critical biological features in the microenvironment of each tumor type that allow these cells to thrive and proliferate.	american elm pollen extract 50 mg/ml injectable solution;artificial neural network;cell secretion;cell signaling;encode (action);genetic algorithm;habitat;international computer games association;large;learning disorders;neoplasms;particle swarm optimization;randomness;sparse matrix;systems biology;tumor microenvironment	Saras Saraswathi;Sundaram Suresh;Narasimhan Sundararajan;Michael T. Zimmermann;Marit Nilsen-Hamilton	2011	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2010.13	biology;genetic algorithm;computer science;bioinformatics;artificial intelligence;machine learning;artificial neural network	Comp.	10.636707277746304	-53.82036840125511	148223
a861c2c6d7950018b3f949c32827be51b3e253a0	learning with hidden variables		Learning and inferring features that generate sensory input is a task continuously performed by cortex. In recent years, novel algorithms and learning rules have been proposed that allow neural network models to learn such features from natural images, written text, audio signals, etc. These networks usually involve deep architectures with many layers of hidden neurons. Here we review recent advancements in this area emphasizing, amongst other things, the processing of dynamical inputs by networks with hidden nodes and the role of single neuron models. These points and the questions they arise can provide conceptual advancements in understanding of learning in the cortex and the relationship between machine learning approaches to learning with hidden nodes and those in cortical circuits.	acclimatization;algorithm;architecture as topic;artificial neural network;audio media;computation;computational model;deep learning;dropout (neural networks);eighty;hidden variable theory;imagenet;machine learning;neural network simulation;neurobiology;neuron;neurons;neuroscience discipline;plausibility structure;randomness;rectifier (neural networks);relevance;rule (guideline);saturated;seventy nine;transfer function;anatomical layer	Yasser Roudi;Graham W. Taylor	2015	Current Opinion in Neurobiology	10.1016/j.conb.2015.07.006	unsupervised learning;feature learning;machine learning;competitive learning;communication	ML	19.13859827200229	-65.83721225687306	148235
ffb530f08d6fadec69eda89d0423409e1e2ea6ac	a novel method for accurate one-dimensional protein structure prediction based on fragment matching	prediccion;proteine;estructura;biokemi;methode;structural biology;molecular biology;bioinformatik berakningsbiologi;protein structure prediction;strukturbiologi;proteina;molekylarbiologi;protein;metodo;biochemistry and molecular biology;prediction;method;structure;biochemistry;bioinformatics computational biology;biokemi och molekylarbiologi	MOTIVATION The precise prediction of one-dimensional (1D) protein structure as represented by the protein secondary structure and 1D string of discrete state of dihedral angles (i.e. Shape Strings) is a prerequisite for the successful prediction of three-dimensional (3D) structure as well as protein-protein interaction. We have developed a novel 1D structure prediction method, called Frag1D, based on a straightforward fragment matching algorithm and demonstrated its success in the prediction of three sets of 1D structural alphabets, i.e. the classical three-state secondary structure, three- and eight-state Shape Strings.   RESULTS By exploiting the vast protein sequence and protein structure data available, we have brought secondary-structure prediction closer to the expected theoretical limit. When tested by a leave-one-out cross validation on a non-redundant set of PDB cutting at 30% sequence identity containing 5860 protein chains, the overall per-residue accuracy for secondary-structure prediction, i.e. Q3 is 82.9%. The overall per-residue accuracy for three- and eight-state Shape Strings are 85.1 and 71.5%, respectively. We have also benchmarked our program with the latest version of PSIPRED for secondary structure prediction and our program predicted 0.3% better in Q3 when tested on 2241 chains with the same training set. For Shape Strings, we compared our method with a recently published method with the same dataset and definition as used by that method. Our program predicted at 2.2% better in accuracy for three-state Shape Strings. By quantitatively investigating the effect of data base size on 1D structure prediction we show that the accuracy increases by approximately 1% with every doubling of the database size.	algorithm;alphabet;amino acid sequence;benchmark (computing);bioinformatics;cross reactions;cross-validation (statistics);database;david jones (programmer);doubling;jones calculus;matching;psipred;period-doubling bifurcation;protein data bank;protein structure prediction;protein, organized by structure;scientific publication;sequence alignment;silo (dataset);test set;three-state logic;protein protein interaction	Tuping Zhou;Nanjiang Shu;Sven Hovmöller	2010	Bioinformatics	10.1093/bioinformatics/btp679	biology;structure;method;prediction;computer science;bioinformatics;artificial intelligence;protein structure prediction;mathematics;structural biology;algorithm	Comp.	10.098997667340388	-54.58825883884587	148538
f9eb164c8535d0d4e6c594212d36d1dd98eb444f	deep salience: visual salience modeling via deep belief propagation	markove random field;hierachical image analysis;unsupervised object detection;visual salience	Visual salience is an intriguing phenomenon observed in biological neural systems. Numerous attempts have been made to model visual salience mathematically using various feature contrasts, either locally or globally. However, these algorithmic models tend to ignore the   problem’s  biological solutions, in which visual salience appears to arise during the propagation of visual stimuli along the visual cortex. In this paper, inspired by the conjecture that salience arises from deep propagation along the visual cortex, we present a Deep Salience model where a multi-layer model based on successive Markov random fields (sMRF) is proposed to analyze the input image successively through its deep belief propagation. As a result, the foreground object can be automatically separated from the background in a fully unsupervised way. Experimental evaluation on the benchmark dataset validated that our Deep Salience model can consistently outperform eleven state-of-the-art salience models, yielding the higher rates in the precision-recall tests and attaining the best F-measure and mean-square error in the experiments. Introduction Automated detection of visual objects in images and videos is a subject of primary interest because of its wide application in image/video indexing, content-aware editing, medical image analysis, intelligent computer-human interface, robotic vision, and visual surveillance. Researchers in artificial intelligence and computer vision have successfully developed a number of methods for object detection, such as AdaBoost face detection (Viola and Jones 2001), SVM-based human detection (Vedalid et al 2009; Dalal and Triggs 2005), and min-cut object segmentation (Rother et al 2004). These approaches usually depend on training on predefined datasets, or on user input such as scribbles or trimaps. However, when no prior knowledge of image content is available, unsupervised object detection is a hard problem, and it has attracted considerable interest from the research community. The past decade has seen consistent progress towards unsupervised image segmentation and object detection. A widely-adopted approach is to consider an image principally as a set of hierarchical contours (Arbelaez et al 2011). This assumes that semantic content and the objects usually correspond to specific image structures. Recent research (Farabet et al 2013; Kohli et al 2013) has also suggested that this hierarchical view of image content may be correlated to the deep learning of image structures (Hinton et al 2006). In a development of this approach, we propose in this paper a Deep Salience model based on successive Markov Random Fields (sMRF) for unsupervised object detection. Our work is conceptually related to the recent pioneering work on hierarchical image analysis (Farabet et al 2013; Kohli et al 2013). Unsupervised object detection usually leads to the topic of visual salience, which stems from psychological research on biological visual perception (Koch and Ullman 1985; Itti and Koch 2001). The earliest bio-inspired computational salience model was proposed by Koch and Ullman (1985), where the contrast between visual stimuli (pixels) was considered as the origin of salience awareness. A number of publications (Itti et al 1998; Ma and Zhang 2003; Harel et al 2006; Hou and Zhang 2007; Judd 2009) have followed this roadmap to develop their salience models using a variety of features. These methods are usually based on local contrast and tend to produce higher salience values near edges instead of uniformly highlighting salient objects. Cheng et al (2011) categorized these approaches as local approaches. Recent efforts have been made towards using global contrasts, where pixels or regions were evaluated with respect to the entire image. Achanta et al (2009) proposed a frequency tuned method that defines pixel salience using region-averaged contrast. Goferman et al (2012) used block-based global contrast. Cheng et al (2011) extend   Achanta’s   method   to   region-based salience estimation. Perazzi et al (2012) further extend this regioncontrast approach by leveraging superpixels. Jiang and Crookes (2012) used mutual information (MI) evaluation with a center-surround a priori map for global salience estimation. Peng et al (2013) introduced the low-rank matrix computation for salience modeling. In summary, both local and global methods have been based on modeling salience using various visual contrast definitions with various features based on pixels, blocks or regions. From a biological viewpoint, we consider the conjecture that human visual salience is a consequence of the deep Copyright © 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence	adaboost;approximation;artificial intelligence;belief propagation;benchmark (computing);british informatics olympiad;categorization;computation;computer vision;deep learning;emoticon;experiment;f1 score;face detection;human–computer interaction;image analysis;image segmentation;jones calculus;koch snowflake;layer (electronics);markov chain;markov random field;mean squared error;medical image computing;minimum cut;mutual information;numerical linear algebra;object detection;pixel;robot;scope (computer science);sethi–ullman algorithm;software propagation;unsupervised learning;user interface;visual objects;word lists by frequency	Richard M. Jiang;Danny Crookes	2014			computer vision;machine learning	AI	24.06203577566486	-56.04439517887819	149468
0ee6064ae5ae95682d85b2c821456dbd9049728f	quantitative comparison of linear and non-linear dimensionality reduction techniques for solar image archives		This work investigates the applicability of several dimensionality reduction techniques for large scale solar data analysis. Using the first solar domain-specific benchmark dataset that contains images of multiple types of phenomena, we investigate linear and nonlinear dimensionality reduction methods in order to reduce our storage costs and maintain an accurate representation of our data in a new vector space. We present a comparative analysis between several dimensionality reduction methods and different numbers of target dimensions by utilizing different classifiers in order to determine the percentage of dimensionality reduction that can be achieved on solar data with said methods, and to discover the method that is the most effective for solar images. Introduction In this work, we present our dimensionality reduction analysis aimed towards the ambitious goal of building a largescale Content Based Image Retrieval (CBIR) system for the Solar Dynamics Observatory (SDO) mission [1]. Our motivation for this work comes from the fact that with the large amounts of data that the SDO mission started transmitting, hand labeling (commonly used by solar physicist in the last decades) of these images is simply impossible. There have been several successful CBIR systems for medical images [2] as well as in other domains [3]; none of them, however, have dealt with the volume of data that the SDO mission generates. This NASA mission, only with its Atmospheric Imaging Assembly (AIA), generates eight 4096 pixels x 4096 pixels images every 10 seconds. This leads to a data transmission rate of approximately 700 gigabytes per day only from the AIA component (the entire mission is expected to be sending about 1.5 terabytes of data per day, for a minimum of 5 years). With such a massive pipeline choosing redundant dimensions on our data will lead to unnecessary data storage, and high search and retrieval costs in our repository. Based on these complications, one of the main goals of this work is to determine the percentage of dimensionality reduction we can achieve using the best methods while maintaining a Copyright © 2012, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. high quality parameter-based representation of the solar images. Dimensionality reduction methods have been shown to produce accurate representations of high dimensional data in a lower dimensional space with very domain specific results in other image retrieval application domains [4-8]. In this work we investigate four linear and four non-linear dimensionality reduction methods with eight different numbers of target dimensions as parameters for each, in order to present a comparative analysis. The novelty of our work is to determine which dimensionality reduction methods produce the best and most consistent results and with which classifiers, on our specific image parameters selected for solar data [17]. Due to domain-specific results, reported by multiple-researchers working on dimensionality reduction in the past [4-8], we believe our results will be of special interest to researchers from the field of medical image analysis, as these images seem to be the closest to our dataset [21. We also identify some interesting combinations of dimensionality reduction methods and classifiers that behave differently across the presented datasets. Our research problem in Solar physics is of great practical relevance for Earth's climate since solar flares endanger the lives of passengers on commercial airline routes going over the poles, interrupt radio communications in bands the military uses, can (and have) knocked down power grids. The systematic feature recognition and the study of the metadata, is a key component of the ultimate prediction of solar activity (space weather). The rest of the paper is organized in the following way: A background overview is presented in the following section. After that we present an overview of the steps and experiments we performed together with our observations. The last includes our conclusions and the future work we propose to complete in order to prepare all parts of a solar CBIR system for integration.	artificial intelligence;benchmark (computing);computational physics;computer data storage;content-based image retrieval;display resolution;experiment;feature recognition;gigabyte;image analysis;medical image computing;medical imaging;nonlinear dimensionality reduction;nonlinear system;pixel;qualitative comparative analysis;relevance;terabyte;transmitter	Juan M. Banda;Rafal A. Angryk;Petrus C. Martens	2012			artificial intelligence;machine learning;dimensionality reduction;nonlinear system;computer science;vector space;nonlinear dimensionality reduction	AI	20.97688521850927	-56.83280106053958	149748
b789009eeddc15af9d1af328a9dbfddc7fa7eb91	integrating binding site predictions using non-linear classification methods	binding site;sampling technique;support vector machine	Currently the best algorithms for transcription factor binding site prediction are severely limited in accuracy. There is good reason to believe that predictions from these different classes of algorithms could be used in conjunction to improve the quality of predictions. In this paper, we apply single layer networks, rules sets and support vector machines on predictions from 12 key algorithms. Furthermore, we use a ‘window’ of consecutive results in the input vector in order to contextualise the neighbouring results. Moreover, we improve the classification result with the aid of underand oversampling techniques. We find that support vector machines outperform each of the original individual algorithms and other classifiers employed in this work with both type of inputs, in that they maintain a better tradeoff between recall and precision.	algorithm;dna binding site;oversampling;precision and recall;support vector machine;transcription (software)	Yi Sun;Mark Robinson;Rod Adams;Paul Kaye;Alistair G. Rust;Neil Davey	2004		10.1007/11559887_14	computer science;machine learning;pattern recognition;data mining	ML	10.408703770570193	-53.510832924733016	149824
a8c4018c0d43c79ae987f42ee58ceeb9c4def545	neural grammar networks in qsar chemistry	biology computing;quantitative structure activity relationship;pediatrics;recursive neural networks;neural nets;training;r squared scores neural grammar networks qsar chemistry quantitative structure activity relationship computational chemistry formal string representation molecule protein;neural grammar networks;testing;computational chemistry;learning systems;artificial neural networks;cheminformatics;proteins;cheminformatics artificial neural networks neural grammar network qsar;machine learning;r squared scores;molecular biophysics;proteins biochemistry biology computing molecular biophysics neural nets;generating function;molecule;qsar;protein;chemistry next generation networking biological system modeling testing computer networks neural networks biology computing learning systems biomedical computing information science;neural grammar network;qsar chemistry;next generation networking;formal string representation;biochemistry;artificial neural network	In this paper, we describe the Neural Grammar Network (NGN) and its application to Quantitative Structure-Activity Relationship (QSAR) in computational chemistry. The NGN is a novel machine learning device that applies the generic function approximation capability of a dynamic recursive neural network to the syntactic structure of a parsed string. In our QSAR task, we represent each molecule by a formal string representation (SMILES and InChI), and utilize an NGN instance to associate each with a real-value that describes the degree of binding, inhibition or affinity a given molecule has with a target protein. We find that the NGN can on average outperform previous work in regression tasks, yielding performances of up to 0.79 (sd = 0.23) in predictive r-squared scores and up to 74.8 (sd = 1.63) percent concordance in classification tasks.	approximation;artificial neural network;coefficient of determination;computational chemistry;concordance (publishing);experiment;formal grammar;formal language;generic function;inchi;machine learning;next-generation network;parsing;performance;processor affinity;quantitative structure–activity relationship;recursion;recursive neural network;simplified molecular-input line-entry system;subject-matter expert	Eddie Y. T. Ma;Stefan C. Kremer	2009	2009 IEEE International Conference on Bioinformatics and Biomedicine	10.1109/BIBM.2009.60	computer science;bioinformatics;artificial intelligence;theoretical computer science;machine learning;quantitative structure–activity relationship;artificial neural network	Robotics	13.055789185394529	-56.03400071207645	150127
1a0e3fada30a1bb41b5e3fb4e9c2c97b2cd8e2ab	this looks like that: deep learning for interpretable image recognition		When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our final decision. In this work, we introduce a deep network architecture that reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification. The model thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, geologists, architects, and others would explain to people on how to solve challenging image classification tasks. The network uses only image-level labels for training, meaning that there are no labels for parts of images. We demonstrate our method on the CUB-200-2011 dataset and the CBIS-DDSM dataset. Our experiments show that our interpretable network can achieve comparable accuracy with its analogous standard non-interpretable counterpart as well as other interpretable deep models.		Chaofan Chen;Oscar Li;Alina Barnett;Jonathan Su;Cynthia Rudin	2018	CoRR		machine learning;deep learning;network architecture;artificial intelligence;mathematics;contextual image classification	ML	22.854929278630234	-53.25993113471585	150182
781b0ddf73d1f6631381cc12c99ef0a3e61cb653	biologically-inspired human motion detection	feed forward;correspondence problem;neural network;biological motion;information system	A model of motion detection is described, inspired by the capability of humans to recognise biological motion even from minimal information systems such as moving light displays. The model, a feed-forward backpropagation neural network, uses labelled joint data, analogous to light points in such displays. In preliminary work, the model achieves 100% person classification on a set of 4 artificial subjects and another of 4 real subjects. Subsequently, 100% motion detection is achieved on a set of 21 subjects. In the latter case, the correspondence problem is also solved by the model, since the network is not ‘told’ which joint is which. Like human beings, the neural networks perform both tasks within a small fraction of the gait cycle.	artificial neural network;backpropagation;cognitive model;correspondence problem;experiment;information system;kinesiology;motion detector;resultant	Vijay Laxmi;John N. Carter;Robert I. Damper	2002			machine learning;artificial neural network;artificial intelligence;feed forward;motion detection;information system;biological motion;correspondence problem;gait;pattern recognition;backpropagation;computer science	ML	22.221020538905286	-63.86629223450127	150718
81c07640997cdec9911f0419301083a7085a9ecf	a 3d model of cyp1b1 explains the dominant 4-hydroxylation of estradiol	modelizacion;alignement sequence;alineacion secuencia;modelisation;citocromo p450;3d model;metabolismo;cytochrome p450;17β estradiol;hydroxylation;sequence alignment;17β oestradiol;hidroxilacion;modeling;metabolism;metabolisme	CYP1A1 and CYP1A2 exhibit catalytic activity predominantly for the 2-hydroxylation of estradiol, whereas CYP1B1 exhibits catalytic activity predominantly for 4-hydroxylation of estradiol. To understand why CYP1B1 predominantly hydroxylates the 4-position of estradiol, we constructed three-dimensional structures of CYP1A1 and CYP1B1 by homology modeling, using the crystal structure of CYP1A2, and studied the docking mode of estradiol with CYP1A1, CYP1A2, and CYP1B1. The results demonstrated that two particular amino acid residues for each CYP, namely Thr124 and Phe260 of CYP1A2, Ser122 and Phe258 of CYP1A1, and Ala133 and Asn265 of CYP1B1, play an important role in estradiol recognition.	3d modeling;amino acids;boat dock;crystal structure;cytochrome p-450 cyp1a1;docking (molecular);estradiol;exhibits as topic;homology (biology);homology modeling;cytochrome p-450 cyp1b1;enzyme activity	Toshimasa Itoh;Hitomi Takemura;Kayoko Shimoi;Keiko Yamamoto	2010	Journal of chemical information and modeling	10.1021/ci1000554	biology;endocrinology;biochemistry;stereochemistry;systems modeling;chemistry;cytochrome p450;sequence alignment;metabolism	ML	10.168744197030845	-61.64204956728557	151057
9cd808152d5aa44af6bb64040329de905cad7123	visual selective attention model considering bottom-up saliency and psychological distance	bottom up;saliency map;affective saliency map;psychological factor;psychological distance;visual features;visual perception;selective attention;bottom up saliency map;reaction time	Congruency or incongruency between a psychological distance and a spatial distance for given visual stimuli can affect reaction time to corresponding visual stimuli in the course of visual perception. Human reacts more rapidly for congruent stimuli than incongruent one. More rapid reaction to visual stimuli is related with higher selectivity property in terms of visual selective attention. Based on this psychological evidence, we propose a new visual selective attention model reflecting the congruency or incongruency between a psychological distance and a spatial distance of visual stimuli as well as bottom-up saliency generated by spatial relativity of primitive visual features. The proposed visual selective attention model can generate more human like visual scan path by considering a psychological factor, which can focus on congruent visual stimuli with higher priority than incongruent one.	top-down and bottom-up design	Young-Min Jang;Sang-Woo Ban;Minho Lee	2010		10.1007/978-3-642-17537-4_26	mental chronometry;computer vision;attention;visual search;visual perception;top-down and bottom-up design;construal level theory;n2pc	AI	22.984591184338605	-65.60132848082847	151260
1b841f4ab8f3065abf9b0c08b7a84036850e04cf	in silico fragment-based drug discovery: setup and validation of a fragment-to-lead computational protocol using s4mple		"""This paper describes the use and validation of S4MPLE in Fragment-Based Drug Design (FBDD)--a strategy to build drug-like ligands starting from small compounds called fragments. S4MPLE is a conformational sampling tool based on a hybrid genetic algorithm that is able to simulate one (conformer enumeration) or more molecules (docking). The goal of the current paper is to show that due to the judicious design of genetic operators, S4MPLE may be used without any specific adaptation as an in silico FBDD tool. Such fragment-to-lead evolution involves either growing of one or linking of several fragment-like binder(s). The native ability to specifically """"dock"""" a substructure that is covalently anchored to its target (here, some prepositioned fragment formally part of the binding site) enables it to act like dedicated de novo builders and differentiates it from most classical docking tools, which may only cope with non-covalent interactions. Besides, S4MPLE may address growing/linking scenarios involving protein site flexibility, and it might also suggest """"growth"""" moves by bridging the ligand to the site via water-mediated interactions if H2O molecules are simply appended to the input files. Therefore, the only development overhead required to build a virtual fragment→ligand growing/linking strategy based on S4MPLE were two chemoinformatics programs meant to provide a minimalistic management of the linker library. The first creates a duplicate-free library by fragmenting a compound database, whereas the second builds new compounds, attaching chemically compatible linkers to the starting fragments. S4MPLE is subsequently used to probe the optimal placement of the linkers within the binding site, with initial restraints on atoms from initial fragments, followed by an optimization of all kept poses after restraint removal. Ranking is mainly based on two criteria: force-field potential energy and RMSD shifts of the original fragment moieties. This strategy was applied to several examples from the FBDD literature with good results over several monitored criteria: ability to generate the optimized ligand (or close analogs), good ranking of analogs among decoy compounds, and accurate predictions of expected binding modes of reference ligands. Simulations included """"classical"""" covalent growing/linking, more challenging ones involving binding site conformational changes, and growth with optional recognition of putatively favorable water-mediated interactions."""		Laurent Hoffer;Jean-Paul Renaud;Dragos Horvath	2013	Journal of chemical information and modeling	10.1021/ci4000163	simulation;bioinformatics;nanotechnology	Comp.	11.363630728092788	-60.44295782420312	151300
beab8f01bfacf50df08f2cba82c10e9d84a5cd35	generalization, segmentation and classification of qualitative motion data	motion perception	"""The analysis and modelling of processes involved in motion perception is the aim of our interdisciplinary cooperation between psychophysics and computer science 3. This paper proposes a qualitative representation of the course of motion that was the outcome of the computer science part of the project. A course of motion is described at two diierent layers that provide diierent levels of granularity, abstraction and accuracy. A vectorial representation at the lower level is qualitative, but still quite accurate and allows switches between deictic and intrinsic frames of reference. A propositional representation at the higher level is more abstract, less accurate and has coarser granularity, but reeects something of the \semantics"""" of the course of motion, in that it identiies meaningful subsequences in the vectorial representation and sums them up to a higher structure. The propositional layer is constructed from the vectorial layer by segmentation of the vector sequence and classiication of the subsequences according to their shape."""	computer science;network switch	Alexandra Musto;Klaus Stein;Andreas Eisenkolb;Kerstin Schill;Wilfried Brauer	1998			machine learning;motion perception;scale-space segmentation;computer science;computer vision;artificial intelligence;pattern recognition;segmentation	AI	20.718931693153376	-64.86604848423171	151360
108d545e27e070903674344f3f9862c0010c4d4d	e-state modeling of hiv-1 protease inhibitor binding independent of 3d information	hiv 1 protease	Data for HIV-1 protease inhibitors (in vitro enzyme binding) were used as a training set to develop a QSAR model based on topological descriptors, including two hydrogen E-state indices, along with a molecular connectivity chi and a kappa shape index. A statistically satisfactory four-variable model was obtained for the 32 compounds in the training set, r2 = 0.86, s = 0.60, and q2 = 0.79, without the use of information from 3D geometries or detailed interaction energy calculations. The model was validated through the prediction of 15 compounds in the external test set, yielding a mean absolute error, MAE, = 0.82. Structure interpretation is given for each variable to assist in the design of new compounds. Structure features emphasized in the model include hydrogen bond donating ability, nonpolar groups, skeletal branching, and molecular globularity. On the basis of these statistical criteria, this E-state model may be considered useful for prediction of pIC50 values for new HIV-1 protease inhibitors.	approximation error;endopeptidases;hiv infections;hydrogen bonding;interaction energy;mav protocol;protease inhibitors;quantitative structure-activity relationship;quantitative structure–activity relationship;test set	Hlaing Hlaing Maw;Lowell H. Hall	2002	Journal of chemical information and computer sciences	10.1021/ci010091z	crystallography;stereochemistry;chemistry;computer science;computational chemistry	Comp.	12.04845221713835	-58.56247465365682	151433
51f2d93006a2706cfc0708631e2ba60f6e980a50	application of conformal prediction in qsar	medical and health sciences;medicin och halsovetenskap	QSAR modeling is a method for predicting properties, e.g. the solubility or toxicity, of chemical compounds using statistical learning techniques. QSAR is in widespread use within the pharmaceutical industry to prioritize compounds for experimental testing or to alert for potential toxicity. However, predictions from a QSAR model are difficult to assess if their prediction intervals are unknown. In this paper we introduce conformal prediction into the QSAR field to address this issue. We apply support vector machine regression in combination with two nonconformity measures to five datasets of different sizes to demonstrate the usefulness of conformal prediction in QSAR modeling. One of the nonconformity measures provides prediction intervals with almost the same width as the size of the QSAR models’ prediction errors, showing that the prediction intervals obtained by conformal prediction are efficient and useful.	machine learning;quantitative structure–activity relationship;support vector machine	Martin Eklund;Ulf Norinder;Scott Boyer;Lars Carlsson	2012		10.1007/978-3-642-33412-2_17	computer science;applicability domain;machine learning	ML	11.62531109740884	-56.12698438631749	151598
a50dfff6c5f29c01033d3d3ed40fb3b37c909c4c	risk estimation in spatial disease clusters: an rbf network approach	estimation theory;pattern clustering;risk analysis diseases estimation theory geographic information systems medical computing monte carlo methods pattern clustering radial basis function networks;neural networks;risk analysis;uncertainty interval neural networks hypothesis testing disease cluster;uncertainty interval;medical computing;spatial cluster detection risk estimation spatial disease clusters rbf network approach geographical coordinates monte carlo simulation probability;radial basis function networks;disease cluster;geographic information systems;hypothesis testing;diseases;sociology radial basis function networks monte carlo methods diseases upper bound estimation;monte carlo methods	This paper proposes a method which is suitable for the estimation of the probability of occurrence of a syndrome, as a function of the geographical coordinates of the individuals under risk. The data describing the location of syndrome cases over the population suffers a moving-average filtering, and the resulting values are fitted by an RBF network performing a regression. Some contour curves of the RBF network are then employed in order to establish the boundaries between four kinds of regions: regions of high-incidence, regions of medium incidence, regions of slightly-abnormal incidence, and regions of normal prevalence. In each region, the risk is estimated with three indicators: a nominal risk, an upper bound risk and a lower bound risk. Those indicators are obtained by adjusting the probability employed for the Monte Carlo simulation of syndrome scenarios over the population. The nominal risk is the probability which produces Monte Carlo simulations for which the empirical number of syndrome cases corresponds to the median. The upper bound and the lower bound risks are the probabilities which produce Monte Carlo simulations for which the empirical values of syndrome cases correspond respectively to the 25% percentile and the 75% percentile. The proposed method constitutes an advance in relation to the currently known techniques of spatial cluster detection, which are dedicated to finding clusters of abnormal occurrence of a syndrome, without quantifying the probability associated to such an abnormality, and without performing a stratification of different sub-regions with different associated risks. The proposed method was applied on data which were studied formerly in a paper that was intended to find a cluster of dengue fever. The result determined here is compatible with the cluster that was found in that reference.	decoding methods;geographic coordinate system;incidence matrix;monte carlo method;radial basis function network;risk assessment;simulation	Fernanda C. Takahashi;Ricardo H. C. Takahashi	2012	2012 11th International Conference on Machine Learning and Applications	10.1109/ICMLA.2012.233	econometrics;statistical hypothesis testing;risk analysis;computer science;machine learning;cluster;estimation theory;artificial neural network;statistics;monte carlo method	ML	18.402637912048707	-61.78616466111363	152077
0a24ec5be6552d03d0b6bafd1b90971e17b29251	survey on deep learning methods in human action recognition		A study on one of the most important issues in a human action recognition task, i.e. how to create proper data representations with a high-level abstraction from large dimensional noisy video data, is carried out. Most of the recent successful studies in this area are mainly focused on deep learning. Deep learning methods have gained superiority to other approaches in the field of image recognition. In this survey, the authors first investigate the role of deep learning in both image and video processing and recognition. Owing to the variety and plenty of deep learning methods, the authors discuss them in a comparative form. For this purpose, the authors present an analytical framework to classify and to evaluate these methods based on some important functional measures. Furthermore, a categorisation of the state-of-the-art approaches in deep learning for human action recognition is presented. The authors summarise the significantly related works in each approach and discuss their performance.	deep learning	Maryam Koohzadi;Nasrollah Moghaddam Charkari	2017	IET Computer Vision	10.1049/iet-cvi.2016.0355	mathematics;machine learning;pattern recognition;deep learning;video processing;abstraction;artificial intelligence	Vision	24.356024216884986	-54.9169717030205	152348
0dd1af8817ad4c2a62e6b78866ad9f875eba81b2	xsede support: revolutionizing the next-generation therapeutic drug discovery	drug discovery;high performance computing;virtual screening;docking;dock6;large scale analysis	"""Drug discovery is a critical but complex and costly endeavor. The rate of approval of new therapeutics by the FDA has been in decline while costs are rising. Increasingly, pharmaceutical companies desire to translate pharmaceutical discovery from academic research in order to decrease risk. Although many researchers have identified very compelling targets, most researchers do not have access to drug discovery resources due to the high cost and complex infrastructure needed to launch a discovery campaign. Long-term objective of this research is to integrate drug interaction simulation software to identify new bioactive molecules and speed drug development with minimum cost and time. This technology is a highly feasible way to rapidly close the therapeutic gap and potentially dramatically improve public health. Initially research was conducted using typical clusters and it took 3 months to perform one run with one conformation of the protein using 1.5 million small molecules. But researchers are interested in working with many proteins with multiple conformations per protein related to entire disease related pathways. At this rate this computational research by itself would take 6 to 7 years of computation on institutional clusters. This resulted in PI applying for the XSEDE allocation with Extended Collaborative Support Services (ECSS) support, which resulted in generation of optimized and scaled the drug interaction workflow on XSEDE supercomputers that reduced computation time for single run from months to 40 minutes using 8000 cores. The results were generated for 5 proteins with 5 conformations with 1.5 million compounds in an afternoon (wall clock time)on Kraken supercomputer which would have taken 5 years of computation on typical cluster. This presentation will discuss about the process from project inception to generating results for publications and proposals for various funding agencies.  PI quotes """"I thought the computation might not be finished in my life span, this collaboration takes my research to new heights""""."""	computation;computer cluster;kraken (supercomputer);simulation software;supercomputer;time complexity;π-calculus	Bhanu Rekepalli;Yuri K. Peterson	2014		10.1145/2616498.2616508	simulation;bioinformatics;engineering;nanotechnology	HPC	13.390267303596714	-60.51935271640107	152546
959589e5cf6a55e5f19197a1a80ff0bd21cb8b0f	"""comments on the article """"evaluation of pka estimation methods on 211 druglike compounds"""""""	modelizacion;criblage virtuel;etude experimentale;constante acidez;groupe fonctionnel;modelisation;constante acidite;virtual screening;grupo funcional;functional group;modeling;estudio experimental;acidity constant;cribado virtual	"""The recent article """"Evaluation of pK(a) Estimation Methods on 211 Druglike Compounds"""" ( Manchester, J.; et al. J. Chem Inf. Model. 2010, 50, 565-571 ) reports poor results for the program Epik. Here, we highlight likely sources for the poor performance and describe work done to improve the performance. Running Epik in the mode intended to calculate pK(a) values for sequentially adding/removing protons, as needed to reproduce the experimental conditions, improves the root mean squared error (RMSE) from 3.0 to 2.18 for the 85 public compounds available from the paper. Despite this improvement, there are still other programs in the Manchester paper that outperform Epik. The primary reason is that the public portion of the data set is not diverse and Epik is missing a few key functional groups in this data set that are heavily represented. We show that incorporation of these missing functional groups into the Epik training set improves the RMSE for the public compounds to 1.04. Furthermore, these enhancements help improve the overall performance of Epik on a large druglike test set."""		John C. Shelley;David Calkins;Arron P. Sullivan	2011	Journal of chemical information and modeling	10.1021/ci100332m	systems modeling;chemistry;virtual screening;artificial intelligence;organic chemistry;computational chemistry;acid dissociation constant;algorithm	ML	12.149059260370338	-56.410832109221005	152910
8823a67606ffe0621e5703a69f2921724cf15a37	predição de função de proteínas através da extração de características físico-químicas		With the conclusion of the Genome project, the number of new discovered proteins has grown, but due to the high cost and delay of the processes of protein function discovery, just a small parcel of them have their function known. This article presents a methodology for the prediction of protein function through the extraction of the characteristics of its structure, which are present in Sting_DB database, the Discrete Cosine Transform, the encoding of primary structure, class balancing and the use of Support Vector Machines. The average values obtained for precision, sensitivity, accuracy and specificity were 80%, 70%, 74% and 77%, respectively. The results were compared with other studies in the literature and showed an increase of 10% in the accuracy rate. 1Instituto de Ciências Exatas e Informática Pontifícia Universidade Católica de Minas Gerais Belo Horizonte, Brasil {taorodrigues,larissa.leijoto}@sga.pucminas.br {polianecoliveira@gmail.com,nobre@pucminas.br} Predição de Função de Proteínas Através da Extração de Características Físico-Químicas	discrete cosine transform;sensitivity and specificity;support vector machine;windows rally	Thiago Assis De Oliveira Rodrigues;Larissa Fernandes Leijoto;Poliane C. Oliveira Brandão;Cristiane Neri Nobre	2015	RITA		artificial intelligence;computer vision;computer science;performance art	Comp.	10.757666113748936	-52.98010053363777	152930
ed98ddcdf20624eb7e62f1aaa6f6b93b4b7fce68	distill: a machine learning approach to ab initio protein structure prediction.	machine learning;protein structure prediction	We present Distill, a simple and effective scalable architecture designed for modelling protein Cα traces based on predicted structural features. Distill targets those chains for which no significant sequential or structural resemblance to any entry of the Protein Data Bank (PDB) can be detected. Distill is composed of: (1) a set of state-of-the-art predictors of protein structural features based on statistical learning techniques and trained on large, non-redundant subsets of the PDB; (2) a simple and fast 3D reconstruction algorithm guided by a pseudo-energy defined according to these predicted features. At CASP6, a preliminary implementation of the system was ranked in the top 20 predictors in the Novel Fold hard target category. Here we test an improved version on a non-redundant set of 258 protein structures showing no homology to the sets employed to train the machine learning modules. Results show that the proposed method can generate topologically correct predictions, especially for relatively short (up to 100-150 residues) proteins. Moreover, we show how our approach makes genomic scale structural modelling tractable by solving hundreds of thousands of protein coordinates in the order of days.	3d modeling;3d reconstruction;accessibility;algorithm;class diagram;cobham's thesis;computation;de novo protein structure prediction;homology (biology);information source;machine learning;map;protein data bank;scalability;tracing (software);user-centered design	Gianluca Pollastri;Davide Baù;Alessandro Vullo	2007		10.1142/9789812708892_0007	computer science;machine learning;protein structure prediction	Comp.	10.779917700546777	-57.2529314752501	153162
057b1ac584ea7f5f707df0a9277b9acd7683c48a	an atlas of forecasted molecular data. 1. internuclear separations of main-group and transition-metal neutral gas-phase diatomic molecules in the ground state	diatomic molecule;transition metal;ground state;molecular data	"""Needed spectroscopic data on diatomic molecules can often be found in the superb critical tables of Huber and Herzberg or in the literature published since 1979. Unfortunately, these sources apply to only a fraction of the diatomic species that can exist and so investigators have had to rely on interpolation, additivity, or ad hoc rules to estimate needed values, all of which require other information that is often lacking. This Atlas presents 1001 additional internuclear separations for use until critical tables are available to fill the needs more precisely. The Atlas was produced by mining the data from Huber and Herzberg for trends with least-squares analysis and with neural network software. There are 162 molecules about whose data Huber and Herzberg had no qualifications and whose data were employed for this work; 248 copies of data with low and high magnitudes were added to reduce the effects of frequency. Internuclear separations for 1001 species not found in Huber and Herzberg are presented, and least-squares predictions supplement some of them. The results, i.e., the Atlas, are presented as Table A, Supporting Information. The average error, based on the average of the absolute differences between the predicted values and tabulated values for the molecules having Huber and Herzberg data, is 0.074 A; if each error is expressed as a percent of the forecast to which it pertains, the average of these errors is 2.94%. There are 25 """"questionable"""" data from Huber and Herzberg, not used in the preparation of the Atlas, for which predictions are included in the Atlas. Of these, 14 agree with the predicted internuclear separations to within twice the stated errors. Additional atlases for other properties of diatomic molecules are in preparation."""	atlases;cervical atlas;copy (object);data table;ground state;http 404;hoc (programming language);interpolation imputation technique;laboratory certification document;least squares;neural network simulation;neural network software;projections and predictions;rule (guideline);scientific publication	Ray Hefferlin;W. Bradford Davis;Jason Ileto	2003	Journal of chemical information and computer sciences	10.1021/ci020291q	atomic physics;transition metal;chemistry;diatomic molecule;computational chemistry;ground state;quantum mechanics	PL	13.825879040548804	-55.30445979407912	153235
072081b4c3957bb4d4046f8a7942a8b369b2907b	modeling task fmri data via deep convolutional autoencoder		Task-based functional magnetic resonance imaging (tfMRI) has been widely used to study functional brain networks under task performance. Modeling tfMRI data is challenging due to at least two problems: the lack of the ground truth of underlying neural activity and the highly complex intrinsic structure of tfMRI data. To better understand brain networks based on fMRI data, data-driven approaches have been proposed, for instance, independent component analysis (ICA) and sparse dictionary learning (SDL). However, both ICA and SDL only build shallow models, and they are under the strong assumption that original fMRI signal could be linearly decomposed into time series components with their corresponding spatial maps. As growing evidence shows that human brain function is hierarchically organized, new approaches that can infer and model the hierarchical structure of brain networks are widely called for. Recently, deep convolutional neural network (CNN) has drawn much attention, in that deep CNN has proven to be a powerful method for learning high-level and mid-level abstractions from low-level raw data. Inspired by the power of deep CNN, in this paper, we developed a new neural network structure based on CNN, called deep convolutional auto-encoder (DCAE), in order to take the advantages of both data-driven approach and CNN’s hierarchical feature abstraction ability for the purpose of learning mid-level and high-level features from complex, large-scale tfMRI time series in an unsupervised manner. The DCAE has been applied and tested on the publicly available human connectome project tfMRI data sets, and promising results are achieved.	autoencoder	Heng Huang;Xintao Hu;Milad Makkie;Qinglin Dong;Yu Zhao;Junwei Han;Lei Guo;Tianming Liu	2017		10.1007/978-3-319-59050-9_33	theoretical computer science;machine learning;pattern recognition	ML	21.167797532571132	-53.854452358859824	153247
a20e86156699b037ca3975976e50b54354e18d08	molecular modeling of the human vasopressin v2 receptor/agonist complex	molecular modeling;amino acid;simulated annealing;oxytocin;g protein coupled receptor;molecular dynamic	The V2 vasopressin renal receptor (V2R), which controls antidiuresis in mammals, is a member of the large family of heptahelical transmembrane (7TM) G protein-coupled receptors (GPCRs). Using the automated GPCR modeling facility available via Internet (http:/(/)expasy.hcuge.ch/swissmod/SWISS-MODEL.+ ++html) for construction of the 7TM domain in accord with the bovine rhodopsin (RD) footprint, and the SYBYL software for addition of the intra- and extracellular domains, the human V2R was modeled. The structure was further refined and its conformational variability tested by the use of a version of the Constrained Simulated Annealing (CSA) protocol developed in this laboratory. An inspection of the resulting structure reveals that the V2R (likewise any GPCR modeled this way) is much thicker and accordingly forms a more spacious TM cavity than most of the hitherto modeled GPCR constructs do, typically based on the structure of bacteriorhodopsin (BRD). Moreover, in this model the 7TM helices are arranged differently than they are in any BRD-based model. Thus, the topology and geometry of the TM cavity, potentially capable of receiving ligands, is in this model quite different than it is in the earlier models. In the subsequent step, two ligands, the native [arginine8]vasopressin (AVP) and the selective agonist [D-arginine8]vasopressin (DAVP) were inserted, each in two topologically non-equivalent ways, into the TM cavity and the resulting structures were equilibrated and their conformational variabilities tested using CSA as above. The best docking was selected and justified upon consideration of ligand-receptor interactions and structure-activity data. Finally, the amino acid residues were indicated, mainly in TM helices 3-7, as potentially important in both AVP and DAVP docking. Among those Cys112, Val115-Lys116, Gln119, Met123 in helix 3; Glu174 in helix 4; Val206, Ala210, Val213-Phe214 in helix 5; Trp284, Phe287-Phe288, Gln291 in helix 6; and Phe307, Leu310, Ala314 and Asn317 in helix 7 appeared to be the most important ones. Many of these residues are invariant for either the GPCR superfamily or the neurophyseal (vasopressin V2R, V1aR and V1bR and oxytocin OR) subfamily of receptors. Moreover, some of the equivalent residues in V1aR have already been found critical for the ligand affinity.		Cezary Czaplewski;Rajmund Kazmierkiewicz;Jerzy Ciarkowski	1998	Journal of computer-aided molecular design	10.1023/A:1007969526447	crystallography;biochemistry;molecular dynamics;amino acid;chemistry;simulated annealing;bioinformatics;g protein-coupled receptor;molecular model	Comp.	11.645126586293559	-60.723875425859184	153391
3820acf2b3106db83e57974788b4719457db5eea	principles and overview of sampling methods for modeling macromolecular structure and dynamics	biochemical simulations;simulation and modeling;protein structure;protein structure prediction;biophysical simulations;macromolecules;algorithms;free energy	Investigation of macromolecular structure and dynamics is fundamental to understanding how macromolecules carry out their functions in the cell. Significant advances have been made toward this end in silico, with a growing number of computational methods proposed yearly to study and simulate various aspects of macromolecular structure and dynamics. This review aims to provide an overview of recent advances, focusing primarily on methods proposed for exploring the structure space of macromolecules in isolation and in assemblies for the purpose of characterizing equilibrium structure and dynamics. In addition to surveying recent applications that showcase current capabilities of computational methods, this review highlights state-of-the-art algorithmic techniques proposed to overcome challenges posed in silico by the disparate spatial and time scales accessed by dynamic macromolecules. This review is not meant to be exhaustive, as such an endeavor is impossible, but rather aims to balance breadth and depth of strategies for modeling macromolecular structure and dynamics for a broad audience of novices and experts.	abbreviations;algorithmic efficiency;anton (computer);architecture as topic;boat dock;community;complement system proteins;computation (action);computational biology;customize;cytology;data quality;de novo protein structure prediction;docking (molecular);dynamical system;electronic structure;equilibrium;evolutionary algorithm;extraction;hidden markov model;image resolution;infection;inspiration function;interaction;intrinsically disordered proteins;kinetics;laboratory;macromolecular docking;markov chain;model selection;molecular dynamics;murine sarcoma viruses;mycobacterium tuberculosis;name;necrosis;neurodegenerative disorders;organic chemistry phenomena;parallel computing;particle;portable document format;qm/mm;quantity;robotics;sampling (signal processing);sampling - surgical action;scientific publication;simulation;solvent models;statistical model;synergy;thermodynamics;track (course);water model;wild type;macromolecule;quinoxaline	Tatiana Maximova;Ryan Moffatt;Buyong Ma;Ruth Nussinov;Amarda Shehu	2016	PLoS computational biology	10.1371/journal.pcbi.1004619	macromolecule;biology;protein structure;biophysics;computer science;bioinformatics;protein structure prediction	ML	12.943014842890715	-63.089307312838685	153615
2039270d7eb59b6f72083963d7bc47fb334b6c1a	large-scale neural systems for vision and cognition	cognitive rules;cross domain rules;learning process;spatial scale;real time;training;cognitive transformation;testing;declarative information;data mining;artmap information fusion network;large scale systems machine vision cognition pattern recognition learning systems remotely operated vehicles image converters computer networks image sensors labeling;subspace constraints;learning system;large scale;neural system;neural systems;pattern matching;relational knowledge;impedance matching;cognition;vision art neural nets cognition pattern recognition;pattern recognition;self organization;information fusion;art neural nets;encoding;vision;expert system;artmap information fusion network neural systems vision pattern recognition cognitive transformation declarative information relational knowledge learning system cognitive rules cross domain rules expert system	Consideration of how people respond to the question What is this? has suggested new problem frontiers for pattern recognition and information fusion, as well as neural systems that embody the cognitive transformation of declarative information into relational knowledge. In contrast to traditional classification methods, which aim to find the single correct label for each exemplar (This is a car), the new approach discovers rules that embody coherent relationships among labels which would otherwise appear contradictory to a learning system (This is a car, that is a vehicle, over there is a sedan). This talk will describe how an individual who experiences exemplars in real time, with each exemplar trained on at most one category label, can autonomously discover a hierarchy of cognitive rules, thereby converting local information into global knowledge. Computational examples are based on the observation that sensors working at different times, locations, and spatial scales, and experts with different goals, languages, and situations, may produce apparently inconsistent image labels, which are reconciled by implicit underlying relationships that the network's learning process discovers. The ARTMAP information fusion system can, moreover, integrate multiple separate knowledge hierarchies, by fusing independent domains into a unified structure. In the process, the system discovers cross-domain rules, inferring multilevel relationships among groups of output classes, without any supervised labeling of these relationships. In order to self-organize its expert system, the ARTMAP information fusion network features distributed code representations which exploit the model's intrinsic capacity for one-to-many learning (This is a car and a vehicle and a sedan) as well as many-to-one learning (Each of those vehicles is a car). Fusion system software, testbed datasets, and articles are available from http://cns.bu.edu/techlab .	cognition;coherence (physics);computation;experience;expert system;one-to-many (data model);pattern recognition;self-organization;sensor;spatial scale;testbed	Gail A. Carpenter	2009	2009 International Joint Conference on Neural Networks	10.1109/IJCNN.2009.5179092	vision;impedance matching;self-organization;cognition;computer science;artificial intelligence;machine learning;pattern matching;data mining;software testing;expert system;encoding	AI	22.187683597983497	-62.61324807416415	154653
f3018128ac5bdd4d39022c72263e5b568e6927cb	combinatorial qsar of ambergris fragrance compounds	computer programs;computers in chemistry	A combinatorial quantitative structure-activity relationships (Combi-QSAR) approach has been developed and applied to a data set of 98 ambergris fragrance compounds with complex stereochemistry. The Combi-QSAR approach explores all possible combinations of different independent descriptor collections and various individual correlation methods to obtain statistically significant models with high internal (for the training set) and external (for the test set) accuracy. Seven different descriptor collections were generated with commercially available MOE, CoMFA, CoMMA, Dragon, VolSurf, and MolconnZ programs; we also included chirality topological descriptors recently developed in our laboratory (Golbraikh, A.; Bonchev, D.; Tropsha, A. J. Chem. Inf. Comput. Sci. 2001, 41, 147-158). CoMMA descriptors were used in combination with MOE descriptors. MolconnZ descriptors were used in combination with chirality descriptors. Each descriptor collection was combined individually with four correlation methods, including k-nearest neighbors (kNN) classification, Support Vector Machines (SVM), decision trees, and binary QSAR, giving rise to 28 different types of QSAR models. Multiple diverse and representative training and test sets were generated by the divisions of the original data set in two. Each model with high values of leave-one-out cross-validated correct classification rate for the training set was subjected to extensive internal and external validation to avoid overfitting and achieve reliable predictive power. Two validation techniques were employed, i.e., the randomization of the target property (in this case, odor intensity) also known as the Y-randomization test and the assessment of external prediction accuracy using test sets. We demonstrate that not every combination of the data modeling technique and the descriptor collection yields a validated and predictive QSAR model. kNN classification in combination with CoMFA descriptors was found to be the best QSAR approach overall since predictive models with correct classification rates for both training and test sets of 0.7 and higher were obtained for all divisions of the ambergris data set into the training and test sets. Many predictive QSAR models were also found using a combination of kNN classification method with other collections of descriptors. The combinatorial QSAR affords automation, computational efficiency, and higher probability of identifying significant QSAR models for experimental data sets than the traditional approaches that rely on a single QSAR method.	ambergris;chirality (chemistry);collections (publication);data modeling;decision trees;decision tree;fragrance (odor);k-nearest neighbors algorithm;moe;odors;overfitting;perfume;predictive modelling;quantitative structure-activity relationship;resampling (statistics);stereochemistry (discipline);support vector machine;test set;trees (plant)	Assia Kovatcheva;Alexander Golbraikh;Scott Oloff;Yun-De Xiao;Weifan Zheng;Peter Wolschann;Gerhard Buchbauer;Alexander Tropsha	2004	Journal of chemical information and computer sciences	10.1021/ci034203t	chemistry;artificial intelligence;machine learning;data mining;mathematics	ML	11.405866297038637	-56.50713129495277	155151
cb689a5d749d726c1f651bed0e96af58245d77b3	inferring relevant features: from qft to pca		In many-body physics, renormalization techniques are used to extract aspects of a statistical or quantum state that are relevant at large scale, or for low energy experiments. Recent works have proposed that these features can be formally identified as those perturbations of the states whose distinguishability most resist coarse-graining. Here, we examine whether this same strategy can be used to identify important features of an unlabeled dataset. This approach indeed results in a technique very similar to kernel PCA (principal component analysis), but with a kernel function that is automatically adapted to the data, or “learned”. We test this approach on handwritten digits, and find that the most relevant features are significantly better for classification than those obtained from a simple gaussian kernel.		Cédric Bény	2018	CoRR	10.1142/S0219749918400129	physics;quantum mechanics;quantum field theory;renormalization;kernel principal component analysis;fisher information metric;statistical physics;quantum state;bayesian inference	ML	16.35684604498972	-54.57792972069113	155228
a6ad2ca6200f464dbacef40afb0a0ca70f21f475	inference of genetic regulatory networks using s-system and hybrid differential evolution	hybrid differential evolution;biology computing;differential evolution;evolutionary computation;iterative algorithms;system modeling;time course;systems biology;biological system modeling;genetic regulatory network;inference mechanisms;epsiv constrained problem genetic regulatory network inference s system hybrid differential evolution systems biology multiobjective optimization approach concentration error slope error interaction measure;multiobjective optimization approach;genetics;epsiv constrained problem;evolution biology;concentration error;kinetic theory;s system;genetic regulatory network inference;interaction measure;system biology;mathematical model;inference mechanisms biology computing evolutionary computation;biological systems;mathematical model biological system modeling optimization kinetic theory inference algorithms evolutionary computation equations;multiobjective optimization;inference algorithms;optimization;parameter estimation;kinetics;slope error;inverse problems	The inference of genetic regulatory networks from time-course data is one of the main challenges in systems biology. The ultimate goal of inferred model is to obtain the expressions quantitatively comprehending every detail and principle of biological systems. This study introduces a multiobjective optimization approach to infer a realizable S-system structure for genetic regulatory networks. The work of inference is to minimize simultaneously the concentration error, slope error and interaction measure in order to find a suitable S-system model structure and its corresponding model parameters. Hybrid differential evolution is applied to solve the epsiv-constrained problem, which is converted from the multiobjective optimization problem, for minimizing the interaction measure with subject to the expectation constraints for the concentration and slope error criteria. This approach could avoid assigning a suitable penalty weight for sum of magnitude of kinetic orders for the penalty problem in order to prune the model structure.	approximation algorithm;biological system;cpu cache;collocation;computation;differential evolution;emoticon;gene regulatory network;interaction network;mathematical optimization;multi-objective optimization;numerical integration;optimization problem;systems biology;video-in video-out;weight function	Pang-Kai Liu;Chiou-Hwa Yuh;Feng-Sheng Wang	2008	2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)	10.1109/CEC.2008.4631024	differential evolution;kinetic theory;mathematical optimization;systems modeling;computer science;inverse problem;artificial intelligence;multi-objective optimization;machine learning;mathematical model;mathematics;estimation theory;systems biology;kinetics	Vision	13.149884172694488	-52.9920862018398	155416
2dfd4438335887a6320a9cdfc7136af63b0fff66	geometric sampling framework for exploring molecular walker energetics and dynamics		The motor protein kinesin is a remarkable natural nanobot that moves cellular cargo by taking 8 nm steps along a microtubule molecular highway. Understanding kinesin's mechanism of operation continues to present considerable modeling challenges, primarily due to the millisecond timescale of its motion, which prohibits fully atomistic simulations. Here we describe the first phase of a physics-based approach that combines energetic information from all-atom modeling with a robotic framework to enable kinetic access to longer simulation timescales. Starting from experimental PDB structures, we have designed a computational model of the combined kinesin-microtubule system represented by the isosurface of an all-atom model. We use motion planning techniques originally developed for robotics to generate candidate conformations of the kinesin head with respect to the microtubule, considering all six degrees of freedom of the molecular walker's catalytic domain. This efficient sampling technique, combined with all-atom energy calculations of the kinesin-microtubule system, allows us to explore the configuration space in the vicinity of the kinesin binding site on the microtubule. We report initial results characterizing the energy landscape of the kinesin-microtubule system, setting the stage for an efficient, graph-based exploration of kinesin preferential binding and dynamics on the microtubule, including interactions with obstacles.	amiga walker;atom;computation;computational model;interaction;isosurface;molecular dynamics;motion planning;nanorobotics;protein data bank;robot;robotics;sampling (signal processing);simulation;six degrees of separation	Bruna Jacobson;Jon Christian L. David;Mitchell C. Malone;Kasra Manavi;Susan R. Atlas;Lydia Tapia	2017		10.1145/3107411.3107503	energy landscape;six degrees of freedom;kinesin;isosurface;configuration space;motion planning;kinesin binding;bioinformatics;computer science;robotics;artificial intelligence	Robotics	12.78581190738843	-62.23800550271333	155456
41bc4af0754074873233faf81faa3cb8f35ef58b	barriers to diffusion in dendrites and estimation of calcium spread following synaptic inputs	dendritic spine;dimension reduction;models theoretical;cytoplasm;calcium;three dimensional;reaction diffusion equation;3d model;high frequency stimulation;diffusion process;dendrites;diffusion;synapses	The motion of ions, molecules or proteins in dendrites is restricted by cytoplasmic obstacles such as organelles, microtubules and actin network. To account for molecular crowding, we study the effect of diffusion barriers on local calcium spread in a dendrite. We first present a model based on a dimension reduction approach to approximate a three dimensional diffusion in a cylindrical dendrite by a one-dimensional effective diffusion process. By comparing uncaging experiments of an inert dye in a spiny dendrite and in a thin glass tube, we quantify the change in diffusion constants due to molecular crowding as D(cyto)/D(water) = 1/20. We validate our approach by reconstructing the uncaging experiments using Brownian simulations in a realistic 3D model dendrite. Finally, we construct a reduced reaction-diffusion equation to model calcium spread in a dendrite under the presence of additional buffers, pumps and synaptic input. We find that for moderate crowding, calcium dynamics is mainly regulated by the buffer concentration, but not by the cytoplasmic crowding, dendritic spines or synaptic inputs. Following high frequency stimulations, we predict that calcium spread in dendrites is limited to small microdomains of the order of a few microns (<5 μm).	approximation algorithm;brownian motion;buffers;calcium;crowding;dendrites;dendritic spines;dendritic spine;dimensionality reduction;dyes;experiment;iontophoresis;micron;organelles;simulation;synapse;synaptic package manager;vertebral column;pump (device)	Armin Biess;Eduard Korkotian;David Holcman	2011		10.1371/journal.pcbi.1002182	biology;neuroscience;calcium;cytoplasm;diffusion process;dendritic spine;diffusion;anatomy	ML	10.188636042790469	-66.1129385323241	155584
552fb18f6f1b06817713fd42686f05ddb4048d90	multidimensional peptide/protein analysis and identification by sequence database search using mass spectrometric data	004;mass spectrometry;reversed phase;electrospray ionization;liquid chromatography;bovine serum albumin;cumulant;database search;mass spectrometric;multidimensional chromatography proteome analysis monolithic column tandem mass spectrometry	In order to generate proteomics data that are suitable to validate protein identification in complex mixtures using multidimensional liquidchromatography-mass spectrometry approaches, we implemented an offline two-dimensional liquid chromatography method combining strong cationexchangeand ion-pair reversed-phase chromatography followed by electrospray ionization tandem mass spectrormetry (ESI-MS/MS) for the analysis of a bovine serum albumin digest. The fragment ion spectra generated by ESI-MS/MS were subsequently analyzed via MASCOT database search. The obtained identification data were evaluated in terms of quality of protein/peptide identification by means of score values, reproducibility of identification in replicate measurements, distribution of tryptic peptides among different fractions, and overall number of unique identified proteins/peptides. Finally, we improved the trapping conditions in the second dimension by using a more hydrophobic amphiphile in the loading buffer. The improvement was demonstrated by comparison of the obtained identification data, such as number of identified peptides, cumulative mowse scores and reproducibility of identification.	cryptographic hash function;mowse;mascot;online and offline;proteomics;self-replicating machine;sequence database	Christian Schley;Matthias Altmeyer;Rolf Müller;Christian G. Huber	2005			chromatography;chemistry;analytical chemistry;environmental chemistry;protein mass spectrometry;top-down proteomics;sample preparation in mass spectrometry;bottom-up proteomics;tandem mass tag;peptide mass fingerprinting	Comp.	11.246298874539008	-58.40778820881481	155654
0fb2a4c86efb9e43cb9a6eab6fc2423678e1811c	the use of automated parameter searches to improve ion channel kinetics for neural modeling	neuron model;computation;globus pallidus;computer simulation;evolutionary algorithms	The voltage and time dependence of ion channels can be regulated, notably by phosphorylation, interaction with phospholipids, and binding to auxiliary subunits. Many parameter variation studies have set conductance densities free while leaving kinetic channel properties fixed as the experimental constraints on the latter are usually better than on the former. Because individual cells can tightly regulate their ion channel properties, we suggest that kinetic parameters may be profitably set free during model optimization in order to both improve matches to data and refine kinetic parameters. To this end, we analyzed the parameter optimization of reduced models of three electrophysiologically characterized and morphologically reconstructed globus pallidus neurons. We performed two automated searches with different types of free parameters. First, conductance density parameters were set free. Even the best resulting models exhibited unavoidable problems which were due to limitations in our channel kinetics. We next set channel kinetics free for the optimized density matches and obtained significantly improved model performance. Some kinetic parameters consistently shifted to similar new values in multiple runs across three models, suggesting the possibility for tailored improvements to channel models. These results suggest that optimized channel kinetics can improve model matches to experimental voltage traces, particularly for channels characterized under different experimental conditions than recorded data to be matched by a model. The resulting shifts in channel kinetics from the original template provide valuable guidance for future experimental efforts to determine the detailed kinetics of channel isoforms and possible modulated states in particular types of neurons.	cerebrovascular accident;clamping (graphics);clinical use template;computation;conductance (graph);conflict (psychology);departure - action;experiment;globus pallidus;ibm notes;ion channel;iontophoresis;kinesiology;kinetics (discipline);kinetics internet protocol;mathematical optimization;modulation;national institute of neurological disorders and stroke;population parameter;protein isoforms;tracing (software);density;interest;nervous system disorder;voltage	Eric B. Hendrickson;Jeremy R. Edgerton;Dieter Jaeger	2010	Journal of Computational Neuroscience	10.1007/s10827-010-0312-x	simulation;bioinformatics;artificial intelligence	ML	12.826635368993209	-62.58810634074198	155846
a8472a2021f79d19b3ead3f9a3317689ca8f2587	developmental word grounding through a growing neural network with a humanoid robot	robot learning;unsupervised learning;humanoid robot;learning process;neural nets;active learning;algorithms auditory perception biomimetics cybernetics humans natural language processing neural networks computer reading robotics vocabulary controlled;noise robustness;incremental knowledge robot;mental models;humanoid robots;word grounding;grounding neural networks humanoid robots speech cognitive science layout robot sensing systems humans natural languages;active learning mechanism;interactive learning;word grounding interactive learning language mental models robot;self organization;audio visual;lifelong learning;language;robot;active learning mechanism word grounding neural network humanoid robot unsupervised approach incremental knowledge robot;unsupervised approach;unsupervised learning humanoid robots neural nets;mental model;neural network	"""This paper presents an unsupervised approach of integrating speech and visual information without using any prepared data. The approach enables a humanoid robot, Incremental Knowledge Robot 1 (IKR1), to learn word meanings. The approach is different from most existing approaches in that the robot learns online from audio-visual input, rather than from stationary data provided in advance. In addition, the robot is capable of learning incrementally, which is considered to be indispensable to lifelong learning. A noise-robust self-organized growing neural network is developed to represent the topological structure of unsupervised online data. We are also developing an active-learning mechanism, called """"desire for knowledge"""", to let the robot select the object for which it possesses the least information for subsequent learning. Experimental results show that the approach raises the efficiency of the learning process. Based on audio and visual data, they construct a mental model for the robot, which forms a basis for constructing IKR1's inner world and builds a bridge connecting the learned concepts with current and past scenes"""	artificial neural network;audio media;biological neural networks;humanoid robot;increment;learning disorders;mental model;self-organization;stationary process;unsupervised learning	Xiaoyuan He;R. Kojima;O. Hasegawa	2007	IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)	10.1109/TSMCB.2006.885309	robot learning;computer science;humanoid robot;artificial intelligence;social robot;machine learning;personal robot;artificial neural network	Robotics	19.571411851439798	-65.06872605552635	155918
2853eb0673957d152642d1d5c1724c1f79fa9087	improved machine learning models for predicting selective compounds	selective compound;selectivity model;accurate computational method;compound selectivity prediction;differentiate compound selectivity property;multi-task method;selectivity prediction performance;improved machine;selectivity prediction;compound selectivity property;cascaded method;conventional selectivity prediction method;machine learning;multi task learning;drug discovery	Small molecular drug discovery is a time-consuming and costly process in which the identification of potential drug candidates serves as an initial and critical step. A successful drug needs to exhibit at least two important properties. The first is that the compound has to bind with high affinity to the protein (this protein is referred to as the target) that it is designed to affect so as to act efficaciously. The second is that the compound has to bind with high affinity to only that protein so as to minimize the likelihood of undesirable side effects. The latter property is related to compound selectivity, which measures how differentially a compound binds to the protein of interest. Experimental determination of compound selectivity usually takes place during the later stages of the drug discovery process. A selectivity test can include binding assays or clinical trials. The problemwith such an approach is that it defers selectivity assessment to the later stages, so if it fails, then significant investments in time and resources get wasted. For this reason, it is highly desirable to have inexpensive and accurate computational methods to predict compound selectivity at earlier stages in the drug discovery process. The use of computational methods to predict properties of chemical compounds has a long history in chemical informatics. The work pioneered byHansch et al. led to the development of computational methods for predicting structure activity relationships (SARs). In recent years, researchers have started to develop similar approaches for building models to predict the selectivity properties of compounds. Such models are referred to as structure selectivity relationship (SSR) models. Existing computational methods for building SSR models fall into two general classes. The first contains methods that determine selectivity by using SSR models, and the second contains methods that build a selectivity model by considering only the target of interest. The disadvantage of the first class of methods is that they rely on models learned by not utilizing information about which of the active compounds are selective and which ones are nonselective. As such, they ignore key information that can potentially lead to an overall better selectivity prediction method. The disadvantage of the second class of methods is that they largely ignore a rich source of information from multiple other proteins, which if properly explored could lead to more realistic and accurate selectivity models. In this paper, we develop two classes of machine learning methods for building SSR models. The first class of methods, referred to as cascaded SSRs, builds on previously developed techniques and incorporates a pair of models on two levels. Level 1 is a standard SSR model which identifies the compounds that bind to the target regardless of their selectivity. Level 2 is a model that further screens the compounds identified by the level 1 model to identify only the subset that binds selectively to the target and not to the other proteins. Such methods exhibit a cascaded architecture, and by decoupling the requirements of accuracy and selectivity, the respective learning tasks are more focused and easier to learn so as to increase the likelihood of developing accurate models. The second class of methods, referred to as multitask SSRs, incorporates information from	british undergraduate degree classification;cpu cache;cascaded integrator–comb filter;cheminformatics;computation;computer multitasking;coupling (computer programming);first-class function;gene prediction;informatics;information source;machine learning;processor affinity;requirement;selectivity (electronic);side effect (computer science)	Xia Ning;Michael A. Walters;George Karypis	2012	Journal of Chemical Information and Modeling	10.1021/ci300201j	computer science;artificial intelligence;machine learning	Comp.	12.160902017055218	-63.65726500817252	155965
270df08752e2142b9078871d8c31bc81d8e186c3	docking and scoring with target-specific pose classifier succeeds in native-like pose identification but not binding affinity prediction in the csar 2014 benchmark exercise		The CSAR 2014 exercise provided an important benchmark for testing current approaches for pose identification and ligand ranking using three X-ray characterized proteins: Factor Xa (FXa), Spleen Tyrosine Kinase (SYK), and tRNA Methyltransferase (TRMD). In Phase 1 of the exercise, we employed Glide and MedusaDock docking software, both individually and in combination, with the special target-specific pose classifier trained to discriminate native-like from decoy poses. All approaches succeeded in the accurate detection of native and native-like poses. We then used Glide SP and MedusaScore scoring functions individually and in combination with the pose-scoring approach to predict relative binding affinities of the congeneric series of ligands in Phase 2 of the exercise. Similar to other participants in the CSAR 2014 exercise, we found that our models showed modest prediction accuracy. Quantitative structure-activity relationship (QSAR) models developed for the FXa ligands using available bioactivity data from ChEMBL showed relatively low prediction accuracy for the CSAR 2014 ligands of the same target. Interestingly, QSAR models built with CSAR data only yielded Spearman correlation coefficients as high as ρ = 0.69 for FXa and ρ = 0.79 for SYK based on 5-fold cross-validation. Virtual screening of the DUD library using the FXa structure was successful in discriminating between active compounds and decoys in spite of poor ranking accuracy of the underlying scoring functions. Our results suggest that two of the three common tasks associated with molecular docking, i.e., native-like pose identification and virtual screening, but not binding affinity prediction, could be accomplished successfully for the CSAR 2014 challenge data set.	benchmark (computing);boat dock;chembl;coefficient;cross reactions;cross-validation (statistics);docking (molecular);factor xa;glide;ligands;processor affinity;protein d-aspartate-l-isoaspartate methyltransferase;quantitative structure-activity relationship;score;scoring functions for docking;tyrosine;virtual screening	Regina Politi;Marino Convertino;Konstantin Popov;Nikolay V. Dokholyan;Alexander Tropsha	2016	Journal of chemical information and modeling	10.1021/acs.jcim.5b00751	simulation;bioinformatics;machine learning	Comp.	10.651037995115734	-57.00747672471968	156128
9aa3522c2a01d896f39857b572dd28281635c80a	tbd: benchmarking and analyzing deep neural network training		The recent popularity of deep neural networks (DNNs) has generated a lot of research interest in performing DNN-related computation efficiently. However, the primary focus is usually very narrow and limited to (i) inference – i.e. how to efficiently execute already trained models and (ii) image classification networks as the primary benchmark for evaluation. Our primary goal in this work is to break this myopic view by (i) proposing a new benchmark for DNN training, called TBD, that uses a representative set of DNN models that cover a wide range of machine learning applications: image classification, machine translation, speech recognition, object detection, adversarial networks, reinforcement learning, and (ii) by performing an extensive performance analysis of training these different applications on three major deep learning frameworks (TensorFlow, MXNet, CNTK) across different hardware configurations (single-GPU, multi-GPU, and multi-machine). TBD currently covers six major application domains and eight different state-of-the-art models. We present a new toolchain for performance analysis for these models that combines the targeted usage of existing performance analysis tools, careful selection of new and existing metrics and methodologies to analyze the results, and utilization of domain specific characteristics of DNN training. We also build a new set of tools for memory profiling in all three major frameworks; much needed tools that can finally shed some light on precisely how much memory is consumed by different data structures (weights, activations, gradients, workspace) in DNN training. By using our tools and methodologies, we make several important observations and recommendations on where the future research and optimization of DNN training should be focused. 1TBD is short for Training Benchmark for DNNs 1 ar X iv :1 80 3. 06 90 5v 2 [ cs .L G ] 1 4 A pr 2 01 8 Image Classification Only Broader (include nonCNN workloads) Training [29][35][37][56][61][62][83][90][95] [10][22][58][66][75][77][99] Inference [12][13][14][25][28][37][39][42][61] [67][68][74][81][86][87][88][90] [103][104] [10][38][46][51][60][75] Table 1: The table above shows a categorization of major computer architecture and systems conference papers (SOSP, OSDI, NSDI, MICRO, ISCA, HPCA, ASPLOS) since 2014. These papers are grouped by their focus along two dimensions: Training versus Inference and Algorithmic Breadth. There are more papers which optimize inference over training (25 vs. 16, 4 papers aim for both training and inference). Similarly more papers use image classification as the only application for evaluation (26 vs. 11).	artificial neural network;benchmark (computing);categorization;computation;computer vision;data structure;deep learning;gradient;graphics processing unit;international conference on architectural support for programming languages and operating systems;international symposium on computer architecture;machine learning;machine translation;mathematical optimization;object detection;profiling (computer programming);reinforcement learning;speech recognition;symposium on operating systems principles;tensorflow;toolchain;workspace	Hongyu Zhu;Mohamed Akrout;Bojian Zheng;Andrew Pelegris;Amar Phanishayee;Bianca Schroeder;Gennady Pekhimenko	2018	CoRR		machine learning;artificial neural network;reinforcement learning;deep learning;toolchain;benchmarking;computer science;data structure;contextual image classification;benchmark (computing);artificial intelligence	ML	23.16607470296094	-54.00307923438189	156569
b6431210b2d0e68668d6639297374844a62a7124	linear and nonlinear 3d-qsar approaches in tandem with ligand-based homology modeling as a computational strategy to depict the pyrazolo-triazolo-pyrimidine antagonists binding site of the human adenosine a2a receptor	binding site;computer programs;computers in chemistry;homology modeling	The integration of ligand- and structure-based strategies might sensitively increase the success of drug discovery process. We have recently described the application of Molecular Electrostatic Potential autocorrelated vectors (autoMEPs) in generating both linear (Partial Least-Square, PLS) and nonlinear (Response Surface Analysis, RSA) 3D-QSAR models to quantitatively predict the binding affinity of human adenosine A3 receptor antagonists. Moreover, we have also reported a novel GPCR modeling approach, called Ligand-Based Homology Modeling (LBHM), as a tool to simulate the conformational changes of the receptor induced by ligand binding. In the present study, the application of both linear and nonlinear 3D-QSAR methods and LBHM computational techniques has been used to depict the hypothetical antagonist binding site of the human adenosine A2A receptor. In particular, a collection of 127 known human A2A antagonists has been utilized to derive two 3D-QSAR models (autoMEPs/PLS&RSA). In parallel, using a rhodopsin-driven homology modeling approach, we have built a model of the human adenosine A2A receptor. Finally, 3D-QSAR and LBHM strategies have been utilized to predict the binding affinity of five new human A2A pyrazolo-triazolo-pyrimidine antagonists finding a good agreement between the theoretical and the experimental predictions.	adenosine a2a receptor;apical ramus of posterior segmental artery;autocorrelation;binding sites;drug discovery;homology modeling;ligands;nonlinear system;processor affinity;pyrimidine dimers;pyrimidines;quantitative structure-activity relationship;quantitative structure–activity relationship;serotonin antagonists;simulation;triazolam	Lisa Michielan;Magdalena Bacilieri;Andrea Schiesaro;Chiara Bolcato;Giorgia Pastorin;Giampiero Spalluto;Barbara Cacciari;Karl Norbert Klotz;Chosei Kaseda;Stefano Moro	2008	Journal of chemical information and modeling	10.1021/ci700300w	stereochemistry;homology modeling;chemistry;bioinformatics;binding site;combinatorial chemistry	Comp.	12.000936927079694	-57.91998874304577	156584
2413ef948b6e906487f793e2df85491d65bf1be6	gash: an improved algorithm for maximizing the number of equivalent residues between two protein structures	software;sequences;resolution;methylglyoxal synthase;structural biology;computational biology bioinformatics;domain;protein structure;conjugate gradient;proteins;algorithms;genetic algorithm;crystal structure;sequence alignment;medicine all;combinatorial libraries;applied mathematics;computer appl in life sciences;structural similarity;multiple alignment;microarrays;bioinformatics;evolution;structure alignment	We introduce GASH, a new, publicly accessible program for structural alignment and superposition. Alignments are scored by the Number of Equivalent Residues (NER), a quantitative measure of structural similarity that can be applied to any structural alignment method. Multiple alignments are optimized by conjugate gradient maximization of the NER score within the genetic algorithm framework. Initial alignments are generated by the program Local ASH, and can be supplemented by alignments from any other program. We compare GASH to DaliLite, CE, and to our earlier program Global ASH on a difficult test set consisting of 3,102 structure pairs, as well as a smaller set derived from the Fischer-Eisenberg set. The extent of alignment crossover, as well as the completeness of the initial set of alignments are examined. The quality of the superpositions is evaluated both by NER and by the number of aligned residues under three different RMSD cutoffs (2,4, and 6Å). In addition to the numerical assessment, the alignments for several biologically related structural pairs are discussed in detail. Regardless of which criteria is used to judge the superposition accuracy, GASH achieves the best overall performance, followed by DaliLite, Global ASH, and CE. In terms of CPU usage, DaliLite CE and GASH perform similarly for query proteins under 500 residues, but for larger proteins DaliLite is faster than GASH or CE. Both an http interface and a simple object application protocol (SOAP) interface to the GASH program are available at http://www.pdbj.org/GASH/ .	alignment;american society of hematology;ash tree;cath;cpu (central processing unit of computer system);central processing unit;classification;conjugate gradient method;correctness (computer science);expectation–maximization algorithm;genetic algorithm;hypertext transfer protocol;immunostimulating conjugate (antigen);interface device component;large;michael j. fischer;numerical analysis;population parameter;quantum superposition;question (inquiry);soap;sampling (signal processing);scop;score;small;structural similarity;test set;turing completeness;citation	Daron M. Standley;Hiroyuki Toh;Haruki Nakamura	2005	BMC Bioinformatics	10.1186/1471-2105-6-221	biology;protein structure;genetic algorithm;resolution;domain;computer science;bioinformatics;crystal structure;theoretical computer science;structural similarity;sequence alignment;evolution;structural biology	Comp.	11.186279513601091	-59.81240287682637	157012
290065098ea37f28e31d7765195a09fb7c6d8cba	'inductive' charges on atoms in proteins: comparative docking with the extended steroid benchmark set and discovery of a novel shbg ligand		We have developed a novel iterative approach for calculation of partial charges in proteins within the framework of the 'molecular capacitance' model. The method operates by an effective 'inductive' electronegativity scale derived from a number of the conventional charge systems including CHARMM, AMBER, MMFF, OPLS, and PEOE among others. Our novel 'inductive' electronegativity equalization procedure allows rapid and conformation sensitive computation of adequate partial charges in proteins. Accuracy of the 'inductive' values was confirmed by their correlation with DFT-computed partial charges in common amino acids. A comparative docking study with an extended steroid data set not only illustrated the adequacy of 'inductive' protein charges but also demonstrated their superior performance compared to several conventional protein charging systems. Subsequent docking with 'inductive' charges resulted in identification of five potential leads as human Sex Hormone Binding Globulin (SHBG) ligands from a commercial library of natural compounds. When the selected substances were evaluated for their ability to bind SHBG in vitro, three of them displaced testosterone from the SHBG steroid-binding site, and with one compound this was achieved at micromolar concentrations.	adaptive histogram equalization;amino acids;assisted model building with energy refinement (amber);benchmark (computing);boat dock;charmm;computation (action);docking (molecular);electric capacitance;gonadorelin;inductive reasoning;iterative method;ligands;merck molecular force field;micromole/liter;opls;partial charge;sex factors;steroids;testosterone	Artem Cherkasov;Zheng Shi;Yvonne Y. Li;Steven J. M. Jones;Magid Fallahi;Geoffrey L. Hammond	2005	Journal of chemical information and modeling	10.1021/ci0498158	stereochemistry;chemistry;computational chemistry	Comp.	10.60476497038552	-60.538142722251784	157539
22a8b7ca2e6449cad31ae4f71bb3587751087e77	deepbox: learning objectness with convolutional networks	proposals image edge detection training computer architecture image segmentation semantics;object detection neural nets;bottom up ranking deepbox learning objectness convolutional neural network object proposals four layer cnn architecture	"""Existing object proposal approaches use primarily bottom-up cues to rank proposals, while we believe that """"objectness"""" is in fact a high level construct. We argue for a data-driven, semantic approach for ranking object proposals. Our framework, which we call DeepBox, uses convolutional neural networks (CNNs) to rerank proposals from a bottom-up method. We use a novel four-layer CNN architecture that is as good as much larger networks on the task of evaluating objectness while being much faster. We show that DeepBox significantly improves over the bottom-up ranking, achieving the same recall with 500 proposals as achieved by bottom-up methods with 2000. This improvement generalizes to categories the CNN has never seen before and leads to a 4.5-point gain in detection mAP. Our implementation achieves this performance while running at 260 ms per image."""	artificial neural network;bottom-up parsing;convolutional neural network;high-level programming language;map	Weicheng Kuo;Bharath Hariharan;Jitendra Malik	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.285	computer vision;computer science;artificial intelligence;theoretical computer science;machine learning;pattern recognition	Vision	24.138845775923645	-53.90068898596113	157630
4d156381d7b07f8aa6bc32dd21c711c579d64b0b	deep learning scene recognition method based on localization enhancement	deep learning;indoor positioning;scene recognition;signals of opportunity	With the rapid development of indoor localization in recent years; signals of opportunity have become a reliable and convenient source for indoor localization. The mobile device cannot only capture images of the indoor environment in real-time, but can also obtain one or more different types of signals of opportunity as well. Based on this, we design a convolutional neural network (CNN) model that concatenates features of image data and signals of opportunity for localization by using indoor scene datasets and simulating the situation of indoor location probability. Using the method of transfer learning on the Inception V3 network model feature information is added to assist in scene recognition. The experimental result shows that, for two different experiment sceneries, the accuracies of the prediction results are 97.0% and 96.6% using the proposed model, compared to 69.0% and 81.2% by the method of overlapping positioning information and the base map, and compared to 73.3% and 77.7% by using the fine-tuned Inception V3 model. The accuracy of indoor scene recognition is improved; in particular, the error rate at the spatial connection of different scenes is decreased, and the recognition rate of similar scenes is increased.	artificial neural network;biological neural networks;chart;charts (publication);cloud computing;conflict (psychology);convolution;convolutional neural network;data collection;deep learning;design of experiments;experiment;extraction;feature (computer vision);generalization (psychology);global positioning system;high availability;information engineering;mobile device;natural science disciplines;network model;real-time locating system;research design;web mapping	Wei Guo;Ran Wu;Yanhua Chen;Xinyan Zhu	2018		10.3390/s18103376	electronic engineering;deep learning;computer vision;engineering;artificial intelligence	Robotics	24.105724309384712	-57.05670278735634	157819
f3eacac80c0fa18e28bbbf5e41adbbd8349ec7e9	windowing functions comparison for peak detection of amplitud coded saw id-tags			window function	Luis Diaz;Ernesto A. Rincon Cruz;Sebastian Velasquez;Juan C. Bohorquez Reyes;Fredy Segura;Nestor Pena Traslavina;Luis Muñoz	2015			window function;pattern recognition;artificial intelligence;computer science	Vision	17.346609241391192	-62.16804924753896	158246
6c5285be77d705033f559ed0a38115c213396e8b	gene prediction by pattern recognition and homology search	genes;forecasting;homology search;reference model;55 biology and medicine basic studies;molecular biology;gene prediction;dna sequencing;pattern recognition;algorithms;biology and medicine basic studies;dna sequence;exons;level 1;molecular structure	This paper presents an algorithm for combining pattern recognition-based exon prediction and database homology search in gene model construction. The goal is to use homologous genes or partial genes existing in the database as reference models while constructing (multiple) gene models from exon candidates predicted by pattern recognition methods. A unified framework for gene modeling is used for genes ranging from situations with strong homology to no homology in the database. To maximally use the homology information available, the algorithm applies homology on three levels: (1) exon candidate evaluation, (2) gene-segment construction with a reference model, and (3) (complete) gene modeling. Preliminary testing has been done on the algorithm. Test results show that (a) perfect gene modeling can be expected when the initial exon predictions are reasonably good and a strong homology exists in the database; (b) homology (not necessarily strong) in general helps improve the accuracy of gene modeling; (c) multiple gene modeling becomes feasible when homology exists in the database for the involved genes.	algorithm;blast;emoticon;gene prediction;homologous gene;pattern recognition;reference model;unified framework	Ying Xu;Edward C. Uberbacher	1996	Proceedings. International Conference on Intelligent Systems for Molecular Biology		biology;dna sequencing;bioinformatics;genetics	Comp.	10.270667244702482	-55.10437444738517	158270
552fc42613cf1321da54122ce4a58dbfe8ac0060	structure-based analysis of protein binding pockets using von neumann entropy		Protein binding sites are regions where interactions between a protein and ligand take place. Identification of binding sites is a functional issue especially in structure-based drug design. This paper aims to present a novel feature of protein binding pockets based on the complexity of corresponding weighted Delaunay triangulation. The results demonstrate that candidate binding pockets obtain less relative Von Neumann entropy which means more random scattering of voids inside them.	quantum statistical mechanics	Negin Forouzesh;Mohammad Reza Kazemi;Ali Mohades	2014		10.1007/978-3-319-08171-7_27	computational chemistry;bioinformatics;computer science;von neumann entropy;binding site;delaunay triangulation;ligand;plasma protein binding	Comp.	11.26557084692829	-59.16117002176409	158980
f9e981487ad87e35a8736092ae74020e0119d60a	automatic non-verbal communication skills analysis: a quantitative evaluation	social signal processing;multi modal data description;e learning;non verbal communication analysis;human behavior analysis;multi modal data fusion	The oral communication competence is defined on the top of the most relevant skills for one’s professional and personal life. Because of the importance of communication in our activities of daily living, it is crucial to study methods to evaluate and provide the necessary feedback that can be used in order to improve these communication capabilities and, therefore, learn how to express ourselves better. In this work, we propose a system capable of evaluating quantitatively the quality of oral presentations in an automatic fashion. The system is based on a multi-modal RGB, depth, and audio data description and a fusion approach in order to recognize behavioral cues and train classifiers able to eventually predict communication quality levels. The performance of the proposed system is tested on a novel dataset containing Bachelor thesis’ real defenses, presentations from an 8th semester Bachelor courses, and Master courses’ presentations at Universitat de Barcelona. Using as groundtruth the marks assigned by actual instructors, our system achieves high performance categorizing and ranking presentations by their quality, and also making real-valued mark predictions.	bachelor of science in information technology;behavioral pattern;best, worst and average case;categorization;feature selection;feature vector;feedback;high- and low-level;modal logic;relevance;viewing frustum	Alvaro Cepero;Albert Clapés;Sergio Escalera	2015	AI Commun.	10.3233/AIC-140617	simulation;computer science;artificial intelligence;machine learning;multimedia	AI	23.37678442026289	-58.83626458829816	159481
7fb533d3b26e1ff71014ccc4921cec45df9c1132	simulation modeling for making decision on clinical trials using acceptability curve of cost-effectveness and expected net benefits		Simulating empirical distributions of costs and health benefits are widely used for making decisions on health interventions. Decisions using acceptability curves (CEAC) are commonly adopted to represents the probability of incremental cost-effectiveness model regarding the Northeast quadrant of joint density of incremental cost and benefits distributions, considering variability within samples. Using an expected net benefits model, we show how to integrate in one curve the distributed points of costs and benefits that fall in Northeast, Southeast, Northwest and Southwest quadrants, considering variability between and within samples. We applied the methods to a clinical trial that evaluates the effects of resonance magnetic image and computerized tomography image in the diagnostic of stroke. Thus, modeling and simulation of expected net benefits allow for drawing an acceptability curve, integrating the four quadrants of joint density of incremental cost and benefits without altering decisions that might be undertaken using the classical acceptability curve approach.	ct scan;heart rate variability;resonance;simulation;tomography	Ismail Abbas	2017	2017 Winter Simulation Conference (WSC)	10.1109/WSC.2017.8248200	operations research;simulation modeling;simulation;clinical trial;computer science;computed tomography;cost–benefit analysis;modeling and simulation;marginal cost;four quadrants	Robotics	17.610328676607047	-63.38403894725187	159841
c76020b9205c86228142f983da121d0f6c15b4df	pharmacophore alignment search tool: influence of scoring systems on text-based similarity searching	global alignment;virtual screening;similarity;pharmacophore elucidation;line notation;similarity search;scoring system	The text-based similarity searching method Pharmacophore Alignment Search Tool is grounded on pairwise comparisons of potential pharmacophoric points between a query and screening compounds. The underlying scoring matrix is of critical importance for successful virtual screening and hit retrieval from large compound libraries. Here, we compare three conceptually different computational methods for systematic deduction of scoring matrices: assignment-based, alignment-based, and stochastic optimization. All three methods resulted in optimized pharmacophore scoring matrices with significantly superior retrospective performance in comparison with simplistic scoring schemes. Computer-generated similarity matrices of pharmacophoric features turned out to agree well with a manually constructed matrix. We introduce the concept of position-specific scoring to text-based similarity searching so that knowledge about specific ligand-receptor binding patterns can be included and demonstrate its benefit for hit retrieval. The approach was also used for automated pharmacophore elucidation in agonists of peroxisome proliferator activated receptor gamma, successfully identifying key interactions for receptor activation.	alignment;chemical library;computation;computer-generated holography;interaction;libraries;ligands;mathematical optimization;natural deduction;ppar gamma;peroxisome proliferator-activated receptors;peroxisome proliferators;pharmacophore;position weight matrix;question (inquiry);receptor activation process;score;search engine;stochastic optimization;text-based (computing);thrombocytopenia;virtual screening	Volker Hähnke;Gisbert Schneider	2011	Journal of computational chemistry	10.1002/jcc.21741	chemistry;line notation;similarity;virtual screening;data mining	Comp.	11.1076237077463	-59.45016941333478	160455
43551fbd778db21de670bdcce60c96b54ad29208	ranking docked models of protein-protein complexes using predicted partner-specific protein-protein interfaces: a preliminary study	biological patents;biomedical journals;score function;protein complex;text mining;europe pubmed central;model generation;citation search;citation networks;research articles;abstracts;protein protein docking;open access;life sciences;clinical guidelines;full text;docking scoring function;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Computational protein-protein docking is a valuable tool for determining the conformation of complexes formed by interacting proteins. Selecting near-native conformations from the large number of possible models generated by docking software presents a significant challenge in practice.  We introduce a novel method for ranking docked conformations based on the degree of overlap between the interface residues of a docked conformation formed by a pair of proteins with the set of predicted interface residues between them. Our approach relies on a method, called PS-HomPPI, for reliably predicting proteinprotein interface residues by taking into account information derived from both interacting proteins. PS-HomPPI infers the residues of a query protein that are likely to interact with a partner protein based on known interface residues of the homo-interologs of the query-partner protein pair, i.e., pairs of interacting proteins that are homologous to the query protein and partner protein. Our results on Docking Benchmark 3.0 show that the quality of the ranking of docked conformations using our method is consistently superior to that produced using ClusPro cluster-size-based and energy-based criteria for 61 out of the 64 docking complexes for which PS-HomPPI produces interface predictions. An implementation of our method for ranking docked models is freely available at: http://einstein.cs.iastate.edu/DockRank/.	benchmark (computing);boat dock;docking (molecular);docking -molecular interaction;interface device component;macromolecular docking;ps-algol;question (inquiry);ps-hr protein, rabbitpox virus	Li C. Xue;Rafael A. Jordan;Yasser El-Manzalawy;Drena Dobbs;Vasant Honavar	2011	ACM-BCB ... ... : the ... ACM Conference on Bioinformatics, Computational Biology and Biomedicine. ACM Conference on Bioinformatics, Computational Biology and Biomedicine	10.1145/2147805.2147866	searching the conformational space for docking;bioinformatics;engineering;data science;data mining	Comp.	10.260475750054734	-59.386942301907055	160623
dc512c9dab64ad89cf525961f10d65d248d1fb00	some insights into convolutional neural networks		As a kind of deep learning model, convolutional neural networks (CNNs) have greatly boosted the state-of-the-art performance and have found their successful applications in many fields, such as computer version, pattern recognition, natural language processing, etc. Many distinguished CNN models, for example, AlexNet, Google inception net, VGGNet, and so on, have been developed for various tasks. In this paper, we provide some insights into convolutional neural networks in three aspects: activation function, convolution operation, pooling operation, and we also discuss the training of CNNs. The insights presented in this paper will provide the potential readers with further understanding of CNNs.	activation function;artificial neural network;convolution;convolutional neural network;deep learning;feature model;information;kernel (operating system);natural language processing;pattern recognition;rectifier (neural networks)	Jun-Hai Zhai;Li-Guang Zang;Su-Fang Zhang	2017	2017 International Conference on Machine Learning and Cybernetics (ICMLC)	10.1109/ICMLC.2017.8107753	deep learning;convolutional neural network;machine learning;pattern recognition;feature extraction;computer science;pooling;convolution;artificial intelligence;activation function	ML	23.680206837230184	-52.840485028087066	160659
c569124a4f449db52a32c9f5a51a5f0ccd5b5cd2	a surface site interaction point methodology for macromolecules and huge molecular databases		Determining the position and magnitude of Surface Site Interaction Points (SSIP) is a useful technique for understanding intermolecular interactions. SSIPs have been used for the prediction of solvation properties and for virtual co-crystal screening. To determine the SSIPs for a molecule, the Molecular Electrostatic Potential Surface (MEPS) is first calculated using ab initio methods such as Density Functional Theory. This leads to a high cost in terms of computation time and is not compatible with the analysis of huge molecular databases. Herein, we present a method for the fast estimation of SSIPs, which is based on the MEPS calculated from MMFF94 atomic partial charges. The results show that this method can be used to calculate SSIPs for large molecular databases with a much higher speed than the original ab initio methodology. © 2017 Wiley Periodicals, Inc.	ab initio quantum chemistry methods;cocrystal;computation;database;databases;databases, molecular;density functional theory;interaction;john d. wiley;merck molecular force field;partial charge;time complexity;macromolecule	Antoni Oliver;Christopher A. Hunter;Rafel Prohens;Josep L. Rosselló	2017	Journal of computational chemistry	10.1002/jcc.24695	chemistry;computational chemistry;nanotechnology	Comp.	12.826891230615722	-60.268654502227356	160738
cecb692b1ad5364a2c09e32330aa38f97f869303	iapr keynote lecture iv: deep learning		Deep learning has arisen around 2006 as a renewal of neural networks research allowing such models to have more layers. Theoretical investigations have shown that functions obtained as deep compositions of simpler functions (which includes both deep and recurrent nets) can express highly varying functions (with many ups and downs and different input regions that can be distinguished) much more efficiently (with fewer parameters) than otherwise. Empirical work in a variety of applications has demonstrated that, when well trained, such deep architectures can be highly successful, remarkably breaking through previous state-of-the-art in many areas, including speech recognition, object recognition, language models, and transfer learning. This talk will summarize the advances that have made these breakthroughs possible, and end with questions about some major challenges still ahead of researchers in order to continue our climb towards AI-level competence.	deep learning;international association for pattern recognition	Yoshua Bengio	2015		10.1109/ACPR.2015.7486451	simulation;computer science;artificial intelligence;operations research	Logic	17.73791942408598	-53.38473293916979	160805
7b40e81b36c3345939907a4b7fafe2ad89ab7c97	optoelectronic method for determining the aluminium involved in symptoms of attention deficit hyperactivity disorder children		Aluminum is a chemical element atomic number 13. It is white-silver, insoluble in water under normal conditions. Despite its natural abundance, aluminum has no known biology function. It is a toxic residue, aluminum sulphate having an LD50 of 6207 mg/kg body, corresponding to 500 g per 80 kg person. Extremely acute toxicity without harm to health is of interest in view of the widespread occurrence of the element in the environment and in trade. Toxicity can be tracked after deposition into the bones and the central nervous system and is particularly high in patients with renal insufficiency. In very high doses, aluminum can cause neuro toxicity associated with altered function of the blood-brain barrier.		Elena Truta;Ana Maria Davitoiu;Ana Mihaela Mitu;Alexandra Andrada Bojescu;Paul Schiopu;Marian Vladescu;Genica Caragea;Luminita Horhota;Maria Gabriela Neicu;Mihai Ionica	2017		10.1007/978-3-319-92213-3_32	acute toxicity;physiology;toxicity;aluminum can;aluminum sulphate;attention deficit hyperactivity disorder;aluminium	HCI	11.7651431038589	-64.51880860516991	161067
f6f8e5e82576da527a30dbc2a78ad3f7a4b4db83	software tools for dna sequence design	self assembly;computer program;software tool;forbidden subsequence;secondary structure;sequence design;dna computing;dna nanotechnology;specific hybridization;dna sequence;nucleic acid;molecular self assembly;melting temperature	The design of DNA sequences is a key problem for implementing molecular self-assembly with nucleic acid molecules. These molecules must meet several physical, chemical and logical requirements, mainly to avoid mishybridization. Since manual selection of proper sequences is too time-consuming for more than a handful of molecules, the aid of computer programs is advisable. In this paper two software tools for designing DNA sequences are presented, the DNASequenceGenerator and the DNASequenceCompiler. Both employ an approach of sequence dissimilarity based on the uniqueness of overlapping subsequences and a graph based algorithm for sequence generation. Other sequence properties like melting temperature or forbidden subsequences are also regarded, but not secondary structure errors or equilibrium chemistry. Fields of application are DNA computing and DNA-based nanotechnology. In the second part of this paper, sequences generated with the DNASequenceGenerator are compared to those from several publications of other groups, an example application for the DNASequenceCompiler is presented, and the advantages and disadvantages of the presented approach are discussed.	algorithm;computer program;dna computing;formal specification;molecular self-assembly;requirement	Udo Feldkamp;Hilmar Rauhe;Wolfgang Banzhaf	2003	Genetic Programming and Evolvable Machines	10.1023/A:1023985029398	nucleic acid;dna sequencing;bioinformatics;theoretical computer science;sequence analysis;molecular self-assembly;self-assembly;genetics;dna computing;algorithm;dna nanotechnology;protein secondary structure	Comp.	13.322450142385511	-59.555529722120546	161193
2958a360ddad29c1f887ce9f2649225422b5f5a1	modeling task fmri data via mixture of deep expert networks		Task-based fMRI (tfMRI) has been a powerful noninvasive tool to study cognitive behaviors of the human brain. Computational modeling of tfMRI data involves two important aspects: the development of algorithms to learn a wide variety of meaningful patterns from specific task fMRI signals and the development of models to discriminate among different tasks. Although both aspects are important, most previous fMRI studies focused either on learning meaningful patterns from single tasks or learning discriminations across different tasks, and as a consequence, a unified approach to studying both aspects is rarely explored. To bridge this knowledge gap, in this study, we adopted the basic idea from convolutional neural network (CNN) and proposed a new variant of CNN called deep expert network (DEN) to model tfMRI data in a hierarchical manner. At the same time, by mixing these DEN models, we are able to achieve discriminations across different tasks. To validate the effectiveness and efficiency of the proposed mixture of DENs (MoDEN), we applied them on three Human Connectome Project (HCP) task fMRI datasets (language, social and working memory tasks). Our experiments achieved promising results and demonstrated the superior ability of MoDEN in modeling task-based fMRI data.	algorithm;artificial neural network;computation;computer simulation;convolutional neural network;denormal number;experiment;expert network;feature learning;human connectome project	Heng Huang;Xintao Hu;Qinglin Dong;Shijie Zhao;Shu Zhang;Yu Zhao;Lei Quo;Tianming Liu	2018	2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)	10.1109/ISBI.2018.8363528	working memory;convolutional neural network;human connectome project;computer science;deep learning;pattern recognition;unsupervised learning;cognition;artificial intelligence	ML	21.08244957481882	-53.98292483591812	161281
ff6d4736cdda100e44c9f8496b424165c27b3e20	a framework for the multi-level fusion of electronic nose and electronic tongue for tea quality assessment	electronic tongue;tea quality assessment;decision fusion;multi level fusion;feature fusion;electronic nose	Electronic nose (E-nose) and electronic tongue (E-tongue) can mimic the sensory perception of human smell and taste, and they are widely applied in tea quality evaluation by utilizing the fingerprints of response signals representing the overall information of tea samples. The intrinsic part of human perception is the fusion of sensors, as more information is provided comparing to the information from a single sensory organ. In this study, a framework for a multi-level fusion strategy of electronic nose and electronic tongue was proposed to enhance the tea quality prediction accuracies, by simultaneously modeling feature fusion and decision fusion. The procedure included feature-level fusion (fuse the time-domain based feature and frequency-domain based feature) and decision-level fusion (D-S evidence to combine the classification results from multiple classifiers). The experiments were conducted on tea samples collected from various tea providers with four grades. The large quantity made the quality assessment task very difficult, and the experimental results showed much better classification ability for the multi-level fusion system. The proposed algorithm could better represent the overall characteristics of tea samples for both odor and taste.	electronic nose;electronic tongue;esthesia;experiment;fingerprint;grade;numerous;odors;tea;algorithm;sensor (device)	Ruicong Zhi;Lei Zhao;Dezheng Zhang	2017		10.3390/s17051007	electronic nose;speech recognition;chemistry;electronic tongue;engineering;data mining	Robotics	17.845332459144196	-60.50622432727031	161394
997329c2aa1729ebdb7c9aadc2e673dc39e4c04d	constraint-free natural image reconstruction from fmri signals based on convolutional neural network	brain decoding;convolutional neural network;functional magnetic resonance imaging;image reconstruction;visual representation	In recent years, research on decoding brain activity based on functional magnetic resonance imaging (fMRI) has made eye-catching achievements. However, constraint-free natural image reconstruction from brain activity remains a challenge, as specifying brain activity for all possible images is impractical. The problem was often simplified by using semantic prior information or just reconstructing simple images, including digitals and letters. Without semantic prior information, we present a novel method to reconstruct natural images from the fMRI signals of human visual cortex based on the computation model of convolutional neural network (CNN). First, we extracted the unit output of viewed natural images in each layer of a pre-trained CNN as CNN features. Second, we transformed image reconstruction from fMRI signals into the problem of CNN feature visualization by training a sparse linear regression to map from the fMRI patterns to CNN features. By iteratively optimization to find the matched image, whose CNN unit features become most similar to those predicted from the brain activity, we finally achieved the promising results for the challenging constraint-free natural image reconstruction. The semantic prior information of the stimuli was not used when training decoding model, and any category of images (not constraint by the training set) could be reconstructed theoretically. We found that the reconstructed images resembled the natural stimuli, especially in position and shape. The experimental results suggest that hierarchical visual features may be an effective tool to express the human visual processing.	achievement;artificial neural network;biological neural networks;cerebral cortex;convolutional neural network;electroencephalography;extraction;haplogroup cz (mtdna);homology (biology);imagenet;imagery;iterative reconstruction;layer (electronics);limewire;manuscripts;mathematical optimization;model of computation;numerous;resonance;sparse matrix;test set;vision;camazepam;fmri;videocassette	Chi Zhang;Kai Qiao;Linyuan Wang;Li Tong;Ying Zeng;Bin Yan	2018		10.3389/fnhum.2018.00242	convolutional neural network;brain activity and meditation;cognitive psychology;visual processing;iterative reconstruction;visualization;computer vision;psychology;visual cortex;functional magnetic resonance imaging;decoding methods;artificial intelligence	ML	21.936885045348358	-56.440819054572486	161481
52a8224f221e255d23c4c7db22646e4cd5ec7e42	classification of radar signal features in electronic warfare with convolutional long-short time memory		Radar signals are time series that have pulse repetition interval, pulse width and pulse amplitude as their features. After reception of them by electronic warfare systems, their features are classified and kept in a database. This procedure brings vision for the system user if the same signal is received again in the future. For this classification purpose, three algorithms were implemented. The first one is a combined network consisting of a Convolutional Neural Network (CNN) and a Long-Short Time Memory (LSTM), the second is a Hybrid Network including the first network and a CNN which enables parallel training having histogram data as its input. The last algorithm is Stacked Autoencoder. Performance analysis was made on real radar data. The best performer is Hybrid Network which reached 99.6% accuracy as it involves histogram data usage and an LSTM for the time series problem. Convolutional LSTM reached 98.3% while Stacked Autoencoder had 87% accuracy.	radar	Mustafa Atahan Nuhoglu	2018		10.1109/SIU.2018.8404452	autoencoder;convolutional neural network;pulse repetition frequency;pulse-amplitude modulation;pattern recognition;artificial intelligence;computer science;recurrent neural network;radar;histogram;pulse-width modulation	EDA	21.216107541372665	-59.390456616654674	161721
aecd49e4805a837902b72e870c79b4a92cda9e3b	using machine learning to determine fold class and secondary structure content from raman optical activity and raman vibrational spectroscopy	support vector machines;pls regression;data mining svm rf;data mining;random forests;raman spectroscopy;machine learning roa spectroscopy raman spectroscopy;machine learning;roa spectroscopy;random forests pls regression support vector machines;svm;rf	The objective of this project was to apply machine learning methods to determine protein secondary structure content and protein fold class from ROA and Raman vibrational spectral data.  Raman and ROA are sensitive to biomolecular structure with  the bands of each spectra corresponding  to  structural elements in  proteins and when combined give a fingerprint of the protein. However, there are many bands of which little is known. There is a need, therefore, to find ways of extrapolating information from spectral bands and investigate which regions of the spectra contain the most useful structural information.  Support Vector Machines (SVM) classification and Random Forests (RF) trees classification were used to mine protein fold class information and Partial Least Squares (PLS) regression was used to determine secondary structure content of proteins.  The classification methods were used to group proteins into ?-helix, ?-sheet, ?/? and disordered fold classes. The PLS regression was used to determine percentage protein structural content from Raman and ROA spectral data. The analyses were performed on spectral bin widths of 10cm-1 and on the spectral amide regions I, II and III.  The full spectra and different combinations of the amide regions were also analysed.  The SVM analyses, classification and regression, generally did not perform well. SVM classification models for example, had low Matthew Correlation Coefficient (MCC) values below 0.5 but this is better than a negative value which would indicate a random chance prediction.  The SVM regression analyses also showed very poor performances with average R2 values below 0.5.  R2 is the Pearson?s correlations coefficient and shows how well predicted and observed structural content values correlate. An R2 value 1 indicates a good correlation and therefore a good prediction model. The Partial Least Squares regression analyses yielded much improved results with very high accuracies. Analyses of full spectrum and the spectral amide regions produced high R2 values of 0.8-0.9 for both ROA and Raman spectral data. This high accuracy was also seen in the analysis of the 850-1100 cm-1 backbone region for both ROA and Raman spectra which indicates that this region could have an important contribution to protein structure analysis. 2nd derivative Raman spectra PLS regression analysis showed very improved performance with high accuracy R2 values of 0.81-0.97. The Random Forest algorithm used here for classification showed good performance. The 2-dimensional plots used to visualise the classification clusters showed clear clusters in some analyses, for example tighter clustering was observed for amide I, amide I & III and amide I & II & III spectral regions than for amide II, amide III and amide II&III spectra analysis. The Random Forest algorithm also determines variable importance which showed spectral bins were crucial in the classification decisions. The ROA Random Forest analyses performed generally better than Raman Random Forest analyses.  ROA Random Forest analyses showed 75% as the highest percentage of correctly classified proteins while Raman analyses reported 50% as the highest percentage.The analyses presented in this thesis have shown that Raman and ROA vibrational spectral contains information about protein secondary structure and these data can be extracted using mathematical methods such as the machine learning techniques presented here. The machine learning methods applied in this project were used to mine information about protein secondary structure and the work presented here demonstrated that these techniques are useful and could be powerful tools in the determination protein structure from spectral data.	machine learning;optical rotation;raman scattering	Myra Kinalwa-Nalule	2012			chemistry;bioinformatics;analytical chemistry;machine learning	ML	13.048507770108602	-56.368224123898635	161805
cb116ecaa85c33d052531054b5adb4a6ae640659	contrasting the social cognition of humans and nonhuman apes: the shared intentionality hypothesis	enculturation;mindreading;cooperation;human development;animal cognition;intrinsic motivation;social cognition;culture;shared intentionality;imitation learning;joint attention;joint action;social norm	Joint activities are ubiquitous in the animal kingdom, but they differ substantially in their underlying psychological states. Humans attribute and share mental states with others in the so-called shared intentionality. Our hypothesis is that our closest nonhuman living relatives also attribute some psychological mechanisms such as perceptions and goals to others, but, unlike humans, they are not necessarily intrinsically motivated to share those psychological states. Furthermore, it is postulated that shared intentionality is responsible for the appearance of a suite of behaviors, including joint attention, declarative communication, imitative learning, and teaching, that are the basis of cultural learning and the social norms and traditions present in every human culture.	animals;behavior;humans;intentionality;learning disorders;mental state;norm (social);pongidae;social communication disorder;social cognition	Josep Call	2009	Topics in cognitive science	10.1111/j.1756-8765.2009.01025.x	psychology;human development;cognitive psychology;social cognition;joint attention;developmental psychology;enculturation;artificial intelligence;animal cognition;communication;social psychology;cooperation;cognitive science;culture	ML	20.058803925190574	-64.8946288759332	161827
16b45e9b8dc7f4e90c851f05e6f3bc044bd42521	human activity detection via wifi signals using deep neural networks		This study proposes a WiFi-based activity detection system using deep neural networks to detect the indoor human states. This system captures useful amplitude information from the channel state information and converts the information to two-dimensional arrays. Next, the two-dimensional arrays are used as inputs to deep neural networks to distinguish the moving and stationary states of people. The powerful inference of deep neural networks simplify the feature extraction and also improve the accuracy of the classification of indoor human states. Our prototype shows that the proposed system can work with WiFi signals with even higher accuracy.		Chien-Cheng Lee;Xiu-Chi Huang	2018	2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)	10.1109/UCC-Companion.2018.00017	channel state information;feature extraction;wireless;artificial neural network;stationary state;inference;pattern recognition;computer science;artificial intelligence	Mobile	21.497328440680814	-59.71318997586316	161920
0ad9bd040a777b20f0d0c7113f2e4b236956a17d	multimodal integration of visual place cells and grid cells for navigation tasks of a real robot		In the present study, we propose a model of multimodal place cells merging visual and proprioceptive primitives. First we will briefly present our previous sensory-motor architecture, highlighting limitations of a visual-only based system. Then we will introduce a new model of proprioceptive localization, giving rise to the so-called grid cells, wich are congruent with neurobiological studies made on rodent. Finally we will show how a simple conditionning rule between both modalities can outperform visual-only driven models by producing robust multimodal place cells. Experiments show that this model enhances robot localization and also allows to solve some benchmark problems for real life robotics applications.	benchmark (computing);experiment;multimodal interaction;real life;robotic mapping;robotics;rule 184	Adrien Jauffret;Nicolas Cuperlier;Philippe Gaussier;Philippe Tarroux	2012		10.1007/978-3-642-33093-3_14	embedded system;computer vision;simulation;mobile robot navigation	Robotics	20.739667783430576	-65.09915875658889	163157
a1dc8492360a36b985ba9cea42ad21ecbe8b1c4f	optical and analytical investigations on dengue virus rapid diagnostic test for igm antibody detection	school of engineering sciences	Evaluation of binding between analytes and its relevant ligands on surface plasmon resonance (SPR) biosensor is of considerable importance for accurate determination and screening of an interference in immunosensors. Dengue virus serotype 2 was used as a case study in this investigation. This research work compares and interprets the results obtained from analytical analysis with the experimental ones. Both the theoretical calculations and experimental results are verified with one sample from each category of dengue serotypes 2 (low, mid, and high positive), which have been examined in the database of established laboratorial diagnosis. In order to perform this investigation, the SPR angle variations are calculated, analyzed, and then validated via experimental SPR angle variations. Accordingly, the error ratios of 5.35, 6.54, and 3.72 % were obtained for the low-, mid-, and high-positive-specific immune globulins of patient serums, respectively. In addition, the magnetic fields of the biosensor are numerically simulated to show the effect of different binding mediums.	dengue fever;dengue virus;diagnostic tests;globulins;immunoglobulins;interaction;interference (communication);ligands;magnetic resonance imaging;numerical analysis;patients;serotype;serum;severe dengue;silver;surface plasmon resonance;tree rearrangement;analyte	Peyman Jahanshahi;Shamala Devi Sekaran;Faisal Rafiq Mahmad Adikan	2015	Medical & Biological Engineering & Computing	10.1007/s11517-015-1262-2	medicine;computer science;virology;mathematics;immunology;physics	SE	11.240105482548593	-64.20485901302554	163191
3014b7379ed2c5957553cea31dc3519a7c9ca6c7	ligand binding to domain-3 of human serum albumin: a chemometric analysis	look up table;human serum albumin;ligand binding;drug development	A detailed chemometric analysis of ligand binding to domain-3A of human serum albumin is described. NMR and fluorescence data on a set of 889 chemically diverse compounds were used to develop a group contribution model based on 74 chemical fragments that is in good agreement with the experimental data (R2 = 0.94, Q2 = 0.90). The structural descriptors used in this analysis comprise a convenient look-up table for quantitatively estimating the effect that a particular group will have on albumin binding. This information can be valuable for optimizing a particular series of compounds for drug development.	chemometrics;estimated;fluorescence;ligands;lookup table;serum albumin;drug development	Philip J. Hajduk;Renaldo Mendoza;Andrew M. Petros;Jeffrey R. Huth;Mark G. Bures;Stephen W. Fesik;Yvonne C. Martin	2003	Journal of computer-aided molecular design	10.1023/A:1025305520585	chromatography;biochemistry;stereochemistry;serum albumin;chemistry;lookup table;bioinformatics;drug development;ligand	Comp.	12.132443355913416	-58.73839635546935	163500
99190cc62ee61aa6970bd11380399e48331146d7	automatic classification of mass spectra by means of digital learning nets-existence of characteristic features of chemical class in mass spectra	mass spectra;automatic classification	Abstract   The application of digital learning nets to the classification of mass spectra assumes that there is a relationship between the data and the defined classification categories. The validity of this assumption is demonstrated. A well defined subset of data from a chemical class can be compiled, which is sufficient to enable other spectra from that category to be classified. Further evidence is provided by a comparison of the behaviour of the net with experimental spectral data and random patterns and the consideration of the results of a 28 group classification.		T. John Stonham;M. A. Shaw	1975	Pattern Recognition	10.1016/0031-3203(75)90008-4	mass spectrum;computer science;machine learning;pattern recognition;data mining;mathematics	Vision	13.0044971303122	-56.04778742823883	163505
317248b0db8f67aa00c01d81d8aa871b5517e062	effects of infrared optical trapping on saccharomyces cerevisiae in a microfluidic system	saccharomyces cerevisiae;laser;microfluidics;optical trapping;phototoxicity	Baker's yeast (Saccharomyces cerevisiae) represents a very popular single-celled eukaryotic model organism which has been studied extensively by various methods and whose genome has been completely sequenced. It was also among the first living organisms that were manipulated by optical tweezers and it is currently a frequent subject of optical micromanipulation experiments. We built a microfluidic system for optical trapping experiments with individual cells and used it for the assessment of cell tolerance to phototoxic stress. Using optical tweezers with the wavelength of 1064 nm, we trapped individual Saccharomyces cerevisiae cells for 15 min and, subsequently, observed their stress response in specially designed microfluidic chambers over time periods of several hours by time-lapse video-microscopy. We determined the time between successive bud formations after the exposure to the trapping light, took account of damaged cells, and calculated the population doubling period and cell areas for increasing trapping power at a constant trapping time. Our approach represents an attractive, versatile microfluidic platform for quantitative optical trapping experiments with living cells. We demonstrate its application potential by assessing the limits for safe, non-invasive optical trapping of Saccharomyces cerevisiae with infrared laser light.	benchmark (computing);bud - plant part;cell survival;cell division;experiment;fluorescence;maxima and minima;microfluidics;micromanipulation;morphologic artifacts;optical trap;organism;period-doubling bifurcation;raman scattering;saccharomyces cerevisiae ab.ige:ratio:pt:ser:qn;sorting;voltage-sensitive dye imaging;biological adaptation to stress;wavelength	Zdeněk Pilát;Alexandr Jonás;Jan Ježek;Pavel Zemánek	2017		10.3390/s17112640	analytical chemistry;microfluidics;optical tweezers;trapping;far-infrared laser;laser;saccharomyces cerevisiae;population;biology;yeast	Comp.	11.039025130643843	-64.96083263186524	163766
7a755b6879fb70b108bb49d8ce17ab420f04dbe5	estimation of acute oral toxicity in rat using local lazy learning	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;computer applications in chemistry;theoretical and computational chemistry;computational biology bioinformatics;consensus model;applicability domain;uk phd theses thesis;local lazy learning;life sciences;acute toxicity;uk research reports;medical journals;europe pmc;documentation and information in chemistry;biomedical research;bioinformatics	BACKGROUND Acute toxicity means the ability of a substance to cause adverse effects within a short period following dosing or exposure, which is usually the first step in the toxicological investigations of unknown substances. The median lethal dose, LD50, is frequently used as a general indicator of a substance's acute toxicity, and there is a high demand on developing non-animal-based prediction of LD50. Unfortunately, it is difficult to accurately predict compound LD50 using a single QSAR model, because the acute toxicity may involve complex mechanisms and multiple biochemical processes.   RESULTS In this study, we reported the use of local lazy learning (LLL) methods, which could capture subtle local structure-toxicity relationships around each query compound, to develop LD50 prediction models: (a) local lazy regression (LLR): a linear regression model built using k neighbors; (b) SA: the arithmetical mean of the activities of k nearest neighbors; (c) SR: the weighted mean of the activities of k nearest neighbors; (d) GP: the projection point of the compound on the line defined by its two nearest neighbors. We defined the applicability domain (AD) to decide to what an extent and under what circumstances the prediction is reliable. In the end, we developed a consensus model based on the predicted values of individual LLL models, yielding correlation coefficients R(2) of 0.712 on a test set containing 2,896 compounds.   CONCLUSION Encouraged by the promising results, we expect that our consensus LLL model of LD50 would become a useful tool for predicting acute toxicity. All models developed in this study are available via http://www.dddc.ac.cn/admetus.	adverse reaction to drug;applicability domain;biochemical processes;coefficient;emoticon;k-nearest neighbors algorithm;lazy evaluation;lazy learning;learning disorders;lenstra–lenstra–lovász lattice basis reduction algorithm;lethal dose 50;lucas–lehmer–riesel test;median graph;quantitative structure–activity relationship;question (inquiry);regression analysis;structure of left lower lobe of lung;test set	Jing Lu;Jianlong Peng;Jinan Wang;Qiancheng Shen;Yi Bi;Likun Gong;Mingyue Zheng;Xiaomin Luo;Weiliang Zhu;Hualiang Jiang;Kaixian Chen	2014		10.1186/1758-2946-6-26	biology;medicine;toxicology;computer science;bioinformatics;applicability domain;chronic toxicity;data mining;acute toxicity	ML	11.755848340792637	-56.09162704343236	163787
e5798d6ac440f424b2175e6d02a0d99f55ded448	how accurate does a force field need to be?	force field	A “generalized atom” force field designed to produce quick approximate results for molecular modelling applications is presented. The main assumptions are: (i) angle terms depend only on the nature of the central atom, and (ii) torsion terms depend only on the conjugation about the central bond. These assumptions lead to an easily parameterized force field which can be applied to a wide range of novel structures, and is incorporated in the Chem-X molecular modelling software. To assess its accuracy, molecular modelling parameters calculated using this force field were compared with those obtained by other recognized methods. Central nervous system drugs were selected for study due to their well-characterized structure-activity relationships. In particular the molecular volume, surface area and aromatic-ring-to-nitrogen distances calculated from Chem-X minimized structures are compared with those obtained from MMZP and X-ray studies.	approximation algorithm;atom;force field (chemistry);molecular modelling;pubchem;quaternions and spatial rotation;torsion (gastropod)	E. Keith Davies;N. W. Murrall	1989	Computers & Chemistry	10.1016/0097-8485(89)80007-5	crystallography;biology;chemistry;computer science;force field;computational chemistry	Visualization	12.37072958284173	-59.33237182449964	164287
02d171835b6e0acbe62a70836ebb15fb9c238292	impact of substrate protonation and tautomerization states on interactions with the active site of arginase i	substrate specificity;experimental design;modelizacion;catalytic domain;cancerology;isomerism;cancer;tumor maligno;etude experimentale;inhibiteur enzyme;stabilization;enzyme inhibitor;plan experiencia;conformation;molecular dynamics;complexe enzyme substrat;hydroxyde;binding sites;hidroxido;molecular dynamics simulation;substrate enzyme complex;hydrolysis;dynamique moleculaire;modelisation;conformacion;hydroxides;lugar activo;estabilizacion;plan experience;cancerologie;estructura datos;arginase;protein binding;structure donnee;tumeur maligne;humans;stabilisation;cancerologia;dinamica molecular;hydrolyse;modeling;site actif;data structure;estudio experimental;disordered system;active site;complejo enzima sustrato;systeme desordonne;malignant tumor;arginine;protons;hidrolisis;inhibidor enzima;sistema desordenado	Human arginase is a binuclear manganese metalloenzyme that participates in the urea cycle. Arginase catalyzes the hydrolysis of L-arginine into L-ornithine and urea and is linked to several disorders such as asthma and cancer. Currently, the protonation and tautomerization state of the substrate when bound to the active site, which contains two manganese ions, is not known. Knowledge of the charge-dependent behavior of arginine in the arginase I environment would be of utility toward understanding the catalytic mechanism and designing inhibitors of this enzyme. The arginine(+/0) species, including all possible neutral tautomers, were modeled using an aminoimidazole analog as template. All-atom molecular dynamics simulations were then performed on each of the charged and neutral species. In addition, a hydroxide ion was included in selected simulations to test its importance. Results show that the positively charged state of arginine is stable in the active site of arginase I, with that stabilization facilitated by the presence of hydroxide. Glu277 is indicated to play a role in stabilizing arginine in the active site and facilitating its ability to assume a catalytically competent conformation in the presence of hydroxide. The reported interactions and modeled arginine-bound arginase I structures can be used as a tool for structure-based inhibitor design, as experimental data on the structure of the substrate-enzyme complex is lacking.		Shanthi Nagagarajan;Fengtian Xue;Alexander D. MacKerell	2013	Journal of chemical information and modeling	10.1021/ci300506y	hydrolysis;biochemistry;stereochemistry;molecular dynamics;chemistry;cancer	Robotics	10.166844278197512	-61.59377959743782	164377
a6f85f2fc633361fdcf04cd3924e322bf39b2f15	neural identification of selected apple pests	digital image analysis;identification of apple pests;artificial neural network	The study was based on neural network modeling methods, including image analysis.Artificial neural networks as a powerful tool to identify pests.The color of pests as the dominant input variable of a neural model.The best classification ability was achieved by MLP topology. The subject of this study was to investigate the possibility of using artificial neural networks as a tool for classification, designed to identify apple orchard pests. The paper presents a classification neural model using optimized learning sets acquired on the basis of the information encoded in the form of digital images of selected pests. This study predominantly deals with the problem of the identification of 6 selected apple pests which are most commonly found in Polish orchards. Neural modeling techniques, including digital image analysis, were used to classify the pests.The qualitative analysis of neural models produced, indicates that multi-layered perceptron (MLP) neural network topology achieve the best classification ability. Representative features, allowing for effective pest identification are 23 visual parameters in the form of 7 selected coefficients of shape and 16 color characteristic of pests. The dominant input variables of a neural model, determining the correct identification of the features, contain information about the color of pests.Our results support the hypothesis that artificial neural networks are an effective tool that supports the process of identification of pests in apple orchards. The resulting neural classifier has been created to assist in the decision-making processes that take place during the production of apples, in the context of protection against pests.	artificial neural network;color;image analysis;multilayer perceptron;multitier architecture;simulation	Piotr Boniecki;Krzysztof Koszela;Hanna Piekarska-Boniecka;Jerzy Weres;Maciej Zaborowicz;Sebastian Kujawa;Arkadiusz Majewski;Barbara Raba	2015	Computers and Electronics in Agriculture	10.1016/j.compag.2014.09.013	computer science;engineering;artificial intelligence;machine learning;data mining;artificial neural network	AI	21.31079689603392	-62.868576462043485	164459
1b6add647cfe041c22c214db554ad7a37aa98fa6	the ruggedness of protein-protein energy landscape and the cutoff for 1/rn potentials	energy;protein complex;proteine;energia;monotone function;binding sites;structural bioinformatics;energie;protein conformation;structure prediction;multiprotein complexes;thermodynamics;algorithms;protein folding;proteina;protein interaction mapping;computational biology;protein;kinetics;critical value;energy landscape	MOTIVATION Computational studies of the energetics of protein association are important for revealing the underlying fundamental principles and for designing better tools to model protein complexes. The interaction cutoff contribution to the ruggedness of protein-protein energy landscape is studied in terms of relative energy fluctuations for 1/r(n) potentials based on a simplistic model of a protein complex. This artificial ruggedness exists for short cutoffs and gradually disappears with the cutoff increase.   RESULTS The critical values of the cutoff were calculated for each of 11 popular power-type potentials with n=0/9, 12 and for two thresholds of 5% and 10%. The artificial ruggedness decreases to tolerable thresholds for cutoffs larger than the critical ones. The results showed that for both thresholds the critical cutoff is a non-monotonic function of the potential power n. The functions reach the maximum at n=3/4 and then decrease with the increase of the potential power. The difference between two cutoffs for 5% and 10% artificial ruggedness becomes negligible for potentials decreasing faster than 1/r(12). The analytical results obtained for the simple model of protein complexes agree with the analysis of artificial ruggedness in a dataset of 62 protein-protein complexes, with different parameterizations of soft Lennard-Jones potential and two types of protein representations: all-atom and coarse-grained. The results suggest that cutoffs larger than the critical ones can be recommended for protein-protein potentials.	computation;jones calculus;large;lennard-jones potential	Anatoly M. Ruvinsky;Ilya A. Vakser	2009	Bioinformatics	10.1093/bioinformatics/btp108	protein folding;biology;protein structure;energy;critical value;monotonic function;bioinformatics;binding site;energy landscape;structural bioinformatics;multiprotein complex;kinetics	Comp.	10.691455585238149	-63.10610394795522	165178
f232355cfa25f536048571927cd562a40d032c16	the concept of template-based de novo design from drug-derived molecular fragments and its application to tar rna	building block;human immunodeficiency virus;automatic generation;virtual screening;drug design;de novo design;evolutionary algorithm;chemical synthesis;perfect match	Principles of fragment-based molecular design are presented and discussed in the context of de novo drug design. The underlying idea is to dissect known drug molecules in fragments by straightforward pseudo-retro-synthesis. The resulting building blocks are then used for automated assembly of new molecules. A particular question has been whether this approach is actually able to perform scaffold-hopping. A prospective case study illustrates the usefulness of fragment-based de novo design for finding new scaffolds. We were able to identify a novel ligand disrupting the interaction between the Tat peptide and TAR RNA, which is part of the human immunodeficiency virus (HIV-1) mRNA. Using a single template structure (acetylpromazine) as reference molecule and a topological pharmacophore descriptor (CATS), new chemotypes were automatically generated by our de novo design software Flux. Flux features an evolutionary algorithm for fragment-based compound assembly and optimization. Pharmacophore superimposition and docking into the target RNA suggest perfect matching between the template molecule and the designed compound. Chemical synthesis was straightforward, and bioactivity of the designed molecule was confirmed in a FRET assay. This study demonstrates the practicability of de novo design to generating RNA ligands containing novel molecular scaffolds.	acepromazine;beilstein database;blue (queue management algorithm);boat dock;chemical space;clinical use template;compiler;de novo transcriptome assembly;docking (molecular);donald becker;drug design;evolutionary algorithm;fifty nine;fluorescence resonance energy transfer;fluorine;forty nine;frequency-hopping spread spectrum;hiv;immunologic deficiency syndromes;interaction;ligands;matching (graph theory);mathematical optimization;mitral valve prolapse syndrome;pharmacophore;prospective search;pseudo brand of pseudoephedrine;rna;spatial variability;synthetic data;tar dosage form;tars;thrombocytopenia-absent radius syndrome;halide	Andreas Schüller;Marcel Suhartono;Uli Fechner;Yusuf Tanrikulu;Sven Breitung;Ute Scheffer;Michael W. Göbel;Gisbert Schneider	2008	Journal of computer-aided molecular design	10.1007/s10822-007-9157-4	biology;chemical synthesis;chemistry;virtual screening;bioinformatics;evolutionary algorithm;organic chemistry;combinatorial chemistry;computational chemistry;nanotechnology;genetics;drug design	Comp.	10.851201669103704	-60.2121697859515	165510
593c98aadfac3cfbe2a2b36f7d9c597be884d991	modeling and simulation of the initial phases of chlorophyll fluorescence from photosystem ii	electron transport chain;modeling and simulation;kinetic model;photosystem ii;least square;quantitative analysis;kinetics;modeling;photosynthesis;chlorophyll fluorescence	A simple kinetic model structure for chlorophyll fluorescence (ChlF) from Photosystem II (PSII) offers practical usefulness in quantitative analysis and extraction of information from measured ChlF. In this work, the major PSII phototransduction kinetics was represented with only five state variables. Parameters were estimated through a least-squares algorithm. The developed model structure could produce the well-known OJIP pattern and fit measured ChlF. Influences of PQ pool size, active Q(B) sites, and Q(A) reduction rate on ChlF emission were simulated and discussed in light of the existing literature.	kinetics internet protocol;least squares;light signal transduction;photosystem ii;simulation;whole earth 'lectronic link;algorithm;chlorophyll fluorescence	Ya Guo;Jinglu Tan	2011	Bio Systems	10.1016/j.biosystems.2010.10.008	biology;botany;systems modeling;chlorophyll fluorescence;electron transport chain;quantitative analysis;photosystem ii;modeling and simulation;photosynthesis;least squares;kinetics	AI	12.429288716344788	-58.601448988529015	165559
84a0bd705e49cff28e28459bedbdadf1ca656ad8	multifingerprint based similarity searches for targeted class compound selection	similarity search	Molecular fingerprints are widely used for similarity-based virtual screening in drug discovery projects. In this paper we discuss the performance and the complementarity of nine two-dimensional fingerprints (Daylight, Unity, AlFi, Hologram, CATS, TRUST, Molprint 2D, ChemGPS, and ALOGP) in retrieving active molecules by similarity searching against a set of query compounds. For this purpose, we used biological data from HTS screening campaigns of four protein families (GPCRs, kinases, ion channels, and proteases). We have established threshold values for the similarity index (Tanimoto index) to be used as starting points for similarity searches. Based on the complementarities between the selections made by using different fingerprints we propose a multifingerprint approach as an efficient tool to balance the strengths and weaknesses of various fingerprints.		Thierry Kogej;Ola Engkvist;Niklas Blomberg;Sorel Muresan	2006	Journal of chemical information and modeling	10.1021/ci0504723	bioinformatics;machine learning;data mining;mathematics;nearest neighbor search;world wide web	Comp.	10.88988642779085	-58.56124397374107	165589
b334db7bc9325937c1efb8e533fa0aa362a19ba2	combinatorial docking and combinatorial chemistry: design of potent non-peptide thrombin inhibitors	score function;combinatorial chemistry;molecular weight;x ray structure;de novo design;chemical reaction	A computational algorithm was used to design automatically novel thrombin inhibitors that are available from a single-step chemical reaction. The compounds do not contain amide bonds, are achiral and have a molecular weight below 400. Of the 10 compounds that were synthesized, five bind to thrombin with a Ki in the nanomolar range. Subsequent X-ray structure determination of the thrombin-inhibitor complex for the best compound (Ki = 95 nM) confirms the predicted binding mode. The novel algorithm is applicable to a broad range of chemical reactions.		Hans-Joachim Böhm;David W. Banner;Lutz Weber	1999	Journal of computer-aided molecular design	10.1023/A:1008040531766	stereochemistry;chemistry;chemical reaction;organic chemistry;combinatorial chemistry;score;molecular mass	Graphics	10.454652144651837	-59.92179452724377	165894
0ade119b08626bcd0053b4608d71c07fd1a35d66	volcano plots in analyzing differential expressions with mrna microarrays	effect size;sample size;fold change;regularization;microarray analysis;differential expression;volcano plot;microarray;signal to noise ratio;gene selection;quantitative method	"""A volcano plot displays unstandardized signal (e.g. log-fold-change) against noise-adjusted/standardized signal (e.g. t-statistic or -log(10)(p-value) from the t-test). We review the basic and interactive use of the volcano plot and its crucial role in understanding the regularized t-statistic. The joint filtering gene selection criterion based on regularized statistics has a curved discriminant line in the volcano plot, as compared to the two perpendicular lines for the """"double filtering"""" criterion. This review attempts to provide a unifying framework for discussions on alternative measures of differential expression, improved methods for estimating variance, and visual display of a microarray analysis result. We also discuss the possibility of applying volcano plots to other fields beyond microarray."""	discriminant;estimated;line level;microarray;recurrence plot;sample variance;volcano plot (statistics);t test	Wentian Li	2012	Journal of bioinformatics and computational biology	10.1142/S0219720012310038	gene-centered view of evolution;sample size determination;biology;regularization;microarray analysis techniques;econometrics;quantitative research;computer science;bioinformatics;microarray;mathematics;volcano plot;signal-to-noise ratio;effect size;statistics	Comp.	13.742969618403531	-54.124210249342944	166034
644b0f9d96c0f4bd69e282b90aa317ae09ef191e	synthesized computational aesthetic evaluation of photos	photo;classification;aesthetic evaluation;feature extraction;mobile application	Assessing aesthetic appeal of images is a highly subjective task which has attracted a lot of interests recently. It is an interdisciplinary subject related to art, psychology, and computer vision. In this paper, we systematically study prior researches of feature extraction in this area, and category them into four groups, low level, rule based, information theory, and visual attention. In each group, the effectiveness and limitations of existing features are examined. Based on the analysis, we propose a comprehensive feature set, which include 16 novel features and 70 well proved features. With this feature set, we build the system under machine learning scheme consisting of an SVM based classifier to estimate if an image is high aesthetic or low aesthetic. The experiments are conducted on public datasets show that our comprehensive feature set outperforms conventional models that concentrate mainly on certain types of features. The combination of our features produces a promising classification accuracy of 82.4% and a good performance comparable to aesthetic rating of human. Finally, we implemented the proposed evaluation system on mobile devices. It can provide real-time feedback to help users capture appealing	academy;algorithmic efficiency;analysis of algorithms;computer vision;experiment;feature extraction;image quality;information theory;logic programming;machine learning;mobile device;online shopping;protein structure prediction;real-time clock;real-time computing;support vector machine	Weining Wang;Dong Cai;Li Wang;Qinghua Huang;Xiangmin Xu;Xuelong Li	2016	Neurocomputing	10.1016/j.neucom.2014.12.106	computer vision;feature extraction;biological classification;computer science;machine learning;data mining;multimedia;feature	Web+IR	23.443934564553683	-58.17709442443375	166552
f3ffa028196bc1b4449285f3d8e04b0d9102ee4e	a comparison of different functions for predicted protein model quality assessment	protein structure prediction;qmean;secondary structure agreement	In protein structure prediction, a considerable number of models are usually produced by either the Template-Based Method (TBM) or the ab initio prediction. The purpose of this study is to find the critical parameter in assessing the quality of the predicted models. A non-redundant template library was developed and 138 target sequences were modeled. The target sequences were all distant from the proteins in the template library and were aligned with template library proteins on the basis of the transformation matrix. The quality of each model was first assessed with QMEAN and its six parameters, which are C_β interaction energy (C_beta), all-atom pairwise energy (PE), solvation energy (SE), torsion angle energy (TAE), secondary structure agreement (SSA), and solvent accessibility agreement (SAE). Finally, the alignment score (score) was also used to assess the quality of model. Hence, a total of eight parameters (i.e., QMEAN, C_beta, PE, SE, TAE, SSA, SAE, score) were independently used to assess the quality of each model. The results indicate that SSA is the best parameter to estimate the quality of the model.		Juan Li;Huisheng Fang	2016	Journal of computer-aided molecular design	10.1007/s10822-016-9924-1	simulation;bioinformatics;data mining	Comp.	11.827498094590576	-59.53745408151267	166681
387058c1697d82eebf86dd8b4c06de661d178bfe	synthesis and bioconjugation of gold nanoparticles as potential molecular probes for light-based imaging techniques	health research;uk clinical guidelines;biological patents;gold nanoparticle;europe pubmed central;citation search;biomedical imaging;breast carcinoma;monoclonal antibody;optical imaging;uk phd theses thesis;life sciences;contrast agent;uk research reports;medical journals;europe pmc;biomedical research;molecular probe;bioinformatics	"""We have synthesized and characterized gold nanoparticles (spheres and rods) with optical extinction bands within the """"optical imaging window."""" The intense plasmon resonant driven absorption and scattering peaks of these nanoparticles make them suitable as contrast agents for optical imaging techniques. Further, we have conjugated these gold nanoparticles to a mouse monoclonal antibody specific to HER2 overexpressing SKBR3 breast carcinoma cells. The bioconjugation protocol uses noncovalent modes of binding based on a combination of electrostatic and hydrophobic interactions of the antibody and the gold surface. We discuss various aspects of the synthesis and bioconjugation protocols and the characterization results of the functionalized nanoparticles. Some proposed applications of these potential molecular probes in the field of biomedical imaging are also discussed."""	adverse reaction to drug;bands;blood supply aspects;breast carcinoma;contrast media;heparin, low-molecular-weight;imaging techniques;immunostimulating conjugate (antigen);interaction;medical imaging;molecular probes;monoclonal antibodies;nanorods;optical imaging;plasmon;protocols documentation;rod photoreceptors;video-in video-out;cellular targeting;tumor tissue	Raja Gopal Rayavarapu;Wilma Petersen;Constantin Ungureanu;Janine N. Post;Ton G. van Leeuwen;Srirang Manohar	2007	International Journal of Biomedical Imaging	10.1155/2007/29817	medical imaging;medicine;pathology;bioinformatics;optical imaging;monoclonal antibody;molecular probe	Visualization	11.427786054492612	-65.38312729033046	166772
e33c235d59471a50e5133c21324e5943271a3ca7	learning protein binding affinity using privileged information	machine learning;privileged information;protein binding affinity prediction;protein-protein interactions	BACKGROUND Determining protein-protein interactions and their binding affinity are important in understanding cellular biological processes, discovery and design of novel therapeutics, protein engineering, and mutagenesis studies. Due to the time and effort required in wet lab experiments, computational prediction of binding affinity from sequence or structure is an important area of research. Structure-based methods, though more accurate than sequence-based techniques, are limited in their applicability due to limited availability of protein structure data.   RESULTS In this study, we propose a novel machine learning method for predicting binding affinity that uses protein 3D structure as privileged information at training time while expecting only protein sequence information during testing. Using the method, which is based on the framework of learning using privileged information (LUPI), we have achieved improved performance over corresponding sequence-based binding affinity prediction methods that do not have access to privileged information during training. Our experiments show that with the proposed framework which uses structure only during training, it is possible to achieve classification performance comparable to that which is obtained using structure-based features. Evaluation on an independent test set shows improved performance over the PPA-Pred2 method as well.   CONCLUSIONS The proposed method outperforms several baseline learners and a state-of-the-art binding affinity predictor not only in cross-validation, but also on an additional validation dataset, demonstrating the utility of the LUPI framework for problems that would benefit from classification using structure-based features. The implementation of LUPI developed for this work is expected to be useful in other areas of bioinformatics as well.	amino acid sequence;baseline (configuration management);bioinformatics;cross reactions;cross-validation (statistics);experiment;kerrison predictor;limited availability;machine learning;phenylpropanolamine;processor affinity;protein engineering;protein structure prediction;silo (dataset);test set;therapeutic procedure;protein protein interaction	Wajid Arshad Abbasi;Amina Asif;Asa Ben-Hur;Fayyaz ul Amir Afsar Minhas	2018		10.1186/s12859-018-2448-z		Comp.	10.331109970685684	-56.06755182991816	166811
f6b114b76109a2e06c34c63f86889db86597e4ce	a connectionist system for learning and recognition of structures: application to handwritten characters	eficacia sistema;connectionist models;image processing;connectionism;learning;caracter manuscrito;edge detection;conexionismo;manuscript character;performance systeme;procesamiento imagen;multilayer perceptron;line detection;transformacion hough;system performance;traitement image;deteccion contorno;aprendizaje;perceptron multicouche;detection contour;connexionnisme;apprentissage;reconnaissance caractere;hough transformation;hough transform;transformation hough;caractere manuscrit;character recognition;reconocimiento caracter;multilayered perceptron	A connectionist system for learning and recognition o f structures is developed. The system is a cascade o f two different modules, one for detecting linear structures (primitives) and the other for integrating these linear structures. A connectionist model implementing Hough transform has been used for the first module. The peaks in the Hough space are found by iterative verification method. A multilayered perceptron (four layers) with suitably chosen number o f nodes and links has been used for the second module. As long as the size of the output layer o f first module remains fixed (even i f the size of input image changes), the same second module can be used and this is because the modules operate independently. The system performance is tested on handwritten Bengali character set. Keywords----Hough transform, Multilayered perceptron, Line detection, Character recognition. 1. I N T R O D U C T I O N An object can be described in terms of descriptions of its parts (primitives) and spatial arrangements (relations) of the parts (noted as structural description). Recognition of the structures basically involves matching of a candidate structure with some prototype structures stored in the model base (Shapiro & Haralick, 1982; Shapiro et al., 1984; Boyer, Vayda, & Kak, 1986; Basak et al., 1993). The main difficulty in structure matching problems is that the presence of noise (and/ or vagueness) may change the description of some of the primitives, thereby affecting the matching performance. Assigning some weights to the primitives and to the relations (reflecting their importance in characterizing various classes) helps, to some extent, in achieving noise tolerance and in handling impreciseness in input. These weights will be higher for the primitives and relations that are most consistent (i.e., important) in characterizing a class. Structural description is widely used in different problems like shape matching, stereo matching, character recognition, etc. In all these problems descriptions should be such that the effect of noise gracefully deAcknowledgements: The work was done while Prof. S. K. Pal held Jawharlal Nehru Fellowship. Thanks are also due to Mr. S. Chakraborty for preparing the diagrams and Mr. A. Mukherjee for his active help in capturing the character images. Requests for reprints should be sent to Jayanta Basak, Machine Intelligence Unit, Indian Statistical Institute, Calcutta 700 035, India. grades the performance of the system. Therefore, to design a recognition system based on structural description, one should pay attention to the proper extraction of the primitives and assignment of weights to the primitives and the relations. The extracted primitives (features) should be as robust as possible. Moreover, the system should be able to assign these weights automatically (supervised or unsupervised learning). For designing a pattern recognition or vision system, one wishes to achieve robustness, adaptability (capability of learning the variations ), and fastness (for realtime applications). Neural networks (Lippmann, 1987; Feldman & Ballard, 1982; Fahlmann & Hinton, 1987), having capability to learn from examples, and having robustness and scopes for parallelism, have recently been used for designing more intelligent recognition systems. The objective of the present investigation is to design a scheme for structural pattern learning and recognition within a connectionist framework. The problem of handwritten character recognition is considered as a candidate for the development of the scheme. Before describing the proposed system, we give a brief review of the neural network-based character recognition systems. In the literature, there exist various approaches based on neural networks for the character recognition problem. Possibly the first attempt was made by Fukushima ( 1987, 1982) for 2-D object recognition. The model (neocognitron) can recognize position and scale	character encoding;computer stereo vision;connectionism;diagram;edge detection;existential quantification;handwriting recognition;hough transform;iterative method;neocognitron;neural networks;optical character recognition;outline of object recognition;parallel computing;pattern matching;pattern recognition;perceptron;prototype;real-time computing;robert haralick;sensor;shapiro polynomials;sobel operator;structural pattern;supervised learning;unsupervised learning;vagueness;word lists by frequency	Jayanta Basak;Nikhil R. Pal;Sankar K. Pal	1995	Neural Networks	10.1016/0893-6080(94)00101-Q	hough transform;connectionism;speech recognition;image processing;computer science;artificial intelligence;machine learning	AI	23.421345273747537	-62.922466021643466	167036
42b044762238e44f55ec69aac9337cac372f8067	large-scale ligand-based predictive modelling using support vector machines	computer applications in chemistry;theoretical and computational chemistry;computational biology bioinformatics;documentation and information in chemistry	The increasing size of datasets in drug discovery makes it challenging to build robust and accurate predictive models within a reasonable amount of time. In order to investigate the effect of dataset sizes on predictive performance and modelling time, ligand-based regression models were trained on open datasets of varying sizes of up to 1.2 million chemical structures. For modelling, two implementations of support vector machines (SVM) were used. Chemical structures were described by the signatures molecular descriptor. Results showed that for the larger datasets, the LIBLINEAR SVM implementation performed on par with the well-established libsvm with a radial basis function kernel, but with dramatically less time for model building even on modest computer resources. Using a non-linear kernel proved to be infeasible for large data sizes, even with substantial computational resources on a computer cluster. To deploy the resulting models, we extended the Bioclipse decision support framework to support models from LIBLINEAR and made our models of logD and solubility available from within Bioclipse.	a library for support vector machines;antivirus software;bioclipse;computational technique;computational resource;computer cluster;decision support system;drug discovery;large;ligands;molecular descriptor;nonlinear system;predictive modelling;radial (radio);radial basis function kernel;silo (dataset);support vector machine;whole earth 'lectronic link	Jonathan Alvarsson;Samuel Lampa;Wesley Schaal;Claes R. Andersson;Jarl E. S. Wikberg;Ola Spjuth	2016		10.1186/s13321-016-0151-5	computer science;bioinformatics;machine learning;data mining	ML	10.849096593118569	-55.41208617003505	167208
8d896c4fb7a72e776b95fb667d66bdf1ac03afa6	identification of h-nmr spectra of xyloglucan oligosaccharides: a comparative study of artificial neural networks and bayesian classification using nonparametric density estimation	artificial neural network;bayesian classification	Human anatomy Genetics Microbiology Neuroscience Cell biology Protein NMR spectroscopy Quantum physics Bioinformatics.	artificial neural network;bayesian network;bioinformatics;computational anatomy;naive bayes classifier;quantum mechanics;rca spectra 70	Faramarz Valafar;Homayoun Valafar;William S. York	1999			naive bayes classifier;density estimation;machine learning;xyloglucan;artificial neural network;proton nmr;artificial intelligence;pattern recognition;nonparametric statistics;mathematics	ML	13.063683037218713	-54.98124515201277	167312
c78c4a8cbbb78da2c9e275eacee15475aa57755d	leveraging chemical background knowledge for the prediction of growth inhibition	anticancer agents;tumours biochemistry cancer cellular biophysics data mining database management systems medical information systems molecular biophysics molecular configurations;growth inhibition;cancer;database management systems;molecular configurations;fragment generation process;chemical background knowledge;tumours;data mining;tumor cells;mean absolute error;human tumor cell line screening database;medical information systems;mechanism of action;molecular biophysics;background knowledge;data mining chemical background knowledge growth inhibition prediction molecular structure activity relationships noncongeneric compounds human tumor cell line screening database anticancer agents fragment generation process mean absolute error;growth inhibition prediction;noncongeneric compounds;molecular structure activity relationships;databases tumors chemical compounds desktop publishing humans inhibitors dna gene expression predictive models testing;cellular biophysics;biochemistry;structured data	We show how chemical background knowledge can he used to improve the prediction performance in structure-activitity relationships (SARs) for non-congeneric compounds. The goal of the study is to build a model of growth-inhibition for the NCI DTP human tumor cell line screening data. The SAR model is based on frequent molecular fragments generated from the structure data. Background knowledge in the form of standard anti-cancer agents (ACAs) grouped by known mechanisms of action is available and used twice: First, the standard agents are treated separately in the fragment generation process. Second, we represent each molecule in terms of the similarities with structures known to be associated with certain mechanisms of action. In experiments, we show that using chemical background knowledge in this way reduces the mean absolute error (MAE) by about 5% compared to initial experiments, and by 9% compared to a previous publication. We conjecture that specific instances and groups of instances are a commonly occurring type of background knowledge that is particularly easy to use and effective in practice	approximation error;experiment;mean squared error;nc (complexity)	Lothar Richter;Stefan Hechtl;Stefan Kramer	2006	Sixth IEEE Symposium on BioInformatics and BioEngineering (BIBE'06)	10.1109/BIBE.2006.253296	biology;biochemistry;data model;toxicology;computer science;bioinformatics;data mining;mean absolute error;cancer;molecular biophysics	Web+IR	10.855010833474388	-54.82538815952114	167355
705be4cd6d77b391a59733cb6c1f34d140869080	a hybrid learning approach for better recognition of visual objects	vision system;image processing;neural networks;learning;multiple classifiers;hybrid learning;pattern recognition;artificial intelligence;mathematics computers information science management law miscellaneous	Real world images often contain similar objects but with different rotations, noise, or other visual alterations. Vision systems should be able to recognize objects regardless of these visual alterations. This paper presents a novel approach for learning optimized structures of classifiers for recognizing visual objects regardless of certain types of visual alterations. The approach consists of two phases. The first phase is concerned with learning classifications of a set of standard and altered objects. The second phase is concerned with discovering an optimized structure of classifiers for recognizing objects from unseen images. This paper presents an application of this approach to a domain of 15 classes of hand gestures. The experimental results show significant improvement in the recognition rate rather than using a single classifier or multiple classifiers with thresholds.	visual objects	Ibrahim F. Imam;Srinivas Gutta	1996			computer vision;image processing;computer science;artificial intelligence;machine learning	Vision	23.241898236790046	-63.361020498846266	167422
f561f2c42e8009cb89b6bc1c9b274e41c1c57b09	an api and visual environment to use neural network to reason about source code		Neural networks are gaining popularity in software engineering. This paper presents a dedicated API and visual environment to train and use a neural networks on software source code related data. This short paper illustrates the API using two examples involving prediction of source code properties.	application programming interface;artificial neural network;neural network software;software engineering	Alexandre Bergel;Paulin Melatagia;Serge Stinckwich	2018		10.1145/3191697.3214340	theoretical computer science;artificial neural network;software;popularity;source code;computer science	SE	17.195196404526005	-52.620664360645286	167950
1bf8fc6a274a038e497819515c1a56cb4923cc1b	enhanced detectability of community structure in multilayer networks through layer aggregation		Many systems are naturally represented by a multilayer network in which edges exist in multiple layers that encode different, but potentially related, types of interactions, and it is important to understand limitations on the detectability of community structure in these networks. Using random matrix theory, we analyze detectability limitations for multilayer (specifically, multiplex) stochastic block models (SBMs) in which L layers are derived from a common SBM. We study the effect of layer aggregation on detectability for several aggregation methods, including summation of the layers' adjacency matrices for which we show the detectability limit vanishes as O(L^{-1/2}) with increasing number of layers, L. Importantly, we find a similar scaling behavior when the summation is thresholded at an optimal value, providing insight into the common-but not well understood-practice of thresholding pairwise-interaction data to obtain sparse network representations.	adjacency matrix;encode;image scaling;interaction;internal pyramidal layer of cerebral cortex;mobitz type ii atrioventricular block;multiplexing;optimization problem;sparse matrix;summation (document);super bit mapping;thresholding (image processing);anatomical layer	Dane Taylor;Saray Shai;Natalie Stanley;Peter J. Mucha	2016	Physical review letters	10.1103/PhysRevLett.116.228301	scaling;random matrix;physics;condensed matter physics;statistical physics;encode;adjacency matrix;community structure;thresholding	ML	15.485765366080468	-53.00957732386029	167993
55b02ea4bb60d5dde9705597d2e8601ce9e9c8bb	workflow and methods of high-content time-lapse analysis for quantifying intracellular calcium signals	cho cells;software;animals;cricetinae;oxidative stress;signal analysis;calcium oscillation;amyloid precursor protein;high content image analysis;calcium;signal processing computer assisted;familial alzheimer s disease;intracellular fluid;presenilin 1;time factors;cricetulus;calcium signaling;algorithms;experimental validation;image analysis;humans;hydrogen peroxide;intracellular calcium;mutant presenilin 1;intracellular ca2;alzheimer disease;software validation;automatic data processing;cell line	Calcium ions (Ca2+) play a fundamental role in a variety of physiological functions in many cell types by acting as a secondary messenger. Variation of intracellular Ca2+ concentration ([Ca2+]i) is often observed when the cell is stimulated. However, it is a challenging task to automatically quantify intracellular [Ca2+]i in a population of cells. In this study, we present a workflow including specific algorithms for the automated intracellular calcium signal analysis using high-content, time-lapse cellular images. The experimental validations indicate the effectiveness of the proposed workflow and algorithms. We applied the workflow to analyze the intracellular calcium signals induced by different concentrations of H2O2 in the cell lines transfected by presenilin-1 (PS-1) that is known to be closely related to the familial Alzheimer’s disease (FAD). The analysis results imply an important role of mutant PS-1, but not normal human PS-1 and mutant human amyloid precursor protein (APP), in enhancing intracellular calcium signaling induced by H2O2.	app protein, human;alzheimer's disease;amyloid beta-protein precursor;calcium signaling;calcium ion;cell (microprocessor);confidence intervals;cultured cell line;displacement mapping;frame (physical object);grayscale;hydrogen peroxide;ions;kramer graph;matching;matlab;microsoft windows;presenilin-1;psychologic displacement;quantitation;sequence analysis;signal processing;algorithm	Fuhai Li;Xiaobo Zhou;Jinmin Zhu;Weiming Xia;Jinwen Ma;Stephen T. C. Wong	2008	Neuroinformatics	10.1007/s12021-008-9016-z	amyloid precursor protein;image analysis;calcium;calcium signaling;pathology;computer science;bioinformatics;hydrogen peroxide;genetics	ML	11.96781899425315	-65.73563760809999	168002
032f674477e8aef20b5884634a4ba7af6f7e4e33	rosettadesign server for protein design	software;protein design;hydrogen bond;low energy;amino acid;amino acid sequence;simulated annealing;satisfiability;protein structure;internet;protein conformation;monte carlo method;experimental validation;user computer interface;monte carlo;protein engineering;sequence analysis protein	The RosettaDesign server identifies low energy amino acid sequences for target protein structures (http://rosettadesign.med.unc.edu). The client provides the backbone coordinates of the target structure and specifies which residues to design. The server returns to the client the sequences, coordinates and energies of the designed proteins. The simulations are performed using the design module of the Rosetta program (RosettaDesign). RosettaDesign uses Monte Carlo optimization with simulated annealing to search for amino acids that pack well on the target structure and satisfy hydrogen bonding potential. RosettaDesign has been experimentally validated and has been used previously to stabilize naturally occurring proteins and design a novel protein structure.	amino acid sequence;amino acids;energy, physics;experiment;hydrogen bonding;internet backbone;mathematical optimization;monte carlo method;pyschological bonding;server (computer);server (computing);simulated annealing;simulation;vertebral column	Yi Liu;Brian Kuhlman	2006	Nucleic Acids Research	10.1093/nar/gkl163	biology;protein structure;bioinformatics;monte carlo method	Comp.	11.42060297699723	-60.92507945185413	168172
f8573ef1263252c831272ae4edee1b1aea61dee2	importance of improving scoring methods in predicting protein free-energy changes	biology computing;protein design;score function;molecular configurations;genetics;proteins biology computing cellular biophysics genetics molecular biophysics molecular configurations;protein database scoring methods protein free energy changes protein structure prediction protein structure sampling protein design rosetta software protein backbone flexibility backrub software mutagenesis;protein structure;proteins predictive models;proteins;protein structure prediction;molecular biophysics;prediction accuracy;predictive models;prediction model;free energy;cellular biophysics	Improving the accuracy of protein structure prediction or sampling as well as the scoring function is the central problem in the current computational study of protein structure and function. Recently, a protocol in the protein design software ROSETTA, called “backrub”, was developed to sample protein backbone flexibility and shown to be able to improve the prediction accuracy of protein structural changes upon mutagenesis. Therefore, it should be very interesting to see if the improvement on structure sampling could as well improve the scoring results such as protein free-energy changes when mutations are introduced. In this paper, a protein database is created to evaluate the effect of this backrub protocol on the prediction of protein free-energy changes upon mutagenesis. The results showed that the improvement on the accuracy of protein structure prediction or sampling alone failed to improve the prediction of protein free-energy changes, and suggested that the further improvement in the current scoring function is the bottleneck.	history of google;internet backbone;protein structure prediction;sampling (signal processing);scoring functions for docking	Sen Liu	2012	Proceedings of 2012 IEEE-EMBS International Conference on Biomedical and Health Informatics	10.1109/BHI.2012.6211654	biology;biophysics;bioinformatics	Comp.	10.866795904661616	-55.14191563345723	168281
1f2c1273c7f03ac6ea8b56558e86a23ddd4c7b4f	inverse protein folding in 2d hp model	biology computing;amino acid sequence;protein protein interactions inverse protein folding 2d hydrophobic polar model amino acid sequence native protein fold drug design;proteins;drug design;molecular biophysics;protein protein interaction;protein folding;biology computing proteins molecular biophysics physiological models;physiological models;proteins lattices amino acids sequences drugs stability approximation algorithms mathematical model mathematics biological system modeling	The inverse protein folding problem is that of designing an amino acid sequence which has a particular native protein fold. This problem arises in drug design where a particular structure is necessary to ensure proper protein-protein interactions. In this paper we show that in the 2D HP model of Dill it is possible to solve this problem for a broad class of structures. These structures can be used to closely approximate any given structure. One of the most important properties of a good protein is its stability - the aptitude not to fold simultaneously into other structures. We show that for a number of basic structures, our sequences have a unique fold.	amino acid sequence;amino acids;anethum graveolens;approximation algorithm;bioinformatics;collection of computer science bibliographies;column (database);computation;drug design;fold (higher-order function);linear programming relaxation;optimal design;protein structure prediction;aptitude;monomer;protein folding;protein protein interaction	Arvind Gupta;Ján Manuch;Ladislav Stacho	2004	Proceedings. 2004 IEEE Computational Systems Bioinformatics Conference, 2004. CSB 2004.	10.1109/CSB.2004.1332444	protein–protein interaction;threading;crystallography;protein folding;biology;biochemistry;protein structure;homology modeling;bioinformatics;loop modeling;protein structure prediction;protein engineering;structural biology;peptide sequence;protein function prediction;protein design;drug design;molecular biophysics	Comp.	11.874615270487512	-53.48649889003333	168573
32d3914e947f73b4d2caf9349b7863d5f18edcb5	a neural wake-sleep learning architecture for associating robotic facial emotions	unsupervised learning;facial expression recognition;learning algorithm;human computer interaction;individual helmholtz machines;mirror neuron;supervised learning;magnetic heads;emotion recognition;associative learning;joints;robotic facial emotions;neuroscience;emotional information processing;robot facial expressions;face recognition;neural wake sleep learning architecture;machine learning;robots;information processing;emotional responses;humans;facial expression;amygdala;unsupervised learning emotion recognition robots;emotional information processing neural wake sleep learning architecture robotic facial emotions robot facial expressions associative learning emotional responses amygdala individual helmholtz machines unsupervised learning supervised learning facial expression recognition;human brain;biological neural networks;biological neural networks joints robots face recognition emotion recognition information processing humans	A novel wake-sleep learning architecture for processing a robotpsilas facial expressions is introduced. According to neuroscience evidence, associative learning of emotional responses and facial expressions occurs in the brain in the amygdala. Here we propose an architecture inspired by how the amygdala receives information from other areas of the brain to discriminate it and generate innate responses. The architecture is composed of many individual Helmholtz machines using the wake-sleep learning algorithm for performing information transformation and recognition. The Helmholtz machine is used since its re-entrant connections support both supervised and unsupervised learning. Potentially it can explain some aspects of human learning of emotional concepts and experience. In this research, a robotic headpsilas facial expression dataset is used. The objective of this learning architecture is to demonstrate the neural basis for the association of recognized facial expressions and linguistic emotion labels. It implies the understanding of emotions from observation and is further used to generate facial expressions. In contrast with other facial expression recognition research, this work concentrates more on emotional information processing and neural concept development, rather than a technical recognition task. This approach has a lot of potential to contribute towards neurally inspired emotional experience in robotic systems.	algorithm;facial recognition system;helmholtz machine;information processing;robot;unsupervised learning	Chi-Yung Yau;Kevin Burn;Stefan Wermter	2008	2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)	10.1109/IJCNN.2008.4634179	unsupervised learning;robot;computer vision;emotion;information processing;computer science;machine learning;supervised learning;mirror neuron;facial expression	Robotics	19.53978498611061	-65.60947819477305	168930
1581d1a73686c41ed831849b1c2092cbedf78763	virtual screening for r-groups, including predicted pic50 contributions, within large structural databases, using topomer comfa	virtual screening	"""Multiple R-groups (monovalent fragments) are implicitly accessible within most of the molecular structures that populate large structural databases. R-group searching would desirably consider pIC50 contribution forecasts as well as ligand similarities or docking scores. However, R-group searching, with or without pIC50 forecasts, is currently not practical. The most prevalent and reliable source of pIC50 predictions, existing 3D-QSAR approaches, is also difficult and somewhat subjective. Yet in 25 of 25 trials on data sets on which a field-based 3D-QSAR treatment had already succeeded, substitution of objective (canonically generated) topomer poses for the original structure-guided manual alignments produced acceptable 3D-QSAR models, on average having almost equivalent statistical quality to the published models, and with negligible effort. Their overall pIC50 prediction error is 0.805, calculated as the average over these 25 topomer CoMFA models in the standard deviations of pIC50 predictions, derived from the 1109 possible """"leave-out-one-R-group"""" (LOORG) pIC50 contributions. (This novel LOORG protocol provides a more realistic and stringent test of prediction accuracy than the customary """"leave-out-one-compound"""" LOO approach.) The associated average predictive r(2) of 0.495 indicates a pIC50 prediction accuracy roughly halfway between perfect and useless. To assess the ability of topomer-CoMFA based virtual screening to identify """"highly active"""" R-groups, a Receiver Operating Curve (ROC) approach was adopted. Using, as the binary criterion for a """"highly active"""" R-group, a predicted pIC50 greater than the top 25% of the observed pIC50 range, the ROC area averaged across the 25 topomer CoMFA models is 0.729. Conventionally interpreted, the odds that a """"highly active"""" R-group will indeed confer such a high pIC50 are 0.729/(1-0.729) or almost 3 to 1. To confirm that virtual screening within large collections of realized structures would provide a useful quantity and variety of R-group suggestions, combining shape similarity with the """"highly active"""" pIC50, the 50 searches provided by these 25 models were applied to 2.2 million structurally distinct R-group candidates among 2.0 million structures within a ZINC database, identifying an average of 5705 R-groups per search, with the highest predicted pIC50 combination averaging 1.6 log units greater than the highest reported pIC50s."""	boat dock;collections (publication);databases;docking (molecular);entity name part qualifier - adopted;ligands;molecular structure;population;projections and predictions;quantitative structure-activity relationship;quantitative structure–activity relationship;receiver operating characteristic;receiver operator characteristics;scientific publication;virtual screening;zinc database;comf1 protein, bacillus subtilis;customary	Richard D. Cramer;Phillip Cruz;Gunther Stahl;William C. Curtiss;Brian Campbell;Brian B. Masek;Farhad Soltanshahi	2008	Journal of chemical information and modeling	10.1021/ci8001556	econometrics;chemistry;virtual screening;computational chemistry;data mining;mathematics;statistics	Comp.	12.53181758463751	-59.70308004555993	169236
5de7b342741196944847714a29d7cd28f948b3c0	chiral analysis of pesticides and drugs of environmental concern: biodegradation and enantiomeric fraction		The importance of stereochemistry for medicinal chemistry and pharmacology is well recognized and the dissimilar behavior of enantiomers is fully documented. Regarding the environment, the significance is equivalent since enantiomers of chiral organic pollutants can also differ in biodegradation processes and fate, as well as in ecotoxicity. This review comprises designed biodegradation studies of several chiral drugs and pesticides followed by enantioselective analytical methodologies to accurately measure the enantiomeric fraction (EF). The enantioselective monitoring of microcosms and laboratory-scale experiments with different environmental matrices is herein reported. Thus, this review focuses on the importance of evaluating the EF variation during biodegradation studies of chiral pharmaceuticals, drugs of abuse, and agrochemicals and has implications for the understanding of the environmental fate of chiral pollutants.	chirality (chemistry);cloud fraction;elegant degradation;entity framework;experiment;medicinal chemistry;redirection (computing);risk assessment	Alexandra S. Maia;Ana Rita Ribeiro;Paula M. L. Castro;Maria Elizabeth Tiritan	2017	Symmetry	10.3390/sym9090196	pesticide;agrochemical;environmental chemistry;combinatorics;mathematics;ecotoxicity;enantioselective synthesis;pollutant;biodegradation;enantiomer;chirality (chemistry)	AI	12.59934831163907	-58.806692913358845	169365
173e02a38dc2c14531bdfa78182f81ce429f4410	structure-based rna function prediction using elastic shape analysis	rna functions elastic shape analysis rna structure alignment rna function prediction rna structures;elasticity;bending;elastic shape analysis;molecular configurations;rna shape manifolds power capacitors accuracy measurement probability distribution;non coding rna;shape analysis;biomechanics;benchmark datasets structure based rna function prediction elastic shape analysis gene transcription rna structure alignment bending stretching geodesic distance esa;rna structures;three dimensional;genetics;geodesic distance;rna structure;structure comparison;rna;rna functions;probability distribution;molecular biophysics;rna bending biomechanics elasticity genetics molecular biophysics molecular configurations;function prediction;rna function prediction;gene transcription;rna structure alignment	In recent years, RNAs have been found to have diverse functions beyond being a messenger in gene transcription. The functions of non-coding RNAs are determined by their structures. Structure comparison/alignment of RNAs provides an effective means to predict their functions. Despite many previous studies on RNA structure alignment, it is still a challenging problem to predict the function of RNA molecules based on their structure information. In this study, we developed a new RNA structure alignment method based on elastic shape analysis (ESA). ESA treats RNA structures as three dimensional curves and performs flexible alignment between two RNA molecules by bending and stretching one of the molecules to match the other. The amount of bending and stretching is quantified by a formal distance, geodesic distance. Based on ESA, a rigorous mathematical framework can be built for RNA structure comparison. Means and covariances can be computed and probability distributions can be constructed for a group of RNA structures. We further applied the method to predict functions of RNA molecules. Our method achieved good performance when tested on benchmark datasets.	benchmark (computing);distance (graph theory);esa;shape analysis (digital geometry);transcription (software)	Jose Laborde;Anuj Srivastava;Jinfeng Zhang	2011	2011 IEEE International Conference on Bioinformatics and Biomedicine	10.1109/BIBM.2011.119	probability distribution;biology;three-dimensional space;nucleic acid structure;structural alignment;bending;geodesic;rna;bioinformatics;biomechanics;shape analysis;non-coding rna;elasticity;transcription;genetics;molecular biophysics	Vision	11.419284964342062	-57.56992302639589	169982
9a35515bf01ffc4b185a9bb66f6adb7ad20b7aa5	chemical similarity using geometric atom pair descriptors		Similarity searches using topological descriptors have proved extremely useful in aiding large-scale screening. In this paper we describe the geometric atom pair, the 3D analog of the topological atom pair descriptor (Carhart et al. J. Chem. Inf. Comput. Sci. 1985, 25, 64−73). We show the results of geometric similarity searches using the CONCORD-build structures of typical small druglike molecules as probes. The database to be searched is a 3D version of the Derwent Standard Drug File that contains an average of 10 explicit conformations per compound. Using objective criteria for determining how good a descriptor is in selecting active compounds from large databases, we compare the results using the geometric versus the topological atom pair. We find that geometric and topological atom pairs are about equally effective in selecting active compounds from large databases. How the two types of descriptors rank active compounds is generally about the same as well, but occasionally active compounds will be se...	atom;chemical similarity	Robert P. Sheridan;Michael D. Miller;Dennis J. Underwood;Simon K. Kearsley	1996	Journal of Chemical Information and Computer Sciences	10.1021/ci950275b	computational chemistry	Theory	11.886981575994232	-59.275268744447246	170496
3af72a9233bacb485aa6b4684d3fe3816a828306	application of etsiat to the toxicity prediction of aliphatic alcohols	spider;electrotopological state indices for atom types etsiat toxicity prediction aliphatic alcohol quantitative structure activity relationship modeling tetrahymena tomato spider bacterial luciferases partial least squares structural information pls analysis hydrophobicity steric hindrance effects;external validity;carbon;quantitative structure activity relationship;least squares approximations;organic compounds;compounds;tomato;bacterial luciferases;tetrahymena;partial least square;biological system modeling;aliphatic alcohols;hydrophobicity;loading;structural information;toxicology;partial least squares;toxicity prediction;quantitative structure activity relationship modeling;mathematical model load modeling compounds predictive models loading carbon biological system modeling;etsiat;mathematical model;pls analysis;toxicity;predictive models;toxicity quantitative structure activity relationship electrotopological state indices for atom types aliphatic alcohols;long chain;health hazards;load modeling;physiological models;aliphatic alcohol;toxicology health hazards hydrophobicity least squares approximations organic compounds physiological models;steric hindrance effects;electrotopological state indices for atom types	ETSIAT was employed in the quantitative structure-activity relationship (QSAR) modeling on the aliphatic alcohol toxicities to the Tetrahymena, tomato, spider, and bacterial luciferases et al‥ The QSAR models were established by partial least squares and further validated by both internal and external validations. The results showed that ETSIAT can characterize the structural information relevant to aliphatic alcohol toxicity very well and the resulting QSAR models were robust and showed high predictive capabilities. Based on the PLS analysis of the QSAR models, it can be inferred that hydrophobicity and steric hindrance effects were key factors influencing the toxicities of the aliphatic alcohols, and there may be different toxicity mechanisms between long-chain and short-chain aliphatic alcohols.	partial least squares regression;quantitative structure–activity relationship	Hu Mei;Lei Yang;Qin Wang;Ning Yan;Li Liu;Jia-Ying Sun;Jiang-An Xie	2010	2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2010.5569766	carbon;external validity;computer science;machine learning;hydrophobe;toxicity;mathematical model;predictive modelling;partial least squares regression;quantitative structure–activity relationship	Robotics	12.733211837426259	-57.57403548027466	170599
b9a669e6d4f9885d5d69c017af480a38c2d082de	harmonic networks: integrating spectral information into cnns		Convolutional neural networks (CNNs) learn filters in order to capture local correlation patterns in feature space. In contrast, in this paper we propose harmonic blocks that produce features by learning optimal combinations of spectral filters defined by the Discrete Cosine Transform. The harmonic blocks are used to replace conventional convolutional layers to construct partial or fully harmonic CNNs. We extensively validate our approach and show that the introduction of harmonic blocks into state-of-the-art CNN baseline architectures results in comparable or better performance in classification tasks on small NORB, CIFAR10 and CIFAR100 datasets.		Matej Uličný;Vladimir A. Krylov;Rozenn Dahyot	2018	CoRR			ML	24.446974182507436	-52.1302836913378	170613
61108f4b7fcb4f765d9f8543ba26854d4a64d3b2	rapid activity prediction of hiv-1 integrase inhibitors: harnessing docking energetic components for empirical scoring by chemometric and artificial neural network approaches	autodock4;pls;soms;molecular docking;hiv-1 integrase;scoring	Improving performance of scoring functions for drug docking simulations is a challenging task in the modern discovery pipeline. Among various ways to enhance the efficiency of scoring function, tuning of energetic component approach is an attractive option that provides better predictions. Herein we present the first development of rapid and simple tuning models for predicting and scoring inhibitory activity of investigated ligands docked into catalytic core domain structures of HIV-1 integrase (IN) enzyme. We developed the models using all energetic terms obtained from flexible ligand-rigid receptor dockings by AutoDock4, followed by a data analysis using either partial least squares (PLS) or self-organizing maps (SOMs). The models were established using 66 and 64 ligands of mercaptobenzenesulfonamides for the PLS-based and the SOMs-based inhibitory activity predictions, respectively. The models were then evaluated for their predictability quality using closely related test compounds, as well as five different unrelated inhibitor test sets. Weighting constants for each energy term were also optimized, thus customizing the scoring function for this specific target protein. Root-mean-square error (RMSE) values between the predicted and the experimental inhibitory activities were determined to be <1 (i.e. within a magnitude of a single log scale of actual IC50 values). Hence, we propose that, as a pre-functional assay screening step, AutoDock4 docking in combination with these subsequent rapid weighted energy tuning methods via PLS and SOMs analyses is a viable approach to predict the potential inhibitory activity and to discriminate among small drug-like molecules to target a specific protein of interest.		Patcharapong Thangsunan;Sila Kittiwachana;Puttinan Meepowpan;Nawee Kungwan;Panchika Prangkio;Supa Hannongbua;Nuttee Suree	2016	Journal of computer-aided molecular design	10.1007/s10822-016-9917-0	bioinformatics;machine learning;combinatorial chemistry	Comp.	10.58202624615148	-56.420797105131314	170792
0f84a81f431b18a78bd97f59ed4b9d8eda390970	striving for simplicity: the all convolutional net		Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding – and building on other recent work for finding simple network structures – we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the “deconvolution approach” for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.	artificial neural network;computer vision;convolution;convolutional neural network;deconvolution;imagenet;outline of object recognition	Jost Tobias Springenberg;Alexey Dosovitskiy;Thomas Brox;Martin A. Riedmiller	2014	CoRR		computer science;artificial intelligence;theoretical computer science;machine learning;mathematics	Vision	22.98025264351294	-52.45002357985181	171095
f9464a9dd6b4f9d14214ebdd3df2aa280de61f5b	computational prediction of alanine scanning and ligand binding energetics in g-protein coupled receptors	receptors g protein coupled;ligands;alanine;models molecular;bioinformatik berakningsbiologi;protein binding;computational biology;biochemistry and molecular biology;bioinformatics computational biology;biokemi och molekylarbiologi	Site-directed mutagenesis combined with binding affinity measurements is widely used to probe the nature of ligand interactions with GPCRs. Such experiments, as well as structure-activity relationships for series of ligands, are usually interpreted with computationally derived models of ligand binding modes. However, systematic approaches for accurate calculations of the corresponding binding free energies are still lacking. Here, we report a computational strategy to quantitatively predict the effects of alanine scanning and ligand modifications based on molecular dynamics free energy simulations. A smooth stepwise scheme for free energy perturbation calculations is derived and applied to a series of thirteen alanine mutations of the human neuropeptide Y1 receptor and series of eight analogous antagonists. The robustness and accuracy of the method enables univocal interpretation of existing mutagenesis and binding data. We show how these calculations can be used to validate structural models and demonstrate their ability to discriminate against suboptimal ones.	alanine;computation;energy, physics;experiment;free energy perturbation;interaction;ligands;molecular dynamics;mutation;name binding;processor affinity;radionuclide imaging;simulation;stepwise regression;receptor	Lars Boukharta;Hugo Gutiérrez-de-Terán;Johan Åqvist	2014		10.1371/journal.pcbi.1003585	computational biology;biology;plasma protein binding;biophysics;cell biology;bioinformatics;ligand	Comp.	10.441593217778008	-60.78023067599933	171316
fa1e466bcff089736154e9bf106c670ec07027cb	deep contextual networks for neuronal structure segmentation		The goal of connectomics is to manifest the interconnections of neural system with the Electron Microscopy (EM) images. However, the formidable size of EM image data renders human annotation impractical, as it may take decades to fulfill the whole job. An alternative way to reconstruct the connectome can be attained with the computerized scheme that can automatically segment the neuronal structures. The segmentation of EM images is very challenging as the depicted structures can be very diverse. To address this difficult problem, a deep contextual network is proposed here by leveraging multi-level contextual information from the deep hierarchical structure to achieve better segmentation performance. To further improve the robustness against the vanishing gradients and strengthen the capability of the back-propagation of gradient flow, auxiliary classifiers are incorporated in the architecture of our deep neural network. It will be shown that our method can effectively parse the semantic meaning from the images with the underlying neural network and accurately delineate the structural boundaries with the reference of low-level contextual cues. Experimental results on the benchmark dataset of 2012 ISBI segmentation challenge of neuronal structures suggest that the proposed method can outperform the state-of-the-art methods by a large margin with respect to different evaluation measurements. Our method can potentially facilitate the automatic connectome analysis from EM images with less human intervention effort.		Hao Chen;Xiaojuan Qi;Jie-Zhi Cheng;Pheng-Ann Heng	2016			computer vision;computer science;artificial intelligence;machine learning;data mining	Vision	22.978373346082176	-52.87255443739086	172038
5bb599bfa6a45369c4a01aac2ad1773ed221c491	using buriedness to improve discrimination between actives and inactives in docking		A continuing problem in protein-ligand docking is the correct relative ranking of active molecules versus inactives. Using the ChemScore scoring function as implemented in the GOLD docking software, we have investigated the effect of scaling hydrogen bond, metal-ligand, and lipophilic interactions based on the buriedness of the interaction. Buriedness was measured using the receptor density, the number of protein heavy atoms within 8.0 A. Terms in the scaling functions were optimized using negative data, represented by docked poses of inactive molecules. The objective function was the mean rank of the scores of the active poses in the Astex Diverse Set (Hartshorn et al. J. Med. Chem., 2007, 50, 726) with respect to the docked poses of 99 inactives. The final four-parameter model gave a substantial improvement in the average rank from 18.6 to 12.5. Similar results were obtained for an independent test set. Receptor density scaling is available as an option in the recent GOLD release.	boat dock;docking (molecular);hydrogen bonding;image scaling;interaction;ligands;loss function;ninety nine;optimization problem;physical inactivity;population parameter;protein data bank;protein–ligand docking;score;scoring functions for docking;test scaling;test set;wavelet;lipophilicity	Noel M. O'Boyle;Suzanne Clare Brewerton;Robin Taylor	2008	Journal of chemical information and modeling	10.1021/ci8000452	econometrics;chemistry;bioinformatics;nanotechnology	Comp.	11.443258495876664	-59.507274237507445	172202
a6a924ac23ac8cf8e38e455afaad93cdd723ef8a	structure-based druggability assessment of the mammalian structural proteome with inclusion of light protein flexibility	animals;proteome;pharmaceutical preparations;pliability;binding sites;models molecular;proteins;drug design;protein conformation;naphthalenes;reproducibility of results;protein binding;models statistical;proteomics;mammals	Advances reported over the last few years and the increasing availability of protein crystal structure data have greatly improved structure-based druggability approaches. However, in practice, nearly all druggability estimation methods are applied to protein crystal structures as rigid proteins, with protein flexibility often not directly addressed. The inclusion of protein flexibility is important in correctly identifying the druggability of pockets that would be missed by methods based solely on the rigid crystal structure. These include cryptic pockets and flexible pockets often found at protein-protein interaction interfaces. Here, we apply an approach that uses protein modeling in concert with druggability estimation to account for light protein backbone movement and protein side-chain flexibility in protein binding sites. We assess the advantages and limitations of this approach on widely-used protein druggability sets. Applying the approach to all mammalian protein crystal structures in the PDB results in identification of 69 proteins with potential druggable cryptic pockets.	4-dichlorobenzene;binding sites;crystal structure;internet backbone;mammals;muscle rigidity;protein data bank;protein family;proteome;vertebral column;protein protein interaction	Kathryn Loving;Andy Lin;Alan C. Cheng	2014		10.1371/journal.pcbi.1003741	biology;protein structure;plasma protein binding;molecular biology;bioinformatics;binding site;proteome;proteomics;drug design	Comp.	10.78084430150071	-61.22779533262047	172248
029d34f8a666a6edafb1065b01dfe789975d7d8e	frame-based facial expression recognition using geometrical features		To improve the human-computer interaction (HCI) to be as good as human-human interaction, building an efficient approach for human emotion recognition is required.These emotions could be fused from several modalities such as facial expression, hand gesture, acoustic data, and biophysiological data. In this paper, we address the frame-based perception of the universal human facial expressions (happiness, surprise, anger, disgust, fear, and sadness), with the help of several geometrical features. Unlike many other geometry-based approaches, the frame-based method does not rely on prior knowledge of a person-specific neutral expression; this knowledge is gained through human intervention and not available in real scenarios. Additionally, we provide a method to investigate the performance of the geometry-based approaches under various facial point localization errors. From an evaluation on two public benchmark datasets, we have found that using eight facial points, we can achieve the state-of-the-art recognition rate. However, this state-of-the-art geometry-based approach exploits features derived from 68 facial points and requires prior knowledge of the person-specific neutral expression.The expression recognition rate using geometrical features is adversely affected by the errors in the facial point localization, especially for the expressions with subtle facial deformations.	acoustic cryptanalysis;benchmark (computing);database;emotion recognition;frame language;human–computer interaction;parsing expression grammar;sadness;sensor	Anwar Saeed;Ayoub Al-Hamadi;Robert Niese;Moftah Elzobi	2014	Adv. Human-Computer Interaction	10.1155/2014/408953	psychology;computer vision;three-dimensional face recognition;communication;social psychology;face hallucination	Vision	23.890252493424548	-59.710421854791285	173036
68820cb4114e705ea8ed4a0a1daf1eaaf32852a8	contours of simulated marine dimethyl sulfide distributions under variation in a gabric mechanism	second order;dimethyl sulfide;photochemistry;simulation framework;dissolved organic matter;community climate system model;cell disruption;large scale;rate constant;bimolecular kinetics;next generation;ocean;sulfur package;optimization;microbial ecology;kinetics	Biogeochemical tracer bins and transformations from an established reduced sulfur cycle mechanism were introduced into the oceanic component of the Community Climate System Model. The resulting global dimethyl sulfide simulation framework was then subjected to variation in plant cell precursor content and the kinetic form of removal terms. Chi square type merit minima were computed analytically along a release rate axis over global, low latitude and localized domains. A band of width 60 degrees centered on the equator proved to be the most effective optimization area because it greatly exceeded resolution of the validation data set but avoided fronts where the driver ecodynamics module overpredicts chlorophyll. Loss terms involving bacterial consumption formulated as bimolecular kinetics and photochemical decay sensitized by dissolved organic matter independently provided superior agreement with data by modulating peaks along the equatorial divergence. Factor of ten reductions in the sulfur precursor content of diatoms or small noncalcite secretors respectively flattened and exaggerated the same features, but intermediate compositions were not tested. The parameter suite of constant intracellular sulfur, second order osmotrophy (microbial uptake) and photochemistry set proportional to photosynthetic radiation is recommended and packaged as a startup mechanism. This particular combination optimizes large-scale reduced sulfur fields across well-understood ecosystems while simultaneously maintaining parsimony. Visualizations from the inverse procedure are offered for multiple mechanisms, as mappings of both normalized deviation and concentration. The value of the rate constant for injection from plant and animal material was often determined to be of the order weeks, consistent with emission via grazing or aging/mortality. Refinement of the model will require linkage to the ecological flows associated with cell disruption, accounting of elemental metabolic stresses which in part govern reduced sulfur storage, addition of true microbial ecology, further studies of open ocean sulfur photochemistry and longer duration/more detailed optimization exercises. A stepwise strategy is outlined for moving through this task list. In next-generation experiments it may prove expedient to fix sulfur content ratios within taxonomic classes while varying the maximum cell content. 2006 Elsevier Ltd. All rights reserved.	apache axis;biogeochemistry;chi;community climate system model;denial-of-service attack;ecology;ecosystem;elemental;experiment;kinetics internet protocol;linkage (software);mathematical optimization;maxima and minima;maximum parsimony (phylogenetics);occam's razor;simulation;stepwise regression	Scott Elliott;Shaoping Chu;David Erickson	2007	Environmental Modelling and Software	10.1016/j.envsoft.2005.11.006	biology;botany;dissolved organic carbon;hydrology;cell disruption;community climate system model;microbial ecology;reaction rate constant;ecology;second-order logic;kinetics		12.111003818576624	-62.51248971744892	173174
9cc11dd9f6ffd4b89efb22ab96b21d1062ce61a7	region graph spectra as geometric global image features	image features;graph spectra;computational method;pattern detection;scale invariant feature transform;fluorescence microscopy;local features;human skin;spectral graph theory	In quantitative biology studies such as drug and siRNA screens, robotic systems automatically acquire thousands of images from cell assays. Because these images are large in quantity and high in content, detecting specific patterns (phenotypes) in them requires accurate and fast computational methods. To this end, we have developed a geometric global image feature for pattern retrieval on large bio-image data sets. This feature is derived by applying spectral graph theory to local feature detectors such as the Scale Invariant Feature Transform, and is effective on patterns with as few as 20 keypoints. We demonstrate successful pattern detection on synthetic shape data and fluorescence microscopy images of GFP-Keratin-14-expressing human skin cells.		Qirong Ho;Weimiao Yu;Hwee Kuan Lee	2009		10.1007/978-3-642-10331-5_24	fluorescence microscope;computer vision;feature detection;computer science;machine learning;pattern recognition;scale-invariant feature transform;mathematics;spectral graph theory;feature	Vision	14.529192899501158	-57.30076407960009	173344
fe21cff0d0cf008a1eb9b36d65573ca01c7d8ae5	encoding sparse features in a bidirectional associative memory	encoding computational modeling biological system modeling brain models load modeling associative memory;stimulus features sparse feature encoding bidirectional associative memory artificial neural networks bipolar coding scheme bam models recall performance;sparse coding artificial intelligence connectionist models bidirectional associative memory;neural nets encoding	Bidirectional Associative Memories (BAMs) are artificial neural networks that can learn and recall various types of associations. Although BAM models have shown great promise at modeling human cognitive processes, these models have often been investigated under conditions where stimuli are densely represented using a bipolar coding scheme. However, research has shown that dense representations are energetically costly given that various stimulus representations need to be detected, processed and analyzed on a daily basis. Instead, biological networks work on minimizing energy expenditure by encoding sparse stimulus features that maximize information representation. This paper extends this line of search and shows that BAM models can improve learning and recall performance in a sparse encoding regime. It provides a strategy for artificial neural networks that seek to maintain valuable processing resources, especially under constraints of noisy representations of stimulus features.	artificial neural network;bidirectional associative memory;biological network;neural coding;sparse matrix	Nareg Berberian;Zoya Aamir;Sébastien Hélie;Sylvain Chartier	2016	2016 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2016.7727875	natural language processing;computer science;theoretical computer science;machine learning;bidirectional associative memory	ML	19.886172386676854	-65.99385004237155	173363
3babc3a78da87f68cf8c9b884205cf7e3f1f977a	cuteness recognition and localization in the photos of animals	rabbits;dogs;localization;recognition;cats;cute images	"""Among the flourishing amount of photos in the social media websites, """"cute"""" images of animals are particularly attractive to the Internet users. This paper considers building an automatic model which can distinguish cute images from non-cute ones. To make the recognition results more interpretable, a lot of efforts are made to find which part of the animal appears attractive to the human users. To validate the success of our proposed method, we collect three new datasets of different animals, i.e., cats, dogs, and rabbits with both cute and non-cute images. Our model obtains promising performance in distinguishing cute images from non-cute ones. Moreover, it outperforms the classical models with not only better recognition accuracy, but also more intuitive localization of the cuteness in the images. The contribution of this paper is three-fold: (1) We collect new datasets for cuteness recognition, (2) We extend the powerful Fisher Vector representation to localize cute part in the animal recognition, and (3) Extensive experimental results show that our proposed method can recognize cute animals of cats, dogs, and rabbits."""	social media	Yu Bao;Jing Yang;Liangliang Cao;Haojie Li;Jinhui Tang	2014		10.1145/2647868.2655046	simulation;internationalization and localization;computer science;artificial intelligence;operating system	Vision	23.149710060592515	-59.5491841615646	173395
3c489bface06250060ac40d5910c6882013ef981	entropy and free energy of a mobile loop based on the crystal structures of the free and bound proteins	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;ligand binding;absolute entropy;uk phd theses thesis;life sciences;mobile loop;crystal structure;uk research reports;medical journals;free energy;europe pmc;biomedical research;bioinformatics	"""A mobile loop changes its conformation from """"open"""" (free enzyme) to """"closed"""" upon ligand binding. The difference in the Helmholtz free energy, ΔF(loop) between these states sheds light on the mechanism of binding. With our """"hypothetical scanning molecular dynamics"""" (HSMD-TI) method ΔF(loop) = F(free) - F(bound) where F(free) and F(bound) are calculated from two MD samples of the free and bound loop states; the contribution of water is obtained by a thermodynamic integration (TI) procedure. In previous work the free and bound loop structures were both attached to the same """"template"""" which was """"cut"""" from the crystal structure of the free protein. Our results for loop 287-290 of AcetylCholineEsterase agree with the experiment, ΔF(loop)~ -4 kcal/mol if the density of the TIP3P water molecules capping the loop is close to that of bulk water, i.e., N(water) = 140 - 180 waters in a sphere of a 18 Å radius. Here we calculate ΔF(loop) for the more realistic case, where two templates are """"cut"""" from the crystal structures, 2dfp.pdb (bound) and 2ace.pdb (free), where N(water) = 40 - 160; this requires adding a computationally more demanding (second) TI procedure. While the results for N(water) ≤ 140 are computationally sound, ΔF(loop) is always positive (18 ± 2 kcal/mol for N(water) = 140). These (disagreeing) results are attributed to the large average B-factor, 41.6 of 2dfp (23.4 Å(2) for 2ace). While this conformational uncertainty is an inherent difficulty, the (unstable) results for N(water) = 160 suggest that it might be alleviated by applying different (initial) structural optimizations to each template."""	clinical use template;control theory;crystal structure;duoxa1 gene;for loop;frequency capping;ligands;mobile phone;mole, unit of measurement;molecular dynamics;name binding;thermodynamic integration;thermodynamics;unstable medical device problem;water model;free energy;kilocalorie	Mihail Mihailescu;Hagai Meirovitch	2010	Entropy	10.3390/e12081946	computer science;crystal structure;nanotechnology;mathematics;ligand	Robotics	11.295946676708576	-62.306583837870996	174151
dee406a7aaa0f4c9d64b7550e633d81bc66ff451	content-adaptive sketch portrait generation by decompositional representation learning	measurement;neural networks;training;journal article;mean square error methods;face;probabilistic logic;hair	Sketch portrait generation benefits a wide range of applications such as digital entertainment and law enforcement. Although plenty of efforts have been dedicated to this task, several issues still remain unsolved for generating vivid and detail-preserving personal sketch portraits. For example, quite a few artifacts may exist in synthesizing hairpins and glasses, and textural details may be lost in the regions of hair or mustache. Moreover, the generalization ability of current systems is somewhat limited since they usually require elaborately collecting a dictionary of examples or carefully tuning features/components. In this paper, we present a novel representation learning framework that generates an end-to-end photo-sketch mapping through structure and texture decomposition. In the training stage, we first decompose the input face photo into different components according to their representational contents (i.e., structural and textural parts) by using a pre-trained convolutional neural network (CNN). Then, we utilize a branched fully CNN for learning structural and textural representations, respectively. In addition, we design a sorted matching mean square error metric to measure texture patterns in the loss function. In the stage of sketch rendering, our approach automatically generates structural and textural representations for the input photo and produces the final result via a probabilistic fusion scheme. Extensive experiments on several challenging benchmarks suggest that our approach outperforms example-based synthesis algorithms in terms of both perceptual and objective metrics. In addition, the proposed method also has better generalization ability across data set without additional training.	alignment;artificial neural network;biologic preservation;biological neural networks;convolutional neural network;dictionary [publication type];end-to-end principle;experiment;eye;feature learning;generalization (psychology);law enforcement;learning disorders;loss function;matching;machine learning;mean squared error;morphologic artifacts;pixel;polynomial-time approximation scheme;representation (action);sketch;algorithm;benefit;contents - htmllinktype	Dongyu Zhang;Liang Lin;Tianshui Chen;Xian Wu;Wenwei Tan;Ebroul Izquierdo	2017	IEEE Transactions on Image Processing	10.1109/TIP.2016.2623485	face;computer vision;computer science;artificial intelligence;machine learning;mathematics;probabilistic logic;artificial neural network;algorithm;measurement;statistics	Vision	21.89640523519791	-56.30744199677749	174352
b8ab9861498767dcd58fdb548b595ce40e6c3007	visualizing the energetics of the dissociation of a metastable molecule	metastasis;biological system modeling;chemical analysis;energy exchange;visualization metastasis displays computational modeling chemistry quantum computing chemical analysis animation biological system modeling energy exchange;visualization;computational modeling;solid object decomposition;displays;animation;chemistry;quantum computing;physically based modeling	Introduction Shown in Figure 1 is the simulation of the energetics of the dissociation of a metastable molecule. The simulation was computed by Dr. Robert Wyatt, a Professor of Chemistry at The University of Texas at Austin. Dr. Wyatt’s simulation uses a model molecule composed of two fragments, A and B. Starting from the molecular state AB, the simulation computes the probability that, under any given conditions, AB will dissociate into the separated components A + B. This may seem like a toy problem, but in real life many molecules demonstrate metastable behavior--a propensity to dissociate--and the work that this simulation is doing may ultimately have an impact on studies of the energetics of larger molecules. An important class of these is biomolecular. This work may be particularly applicable to energy transfer problems in biological systems.	biological system;molecular modelling;real life;simulation;toy problem	David Guzman;Reuben Reyes;Karla Vega;Kelly P. Gaither;Robert Wyatt	2004	IEEE Visualization 2004	10.1109/VISUAL.2004.123	anime;simulation;visualization;computer science;theoretical computer science;quantum computer;computational model;quantum mechanics	Embedded	14.843161294337783	-62.074430563830646	174622
a964cbff1389e31e3c8fde4a7c50f724450f63fa	g protein- and agonist-bound serotonin 5-ht2a receptor model activated by steered molecular dynamics simulations	ligando;modelizacion;crystalline structure;chimie informatique;criblage virtuel;recepteur β2 adrenergique;molecular model;β2 adrenergic receptor;biologia molecular;conformation;molecular dynamics;modelo molecular;computational chemistry;recepteur couple proteine g;activation;structure proteine;proteina g;dynamique moleculaire;g protein coupled receptor;modelisation;protein structure;proteine g;virtual screening;receptor acoplado proteina g;conformacion;receptor β2 adrenergico;modele moleculaire;structure cristalline;homology;activacion;molecular biology;ligand;g protein;quimica informatica;dinamica molecular;homologia;modeling;cribado virtual;estructura cristalina;biologie moleculaire;homologie	A 5-HT(2A) receptor model was constructed by homology modeling based on the β(2)-adrenergic receptor and the G protein-bound opsin crystal structures. The 5-HT(2A) receptor model was transferred into an active conformation by an agonist ligand and a G(αq) peptide in four subsequent steered molecular dynamics (MD) simulations. The driving force for the transformation was the addition of several known intermolecular and receptor interhelical hydrogen bonds enforcing the necessary helical and rotameric movements. Subsquent MD simulations without constraints confirmed the stability of the activated receptor model as well as revealed new information about stabilizing residues and bonds. The active 5-HT(2A) receptor model was further validated by retrospective ligand screening of more than 9400 compounds, whereof 182 were known ligands. The results show that the model can be used in drug discovery for virtual screening and structure-based ligand design as well as in GPCR activation studies.	adrenergic receptor;agonist;computer simulation;crystal structure;drug discovery;homology (biology);homology modeling;hydrogen;ligands;molecular dynamics;movement;receptor, serotonin, 5-ht2a;serotonin;virtual screening	Vignir Ísberg;Thomas Balle;Tommy Sander;Flemming Steen Jørgensen;David E. Gloriam	2011	Journal of chemical information and modeling	10.1021/ci100402f	biochemistry;stereochemistry;protein structure;homology;molecular dynamics;systems modeling;chemistry;virtual screening;crystal structure;g protein-coupled receptor;g protein;molecular model;computational chemistry;ligand	Comp.	10.258394050403139	-61.698938550207565	174846
d4a3a91804519faeda9f9eb693a7fdae9f078cfa	artificial neural network study on organ-targeting peptides	negative control;receiver operator characteristic;organ targeting peptide;statistical significance;phage display;random sequence;vhse descriptor;roc curve;roc score;positive control;artificial neural network;neural network	We report a new approach to studying organ targeting of peptides on the basis of peptide sequence information. The positive control data sets consist of organ-targeting peptide sequences identified by the peroral phage-display technique for four organs, and the negative control data are prepared from random sequences. The capacity of our models to make appropriate predictions is validated by statistical indicators including sensitivity, specificity, enrichment curve, and the area under the receiver operating characteristic (ROC) curve (the ROC score). VHSE descriptor produces statistically significant training models and the models with simple neural network architectures show slightly greater predictive power than those with complex ones. The training and test set statistics indicate that our models could discriminate between organ-targeting and random sequences. We anticipate that our models will be applicable to the selection of organ-targeting peptides for generating peptide drugs or peptidomimetics.		Eunkyoung Jung;Junhyoung Kim;Seung-Hoon Choi;Minkyoung Kim;Hokyoung Rhee;Jae-Min Shin;Kihang Choi;Sang-Kee Kang;Nam-Kyung Lee;Yun-Jaie Choi	2010	Journal of computer-aided molecular design	10.1007/s10822-009-9313-0	machine learning;pattern recognition;receiver operating characteristic;artificial neural network;statistics	ML	10.291053098556763	-56.01387939648717	174882
b0a124e5125a98b1ef892ed389af8aa1f49a6c85	eavesdropping opponent agent communication using deep learning		We present a method for learning to interpret and understand foreign agent communication. Our approach is based on casting the contents of intercepted opponent agent communication to a bit-level representation and on training and employing deep convolutional neural networks for decoding the meaning of received messages. We empirically evaluate our method on real-world data acquired from the multi-agent domain of robotic soccer simulation, demonstrating the effectiveness and robustness of the learned decoding models.	deep learning	Thomas Gabel;Alaa Tharwat;Eicke Godehardt	2017		10.1007/978-3-319-64798-2_13	distributed computing;robustness (computer science);computer science;convolutional neural network;machine learning;deep learning;foreign agent;decoding methods;adversary;artificial intelligence;eavesdropping	HCI	19.204569240144497	-53.09705600169396	175036
adf7cf293282c1ae58e7d501b6ba4e0bedbc4a10	the impact of plasma protein binding on toxic plasma drug concentration	distribution;multiple linear regression;plasma protein binding;free fraction;toxic plasma drug concentration;mlr;disposition;lipophilicity;adme;plasma concentration;acute toxicity	Drugs with high plasma protein binding are relatively protected from first-pass hepatic metabolism. However, there are concerns about potential problems caused by high plasma protein binding, one of which is toxicity. Statistical analyses of plasma concentration thresholds for toxic and comatose-lethal effects of drugs by multiple linear regression (MLR) models indicate that high plasma protein binding is slightly associated with toxicity of drugs in general, but not in the high (≥90%) plasma protein binding range. Lipophilicity as determined by the octanol/water partition coefficient (Log P) is a major driver of acute toxicity, which can largely and perhaps entirely explain the higher acute toxicity of drugs with high plasma protein binding. Neither plasma protein binding nor lipophilicity have any relation to the therapeutic window of drugs.	plasma active	Andreas Svennebring	2016	I. J. Computational Biology and Drug Design	10.1504/IJCBDD.2016.10000938	pharmacology;distribution;chromatography;plasma protein binding;chemistry;lipophilicity;toxicology;computer science;bioinformatics;linear regression;acute toxicity;adme;disposition	Comp.	11.439800251479744	-63.83153058237189	175452
fbd5e0d339efaa4761bde1e3e95697601ff01a96	seamless remote browsing and coarse-to-fine compressed retrieval using a scalable image representation	institutional repositories;fedora;vital;vtls;ils	"""In today's information society, two important trends can be observed : (1) the digital universe -particularly still or moving images- is growing exponentially and (2) computer networks are becoming more heterogeneous. The former raises the need for tools to guide the user within large data spaces, while the latter requires a representation of information that is sufficiently flexible to adapt itself optimally to various network constraints or user requirements. In this thesis, we focus on image information and address these needs by investigating three complementary aspects of a remote image server. First, a scalable image representation, namely JPEG2000, is studied. This compression standard organizes the image information in such a way that almost any spatial area can easily be extracted at any resolution and bit-depth. This scalability implies a high computational load that may require a hardware implementation when dealing with real-time constraints. The main characteristics of such hardware architecture are also presented. Secondly we consider the issues of scheduling and caching JPEG2000 data in client/server interactive browsing applications under memory and channel bandwidth constraints. We evaluate several strategies to schedule data packets that are likely to become relevant to expected future Windows-of-Interest (""""pre-fetching"""" techniques), and show how the system reactivity can be improved. Finally, we investigate how to perform categorization and retrieval tasks directly in the compressed domain. Thanks to the JPEG 2000 scalability the discriminant level of a compressed image characterization is directly related to the amount of extracted data. Taking this finding into account, an original representation, called integral volumes, is introduced to store such characterization. Combining integral volumes with random decision trees, we propose a JPEG 2000 image classifier that achieves performances similar to the best uncompressed image classification results obtained on several freely available databases. Eventually, to address the image retrieval problem, a cascade of such classifiers is used, enabling a cost-optimized coarse-to-fine retrieval process."""	scalability;seamless3d	Antonin Descampe	2008			image retrieval;computer science;theoretical computer science;data mining;database	Vision	20.17866732585264	-59.85153813548372	175696
6df40bce7f254d01dd650e2462714ae03df8261a	towards reactive acoustic jamming for personal voice assistants		Personal Voice Assistants (PVAs) such as the Amazon Echo are commonplace and it is now likely to always be in range of at least one PVA. Although the devices are very helpful they are also continuously monitoring conversations. When a PVA detects a wake word, the immediately following conversation is recorded and transported to a cloud system for further analysis. In this paper we investigate an active protection mechanism against PVAs: reactive jamming. A Protection Jamming Device (PJD) is employed to observe conversations. Upon detection of a PVA wake word the PJD emits an acoustic jamming signal. The PJD must detect the wake word faster than the PVA such that the jamming signal still prevents wake word detection by the PVA. The paper presents an evaluation of the effectiveness of different jamming signals. We quantify the impact of jamming signal and wake word overlap on jamming success. Furthermore, we quantify the jamming false positive rate in dependence of the overlap. Our evaluation shows that a 100% jamming success can be achieved with an overlap of at least 60% with a negligible false positive rate. Thus, reactive jamming of PVAs is feasible without creating a system perceived as a noise nuisance.	acoustic cryptanalysis;additive white gaussian noise;amazon dolphins;amazon simple storage service;amazona;audio media;frequency band;noise-induced hearing loss;polyvinyl alcohol;protection mechanism;radio jamming;signal-to-noise ratio;sleep wake transition disorders;voice disorders	Peng Cheng;Ibrahim Ethem Bagci;Jeff Yan;Utz Roedig	2018		10.1145/3267357.3267359	speech recognition;false positive rate;jamming;'active' protection;computer science	Mobile	17.565551496874726	-62.65220611692649	175985
bdb479b930f59427b8c0033535b8bc7304aa9642	parallel decomposition of 3d surfaces in images of local descriptors for molecular screening	structure matching;surface morphology proteins shape councils parallel algorithms surface fitting mathematics information technology electrostatics amino acids;biology computing;spin images;protein function;parallel algorithm;local descriptor;macromolecular surface;biological component function;image matching;isosurfaces;parallel algorithm parallel decomposition 3d surface local descriptor molecular screening structure matching molecular biology protein function macromolecular surface biological component interaction biological component function 3d representation;biological component interaction;biology;spin images parallel molecular screening;data mining;surface morphology;molecular screening;proteins;shape;3d surface;three dimensional displays;image representation;molecular biology;molecular biophysics;parallel decomposition;macromolecules;parallel molecular screening;3d representation;biological techniques;proteins biological techniques biology computing image matching image representation macromolecules molecular biophysics parallel algorithms;parallel algorithms;in silico	An emerging application field for structure matching is related to in silico studies of molecular biology. Considering that protein function is mainly related to its external morphology, the possibility to match macromolecular surfaces is very important to infer information about the possible interaction and function of biological components. Most of the present approaches aim to match complete objects and they exploit a complete 3D representations of them. These aspects do not suit well with the local formulation of the matching issue in the biological application domain. The problem is that the shapes of macromolecules that are experimentally known to dock cannot present high similarities except in small important functional regions. This is the reason why we chose to adapt a well known image based method, originally studied for matching similar structures, for screening  possible interactions of couples of macromolecule.  In this context the proposed work presents a parallel algorithm able to efficiently decompose high detailed 3D surfaces into a proper set of images to use for the macromolecular screening.	application domain;applicative programming language;computational anatomy;computer stereo vision;computer vision;experiment;interaction;mathematical morphology;parallel algorithm;parallel computing;run time (program lifecycle phase)	Daniele D'Agostino;Andrea Clematis;Ivan Merelli;Paolo Cozzi;Luciano Milanesi	2009	2009 17th Euromicro International Conference on Parallel, Distributed and Network-based Processing	10.1109/PDP.2009.28	parallel computing;computer science;bioinformatics;theoretical computer science;parallel algorithm;molecular biophysics	Visualization	14.082950004751371	-60.32945717846798	175993
10ceb4e8e9c9e9988d147d116f30334538469c6f	structural modeling of hcv ns3/4a serine protease drug-resistance mutations using end-point continuum solvation and side-chain flexibility calculations		Computational methods of modeling protein-ligand interactions have gained widespread application in modern drug discovery. In continuum solvation-based methods of binding affinity estimation, limited description of solvent environment and protein flexibility is traded for a time scale that fits medicinal chemistry test cycles. The results of this speed-accuracy trade-off have been promising in terms of modeling structure-activity relationships of ligand series against protein targets. The potential of these approaches in recapitulating structural and energetic effects of resistance mutations, which involve large changes in binding affinity, remains relatively unexplored. We used continuum solvation binding affinity predictions and graph theory-based flexibility calculations to model thirteen drug resistance mutations in HCV NS3/4A serine protease, against three small-molecule inhibitors, with a 2-fold objective: quantitative assessment of binding energy predictions against experimental data and elucidation of structural/energetic determinants of resistance. The results show statistically significant correlation between predicted and experimental binding affinities, with R(2) and predictive index of up to 0.83 and 0.91, respectively. The level of accuracy was consistent with what has been reported for the inverse problem of binding affinity estimation of congeneric ligands against the same target. The quality of predictions was poor for mutations involving induced-fit effects, primarily, because of the lack of entropy terms. Flexibility analysis explained this discrepancy by indicating characteristic changes in side-chain mobility of a key binding site residue. The combined results from two approaches provide novel insights regarding the molecular mechanism of resistance. NS3/4A inhibitors, with large P2 substituents, derive high affinity with optimal van der Waals interactions in the S2 subsite, in order to overcome unfavorable desolvation and entropic cost of induced-fit effects. High-level resistance mutations tend to increase the desolvation and/or entropic barrier to ligand binding. The lead optimization strategies should, therefore, address the balance of these opposing energetic contributions in both the wild-type and mutant target.	apache continuum;barrier function;chemical procedure;class diagram;computation;discrepancy function;drug discovery;endopeptidases;fits;gain;graph - visual representation;graph theory;hepatitis c virus;implicit solvation;interaction;keyboard shortcut;ligands;mathematical optimization;medicinal chemistry;mutation;processor affinity;serine proteases;triune continuum paradigm	Hajira Ahmed Hotiana;Muhammad K. Haider	2013	Journal of chemical information and modeling	10.1021/ci3004754	crystallography;biochemistry	Comp.	10.373350863577148	-61.79174140883495	176048
80a450ef03fd476dcdec4eea1b67f4e1819719cc	literature-based generation of hypotheses on chemical composition using database co-occurrence of chemical compounds	chemical composition	"""Candidates for identification of unknown constituents in a sample to be chemically analyzed are hypothetical. It is proposed to generate these hypotheses according to the co-occurrence of different chemical compounds with a known sample constituent in the chemical literature. The efficiency of the co-occurrence approach for predicting chemical compositions was tested for 67 impurities in 17 chemical/pharmaceutical products. The relative co-occurrence of impurity compounds and these products in the Chemical Abstracts Service database was evaluated and compared with corresponding values for several reference groups of probability sampled compounds from the literature. Almost all impurities (97%) and only < or = 8% randomly sampled compounds co-occurred with these chemical products. Mean and median values of relative co-occurrence for impurities are much higher than those of probability sampled compounds which co-occurred with the products. For the combination of impurities and the probability sample of 396 interfering compounds, the power to predict the chemical composition using the highest co-occurrences is 0.49-0.59. The co-occurrence value can also be considered as an """"empiric"""" indicator of chemical similarity useful to generate new hypotheses on relationships both between compounds and between compounds and their properties."""		Boris L. Milman	2005	Journal of chemical information and modeling	10.1021/ci049716u	chemical composition;chemistry;toxicology;environmental chemistry;organic chemistry	Web+IR	11.926207480292765	-58.206921323316905	176313
0e95840e1b9c2f0435d08f5e05b28e40cfe3afd7	synapse maintenance in the where-what networks	unsupervised learning;object recognition;perforation;model based approach;network performance synapse maintenance where what network biologically inspired framework object recognition complex background leaked in background pixel motor supervised learning unsupervised learning post synaptic neuron;learning object;unsupervised learning object recognition;receptive field;neurons maintenance engineering training object recognition equations training data retina	General object recognition in complex backgrounds is still challenging. On one hand, the various backgrounds, where object may appear at different locations, make it difficult to find the object of interest. On the other hand, with the numbers of locations, types and variations in each type (e.g., rotation) increasing, conventional model-based approaches start to break down. The Where-What Networks (WWNs) were a biologically inspired framework for recognizing learned objects (appearances) from complex backgrounds. However, they do not have an adaptive receptive field for an object of a curved contour. Leaked-in background pixels will cause problems when different objects look similar. This work introduces a new biologically inspired mechanism - synapse maintenance and uses both supervised (motor-supervised for class response) and unsupervised learning (synapse maintenance) to realize objects recognition. Synapse maintenance is meant to automatically decide which synapse should be active firing of the post-synaptic neuron. With the synapse maintenance, the network has achieved a significant improvement in the network performance.	experiment;network performance;neuron;outline of object recognition;pixel;synapse;unsupervised learning;world wide name	Yuekai Wang;Xiaofeng Wu;Juyang Weng	2011	The 2011 International Joint Conference on Neural Networks	10.1109/IJCNN.2011.6033591	unsupervised learning;computer vision;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;machine learning;receptive field	Vision	22.712147065066034	-63.79292980759162	176446
6e31bc389a033bbef3dc832f6d9f8d8c14abc976	the free energy of locking a ring: changing a deoxyribonucleoside to a locked nucleic acid	dna;free energy perturbation;lna;bridged ring;md simulation;ribose	Locked nucleic acid (LNA), a modified nucleoside which contains a bridging group across the ribose ring, improves the stability of DNA/RNA duplexes significantly, and therefore is of interest in biotechnology and gene therapy applications. In this study, we investigate the free energy change between LNA and DNA nucleosides. The transformation requires the breaking of the bridging group across the ribose ring, a problematic transformation in free energy calculations. To address this, we have developed a 3-step (easy to implement) and a 1-step protocol (more efficient, but more complicated to setup), for single and dual topologies in classical molecular dynamics simulations, using the Bennett Acceptance Ratio method to calculate the free energy. We validate the approach on the solvation free energy difference for the nucleosides thymidine, cytosine, and 5-methyl-cytosine. © 2017 The Authors. Journal of Computational Chemistry Published by Wiley Periodicals, Inc.	bridging (networking);cell nucleus;cytosine;dna computing;deoxyribonucleosides;dual;john d. wiley;journal of computational chemistry;lock (computer science);low-noise amplifier;molecular dynamics;nucleic acids;nucleosides;preparation;rna;ribose;scientific publication;simulation;thymidine;free energy;latent orf73 antigen, human herpesvirus 8;locked nucleic acid	You Xu;Alessandra Villa;Lennart Nilsson	2017		10.1002/jcc.24692	biochemistry;stereochemistry;molecular biology;chemistry;free energy perturbation;low-noise amplifier;dna	Comp.	10.837885099387568	-62.87081213191759	176885
1ed49c3412cec519c2578baf6a5c6c3bf3a130b1	ligand-optimized homology models of d1 and d2 dopamine receptors: application for virtual screening		"""Recent breakthroughs in crystallographic studies of G protein-coupled receptors (GPCRs), together with continuous progress in molecular modeling methods, have opened new perspectives for structure-based drug discovery. A crucial enhancement in this area was development of induced fit docking procedures that allow optimization of binding pocket conformation guided by the features of its active ligands. In the course of our research program aimed at discovery of novel antipsychotic agents, our attention focused on dopaminergic D2 and D1 receptors (D2R and D1R). Thus, we decided to investigate whether the availability of a novel structure of the closely related D3 receptor and application of induced fit docking procedures for binding pocket refinement would permit the building of models of D2R and D1R that facilitate a successful virtual screening (VS). Here, we provide an in-depth description of the modeling procedure and the discussion of the results of a VS benchmark we performed to compare efficiency of the ligand-optimized receptors in comparison with the regular homology models. We observed that application of the ligand-optimized models significantly improved the VS performance both in terms of BEDROC (0.325 vs 0.182 for D1R and 0.383 vs 0.301 for D2R) as well as EF1% (20 vs 11 for D1R and 18 vs 10 for D2R). In contrast, no improvement was observed for the performance of a D2R model built on the D3R template, when compared with that derived from the structure of the previously published and more evolutionary distant β2 adrenergic receptor. The comparison of results for receptors built according to various protocols and templates revealed that the most significant factor for the receptor performance was a proper selection of """"tool ligand"""" used in induced fit docking procedure. Taken together, our results suggest that the described homology modeling procedure could be a viable tool for structure-based GPCR ligand design, even for the targets for which only a relatively distant structural template is available."""	adrenergic receptor;antipsychotic agents;benchmark (computing);boat dock;clinical use template;drd2 wt allele;docking (molecular);docking -molecular interaction;dopamine d2 receptor;dopamine hydrochloride;drug discovery;homology modeling;ligands;mathematical optimization;protocols documentation;refinement (computing);scientific publication;virtual screening;molecular modeling	Marcin Kolaczkowski;Adam Bucki;Marcin Feder;Maciej Pawlowski	2013	Journal of chemical information and modeling	10.1021/ci300413h	simulation;bioinformatics	Visualization	10.97152206197913	-60.10827686211921	176947
70a5bb1938a71802ea910e739b765fd9136b330f	a cognitive approach for robots' vision using unsupervised learning and visual saliency	visual saliency;unsupervised learning;object recognition	In this work we contribute to development of an online unsupervised technique allowing learning of objects from unlabeled images and their detection when seen again. We were inspired by early processing stages of human visual system and by existing work on human infants learning. We suggest a novel fast algorithm for detection of visually salient objects, which is employed to extract objects of interest from images for learning. We demonstrate how this can be used in along with state-of-the-art object recognition algorithms such as SURF and Viola-Jones framework to enable a machine to learn to re-detect previously seen objects in new conditions. We provide results of experiments done on a mobile robot in common office environment with multiple every-day objects.	robot;unsupervised learning	Dominik Maximilián Ramík;Christophe Sabourin;Kurosh Madani	2011		10.1007/978-3-642-21501-8_11	semi-supervised learning;unsupervised learning;computer vision;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition	Robotics	22.41659710496181	-64.11927072822265	177114
b17fa644a09762597236121dff7b7fe3bb32c70b	a validation study on the practical use of automated de novo design	simulated annealing;cyclooxygenase;drug design;matrix metalloproteinase;de novo design;estrogen receptor;similarity measure;cyclin dependent kinase;validation studies	The de novo design program Skelgen has been used to design inhibitor structures for four targets of pharmaceutical interest. The designed structures are compared to modeled binding modes of known inhibitors (i) visually and (ii) by means of a novel similarity measure considering the size and spatial proximity of the maximum common substructure of two small molecules. It is shown that the Skelgen algorithm generates representatives of many inhibitor classes within a very short time and that the new similarity measure is useful for comparing and clustering designed structures. The results demonstrate the necessity of properly defining search constraints in practical applications of de novo design.		Martin Stahl;Nikolay P. Todorov;Timothy James;Harald Mauser;Hans-Joachim Böhm;Philip M. Dean	2002	Journal of computer-aided molecular design	10.1023/A:1021242018286	biology;simulation;chemistry;simulated annealing;bioinformatics;estrogen receptor;cyclin-dependent kinase;genetics;matrix metalloproteinase;drug design	EDA	10.12130512923337	-58.71201489397165	177760
07e3f9cad17315cffb77b53a7e2fe72ed28ab946	a bayesian network model for protein fold and remote homologue recognition	bayesian network;hidden markov model;amino acid;amino acid sequence;posterior probability;recognition;graphical representation;confusion matrix;secondary structure;probability theory;model;protein folding;cross validation;structural classification of proteins;three dimensional structure;protein	MOTIVATION The Bayesian network approach is a framework which combines graphical representation and probability theory, which includes, as a special case, hidden Markov models. Hidden Markov models trained on amino acid sequence or secondary structure data alone have been shown to have potential for addressing the problem of protein fold and superfamily classification.   RESULTS This paper describes a novel implementation of a Bayesian network which simultaneously learns amino acid sequence, secondary structure and residue accessibility for proteins of known three-dimensional structure. An awareness of the errors inherent in predicted secondary structure may be incorporated into the model by means of a confusion matrix. Training and validation data have been derived for a number of protein superfamilies from the Structural Classification of Proteins (SCOP) database. Cross validation results using posterior probability classification demonstrate that the Bayesian network performs better in classifying proteins of known structural superfamily than a hidden Markov model trained on amino acid sequences alone.	accessibility;amino acid sequence;amino acids;bayesian network;classification;confusion matrix;cross reactions;hidden markov model;homologous gene;markov chain;network model;superfamily;scop	A. Raval;Zoubin Ghahramani;David L. Wild	2002	Bioinformatics	10.1093/bioinformatics/18.6.788	protein folding;biology;probability theory;amino acid;confusion matrix;variable-order bayesian network;computer science;bioinformatics;machine learning;pattern recognition;bayesian network;mathematics;peptide sequence;graphical model;posterior probability;hidden markov model;cross-validation;protein secondary structure	Comp.	10.102204429074936	-56.81233741702587	177848
6f2868e4ea92062a20d394c0bdd4ff31b5cf2f13	lingo, an efficient holographic text based method to calculate biophysical properties and intermolecular similarities		SMILES strings are the most compact text based molecular representations. Implicitly they contain the information needed to compute all kinds of molecular structures and, thus, molecular properties derived from these structures. We show that this implicit information can be accessed directly at SMILES string level without the need to apply explicit time-consuming conversion of the SMILES strings into molecular graphs or 3D structures with subsequent 2D or 3D QSPR calculations. Our method is based on the fragmentation of SMILES strings into overlapping substrings of a defined size that we call LINGOs. The integral set of LINGOs derived from a given SMILES string, the LINGO profile, is a hologram of the SMILES representation of the molecule described. LINGO profiles provide input for QSPR models and the calculation of intermolecular similarities at very low computational cost. The octanol/water partition coefficient (LlogP) QSPR model achieved a correlation coefficient R2=0.93, a root-mean-square error RRMS=0.49 log units, a goodness of prediction correlation coefficient Q2=0.89 and a QRMS=0.61 log units. The intrinsic aqueous solubility (LlogS) QSPR model achieved correlation coefficient values of R2=0.91, Q2=0.82, and RRMS=0.60 and QRMS=0.89 log units. Integral Tanimoto coefficients computed from LINGO profiles provided sharp discrimination between random and bioisoster pairs extracted from Accelrys Bioster Database. Average similarities (LINGOsim) were 0.07 for the random pairs and 0.36 for the bioisosteric pairs.	3d computer graphics;algorithmic efficiency;coefficient;extraction;fragmentation (computing);graph - visual representation;holography;lingo (programming language);molecular structure;molecular graph;octanols;quantitative structure–activity relationship;simplified molecular input line entry specification;simplified molecular-input line-entry system;substring;text-based (computing)	David Vidal;Michael Thormann;Miquel Pons	2005	Journal of chemical information and modeling	10.1021/ci0496797	artificial intelligence;theoretical computer science;machine learning;computational chemistry;mathematics	Comp.	13.407010820181899	-58.51467594199915	178044
511c02e24728415acc201042915a2e7e4245deb2	genetic programming for qsar investigation of docking energy	genetic program;settore inf 01 informatica;human serum albumin;drug discovery;kernel function;statistical method;genetic programming;docking energy;regression;drug design;machine learning;molecular descriptor;least square;acute toxicity;binding affinity;qsar;support vector machine;estrogen receptor;computational biology;drug development;artificial neural network;neural network;organic compound	Statistical methods, and in particular Machine Learning, have been increasingly used in the drug development workflow to accelerate the discovery phase and to eliminate possible failures early during clinical developments. In the past, the authors of this paper have been working specifically on two problems: (i) prediction of drug induced toxicity and (ii) evaluation of the target–drug chemical interaction based on chemical descriptors. Among the numerous existing Machine Learning methods and their application to drug development (see for instance [F. Yoshida, J.G. Topliss, QSAR model for drug human oral bioavailability, Journal of Medicinal Chemistry 43 (2000) 2575–2585; Frohlich, J. Wegner, F. Sieker, A. Zell, Kernel functions for attributed molecular graphs—a new similarity based approach to ADME prediction in classification and regression, QSAR and Combinatorial Science, 38(4) (2003) 427– 431; C.W. Andrews, L. Bennett, L.X. Yu, Predicting human oral bioavailability of a compound: development of a novel quantitative structure–bioavailability relationship, Pharmacological Research 17 (2000) 639–644; J Feng, L. Lurati, H. Ouyang, T. Robinson, Y. Wang, S. Yuan, S.S. Young, Predictive toxicology: benchmarking molecular descriptors and statistical methods, Journal of Chemical Information Computer Science 43 (2003) 1463–1470; T.M. Martin, D.M. Young, Prediction of the acute toxicity (96-h LC50) of organic compounds to the fat head minnow (Pimephales promelas) using a group contribution method, Chemical Research in Toxicology 14(10) (2001) 1378–1385; G. Colmenarejo, A. Alvarez-Pedraglio, J.L. Lavandera, Chemoinformatic models to predict binding affinities to human serum albumin, Journal of Medicinal Chemistry 44 (2001) 4370–4378; J. Zupan, P. Gasteiger, Neural Networks in Chemistry and Drug Design: An Introduction, 2nd edition, Wiley, 1999]), we have been specifically concerned with Genetic Programming. A first paper [F. Archetti, E. Messina, S. Lanzeni, L. Vanneschi, Genetic programming for computational pharmacokinetics in drug discovery and development, Genetic Programming and Evolvable Machines 8(4) (2007) 17–26] has been devoted to problem (i). The present contribution aims at developing a Genetic Programming based framework on which to build specific strategies which are then shown to be a valuable tool for problem (ii). In this paper, we use target estrogen receptor molecules and genistein based drug compounds. Being able to precisely and efficiently predict their mutual interaction energy is a very important task: for example, it may have an immediate relationship with the efficacy of genistein based drugs in menopause therapy and also as a natural prevention of some tumors. We compare the experimental results obtained by Genetic Programming with the ones of a set of ‘‘non-evolutionary’’ Machine Learning methods, including Support Vector Machines, Artificial Neural Networks, Linear and Least Square Regression. Experimental results confirm that Genetic Programming is a promising technique from the viewpoint of the accuracy of the proposed solutions, of the generalization ability and of the correlation between predicted data and correct ones. 2009 Elsevier B.V. All rights reserved.	artificial neural network;cheminformatics;computer science;docking (molecular);genetic programming;interaction energy;john d. wiley;jure zupan;machine learning;medicinal chemistry;molecular descriptor;molecular graph;neural networks;neural network software;quantitative structure–activity relationship;regular expression;support vector machine	Francesco Archetti;Ilaria Giordani;Leonardo Vanneschi	2010	Appl. Soft Comput.	10.1016/j.asoc.2009.06.013	molecular descriptor;kernel;genetic programming;support vector machine;serum albumin;regression;computer science;bioinformatics;artificial intelligence;machine learning;estrogen receptor;acute toxicity;ligand;quantitative structure–activity relationship;least squares;drug discovery;artificial neural network;drug design	AI	12.100414755589922	-57.31841688578589	178310
eb41eaa803ce4657647ae5ae4762c7c7bf7e1959	automated analysis of images from confocal laser scanning microscopy applied to observation of calcium channel subunits in nerve cell model line subjected electroporation and calcium		We assess a possibility of applying automated image analysis to immunofluorescence microphotographs from confocal laser scanning microscopy (CLSM). Several modes of automated analysis were tested to inspect differentiation of voltage dependent calcium channel subunits from a model of nerve cells PC12 (rat pheochromocytoma) subjected to electroporation (EP), with regard to extracellular calcium level. The objective of the experiments was evaluating sensitivity of the channel expression to the presence of calcium and electroporation voltage. For this purpose non-selective nanopores of a controlled conductivity were generated in the cell membrane using electroporation, at physiological or increased extracellular calcium concentrations. Introduction of Ca2+ into the cells was possible through electropores and physiological voltage-dependent calcium channels. Two subunits of calcium channel (α1H and α1G) were immunofluorescentically stained and the automated analysis of changes in cellular morphology was performed, based on comparative assessment of the fluorescence signal. The automated analysis allowed apparently higher observation capabilities. The results showed morphological changes in the channel subunits and higher expression of the channel, following its exposition to electric field or calcium.	neuron	Julita Kulbacka;Marek Kulbacki;Jakub Segen;Anna Choromanska;Jolanta Saczko;Magda Dubińska-Magiera;Malgorzata Kotulska	2015		10.1007/978-3-319-15705-4_29	computer science;data mining;extracellular;electroporation;immunofluorescence;fluorescence;calcium channel;biophysics;calcium;voltage-dependent calcium channel;cell membrane	Robotics	12.595619489942308	-65.98062165586312	178732
622d53dadbb89c1e70578d57cbb6dcdcf48254e8	protein threading with residue-environment matching by artificial neural networks	score function;decoy set;amino acid;protein sequence;amino acid sequence;protein threading;protein structure;artificial neural networks;knowledge based potential;three dimensional structure;potential energy;artificial neural network;knowledge base	Protein threading programs align a probe amino acid sequence onto a library of representative folds of known protein structure to identify a structural homology. A scoring function is usually formulated in terms of the threading energy to evaluate protein sequence-structure fitness. The structure that yields the lowest total energy is considered the leading template of the probe protein. An alternative approach is to predict the probabilities of observing amino acid side-chains in structural environment without considering the energy of contacts. In this paper, a model named TES is proposed on building a new environment-specific protein sequence-structure mapping with artificial neural network. The decoy sets obtained from the web are used to test the proposed TES method on discrimination of native and decoy protein three-dimensional structure. The verified approach shows that the performance of the proposed method is comparable to those of knowledge-based potential energy function.	align (company);artificial neural network;homology (biology);mathematical optimization;peptide sequence;scoring functions for docking;statistical potential;thread (computing);threading (protein sequence)	Nan Jiang;Wendy Xinyu Wu;Ian M. Mitchell	2004		10.1145/967900.967943	threading;protein structure;amino acid;computer science;bioinformatics;machine learning;potential energy;protein sequencing;protein structure prediction;peptide sequence;score;artificial neural network	Comp.	10.917651505792373	-57.40628407143797	178885
ea9025f8231bb4bceb853be50fd6db55808f3ffe	classification and prediction of protein side-chains by neural network techniques	neural network	Neural Network methodology is used to classify and predict side-chain configurations in proteins on the basis of their sequence and in some cases also Cα-atomic distance information. In some of these methods, where Potts Associative Memories are employed, a mixed set of Potts systems each describe the various orientational states of a particular side-chain. The methods can find the correct side-chain orientations in proteins reasonably well after being trained on a data set of other proteins of known 3-dimensional structure.	artificial neural network	J. Irwin;Henrik G. Bohr;K. Mochizuki;Peter G. Wolynes	1992	Int. J. Neural Syst.	10.1142/S0129065792000504	computer science;artificial intelligence;machine learning;pattern recognition;time delay neural network;artificial neural network	ML	10.698862886382036	-55.71602182996233	178940
4f3d0ca48fbb587a0aa57b1fd2e51e99f54d9f74	building three-dimensional ribonucleic acid structures	dna;symbol manipulation;biology computing;nonlinear optimization methods three dimensional ribonucleic acid structures mc sym intelligent computer system 3d ribonucleic acid structures low resolution data symbolic computations numerical computations all atom sketches;ribonucleic acid;expert systems;nonlinear programming;low resolution;three dimensional;genetics;buildings rna intelligent systems intelligent structures nuclear magnetic resonance biochemistry laboratories optimization methods biology computing proteins;data visualisation;proteins;numerical computation;molecular biophysics;macromolecules;nonlinear optimization;3d structure;expert systems macromolecules biology computing symbol manipulation nonlinear programming	orative research effort on ribonucleic acid (RNA) structure and function that started in 1987 in Robert Cedergren’s laboratory at the Université de Montréal. It involves multidisciplinary research in the fields of molecular biology, biochemistry, and computer science. The project ultimately produced MCSym, an intelligent system designed for building RNA three-dimensional (3D) structures. The RNA structure determination problem (R3D) is the search for assignments of 3D coordinates to each atom in an RNA molecule consistent with experimental or theoretical structural data when no high-resolution techniques can be applied, such as x-ray crystallography or nuclear magnetic resonance (NMR) spectroscopy. The goal of solving R3D is to fill the gap between the number of known RNA molecules and the number of resolved structures at good resolution. R3D can be solved by using a synergetic approach between laboratory experiments and theoretical modeling. Experimental results are input to theoretical modeling, the results of which help design more precise and concise laboratory experiments. The modeling counterpart involves four steps. Step 1 consists of collecting and interpreting RNA structural data. Step 2 includes a geometrical interpretation of the accumulated structural data. Step 3 is building 3D structures, or models, that are consistent with the data prepared and formatted in Step 2. Finally, an optional Step 4 consists of refining the structures produced in Step 3 via numerical optimization methods, such as molecular mechanics and dynamics. Two of MC-Sym’s main components are the MC-Core library (http://mccore.sourceforge.net) and the MC-Sym computer program itself. MCCore is a set of C++ data structures and algorithms that store and manipulate RNA, DNA, and 3D protein structures. MC-Sym lets molecular biologists project objectively the various types and sources of RNA structural data in all-atom 3D structures (see www-lbit.iro.umontreal.ca/mcsym/). This article describes MC-Sym’s role in RNA structure determination and modeling.	acid;algorithm;artificial intelligence;atom;c++;computer program;computer science;data structure;experiment;image resolution;mathematical optimization;molecular mechanics;resonance;sourceforge;synergetics (haken)	François Major	2003	Computing in Science and Engineering	10.1109/MCISE.2003.1225860	macromolecule;three-dimensional space;computational science;rna;simulation;image resolution;nonlinear programming;computer science;bioinformatics;theoretical computer science;mathematics;dna;algorithm	Comp.	13.431412933774745	-60.87409180389541	179242
f38cedfd93f421058682c30635a64fc0a2498192	understanding deep convolutional networks through gestalt theory		The superior performance of deep convolutional networks over high-dimensional problems have made them very popular for several applications. Despite their wide adoption, their underlying mechanisms still remain unclear with their improvement procedures still relying mainly on a trial and error process. We introduce a novel sensitivity analysis based on the Gestalt theory for giving insights into the classifier function and intermediate layers. Since Gestalt psychology stipulates that perception can be a product of complex interactions among several elements, we perform an ablation study based on this concept to discover which principles and image context significantly contribute in the network classification. Our results reveal that convnets follow most of the visual cortical perceptual mechanisms defined by the Gestalt principles at several levels. The proposed framework stimulates specific feature maps in classification problems and reveal important network attributes that can produce more explainable network models.		Angelos Amanatiadis;Kaustubh C Patankar;Elias B. Kosmatopoulos	2018	2018 IEEE International Conference on Imaging Systems and Techniques (IST)	10.1109/IST.2018.8577159	artificial intelligence;machine learning;visualization;perception;gestalt psychology;network model;pattern recognition;computer science;trial and error	Vision	21.39824844464644	-53.55625702958662	179957
cf41672906c4bbbec3b67471cbce59a3111530ec	surface networks		We study data-driven representations for three-dimensional triangle meshes, which are one of the prevalent objects used to represent 3D geometry. Recent works have developed models that exploit the intrinsic geometry of manifolds and graphs, namely the Graph Neural Networks (GNNs) and its spectral variants, which learn from the local metric tensor via the Laplacian operator. Despite offering excellent sample complexity and built-in invariances, intrinsic geometry alone is invariant to isometric deformations, making it unsuitable for many applications. To overcome this limitation, we propose several upgrades to GNNs to leverage extrinsic differential geometry properties of three-dimensional surfaces, increasing its modeling power. In particular, we propose to exploit the Dirac operator, whose spectrum detects principal curvature directions - this is in stark contrast with the classical Laplace operator, which directly measures mean curvature. We coin the resulting models Surface Networks (SN). We prove that these models define shape representations that are stable to deformation and to discretization, and we demonstrate the efficiency and versatility of SNs on two challenging tasks: temporal prediction of mesh deformations under non-linear dynamics and generative models using a variational autoencoder framework with encoders/decoders given by SNs.	artificial neural network;autoencoder;computational complexity theory;computer graphics;deep learning;dirac operator;discretization;dynamical system;encoder;experiment;first-order predicate;generative model;geometry processing;image scanner;isometric projection;message passing;nonlinear system;point cloud;sample complexity;sensor;triangle mesh;variational principle	Ilya Kostrikov;Joan Bruna;Daniele Panozzo;Denis Zorin	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00269	manifold;autoencoder;artificial intelligence;pattern recognition;dirac operator;differential geometry;invariant (mathematics);computer science;topology;laplace operator;mean curvature;principal curvature	Vision	23.58493778285049	-52.31266144648491	180324
506212fa4ec808a7bbd497598af4fbb1efcdcb67	producing high-accuracy lattice models from protein atomic coordinates including side chains	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;uk phd theses thesis;life sciences;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	Lattice models are a common abstraction used in the study of protein structure, folding, and refinement. They are advantageous because the discretisation of space can make extensive protein evaluations computationally feasible. Various approaches to the protein chain lattice fitting problem have been suggested but only a single backbone-only tool is available currently. We introduce LatFit, a new tool to produce high-accuracy lattice protein models. It generates both backbone-only and backbone-side-chain models in any user defined lattice. LatFit implements a new distance RMSD-optimisation fitting procedure in addition to the known coordinate RMSD method. We tested LatFit's accuracy and speed using a large nonredundant set of high resolution proteins (SCOP database) on three commonly used lattices: 3D cubic, face-centred cubic, and knight's walk. Fitting speed compared favourably to other methods and both backbone-only and backbone-side-chain models show low deviation from the original data (~1.5 Å RMSD in the FCC lattice). To our knowledge this represents the first comprehensive study of lattice quality for on-lattice protein models including side chains while LatFit is the only available tool for such models.	cubic function;curve fitting;discretization;evaluation;image resolution;internet backbone;lattice model (physics);mathematical optimization;refinement (computing);scop;vertebral column	Martin Mann;Rhodri Saunders;Cameron Smith;Rolf Backofen;Charlotte M. Deane	2012		10.1155/2012/148045	biology;medical research;medicine;computer science;bioinformatics;data mining;operations research	Comp.	12.428490264847081	-60.11559965154507	180487
12fa965af814e712dbe65ac45eaa43108b059563	pe-db: a database of structural ensembles of intrinsically disordered and of unfolded proteins	community;x ray diffraction;qd chemistry kemia;nuclear magnetic resonance;pre eclampsia;diagnostic radiologic examination;pericardial effusion;emphysema pulmonary;internet;scattering small angle;pulmonary edema;nuclear magnetic resonance biomolecular;education physical;pulmonary embolism;phosphatidylethanolamines;intrinsically disordered proteins;plasma exchange;protein unfolding;databases protein;peru	The goal of pE-DB (http://pedb.vib.be) is to serve as an openly accessible database for the deposition of structural ensembles of intrinsically disordered proteins (IDPs) and of denatured proteins based on nuclear magnetic resonance spectroscopy, small-angle X-ray scattering and other data measured in solution. Owing to the inherent flexibility of IDPs, solution techniques are particularly appropriate for characterizing their biophysical properties, and structural ensembles in agreement with these data provide a convenient tool for describing the underlying conformational sampling. Database entries consist of (i) primary experimental data with descriptions of the acquisition methods and algorithms used for the ensemble calculations, and (ii) the structural ensembles consistent with these data, provided as a set of models in a Protein Data Bank format. PE-DB is open for submissions from the community, and is intended as a forum for disseminating the structural ensembles and the methodologies used to generate them. While the need to represent the IDP structures is clear, methods for determining and evaluating the structural ensembles are still evolving. The availability of the pE-DB database is expected to promote the development of new modeling methods and leads to a better understanding of how function arises from disordered states.		Mihaly Varadi;Simone Kosol;Pierre Lebrun;Erica Valentini;Martin Blackledge;A. Keith Dunker;Isabella C. Felli;Julie Deborah Forman-Kay;Richard W. Kriwacki;Roberta Pierattelli;Joel L. Sussman;Dmitri I. Svergun;Vladimir N. Uversky;Michele Vendruscolo;David S. Wishart;Peter E. Wright;Peter Tompa	2014		10.1093/nar/gkt960	biology;community;the internet;unfolded protein response;x-ray crystallography	DB	12.948137120585187	-60.07560729561615	180562
95fadf3175bd6c5b8cba8f7b4d117f9993af3cd8	fast and accurate prediction of protein side-chain conformations	software;models molecular;proteins;protein conformation;algorithms	SUMMARY We developed a fast and accurate side-chain modeling program [Optimized Side Chain Atomic eneRgy (OSCAR)-star] based on orientation-dependent energy functions and a rigid rotamer model. The average computing time was 18 s per protein for 218 test proteins with higher prediction accuracy (1.1% increase for χ(1) and 0.8% increase for χ(1+2)) than the best performing program developed by other groups. We show that the energy functions, which were calibrated to tolerate the discrete errors of rigid rotamers, are appropriate for protein loop selection, especially for decoys without extensive structural refinement.   AVAILABILITY OSCAR-star and the 218 test proteins are available for download at http://sysimm.ifrec.osaka-u.ac.jp/OSCAR CONTACT: standley@ifrec.osaka-u.ac.jp   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	3d computer graphics;bioinformatics;computation (action);download;gucy2c protein, human;muscle rigidity;refinement (computing)	Shide Liang;Dandan Zheng;Chi Zhang;Daron M. Standley	2011		10.1093/bioinformatics/btr482	biology;protein structure;simulation;computer science;bioinformatics;theoretical computer science	Comp.	11.95299295625089	-60.16285372236268	180706
f3de37811b3aed753160e3128e4aac80256ab9da	learning-based just-noticeable-quantization- distortion modeling for perceptual video coding		Conventional predictive video coding-based approaches are reaching the limit of their potential coding efficiency improvements, because of severely increasing computation complexity. As an alternative approach, perceptual video coding (PVC) has attempted to achieve high coding efficiency by eliminating perceptual redundancy, using just-noticeable-distortion (JND) directed PVC. The previous JNDs were modeled by adding white Gaussian noise or specific signal patterns into the original images, which were not appropriate in finding JND thresholds due to distortion with energy reduction. In this paper, we present a novel discrete cosine transform-based energy-reduced JND model, called ERJND, that is more suitable for JND-based PVC schemes. Then, the proposed ERJND model is extended to two learning-based just-noticeable-quantization-distortion (JNQD) models as preprocessing that can be applied for perceptual video coding. The two JNQD models can automatically adjust JND levels based on given quantization step sizes. One of the two JNQD models, called LR-JNQD, is based on linear regression and determines the model parameter for JNQD based on extracted handcraft features. The other JNQD model is based on a convolution neural network (CNN), called CNN-JNQD. To our best knowledge, our paper is the first approach to automatically adjust JND levels according to quantization step sizes for preprocessing the input to video encoders. In experiments, both the LR-JNQD and CNN-JNQD models were applied to high efficiency video coding (HEVC) and yielded maximum (average) bitrate reductions of 38.51% (10.38%) and 67.88% (24.91%), respectively, with little subjective video quality degradation, compared with the input without preprocessing applied.	algorithmic efficiency;artificial neural network;attempt;biological neural networks;computation;convolution;data compression;discrete cosine transform;distortion;elegant degradation;encoder;experiment;extraction;frame (physical object);high efficiency video coding;just-noticeable difference;lr parser;norm (social);normal statistical distribution;pixel;population parameter;preprocessor;quantization (signal processing);rna, untranslated;zero suppression	Sehwan Ki;Sung-Ho Bae;Munchurl Kim;Hyunsuk Ko	2018	IEEE Transactions on Image Processing	10.1109/TIP.2018.2818439	convolutional neural network;encoder;coding (social sciences);quantization (signal processing);discrete cosine transform;artificial intelligence;algorithmic efficiency;pattern recognition;distortion;subjective video quality;mathematics	Vision	20.38833606414925	-55.50332643695648	181161
5f51908417158a25df9cbb628da6c812601f4eb5	epigenetic sensorimotor pathways and its application to developmental object learning	robot sensing systems;biology computing;genomics;recurrent neural nets learning artificial intelligence multilayer perceptrons;open ended learning;learning algorithm;genomic equivalence principle epigenetic sensorimotor pathways developmental object learning central nervous system multi layer in place learning network recurrent network open ended learning;computer model;signal design;multilayer perceptrons;real time;working environment noise;genomic equivalence principle;biological system modeling;neurons radar robot sensing systems radar imaging genomics bioinformatics computer architecture;epigenetic sensorimotor pathways;computer networks;developmental object learning;computer architecture;computational modeling;recurrent network;signal processing;genome equivalents;radar imaging;visual analysis;cross validation;recurrent neural nets;neurons;learning artificial intelligence;place learning;robot vision systems;central nervous system;multi layer in place learning network;cameras;internal model;radar;bioinformatics	A pathway in the central nervous system (CNS) is a path through which nervous signals are processed in an orderly fashion. A sensorimotor pathway starts from a sensory input and ends at a motor output, although almost all pathways are not simply unidirectional. In this paper, we introduce a simple, biologically inspired, unified computational model - Multi-layer In-place Learning Network (MILN), with a design goal to develop a recurrent network, as a function of sensorimotor signals, for open-ended learning of multiple sensorimotor tasks. The biologically motivated MILN provides automatic feature derivation and pathway refinement from the temporally real-time inputs. The work presented here is applied in the challenging application field of developing reactive behaviors from a video camera and a (noisy) radar range sensor for a vehicle-based robot in open, natural driving environments. An internal model of the agentpsilas experience of the environments is created and refined from the ground-up using a cell-centered model, based on the genomic equivalence principle. The outputs can be imposed by a teacher, at the same time as the learning is active. At any time instant, sensory information from the radar allows the system to focus its visual analysis on relatively small areas within the image plane (attention selection), in a computationally efficient way, suitable for real-time training. This system was trained with data from 10 different city and highway road environments, and cross validation shows that MILN was able to correctly recognize above 95% of the radar-extracted images from the multiple environments. The in-place learning mechanism compares with other learning algorithms favorably, as results of a comparison indicate that in-place learning is the only one to fit all the specified criteria of development of a general-purpose sensorimotor pathway.	algorithmic efficiency;autonomous car;autonomous robot;cns;computational model;gene regulatory network;general-purpose modeling;image plane;in-place algorithm;machine learning;neural coding;neuron;nonlinear gameplay;online machine learning;outline of object recognition;pixel;radar;real-time clock;real-time locating system;recurrent neural network;refinement (computing);sensor;sparse matrix;systems architecture;top-down and bottom-up design;turing completeness	Zhengping Ji;Matthew D. Luciw;Juyang Weng	2008	2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)	10.1109/CEC.2008.4631333	genomics;internal model;simulation;computer science;artificial intelligence;central nervous system;machine learning;signal processing;radar imaging;computational model;radar;cross-validation	Vision	20.239669793585556	-65.6188441496211	181209
6366f221f03dea370a6f8d3da02add80f98be505	parameter turning of pid controller based on molecular beacon dna computing	molecular beacon;base pairing;pid controller	Molecular beacon deoxyribonucleic acid computing is new research focus of intelligent control theory in recent years, it is also new bionic algorithm. It is well known that a very important problem how to determine or tune the proportional integral derivative controller parameters, because these parameters have a great influence on the stability and the performance of the control system. Parameter turning of proportional integral derivative controller by using molecular beacon deoxyribonucleic acid computing can avoid system earlyripe and find global optimal solution rapidly. Molecular beacon is a single strand of deoxyribonucleic acid base pairs formed their own part of the hairpin-like fluorescent probes, the use of molecular beacon can readily detect the concentration of deoxyribonucleic acid molecule which matching with it in test tube, the result of detection can decide which need to be copied and which need to be discarded. The molecular beacon deoxyribonucleic acid computing has high reliability and easy operation for proportional integral derivative controller parameter tuning. The result of simulation proves that molecular beacon deoxyribonucleic acid computing algorithm has distinct advantages than traditional algorithm. molecular beacon deoxyribonucleic acid computing is bound to has very great impact on intelligent control in the future.	algorithm;computer;control system;control theory;dna computing;dna microarray;degree of parallelism;instability;intelligent control;pid;parallel computing;programming language;sensitivity and specificity;simulation;software bug;strand (programming language)	Yourui Huang;Xiaomin Tian;Jing Wang;Hongping Zhou	2012	JSW		pid controller;real-time computing;base pair;molecular beacon	HPC	16.452756936055064	-58.53405219445418	181549
e745ef4d11447033c032257efe730d1977be1115	a polynomial time algorithm for computing the area under a gdt curve	biological patents;biomedical journals;text mining;europe pubmed central;citation search;physiological cellular and medical topics;citation networks;computational biology bioinformatics;research articles;abstracts;open access;life sciences;clinical guidelines;algorithms;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Progress in the field of protein three-dimensional structure prediction depends on the development of new and improved algorithms for measuring the quality of protein models. Perhaps the best descriptor of the quality of a protein model is the GDT function that maps each distance cutoff θ to the number of atoms in the protein model that can be fit under the distance θ from the corresponding atoms in the experimentally determined structure. It has long been known that the area under the graph of this function (GDT_A) can serve as a reliable, single numerical measure of the model quality. Unfortunately, while the well-known GDT_TS metric provides a crude approximation of GDT_A, no algorithm currently exists that is capable of computing accurate estimates of GDT_A. We prove that GDT_A is well defined and that it can be approximated by the Riemann sums, using available methods for computing accurate (near-optimal) GDT function values. In contrast to the GDT_TS metric, GDT_A is neither insensitive to large nor oversensitive to small changes in model’s coordinates. Moreover, the problem of computing GDT_A is tractable. More specifically, GDT_A can be computed in cubic asymptotic time in the size of the protein model. This paper presents the first algorithm capable of computing the near-optimal estimates of the area under the GDT function for a protein model. We believe that the techniques implemented in our algorithm will pave ways for the development of more practical and reliable procedures for estimating 3D model quality.	approximation algorithm;cobham's thesis;computation (action);cubic function;estimated;experiment;gamma-delta tocotrienol;global distance test;graph - visual representation;map;numerical analysis;p (complexity);polynomial;staphylococcal protein a;whole earth 'lectronic link	Aleksandar Poleksic	2015		10.1186/s13015-015-0058-0	text mining;medical research;computer science;bioinformatics;data science;data mining;mathematics;algorithm	Comp.	13.321499120334684	-59.22790685325211	181557
02acb8e0835df4911a71eb135c231abb86a7dec9	mmgbsa as a tool to understand the binding affinities of filamin-peptide interactions		Filamins (FLN) are large dimeric proteins that cross-link actin and work as important scaffolds in human cells. FLNs consist of an N-terminal actin-binding domain followed by 24 immunoglobulin-like domains (FLN1-24). FLN domains are divided into four subgroups based on their amino acid sequences. One of these subgroups, including domains 4, 9, 12, 17, 19, 21, and 23, shares a similar ligand-binding site between the β strands C and D. Several proteins, such as integrins β2 and β7, glycoprotein Ibα (GPIbα), and migfilin, have been shown to bind to this site. Here, we computationally estimated the binding free energies of filamin A (FLNa) subunits with bound peptides using the molecular mechanics-generalized Born surface area (MMGBSA) method. The obtained computational results correlated well with the experimental data, and they ranked efficiently both the binding of one ligand to all used FLNa-domains and the binding of all used ligands to FLNa21. Furthermore, the steered molecular dynamics (SMD) simulations pinpointed the binding hot spots for these complexes. These results demonstrate that molecular dynamics combined with free energy calculations are applicable to estimating the energetics of protein-protein interactions and can be used to direct the development of novel FLN function modulators.	amino acid sequence;amino acids;computation;energy, physics;estimated;exanthema;fblim1 gene;flna gene;flna wt allele;filamin a;filamins;implicit solvation;integrins;ligands;molecular dynamics;molecular mechanics;platelet membrane glycoprotein ib;simulation;surface-mount technology;free energy;protein protein interaction	Mikko Ylilauri;Olli T. Pentikäinen	2013	Journal of chemical information and modeling	10.1021/ci4002475	experimental data;actin;bioinformatics;flna;filamin;glycoprotein;amino acid;integrin;chemistry;peptide;molecular biology	Comp.	10.289972737192251	-60.13681089849343	181578
56c71eb43103364a1cbf5c1d64174d371f47764d	modular parameter identification of biomolecular networks	92c42;systems biology;90c30;modularization;parameter identification;34a55	The increasing complexity of dynamic models in systems and synthetic biology poses computational challenges especially for the identification of model parameters. While modularization of the corresponding optimization problems could help reduce the “curse of dimensionality,” abundant feedback and crosstalk mechanisms prohibit a simple decomposition of most biomolecular networks into subnetworks, or modules. Drawing on ideas from network modularization and multiple-shooting optimization, we present here a modular parameter identification approach that explicitly allows for such interdependencies. Interfaces between our modules are given by the experimentally measured molecular species. This definition allows deriving good (initial) estimates for the inter-module communication directly from the experimental data. Given these estimates, the states and parameter sensitivities of different modules can be integrated independently. To achieve consistency between modules, we iteratively adjust the estimates for i...		Moritz Lang;Jörg Stelling	2016	SIAM J. Scientific Computing	10.1137/15M103306X	mathematical optimization;computer science;bioinformatics;theoretical computer science;machine learning;modular programming;systems biology	HPC	13.381082723046502	-52.6304483435717	181618
fa76c875684210548275236f01e382cd82d747de	optical dna biosensor based on square-planar ethyl piperidine substituted nickel(ii) salphen complex for dengue virus detection	dengue virus detection;nickel(ii) salphen complex;optical dna biosensor;porous silica nanospheres;reflectance measurement;synthetic dna binder	A sensitive and selective optical DNA biosensor was developed for dengue virus detection based on novel square-planar piperidine side chain-functionalized N,N'-bis-4-(hydroxysalicylidene)-phenylenediamine-nickel(II), which was able to intercalate via nucleobase stacking within DNA and be functionalized as an optical DNA hybridization marker. 3-Aminopropyltriethoxysilane (APTS)-modified porous silica nanospheres (PSiNs), was synthesized with a facile mini-emulsion method to act as a high capacity DNA carrier matrix. The Schiff base salphen complexes-labelled probe to target nucleic acid on the PSiNs renders a colour change of the DNA biosensor to a yellow background colour, which could be quantified via a reflectance transduction method. The reflectometric DNA biosensor demonstrated a wide linear response range to target DNA over the concentration range of 1.0 × 10-16-1.0 × 10-10 M (R² = 0.9879) with an ultralow limit of detection (LOD) at 0.2 aM. The optical DNA biosensor response was stable and maintainable at 92.8% of its initial response for up to seven days of storage duration with a response time of 90 min. The reflectance DNA biosensor obtained promising recovery values of close to 100% for the detection of spiked synthetic dengue virus serotypes 2 (DENV-2) DNA concentration in non-invasive human samples, indicating the high accuracy of the proposed DNA analytical method for early diagnosis of all potential infectious diseases or pathological genotypes.	3-(triethoxysilyl)propylamine;base excess:scnc:pt:blda:qn:calculated;communicable diseases;dengue fever;dengue virus 2;dhrystone;early diagnosis;emulsions;limit of detection;maxima and minima;murine sarcoma viruses;nanospheres;nucleic acid hybridization;nucleic acids;rendering (computer graphics);repast (modeling toolkit);response time (technology);schiff bases;serotype;silicic acid;silicon dioxide;stacking;transduction (machine learning);icosapent ethyl;nucleobase;piperidine;salphen	Eda Yuhana Ariffin;Lingling Tan;Nurul Huda Abd. Karim;Lee Yook Heng	2018		10.3390/s18041173	engineering;analytical chemistry;stacking;piperidine;dna;nucleic acid;transduction (genetics);dengue virus;nucleobase;biosensor	Vision	11.184915183610284	-65.05009114615153	182569
5c4f2045324f90045c2e00c56e428c978dee353e	factoring tertiary classification into binary classification improves neural network for protein secondary structure prediction	biology computing;encoding scheme tertiary classification binary classification neural network protein secondary structure prediction bioinformatics research cross validation method computational biology position specific scoring matrix;neural nets molecular biophysics proteins biology computing;neural nets;neural networks testing accuracy encoding computational biology speech recognition computer science matrix decomposition protein sequence drugs;proteins;protein secondary structure prediction;molecular biophysics;prediction accuracy;binary classification;cross validation;computational biology;neural network	Protein secondary structure prediction is one of the most important problems in bioinformatics research. When the traditional tertiary classifier is used in our neural network, 72% accuracy is reached. Since the neural network might not work very well in three-class classification for certain domains, the three-class problem is reduced to six binary class problems for the first time to carry out protein secondary structure prediction. With the combination of six binary classifiers, we experiment and test several tertiary classifiers. Additionally, three new tertiary classifiers are proposed in this study: MAX/spl I.bar/HEC, ONE/spl I.bar/TO/spl I.bar/ONE/spl I.bar/MAX and ONE/spl I.bar/TO/spl I.bar/ONE/spl I.bar/VOTE. ONE/spl I.bar/TO/spl I.bar/ONE/spl I.bar/VOTE outperforms the six other experimental tertiary classifiers in this study. ONE/spl I.bar/TO/spl I.bar/ONE/spl I.bar/VOTE tertiary classifier with PSSM encoding scheme obtains 74.02% test accuracy on RS126 dataset. To the best of our knowledge, this is the best result for RS126 dataset with the cross-validation method for neural network. The improvement of prediction accuracy indicates that decomposition of the multiclass problem into several binary class problems may be applied to other areas of computational biology in order to increase generalization power of neural networks.	artificial neural network;binary classification;bioinformatics;crc-based framing;computation;computational biology;cross-validation (statistics);integer factorization;line code;max;multiclass classification;position weight matrix;protein structure prediction	Wei Zhong;Gulsah Altun;Hae-Jin Hu;Robert W. Harrison;Phang C. Tai;Yi Pan	2004	2004 Symposium on Computational Intelligence in Bioinformatics and Computational Biology	10.1109/CIBCB.2004.1393951	binary classification;computer science;bioinformatics;machine learning;data mining;artificial neural network;cross-validation	ML	10.2739588660715	-53.865825049855445	182582
0ee95d48a538024dc7850a7a9a116e64fa2ad243	accurate prediction of enthalpies of formation for a large set of organic compounds	organic compounds;journal;enthalpy of formation;least square;dft;organic compound	This article describes a multiparameter calibration model, which improves the accuracy of density functional theory (DFT) for the prediction of standard enthalpies of formation for a large set of organic compounds. The model applies atom based, bond based, electronic, and radical environmental correction terms to calibrate the calculated enthalpies of formation at B3LYP/6-31G(d,p) level by a least-square method. A diverse data set of 771 closed-shell compounds and radicals is used to train the model. The leave-one-out cross validation squared correlation coefficient q(2) of 0.84 and squared correlation coefficient r(2) of 0.86 for the final model are obtained. The mean absolute error in enthalpies of formation for the dataset is reduced from 4.9 kcal/mol before calibration to 2.1 kcal/mol after calibration. Five-fold cross validation is also used to estimate the performance of the calibration model and similar results are obtained.	ab initio quantum chemistry methods;analysis of algorithms;approximation error;basis set (chemistry);calibration;coefficient;cross infection;cross reactions;cross-validation (statistics);duoxa1 gene;density functional theory;electron shell;energy, physics;functional theories of grammar;hybrid functional;large;mav protocol;organic chemicals;organometallic compounds;utility functions on indivisible goods;kilocalorie	Cun-Xi Liu;Hai-Xia Wang;Ze-Rong Li;Chong-Wen Zhou;Hanbing Rao;Xiang-Yuan Li	2010	Journal of computational chemistry	10.1002/jcc.21550	standard enthalpy change of formation;chemistry;discrete fourier transform;computational chemistry;physical chemistry;least squares	ML	12.300364360865672	-58.41772440907047	183150
cef499b595e4bec28612aefde77986aaf22adbdf	visualizing movements of protein tunnels in molecular dynamics simulations	delaunay triangulation;visualization;i 3 7 computer graphics;animation;animationvisible line surface algorithms;three dimensional graphics and realism;tunnel;protein;voronoi diagram	Analysis and visualization of molecules and their structural features help biochemists and biologists to better understand protein behavior. Studying these structures in molecular dynamics simulations enhances this understanding. In this paper we introduce three approaches for animating specific inner pathways composed of an empty space between atoms, called tunnels. These tunnels facilitate the transport of small molecules, water solvent and ions in many proteins. They help researchers understand the structure-function relationships of proteins and the knowledge of tunnel properties improves the design of new inhibitors. Our methods are derived from selected tunnel representations when each stresses some of the important tunnel properties — width, shape, mapping of physico-chemical properties, etc. Our methods provide smooth animation of the movement of tunnels as they change their length and shape throughout the simulation.	computer simulation;molecular dynamics	Barbora Kozlíková;Adam Jurcík;Jan Byska;Ondrej Strnad;Jirí Sochor	2014		10.2312/vcbm.20141188	simulation;computer science;theoretical computer science;computer graphics (images)	Comp.	14.833755089678593	-62.04708171650863	183253
033238af184d3b9f869c6c584e8132dd3ae29109	feature extraction of electronic nose signals using qpso-based multiple kfda signal processing	classification;electronic nose;feature extraction;multiple kernel learning;weighted kernels fisher discriminant analysis	The aim of this research was to enhance the classification accuracy of an electronic nose (E-nose) in different detecting applications. During the learning process of the E-nose to predict the types of different odors, the prediction accuracy was not quite satisfying because the raw features extracted from sensors' responses were regarded as the input of a classifier without any feature extraction processing. Therefore, in order to obtain more useful information and improve the E-nose's classification accuracy, in this paper, a Weighted Kernels Fisher Discriminant Analysis (WKFDA) combined with Quantum-behaved Particle Swarm Optimization (QPSO), i.e., QWKFDA, was presented to reprocess the original feature matrix. In addition, we have also compared the proposed method with quite a few previously existing ones including Principal Component Analysis (PCA), Locality Preserving Projections (LPP), Fisher Discriminant Analysis (FDA) and Kernels Fisher Discriminant Analysis (KFDA). Experimental results proved that QWKFDA is an effective feature extraction method for E-nose in predicting the types of wound infection and inflammable gases, which shared much higher classification accuracy than those of the contrast methods.	coefficient;feature extraction;gases;kernel (operating system);linear discriminant analysis;locality of reference;odors;particle swarm optimization;principal component analysis;projections and predictions;quantum;rhinorrhea;signal processing;united states food and drug administration;wound infection;sensor (device)	Tailai Wen;Jia Yan;Daoyu Huang;Kun Lu;Changjian Deng;Tanyue Zeng;Song Yu;Zhiyi He	2018		10.3390/s18020388	electronic engineering;feature extraction;electronic nose;multiple kernel learning;principal component analysis;matrix (mathematics);signal processing;engineering;linear discriminant analysis;pattern recognition;particle swarm optimization;artificial intelligence	AI	17.199053664444875	-55.80945104334851	183398
07c4fc48ad7b7d1a417b0bb72d0ae2d4efc5aa83	depthwise separable convolutions for neural machine translation		"""Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency. They have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves new state-of-the-art results. In addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new """"super-separable"""" convolution operation that further reduces the number of parameters and computational cost for obtaining state-of-the-art results."""		Lukasz Kaiser;Aidan N. Gomez;François Chollet	2017	CoRR		artificial intelligence;machine translation;computer science;machine learning;architecture;separable space;theoretical computer science;computation;convolution;contextual image classification;dilation (morphology)	NLP	23.246341137159572	-52.10186923247407	183400
9447cebbe55b646fb0a181f90571bc86f19436ee	np-completeness of the energy barrier problem without pseudoknots and temporary arcs	np completeness;folding pathway;nucleic acids;folding pathways;computational complexity;secondary structure;base pair;energy barrier	Knowledge of energy barriers between pairs of secondary structures for a given DNA or RNA molecule is useful, both in understanding RNA function in biological settings and in design of programmed molecular systems. Current heuristics are not guaranteed to find the exact energy barrier, raising the question whether the energy barrier can be calculated efficiently. In this paper, we study the computational complexity of a simple formulation of the energy barrier problem, in which each base pair contributes an energy of −1 and only base pairs in the initial and final structures may be used on a folding pathway from initial to final structure. We show that this problem is NP-complete.	computational complexity theory;gene regulatory network;heuristic (computer science);karp's 21 np-complete problems;np-completeness	Ján Manuch;Chris Thachuk;Ladislav Stacho;Anne Condon	2010	Natural Computing	10.1007/s11047-010-9239-4	nucleic acid;np-complete;base pair;computer science;bioinformatics;mathematics;computational complexity theory;genetics;algorithm;protein secondary structure	Comp.	13.171484522417655	-61.91395682363803	183511
73b850bdd19e74b862b523af7ea70b66587f06b5	a multiway 3d qsar analysis of a series of (s)-n-[(1-ethyl-2-pyrrolidinyl)methyl]-6-methoxybenzamides	molecular modeling;dopamine d2 receptor;parafac;3d qsar;pharmacophore;regression;agents;antagonists;multiway calibration;word alignment;principal component analysis;multilinear pls;cross validation;models;binding;alignment;leave one out	Recently, the multilinear PLS algorithm was presented by Bro and later implemented as a regression method in 3D QSAR by Nilsson et al. In the present article a well-known set of (S)-N-[(1-ethyl-2-pyrrolidinyl)methyl]-6-methoxybenzamides, with affinity towards the dopamine D2 receptor subtype, was utilised for the validation of the multilinear PLS method. After exhaustive conformational analyses on the ligands, the active analogue approach was employed to align them in their presumed pharmacologically active conformations, using (-)-piquindone as a template. Descriptors were then generated in the GRID program, and 40 calibration compounds and 18 test compounds were selected by means of a principal component analysis in the descriptor space. The final model was validated with different types of cross-validation experiments, e.g. leave-one-out, leave-three-out and leave-five-out. The cross-validated Q2 was 62% for all experiments, confirming the stability of the model. The prediction of the test set with a predicted Q2 of 62% also established the predictive ability. Finally, the conformations and the alignment of the ligands in combination with multilinear PLS, obviously, played an important role for the success of our model.	align (company);alignment;analog;bilinear filtering;bro;clinical use template;cross infection;cross reactions;cross-validation (statistics);dopamine d2 receptor;experiment;interpretation process;jsp model 2 architecture;large;ligands;papillon-lefevre disease;primary lateral sclerosis, adult, 1;principal component analysis;processor affinity;quantitative structure-activity relationship;quantitative structure–activity relationship;test set;weight;algorithm	Jonas Nilsson;Evert J. Homan;Age K. Smilde;Cor J. Grol;Håkan Wikstrüm	1998	Journal of computer-aided molecular design	10.1023/A:1007977010551	dopamine receptor d2;chemistry;regression;pharmacophore;software agent;machine learning;molecular model;cross-validation;statistics;principal component analysis	Comp.	10.744280250515622	-58.070018387175246	183672
86d4415f8fcc051c57463a170e01134fa085042d	phaselink: a deep learning approach to seismic phase association		Seismic phase association is a fundamental task in seismology that pertains to linking together phase detections on different sensors that originate from a common earthquake. It is widely employed to detect earthquakes on permanent and temporary seismic networks, and underlies most seismicity catalogs produced around the world. This task can be challenging because the number of sources is unknown, events frequently overlap in time, or can occur simultaneously in different parts of a network. We present PhaseLink, a framework based on recent advances in deep learning for grid-free earthquake phase association. Our approach learns to link phases together that share a common origin, and is trained entirely on tens of millions of synthetic sequences of Pand S-wave arrival times generated using a simple 1D velocity model. Our approach is simple to implement for any tectonic regime, suitable for real-time processing, and can naturally incorporate errors in arrival time picks. Rather than tuning a set of ad hoc hyperparameters to improve performance, PhaseLink can be improved by simply adding examples of problematic cases to the training dataset. We demonstrate the state-of-the-art performance of PhaseLink on a challenging recent sequence from southern California, and synthesized sequences from Japan designed to test the point at which the method fails. These tests show that PhaseLink can precisely associate Pand S-picks to events that are separated by∼ 12 seconds in origin time. This approach is expected to improve the resolution of seismicity catalogs, add stability to real-time seismic monitoring, and streamline automated processing of large seismic datasets.		Zachary E. Ross;Yisong Yue;Men-Andrin Meier;Egill Hauksson;Thomas H. Heaton	2018	CoRR		geophysics;induced seismicity;hyperparameter;geology;deep learning;tectonics;artificial intelligence;seismology	ML	17.678406547970575	-55.065716962605926	183907
970819e6e5034dcb4ca98f2bdaeb327ddba9a45e	balanced two-stage residual networks for image super-resolution		In this paper, balanced two-stage residual networks (BTSRN) are proposed for single image super-resolution. The deep residual design with constrained depth achieves the optimal balance between the accuracy and the speed for super-resolving images. The experiments show that the balanced two-stage structure, together with our lightweight two-layer PConv residual block design, achieves very promising results when considering both accuracy and speed. We evaluated our models on the New Trends in Image Restoration and Enhancement workshop and challenge on image super-resolution (NTIRE SR 2017). Our final model with only 10 residual blocks ranked among the best ones in terms of not only accuracy (6th among 20 final teams) but also speed (2nd among top 6 teams in terms of accuracy). The source code both for training and evaluation is available in https://github.com/ychfan/sr_ntire2017.	autostereogram;experiment;image restoration;super-resolution imaging	Yuchen Fan;Honghui Shi;Jiahui Yu;Ding Liu;Wei Han;Haichao Yu;Zhangyang Wang;Xinchao Wang;Thomas S. Huang	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2017.154	algorithm;residual;image restoration;artificial intelligence;pattern recognition;source code;artificial neural network;logic gate;data mining;computer science;ranking;block design;image resolution	Vision	20.452876569271325	-56.23938183615462	184177
92d1341800d4715cc7d255cadffc4bd3e1f3e8e5	modfold6: an accurate web server for the global and local quality estimation of 3d protein models	endnotes;pubications	Methods that reliably estimate the likely similarity between the predicted and native structures of proteins have become essential for driving the acceptance and adoption of three-dimensional protein models by life scientists. ModFOLD6 is the latest version of our leading resource for Estimates of Model Accuracy (EMA), which uses a pioneering hybrid quasi-single model approach. The ModFOLD6 server integrates scores from three pure-single model methods and three quasi-single model methods using a neural network to estimate local quality scores. Additionally, the server provides three options for producing global score estimates, depending on the requirements of the user: (i) ModFOLD6_rank, which is optimized for ranking/selection, (ii) ModFOLD6_cor, which is optimized for correlations of predicted and observed scores and (iii) ModFOLD6 global for balanced performance. The ModFOLD6 methods rank among the top few for EMA, according to independent blind testing by the CASP12 assessors. The ModFOLD6 server is also continuously automatically evaluated as part of the CAMEO project, where significant performance gains have been observed compared to our previous server and other publicly available servers. The ModFOLD6 server is freely available at: http://www.reading.ac.uk/bioinf/ModFOLD/.	access network;arnold;artificial neural network;bioinformatics;biological neural networks;biopolymers;casp;casp12 gene;clinical use template;database;dictionaries as topic;dictionary;estimated;hydrogen bonding;jones calculus;mandibular right second molar tooth;nar 2;psipred;pattern recognition;precedence effect;proteomics;pyschological bonding;realms of the haunting;requirement;schmidt decomposition;scientific publication;server (computer);server (computing);web server;web service;workbench	Ali H. A. Maghrabi;Liam James McGuffin	2017		10.1093/nar/gkx332	biology	Web+IR	10.85533264393615	-57.08297474555069	185083
a6dd9035309c16c0643506dedca8ccc74ac9ee58	a benchmark dataset to study the representation of food images		It is well-known that people love food. However, an insane diet can cause problems in the general health of the people. Since health is strictly linked to the diet, advanced computer vision tools to recognize food images (e.g. acquired with mobile/wearable cameras), as well as their properties (e.g., calories), can help the diet monitoring by providing useful information to the experts (e.g., nutritionists) to assess the food intake of patients (e.g., to combat obesity). The food recognition is a challenging task since the food is intrinsically deformable and presents high variability in appearance. Image representation plays a fundamental role. To properly study the peculiarities of the image representation in the food application context, a benchmark dataset is needed. These facts motivate the work presented in this paper. In this work we introduce the UNICT-FD889 dataset. It is the first food image dataset composed by over 800 distinct plates of food which can be used as benchmark to design and compare representation models of food images. We exploit the UNICT-FD889 dataset for Near Duplicate Image Retrieval (NDIR) purposes by comparing three standard state-of-the-art image descriptors: Bag of Textons, PRICoLBP and SIFT. Results confirm that both textures and colors are fundamental properties in food representation. Moreover the experiments point out that the Bag of Textons representation obtained considering the color domain is more accurate than the other two approaches for NDIR.	assistive technology;benchmark (computing);color;computer vision;context (computing);experiment;image retrieval;parabolic antenna;performance;scale-invariant feature transform;spatial variability;visual descriptor;wearable computer	Giovanni Maria Farinella;Dario Allegra;Filippo Stanco	2014		10.1007/978-3-319-16199-0_41	computer vision;data science;data mining	Vision	24.399296037369865	-59.49598676163393	185314
036260704a49d0a2f2fda1564040ec3d0ababde6	deep learning in intrusion detection perspective: overview and further challenges		Deep learning techniques are famous due to Its capability to cope with large-scale data these days. They have been investigated within various of applications e.g., language, graphical modeling, speech, audio, image recognition, video, natural language and signal processing areas. In addition, extensive researches applying machine-learning methods in Intrusion Detection System (IDS) have been done in both academia and industry. However, huge data and difficulties to obtain data instances are hot challenges to machine-learning-based IDS. We show some limitations of previous IDSs which uses classic machine learners and introduce feature learning including feature construction, extraction and selection to overcome the challenges. We discuss some distinguished deep learning techniques and its application for IDS purposes. Future research directions using deep learning techniques for IDS purposes are briefly summarized.	computer vision;deep learning;feature learning;feature vector;intrusion detection system;machine learning;natural language;signal processing	Kwangjo Kim;Muhamad Erza Aminanto	2017	2017 International Workshop on Big Data and Information Security (IWBIS)	10.1109/IWBIS.2017.8275095	feature extraction;natural language;unsupervised learning;signal processing;deep learning;intrusion detection system;machine learning;computer science;feature learning;artificial intelligence	Security	21.719986912925112	-58.00741159514022	185461
0fdd746889f7701b6205feb08669671ed50cb6bc	are automated molecular dynamics simulations and binding free energy calculations realistic tools in lead optimization? an evaluation of the linear interaction energy (lie) method	farmaceutisk vetenskap;molecular dynamic simulation;article letter to editor;pharmaceutical sciences;binding free energy	An extensive evaluation of the linear interaction energy (LIE) method for the prediction of binding affinity of docked compounds has been performed, with an emphasis on its applicability in lead optimization. An automated setup is presented, which allows for the use of the method in an industrial setting. Calculations are performed for four realistic examples, retinoic acid receptor gamma, matrix metalloprotease 3, estrogen receptor alpha, and dihydrofolate reductase, focusing on different aspects of the procedure. The obtained LIE models are evaluated in terms of the root-mean-square (RMS) errors from experimental binding free energies and the ability to rank compounds appropriately. The results are compared to the best empirical scoring function, selected from a set of 10 scoring functions. In all cases, good LIE models can be obtained in terms of free-energy RMS errors, although reasonable ranking of the ligands of dihydrofolate reductase proves difficult for both the LIE method and scoring functions. For the other proteins, the LIE model results in better predictions than the best performing scoring function. These results indicate that the LIE approach, as a tool to evaluate docking results, can be a valuable asset in computational lead optimization programs.		Eva Stjernschantz;John Marelius;Carmen Medina;Micael Jacobsson;Nico P. E. Vermeulen;Chris Oostenbrink	2006	Journal of chemical information and modeling	10.1021/ci0601214	pharmacology;simulation;chemistry;computer science;bioinformatics;pharmaceutical sciences;computational chemistry;mathematics;algorithm;statistics	Comp.	12.382616207692523	-59.45343048861059	185527
6bcfb27ecc412971044683c9b17bc0ba03101b25	turbo similarity searching: effect of fingerprint and dataset on virtual-screening performance	chemical database;similar property principle;virtual screening;similarity searching;similarity search;chemoinformatics;turbo similarity searching	Turbo similarity searching uses information about the nearest neighbours in a conventional chemical similarity search to increase the effectiveness of virtual screening, with a data fusion approach being used to combine the nearest-neighbour information. A previous paper suggested that the approach was highly effective in operation; this paper further tests the approach using a range of different databases and of structural representations. Searches were carried out on three different databases of chemical structures, using seven different types of fingerprint, as well as molecular holograms, physicochemical properties, topological indices and reduced graphs. The results show that turbo similarity searching can indeed enhance retrieval but that this is normally achieved only if the similarity search that acts as its starting point has already achieved at least some reasonable level of search effectiveness. In other cases, a modified version of TSS that uses the nearest-neighbour information for approximate machine learning can be used effectively. Whilst useful for qualitative (active/inactive) predictions of biological activity, turbo similarity searching does not appear to exhibit any predictive power when quantitative property data is available.	approximation algorithm;chemical similarity;database;fingerprint;machine learning;similarity search;topological index;virtual screening;while	Eleanor J. Gardiner;Valerie J. Gillet;Maciej Haranczyk;Jérôme Hert;John D. Holliday;Nurul Malim;Yogendra Patel;Peter Willett	2009	Statistical Analysis and Data Mining	10.1002/sam.10037	virtual screening;cheminformatics;machine learning;data mining;mathematics;chemical database;information retrieval	Web+IR	11.24279272022606	-57.83608056215126	185905
9f7ccc74fa05a768d711e4fbad7f68b7f697de6c	database supported candidate search for metabolite identification		Mass spectrometry is an important analytical technology for the identification of metabolites and small compounds by their exact mass. But dozens or hundreds of different compounds may have a similar mass or even the same molecule formula. Further elucidation requires tandem mass spectrometry, which provides the masses of compound fragments, but in silico fragmentation programs require substantial computational resources if applied to large numbers of candidate structures. We present and evaluate an approach to obtain candidates from a relational database which contains 28 million compounds from PubChem. A training phase associates tandem-MS peaks with corresponding fragment structures. For the candidate search, the peaks in a query spectrum are translated to fragment structures, and the candidates are retrieved and sorted by the number of matching fragment structures. In the cross validation the evaluation of the relative ranking positions (RRP) using different sizes of training sets confirms that a larger coverage of training data improves the average RRP from 0.65 to 0.72. Our approach allows downstream algorithms to process candidates in order of importance.	algorithm;computation;computational resource;downstream (software development);fragmentation (computing);large;matching;pubchem;relational database;tandem mass spectrometry;triangulation	Christian Hildebrandt;Sebastian Wolf;Steffen Neumann	2011	Journal of integrative bioinformatics	10.2390/biecoll-jib-2011-157	bioinformatics;data mining	Comp.	10.623064850272888	-57.996686107376156	186431
719470b16976f8c7376458d4b097f264b87faa79	decoding semantics categorization during natural viewing of video streams	decoding;semantics decoding brain modeling streaming media multimedia communication;semantics;accuracy;language system semantics categorization decoding natural video stream viewing human brain functional mechanism semantics oriented multimedia analysis functional brain imaging brain imaging data acquisition functional brain response modeling brain decoding study sparse multinomial logistic regression algorithm smlr algorithm brain region exploration functional interaction naturalistic video stream functional magnetic resonance imaging fmri semantics perception large scale brain network intrasubject brain decoding model intersubject brain decoding model working memory system emotion attention vision system;brain modeling;streaming media;multimedia communication;visual perception biomedical mri brain data acquisition decoding medical image processing multimedia systems video streaming;semantics categorization brain decoding functional magnetic resonance imaging fmri naturalistic stimuli	Exploring the functional mechanism of the human brain during semantics categorization and subsequently leverage current semantics-oriented multimedia analysis by functional brain imaging have been receiving great attention in recent years. In the field, most of existing studies utilized strictly controlled laboratory paradigms as experimental settings in brain imaging data acquisition. They also face the critical problem of modeling functional brain response from acquired brain imaging data. In this paper, we present a brain decoding study based on sparse multinomial logistic regression (SMLR) algorithm to explore the brain regions and functional interactions during semantics categorization. The setups of our study are two folds. First, we use naturalistic video streams as stimuli in functional magnetic resonance imaging (fMRI) to simulate the complex environment for semantics perception that the human brain has to process in real life. Second, we model brain responses to semantics categorization as functional interactions among large-scale brain networks. Our experimental results show that semantics categorization can be accurately predicted by both intrasubject and intersubject brain decoding models. The brain responses identified by the decoding model reveal that a wide range of brain regions and functional interactions are recruited during semantics categorization. Especially, the working memory system exhibits significant contributions. Other substantially involved brain systems include emotion, attention, vision and language systems.	algorithm;categorization;data acquisition;interaction;large scale brain networks;multinomial logistic regression;real life;resonance;simulation;sparse matrix;streaming media	Xintao Hu;Lei Guo;Junwei Han;Tianming Liu	2015	IEEE Transactions on Autonomous Mental Development	10.1109/TAMD.2015.2415413	computer vision;speech recognition;computer science;semantics;accuracy and precision;communication	Comp.	23.094928383872357	-59.441043273224764	186812
de51b21ba3cc92507d4c815e31face5ee29c6783	padme: a deep learning-based framework for drug-target interaction prediction		In silico drug-target interaction (DTI) prediction is an important and challenging problem in biomedical research with a huge potential benefit to the pharmaceutical industry and patients. Most existing methods for DTI prediction including deep learning models generally have binary endpoints, which could be an oversimplification of the problem, and those methods are typically unable to handle cold-target problems, i.e., problems involving target protein that never appeared in the training set. Towards this, we contrived PADME (Protein And Drug Molecule interaction prEdiction), a framework based on Deep Neural Networks, to predict real-valued interaction strength between compounds and proteins. PADME takes both compound and protein information as inputs, so it is capable of solving cold-target (and cold-drug) problems. To our knowledge, we are the first to combine Molecular Graph Convolution (MGC) for compound featurization with protein descriptors for DTI prediction. We used multiple cross-validation split schemes and evaluation metrics to measure the performance of PADME on multiple datasets, including the ToxCast dataset, which we believe should be a standard benchmark for DTI problems, and PADME consistently dominates baseline methods. The results of a case study, which predicts the interactions between various compounds and androgen receptor (AR), suggest PADME’s potential in drug development. The scalability of PADME is another advantage in the age of Big Data.	artificial neural network;baseline (configuration management);benchmark (computing);big data;binary classification;cold start;convolution;cross-validation (statistics);data pre-processing;deep learning;experiment;interaction;molecular graph;nuclear receptor signaling atlas;preprocessor;scalability;test set	Qingyuan Feng;Evgenia V. Dueva;Artem Cherkasov;Martin Ester	2018	CoRR		artificial intelligence;mathematics;cheminformatics;machine learning;artificial neural network;deep learning;training set;target protein	Comp.	11.462836740849673	-54.8974217148964	186936
32f215640451bb0a06ac97e04c8f89f3dbeb5e15	time series classification with hive-cote: the hierarchical vote collective of transformation-based ensembles		A recent experimental evaluation assessed 19 time series classification (TSC) algorithms and found that one was significantly more accurate than all others: the Flat Collective of Transformation-based Ensembles (Flat-COTE). Flat-COTE is an ensemble that combines 35 classifiers over four data representations. However, while comprehensive, the evaluation did not consider deep learning approaches. Convolutional neural networks (CNN) have seen a surge in popularity and are now state of the art in many fields and raises the question of whether CNNs could be equally transformative for TSC.  We implement a benchmark CNN for TSC using a common structure and use results from a TSC-specific CNN from the literature. We compare both to Flat-COTE and find that the collective is significantly more accurate than both CNNs. These results are impressive, but Flat-COTE is not without deficiencies. We significantly improve the collective by proposing a new hierarchical structure with probabilistic voting, defining and including two novel ensemble classifiers built in existing feature spaces, and adding further modules to represent two additional transformation domains. The resulting classifier, the Hierarchical Vote Collective of Transformation-based Ensembles (HIVE-COTE), encapsulates classifiers built on five data representations. We demonstrate that HIVE-COTE is significantly more accurate than Flat-COTE (and all other TSC algorithms that we are aware of) over 100 resamples of 85 TSC problems and is the new state of the art for TSC. Further analysis is included through the introduction and evaluation of 3 new case studies and extensive experimentation on 1,000 simulated datasets of 5 different types.		Jason Lines;Sarah Taylor;Anthony J. Bagnall	2018	TKDD	10.1145/3182382	convolutional neural network;machine learning;artificial intelligence;deep learning;probabilistic logic;computer science;voting;popularity	Web+IR	22.526564106849904	-52.395238807790435	187007
c5c6b27eaaa506486370036657ab3209b741a168	necessity of high-resolution for coarse-grained modeling of flexible proteins	membrane insertion;hairpin;protein folding	The popular MARTINI coarse-grained (CG) force field requires the protein structure to be fixed, and is unsuitable for simulating dynamic processes such as protein folding. Here, we examine the feasibility of developing a flexible protein model within the MARTINI framework. The results demonstrate that the MARTINI CG scheme does not properly describe the volume and packing of protein backbone and side chains and leads to excessive collapse without structural restraints in explicit CG water. Combining atomistic protein representation with the MARTINI CG solvent, such as in the PACE model, dramatically improves description of flexible protein conformations. Yet, the CG solvent is insufficient to capture the conformational dependence of protein-solvent interactions, and PACE is unable to properly model context dependent conformational transitions. Taken together, high physical resolution at or near the atomistic level is likely necessary for flexible protein models with explicit, microscopic solvents, and the coarse-graining needs to focus on possible simplification in interaction potentials. © 2016 Wiley Periodicals, Inc.	coarse tremor;force field (chemistry);image resolution;interaction;internet backbone;john d. wiley;level of detail;mesalina martini;molecular dynamics;set packing;simulation;vertebral column;protein folding	Zhiguang Jia;Jianhan Chen	2016	Journal of computational chemistry	10.1002/jcc.24391	protein folding;biochemistry;molecular biology;chemistry	Comp.	12.032943894119281	-61.84664631039496	187055
35346d895bd945c3299a9472f291998b26de794b	modeling and optimization of the specificity in cell signaling pathways based on a high performance multi-objective evolutionary algorithm	diaphonie;especificidad;modelizacion;multiobjective programming;valor elevado;optimum pareto;programmation multiobjectif;salida;optimisation;energie libre;haute performance;mitogen activated protein kinase;proteine;optimizacion;crosstalk;model system;cascade;stress response;termodinamica;dissociation constant;desarrolo cognitivo;biologie cellulaire;multi objective evolutionary algorithm;intelligence artificielle;valeur elevee;fuite;algoritmo genetico;biologia celular;cognitive development;modelisation;transferencia conocimiento;diafonia;transfert des connaissances;developpement cognitif;cascada;leak;developmental biology;algorithme genetique;thermodynamique;alto rendimiento;knowledge transfer;thermodynamics;high value;artificial intelligence;algorithme evolutionniste;genetic algorithm;algoritmo evolucionista;optimization;proteina;signaling pathway;energia libre;inteligencia artificial;specificity;evolutionary algorithm;specificite;protein;pareto optimum;modeling;high performance;optimo pareto;free energy;pareto optimality;cell signaling;cell biology;programacion multiobjetivo	A central question in cell and developmental biology is how signaling pathways maintain specificity and avoid erroneous cross-talk so that distinct signals produce the appropriate changes. In this paper, a model system of the yeast mating, invasive growth and stress-responsive mitogen activated protein kinase (MAPK) cascades for scaffolding-mediated is developed. Optimization with respect to the mutual specificity of this model system is performed by a high performance multi-objective evolutionary algorithm (HPMOEA) based on the principles of the minimal free energy in thermodynamics. The results are good agreement with published experimental data. (1) Scaffold proteins can enhance specificity in cell signaling when different pathways share common components; (2) The mutual specificity could be accomplished by a selectively-activated scaffold that had a relatively high value of dissociation constant and reasonably small values of leakage rates; (3) When Pareto-optimal mutual specificity is achieved, the coefficients, deactivation rates reach fastest, association and leakage rates reach slowest.	cell signaling;evolutionary algorithm;sensitivity and specificity	Xiufen Zou;Yu Chen;Zishu Pan	2006		10.1007/11903697_97	fight-or-flight response;dissociation constant;simulation;crosstalk;systems modeling;genetic algorithm;cell signaling;computer science;artificial intelligence;evolutionary algorithm;developmental biology;mitogen-activated protein kinase;cascade;cognitive development;operations research;signal transduction	HPC	10.891352751961419	-61.90864218629589	187102
23a5b08d08305b03457a4a45865192c46f1ea79a	saccadic object recognition with an active vision system	integrated approach;object recognition;feature detection;high resolution;image understanding;associative memory;sparse coding;scene analysis;active vision	G.{J. Gie ng H. Jan en H. Mallot Institut f ur Neuroinformatik, Ruhr{Universit at Bochum 44780 Bochum, Germany e{mail heja@neuroinformatik.ruhr-uni-bochum.de Abstract We propose an active vision system for saccadic camera gaze shifts and explorative scene analysis as a new integral approach to image understanding. The model consists of two sensory subsystems: preattentive peripheral feature detection and high resolution foveal image identi cation based on a hypercolumnar representation. Visual objects are non{explicitly stored in two sparsely coded associative memories separating xation locations from identities of foveal views. An egocentric interest map integrates bottom{up and top{ down information sources and decides when to generate a camera movement. A selective masking of preattentive processes supports a cooperation with cognitive object recognition. The system is easily extendible, copes with occlusions and distortions and can be driven in di erent modes for exploration tasks. This model is able to perform visual search and reproduce ndings in the human visual system. 1 System Survey In this paper we propose an animate image understanding system for static two{dimensional scene analysis and behavioural object recognition by means of explicit camera gaze shifts. The function of the system is designed by a hierarchy of physiological and anatomical motivated structures (i.e., fovea, hypercolumns, associative memory). The scene is supposed to be a mainly static external memory bu er. An implicit selective attention mechanism which is considered as a behavioural context or system status modulates perception by controlling information ow and distributing processing resources to scene locations. Edited version, originally published in: Proceedings of the International Conference on Pattern Recognition (ICPR), pp. 664 { 667, IEEE (1992) ySupported by the German Federal Department of Research and Technology (BMFT), Grant No. ITR8800K4 Scene	active vision;computer vision;content-addressable memory;distortion;extensibility;feature detection (computer vision);feature detection (web development);hitachi hd44780 lcd controller;image resolution;outline of object recognition;pattern recognition;peripheral;visual objects	Gerd-Jürgen Giefing;H. Janßen;Hanspeter A. Mallot	1992			computer vision;image resolution;active vision;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;feature detection;3d single-object recognition;neural coding	Vision	21.902860459774494	-65.86089600340125	187160
1a85dad79d68a986d474b18c7479fe89e8210048	exploring non-linear distance metrics in the structure-activity space: qsar models for human estrogen receptor	chemical space;distance metrics;human estrogen receptor;molecular similarity;qsar models;structure–activity landscape	BACKGROUND Quantitative structure-activity relationship (QSAR) models are important tools used in discovering new drug candidates and identifying potentially harmful environmental chemicals. These models often face two fundamental challenges: limited amount of available biological activity data and noise or uncertainty in the activity data themselves. To address these challenges, we introduce and explore a QSAR model based on custom distance metrics in the structure-activity space.   METHODS The model is built on top of the k-nearest neighbor model, incorporating non-linearity not only in the chemical structure space, but also in the biological activity space. The model is tuned and evaluated using activity data for human estrogen receptor from the US EPA ToxCast and Tox21 databases.   RESULTS The model closely trails the CERAPP consensus model (built on top of 48 individual human estrogen receptor activity models) in agonist activity predictions and consistently outperforms the CERAPP consensus model in antagonist activity predictions.   DISCUSSION We suggest that incorporating non-linear distance metrics may significantly improve QSAR model performance when the available biological activity data are limited.	chemical structure;database;estrogens;image noise;k-nearest neighbors algorithm;nonlinear system;quantitative structure-activity relationship;single linkage cluster analysis;estrogen receptor alpha, human	Ilya A. Balabin;Richard S. Judson	2018	Journal of cheminformatics	10.1186/s13321-018-0300-0	data mining;chemical space;estrogen receptor;computer science;quantitative structure–activity relationship	ML	10.523035456011366	-57.64192928370376	187187
d0ae2992bc74d5edcf035d7ec4ec19e7a75d7c89	desirability-based multiobjective optimization for global qsar studies: application to the design of novel nsaids with improved analgesic, antiinflammatory, and ulcerogenic profiles	overall desirability function;nsaids;drug discovery;ulcerogenic index;global qsar;multiobjective optimization;chemoinformatics	Up to now, very few reports have been published concerning the application of multiobjective optimization (MOOP) techniques to quantitative structure-activity relationship (QSAR) studies. However, none reports the optimization of objectives related directly to the desired pharmaceutical profile of the drug. In this work, for the first time, it is proposed a MOOP method based on Derringer's desirability function that allows conducting global QSAR studies considering simultaneously the pharmacological, pharmacokinetic and toxicological profile of a set of molecule candidates. The usefulness of the method is demonstrated by applying it to the simultaneous optimization of the analgesic, antiinflammatory, and ulcerogenic properties of a library of fifteen 3-(3-methylphenyl)-2-substituted amino-3H-quinazolin-4-one compounds. The levels of the predictor variables producing concurrently the best possible compromise between these properties is found and used to design a set of new optimized drug candidates. Our results also suggest the relevant role of the bulkiness of alkyl substituents on the C-2 position of the quinazoline ring over the ulcerogenic properties for this family of compounds. Finally, and most importantly, the desirability-based MOOP method proposed is a valuable tool and shall aid in the future rational design of novel successful drugs.		Maykel Cruz-Monteagudo;Fernanda Borges;M Natália Dias Soeiro Cordeiro	2008	Journal of computational chemistry	10.1002/jcc.20994	pharmacology;stereochemistry;chemistry;toxicology;cheminformatics;multi-objective optimization;drug discovery	SE	10.650208078550337	-60.1740918303271	187557
ef67102ca1908c685dca746fc1e61cfd4e83cc6b	introduction of noe data to an automated structure elucidation system, chemics. three-dimensional structure elucidation using the distance geometry method	structure elucidation;distance geometry;three dimensional structure	An automated structure elucidation system for organic compounds, CHEMICS has a function in which it generates chemical structures consistent with input data (molecular formula, NMR spectra etc.) in the form of a two-dimensional structure. Recently, with the development of a measurement technique of 2D-NMR, it makes clear the assignment relation between NMR signals and carbons and hydrogens which constitute the two-dimensional structure. This clear assignment relation made it possible not only to drastically reduce the number of the candidate structures generated by CHEMICS but also to utilize NOE data for building up three-dimensional candidates from two-dimensional ones. Thus, it became possible to know a rough conformation of the candidate structures in sblution.		Kimito Funatsu;Minoru Nishizaki;Shin-ichi Sasaki	1994	Journal of Chemical Information and Computer Sciences	10.1021/ci00020a008	chemistry;organic chemistry;chemical structure;distance geometry	Comp.	13.287736082170518	-59.50550351984152	188027
e0a0d809d4d9324f0ad471c76c0aff54f0acc7e3	neurox: a toolkit for analyzing individual neurons in neural networks		We present a toolkit to facilitate the interpretation and understanding of neural network models. The toolkit provides several methods to identify salient neurons with respect to the model itself or an external task. A user can visualize selected neurons, ablate them to measure their effect on the model accuracy, and manipulate them to control the behavior of the model at the test time. Such an analysis has a potential to serve as a springboard in various research directions, such as understanding the model, better architectural choices, model distillation and controlling data biases.		Fahim Dalvi;Avery Nortonsmith;David Anthony Bau;Yonatan Belinkov;Hassan Sajjad;Nadir Durrani;James Glass	2018	CoRR			NLP	20.84153058047283	-54.06737663226511	188384
4656c54d3910c2cee9dae32c2ec02a45bfe5ba78	research of sludge compost maturity degree modeling method based on fuzzy neural network for sewage treatment	fuzzy neural network;l-m algorithm;maturity degree;modeling;sludge compost;sewage treatment	Chemically stable dithranol compositions useful as topical treatment for inflammatory disorders such as psoriasis, eczema and seborrheic dermatitis are disclosed. Incorporation of a certain alpha hydroxyacid as its free acid, lactone, amide or salt form in dithranol containing compositions has been found to chemically stabilize said compositions. The alpha hydroxyacids include glyceric acid, gluconic acid, galacturonic acid, glucuronic acid, glucoheptonic acid, galactonic acid, malic acid, mucic acid, citric acid, saccharic acid, tartaric acid, tartronic acid, isocitric acid and glucuronamide. A single member of the above alpha hydroxyacids may be present in a total amount of from 0.01 to 1 percent by weight of the total composition, or a plurality thereof may be present in a preferred concentration range of from 0.02 to 0.5 percent by weight of the total composition.	artificial neural network;capability maturity model	Meijuan Gao;Jingwen Tian;Yujuan Xiang	2007			malic acid;mathematical optimization;citric acid;saccharic acid;mucic acid;organic chemistry;glucuronic acid;computer science;glyceric acid;tartronic acid;isocitric acid	ML	10.329514960885165	-64.48531800128474	189135
1241b2f9f5bccd3daeee3302747a9a1583a2b225	better deep visual attention with reinforcement learning in action recognition	action recognition;deep reinforcement learning;visual attention	Deep visual attention in computer vision has attracted much attention over the past years, which achieves great contributions especially in image classification, image caption and action recognition. However, due to taking BP training wholly or partially, they can not show the true power of attention in computational efficiency and focusing accuracy. Our intuition is that attention mechanism should be similar to the process in which human draw attention and select the next location to focus, by observing, analyzing and jumping instead of existing describing continuous features. Based on this insight, we formulate our model as a recurrent neural network-based agent that chooses attention region by reinforcement learning at each timestep. In experiments, our model explicitly outperforms baselines not only in focusing and recognizing accuracy, but also consumes much less computational resources, which can be honored as better deep visual attention.	action potential;analysis of algorithms;artificial neural network;baseline (configuration management);bayesian approaches to brain function;big data;computation;computational resource;computer vision;experiment;recurrent neural network;reinforcement learning	Gang Wang;Wenmin Wang;Jingzhuo Wang;Yaohua Bu	2017	2017 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2017.8050638	visualization;feature extraction;computer science;machine learning;recurrent neural network;contextual image classification;reinforcement learning;artificial intelligence;intuition	Vision	23.191522859743312	-54.590480117134106	189349
1579253a65b5b25d510ca72d4598ea7443884d4f	homology modeling, binding site identification and docking in flavone hydroxylase cyp105p2 in streptomyces peucetius atcc 27952	ligand binding;binding site;molecular dynamics simulation;molecular dynamic simulation;cytochrome p450;model building;structure and function;secondary structure;flavone;molecular dynamic;homology modeling;ligand binding site;molecular docking;active site	Homology models of cytochrome P450 105P2 (CYP105P2) were constructed using four P450 structures, CYP105A1, CYP105, CYP165B3 and CYP107L1, as templates for the model building. Using Accelrys Discovery Studio 2.1 software, the lowest energy CYP105P2 model was then assessed for stereochemical quality and side-chain environment. Further active site optimization of the CYP105P2 model built using these templates was performed by molecular dynamics to generate the final CYP105P2 model. The substrates, flavone, flavanone, quercetin and naringenin, were docked into the model. The model-flavone complex was used to validate the active site architecture, and structurally and functionally important residues were identified by subsequent characterization of the secondary structure.		Bashistha Kumar Kanth;Kwangkyoung Liou;Jae Kyung Sohng	2010	Computational biology and chemistry	10.1016/j.compbiolchem.2010.08.002	biology;biochemistry;stereochemistry;molecular dynamics;homology modeling;model building;chemistry;docking;cytochrome p450;bioinformatics;binding site;active site;ligand;protein secondary structure	Comp.	11.050964681399908	-59.55213202247082	190315
aedd7e42d2ded80f400c5ee9f8ea3f24d89f6816	sparse-matrix belief propagation		We propose sparse-matrix belief propagation, which executes loopy belief propagation in pairwise Markov random fields by replacing indexing over graph neighborhoods with sparsematrix operations. This abstraction allows for seamless integration with optimized sparse linear algebra libraries, including those that perform matrix and tensor operations on modern hardware such as graphical processing units (GPUs). The sparse-matrix abstraction allows the implementation of belief propagation in a high-level language (e.g., Python) that is also able to leverage the power of GPU parallelization. We demonstrate sparse-matrix belief propagation by implementing it in a modern deep learning framework (PyTorch), measuring the resulting massive improvement in running time, and facilitating future integration into deep learning models.		R. Patrick Bixler	2018			computer science;random field;machine learning;belief propagation;search engine indexing;sparse matrix;deep learning;matrix (mathematics);python (programming language);markov chain;artificial intelligence	ML	22.549898999128818	-52.222520556104065	190434
a66ed3ad36fa80b6be8d3e7f9e9782f36b0c40fd	chemical graph transformation with stereo-information		Double Pushout graph transformation naturally facilitates the modelling of chemical reactions: labelled undirected graphs model molecules and direct derivations model chemical reactions. However, the most straightforward modelling approach ignores the relative placement of atoms and their neighbours in space. Stereoisomers of chemical compounds thus cannot be distinguished, even though their chemical activity may differ substantially. In this contribution we propose an extended chemical graph transformation system with attributes that encode information about local geometry. The modelling approach is based on the so-called “ordered list method”, where an order is imposed on the set of incident edges of each vertex, and permutation groups determine equivalence classes of orderings that correspond to the same local spatial embedding. This method has previously been used in the context of graph transformation, but we here propose a framework that also allows for partially specified stereoinformation. While there are several stereochemical configurations to be considered, we focus here on the tetrahedral molecular shape, and suggest general principles for how to treat all other chemically relevant local geometries. We illustrate our framework using several chemical examples, including the enumeration of stereoisomers of carbohydrates and the stereospecific reaction for the aconitase enzyme in the citirc acid cycle.	acid;encode;graph (discrete mathematics);graph rewriting;molecular graph;turing completeness	Jakob L. Andersen;Christoph Flamm;Daniel Merkle;Peter F. Stadler	2017		10.1007/978-3-319-61470-0_4	computer science;discrete mathematics;combinatorics;pushout;vertex (geometry);enumeration;equivalence class;molecule;graph rewriting;embedding;tetrahedron	Graphics	14.040059773062103	-59.331058406209685	190460
e17e38efc2f21b6cf1ce3b424a3a672fe33b5d37	adaptive partitioning spline neural networks: template matching, memorization, inhibitor connections, inversion, semi-sup, topology search		Deep Neural Networks (DNNs) are universal function approximators providing state-ofthe-art solutions on wide range of applications. Common perceptual tasks such as speech recognition, image classification, and object tracking are now commonly tackled via DNNs. Some fundamental problems remain: (1) the lack of a mathematical framework providing an explicit and interpretable input-output formula for any topology, (2) quantification of DNNs stability regarding adversarial examples (i.e. modified inputs fooling DNN predictions whilst undetectable to humans), (3) absence of generalization guarantees and controllable behaviors for ambiguous patterns, (4) leverage unlabeled data to apply DNNs to domains where expert labeling is scarce as in the medical field. Answering those points would provide theoretical perspectives for further developments based on a common ground. Furthermore, DNNs are now deployed in tremendous societal applications, pushing the need to fill this theoretical gap to ensure control, reliability, and interpretability. 1 ar X iv :1 71 0. 09 30 2v 3 [ st at .M L ] 6 N ov 2 01 7	artificial neural network;computer vision;neural networks;neural network software;speech recognition;spline (mathematics);template matching;utm theorem;while	Randall Balestriero;Richard G. Baraniuk	2017	CoRR			ML	20.589879578041398	-52.43390361570668	190497
bdf961e8b0a893525010757909e8916f9bb27e0b	unsupervised mouse behavior analysis: a data-driven study of mice interactions	mice;supervised learning;input variables;rodents;feature extraction;nose;data models	Automatic analysis of rodent behavior has been receiving growing attention in recent years since rodents have been the reference species for many neuroscientific studies, with the social interaction being among the subjects of the most important ones. Systems that are employed in these studies are mainly based on tracking of mice and activity classification through supervised learning methods, trained on datasets manually annotated by experts. In this paper, we introduce a completely unsupervised way of analysing tracking data for the automatic identification of social and non-social behaviors using models capable of spotting regularities in the data. In particular, a mean-covariance Restricted Boltzmann Machine is employed to abstract higher-level behavioral configurations of mice interacting in an arena for a long time.	automatic identification and data capture;computer mouse;experiment;interaction;latent variable;restricted boltzmann machine;statistical classification;supervised learning;unsupervised learning	Vasiliki-Maria Katsageorgiou;Matteo Zanotto;Huiping Huang;Valentina Ferretti;Francesco Papaleo;Diego Sona;Vittorio Murino	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7899754	unsupervised learning;data modeling;feature extraction;computer science;artificial intelligence;machine learning;pattern recognition;supervised learning	Robotics	20.190375271942745	-63.092937394406334	190574
1c8b7870fdd4e271a2f08618193a22d0bd398a6a	absolute net charge and the biological activity of oligopeptides	structural model;platelet activation;computer programs;biological activity;computers in chemistry;sequence analysis;three dimensional structure;3d structure	Sequences of human proteins are frequently prepared as synthetic oligopeptides to assess their functional ability to act as compounds modulating pathways involving the parent protein. Our objective was to analyze a set of oligopeptides, to determine if their solubility or activity correlated with features of their primary sequence, or with features of properties inferred from three-dimensional structural models derived by conformational searches. We generated a conformational database for a set of 78 oligopeptides, derived from human proteins, and correlated their 3D structures with solubility and biological assay activity (as measured by platelet activation and inhibition). Parameters of these conformers (frequency of coil, frequency of turns, the degree of packing, and the energy) did not correlate with solubility, which was instead partly predicted by two measures obtained from primary sequence analysis, that is, the hydrophobic moment and the number of charges. The platelet activity of peptides was correlated with a parameter derived from the structural modeling; this was the second virial coefficient (a measure of the tendency for a structure to autoaggregate). This could be explained by an excess among the active peptides of those which had either a large number of positive charges or in some cases a large number of negative charges, with a corresponding deficit of peptides with a mixture of negative and positive charges. We subsequently determined that a panel of 523 commercially available (and biologically active) peptides shared this elevation of absolute net charge: there were significantly lower frequencies of peptides of mixed charges compared to expectations. We conclude that the design of biologically active peptides should consider favoring those with a higher absolute net charge.	biological assay;blood platelets;class diagram;coefficient;coil device component;ephrin type-b receptor 1, human;inference;offset binary;oligopeptides;platelet activation;population parameter;sequence analysis;set packing;synthetic data	Laavanya Parthasarathi;Marc Devocelle;Chresten R. Søndergaard;Ivan Baran;Colm O'Dushlaine;Norman E. Davey;Richard J. Edwards;Niamh Moran;Dermot Kenny;Denis C. Shields	2006	Journal of chemical information and modeling	10.1021/ci0600760	crystallography;biochemistry;stereochemistry;chemistry;bioinformatics;organic chemistry;sequence analysis;biological activity	Comp.	11.538735420404366	-58.60115804342557	190747
86a71881f15de18ad75b539bee43dd2b0b563fcf	improving protein-ligand binding site prediction accuracy by classification of inner pocket points using local features	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;computer applications in chemistry;theoretical and computational chemistry;computational biology bioinformatics;uk phd theses thesis;life sciences;uk research reports;medical journals;europe pmc;documentation and information in chemistry;biomedical research;bioinformatics	BACKGROUND Protein-ligand binding site prediction from a 3D protein structure plays a pivotal role in rational drug design and can be helpful in drug side-effects prediction or elucidation of protein function. Embedded within the binding site detection problem is the problem of pocket ranking - how to score and sort candidate pockets so that the best scored predictions correspond to true ligand binding sites. Although there exist multiple pocket detection algorithms, they mostly employ a fairly simple ranking function leading to sub-optimal prediction results.   RESULTS We have developed a new pocket scoring approach (named PRANK) that prioritizes putative pockets according to their probability to bind a ligand. The method first carefully selects pocket points and labels them by physico-chemical characteristics of their local neighborhood. Random Forests classifier is subsequently applied to assign a ligandability score to each of the selected pocket point. The ligandability scores are finally merged into the resulting pocket score to be used for prioritization of the putative pockets. With the used of multiple datasets the experimental results demonstrate that the application of our method as a post-processing step greatly increases the quality of the prediction of Fpocket and ConCavity, two state of the art protein-ligand binding site prediction algorithms.   CONCLUSIONS The positive experimental results show that our method can be used to improve the success rate, validity and applicability of existing protein-ligand binding site prediction tools. The method was implemented as a stand-alone program that currently contains support for Fpocket and Concavity out of the box, but is easily extendible to support other tools. PRANK is made freely available at http://siret.ms.mff.cuni.cz/prank.	binding sites;concave function;drug design;embedding;existential quantification;extensibility;ligand binding domain;ligands;merge;name;out of the box (feature);random forest;ranking (information retrieval);score;thinking outside the box;video post-processing;algorithm	Radoslav Krivák;David Hoksza	2015		10.1186/s13321-015-0059-5	biology;medical research;medicine;computer science;bioinformatics;data science;data mining	ML	10.534898275194791	-57.60935202047322	191025
944a41ec310fe1ca1bd2a0bb5195fa7c1e4dad46	real-time object recognition based on cortical multi-scale keypoints		In recent years, a large number of impressive object categorisation algorithms have surfaced, both computational and biologically motivated. While results on standardised benchmarks are impressive, very few of the best-performing algorithms took run-time performance into account, rendering most of them useless for real-time active vision scenarios such as cognitive robots. In this paper, we combine cortical keypoints based on primate area V1 with a state-of-the-art nearest neighbour classifier, and show that such a system can approach state-of-the-art categorisation performance while meeting the real-time constraint.	active vision;algorithm;categorization;cognitive robotics;computation;computational complexity theory;feature learning;international symposium on fundamentals of computation theory;outline of object recognition;real-time clock;real-time computing;real-time locating system;robot;scale-invariant feature transform	Kasim Terzic;João Fabrício Mota Rodrigues;J. M. Hans du Buf	2013		10.1007/978-3-642-38628-2_37	computer vision;artificial intelligence;machine learning;pattern recognition	Vision	23.432777424422657	-56.139827398517454	191283
0afd6fa7ba307a05ca95c2cd2a807d3d20945617	a simple algorithm for superimposing sets of nmr derived structures: its application to the conformational study of cephalomannine in lipophobic and lipophilic solution		A simple iterative method for superimposing sets of NMR derived structures and calculation of the root mean square deviation (RMSD) of the sets is described. It was compared to the commonly used algorithm involving pairwise best fitting in the conformational study of the taxoid anticancer drug cephalomannine in lipophobic and lipophilic solvents. Lower RMSD values were obtained, indicating a better superposition of the structures in the sets. The conformations of cephalomannine in the two solvent systems reported are in good agreement with earlier conformational studies on other active taxoids.	iterative method;magnetic resonance imaging;mean squared error;plant roots;simple algorithm;solvents;taxoids;cephalomannine;lipophilicity	Guillermo Moyna;Sanjai Mediwala;Howard J. Williams;A. I. Scott	1996	Journal of chemical information and computer sciences	10.1021/ci960118s	crystallography;stereochemistry;chemistry;analytical chemistry	Visualization	12.037881412963705	-59.4593123620388	191948
b6ed6e49fc4950c23b76c7fc491dd43d9b5b4f97	a bayesian ensemble approach for epidemiological projections	forecasting;sensitivity and specificity;animals;simulation and modeling;data interpretation statistical;incidence;infectious disease control;epidemiological methods and statistics;bayes theorem;foot and mouth disease;great britain;plos computational biology;epidemiology;population surveillance;risk factors;farms;veterinary diseases;probability distribution;reproducibility of results;models statistical;algorithms;humans;biologiska vetenskaper;biological sciences;computer simulation;disease outbreaks	Mathematical models are powerful tools for epidemiology and can be used to compare control actions. However, different models and model parameterizations may provide different prediction of outcomes. In other fields of research, ensemble modeling has been used to combine multiple projections. We explore the possibility of applying such methods to epidemiology by adapting Bayesian techniques developed for climate forecasting. We exemplify the implementation with single model ensembles based on different parameterizations of the Warwick model run for the 2001 United Kingdom foot and mouth disease outbreak and compare the efficacy of different control actions. This allows us to investigate the effect that discrepancy among projections based on different modeling assumptions has on the ensemble prediction. A sensitivity analysis showed that the choice of prior can have a pronounced effect on the posterior estimates of quantities of interest, in particular for ensembles with large discrepancy among projections. However, by using a hierarchical extension of the method we show that prior sensitivity can be circumvented. We further extend the method to include a priori beliefs about different modeling assumptions and demonstrate that the effect of this can have different consequences depending on the discrepancy among projections. We propose that the method is a promising analytical tool for ensemble modeling of disease outbreaks.	bayesian network;discrepancy function;epidemiology;estimated;exemplification;foot-and-mouth disease;mouth diseases;paget's disease, mammary;projections and predictions;quantity	Tom Lindström;Michael J. Tildesley;Colleen Webb	2015		10.1371/journal.pcbi.1004187	computer simulation;probability distribution;incidence;epidemiology;forecasting;computer science;bayes' theorem;operations research;risk factor;statistics	AI	13.963812390981968	-53.792708640514384	192515
16ec5ce495520bd13f0f75ea4306569ca831439c	genetic algorithms for protein tertiary structure prediction	genetics;force field;structure prediction;genetic algorithm;fitness function	A genetic algorithm is used to search energetically and structurally favorable conformations. We use a hybrid protein representation, three operators to manipulate the protein 'genes', and a fitness function based on a simple force field. The prototype was applied to the ab initio prediction of Crambin. None of the conformations generated with a non-biased fitness function are similar to the native conformation but all of them show a much better overall fitness than the native structure. If guided by r.m.s. deviation the native conformation was reproduced at 1.3 AA. Therefore, the genetic algorithm's search was successful but the fitness function was no good indicator for native structure. In a side chain placement experiment Crambin was reproduced at 1.86 AA r.m.s. deviation.	genetic algorithm	Steffen Schulze-Kremer	1992		10.1007/3-540-56602-3_141	biology;bioinformatics;genetics;evolutionary biology	Theory	11.985449036122542	-60.69806060589541	192684
55a907273f7a1a965f843f2517aaa87917f5ad1e	virtual screening and prediction of site of metabolism for cytochrome p450 1a2 ligands	virtual screening	With the availability of an increasing number of high resolution 3D structures of human cytochrome P450 enzymes, structure-based modeling tools are more readily used. In this study we explore the possibilities of using docking and scoring experiments on cytochrome P450 1A2. Three different questions have been addressed: 1. Binding orientations and conformations were successfully predicted for various substrates. 2. A virtual screen was performed with satisfying enrichment rates. 3. A classification of individual compounds into active and inactive was performed. It was found that while docking can be used successfully to address the first two questions, it seems to be more difficult to perform the classification. Different scoring functions were included, and the well-characterized water molecule in the active site was included in various ways. Results are compared to experimental data and earlier classification data using machine learning methods. The possibilities and limitations of using structure-based drug design tools for cytochrome P450 1A2 come to light and are discussed.	boat dock;cytochrome p450;docking (molecular);drug design;experiment;gene ontology term enrichment;machine learning;neural binding;physical inactivity;score;scoring functions for docking;virtual desktop;virtual screening;whole earth 'lectronic link	Poongavanam Vasanthanathan;Jozef Hritz;Olivier Taboureau;Lars Olsen;Flemming Steen Jørgensen;Nico P. E. Vermeulen;Chris Oostenbrink	2009	Journal of chemical information and modeling	10.1021/ci800371f	biology;biochemistry;chemistry;virtual screening;toxicology;bioinformatics;computational chemistry	ML	10.717986499369985	-58.4059343923684	193529
567c140e32909198cf582d9f9fedc00a9486f95c	information processing in primate retinal ganglion	image recognition;한국정보통신학회;information processing in primate retinal ganglion;vol 2 no 2;sung kwan je;retinal ganglion;gwang baek kim;compression;the korea institute of information and communication engineering;artificial vision;journal of information and communication convergence engineering 제2권 제2호;artificial neural network;jae hyun cho	Most of the current computer vision theories are based on hypotheses that are difficult to apply to the real world, and they simply imitate a coarse form of the human visual system. As a result, they have not been showing satisfying results. In the human visual system, there is a mechanism that processes information due to memory degradation with time and limited storage space. Starting from research on the human visual system, this study analyzes a mechanism that processes input information when information is transferred from the retina to ganglion cells. In this study, a model for the characteristics of ganglion cells in the retina is proposed after considering the structure of the retina and the efficiency of storage space. The MNIST database of handwritten letters is used as data for this research, and ART2 and SOM as recognizers. The results of this study show that the proposed recognition model is not much different from the general recognition model in terms of recognition rate, but the efficiency of storage space can be improved by constructing a mechanism that processes input information.	information processing	Sung-kwan Je;Jae-Hyun Cho;Gwang-Baek Kim	2004	J. Inform. and Commun. Convergence Engineering		computer vision;computer science;artificial intelligence;communication	DB	22.060395700156153	-63.94004362054108	193613
8bc3564ceb0e41b57d8768bea0ab7c539c0bf5b4	a framework for scalable summarization of video	estensibilidad;hierarchical clustering;iterative method;resume video;scalability video sequences video compression iterative algorithms video coding permission visualization information analysis algorithm design and analysis clustering algorithms;bitstream extraction;video signal processing;abstracting;compact design;concepcion compacta;semantics;video sequences;transform coding;summarization;metodo iterativo;scalable summarization;algorithme;algorithm;video coding;visualization;hierarchical classification;senal video;signal video;couverture;compact representation;ranking;conception compacte;methode iterative;compressed domain;image sequence;signal classification;elaboracion resumen;summarization compressed domain hierarchical clustering ranking scalability scalable summary;classification hierarchique;iterative ranking procedure;traitement signal video;classification signal;clustering algorithms;video signal;bitstream extraction scalable summarization video sequences iterative ranking procedure;resumen video;video summary;secuencia imagen;coverage;extensibilite;scalability;classification automatique;automatic classification;encoding;elaboration resume;clasificacion automatica;clasificacion jerarquizada;context;scalable summary;sequence image;algoritmo;cobertura	Video summaries provide compact representations of video sequences, with the length of the summary playing an important role, trading off the amount of information conveyed and how fast it can be visualized. This letter proposes scalable summarization as a method to easily adapt the summary to a suitable length, according to the requirements in each case, along with a suitable framework. The analysis algorithm uses a novel iterative ranking procedure in which each summary is the result of the extension of the previous one, balancing information coverage and visual pleasantness. The result of the algorithm is a ranked list, a scalable representation of the sequence useful for summarization. The summary is then efficiently generated from the bitstream of the sequence using bitstream extraction.	algorithm;automatic summarization;bitstream;grammar-based code;high-level programming language;iterative method;requirement;scalability;semantic analysis (compilers);simple features;storyboard	Luis Herranz;José María Martínez Sanchez	2010	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2010.2057020	scalability;transform coding;visualization;multi-document summarization;ranking;computer science;theoretical computer science;automatic summarization;machine learning;data mining;semantics;hierarchical clustering;iterative method;cluster analysis;information retrieval;encoding;statistics	Vision	18.522327503999094	-58.42859276864267	193931
bf15c9c9905c3af4e2eb9d094cad719d9ca49e05	identifying style of 3d shapes using deep metric learning	keywords;style similarity;i 3 5 computer graphics;i 3 5 computer graphics computational geometry and object modeling;computational geometry and object modeling;categories and subject descriptors according to acm ccs;deep metric learning	We present a method that expands on previous work in learning human perceived style similarity across objects with different structures and functionalities. Unlike previous approaches that tackle this problem with the help of hand-crafted geometric descriptors, we make use of recent advances in metric learning with neural networks (deep metric learning). This allows us to train the similarity metric on a shape collection directly, since any lowor high-level features needed to discriminate between different styles are identified by the neural network automatically. Furthermore, we avoid the issue of finding and comparing sub-elements of the shapes. We represent the shapes as rendered images and show how image tuples can be selected, generated and used efficiently for deep metric learning. We also tackle the problem of training our neural networks on relatively small datasets and show that we achieve style classification accuracy competitive with the state of the art. Finally, to reduce annotation effort we propose a method to incorporate heterogeneous data sources by adding annotated photos found online in order to expand or supplant parts of our training data.	artificial neural network;computation;feature recognition;geometry processing;high- and low-level;precomputation;preprocessor;recommender system;requirement;similarity learning;visual descriptor	Isaak Lim;Anne Gehre;Leif Kobbelt	2016	Comput. Graph. Forum	10.1111/cgf.12977	computer vision;computer science;artificial intelligence;theoretical computer science;machine learning;mathematics;geometry;algorithm;computer graphics (images)	ML	23.749793535860416	-53.680091968188215	194072
0c49fbcfb9455eca0c4fa2b071591627b4c436fc	skin beautification detection using sparse coding		In the past years, skin beautifying softwares have been widely used in portable devices for social activities, which have the functionalities of turning one's skin into flawless complexion. With a huge number of photos uploaded to social media, it is useful for users to distinguish whether a photo is beautified or not. To address this problem, in this paper, we propose a skin beautification detection method by mining and distinguishing the intrinsic features of original photos and the corresponding beautified photos. To this aim, we propose to use sparse coding to learn two sets of basis functions using densely sampled patches from the original photos and the beautified photos, respectively. To detect whether a test photo is beautified, we represent the sampled patches from the photo using the learned basis functions and then see which set of basis functions produces more sparse coefficients. To our knowledge, our effort is the first one to detect skin beautification. To validate the effectiveness of the proposed method, we collected about 1000 photos including both the original photos and the photos beautified by a software. Our experimental results indicate the proposed method achieved a desired detection accuracy of over 80%.	basis function;coefficient;neural coding;personal digital assistant;skin (computing);social media;sparse matrix	Tianyang Sun;Xinyu Hui;Zihao Wang;Shengping Zhang	2017	2017 Fifteenth IAPR International Conference on Machine Vision Applications (MVA)	10.23919/MVA.2017.7986916	pattern recognition;artificial intelligence;computer vision;beautification;software;neural coding;basis function;upload;computer science	AI	22.65746944017639	-58.73747615712702	194245
ec05078be14a11157ac0e1c6b430ac886124589b	longitudinal face aging in the wild - recent deep learning approaches		Face Aging has raised considerable attentions and interest from the computer vision community in recent years. Numerous approaches ranging from purely image processing techniques to deep learning structures have been proposed in literature. In this paper, we aim to give a review of recent developments of modern deep learning based approaches, i.e. Deep Generative Models, for Face Aging task. Their structures, formulation, learning algorithms as well as synthesized results are also provided with systematic discussions. Moreover, the aging databases used in most methods to learn the aging process are also reviewed. Keywords-Face Aging, Face Age Progression, Deep Generative Models.	algorithm;color gradient;computer vision;database;deep learning;image processing;image quality;machine learning;nonlinear system	Chi Nhan Duong;Khoa Luu;Kha Gia Quach;Tien D. Bui	2018	CoRR		generative grammar;image processing;machine learning;deep learning;computer science;artificial intelligence;ranging	ML	24.168263738671612	-55.16273536228102	194397
618eb680cb7009ac38d0096c0b90151623397ce7	noise reduction method for molecular interaction energy: application to in silico drug screening and in silico target protein screening	computer programs;noise reduction;computers in chemistry;drug screening;molecular interactions;in silico	We developed a new method to improve the accuracy of molecular interaction data using a molecular interaction matrix. This method was applied to enhance the database enrichment of in silico drug screening and in silico target protein screening using a protein-compound affinity matrix calculated by a protein-compound docking software. Our assumption was that the protein-compound binding free energy of a compound could be improved by a linear combination of its docking scores with many different proteins. We proposed two approaches to determine the coefficients of the linear combination. The first approach is based on similarity among the proteins, and the second is a machine-learning approach based on the known active compounds. These methods were applied to in silico screening of the active compounds of several target proteins and in silico target protein screening.		Yoshifumi Fukunishi;Satoru Kubota;Haruki Nakamura	2006	Journal of chemical information and modeling	10.1021/ci060152z	biology;toxicology;computer science;bioinformatics;noise reduction;combinatorial chemistry	Comp.	10.326678154700224	-58.51222966639747	194446
c98338e459cb67ce7be892f2ddaba60af1575d7e	rational design of new class of bh3-mimetics as inhibitors of the bcl-xl protein	cancerology;criblage;treatment resistance;proteine;resistance traitement;cancer;tumor maligno;etude experimentale;screening;interrogation base donnee;sobrevivencia;molecular dynamics;interrogacion base datos;hombre;pharmacophore;dynamique moleculaire;lead compound;molecular dynamics method;cancerologie;human;depistage;descubrimiento;cernido;survie;tumeur maligne;medical screening;proteina;methode dynamique moleculaire;cancerologia;dinamica molecular;survival;resistencia tratamiento;protein;article;estudio experimental;plomo compuesto;database query;in vitro;farmacoforo;malignant tumor;metodo dinamico molecular;homme;compose du plomb	The Bcl-2 family of proteins plays an important role in the intrinsic pathway of cell apoptosis. Overexpression of pro-survival members of this family of proteins is often associated with the development of many types of cancer and confers resistance against conventional therapeutic treatments. Accordingly, antagonism of its protective function has emerged as an encouraging anticancer strategy. In the present work, we use a pharmacophore for describing interaction between the BH3 domain of different pro-apoptotic members and the pro-survival protein Bcl-x(L) in order to identify new lead compounds. In the strategy followed in the present work, the pharmacophore was derived from molecular dynamics studies of different Bcl-x(L)/BH3 complexes. This pharmacophore was later used as query for 3D database screening. Hits obtained from the search were computationally assessed, and a subset proposed for in vitro testing. Two of the 15 compounds assayed were found able to disrupt the Bcl-x(L)/Bak(BH3) complex with IC(50) values in the lower micromolar range. Finally, docking studies were performed to explore the binding mode of these compounds to Bcl-x(L) for further modifications.	apoptosis;bcl-xl protein;bh3 domain;bad protein;boat dock;cyclin d1;docking (molecular);gene regulatory network;in vitro [publication type];lead compound;micromole/liter;molecular dynamics;neoplasms;pharmacophore;protein family;subgroup	Marta Pinto;Maria del Mar Orzaez;Laura Delgado-Soler;Juan J. Perez;Jaime Rubio-Martinez	2011	Journal of chemical information and modeling	10.1021/ci100501d	biology;stereochemistry;molecular dynamics;chemistry;computational chemistry;quantum mechanics;cancer	Comp.	10.23035356178431	-61.67307707191384	194508
34785299c5e8604f5b7c09c8190915da7429adbb	biased competition in visual processing hierarchies: a learning approach using multiple cues	health research;uk clinical guidelines;biological patents;hierarchical system;bottom up;learning;europe pubmed central;top down;citation search;large scale;statistical learning;uk phd theses thesis;directional data;life sciences;data flow;visual attention;uk research reports;medical journals;visual processing;autonomous learning;europe pmc;object detection;biomedical research;bioinformatics	In this contribution, we present a large-scale hierarchical system for object detection fusing bottom-up (signal-driven) processing results with top-down (model or task-driven) attentional modulation. Specifically, we focus on the question of how the autonomous learning of invariant models can be embedded into a performing system and how such models can be used to define object-specific attentional modulation signals. Our system implements bi-directional data flow in a processing hierarchy. The bottom-up data flow proceeds from a preprocessing level to the hypothesis level where object hypotheses created by exhaustive object detection algorithms are represented in a roughly retinotopic way. A competitive selection mechanism is used to determine the most confident hypotheses, which are used on the system level to train multimodal models that link object identity to invariant hypothesis properties. The top-down data flow originates at the system level, where the trained multimodal models are used to obtain space- and feature-based attentional modulation signals, providing biases for the competitive selection process at the hypothesis level. This results in object-specific hypothesis facilitation/suppression in certain image regions which we show to be applicable to different object detection mechanisms. In order to demonstrate the benefits of this approach, we apply the system to the detection of cars in a variety of challenging traffic videos. Evaluating our approach on a publicly available dataset containing approximately 3,500 annotated video images from more than 1 h of driving, we can show strong increases in performance and generalization when compared to object detection in isolation. Furthermore, we compare our results to a late hypothesis rejection approach, showing that early coupling of top-down and bottom-up information is a favorable approach especially when processing resources are constrained.	algorithm;autonomous robot;bootstrapping (compilers);bottom-up parsing;bottom-up proteomics;dataflow;delta-sigma modulation;deny (action);detectors;directional statistics;embedded system;embedding;generalization (psychology);greater than;modulation;multimodal interaction;object detection;preprocessor;rejection sampling;sensitivity and specificity;silo (dataset);top-down and bottom-up design;zero suppression;benefit;facilitation	Alexander Rainer Tassilo Gepperth;Sven Rebhan;Stephan Hasler;Jannik Fritsch	2010		10.1007/s12559-010-9092-x	psychology;neuroscience;computer science;bioinformatics;artificial intelligence;machine learning;top-down and bottom-up design;communication;cognitive science;statistics	ML	23.404247717273947	-55.95806294643055	194652
ad4c3c80138f9c41d9cbd8dfb02d3ffbe1f4fe1c	a parallel artificial neural network learning scheme based on radio wave fingerprint for indoor localization		Radio wave fingerprinting is known to be the best method for indoor positioning, and its performance depends greatly on the data comparison algorithm that is used. This paper implements a radio wave fingerprint positioning method with artificial neural network learning to improve the performance of a conventional radio fingerprint positioning algorithm based on the Euclidean distance. We propose a parallel learning method to reduce the error in the indoor height and an indoor positioning data augmentation method for data generalization. This method exhibits a higher performance than an existing Euclidean distance based positioning method. In particular, the data augmentation technique can be applied without depending on the specific positioning algorithm.	algorithm;artificial neural network;convolutional neural network;euclidean distance;fingerprint (computing);radio wave	Chan Uk Park;Hong-Gi Shin;Yong-Hoon Choi	2018	2018 Tenth International Conference on Ubiquitous and Future Networks (ICUFN)	10.1109/ICUFN.2018.8437009	computer vision;euclidean distance;artificial neural network;radio wave;computer science;distributed computing;fingerprint recognition;artificial intelligence	Robotics	19.55425866476338	-55.78014406097317	195204
18737f415aeb902b5babd01b0800da0dfbf559d7	a goal oriented attention model for efficient object search		Prior knowledge of the target accelerates target detection in visual search tasks. This paper suggests a new computational model which biases the bottom-up features with known target representation so as to make the target more salient and to speed up object search. The proposed model consists of two of models, learning model and searching model. Learning model is incrementally learns and memorizes primitive features of target object and yields trained data, and searching model finds desired targets through biasing feature maps and saliency map for selectively attending to a target object. The information in trained data is used as a biasing signal. In order to evaluate the performance of our model, we compared our model with previous bottom-up model and trained model in top-down guided search. Average number of false detections before target found was used as a performance criteria in our experiments. The results show that our model successfully finds desired target in natural cluttered scenes faster than previous models.		Kyung Joo Cheoi	2012		10.1007/978-3-642-32692-9_21	computer vision;salient;visual search tasks;speedup;saliency map;distributed computing;goal orientation;computer science;top-down and bottom-up design;artificial intelligence	ML	22.748220689903327	-64.46596714408012	195622
3d5826700c22e5aa5f03364e68301e6d313495ec	a deep learning interpretable classifier for diabetic retinopathy disease grading		Deep neural network models have been proven to be very successful in image classification tasks, also for medical diagnosis, but their main concern is its lack of interpretability. They use to work as intuition machines with high statistical confidence but unable to give interpretable explanations about the reported results. The vast amount of parameters of these models make difficult to infer a rationale interpretation from them. In this paper we present a diabetic retinopathy interpretable classifier able to classify retine images into the different levels of disease severity and of explaining its results by assigning a score for every point in the hidden and input space, evaluating its contribution to the final classification in a linear way. The generated visual maps can be interpreted by an expert in order to compare its own knowledge with the interpretation given by the model.	algorithm;computer vision;deep learning;design rationale;interpretation (logic);map;pixel;relevance;software propagation;the 100	Jordi de La Torre;Aïda Valls;Domenec Puig	2017	CoRR		artificial neural network;machine learning;grading (education);medical diagnosis;deep learning;pattern recognition;diabetic retinopathy;mathematics;interpretability;contextual image classification;classifier (linguistics);artificial intelligence	ML	21.710651835776687	-54.39191959882439	195867
e3b945100a3051521aaa30040531472154fe47d1	advancements and challenges in computational biology	humans;computational biology	Computational biology has soared from being an auxiliary discipline to being a crucial element for progress in practically all aspects of the biological sciences. In this annual Editorial, I would like to step back, consider significant computational biology advances of the last decade, and reflect on some key challenges ahead. The timing is particularly appropriate. PLOS Computational Biology, the premier journal in computational biology, is approaching its tenth anniversary. The task is daunting; not only has the field come a long way in ten years but it is broad with many advances to consider. In addition, since computational biology has become closely tied to experimental research, progress is not purely computational; it is tied to experiment. And that’s as it should be. Ten years ago, computational biology was not entirely trusted by experimental biologists. By contrast, today computational biology is integrated in the community. It’s easier for computational biologists to collaborate across disciplines. Laboratory scientists have a better understanding of the merit of computational models for hypothesis generation as well as the need to iterate between modeling and laboratory testing [1]. We have witnessed huge leaps in biological computing [2]. We now have at our disposal large information-rich resources, and we are increasingly able to integrate and understand the vast quantities of data that they encompass. We have also made big strides toward multiscale biological modeling, and we have a vastly more networked world of researchers and their data. Analysis of massive gene expression and proteomic data permitted the construction of comprehensive and predictive models for cellular pathways, as well as software for inferring interaction networks, and steps toward modeling of cells. Genes susceptible to disease have been identified and, on a different level, the electrical behavior of neurons has been modeled. Molecules have been imaged in action and networks that regulate cell functions untangled. Matching targets for selective cancer therapy is difficult. Nonetheless, recent strategies have been proposed to restrict the combinatorial space, minimize toxicity, and increase the precision and power of such restrictive combinations, altogether leading to drugs that could be tested in clinical trials. Leveraging the enhanced identification of drug targets, including repertoires of redundant pathway combinations, has been helped by such innovative concepts [3]. Formidable challenges include: the establishment of computer networks for surveillance of disease; mapping the pathways and biological networks associated with the initiation, growth and spread of cancer; predicting function and mutational dysfunction in disease from the structure of complex molecules; resolving the mechanisms of oncogenic mutations and the cellular network which is rewired in cancer; achieving accurate, efficient, and comprehensive dynamic models; and moving from artificial intelligence to the ‘‘connectome’’—the connections among all of the neurons of the brain. Multiscale biological modeling—an area where vast progress has been made during the last decade—still faces major challenges. To tackle this aim, hybrid methods across disciplines, scales, and sources are essential. Hybrid methods integrate data from, for example, serial crystallography and time-resolved wide-angle X-ray scattering, microand nano-crystals for (future) freeelectron lasers, electron microscopy, fluorescence resonance energy transfer (FRET), cross-linking data, small-angle X-ray scattering, crystallography, nuclear magnetic resonance (NMR), and more. Equally important is the development of protocols for model validation. We may expect an influx of models based on experimental data integration. If these are to be deposited in a public archival system, which is now a community aim, such clear protocols are essential for maintaining quality control. Finally, studying the dynamics of large integrated models is increasingly used to improve our understanding of how large complexes function in the cell and how they are regulated. The dynamics of such large associations provides an additional hugely complex layer; to date, we are still struggling to comprehend the dynamics of single molecules and their associations. This is compounded by the fact that large regions of the molecules can be disordered, and multiple temporal post-translational modifications take place, with different combinations spelling distinct functions. On a different level, improved tumor mutational analysis platforms and knowledge of the redundant pathways, which can take over in cancer, may not only supplement known actionable findings but forecast possible cancer progression and resistance. Such forward-looking can be powerful, endowing the oncologist with mechanistic insight and cancer prognosis, and consequently more informed treatment options. Lastly, the community faces the global challenge of linking genetics to phenotype, including the genetics of cancer. Genetics is mediated by dynamic conformational ensembles. Powerful ideas such as that of the free energy landscape [4], imported from physics and chemistry, can help solve the mysteries of life. Biomolecules are not static sculptures; they are dynamic objects that are always interconverting between structures with varying energies. Such ideas help to understand how and why one-dimensionally connected biomolecules can organize themselves into	adverse reaction to drug;archive;artificial intelligence;biological science disciplines;cell physiology;color gradient;computation (action);computational biology;computational model;crystallography;disposal;drug delivery systems;energy, physics;experiment;face;fluorescence resonance energy transfer;gnu nano;gene expression;gene regulatory network;genetic translation process;interaction network;iteration;laboratory procedures;lasers;magnetic resonance imaging;mathematical model;mental association;mutation;neoplasms;occur (action);physical object;post-translational protein processing;predictive modelling;projections and predictions;proteomics;protocols documentation;quantity;scanning electron microscopy;transcription initiation;cancer therapy;computer network;free energy;spelling;tenth	Ruth Nussinov	2015		10.1371/journal.pcbi.1004053	computational biology;biology;computer science;bioinformatics;operations research	Comp.	12.849747863750567	-63.054405114748825	195973
8e06017bf1afe6fcc6878da7c15aed866a34755b	attributing fake images to gans: analyzing fingerprints in generated images		Research in computer graphics has been in pursuit of realistic image generation for a long time. Recent advances in machine learning with deep generative models have shown increasing success of closing the realism gap by using datadriven and learned components. There is an increasing concern that real and fake images will become more and more difficult to tell apart. We take a first step towards this larger research challenge by asking the question if and to what extend a generated fake image can be attribute to a particular Generative Adversarial Networks (GANs) of a certain architecture and trained with particular data and random seed. Our analysis shows single samples from GANs carry highly characteristic fingerprints which make attribution of images to GANs possible. Surprisingly, this is even possible for GANs with same architecture and same training that only differ by the training seed.	closing (morphology);display resolution;fingerprint;frequency band;generative adversarial networks;generative model;glossary of computer graphics;machine learning;random seed;semantic web	Ning Yu;Larry Davis;Mario Fritz	2018	CoRR		adversarial system;artificial intelligence;generative grammar;realism;random seed;architecture;computer science;machine learning;graphics;computer graphics	ML	22.350180630679382	-53.288210719059855	196078
6b2ffac52513f5d1477889e02cc5900693a9ad8a	computational features evaluation for rna secondary structure prediction	secondary structure prediction;prediction method;rna sequences thermodynamics probability phylogeny biology computing genetic mutations educational institutions mechatronics automation;comparative sequence analysis;phylogeny;prediction theory bioinformatics classification macromolecules molecular biophysics;rna secondary structure;training;qualitative analysis;gold standard;data mining;classification;computational molecular biology;rna;prediction theory;stacking;rfam alignments rna secondary structure prediction computational molecular biology comparative sequence analysis homologous sequences alignment base pair feature selection classification;homologous sequences alignment;molecular biophysics;macromolecules;thermodynamics;feature selection;sequence alignment;rna secondary structure prediction;rfam alignments;base pair;bioinformatics	Computational prediction of RNA secondary structure is classical open problem in computational molecular biology. Comparative sequence analysis is the gold standard method when given homologous sequences alignment. The essential of this method is a classification problem: to judge if any two columns of an alignment correspond to a base pair using provided information by alignment. However, all existing prediction methods select computational features by qualitative analysis, without a uniform criterion. Here, we collected various computational features used in existing prediction methods, and quantitatively compare the classification capability of those features by feature selection technique. As a result, an optimum subset of features was selected for predicting RNA secondary structure by classification. The test on 49 Rfam alignments shows the effectiveness of the selected features. Keywords-RNA secondary structure; classification problem; computational features; quantitatively; subset of features	column (database);computation;feature selection;homology (biology);protein structure prediction;rfam;sequence analysis	Yingjie Zhao;Qingshan Ni;Zhengzhi Wang	2009	2009 2nd International Conference on Biomedical Engineering and Informatics	10.1109/BMEI.2009.5304921	computational biology;macromolecule;biology;rna;base pair;biological classification;gold standard;computer science;bioinformatics;qualitative research;stacking;sequence alignment;nucleic acid secondary structure;feature selection;genetics;molecular biophysics	Comp.	10.077331837095123	-53.78743128108995	196460
598c6522af38682e906ecb8b19b293b71fff1162	feedback memetic algorithms for modeling gene regulatory networks	i. introduction;systems biology;dna;feedback;evolutionary computation;network topology;mathematical model;nonlinear system;differential equations;evolutionary algorithm;memetic algorithm;gene regulatory network	In this paper we address the problem of finding gene regulatory networks from experimental DNA microarray data. We focus on the evaluation of the performance of memetic algorithms on the inference problem. These algorithms are used to evolve an underlying quantitative mathematical model. The dynamics of the regulatory system are modeled with two commonly used approaches, namely linear weight matrices and S-systems. Due to the complexity of the inference problem, some researchers suggested evolutionary algorithms for this purpose. We introduce memetic enhancements to this optimization process to infer the parameters of sparsely connected nonlinear systems from the observed data. Due to the limited number of available data, the inferring problem is underdetermined and ambiguous. Further on, the problem often is multimodal and therefore appropriate optimization strategies become necessary. We propose a memetic method, which separates the overall inference problem into two subproblems to find the correct network: first, the search for a valid topology, and secondly, the optimization of the parameters of the mathematical model. The performance and the properties of the proposed methods are evaluated and compared to standard algorithms found in the literature.	dna microarray;evolutionary algorithm;gene regulatory network;mathematical model;mathematical optimization;memetic algorithm;memetics;multimodal interaction;nonlinear system	Christian Spieth;Felix Streichert;Jochen Supper;Nora Speer;Andreas Zell	2005	2005 IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology		gene regulatory network;mathematical optimization;nonlinear system;computer science;bioinformatics;artificial intelligence;machine learning;evolutionary algorithm;mathematical model;feedback;genetics;differential equation;systems biology;dna;network topology;memetic algorithm;evolutionary computation	AI	12.607692497531472	-52.665290005847176	196733
4dd836a213bf2ad7fd250517a3f3cb14faca0c37	comparison between self-guided langevin dynamics and molecular dynamics simulations for structure refinement of protein loop conformations	conformational sampling;simulation;molecular dynamics;protein variable regions;computational chemistry;proteins;replica exchange methods;comparative protein models;reprints	This article presents a comparative analysis of two replica-exchange simulation methods for the structure refinement of protein loop conformations, starting from low-resolution predictions. The methods are self-guided Langevin dynamics (SGLD) and molecular dynamics (MD) with a Nosé-Hoover thermostat. We investigated a small dataset of 8- and 12-residue loops, with the shorter loops placed initially from a coarse-grained lattice model and the longer loops from an enumeration assembly method (the Loopy program). The CHARMM22 + CMAP force field with a generalized Born implicit solvent model (molecular-surface parameterized GBSW2) was used to explore conformational space. We also assessed two empirical scoring methods to detect nativelike conformations from decoys: the all-atom distance-scaled ideal-gas reference state (DFIRE-AA) statistical potential and the Rosetta energy function. Among the eight-residue loop targets, SGLD out performed MD in all cases, with a median of 0.48 Å reduction in global root-mean-square deviation (RMSD) of the loop backbone coordinates from the native structure. Among the more challenging 12-residue loop targets, SGLD improved the prediction accuracy over MD by a median of 1.31 Å, representing a substantial improvement. The overall median RMSD for SGLD simulations of 12-residue loops was 0.91 Å, yielding refinement of a median 2.70 Å from initial loop placement. Results from DFIRE-AA and the Rosetta model applied to rescoring conformations failed to improve the overall detection calculated from the CHARMM force field. We illustrate the advantage of SGLD over the MD simulation model by presenting potential-energy landscapes for several loop predictions. Our results demonstrate that SGLD significantly outperforms traditional MD in the generation and populating of nativelike loop conformations and that the CHARMM force field performs comparably to other empirical force fields in identifying these conformations from the resulting ensembles.	amino acids;charmm;casio loopy;cmap (font);force field (chemistry);implicit solvation;internet backbone;lattice model (physics);mathematical optimization;molecular dynamics;nosé–hoover thermostat;parallel tempering;plant roots;population;qualitative comparative analysis;refinement (computing);score;simulation;solvent models;statistical potential;thermostat device component;vertebral column	Mark A. Olson;Sidhartha Chaudhury;Michael S. Lee	2011	Journal of computational chemistry	10.1002/jcc.21883	fox proteins;molecular dynamics;chemistry;computational chemistry	Comp.	12.191834794514287	-60.35401477699654	196909
42c2f6134d55d96141045ed6f752e4c4ef6d88d4	dsx: a knowledge-based scoring function for the assessment of protein-ligand complexes		We introduce the new knowledge-based scoring function DSX that consists of distance-dependent pair potentials, novel torsion angle potentials, and newly defined solvent accessible surface-dependent potentials. DSX pair potentials are based on the statistical formalism of DrugScore, extended by a much more specialized set of atom types. The original DrugScore-like reference state is rather unstable with respect to modifications in the used atom types. Therefore, an important method to overcome this problem and to allow for robust results when deriving pair potentials for arbitrary sets of atom types is presented. A validation based on a carefully prepared test set is shown, enabling direct comparison to the majority of other popular scoring functions. Here, DSX features superior performance with respect to docking- and ranking power and runtime requirements. Furthermore, the beneficial combination with torsion angle-dependent and desolvation-dependent potentials is demonstrated. DSX is robust, flexible, and capable of working together with special features of popular docking engines, e.g., flexible protein residues in AutoDock or GOLD. The program is freely available to the scientific community and can be downloaded from our Web site www.agklebe.de .	accessible surface area;atom;autodock;boat dock;control theory;docking (molecular);formal system;requirement;score;scoring functions for docking;test set;torsion (gastropod);unstable medical device problem;gold	Gerd Neudert;Gerhard Klebe	2011	Journal of chemical information and modeling	10.1021/ci200274q	theoretical computer science;mathematics;algorithm		10.949427050455748	-59.48214809071384	196970
ce4f199ca6df3e8515b0f75f3a69c917d2d85b66	trainable co-occurrence activation unit for improving convnet		A deep neural network is one of the promising approach to produce state-of-the-art performance on various fields such as pattern recognition and signal processing. While the network architecture is intensively studied, as to the network components, non-linear activation functions are the main subject of research in the literature. Most of the activation functions, such as a rectified linear unit (ReLU), operate on each of feature channels in an element-wise manner and thus can be regarded as extracting occurrence characteristics from the input feature map. In this paper, we propose a co-occurrence activation unit to work across feature channels by extending the element-wise activation function. In contrast to the original co-occurrence formulation applied to hand-crafted feature extraction methods, the proposed co-occurrence unit is trainable by a gradient-based optimization through back-propagation learning and exploits the co-occurrence relationships among the feature channels. The experimental results on image classification datasets show that the proposed co-occurrence activation unit embedded into various types of ConvNets favorably improve classification performance.	activation function;artificial neural network;backpropagation;computer vision;deep learning;embedded system;feature extraction;gradient;mathematical optimization;network architecture;nonlinear system;pattern recognition;rectifier (neural networks);signal processing;software propagation	Takumi Kobayashi	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461517	artificial neural network;network architecture;signal processing;feature extraction;artificial intelligence;contextual image classification;pattern recognition;convolution;rectifier (neural networks);activation function;computer science	Vision	24.522047433476946	-52.192840129963436	197419
f5fe399fd457d6b6b475612e73cf31f217dfe50a	a study of visual attention system based on recognition feedback for image sequence	vision system;image features;image sequence;visual features;visual attention;high speed	The research focuses on approach for robot's vision system. We propose the selective visual attention system which realizes high-speed processing flexible to environmental change. In order to obtain visual attention point, proposed system extracts three kinds of visual features: still image feature, blinking feature and motion feature. Proposed system introduced intention maps to attend interested area. Intention maps are updated dynamically to represent the existence of attention target. As the result of experiments, the proposed system can confirm visual transition to the interest object.	bottom-up proteomics;experiment;feature (computer vision);map;top-down and bottom-up design	Makoto Ito;Yoshikazu Yano;Shigeru Okuma	2005			computer vision;simulation;machine vision;visual search;computer science;human visual system model;gaze-contingency paradigm;feature	Robotics	22.828822092795317	-64.99743886083955	197561
47823a125b5c167957be0c04f974d52f78d934d2	an efficient deep residual-inception network for multimedia classification		Deep learning has led to many breakthroughs in machine perception and data mining. Although there are many substantial advances of deep learning in the applications of image recognition and natural language processing, very few work has been done in video analysis and semantic event detection. Very deep inception and residual networks have yielded promising results in the 2014 and 2015 ILSVRC challenges, respectively. Now the question is whether these architectures are applicable to and computationally reasonable in a variety of multimedia datasets. To answer this question, an efficient and lightweight deep convolutional network is proposed in this paper. This network is carefully designed to decrease the depth and width of the state-of-the-art networks while maintaining the high-performance. The proposed deep network includes the traditional convolutional architecture in conjunction with residual connections and very light inception modules. Experimental results demonstrate that the proposed network not only accelerates the training procedure, but also improves the performance in different multimedia classification tasks.	computer vision;data mining;deep learning;machine perception;natural language processing;video content analysis	Samira Pouyanfar;Shu-Ching Chen;Mei-Ling Shyu	2017	2017 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2017.8019447	convolutional neural network;residual;computer science;computer vision;theoretical computer science;architecture;deep learning;artificial intelligence;machine learning;machine perception;multimedia	ML	24.31693853444134	-54.21728056951518	197938
d14fd5deadb2752aa05d7a89c9a8ea22032dc65a	coping with variability in motion based activity recognition	imu;segmentation;motif detection;wearable computing;activity recognition	A key issue in the automatic recognition of human activities with body worn sensors is the variability of human motions and the huge space of possibilities for executing even fairly simple actions. In this article we introduce a new algorithm to address this issue. The core idea is that often even highly variable actions include short more or less invariant parts which are due to hard physical constraints. The aim is to develop a method that can identify such invariants and use them to improve the classification of the respective activities. The method is meant to be combined with existing classification approaches in an ensemble like fashion, being applied only to the classes for which appropriate invariants can be found and leaving the other classes to be handled by classical methods. We compare our method's results to prior publications on two well known data sets and are able to improve the classification in 5 of 23 respectively 4 of 19 classes, in same cases by a large margin (best case is from 27% to 76% in the first and from 50% to 64% in the second). In each data set there is only one class for which we make the recognition worse and in both cases it is one with poor results to start with and a relatively small decrease (from 54% to 45% in the first and from 65% to 62% in the second). The results are achieved for an user independent case.	activity recognition;best, worst and average case;class;coping behavior;cross-validation (statistics);departure - action;invariant (computer science);motion;spatial variability;statistical classification;algorithm;executing - querystatuscode;sensor (device);triangulation	Matthias Kreil;Bernhard Sick;Paul Lukowicz	2016		10.1145/2948963.2948967	simulation;computer science;artificial intelligence;data mining	AI	20.2745331652506	-57.84207281900242	198386
494aadbeaee0297d0603dcd77408fedf33d69dd1	significance analysis and multiple pharmacophore models for differentiating p-glycoprotein substrates	p glycoprotein	P-glycoprotein (Pgp) mediated drug efflux affects the absorption, distribution, and clearance of a broad structural variety of drugs. Early assessment of the potential of compounds to interact with Pgp can aid in the selection and optimization of drug candidates. To differentiate nonsubstrates from substrates of Pgp, a robust predictive pharmacophore model was targeted in a supervised analysis of three-dimensional (3D) pharmacophores from 163 published compounds. A comprehensive set of pharmacophores has been generated from conformers of whole molecules of both substrates and nonsubstrates of P-glycoprotein. Four-point 3D pharmacophores were employed to increase the amount of shape information and resolution, including the ability to distinguish chirality. A novel algorithm of the pharmacophore-specific t-statistic was applied to the actual structure-activity data and 400 sets of artificial data (sampled by decorrelating the structure and Pgp efflux activity). The optimal size of the significant pharmacophore set was determined through this analysis. A simple classification tree using nine distinct pharmacophores was constructed to distinguish nonsubstrates from substrates of Pgp. An overall accuracy of 87.7% was achieved for the training set and 87.6% for the external independent test set. Furthermore, each of nine pharmacophores can be independently utilized as an accurate marker for potential Pgp substrates.	algorithm;chirality (chemistry);decision tree learning;decorrelation;drug efflux;mathematical optimization;p-glycoprotein;pharmacophore;pretty good privacy;scientific publication;test set	Wuxiong Li;Leping Li;John Eksterowicz;Xuefeng Bruce Ling;Mario G. Cardozo	2007	Journal of chemical information and modeling	10.1021/ci700284p	biology;stereochemistry;chemistry;bioinformatics;combinatorial chemistry	ML	10.08769636320791	-59.57526505255523	198399
c3c1d1f198e0c8e9c4ce4b6e2d4e2aafd0be4735	dissociable neural representations of adversarially perturbed images in deep neural networks and the human brain		Despite the remarkable similarities between deep neural networks (DNN) and the human brain as shown in previous studies, the fact that DNNs still fall behind humans in many visual tasks suggests that considerable differences still exist between the two systems. To probe their dissimilarities, we leverage adversarial noise (AN) and adversarial interference (AI) images that yield distinct recognition performance in a prototypical DNN (AlexNet) and human vision. The evoked activity by regular (RE) and adversarial images in both systems is thoroughly compared. We find that representational similarity between RE and adversarial images in the human brain resembles their perceptual similarity. However, such representation-perception association is disrupted in the DNN. Especially, the representational similarity between RE and AN images idiosyncratically increases from low- to high-level layers. Furthermore, forward encoding modeling reveals that the DNN-brain hierarchical correspondence proposed in previous studies only holds when the two systems process RE and AI images but not AN images. These results might be due to the deterministic modeling approach of current DNNs. Taken together, our results provide a complementary perspective on the comparison between DNNs and the human brain, and highlight the need to characterize their differences to further bridge artificial and human intelligence research.		Chi Zhang;X Duan;Linyuan Wang;Yongli Li;Bin Yan;Guoen Hu;Ruyuan Zhang;Li Tong	2018	CoRR			AI	21.197648579105348	-53.46552619765321	199377
2fbac17b4063a215f5325e38d0c7ed7c3efbad40	protein fold recognition using segmentation-based feature extraction model	physicochemicalbased features;ensemble of different classifiers;institute for integrated and intelligent systems;faculty of science environment engineering and technology;adaboost m1;080109;segmented density;pattern recognition and data mining;segmented distribution;svm	Protein Fold recognition (PFR) is considered as an important step towards protein structure prediction. It also provides significant information about general functionality of a given protein. Despite all the efforts have been made, PFR still remains unsolved. It is shown that appropriately extracted features from the physicochemical-based attributes of the amino acids plays crucial role to address this problem. In this study, we explore 55 different physicochemical-based attributes using two novel feature extraction methods namely segmented distribution and segmented density. Then, by proposing an ensemble of different classifiers based on the AdaBoost.M1 and Support Vector Machine (SVM) classifiers which are diversely trained on different combinations of features extracted from these attributes, we outperform similar studies found in the literature for over 2% for the PFR task.	adaboost;error detection and correction;feature extraction;naive bayes classifier;norm (social);protein structure prediction;random forest;support vector machine;threading (protein sequence)	Abdollah Dehzangi;Abdul Sattar	2013		10.1007/978-3-642-36546-1_36	support vector machine;computer science;machine learning;pattern recognition;data mining	ML	10.225114167140351	-53.20870030586822	199383
ae2f7061e1a15466d562853b3149773756c43fcc	parsing nucleic acid pseudoknotted secondary structure: algorithm and applications	dynamic programming algorithm;pseudoknots;linear time algorithm;minimum free energy;dynamic program;rna;secondary structure;thermodynamics;rna secondary structure prediction;nucleic acid	"""Accurate prediction of pseudoknotted nucleic acid secondary structure is an important computational challenge. Prediction algorithms based on dynamic programming aim to find a structure with minimum free energy according to some thermodynamic (""""sum of loop energies"""") model that is implicit in the recurrences of the algorithm. However, a clear definition of what exactly are the loops in pseudoknotted structures, and their associated energies, has been lacking. In this work, we present a complete classification of loops in pseudoknotted nucleic secondary structures, and describe the Rivas and Eddy and other energy models as sum-of-loops energy models. We give a linear time algorithm for parsing a pseudoknotted secondary structure into its component loops. We give two applications of our parsing algorithm. The first is a linear time algorithm to calculate the free energy of a pseudoknotted secondary structure. This is useful for heuristic prediction algorithms, which are widely used since (pseudoknotted) RNA secondary structure prediction is NP-hard. The second application is a linear time algorithm to test the generality of the dynamic programming algorithm of Akutsu for secondary structure prediction. Together with previous work, we use this algorithm to compare the generality of state-of-the-art algorithms on real biological structures."""	algorithm;cell nucleus;dynamic programming;energy, physics;heuristic;np-hardness;nucleic acids;parsing;pierce oscillator;protein structure prediction;ranitidine 150 mg oral tablet;recurrence (disease attribute);recurrence relation;test set;thermodynamics;time complexity;algorithm;dirk;free energy	Baharak Rastegari;Anne Condon	2007	Journal of computational biology : a journal of computational molecular cell biology	10.1089/cmb.2006.0108	biology;nucleic acid;rna;computer science;bioinformatics;theoretical computer science;dynamic programming;genetics;algorithm;protein secondary structure	Comp.	13.206429607382995	-62.14004713858331	199665
096ef3f0b14e0589e1921d897b492c14fe0fcaf4	long-distance object recognition with image super resolution: a comparative study		Monitor systems are ubiquitously deployed in public areas. However, monitor systems face a major challenge regarding long-distance object recognition. Super-resolution constitutes a popular choice to address this challenge. Since super-resolution methods are used in many applications, it is necessary to understand these methods and make a comparative study of them. In this paper, we perform a comparative study on six super-resolution methods over two recognition algorithms. The paper evaluates super-resolution performance based on recognition accuracy, and serves as a summary assessment of image super-resolution algorithms.	algorithm;outline of object recognition;super-resolution imaging	Xiaomin Yang;Wei Wu;Kai Liu;Pyoung Won Kim;Arun Kumar Sangaiah;Gwanggil Jeon	2018	IEEE Access	10.1109/ACCESS.2018.2799861	iterative reconstruction;distributed computing;cognitive neuroscience of visual object recognition;superresolution;computer science;computer vision;artificial intelligence;image resolution	Vision	24.423011293031777	-55.54107907930597	199736
694f20659da09cf6ba56854902261570e349cff3	molecular dynamics simulation study on the structural stabilities of polyglutamine peptides	clinical data;huntingtin;pbc;hydrogen bond;huntington s disease;polyglutamine;protein aggregation;molecular dynamics;molecular dynamic simulation;moe;polyq;molecular dynamic;rmsd;md;structural stability;β helix	It is known that Huntington's disease patients commonly have glutamine (Q) repeat sequences longer than a critical length in the coding area of Huntingtin protein in their genes. As the polyglutamine (polyQ) region becomes longer than the critical length, the disease occurs and Huntingtin protein aggregates, both in vitro and in vivo, as suggested by experimental and clinical data. The determination of polyglutamine structure is thus very important for elucidation of the aggregation and disease mechanisms. Here, we perform molecular dynamics calculations on the stability of the structure based on the beta-helix structure suggested by Perutz et al. (2002) [Perutz, M.F., Finch, J.T., Berriman, J., Lesk, A., 2002. Amyloid fibers are water-filled nanotubes. Proc. Natl. Acad. Sci. USA 99, 5591]. We ensure that perfect hydrogen bonds are present between main chains of the beta-helix based on the previous studies, and perform simulations of stretches with 20, 25, 30, 37 and 40 glutamine residues (20Q, 25Q, 30Q, 37Q and 40Q) for the Perutz models with 18.5 and 20 residues per turn (one coil). Our results indicate that the structure becomes more stable with the increase of repeated number of Q, and there is a critical Q number of around 30, above which the structure of the Perutz model is kept stable. In contrast to previous studies, we started molecular dynamics simulations from conformations in which the hydrogen bonds are firmly formed between stacked main chains. This has rendered the initial beta-helix structures of polyQ much more stable for longer time, as compared to those proposed previously. Model calculations for the initial structures of polyQ dimer and tetramer have also been carried out to study a possible mechanism for aggregation.		Hajime Ogawa;Miki Nakano;Hirofumi Watanabe;E. B. Starikov;Stuart M. Rothstein;Shigenori Tanaka	2008	Computational biology and chemistry	10.1016/j.compbiolchem.2007.11.001	crystallography;biochemistry;molecular dynamics;root-mean-square deviation;chemistry;computational chemistry;mathematics;structural stability;protein aggregation;hydrogen bond;quantum mechanics	Comp.	10.246170315187786	-63.019165600390316	199805
a123c33dfbd0f997aaaa1d2dcd2b255deb62c1f2	using v1-based models for difference perception and change detection	contrast sensitivity;human vision and color perception;visual system	ABSTRACT Using V1 -based models, it is possible to investigate the features of human visual processing that influence difference perception and change detection. V1 -based models were built based on the same basic constructs of the human visual system, incorporating mechanisms of visual processing such as colour opponency, receptive field tuning, contrast sensitivity, linear and non -linear behaviour, and response po oling. Three studies were conducted to investigate the use of such models in difference perception and change detection. These studies demonstrate the various applications of human vision models and highlight several key considerations that need to be made when using them. 1. Introduction As knowledge about the human visual system grows, computational models aimed at predicting human visual behaviour have emerged and evolved. Earlier models were generally limited to predicting how humans perceive simple s timulisuch as lines and edges 2 . Recent models began to predict perception of realistic visual scenes, with some degree of success		Pei Ying Chua;Kenneth Kwok	2015		10.1117/12.2175690	computer vision;visual system;artificial intelligence;human visual system model	Vision	23.097695831257212	-65.99470528690746	199904
a7a761be06ef7b472f89416ee2c3edf48ddd3379	solvent design for crystallization of carboxylic acids	solvents;morphology;crystallization;computer aided molecular design;carboxylic acids;prediction model;product design;database search	Critical to crystallization chemical product design is the choice of an appropriate solvent. Traditional methods have focused on bench scale experiments using classes of solvents (e.g. polarity) with the different classes giving rise to different crystal morphologies. However, there are instances where some solvents belonging to a particular class give completely different morphology from other solvents in the same class. There has been some modeling effort aimed at predicting crystal morphology. A major drawback with some of these morphology prediction models is that they tend to be limited in application. It is clear that the solvent selection, with respect to crystal morphology cannot be carried out efficiently by just experimentation or modeling alone. This paper outlines a systematic methodology which combines targeted bench scale crystallization experiments, an efficient computer-aided molecular design (CAMD) approach and a database search approach for the design and selection of solvents for crystallization of carboxylic acids.		Arunprakash T. Karunanithi;Charles Acquah;Luke E. K. Achenie;Shanthakumar Sithambaram;Steven L. Suib	2009	Computers & Chemical Engineering	10.1016/j.compchemeng.2008.11.003	crystallography;stereochemistry;database search engine;morphology;computer science;organic chemistry;predictive modelling;crystallization;product design	SE	12.600220779962774	-59.19701401768742	199907
