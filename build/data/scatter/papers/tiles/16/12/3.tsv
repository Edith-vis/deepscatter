id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
7833b8aeb94a03ac1463dc12cb313184a65d7da2	contrast enhancement of back-light images via a regional rank-1 constraint	image resolution image enhancement;rank 1 back light images contrast enhancement dynamic range;histograms image color analysis image restoration benchmark testing dynamic range gray scale color;pixel wise gain correction scheme regional rank 1 constraint back light image contrast enhancement method textural details under exposed regions	In this paper, we propose a novel approach for enhancing the contrast of back-light images. Although many contrast enhancement methods have been proposed in the literatures, they are still prone to fail especially in restoring the textural details of the under-exposed regions in back-light images. To cope with this problem, the proposed approach employs a new pixel-wise gain correction scheme that adopts a regional rank-1 constraint. Based on this scheme, the contrast of the back-light image is enhanced notably while allowing smooth intensity changes and suppressing intensity inversions around adjacent pixels. In the experiments, the effectiveness of the proposed method is demonstrated qualitatively and quantitatively comparing with the state-of-the-art methods.	experiment;inversion (discrete mathematics);pixel	Ja-Won Seo;Seong-Dae Kim	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7351344	computer vision;image resolution;color image;computer graphics (images)	Vision	57.82532084172147	-61.50236223380263	91458
b0ae88e73cc6353797aeb4b0f2c772eb4a85364a	subjective assessment of super-resolution 4k video using paired comparison	video signal processing high definition television image reconstruction image resolution;image resolution;observers;international telecommunication union radiocommunication sector super resolution 4k video 4k television sets super resolution function super resolution image reconstruction nonlinear signal processing video equipment lanczos filter scheffe paired comparison test;video equipment;image quality;hdtv;signal resolution;observers image resolution image quality hdtv video equipment signal resolution	4K television (TV) sets are available in the retail market and some are advertised as featuring a super-resolution (SR) function. In particular, most rely on super-resolution image reconstruction (SRR) to provide high-resolution images. Recently, another SR technology that uses nonlinear signal processing (NLSP) and can work in real time has been proposed. A visual performance of the technology needs to be demonstrated subjectively because the performance of the video equipment is determined subjectively by the end user. The subjective performance of these two SR functions has not been verified. Here, subjective assessments comparing SRR and NLSP and a simple extension by the Lanczos filter (without SR) were conducted. 4K video equipment and 4K experimental videos were used. The assessments combined Scheffe's paired comparison test and BT.500, the International Telecommunication Union-Radiocommunication Sector's methodology for the subjective assessment of the quality of TV pictures. The assessment results were statistically analyzed, and the results indicated that NLSP demonstrated superior resolution.	4k resolution;image resolution;iterative reconstruction;lanczos resampling;netware link services protocol;nonlinear system;real-time computing;simpl;signal processing;super-resolution imaging;television set	Masaki Sugie;Seiichi Gohshi;Hirohisa Takeshita;Chinatsu Mori	2014	2014 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS)	10.1109/ISPACS.2014.7024422	computer vision;video;image processing;computer science;video quality;multimedia;video processing;computer graphics (images)	Arch	64.10178523205896	-61.60741785011037	91539
7d1ab1e24a995e3063b6a9907a1b80b6a044a3f1	a new image matching algorithm based on the relative position of intensity	satellite positioning;3d image reconstruction;relative position;illumination variation;image pixel relative intensity position;image resolution;image matching;satellite positioning image matching algorithm illumination variation noise pollution image pixel relative intensity position signal to noise ratio 3d image reconstruction computer vision;computer vision;signal noise ratio;image matching lighting economic indicators pollution measurement signal to noise ratio geometry noise robustness gaussian noise laboratories character generation;image reconstruction;image matching algorithm;noise pollution;signal to noise ratio;image resolution computer vision image matching image reconstruction	Illumination variations and noise pollution are the main factors that weaken most current image-matching algorithms. In order to get rid of these problems, a new image matching algorithm is proposed. It is based on every pixel's relative position of intensity (RPI) among its neighborhood instead of the intensity value of itself directly. A series of examples to demonstrate the algorithm and necessary results are also included. Experimental results reveal that the proposed RPI still can achieve correct matching under the condition of illumination variations, low signal noise ratio (SNR) and low fore-back ground contrast. This method serves as a general tool for high-precise image matching that are applicable to other vision problems such as 3-D reconstruction and satellite positioning.	algorithm;image registration;noise (electronics);pixel;signal-to-noise ratio	Xiaohang Hu;Shuangxia Pan;Han Bai	2007	2007 10th IEEE International Conference on Computer-Aided Design and Computer Graphics	10.1109/CADCG.2007.4407880	computer vision;electronic engineering;computer science;signal-to-noise ratio	EDA	59.667224855881024	-58.183353012878094	91595
525995c5c337bb9abf03e666819d18788e9fd3b1	image-based texture replacement using multiview images	multiview stereo;estimation algorithm;augmented reality;texture replacement	Augmented reality is concerned with combining real-world data, such as images, with artifcial data. Texture replacement is one such task. It is the process of painting a new texture over an existing textured image patch, such that depth cues are maintained. This paper proposes a general and automatic approach for performing texture replacement, which is based on multiview stereo techniques that produce depth information at every pixel. The use of several images allows us to address the inherent limitation of previous studies, which are constrained to specifc texture classes, such as textureless or near-regular textures. To be able to handle general textures, a modifed dense correspondence estimation algorithm is designed and presented.	algorithm;augmented reality;depth perception;pixel	Doron Tal;Ilan Shimshoni;Ayellet Tal	2008		10.1145/1450579.1450619	image texture;computer vision;augmented reality;computer science;multimedia;texture atlas;texture compression;texture filtering;computer graphics (images)	Vision	58.33829869254002	-53.718996474930236	91653
804c517689d2945766ff8504bce2633d9bcee341	development of an image/threshold database for designing and testing human vision models	databases;modelizacion;prediccion;human performance;base donnee;human vision;image processing;data compression;image communication;psicofisica;performance;specification;image database;database;procesamiento imagen;base dato;hombre;traitement image;modelisation;stimulus visuel;human vision and color perception;image compression;especificacion;visual stimulus;human visual system;image quality;human;vision science;psychophysique;algorithms;estimulo visual;compresion dato;rendimiento;modeling;vision;prediction;compression donnee;homme;psychophysics	Models that predict human performance on narrow classes of visual stimuli abound in the vision science literature. However, the vision and the applied imaging communities need robust general-purpose, rather than narrow, computational human visual system (HVS) models to evaluate image fidelity and quality and ultimately improve imaging algorithms. Psychophysical measures of image quality are too costly and time consuming to gather to evaluate the impact each algorithm modification might have on image quality. Several general-purpose early HVS models currently exist but direct comparisons of the models on the same data sets are rarely made, making it difficult to evaluate their utility. Moreover, researchers designing a new model are confronted with the decision of what data set to use to set model parameters. To address these issues about 60 researchers interested in vision modeling have formed a group tentatively called Modelfest. One of the group's goals is to develop a public database of test images with threshold data from multiple laboratories for designing and testing HVS models. The data set will be available on the WEB for all modelers to use in HVS model development. The group may also provide a threshold database with stimulus specifications derived from the existing vision literature for model design and testing. Although the space of possible stimuli is enormous, this first year's data collection effort is limited to detection thresholds for static gray-scale 2D images. In future years, the database may be extended to include discrimination (masking) as well as detection thresholds for dynamic, color and gray scale image sequences. The purpose of this first report is to invite the Vision Science and Electronic Imaging community to participate in this effort and inform them of the developing data set, which will be available to all interested researchers. This first paper presents the display specifications, psychophysical methods and stimulus definitions for the Year One effort. The threshold data will be collected by each of the authors over the next few months and presented on the WEB along with the stimuli.	algorithm;color;computation;database;general-purpose markup language;general-purpose modeling;grayscale;human reliability;human visual system model;image quality;world wide web	Thom Carney;Stanley A. Klein;Christopher W. Tyler;D. Amnon Silverstein;Brent Beutter;Dennis Levi;Andrew B. Watson;Adam J. Reeves;Anthony Matthew Norcia;Chien-Chung Chen;Walter Makous;Miguel P. Eckstein	1999		10.1117/12.348473	computer vision;simulation;computer science;artificial intelligence	Vision	63.342079796922064	-61.78977849352722	91851
6be966504d72b77886af302cf7e28a7d5c14e624	multiview projectors/cameras system for 3d reconstruction of dynamic scenes	moving object;optimisation;image motion analysis;video signal processing;three dimensional;scene reconstruction;optical projectors;video signal processing cameras image motion analysis image reconstruction mesh generation optical projectors optimisation;shape;vectors;three dimensional displays;image color analysis;image reconstruction;noise reduction;video frame rate multiview projector camera system 3d dynamic scene reconstruction active vision systems moving object capture plane parameter optimization light sectioning algorithm noise reduction algorithm mesh generation algorithm;optimization;mesh generation;cameras shape three dimensional displays image reconstruction image color analysis optimization vectors;3d reconstruction;parameter optimization;cameras;dynamic scenes;active vision	Active vision systems are usually limited to either partial or static scene reconstructions. In this paper, we propose to acquire the entire 3D shape of a dynamic scene. This is performed using a multiple projectors and cameras system, that allows to recover the entire shape of the object within a single scan at each frame. Like previous approaches, a static and simple pattern is used to avoid interferences of multiple patterns projected on the same object. In this paper, we extend the technique to capture a dense entire shape of a moving object with accuracy and high video frame rate. To achieve this, we mainly propose two additional steps; one is checking the consistency between the multiple cameras and projectors, and the other is an algorithm for light sectioning based on a plane parameter optimization. In addition, we also propose efficient noise reduction and mesh generation algorithm which are necessary for practical applications. In the experiments, we show that we can successfully reconstruct dense entire shapes of moving objects. Results are illustrated on real data from a system composed of six projectors and six cameras that was actually built.	3d reconstruction;active vision;algorithm;error detection and correction;experiment;mathematical optimization;mesh generation;movie projector;multiview video coding;noise reduction;scene graph	Ryo Furukawa;Ryusuke Sagawa;Amaël Delaunoy;Hiroshi Kawasaki	2011	2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)	10.1109/ICCVW.2011.6130441	3d reconstruction;iterative reconstruction;three-dimensional space;mesh generation;computer vision;active vision;shape;computer science;noise reduction;mathematics;computer graphics (images)	Vision	54.907886770457964	-53.225597325487776	91905
322d1a0311dcef63249343f0215668f2dfb43610	estimating all frequency lighting using a color/depth image	scene analysis frequency lighting estimation color image depth image spherical harmonic basis function nondiffuse reflection scene geometry high frequency lighting;geometry;scene analysis and understanding inverse rendering all frequency illumination lighting estimation brdf;image colour analysis;natural scenes;natural scenes geometry image colour analysis;lighting vectors frequency estimation image color analysis color estimation rendering computer graphics	This paper presents a novel approach to estimating the lighting using a pair of one color and one depth image. To effectively model all frequency lighting, we introduce a hybrid representation for lighting; the combination of spherical harmonic basis functions and point lights. Upon the existing framework of spherical harmonics based diffuse reflection, we divide the color image into diffuse and non-diffuse reflections. Then, we use the diffuse reflection for estimating the low frequency lighting. For high frequency lighting, we obtain the specular reflections by analyzing the non-diffuse reflection. Knowing specular reflections and scene geometry, we are capable of computing the direction of point lights, inverting the reflected ray with respect to the surface normal. Then, we optimize the intensity of point lights by analysis by synthesis paradigm. By superimposing the low and high frequency lighting, we recover the lighting present in the scene. While existing methods use the low frequency lighting to infer the high frequency lighting, we propose to use the nondiffuse reflection for directly estimating the high frequency lighting. In this way, we make good use of the non-diffuse reflections in scene analysis and understanding. Experimental results show that the proposed approach is an effective solution for the lighting estimation of real world environment.	basis function;color depth;color image;diffuse reflection;normal (geometry);programming paradigm;ray (optics);reflection (computer graphics);speech coding	Hyunjung Shim	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6466922	computer vision;rendering;volumetric lighting;mathematics;per-pixel lighting;global illumination;image-based lighting;spherical harmonic lighting;computer graphics (images)	Vision	59.293552619437065	-53.20147507032639	91929
c07f00d7adc57f621de35b1ba46e00c9df321925	forensics of image blurring and sharpening history based on nsct domain	computers;vectors image enhancement image forensics image restoration transforms;vectors;feature extraction;decision support systems;transforms;similarity calculation multi manipulated image detection digital image forensic technologies single manipulated detection methods image manipulation history image blurring image sharpening nonsubsampled contourlet transform domain nsct domain extremum feature local directional similarity vector directional subband;correlation;conferences;decision support systems vectors feature extraction correlation transforms computers conferences	Detection of multi-manipulated image has always been a more realistic direction for digital image forensic technologies, which extremely attracts interests of researchers. However, mutual affects of manipulations make it difficult to identify the process using existing single-manipulated detection methods. In this paper, a novel algorithm for detecting image manipulation history of blurring and sharpening is proposed based on non-subsampled contourlet transform (NSCT) domain. Two main sets of features are extracted from the NSCT domain: extremum feature and local directional similarity vector. Extremum feature includes multiple maximums and minimums of NSCT coefficients through every scale. Under the influence of blurring or sharpening manipulation, the extremum feature tends to gain ideal discrimination. Directional similarity feature represents the correlation of a pixel and its neighbors, which can also be altered by blurring or sharpening. For one pixel, the directional vector is composed of the coefficients from every directional subband at a certain scale. Local directional similarity vector is obtained through similarity calculation between the directional vector of one random selected pixel and the directional vectors of its 8-neighborhood pixels. With the proposed features, we are able to detect two particular operations and determine the processing order at the same time. Experiment results manifest that the proposed algorithm is effective and accurate.	algorithm;box blur;coefficient;computer forensics;contourlet;digital image;experiment;futures studies;maxima and minima;pixel;sensor;support vector machine	Yahui Liu;Yao Zhao;Rongrong Ni	2014	Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific	10.1109/APSIPA.2014.7041728	computer vision;feature detection;machine learning;pattern recognition;mathematics	Vision	54.973303637123976	-63.391137934433424	91968
278b31db3e36002b5c2cd642c7225593d14d3800	hatching for 3d prints: line-based halftoning for dual extrusion fused deposition modeling		This work presents a halftoning technique to manufacture 3D objects with the appearance of continuous grayscale imagery for Fused Deposition Modeling (FDM) printers. While droplet-based dithering is a common halftoning technique, this is not applicable to FDM printing, since FDM builds up objects by extruding material in semi-continuous paths. The line-based halftoning principle called ’hatching’ is applied to the line patterns naturally occuring in FDM prints, which are built up in a layer-by-layer fashion. The proposed halftoning technique isn’t limited by the challenges existing techniques face; existing FDM coloring techniques greatly influence the surface geometry and deteriorate with surface slopes deviating from vertical or greatly influence the basic parameters of the printing process and thereby the structural properties of the resulting product. Furthermore, the proposed technique has little effect on printing time. Experiments on a dual-nozzle FDM printer show promising results. Future work is required to calibrate the perceived tone. c © 2018 Elsevier B.V. All rights reserved.	algorithm;chemical vapor deposition;color space;continuous tone-coded squelch system;convex hull;dither;dual-tone multi-frequency signaling;experiment;finite difference method;fused filament fabrication;glossary of computer graphics;graph coloring;grayscale;hogging and sagging;leo (computer);modulation;monochrome;normal (geometry);physical vapor deposition;printer (computing);printing;semi-continuity;semiconductor industry;specularity;subsurface (software);triangulated irregular network;viewing angle	Tim Kuipers;Willemijn Elkhuizen;Jouke Verlinden;Eugeni Doubrovski	2018	Computers & Graphics	10.1016/j.cag.2018.04.006	computer science;computer vision;grayscale;artificial intelligence;fused deposition modeling;hatching;dither;extrusion	Graphics	60.47505268934164	-52.261605258571954	92100
ff402258b93733c27fdf54e67e790b2b3bfd8604	reduction in impulse noise in digital images through a new adaptive artificial neural network model	impulse noise;image restoration;image enhancement;neural network	In this paper, an adaptive artificial neural network model is developed in order to restore severely corrupted images. The proposed new and effective impulse noise reduction filter is named as adaptive neural network models with an algorithm based on artificial neural networks. Networks trained at different noise intensities get activated according to the intensity of the noise and estimate the most suitable neighboring pixel that can replace the corrupted pixel. The proposed algorithm reduces impulse noise effectively while also protecting the details. Experimental results show that the proposed algorithm performs better compared with other traditional filters.	algorithm;artificial neural network;digital image;extended precision;impulse noise (audio);iteration;network model;noise reduction;peak signal-to-noise ratio;pixel;simulation	Cafer Budak;Mustafa Türk;Abdullah Toprak	2014	Neural Computing and Applications	10.1007/s00521-014-1767-x	image restoration;computer vision;impulse noise;computer science;machine learning;artificial neural network	ML	56.56357672733905	-65.42501192376051	92170
5609154761b155366283f1a4f73822da844c1acc	design concepts for an on-board parallel image processor	parallel imaging	Abstract   A compact parallel image processing system concept has been developed. The main features of this system is the use of off-axis paraboloidal mirror segments as collimating, Fourier transforming and image reconstructing elements, and the use of a GaAs laser diode as the coherent radiation source. Preliminary experiments to demonstrate the usefulness of this system have been performed.	image processor;on-board data handling	Akram S. Husain-Abidi	1973	Pattern Recognition	10.1016/0031-3203(73)90004-6	computer vision;computer science	Vision	64.45643135793698	-57.209341063811344	92350
51e8e8c4cac8260ef21c25f9f2a0a68aedbc6d58	deep generative adversarial compression artifact removal		Compression artifacts arise in images whenever a lossy compression algorithm is applied. These artifacts eliminate details present in the original image, or add noise and small structures; because of these effects they make images less pleasant for the human eye, and may also lead to decreased performance of computer vision algorithms such as object detectors. To eliminate such artifacts, when decompressing an image, it is required to recover the original image from a disturbed version. To this end, we present a feed-forward fully convolutional residual network model trained using a generative adversarial framework. To provide a baseline, we show that our model can be also trained optimizing the Structural Similarity (SSIM), which is a better loss with respect to the simpler Mean Squared Error (MSE). Our GAN is able to produce images with more photorealistic details than MSE or SSIM based networks. Moreover we show that our approach can be used as a pre-processing step for object detection in case images are degraded by compression to a point that state-of-the art detectors fail. In this task, our GAN method obtains better performance than MSE or SSIM trained networks.	algorithm;baseline (configuration management);compression artifact;computer vision;experiment;flow network;lossy compression;mean squared error;network model;object detection;preprocessor;sensor;structural similarity	Leonardo Galteri;Lorenzo Seidenari;Marco Bertini;Alberto Del Bimbo	2017	2017 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2017.517	machine learning;residual;artificial intelligence;lossy compression;computer vision;iterative reconstruction;object detection;computer science;structural similarity;pattern recognition;compression artifact;image resolution;mean squared error	Vision	56.9055979166104	-59.779326339407106	92487
e4208b642ca9070cd77f755d27ca9d3fc9bce8d8	3d analysis and image-based rendering for immersive tv applications	analisis imagen;video object;image tridimensionnelle;rendu image;television 3 dimensiones;image processing;restitucion imagen;three dimensional television;perception profondeur;procesamiento imagen;digital television;traitement image;depth perception;image rendering;tridimensional image;image analysis;image based rendering;motion parallax;television 3 dimensions;analyse image;3d display;imagen tridimensional	Abstract   Depth perception in images and video has been a relevant research issue for years, with the main focus on the basic idea of “stereoscopic” viewing. However, it is well known from the literature that stereovision is only one of the relevant depth cues and that motion parallax, as well as color, brightness and geometric appearance of video objects are at least of the same importance and that their individual influence mainly depending on the object distance. Thus, for depth perception it may sometimes be sufficient to watch pictures or movies on large screens with brilliant quality or to provide head-motion parallax viewing on conventional 2D displays. Based on this observation we introduce an open, flexible and modular immersive TV system that is backwards compatible to today's 2D digital television and that is able to support a wide range of different 2D and 3D displays. The system is based on a three-stage concept and aims to add more and more depth cues at each additional layer.		Christoph Fehn;Eddie Cooke;Oliver Schreer;Peter Kauff	2002	Sig. Proc.: Image Comm.	10.1016/S0923-5965(02)00079-6	parallax;computer vision;image analysis;image-based modeling and rendering;stereo display;digital television;image processing;depth perception;computer science;multimedia;computer graphics (images)	Visualization	61.03048416961803	-54.772813739005564	92940
4d30e28f04263cffaa17668fb21d309a77d355d7	hidden surface removal in full-parallax cghs by silhouette approximation	hidden surface removal;3d image	In this paper we report a method of removing hidden surfaces in full-parallax computer-generated holograms. First, we examine the handling of light shielding by tilted patches in terms of wave optics, and then propose a silhouette approximation. In addition, we derive a recurrence formula to synthesize object light waves with hidden surface removal by the proposed method. We show that appearance of optically reconstructed surface object of the full-parallax holograms generated by the proposed method varies in a natural manner as the observation point is changed. This study achieves hidden surface removal in full-parallax holograms, which was believed to be possible only if vertical parallax is ignored. © 2007 Wiley Periodicals, Inc. Syst Comp Jpn, 38(6): 53–61, 2007; Published online in Wiley InterScience (). DOI 10.1002&sol;scj.20345 Copyright © 2004 Wiley Periodicals, Inc.	approximation;hidden surface determination;parallax	Akinobu Kondoh;Kyoji Matsushima	2007	Systems and Computers in Japan	10.1002/scj.20345	stereoscopy;computer vision;hidden surface determination;computer science;artificial intelligence;computer graphics (images)	Robotics	59.975492115390665	-52.42196881198637	93281
028b8fd079227294f591d65b912fb8ad21fa199d	monocular 3d vision using real-time generated scene with depth of field effect	3d imaging;real time;depth of field;3 d display;real time stereogram;human vision system;3d vision;3d display;monocular stereoscopic display	The human vision system has visual functions for viewing 3D images with a correct depth. These functions are called accommodation, vergence and binocular stereopsis. Most 3D display system utilizes binocular stereopsis. The authors have developed a monocular 3D vision system with accommodation mechanism, which is useful function for perceiving depth.	binocular vision;field effect (semiconductor);nvidia 3d vision;real-time locating system;stereo display;stereopsis;vergence	Takashi Hosomi;Kunio Sakamoto	2009		10.1007/978-3-642-04052-8_35	computer stereo vision;stereoscopy;computer vision;stereo display;monocular vision;stereopsis;optics;autostereogram;stereoblindness;computer graphics (images)	Vision	61.98476375326583	-55.99522318328993	93341
55ed3b45dcb55dcbbd88f9c01acd701994ffa547	constrained nmf for multiple exhibition on a single display	image resolution;matrix decomposition display devices image sequences;block principal pivoting technique constrained nmf multiple exhibition single display temporal psycho visual modulation display paradigm personalized views independent views nonnegative matrix factorization image sequences nmf glass free viewing tpvm display system	Temporal Psycho Visual Modulation (TPVM) was proposed recently as a new display paradigm that enables multi-exhibition on a single screen: people with different viewing devices can see independent and personalized views. Early studies have shown that TPVM can be implemented by nonnegative matrix factorization (NMF) of image sequences with additional constraints. In this paper, we introduce an algorithm that makes possible the glass-free viewing in a TPVM display system. More specifically, we design new constraints for the NMF problem and derive the solution using the block principal pivoting technique. The proposed algorithm can be readily implemented in a TPVM display system to generate the display frames. Experimental results are provided to show the effectiveness of the proposed algorithm.	algorithm;modulation;non-negative matrix factorization;personalization;programming paradigm	Lihao Wang;Guangtao Zhai	2015	2015 Picture Coding Symposium (PCS)	10.1109/PCS.2015.7170093	computer vision;image resolution;computer science;mathematics;multimedia;computer graphics (images)	Graphics	59.86413270918448	-54.03900574016503	93603
43d24d110ed38550b70d5865bb8f167a96ab79cb	enabling reliable change detection for independently compressed sar images		This paper develops distortion metrics for compressed synthetic aperture radar (SAR) imagery from change detection test statistics. These metrics are used to predict lossy image compression’s impact on change detection performance. The metrics do not require the intended change detection comparison image to provide these benefits. An SAR compression system leveraging the distortion metrics is proposed. The system generates a bad-pixel mask highlighting potential false alarms that are generated due to compression and are subsequently discarded in the change detection process. The proposed system’s performance is demonstrated through noncoherent change detection analysis after JPEG2000 and JPEG image compression. Similarly, a coherent change detection system is evaluated after JPEG2000 image compression. For noncoherent change detection at large compression ratios (CRs) using JPEG2000, the proposed system provides a 33% reduction in false alarms at a 0.1 probability of detection as well as the ability to maintain near-distortionless false alarm rates across a wide range of CRs. At a 0.1 probability of detection for coherent change detection, the system provides a 37% reduction in false alarms at modest CRs. The coherent change detection system is also demonstrated to maintain low false alarm rates across a range of CRs.	charge-coupled device;coherence (physics);data compression ratio;defective pixel;distortion;image compression;image quality;jpeg 2000;lossy compression;mathematical optimization;rate–distortion optimization;synthetic data	Christopher D. McGuinness;Eric J. Balster	2017	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2017.2701200	lossy compression;artificial intelligence;image compression;mathematics;remote sensing;false alarm;computer vision;distortion;transform coding;change detection;jpeg;jpeg 2000	Vision	66.90678773154657	-62.520300555485804	93605
bf932c53102ea38ab65ec0c5c3e4b05ae79dbc82	fake fur rendering	animals;fur;surface rendering;probabilistic model;natural phenomena;large classes;surface area;anisotropic shading	A probabilistic lighting model is presented for thin coats of fur over skin. Previous methods for rendering furry objects and creatures have addressed the case where individual strands or tufts of hair may be resolvable at the pixel level. These methods are often computationally intensive. However, a large class of realworld cases where individual hairs are much smaller than the size of a pixel can be addressed using a probabilistic model for the expected value of reflected light within a small surface area. Under the assumption that hair parameters are slowly varying across the skin, lighting calculations are performed on a reference hair with prefiltered parameters. The reflected light from individual hairs and from the skin below is blended using the expectation of a ray striking a hair in that area as the opacity of the fur coating. Approximations for hair-to-hair shadowing and hair-to-skin shadowing can be made using the same hit-expectation model. Our system can be implemented in existing commercial surfacerendering software at a much lower computational cost than typical resolvable-hair methods. CR	algorithmic efficiency;approximation;computation;pixel;rendering (computer graphics);shading;skin (computing);statistical model	Dan B. Goldman	1997		10.1145/258734.258807	statistical model;fur;surface area;geometry;statistics;computer graphics (images)	Graphics	63.78395727306732	-52.373366019384086	93620
8278459e6f0882b2a66a5a6dae52d2324456bfa7	color compensation using nonlinear luminance-rgb component curve of a camera	color saturation;accurate color value;nonlinear luminance-rgb component curve;gray scale image;color component;digital image;color image processing method;color compensation;color mapping;methods result;novel color mapping method;color enhancement	color saturation;accurate color value;nonlinear luminance-rgb component curve;gray scale image;color component;digital image;color image processing method;color compensation;color mapping;methods result;novel color mapping method;color enhancement		Sejung Yang;Yoon-Ah Kim;Chaerin Kang;Byung-Uk Lee	2011		10.1007/978-3-642-24031-7_62	demosaicing;color histogram;false color;rgb color model;computer vision;icc profile;color quantization;hsl and hsv;color normalization;color depth;color image;mathematics;color balance;color space;computer graphics (images)	Robotics	59.17004881023554	-62.318902842468844	93671
4b73748bd6610d6086f6c5173aefb1f6d2876bdd	target-aided compression of sar imagery for transmission over noisy wireless channels	discrete wavelet transforms;channel coding;wireless channels;image coding;error concealment;image resolution;constant false alarm rate;contextual information;noise measurement;image resolution laplace equations lead satellites discrete wavelet transforms noise measurement image coding;laplace equations;lead;region of interest;satellites;sar image;error resilience;target detection;synthetic aperture radar	In this paper, we present a target-aided, error-resilient system for coding synthetic aperture radar (SAR) imagery, whereby regions of interest and background information are coded independently of each other. A multiresolution constant-false-alarm-rate (CFAR) detection scheme is utilized to discriminate between target regions and natural clutter. Based upon the detected target regions, we apply less compression to targets, and more compression to background data. This methodology preserves relevant features of targets for further analysis, and preserves the background only to the extent of providing contextual information. The resilience to channel errors is obtained without the use of channel coding or error concealment techniques. The robust nature of the coder eliminates the appearance of impulsive channel-error-induced artifacts in the decoded imagery, while increasing the security level of the encoded bit stream. The proposed coder is evaluated in terms of target detection performance on the decoded SAR image for a wide variety of channel conditions and encoding bit rates. The resulting system dramatically reduces the bandwidth requirements of the digital SAR imagery, and is shown to provide outstanding performance in adverse channel conditions.	aperture (software);artifact (error);bitstream;channel capacity;clutter;code;constant false alarm rate;error concealment;forward error correction;region of interest;requirement;synthetic data	Robert J. Bonneau;Glen P. Abousleman	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5745387	computer vision;lead;synthetic aperture radar;image resolution;channel code;computer science;noise measurement;constant false alarm rate;satellite;region of interest	Visualization	66.83637147979219	-62.50441582862367	93674
c08ce05d1d096fb8410859b2134f38b3db308b6f	spatial effects of video compression on classification in convolutional neural networks		A collection of computer vision applications reuse pre-learned features to analyse video frame-by-frame. Those features are classically learned by Convolutional Neural Networks (CNN) trained on high quality images. However, available video content is almost always subject to compression which is nearly never considered during the analysis process. In this paper, we present an empirical study to measure how the visual discrepancy of compressed data limit the learning performance of the CNN model. The learning performance is evaluated using a benchmark of synthetic datasets compressed at various levels using H.264/AVC. We measure the image quality quantitatively using classical evaluation metrics such as Peak Signal to Noise Ratio and Structural SIMilarity. A cross-evaluation is performed to measure the robustness of the CNN model in processing for a wide range of quality-varying visual data. Our experimental results have shown that the performance of the CNN depends on the compression rate. The results show that, in general, higher compression results in lower performance. However performance on lower quality test data can be improved by using lower quality data for CNN training. Finally, our work demonstrates that conditioning the CNN with the compression properties could potentially lead to better learning.		Pamela Johnston;Eyad Elyan;Chrisina Jayne	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8489370	convolutional neural network;artificial intelligence;machine learning;robustness (computer science);data compression;peak signal-to-noise ratio;pattern recognition;image quality;transform coding;structural similarity;computer science;data compression ratio	Vision	63.07978156627217	-64.99859488485244	93730
654d679d02708516d82c4acad457a479f4be1636	dense scattering layer removal	haze removal;noise reduction	We propose a new model, together with advanced optimization, to separate a thick scattering media layer from a single natural image. It is able to handle challenging underwater scenes and images taken in fog and sandstorm, both of which are with significantly reduced visibility. Our method addresses the critical issue -- this is, originally unnoticeable impurities will be greatly magnified after removing the scattering media layer -- with transmission-aware optimization. We introduce non-local structure-aware regularization to properly constrain transmission estimation without introducing the halo artifacts. A selective-neighbor criterion is presented to convert the unconventional constrained optimization problem to an unconstrained one where the latter can be efficiently solved.	constrained optimization;constraint (mathematics);distance fog;mathematical optimization;optimization problem	Qiong Yan;Li Xu;Jiaya Jia	2013		10.1145/2542355.2542373	computer vision;simulation;telecommunications;computer science;noise reduction	Vision	57.53993950271501	-59.73051566429449	93983
9c27aa472c1a7305db495e297e3959d4e66a55e7	a single image de-hazing algorithm based on hybrid filter	scattering;image restoration;filtering algorithms;image edge detection;image color analysis;atmospheric waves	Images captured in foggy weather often suffer from bad visibility, because of bad atmospheric visibility. The existence of particles, dust and water impurities always fade the color and lead to a bad contrast of the observed objects. And then, we will feel indistinct by our eyes. We introduce a novel algorithm of single image that based on hybrid filter in this paper. The algorithm is based on an advanced median filter, and combined with average filter. The visibility of photos has been greatly improved by use of iteration. Experimental results on a variety of haze images demonstrate the effectiveness of the advanced method, which contains the fast speed of Median filter and also gets more distinct in details compared with other algorithms. The main advantage of the proposed algorithm compared with others is its speed. This speed allows visibility restoration to be applied for the first time within real-time processing applications such as sign, lane-marking and obstacle detection.	algorithm;autostereogram;circuit restoration;color image;item unique identification;iteration;median filter;real-time clock	Weili Wang;Youguang Chen	2016	2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)	10.1109/CISP-BMEI.2016.7852703	image restoration;computer vision;atmospheric wave;optics;scattering;computer graphics (images)	Vision	57.831775116981284	-59.9203258975815	94086
d67222052f4907561681b88ebdacd8444c5b81c3	an imaging problem: restoration of blurred digital characters	image processing;restoration;character;traitement image;algorithme;algorithm;algorritmo;caractere;restauration;modification	Abstract   The restoration or sharpening of blurred images of letters or other characters differs from the general restoration problem in that the original object is known to be quantized to just two levels, 0 and 1. Two nonlinear modifications of a classical method for the solution of integral equations are suggested as candidates for a fast, accurate, restoration algorithm. Letters defined by a digital matrix were blurred by spreading each matrix element uniformly over the area of four matrix elements. Under this type of blurring, van Cittert's solution by the method of successive substitutions is apparently unsuited because it generates a noncorvergent series. However, successive stages have been shown to sometimes improve before ultimately diverging; therefore one or two stages of correction followed by requantization has practical potential. Alternatively, theory shows that convergence could be reinstated by further deliberate blurring before restoration. Ordinarily deliberate blurring would be avoided on the grounds that the signal-to-noise ratio would deteriorate; but subsequent requantization to the levels might invalidate these grounds. Tests of the latter idea, using block capitals digitized on a 7 × 5 matrix, which is about as coarse as one can use, have proved favorable. The 2-stage version of the procedure is rather complicated when arrived at via the conceptual approach described, but a simple negative feed-forward flow diagram has been found which is equivallent and which greatly simplifies the restoration procedure.	circuit restoration	R. N. Bracewell	1985	Computer Vision, Graphics, and Image Processing	10.1016/0734-189X(85)90129-X	image restoration;computer vision;image processing;computer science;artificial intelligence;mathematics;geometry;character;algorithm	Vision	55.77285428230194	-62.139626378609	94112
aae099afce25c02958980c475b16e01bcd927c9c	high efficiency depth image-based rendering with simplified inpainting-based hole filling	hole filling;inpainting;view synthesis;crack filling;depth image based rendering dibr;graphics processing unit gpu	Hole and crack filling is the most important issue in depth-image-based rendering (DIBR) algorithms for generating virtual view images when only one view image and one depth map are available. This paper proposes a priority patch inpainting algorithm for hole filling in DIBR algorithms by generating multiple virtual views. A texture-based interpolation method is applied for crack filling. Then, an inpainting-based algorithm is applied patch by patch for hole filling. A prioritized method for selecting the critical patch is also proposed to reduce computation time. Finally, the proposed method is realized on the compute unified device architecture parallel computing platform which runs on a graphics processing unit. Simulation results show that the proposed algorithm is 51-fold faster for virtual view synthesis and achieves better virtual view quality compared to the traditional DIBR algorithm which contains depth preprocessing, warping, and hole filling.		Pin-Chen Kuo;Jhih-Ming Lin;Bin-Da Liu;Jar-Ferr Yang	2016	Multidim. Syst. Sign. Process.	10.1007/s11045-015-0378-8	computer vision;computer science;theoretical computer science;inpainting;computer graphics (images)	Visualization	66.61912356697219	-52.115829549043106	94544
bc1d3d21d83df0dbc6169e3b49718d59faab50cd	learning data-driven image similarity measure	complexity theory;standards;indexes;quality assessment;feature extraction;image quality;optimization	Image quality assessment gains a greater interest due to development of digital imaging and storage. In that field, structural similarity (SSIM) index has been shown to favorably agree with human perceptual assessment, significantly outperforming the method of mean squared error, i.e., L2 distance. The similarity measure function in SSIM which compares a target (distorted) image with its reference (original) image is handcrafted in a simple form via a top-down approach based on the human visual system. It, however, might lack optimality without directly considering the relationships between image data and the perceptual assessment (scores). In this paper, we propose a method to construct an image similarity measure based on actual data. The proposed method optimizes a similarity measure function by exploiting annotated data in a bottom-up and data-driven manner, while retaining the favorable property of structural similarity in SSIM. The non-linear similarity function is optimized as the global optimum of high generalization power. In addition, the proposed method is simply formulated and thus applicable to the family of SSIM, especially to FSIM which has been recently proposed exhibiting superior performance to SSIM. The experimental results on image quality assessment demonstrate the effectiveness of the proposed method compared to the other methods.	bottom-up parsing;bottom-up proteomics;digital imaging;experiment;global optimization;image quality;mean squared error;nonlinear system;overfitting;pattern recognition;similarity measure;structural similarity;top-down and bottom-up design	Takumi Kobayashi	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7900206	image quality;database index;computer vision;feature extraction;computer science;artificial intelligence;machine learning;data mining;mathematics	Vision	62.754958926811106	-65.48636488722705	95161
1db1338c999d2cfab7adcf5a4bd695f8896f259d	near-invariant blur for depth and 2d motion via time-varying light field analysis	depth and motion invariant photography;extended depth of field;defocus and motion deblurring	Recently, several camera designs have been proposed for either making defocus blur invariant to scene depth or making motion blur invariant to object motion. The benefit of such invariant capture is that no depth or motion estimation is required to remove the resultant spatially uniform blur. So far, the techniques have been studied separately for defocus and motion blur, and object motion has been assumed 1D (e.g., horizontal). This article explores a more general capture method that makes both defocus blur and motion blur nearly invariant to scene depth and in-plane 2D object motion. We formulate the problem as capturing a time-varying light field through a time-varying light field modulator at the lens aperture, and perform 5D (4D light field + 1D time) analysis of all the existing computational cameras for defocus/motion-only deblurring and their hybrids. This leads to a surprising conclusion that focus sweep, previously known as a depth-invariant capture method that moves the plane of focus through a range of scene depth during exposure, is near-optimal both in terms of depth and 2D motion invariance and in terms of high-frequency preservation for certain combinations of depth and motion ranges. Using our prototype camera, we demonstrate joint defocus and motion deblurring for moving scenes with depth variation.	deblurring;gaussian blur;light field;modulation;motion estimation;prototype;resultant	Yosuke Bando;Henry Holtzman;Ramesh Raskar	2013	ACM Trans. Graph.	10.1145/2451236.2451239	computer vision;motion estimation;mathematics;optics;motion field;computer graphics (images)	Graphics	63.51662427660496	-53.93181769658835	95323
013d5cea3bdaa3c7d09a62c94aaeb41d7a30cede	discontinuity-adaptive shape from focus using a non-convex prior	shape estimation;cost function;markov random field;shape from focus;3d structure	Shape from focus (SFF) is a widely used technique for determining the 3D structure of textured microscopic objects. However, SFF output depends critically on the number of observations used and the focus measure operator adopted. In this paper, we propose a new SFF method that can provide rich structure information given limited number of observations. We observe that depth is non-linearly related to the observations and pose the shape estimation as a minimization problem within a Maximum A Posteriori (MAP) - Markov Random Field (MRF) framework. We incorporate a discontinuity-adaptive MRF prior for the underlying structure. The resulting cost function is non-convex in nature which we minimize using Graduated non-convexity algorithm. When tested on synthetic as well as real objects, the results obtained are quite impressive.	reflections of signals on conducting lines	Krishnamurthy Ramnath;A. N. Rajagopalan	2009		10.1007/978-3-642-03798-6_19	computer vision;mathematical optimization;pattern recognition;mathematics	Vision	55.59923881617403	-53.60709109445683	95337
8d48bc4b0b6ea9c2f024c783cd8aa980f72300fc	a resolution enhancement algorithm for an asymmetric resolution stereo video	signal image and speech processing;biometrics;pattern recognition;image processing and computer vision	This paper presents a resolution enhancement algorithm in an asymmetric resolution stereo video for improving the video resolution. In this stereo video architecture, a scene is captured by two cameras to form two views, one view is a lower-resolution video and the other is a full-resolution video. The goal is to enhance the lower-resolution video to a full-resolution video. In the lower-resolution video, frames synchronized with full-resolution video are enhanced via disparity estimation algorithm, while the rest frames are improved by mono-view video super-resolution based on key frames method. The experimental results demonstrate that the proposed method is effective for both visual and objective qualities.	algorithm;binocular disparity;display resolution;key frame;super-resolution imaging	Jing Ge;Ju Liu;Yunlong Zhao;Boyang Zhang	2015	EURASIP J. Image and Video Processing	10.1186/s13640-015-0079-0	video compression picture types;stereo cameras;computer vision;uncompressed video;computer science;video capture;archaeology;video tracking;pattern recognition;block-matching algorithm;multimedia;video processing;motion compensation;video post-processing;video denoising;biometrics;multiview video coding;computer graphics (images)	Vision	56.837608551258896	-55.234664013766135	95399
f665104afe32d7a8278c2a7c11f84cbbeee4b13c	a fusion framework for camouflaged moving foreground detection in the wavelet domain		Detecting camouflaged moving foreground objects has been known to be difficult due to the similarity between the foreground objects and the background. Conventional methods cannot distinguish the foreground from background due to the small differences between them and thus suffer from under-detection of the camouflaged foreground objects. In this paper, we present a fusion framework to address this problem in the wavelet domain. We first show that the small differences in the image domain can be highlighted in certain wavelet bands. Then the likelihood of each wavelet coefficient being foreground is estimated by formulating foreground and background models for each wavelet band. The proposed framework effectively aggregates the likelihoods from different wavelet bands based on the characteristics of the wavelet transform. Experimental results demonstrated that the proposed method significantly outperformed existing methods in detecting camouflaged foreground objects. Specifically, the average F-measure for the proposed algorithm was 0.87 compared with 0.71 to 0.8 for the other state-of-the-art methods.	algorithm;bands;coefficient;match moving;physical object;sensor;wavelet transform	Shuai Li;Dinei A. F. Florêncio;Wanqing Li;Yaqin Zhao;Chris Cook	2018	IEEE Transactions on Image Processing	10.1109/TIP.2018.2828329	fusion;wavelet;computer vision;wavelet transform;artificial intelligence;foreground detection;pattern recognition;mathematics	Vision	54.89829897073523	-59.608688417694964	95415
ae8bdac31967f3222e05e7b2710627cea4a84464	feature and noise adaptive unsharp masking based on statistical hypotheses test	image features;testing frequency aquaculture adaptive filters boosting computer vision image enhancement nonlinear filters background noise filtering;statistical hypotheses test;noise adaptive unsharp masking;testing;indexing terms;statistical hypothesis testing;statistical testing image enhancement;artificial neural networks;image enhancement;adaptive filters;hypotheses testing;local contrast enhancement noise adaptive unsharp masking statistical hypotheses test image enhancement noise detection;image edge detection;feature extraction;statistical testing;noise detection;correlation;local contrast enhancement;high frequency;noise	The conventional unsharp masking (UM) enhances the visual appearances of images by adding their amplified high frequency components. However, the noise component of the input image also tends to be amplified due to the nature of the UM. Hence, the application of the conventional UM is not suitable when noise is present. This paper exploits the statistical theories proposed in A. Polesel, et al., (1997) and Y.-H. Kim and J. Lee, (Nov 2005) for detecting noise and image feature of the input image so that the UM could be adaptively applied accordingly. By applying the proposed algorithm, it is made possible to enhance local contrast of the image, especially, the area with small details, without boosting up the noise counterpart. This results in natural looking output image.	algorithm;emoticon;feature (computer vision);image noise;pixel;population;randomness;sensor;simulation;theory;unsharp masking	Yeong-Hwa Kim;Yong Cho	2008	IEEE Transactions on Consumer Electronics	10.1109/TCE.2008.4560166	computer vision;statistical hypothesis testing;speech recognition;computer science;pattern recognition;mathematics;artificial neural network;statistics	Vision	56.445780857506804	-65.30963418625505	95557
abc75228a44dffcd24994dc132097f0eb2ef1665	joint multi-frame super-resolution and matting	image resolution;inverse problems image resolution image sequences;inverse problems joint multiframe superresolution frame matting image sequence;image resolution robustness bayesian methods image reconstruction cameras convergence hair;inverse problems;image sequences	Matting and super-resolution of frames from an image sequence have been studied independently in the literature. We propose a unified formulation to solve both inverse problems by assimilating matting within the super-resolution model. We adopt a multi-frame approach which uses data from adjacent frames to increase the resolution of the matte as well as foreground.	data assimilation;frame language;matte display;super-resolution imaging	Sahana M. Prabhu;A. N. Rajagopalan	2012	Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)		computer vision;image resolution;computer science;inverse problem;computer graphics (images)	Vision	56.37162118825216	-55.71409950919897	95647
cc07250c83f1883151d0e1ca33b729836a3b4283	video bsckground inpainting using dynamic texture synthesis	linear dynamic system;camera motion estimation;background mosaic;image segmentation;spatial domain;motion compensation;video signal processing;training;blending based post processing;motion estimation;cameras filling motion estimation control system synthesis motion segmentation computer vision stochastic processes feedback control image restoration humans;filling;image texture;video background inpainting;camera motion;ghost shadow;flicker noise video background inpainting dynamic texture synthesis camera motion estimation background mosaic linear dynamic system blending based post processing spatial domain temporal domain ghost shadow spatio temporal unsmoothed transition;dynamic texture;vectors;dynamics;spatio temporal unsmoothed transition;video cameras;linear dynamical system;pixel;television interference;spatiotemporal phenomena;temporal domain;dynamic texture synthesis;video signal processing image segmentation image texture motion compensation spatiotemporal phenomena television interference video cameras;flicker noise;cameras	This paper presents a novel framework that aims to complete background holes with still or dynamic textures in a video captured by a fixed or moving camera In our method, background mosaics are constructed based on camera motion estimation to establish the correspondence map between the to-be-filled pixels and the corresponding available or synthesized data obtained/trained from the non-occluded background in neighboring frames. By using a linear dynamic system model for dynamic texture analysis and synthesis, the proposed method can handle the completion of dynamic textured background. After filling the background holes with synthesized background data, we perform Mending-based post-processing on the boundaries of completed background in both the spatial and temporal domains to alleviate the artifacts of ghost shadows and flickers due to the spatio-temporal unsmoothed transitions on the boundaries. Experimental results show that the proposed method can nicely maintain both the spatial and temporal consistency of background after completion.	dynamical system;inpainting;motion estimation;pixel;seamless3d;texture synthesis;video post-processing	Chia-Wen Lin;Nai-Chia Cheng	2010	Proceedings of 2010 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2010.5537409	flicker noise;image texture;linear dynamical system;computer vision;dynamics;computer science;motion estimation;multimedia;image segmentation;motion compensation;pixel;computer graphics (images)	Vision	57.25986761249533	-53.868109219328794	95778
0c693ad7cda4e0a151bce15fd0dd339ffec47499	a novel histogram based fuzzy impulse noise restoration method for colour images	science general;fonction green;fuzzy rule based system;fuzzy set;restauration image;image processing;funcion green;impulse noise;logique floue;procesamiento imagen;logica difusa;image restoration;traitement image;fuzzy logic;restauracion imagen;histogram;histogramme;imagen color;histograma;image couleur;green function;color image	In this paper, we present a new restoration technique for colour images. This technique is developed for restoring colour images that are corrupted with impulse noise. The estimated histograms for the colour component differences (red-green, red-blue and green-blue) are used to construct fuzzy sets. Those fuzzy sets are then incorporated in a fuzzy rule based system in order to filter out the impulse noise. Experiments finally show the shortcomings of the conventional methods in contrast to the proposed method.	circuit restoration	Stefan Schulte;Valérie De Witte;Mike Nachtegael;Dietrich Van der Weken;Etienne E. Kerre	2005		10.1007/11558484_79	fuzzy logic;image restoration;computer vision;color image;image processing;impulse noise;computer science;artificial intelligence;histogram;mathematics;green's function;fuzzy set	Vision	55.66809061546575	-65.09543790832872	95906
7e8a725ab4c772c432b951d08c257dea404f312a	calculating a color gamut border using a grid approach.	relative position	Colors are a way of communication that emphasizes the information of a given illustration. Therefore color faithfulness is evident. To know if a given color is within a special color gamut first of all this gamut needs to be known. To calculate the color gamut of a device a number of solutions have been suggested. Most of them are dealing with the fact that they over- or underestimate the real volume of the color gamut, respectively. Our approach applies a grid onto the hull of the color volume. Using this approach a very accurate information about the border is received independently of a convex or non-convex shape. Having got this basic grid information it comes out that a cut at a given L* value can be calculated. Therefore for a given color value a slice of the color gamut can be calculated and the relative position can be obtained. Using this procedure it is also possible to determine the volume of the color gamut.		Andreas Willert;Martin Flaspöhler;Arved C. Hübler	2004			computer vision;geography;optics;computer graphics (images)	Robotics	62.00241169119268	-53.725133796858486	96069
8f534d49b3cb39b791f8876a4fe2d5bcb306f051	a fast image restoration method based on an improved criminisi algorithm		This paper proposes an improved Criminisi image restoration algorithm that produces better repairs and reduces the computational time. First, we improved the priority calculation and included a step that transforms the original confidence term into an index to achieve a more precise repair. Second, in large damaged areas of an image, we use a local searching method to find the optimal matching block to speed up the repair process. Our experimental results show that the improved method significantly increased the speed of the method, effectively retained image structures, and produced better visual effects.	algorithm;circuit restoration;computation;image restoration;optimal matching;screen burn-in;speedup;time complexity;visual effects	Yue Chi;Ning He;Qi Zhang	2017	JCP		artificial intelligence;image restoration;computer science;pattern recognition	Vision	54.14388067149296	-63.853671320489724	96453
76c24944bb51454fd99685e72b1b25af90a1c5a4	enhancing perceived quality of compressed images and video with anisotropic diffusion and fuzzy filtering	fuzzy filter;h 264 avc;anisotropic diffusion;visual quality	Fuzzy filtering has recently been applied and optimized for reducing distortion in compressed images and video. In this paper, we present a method combining the powerful anisotropic diffusion equations with fuzzy filtering for removing blocking and ringing artifacts. Due to the directional nature of these artifacts, we have applied directional anisotropic diffusion. In order to improve the performance of the algorithm, we select the threshold parameter for the diffusion coefficient adaptively. Two different methods based on this approach are presented: one designed for still images and the other for YUV video sequences. For the video sequences, different filters are applied to luminance (Y) and chrominance (U,V) components. The performance of the proposed method has been compared against several other methods by using different objective quality metrics and a subjective comparison study. Both objective and subjective results on JPEG compressed images, as well as MJPEG and H.264/AVC compressed video, indicate that the proposed algorithms employing directional and spatial fuzzy filters achieve better artifact reduction than other methods. In particular, robust improvements with H.264/AVC video have been gained with several different	algorithm;anisotropic diffusion;artifact (software development);blocking (computing);coefficient;data compression;distortion;h.264/mpeg-4 avc;jpeg;peak signal-to-noise ratio;pixel;ringing (signal);ringing artifacts;solid-state drive;visual artifact	Ehsan Nadernejad;Jari Korhonen;Søren Forchhammer;Nino Burini	2013	Sig. Proc.: Image Comm.	10.1016/j.image.2012.12.001	computer vision;computer science;theoretical computer science;multimedia;anisotropic diffusion	Vision	58.022717992248765	-63.42714374945854	96622
6f9b737e0515b496f9036a7f9fe7bdbc68ed8a84	arbitrary stereoscopic view generation using multiple omnidirectional image sequences	stereoscopic image;multiple omnidirectional image sequences;penalty function arbitrary stereoscopic view generation multiple omnidirectional image sequences image based rendering approach stereoscopic vision appropriate ray information;sensors;omnidirectional image;stereoscopic vision;image based rendering approach;image edge detection;three dimensional displays;solid modeling;arbitrary stereoscopic view generation;image sequence;stereo image processing;appropriate ray information;novel view generation;information need;image based rendering;rendering computer graphics;omnidirectional image stereoscopic image novel view generation image based rendering;image sequences rendering computer graphics cameras three dimensional displays solid modeling image edge detection sensors;cameras;stereo image processing image sequences rendering computer graphics;penalty function;image sequences	This paper proposes a novel method for generating arbitrary stereoscopic view from multiple omni directional image sequences. Although conventional methods for arbitrary view generation with an image-based rendering approach can create binocular views, positions and directions of viewpoints for stereoscopic vision are limited to a small range. In this research, we attempt to generate arbitrary stereoscopic views from omni directional image sequences that are captured in various multiple paths. To generate a high-quality stereoscopic view from a number of images captured at various viewpoints, appropriate ray information needs to be selected. In this paper, appropriate ray information is selected from a number of omni directional images using a penalty function expressed as ray similarity. In experiments, we show the validity of this penalty function by generating stereoscopic view from multiple real image sequences.	binocular vision;experiment;information needs;penalty method;stereopsis;stereoscopy	Maiya Hori;Masayuki Kanbara;Naokazu Yokoya	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.79	information needs;computer vision;image-based modeling and rendering;computer science;sensor;stereopsis;penalty method;multimedia;solid modeling;computer graphics (images)	Vision	57.51885384597219	-55.44322834466028	96676
38c46c5ad8764baff63ed9989b948873572ea8d9	a new approach of improving cfa image for digital camera's		This paper work directly towards the improving the quality of the image for the digital cameras and other visual capturing products. In this Paper, the authors clearly defines the problems occurs in the CFA image. A different methodology for removing the noise is discuses in the paper for color correction and color balancing of the image. At the same time, the authors also proposed a new methodology of providing denoisiing process before the demosaickingfor the improving the image quality of CFA which is much efficient then the other previous defined. The demosaicking process for producing the colors in the image in a best way is also discuss.	bayer filter;color balance;control flow analysis;demosaicing;digital camera;image processing;image quality;noise reduction	Manoj Kumar;Vikas Kaushik;Pradeep Singla	2012	CoRR		computer vision;computer science;multimedia;computer graphics (images)	Vision	59.069808531006835	-61.51927941603302	96986
37296c9e14f10fc0bd83aa9ece4311cbe598ec14	novel edge detection	scale multiplication;discrete wavelet transforms;additive white gaussian noise;discrete wavelet transform;image segmentation;image resolution;edge detection;detection efficiency;image edge detection additive white noise gaussian noise discrete wavelet transforms noise reduction awgn wavelet coefficients degradation image analysis image segmentation;awgn;wavelet decomposition;splines mathematics;feature extraction;splines mathematics awgn curve fitting discrete wavelet transforms edge detection feature extraction image denoising image resolution image segmentation;image quality;image thresholding;object boundar extraction;image denoising;curve fitting;boundary detection;cubic spline edge detection image denoising additive white gaussian noise object boundar extraction image quality image segmentation discrete wavelet transform image thresholding scale multiplication boundary detection curve fitting;cubic spline	The purpose of this paper is to develop an algorithm for denoising images corrupted with additive white Gaussian noise (AWGN) with a view to extract object's boundary. The noise degrades quality of the images and makes interpretations, analysis and segmentation of images harder. A pixel is said to be a boundary pixel if its deleted neighborhood contains at least one point from the object and one point from the object's complement. Discrete wavelet transform (DWT) using scale correlation is a denoising approach that reveals boundary pixels more effectively than the simple wavelet decomposition. The detail coefficients in concordant bands are correlated and then synthesized after soft thresholding, which suppresses noise but signifies smooth intensity variations. The wavelet coefficients of noise have much trivial correlation than the wavelet coefficients of boundaries that propagate along the scale. Scale multiplication improves the localization accuracy significantly while keeping high detection efficiency. The combination of noise filtering coupled with boundary detection in a single algorithm enables disconnected boundary detection in a noisy scenario. Curve fitting or cubic spline can then augment the boundaries to estimate missing pixels	additive white gaussian noise;algorithm;coefficient;cubic hermite spline;cubic function;curve fitting;database;discrete wavelet transform;edge detection;interpolation;mike lesser;noise reduction;pixel;signal-to-noise ratio;spline (mathematics);thinning;thresholding (image processing);utility functions on indivisible goods	Muhammad Saleem;Imran Touqir;Adil Masood Siddiqui	2007	Fourth International Conference on Information Technology (ITNG'07)	10.1109/ITNG.2007.137	additive white gaussian noise;computer vision;mathematical optimization;computer science;pattern recognition;statistics	Vision	54.69634567341088	-65.41880183122733	97133
d82e47cc4764407a79ff09b31415b77876b65d85	the contracting curve density algorithm and its application to model-based image segmentation	iterative refinement;partial occlusion contracting curve density algorithm model based image segmentation deformable model fitting image data uncertain a priori knowledge model parameters initial probability distribution local statistics locally adapted criteria spatially changing properties map estimation rgb images subpixel accuracy;probability;image segmentation;maximum likelihood estimation;image texture;local adaptation;image segmentation image edge detection deformable models statistical distributions parameter estimation three dimensional displays integrated circuit modeling charge coupled devices curve fitting statistics;a priori knowledge;deformation;probability distribution;map estimation;maximum likelihood estimation image segmentation deformation probability image texture;deformable model	In this paper we address the problem of model-based image segmentation by fitting deformable models to the image data. From uncertain a priori knowledge of the model parameters an initial probability distribution of the model edge in the image is obtained. From the vicinity of the surmised edge local statistics are learned for both sides of the edge. These local statistics provide locally adapted criteria to distinguish the two sides of the edge even in the presence of spatially changing properties such as texture, shading, or color. Based on the local statistics the model parameters are iteratively refined using a MAP estimation. Experiments with RGB images show that the method is capable of achieving high subpixel accuracy and robustness even in the presence of texture, shading, clutter, and partial occlusion.	3d pose estimation;algorithm;approximation;charge-coupled device;clutter;color;experiment;image segmentation;pixel;shading;statistical model;texture mapping	Robert Hanek	2001		10.1109/CVPR.2001.990561	probability distribution;image texture;computer vision;a priori and a posteriori;computer science;segmentation-based object categorization;pattern recognition;probability;mathematics;maximum likelihood;image segmentation;scale-space segmentation;deformation;statistics	Vision	53.78062880493334	-54.69115669117066	97289
b5408f00d10956e8c2f4de6c624666606b9daa76	the impact of ucr on scanner calibration	degree of freedom	"""Most present day color scanners, with red, green, and blue filters, are non-colorimetric. This means that their outputs cannot be linearly transformed into CIE tristimulus values for arbitrary input materials. On the other hand, by restricting oneself to a single class of inputs such as photographic, lithographic, or xerographic materials, very accurate scanner calibrations are possible. In this paper, we conjecture that such accurate calibrations can be achieved if the input document is made with only """"three colorants"""", i.e., has only three independent degrees of freedom. The validity of the above conjecture is tested experimentally using a CMYK (four colorant) printer. Usually in CMYK printing, there are only three fundamental degrees of freedom. Even though four colorants are used, the amounts of these colorants are inter-related through the method used for undercolor removal (UCR). The fourth degree of freedom is re-introduced when the method of undercolor removal is varied. To test the above conjecture concerning """"degrees of freedom"""", we evaluated the impact of different UCR methods on scanner calibration accuracy. The paper also proposes an analytic similarity measure for comparing color spectra from different media, which is shown to be in fair agreement with the experimental results."""	computability in europe;experiment;image scanner;printer (computing);printing;similarity measure	Gaurav Sharma;Shen-Ge Wang;Deepthi Sidavanahalli	1998			degrees of freedom (statistics);calibration;conjecture;computer vision;scanner;similarity measure;geography;artificial intelligence;optics	Robotics	64.29925687625578	-58.59359353685951	97622
43a8c33b662b16547b49ff429ab0dc2714a6e82d	edge-preserving texture filtering for real-time rendering	filtering;texture mapping;real time processing;visual quality;resampling;article;memory bandwidth;hardware implementation;real time rendering	Texture filtering is essential in enhancing the visual quality of real-time rendering. Conventional schemes do not consider the characteristics of texture content, thus the sharpness of edges in texture images cannot be retained. This paper proposes a novel texture-filtering algorithm, which consists of edge-preserving interpolation and edge-preserving MIP-map prefiltering. The memory bandwidth requirement is kept the same as in conventional schemes by dynamically adjusting the interpolation kernel. Hardware implementation is also provided to show the real-time processing capability.	algorithm;aliasing;anisotropic filtering;bump mapping;computer memory;graphics processing unit;interpolation;kernel (operating system);map;memory bandwidth;real-time clock;real-time operating system;real-time transcription;texture filtering;texture mapping	Yuan-Chung Lee;Chein-Wei Jen	2003	The Visual Computer	10.1007/s00371-002-0169-8	filter;texture mapping;computer vision;real-time computing;3d rendering;rendering;resampling;computer science;real-time rendering;texture memory;alternate frame rendering;texture compression;texture filtering;memory bandwidth;computer graphics (images)	Graphics	66.08280268226866	-53.5085448570253	97858
37a46ad2823de12c65ef3ca6c3caf3d88b011021	high resolution depth image recovery algorithm based on the modeling of the sum of an average distance image and a surface image	minimization;image segmentation;image resolution;color;noise measurement;linear programming;cameras	This paper proposes a new depth image recovery algorithm which recovers a high resolution depth image using RGB color image from a very low resolution depth image. In order to achieve a high recovery performance, this paper represents the high resolution depth image as the sum of an average distance image and a surface image. Experimental examples show that the proposed algorithm achieves a high resolution depth recovery from a very low resolution depth image effectively.	algorithm;color image;image resolution	Kazunori Uruma;Katsumi Konishi;Tomohiro Takahashi;Toshihiro Furukawa	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532877	image quality;image texture;image restoration;computer vision;feature detection;range segmentation;image resolution;image gradient;binary image;u-matrix;image processing;computer science;noise measurement;linear programming;mathematics;image segmentation;distance transform;sub-pixel resolution;computer graphics (images)	Robotics	57.58794258582577	-59.687261966793656	97916
a42a5ab082167da28428eb636744f5cb68286457	fpga based multi-shell filter for hot pixel removal within colour filter array demosaicing	image sensors;visualization;image color analysis;periodic structures;field programmable gate arrays;meteorology;cameras	Modern cameras have an image sensor with millions of pixels, among which some of them are inevitably defective (known as hot pixels). Moreover, the number of hot pixels will increase with ageing. colour filter array (CFA) demosaicing will result in colour artifacts caused by these hot pixels. Such artifacts are virtually impossible to remove after demosaicing. In this paper, we propose two effective filter structures for removing low and high density hot pixels within CFA demosaicing for efficient implementation on FGPA for real-time processing. It has been shown that our proposed filter structures are able to remove hot pixels effectively with minimal colour artifacts while preserving image details.	bayer filter;color filter array;defective pixel;demosaicing;field-programmable gate array;filter bank;image sensor;pixel density;preprocessor;real-time clock;streisand effect	Donald Bailey;J. S. Jimmy Li	2016	2016 International Conference on Image and Vision Computing New Zealand (IVCNZ)	10.1109/IVCNZ.2016.7804450	demosaicing;computer vision;visualization;bayer filter;computer science;image sensor;field-programmable gate array;computer graphics (images)	Robotics	60.572798874035804	-58.468443172859295	98080
69c90948b27f610d2ddbf09e4a2bbe7b47363fe6	improved method for color image enhancement based on luminance and color contrast	image enhancement;color image	4X u= X+ 15Y+3Z Abstract. Distinct from gray-scale image enhancement, color image enhancement has two additional requirements. One is to preserve the natural color of the original image, and the other is to effectively present the information contained in the luminance and color components. We show that the color difference map and the luminance map represent different aspects of a color image. An enhancement scheme for color images is proposed based on this observation. In this method, an adaptive luminance masking function is generated from the local luminance contrast and color contrast. Overenhancement distortion, which is caused by the constraint of the display range, is also analyzed. A refinement algorithm is developed to solve this problem. where 0h2ii , (1)	algorithm;color image;distortion;grayscale;image editing;refinement (computing);requirement	Ningyan Liu;Hong Yan	1994	J. Electronic Imaging	10.1117/12.173323	color histogram;rgb color model;computer vision;color filter array;hsl and hsv;lightness;color normalization;color image;image gradient;chromaticity;computer science;color balance;color space;histogram equalization;spectral color	Robotics	58.69910550803551	-61.8975369855918	98219
00d1883245b1a9d3be383718cd130504ab946e48	correcting photometric distortion of document images on a smartphone	noise bismuth educational institutions accuracy optical character recognition software conferences robustness;noise removal technique photometric distortion smartphone moir e pattern noise separated smoothing process moir e pattern removal recaptured document image lcd monitor;smoothing methods document image processing image denoising photometry	This paper presents efficient and robust methods for correcting photometric distortion on document images caused by moir'e pattern noise and specular highlight in smartphone. Our algorithm uses separated smoothing process for moir'e pattern removal on recaptured document image from LCD monitor. Furthermore, contrast of characters in specular highlight area is enhanced using subtraction and noise removal technique.	algorithm;distortion;smartphone;smoothing;specular highlight;thin-film-transistor liquid-crystal display	Christian Simon;Williem;Jihwan Choe;Il Dong Yun;In Kyu Park	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2014.38	computer vision;computer science;computer graphics (images)	Vision	55.81644975264496	-60.813565439822135	98578
157eb3e01724b5fddd2fad72311c5d7bc16832e5	a novel post-processing method to improve the ability of reconstruction for video leaking signal		The confidential information can be reconstructed by received weak electromagnetic signal from a radiation object, such as a computer display. Usually, the radiation signal is submerged in strong noise and faded in channel, so it is not an easy task to understand the information existed in the signal. In this paper, we propose a new post-processing system to improve the visual quality of reconstruction image in sparse domain. Different from filter and enhancement technologies in image filed, our system focuses on data shrink and repair using the methods in machine learning. It can not only remove the noise interference, but also complement the lost high-frequency and compensate the distortion. Experimental section displays complete procedures and better performance, and it also proves the effectiveness of the novel framework.	video post-processing	Xuejie Ding;Meng Zhang;Jun Shi;Wei-qing Huang	2015		10.1007/978-3-319-29814-6_12	computer vision;speech recognition;telecommunications	Vision	58.92119787473229	-58.9476991905404	98743
37657abed84bf349cd9bc852db3551dadb85ba9d	multi-view inpainting for rgb-d sequence		In this work we propose a novel approach to remove undesired objects from RGB-D sequences captured with freely moving cameras, which enables static 3D reconstruction. Our method jointly uses existing information from multiple frames as well as generates new one via inpainting techniques. We use balanced rules to select source frames; local homography based image warping method for alignment and Markov random field (MRF) based approach for combining existing information. For the left holes, we employ exemplar based multi-view inpainting method to deal with the color image and coherently use it as guidance to complete the depth correspondence. Experiments show that our approach is qualified for removing the undesired objects and inpainting the holes.	3d reconstruction;baseline (configuration management);color image;experiment;homography (computer vision);image warping;inpainting;markov chain;markov random field;mathematical optimization;neural coding	Feiran Li;Gustavo Alfonso Garcia Ricardez;Jun Takamatsu;Tsukasa Ogasawara	2018	2018 International Conference on 3D Vision (3DV)	10.1109/3DV.2018.00060	artificial intelligence;markov random field;inpainting;3d reconstruction;pattern recognition;image warping;color image;computer science;rgb color model;image quality;homography	Vision	56.13652470354169	-55.45618871183781	98970
6253f8f4012a9a326dbb575cc4e34c19b85cf206	variable selection for image quality assessment using a neural network based approach	databases;human appreciation;image coding;data compression;neural nets;compressed image quality assessment;neural nets data compression image coding;distortion measurement;transform coding;mean opinion score variable selection system compressed image quality assessment neural network image coding systems distortion measure human appreciation;image coding systems;variable selection;artificial neural networks;image quality artificial neural networks distortion measurement databases transform coding humans feature extraction;distortion measure;feature extraction;image quality;mean opinion score;image quality assessment;humans;subjective evaluation;variable selection system;variable selection neural network image coding image quality assessment mean opinion score;artificial neural network;neural network	Compressed image quality assessment is of increasing importance in image coding systems where the schemes optimization is based on the distortion measure. There exist many distortion measures in the literature which are often validated by comparing them to the human appreciation of the image quality, in particular the Mean Opinion Score (MOS).	artificial neural network;distortion;feature selection;image quality;mathematical optimization	Atidel Lahouhou;Emmanuel Viennet;Mourad Haddadi	2010	2010 2nd European Workshop on Visual Information Processing (EUVIP)	10.1109/EUVIP.2010.5699110	image quality;computer vision;computer science;machine learning;pattern recognition	AI	62.99428872528245	-65.08281387730959	99021
db11266a21a9c862b49eab3a1a7939f7ce6cbe79	a hybrid image restoring algorithm for interlaced video	detectors;interpolation;image motion analysis;performance evaluation;psnr;video signal processing;real time;motion estimation;image restoration;visual quality;temporal information;interpolation image de interlacing image restoration interlaced video;computational complexity hybrid image restoring algorithm interlaced video motion degree detector;image edge detection;computational complexity;image quality;pixel;image de interlacing;video signal processing image motion analysis image restoration interpolation;hybrid image restoring algorithm;interlaced video;image restoration interpolation motion detection detectors motion estimation computational complexity performance evaluation hardware tv image quality;tv;correlation;motion degree detector;signal processing algorithms;motion detection;hardware implementation;quantitative evaluation;hardware	In this letter, a novel image restoring technique for interlaced video is presented. We employ a motion-degree detector to determine the type of motion in the adjacent pictures efficiently and a hybrid interpolation based on spatial and/or temporal information to restore the missing pixels for each motion type. Extensive experimental results demonstrate that our method can obtain better performances in terms of both quantitative evaluation and visual quality than the state-of-the-art image restoring techniques. Since the proposed method requires low computational complexity, it is very suitable for real-time hardware implementation and can be applied to current television systems.	algorithm;computational complexity theory;interlaced video;interpolation;performance;pixel;real-time clock	Chih-Yuan Lien;Chung-Ping Young;Pei-Yin Chen	2009	IEEE Signal Processing Letters	10.1109/LSP.2008.2011715	image quality;image restoration;computer vision;detector;peak signal-to-noise ratio;interpolation;computer science;theoretical computer science;motion estimation;mathematics;computational complexity theory;correlation;pixel;computer graphics (images)	Graphics	58.61215018277245	-58.6138124109705	99860
d311dc44122e917d2f346cc3dbd4757aaa8b23d2	super-resolution mosaicking of uav surveillance video	uav;least squares approximations;feature points;kernel;video surveillance;image segmentation;image resolution;bilateral total variance hubert method;generic model;real time;mosaic;inverse system;remotely operated vehicles;maximum a posteriori estimation;video surveillance image resolution image segmentation least squares approximations remotely operated vehicles;large scale;random m least squares;sift;resolution enhancement;least square;bilateral total variance hubert method uav surveillance video multiframe superresolution mosaicking algorithm feature points sift random m least squares image separation maximum a posteriori estimation underdetermined sparse linear system ill posed large scale inverse system;mathematical model;multiframe superresolution mosaicking algorithm;image separation;uav surveillance video;super resolution;cross validation;atmospheric modeling;unmanned aerial vehicles surveillance least squares approximation maximum a posteriori estimation linear systems large scale systems kernel image resolution cameras spatial resolution;underdetermined sparse linear system;uav mosaic super resolution;unmanned aerial vehicles;sparse linear system;algorithm design and analysis;ill posed large scale inverse system;data models	This paper explains and implements our efficient multi-frame super-resolution mosaicking algorithm. In this algorithm, feature points between images are matched using SIFT, and then random M-least squares is used to estimate the homography between frames. Next, separate frames are registered and the overlapping region is extracted. A generative model is then adopted and combined with maximum a posteriori estimation to construct the underdetermined sparse linear system. To solve the ill-posed large-scale inverse system, we derive and implement a new hybrid regularization (bilateral total variance Hubert) method. Cross validation is utilized to estimate the derivative of the blur kernel as well as the regularization parameter. Super-resolution is then applied to the individual sub-frames from the overlapping region. Finally, multi-band blending is used to stitch these resolution-enhanced frames to form the final image. The whole process is semi-real time (roughly 30 seconds for 35 frames) and the effectiveness of our algorithm is validated by applying it to real and synthetic UAV video frames.	algorithm;alpha compositing;bilateral filter;closed-circuit television;cross-validation (statistics);gaussian blur;generative model;homography (computer vision);image stitching;least squares;linear system;matrix regularization;scale-invariant feature transform;semiconductor industry;sparse matrix;super-resolution imaging;synthetic intelligence;unmanned aerial vehicle;video clip;well-posed problem	Yi Wang;Ronald A. Fevig;Richard R. Schultz	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4711762	remotely operated underwater vehicle;inverse system;data modeling;algorithm design;computer vision;atmospheric model;mathematical optimization;mosaic;kernel;image resolution;computer science;maximum a posteriori estimation;machine learning;pattern recognition;mathematical model;scale-invariant feature transform;image segmentation;least squares;cross-validation;statistics;superresolution	Vision	54.12908666053457	-54.277286118787735	99945
a63c3022d4168cd9804df83946515068b6268094	3d television system based on integral photography	high resolution;lens array;three dimensional television;display three dimensional images;photography;photographic technique;three dimensional television photography three dimensional displays;arrays;lenses three dimensional displays arrays tv films ip networks photography;information and communication technology;three dimensional displays;image quality;3d television system;lenses;ip networks;3d display method;tv;integral photography;3d display;films;3d display method 3d television system integral photography photographic technique lens array display three dimensional images	Integral Photography (IP) is a photographic technique in which a lens array consisting of a large number of tiny lenses is used to capture and display three-dimensional (3D) images [1]. The displayed 3D images are optical real images, which give a natural 3D feeling in principle, without special viewing glasses. This is considered to be a suitable 3D display method for future 3D television systems. This paper describes an integral 3D television system in which Super Hi-Vision [2] is applied to IP.	3d television;integral imaging;multiple sub-nyquist sampling encoding;stereo display	Tomoyuki Mishina	2010	28th Picture Coding Symposium	10.1109/PCS.2010.5702465	image quality;computer vision;information and communications technology;image resolution;stereo display;photography;lens;computer graphics (images)	Graphics	62.6121877561135	-56.45249066502148	100010
eab2267e74c6bc35bc695b1766227377232746a7	registration-tolerant extended visual cryptography	eye;image processing;opacity;imagen medio tinte;image demi teinte;quality improvement;half tone image;procesamiento imagen;imagen nivel gris;traitement image;registro imagen;visual cryptography;boolean operation;cryptographie visuelle ettendue;recalage image;criptografia;cryptography;image registration;image niveau gris;cryptographie;transparency;grey level image	Extended Visual Cryptography is a method which encodes a number of images so that when the images are superimposed, the hidden image appears without a trace of original images. The decryption is done directly by human eyes without cryptographic calculations. The proposing system takes three pictures as input and generates two images which correspond to two of the input pictures. The third picture is perceived by superimposing the two output images. Previous methods are based on halftoning and Boolean operations. Transparency values must be quantized before encryption, and a pixel is halftoned by a fixed numbers of completely transparent and opaque subpixels. Then a transparency of the superimposed pixel is controlled by changing the subpixel arrangements of the two output pixels. Since the subpixel arrangement is basically random, a tradeoff exists that to express the more graylevels, each subpixel must become the smaller, making it the more difficult to superimpose by hand. Our new approach tolerates registration error for the third image and eases the difficulty, by adopting concentric-circular subpixel arrangement and continuous grayscale subpixel values. The system becomes considerably robust to the registration error. Also, it achieves quality improvement for all three images, by explicitly dealing with continuous graylevels.	boolean algebra;boolean operations on polygons;cluster analysis;dynamic range;grayscale;image quality;pixel;quantization (signal processing);terms of service;visual cryptography	Mizuho Nakajima;Yasushi Yamaguchi	2003		10.1117/12.473920	computer vision;mathematics;cartography;computer graphics (images)	Vision	60.63055097766863	-59.5815681782785	100080
3d0f9d0c977558a163cdc21c9437784bef86bf42	combining shape-from-shading and stereo using gaussian-markov random fields	shape estimation;reliability;shape from shading;gaussian processes;gaussian processes shape costs belief propagation low pass filters computer science gaussian distribution process control optical propagation optical reflection;shape recognition;gaussian markov random field;stereo image processing gaussian distribution gaussian processes markov processes reliability shape recognition;surface treatment;computational modeling;shape;belief propagation;pixel;stereo image processing;map estimation;ground truth;markov processes;belief propagation gaussian markov random fields stereo information shape from shading information local reliability gaussian distributions shape estimate disparity measurements surface orientation map estimate;gaussian distribution;cameras	In this paper we present a method of combining stereo and shape-from-shading information, taking account of the local reliability of each shape estimate. Local estimates of disparity and orientation are modelled using Gaussian distributions. A Gaussian-Markov random field is used to represent the disparity-map, taking into account interactions between disparity measurements and surface orientation, and the MAP estimate found using belief propagation. Local estimates of the precision of disparities and surface normals are found and used to control the process so that the most accurate data source is used in each region. We assess the performance of our approach using both synthetic and real stereo pairs, and compare against ground truth.	belief propagation;bell test experiments;binocular disparity;experiment;ground truth;interaction;markov chain;markov random field;norm (social);normal (geometry);photometric stereo;shading;software propagation;synthetic intelligence	Tom S. F. Haines;Richard C. Wilson	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761513	normal distribution;computer vision;photometric stereo;ground truth;shape;computer science;machine learning;pattern recognition;reliability;gaussian process;mathematics;markov process;computational model;pixel;statistics;belief propagation	Vision	53.806329958745735	-54.13529854342262	100160
5e0075b6c7cf42345db8ff0e29f4b75098ad74a2	color-to-gray conversion using isomap	high dimensionality;color space;dimension reduction;color to gray;isomap;color image	In this paper we present a new algorithm to transform an RGB color image to a grayscale image. We propose using nonlinear dimension reduction techniques to map higher dimensional color vectors to lower dimensional ones. This approach generalizes the gradient domain manipulation for high dimensional images. Our experiments show that the proposed algorithm generates competitive results and reaches a good compromise between quality and speed.	algorithm;color image;dimensionality reduction;display resolution;experiment;gradient;grayscale;isomap;nonlinear system;pc game;preprocessor;shader	Ming Cui;Jiuxiang Hu;Anshuman Razdan;Peter Wonka	2009	The Visual Computer	10.1007/s00371-009-0412-7	color histogram;computer vision;hsl and hsv;color image;computer science;mathematics;color balance;color space;dimensionality reduction;computer graphics (images)	Vision	58.90342891984752	-64.20948930947856	100188
eefd314fcc2490314094b00ac1b667a06b83efff	multidimensional characterization of the perceptual quality of noise-reduced computed tomography images	perceptual quality;image processing;computed tomography;human subjects;cognitive process;noise reduction;image quality;multidimensional scaling	Abstract   Judging the perceptual quality of processed images is a cognitive process in which the perception of image attributes such as sharpness and noisiness plays an important role. In this paper, we use multidimensional scaling to study the perceptual factors that influence the quality impression of Computed Tomography (CT) images processed by a noise-reduction technique. We also characterize intersubject differences in the assessment of image attributes. We show that multidimensional scaling can be used reliably for the characterization of the subjective performance of image-processing algorithms. Evaluations using human subjects, such as the ones presented in this paper, will continue to play an important role, since objective measures for perceptual image quality, with proven validity in a broad range of applications, are not expected in the near future.	ct scan;tomography	Boris Escalante-Ramírez;Jean-Bernard Martens;Huib de Ridder	1995	J. Visual Communication and Image Representation	10.1006/jvci.1995.1027	image quality;computer vision;simulation;cognition;multidimensional scaling;image processing;computer science;noise reduction;multimedia	Vision	62.248494237356766	-63.73487515941478	100215
3b66d3e3d7bf22f555d7c0aa27827118f3d34a77	an effective error resilient 3d view synthesis method	ghost;multiview video coding;interpolation projection;corona;view synthesis;pinhole;depth map;3d video	1047-3203/$ see front matter 2012 Elsevier Inc. A http://dx.doi.org/10.1016/j.jvcir.2012.04.006 ⇑ Corresponding author. E-mail address: byan@fudan.edu.cn (B. Yan). View synthesis is a crucial process in current 3D video applications. Currently, the existing view synthesis techniques may introduce visual artifacts such as corona, pinholes and ghosts into pictures, which degrade the visual experience greatly. In this paper, we will introduce an error resilient 3D view synthesis approach, which is able to effectively remove these artifacts. Specifically, we first detect the regions mixed with foreground and background pixels to avoid corona artifacts. Then, we resize images and conduct projection on the resized images to reduce pinhole artifacts. Finally, an improved view blending algorithm is proposed to reduce ghosting artifacts. Simulation results demonstrate that our proposed method outperforms others significantly in removing view artifacts. 2012 Elsevier Inc. All rights reserved.	3d film;algorithm;alpha compositing;interpolation;pixel;ringing artifacts;rollover (key);simulation;view synthesis;visual artifact	Bo Yan;Bo Yang	2013	J. Visual Communication and Image Representation	10.1016/j.jvcir.2012.04.006	computer vision;corona;computer science;multiview video coding;depth map;computer graphics (images)	Graphics	57.79894114303644	-60.575381168235936	100317
8d6b8272e9a0fd54b8f41eef81d80a67dbdb05a2	performance evaluation for 2d and 3d filtering methods of noise removal in color images	databases;digital cameras;distortion;denoising	Color images formed by modern digital cameras are often noisy, especially if they are captured in bad illumination conditions. This makes desirable to remove the noise by image pre-filtering. A specific feature of the noise observed for the considered application is that it can be spatially correlated. Filters to be applied have to effectively suppress noise introducing only negligible distortions into processed images. Moreover, such filters have to be fast enough and tested for a variety of natural images and noise properties. Another specific requirement is that a visual quality of processed images has to be paid a specific attention. To carry out intensive testing of some denoising approaches, a recently designed database TID2008 of distorted images provides a good opportunity since it contains 25 different images corrupted by i.i.d. and spatially correlated noise with several levels of variances. Taking into account the known fact that the color components are highly correlated, both modern 2D (component-wise) and 3D (vector) filtering techniques are studied. It is demonstrated that the use of 3D filters that allow exploiting inter-channel correlation provides considerably better results in terms of conventional and visual quality metrics. It is also shown how 3D filter based on discrete cosine transform (DCT) can be adapted to a spatial correlation of noise. This adaptation produces sufficient increase of the filter's efficiency. Examples of filter's performance are presented.© (2012) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.	performance evaluation	Nikolay N. Ponomarenko;Vladimir V. Lukin;Alexander A. Zelensky;Karen O. Egiazarian;Jaakko Astola	2012		10.1117/12.906379	gradient noise;median filter;image noise;computer vision;simulation;colors of noise;distortion;noise measurement;noise reduction	Vision	58.75136083787148	-63.19086868412705	100492
22e897e4ea9f5fedf72dcb5935f7403e30f9b955	uncontrolled modulation imaging	imaging setting;modulation imaging;modulation control;imaging process simple;prototype modulation imaging system;modulation work;uncontrolled modulation imaging;image acquisition;high dynamic range;corresponding modulation level;computational synchronization;synchronisation;image processing;modulation;servo control;field of view;multispectral images	To obtain high dynamic range or hyperspectral images, multiple frames of the same field of view are acquired while the imaging settings are modulated; images are taken at different exposures or through different wavelength bands. A major problem associated with such modulations has been the need for perfect synchronization between image acquisition and modulation control. In the past, this problem has been addressed by using sophisticated servo-control mechanisms. In this work, we show that the process of modulation imaging can be made much simpler by using vision algorithms to automatically relate each acquired frame to its corresponding modulation level. This correspondence is determined solely from the acquired image sequence and does not require measurement or control of the modulation. The image acquisition and the modulation work continuously, in parallel, and independently. We refer to this approach as computational synchronization. It makes the imaging process simple and easy to implement. We have developed a prototype modulation imaging system that uses computational synchronization and used it to acquire high dynamic range and multispectral images.	algorithm;control system;high dynamic range;modulation;multispectral image;prototype;servo;uncontrolled format string	Yoav Y. Schechner;Shree K. Nayar	2004	Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.	10.1109/CVPR.2004.255	multispectral image;synchronization;computer vision;dynamic imaging;servo control;field of view;image processing;computer science;modulation	Vision	61.88427606187693	-55.679035952114205	100562
126b4cf0f78c17cc703568d0ff83fc403c8013f0	a new metric for judder in high frame-rate video	indexes;decision support systems	Recent advances on high frame-rate (HFR) hold-type display systems have resulted in the increased demands for HFR contents. Frame-rate up conversion (FRUC) technique could be used for creating HFR contents from the existing low frame-rate (LFR) contents. However, the judder artifacts caused by FRUC would degrade the quality of HFR. The aim of this work is to analyze the cause of judder in the reconstructed HFR video from FRUC, and to propose a new metric to evaluate judder. A judder measurement method using subjective assessments is presented. As the prior study defines judder as the differences between the natural motion and result from the sampled motion of video, the proposed metric quantifies the judder by measuring the differences in retinal space. Experimental results demonstrate that the proposed judder determination method is consistent with the subjective assessment results.		Se Ri Oh;Dongchan Kim;Pyeong Gang Heo;Hyun Wook Park	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7533071	database index;computer vision;simulation;computer science;mathematics	Robotics	63.25328345542846	-63.573451757918825	100612
dca1da5f0e70e00e97e7e840c6dd2c3626250b02	detecting non-stereoscopic to stereoscopic image splicing with the use of disparity maps	splicing;digital forensics;image tampering;image editing;stereo;digital image forensics;digital image	Splicing is a common image manipulation technique, where the content of more than one image is combined to form a composite image. With recent developments in image editing software, almost anyone can splice images. Splicing can be used to tamper with photographs to depict an event that never occurred. With the developments in digital stereoscopic (3D) cameras and images, it has become a necessity to be able to detect splicing in stereoscopic images. Several techniques have been developed to help a digital forensic investigator identify spliced digital images. However, there is a lack of research on exploiting depth information when detecting splicing in stereoscopic images.  This paper proposes a technique to detect simple non-stereoscopic to stereoscopic splicing with the aid of disparity maps. An experiment was performed to test the technique on 35 spliced stereoscopic images. Results indicate that the proposed technique could be used to aid detection. However, detected areas require post-investigation to confirm splicing.	binocular disparity;digital camera;digital image;graphics software;image editing;sensor;splice (system call);stereoscopy	Mark-Anthony Fouche;Martin S. Olivier	2011		10.1145/2072221.2072255	computer vision;geography;digital image processing;multimedia;digital image;computer graphics (images)	HCI	63.568255748575346	-58.16613748458343	100715
9af8ba14aee18dd99a0b035c30ff8454fc0a2482	brdf estimation of complex materials with nested learning.		The estimation of the optical properties of a material from RGB-images is an important but extremely ill-posed problem in Computer Graphics. While recent works have successfully approached this problem even from just a single photograph, significant simplifications of the material model are assumed, limiting the usability of such methods. The detection of complex material properties such as anisotropy or Fresnel effect remains an unsolved challenge. We propose a novel method that predicts the model parameters of an artist-friendly, physically-based BRDF, from only two low-resolution shots of the material. Thanks to a novel combination of deep neural networks in a nested architecture, we are able to handle the ambiguities given by the nonorthogonality and non-convexity of the parameter space. To train the network, we generate a novel dataset of physicallybased synthetic images. We prove that our model can recover new properties like anisotropy, index of refraction and a second reflectance color, for materials that have tinted specular reflections or whose albedo changes at glancing angles.	artificial neural network;baseline (configuration management);bidirectional reflectance distribution function;computer graphics;convolutional neural network;deep learning;experiment;extrapolation;feature learning;reflection (computer graphics);statistical classification;synthetic intelligence;usability;well-posed problem	Raquel Vidaurre;Dan Casas;Elena Garces;Jorge Lopez-Moreno	2018	CoRR			Vision	59.32551615886398	-52.44500076401792	100972
c59662aeaed97fe949c6129a0631023390ccd6a4	a real-time 3d range image sensor based on a novel tip-tilt-piston micromirror and dual frequency phase shifting	lasers;fringe analysis;3d modeling;range imaging;sensors;phase shifting;high speed cameras;light emitting diodes;stereoscopy;image sensors;projection systems;micromirrors;lcds;image acquisition;graphics processing units;bandpass filters;video;cameras;light sources	Structured light is a robust and accurate method for 3D range imaging in which one or more light patterns are projected onto the scene and observed with an off-axis camera. Commercial sensors typically utilize DMD- or LCD-based LED projectors, which produce good results but have a number of drawbacks, e.g. limited speed, limited depth of focus, large sensitivity to ambient light and somewhat low light efficiency. We present a 3D imaging system based on a laser light source and a novel tip-tilt-piston micro-mirror. Optical interference is utilized to create sinusoidal fringe patterns. The setup allows fast and easy control of both the frequency and the phase of the fringe patterns by altering the axes of the micro-mirror. For 3D reconstruction we have adapted a Dual Frequency Phase Shifting method which gives robust range measurements with sub-millimeter accuracy. The use of interference for generating sine patterns provides high light efficiency and good focusing properties. The use of a laser and a bandpass filter allows easy removal of ambient light. The fast response of the micro-mirror in combination with a high-speed camera and real-time processing on the GPU allows highly accurate 3D range image acquisition at video rates. © (2015) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.	image sensor;range imaging;real-time clock	Øystein Skotheim;Henrik Schumann-Olsen;Jostein Thorstensen;Anna N. Kim;Matthieu Lacolle;Karl-Henrik Haugholt;Thor Bakke	2015		10.1117/12.2078976	stereoscopy;computer vision;video;laser;sensor;image sensor;band-pass filter;phase;optics;physics;computer graphics (images);light-emitting diode	Robotics	60.91276901270079	-56.83329269194938	100985
5aee1b53d182ec6a4c4c66e68a718c8efa726294	colour image difference analysis: quantitative versus qualitative methods	qualitative method;image compression colour image difference analysis statistical correlation quantitative method quantitative methods qualitative psychophysical method;image coding;data compression;image coding image colour analysis data compression;image coding psychology pixel image color analysis image analysis image quality statistical distributions testing image processing monitoring;image colour analysis;quantitative method	This paper describes investigation of statistical correlation between the results of two image colour difference criteria: qualitative and quantitative. The aim is that, provided there is consistently high correlation between the results of the different methods, the quantitative method, which is relatively more convenient, could be used instead of the much more laborious qualitative psychophysical method.		Byoung-Ho Kang;Maeng-Sub Cho	2001		10.1109/ICSMC.2001.972898	data compression;image texture;computer vision;quantitative research;qualitative research;mathematics;multimedia;statistics;standard test image;computer graphics (images)	Vision	62.31007786960044	-63.35613774917904	101006
98fb5e63ee3e1786870802df4bae7cc822b698e8	boundary matching based spatial interpolation for consecutive block loss concealment	interpolation;block loss recovery spatial error concealment robust video communication;interpolation signal processing algorithms image reconstruction psnr streaming media image edge detection complexity theory;edge detection;image matching;video coding;smoothing methods;block loss recovery;video coding edge detection image matching interpolation smoothing methods;robust video communication;matching direction crossing removal boundary matching based spatial interpolation consecutive block loss spatial error concealment algorithm lost packet recovery still images intracoded video frames 1d boundary matching edge information preservation hsia algorithm performance improvement subjective quality objective quality smoothing operation averaging filter adaptive searching range expansion;spatial error concealment	This paper presents a spatial error concealment algorithm for recovering the lost packets in still images or Intra-coded frames in video. The idea is based on the work proposed by Hsia [1], where a 1-D boundary matching is applied to recover the lost areas while preserving the edge information. However, we modify Hsia's algorithm in several aspects to improve its performance in terms of subjective and objective quality, i.e., smoothing operation by an averaging filter, adaptive searching range expansion, and matching direction crossing removing. Simulation results shows superiority of the proposed algorithm compared with Hsia's algorithm and some other state-of-the-art algorithms.	block-matching algorithm;error concealment;mobile phone;multivariate interpolation;simplex algorithm;simulation;smoothing;video sensor technology	Shaoshuai Gao	2012	2012 IEEE International Symposium on Multimedia	10.1109/ISM.2012.32	computer vision;mathematical optimization;edge detection;interpolation;computer science;theoretical computer science	Vision	58.17172001339557	-58.856262188989135	101237
16f913d085c93d7482f075e1e4104c06a8c9d679	an entropy-based objective evaluation method for image segmentation	vision ordenador;entropia;methode empirique;image segmentation;empirical analysis;image processing;metodo empirico;evaluation method;empirical method;procesamiento imagen;analisis objetivos;parameterization;traitement image;segmentation evaluation;parametrizacion;computer vision;object segmentation;minimum description length;entropie;segmentation image;vision ordinateur;entropy;theorie information;video;analyse objective;parametrisation;objective analysis;quantitative evaluation;information theory;computer vision technology;teoria informacion	Accurate image segmentation is important for many image, video and computer vision applications. Over the last few decades, many image segmentation methods have been proposed. However, the results of these segmentation methods are usually evaluated only visually, qualitatively, or indirectly by the effectiveness of the segmentation on the subsequent processing steps. Such methods are either subjective or tied to particular applications. They do not judge the performance of a segmentation method objectively, and cannot be used as a means to compare the performance of different segmentation techniques. A few quantitative evaluation methods have been proposed, but these early methods have been based entirely on empirical analysis and have no theoretical grounding. In this paper, we propose a novel objective segmentation evaluation method based on information theory. The new method uses entropy as the basis for measuring the uniformity of pixel characteristics (luminance is used in this paper) within a segmentation region. The evaluation method provides a relative quality score that can be used to compare different segmentations of the same image. This method can be used to compare both various parameterizations of one particular segmentation method as well as fundamentally different segmentation techniques. The results from this preliminary study indicate that the proposed evaluation method is superior to the prior quantitative segmentation evaluation techniques, and identify areas for future research in objective segmentation evaluation.	circuit complexity;computer vision;image segmentation;information theory;lazy evaluation;pixel;usability;video	Hui Zhang;Jason E. Fritts;Sally A. Goldman	2004		10.1117/12.527167	computer vision;simulation;computer science;artificial intelligence;segmentation-based object categorization;image segmentation;scale-space segmentation	Vision	62.562478684508676	-63.417487070966814	101241
ffb136a45106e6b4b52e6bdb9a15582ca8d96161	achieving high angular resolution via view synthesis: quality assessment of 3d content on super multiview lightfield display		Super multiview (SMV) lightfield displays provide glasses- free immersive 3D experience via continuous motion parallax. Such displays require input views acquired at high angular resolution, which could be economically and technologically prohibitive. To achieve high view density, synthesizing absent views from the captured ones has recently been suggested. Clearly, a critical proportion of the views should be unsynthesized in order to ensure acceptable quality of the 3D content. In order to determine the said proportion we subjectively assessed the degradation in perceptual 3D quality vis-à- vis the proportion of synthesized views. The qualitative analysis of the subjective scores ascertained that the quality of 3D perception is mainly affected by discontinuities in motion parallax introduced by view synthesis, along with 2D quality of the constituent views. The subjective scores collected in the process exhibited a three-regime behaviour, corresponding to small, moderate and large proportion of synthesized views. For a high 3D quality experience, we recommend operation in the first regime, albeit its high sensitivity to the proportion of synthesized views. The second regime, less susceptible to the said proportion, can be considered when a moderate perceptual quality is acceptable. The third regime, exhibiting monotonous decrease in perceptual quality, is not recommended.	angularjs;elegant degradation;image resolution;page view;parallax;view synthesis	Roopak R. Tomboli;Balasubramanyam Appina;Sumohana S. Channappayya;Soumya Jana	2017	2017 International Conference on 3D Immersion (IC3D)	10.1109/IC3D.2017.8251906	view synthesis;parallax;computer vision;angular resolution;image resolution;classification of discontinuities;mathematics;artificial intelligence	Vision	63.87691192574581	-62.284148149126395	101244
275423cacdb416a488a14c5fc07d0477f082f4e9	optical flow guided tv-l1 video interpolation and restoration	robust denoising algorithm;temporal derivative;impaired movie sequence;tv-l1 denoising algorithm;benchmark datasets;temporal gradient;tv-l1 video interpolation;impaired frame;optical flow;video restoration;video sequence;video post-processing	The ability to generate intermediate frames between two given images in a video sequence is an essential task for video restoration and video post-processing. In addition, restoration requires robust denoising algorithms, must handle corrupted frames and recover from impaired frames accordingly. In this paper we present a unified framework for all these tasks. In our approach we use a variant of the TV-L denoising algorithm that operates on image sequences in a space-time volume. The temporal derivative is modified to take the pixels’ movement into account. In order to steer the temporal gradient in the desired direction we utilize optical flow to estimate the velocity vectors between consecutive frames. We demonstrate our approach on impaired movie sequences as well as on benchmark datasets where the ground-truth is known.	algorithm;benchmark (computing);circuit restoration;gradient;interpolation;noise reduction;optical flow;unified framework;velocity (software development);video post-processing	Manuel Werlberger;Thomas Pock;Markus Unger;Horst Bischof	2011		10.1007/978-3-642-23094-3_20	computer vision	Vision	56.402610410116324	-56.009432430295945	101346
3ec89eea38d4000158732df3b111d115e9790ddd	directional interpolation of noisy images	image interpolation schemes;interpolation;image interpolation;noisy images;data fusion;joints;indexing terms;noise measurement;interpolation noise reduction filtering image resolution humans visual system covariance matrix image processing estimation fuses;directional estimation;directional interpolation estimation;fine structure;estimation;image acquisition;noise reduction;pixel;image acquisition process;image denoising;denoising;parameter estimation;parameter estimation noisy images image denoising directional interpolation estimation image interpolation schemes image acquisition process;data fusion interpolation denoising directional estimation;noise;interpolation image denoising	Most of the existing image interpolation schemes assume that the image is noise free. This assumption is invalid in practice because noise will be corrupted in the image acquisition process. The conventional way is to denoise the image first and then interpolate the denoised image. The denoising process, however, may smooth much the image details and introduce some artifacts, which could be amplified in the interpolation process. This paper presents a directional estimation scheme to implement denoising and interpolation simultaneously. For each noisy sample, we compute multiple directional estimates of it and then fuse them for a more accurate output. The estimation parameters computed in the denoising process can be subsequently used for interpolation. Compared with the schemes that perform denoising and interpolation in tandem, the proposed method can better reproduce the image fine structures and reduce much the interpolation artifacts.	digital image processing;interpolation;noise reduction	Lei Zhang;Xin Li	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4711834	demosaicing;computer vision;mathematical optimization;bilinear interpolation;computer science;stairstep interpolation;pattern recognition;noise reduction;mathematics;nearest-neighbor interpolation;multivariate interpolation;non-local means;statistics;image scaling	Vision	58.116903139447324	-65.9413549967251	101381
04a77bc6cef68f708c0902f597b21a637ee397b2	vision-an architecture for global illumination calculations	wavelet radiance global illumination calculations vision architecture rendering architecture physical rendering process object oriented class hierarchy illumination algorithms simulation abstract design implementation issues global lighting subsystem path tracing bidirectional estimators irradiance caching hierarchical radiosity wavelet radiosity;object oriented methods;lighting layout object oriented modeling computer architecture rendering computer graphics differential equations integral equations computer graphics algorithm design and analysis image generation;computer graphic;wavelet transforms;image synthesis;brightness;global illumination;wavelet transforms lighting rendering computer graphics object oriented methods digital simulation ray tracing brightness;object oriented;indexation;ray tracing;index termscomputer graphics;object oriented analysis and design;lighting;point of view;rendering computer graphics;rendering architecture;digital simulation	| So far, the problem of global illumination calculation has almost exclusively been approached from an algorithmic point of view. In this paper we propose an architectural approach to global illumination. The proposed rendering architecture Vision is derived from a model of the physical rendering process, which is subsequently mapped onto an object-oriented hierarchy of classes. This design is powerful and exible enough to support and exploit a large body of existing illumination algorithms for the simulation of various aspects of the underlying physical model. Additionally , the Vision architecture ooers a platform for developing new algorithms and for combining them to create new rendering solutions. We discuss both abstract design as well as implementation issues. In particular, we give a detailed description of the global Lighting subsystem and show how algorithms for path tracing, bidirectional estimators, irradiance caching, hierarchical radiosity, wavelet radiosity, and wavelet radiance have been implemented within Vision.	algorithm;global illumination;path tracing;radiosity (computer graphics);simulation;wavelet	Philipp Slusallek;Hans-Peter Seidel	1995	IEEE Trans. Vis. Comput. Graph.	10.1109/2945.468387	object-oriented analysis and design;ray tracing;computer vision;path tracing;radiosity;simulation;rendering;computer science;lighting;programming language;object-oriented programming;global illumination;brightness;wavelet transform;computer graphics (images)	Graphics	66.95743479654502	-52.27866310798178	101650
a4ba3795601731f0bc122b8330de171c99976547	edge detection using advanced grey prediction model	artificial information problems;edge pixel;enhancement parameter;edge detection;image edge detection algorithms;prediction image;prediction algorithms;advanced grey prediction model;prediction theory edge detection image enhancement;edge pixel advanced grey prediction model image edge detection algorithms artificial information problems neighborhood pixels target pixel translation transformation logarithmic transformation prediction image enhancement parameter;logarithmic transformation;image enhancement;computational modeling;prediction theory;image edge detection;translation transformation;predictive models;image edge detection predictive models pixel prediction theory image resolution feature extraction image analysis laplace equations mathematics uncertain systems;prediction model;neighborhood pixels;algorithm design and analysis;target pixel;noise;data models	Conventional image edge detection algorithms generate the information undetected and artificial information problems. In order to detect the image edge more effectively, an improved method based on grey model (GM) is brought forward. The neighborhood pixels of target pixel are selected to build the model. These data are preprocessed by translation transformation and logarithmic transformation. The prediction image is gained by the improved GM(1,1) with enhancement parameter p. The original image subtracted from prediction image gives the edge of image. With the improved algorithm, 80 standard images with bmp format 256⋆256 resolution are processed, and the result shows that not only the new algorithm gain more precise and useful edge pixel, but also the details of the image are obtained. The improved algorithm has better performance than the original algorithm.	algorithm;edge detection;pixel	Songyun Xie;Yubin Xie;Wei Yang;Ge Wang	2010	2010 International Conference on Networking, Sensing and Control (ICNSC)	10.1109/ICNSC.2010.5461610	computer vision;feature detection;image gradient;binary image;computer science;morphological gradient;machine learning;pattern recognition;mathematics;predictive modelling;statistics	Robotics	56.518369622857506	-64.8154185590516	101738
04fba0f04cbc104a304d5c6157b98f32fe8befa6	facedirector: continuous control of facial performance in video	video signal processing emotion recognition face recognition synchronisation;timing control facedirector continuous control multiple facial performance facial expression emotional state arbitrary weighted combination robust nonlinear audio visual synchronization technique visual cue audio cue robust spatiotemporal correspondence dense spatiotemporal correspondence seamless facial blending approach local appearance image space emotion transition performance correction;synchronization face robustness three dimensional displays interpolation	We present a method to continuously blend between multiple facial performances of an actor, which can contain different facial expressions or emotional states. As an example, given sad and angry video takes of a scene, our method empowers the movie director to specify arbitrary weighted combinations and smooth transitions between the two takes in post-production. Our contributions include (1) a robust nonlinear audio-visual synchronization technique that exploits complementary properties of audio and visual cues to automatically determine robust, dense spatiotemporal correspondences between takes, and (2) a seamless facial blending approach that provides the director full control to interpolate timing, facial expression, and local appearance, in order to generate novel performances after filming. In contrast to most previous works, our approach operates entirely in image space, avoiding the need of 3D facial reconstruction. We demonstrate that our method can synthesize visually believable performances with applications in emotion transition, performance correction, and timing control.	align (company);alpha compositing;anton (computer);experiment;interpolation;koch snowflake;nonlinear system;olga (technology);performance;schmidt decomposition;seamless3d;temporal logic	Charles Malleson;Jean Charles Bazin;Oliver Wang;Derek Bradley;Thabo Beeler;Adrian Hilton;Alexander Sorkine-Hornung	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.453	computer vision;speech recognition	Vision	57.525624611000055	-53.679921479298635	101824
54041ea6f47d767b76c477cd327ada57efc93be7	microuniformity: an image quality metric for measuring noise		An overview of a new metric for measuring perceived uniformity of hardcopy, color prints is presented. We discuss the algorithm, and present examples of how the metric can be applied to a wide range of non-uniformity problems.	algorithm;circuit complexity;image quality	B. Mishra;Rene Rasmussen	2000			image noise;computer vision;image quality;noise measurement;artificial intelligence;computer science	Robotics	61.4685635692315	-62.89496938816223	101860
db0553ec88b1fb0d7d580262c6c253f1b5e563a6	digital zoom camera with image sharpening and suppression	image motion analysis;pan tilt zoom;noise reduction method digital zoom camera image sharpening image suppression video camera optical zooming mechanism visual surveillance manufacturing quality measurement region of interest autonomous pan tilt functionality zoom functionality motion detection;indexing terms;digital cameras surveillance motion detection colored noise charge coupled image sensors manufacturing shape target tracking noise reduction system testing;visual surveillance;video cameras;noise reduction;region of interest;image motion analysis video cameras image denoising;image denoising;quality measures;motion detection	The intention of zooming in a video camera is to obtain more detailed visual information than the full frame view. The optical zooming mechanisms provide this additional information, however, when optically zoomed, the camera no longer detects the full frame view, which was detected before zooming. Digital zooming can detect the full view at all times, however, fails to provide additional visual information when zoomed. In applications such as visual surveillance and manufacturing quality measurement, it is required to detect the full frame view at all times, the camera should be capable of providing additional visual information when zoomed to the region of interest (ROI). This paper presents a video camera system incorporating an autonomous pan, tilt and zoom functionality based on ROI decided upon motion detection capability, which provides more visual information in zoomed modes while detecting the full view at all times. The intelligent ROI decision can be made not only based on motion but also shape, color, sizes etc. of targeted objects for tracking. Image sharpening and noise reduction methods are also provided within the camera system. Standard testing results indicate that this camera system can resolve up to 800 TV lines and produces comparable color quality to other CCD based CCTV surveillance cameras, especially at good lighting conditions.	algorithm;anisotropic diffusion;autonomous robot;bilinear filtering;cmos;charge-coupled device;closed-circuit television;digital zoom;display resolution;field-programmable gate array;gradient;image scaling;interpolation;noise reduction;nonlinear system;pixel;region of interest;sensor;television lines;unsharp masking;zero suppression	Chaminda Weerasinghe;Magnus Nilsson;Serge Lichman;Igor Kharitonenko	2004	IEEE Transactions on Consumer Electronics	10.1109/TCE.2004.1341679	smart camera;computer vision;simulation;index term;computer science;digital zoom;noise reduction;computer graphics (images);region of interest	Vision	59.76214610748623	-58.055172559172746	101993
3dcc5335bdba22247bd316d0b27479ff2b7ec249	design tool of color schemes on the cielab space	information visualization;image colour analysis data visualisation;cie1976l a b;information visualization color scheme design cie1976l a b;image color analysis brightness aerospace electronics data visualization visualization color shape;color scheme design;color scheme design color schemes information visualization data visual representations color scheme design cielab color space euclidean distances	Color plays an important role in information visualization. To use the colors as visual representations of the data effectively, designing color schemes that consider perceptual differences between colors is necessary. We developed a tool to assist in color-scheme design when using the CIE1976L*a*b* (or CIELAB) color space. The developed tool provides functions to assist in designing color schemes for nominal, ordinal, and quantitative data. The CIELAB color space has a property in which the Euclidean distances between any two colors in the space can approximate the perceptual differences between them. Thus, designing color schemes for data representation is convenient. However, because the CIELAB color space has a distorted shape, color scheme design in such space is not easy. Our tool allows the designer to see the positions of colors in the CIELAB color space visually to facilitate scheme design. Moreover, our tool provides information to the designer on the distinguishability of the selected colors, and offers functions to correct color schemes automatically.	approximation algorithm;color space;data (computing);design tool;euclidean distance;information visualization;ordinal data	Kazuo Misue;Hatsune Kitajima	2016	2016 20th International Conference Information Visualisation (IV)	10.1109/IV.2016.24	color gradient;color histogram;false color;rgb color model;computer vision;icc profile;color model;color quantization;hsl and hsv;color depth;color image;color difference;mathematics;color balance;color space;web colors;engineering drawing;computer graphics (images)	Robotics	61.17511790576598	-62.78252334833632	102002
5c5b8910ffd4ce925b361e2cf3157ecd6de68190	color deconvolution method with dab scatter correction for bright field image analysis		For histochemical staining, to highlight multiple biomarkers within a sample, multiple stains with different light spectral absorption characteristics are deployed (i.e. multiplexing). To reconstruct the single stain contrast from a multiplexed sample, the conventional color deconvolution method assumes that light extinction follows Lambert-Beer’s law during imaging process and the optical density (OD) measured from the image is linearly related to the stain amount. However, this assumption does not hold well for commonly used diaminobenzidine (DAB) stain due to its precipitate-forming reaction during sample processing. Besides absorption, scattering also contributes to the light extinction process which causes the non-linear relation between the OD value and the stain amount. Therefore, using the conventional method may not have sufficient accuracy for quantified stain analysis, especially when DAB presents at high concentration levels. In this paper, our study shows that DAB presents different chromatic properties at different concentration levels. Therefore, we propose a new color deconvolution method to address the issue by employing a set of reference colors vectors, each of which characterizes a DAB concentration level. Then, the reference color vector that best represents the true DAB concentration level in the mixture is automatically selected for color deconvolution. Both visual and quantified assessments are provided to show that the method enables detection for a broader dynamic range of DAB concentration and therefore should be preferred by the user for bright field image analysis.	deconvolution;image analysis	Yao Nie;Christian Roessler;Emilia Andersson;Oliver Grimm	2018		10.1117/12.2293576	analytical chemistry;dynamic range;stain;deconvolution;staining;scattering;mathematics	Vision	66.87562081626365	-64.11257022982457	102005
1e1189b25ca3dae71e5b4d21f9e47144bec3c8fe	order consistent change detection via fast statistical significance testing	fast statistical significance testing;monotone regression;change detection;building block;testing lighting robustness surveillance filtering gain measurement cameras joining processes quantization image sequences;automatic camera gain;statistical significance;testing;image sensors;unknown monotonic transformations;global illumination;visual surveillance;sensitivity;distance measurement;random noise;order consistent change detection;computational complexity;fast algorithm;pixel;monotonic regression;image sequence;mathematical model;image sequences order consistent change detection fast statistical significance testing visual surveillance applications illumination dependency unknown monotonic transformations automatic camera gain monotonic regression;regression analysis;statistical testing;lighting;statistical testing computational complexity image sensors image sequences regression analysis;illumination dependency;visual surveillance applications;noise;image sequences	Robustness to illumination variations is a key requirement for the problem of change detection which in turn is a fundamental building block for many visual surveillance applications. The use of ordinal measures is a powerful way of filtering out illumination dependency in representing appearance, and several such measures have been proposed in the past for change detection. By design, these measures are invariant to unknown monotonic transformations that may be caused due to global illumination changes or automatic camera gain. However, previous work has left theoretical and practical gaps that limit their full potential from being realized. For instance, random noise has not been given a principled treatment. In this paper, we formulate the change detection problem in terms of order consistency and show that in the presence of noise with known statistical properties, significance tests for order consistency yield much better results than the state of the art. Since ordinal measures require a reordering of patches, they are usually expensive in practice (O(n*log n) at best). We improve upon this by connecting the problem to monotonic regression, and applying a fast algorithm from the corresponding literature. We also show that good trade offs between speed and accuracy can be made by quantization to achieve accurate and very fast matching algorithms in practice. We demonstrate superior performance on statistical simulations as well as real image sequences.	active set method;algorithm;apache axis;closed-circuit television;closure (computer programming);computation;convex function;global illumination;isotonic regression;linear inequality;noise (electronics);non-monotonic logic;ordinal data;outline of object recognition;quantization (signal processing);significance arithmetic;simulation;statistical model;turing completeness;utility functions on indivisible goods	Maneesh Kumar Singh;Vasu Parameswaran;Visvanathan Ramesh	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2008.4587668	computer vision;statistical hypothesis testing;sensitivity;isotonic regression;computer science;noise;machine learning;image sensor;mathematical model;lighting;mathematics;statistical significance;software testing;computational complexity theory;global illumination;change detection;pixel;regression analysis;statistics	Vision	55.03508925400228	-54.22655275822065	102060
de1931ab2649e5b067af7cf5432dee60108a2b29	inverse ordered dithered halftoning using permutation filters	image sampling;nonlinear filters;nonlinear filters image reconstruction filtering theory inverse problems image sampling;non linear filtering;visual quality;image reconstruction;visual cues;high visual quality inverse ordered dithered halftoning space rank ordering continuous tone image reconstruction modular nonlinear filters binary permutation filters continuous tone information image details visual cues halftone samples multiset permutation pixels halftone observation window window size simulations image source characteristics;image reconstruction information filtering information filters signal processing gray scale low pass filters reconstruction algorithms filtering theory research and development robustness;filtering theory;inverse problems	"""The problem of reconstructing a continuous-tone image given its ordered dithered halftone image is considered. We utilize a modular class of non-linear filters, denoted as binary permutation filters, which can reconstruct the continuous-tone information as well as image details which provide important visual cues. Binary permutation filters are based on the space-rank ordering of the halftone samples which is provided by the multiset permutation of the """"on"""" pixels in a halftone observation window. By varying the space-rank order information utilized in the estimate, for a given window size, we obtain a wide range of filters. We present simulations showing that binary permutation filters are modular, robust to image source characteristics, and that their results produce high visual quality image reconstruction. >"""	dither	Yeong-Taeg Kim;Gonzalo R. Arce	1994		10.1109/ICIP.1994.413509	iterative reconstruction;computer vision;sensory cue;inverse problem;theoretical computer science;machine learning;mathematics	Theory	59.25013787248652	-64.33719238637158	102092
5c822440327319a6032355b7e84b8701a189b511	automatic detection of 3d quality defects in stereoscopic videos using binocular disparity	binocular disparity visual discomfort 3d quality stereoscopic video;3d quality binocular disparity stereoscopic video visual discomfort;measurement;stereo image processing three dimensional displays videos estimation quality assessment visualization measurement;visualization;quality assessment;estimation;three dimensional displays;stereo image processing;videos	The 3D video quality issues that may disturb the human visual system and negatively impact the 3D viewing experience are well known and become more relevant as the availability of 3D video content increases, primarily through 3D cinema, but also through 3D television. In this paper, we propose four algorithms that exploit available stereo disparity information, in order to detect disturbing stereoscopic effects, namely, stereoscopic window violations, bent window effects, uncomfortable fusion object objects, and depth jump cuts on stereo videos. After detecting such issues, the proposed algorithms characterize them, based on the stress they cause to the viewer’s visual system. Qualitative representative examples, quantitative experimental results on a custom-made video data set, a parameter sensitivity study, and comments on the computational complexity of the algorithms are provided, in order to assess the accuracy and the performance of stereoscopic quality defect detection.	3d film;3d television;algorithm;binocular disparity;binocular vision;cinema 4d;computational complexity theory;digital video;sensor;software bug;stereoscopy;window function	Sotirios Delis;Ioannis Mademlis;Nikos Nikolaidis;Ioannis Pitas	2017	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2015.2511518	binocular disparity;computer vision;estimation;visualization;computer science;mathematics;multimedia;measurement;statistics;computer graphics (images)	Vision	63.69311968891326	-62.558787883171064	102236
003c124df9e30d69c74da1098e525a364682a66a	relative gradients for image lighting correction	poisson equation;image recognition;poisson equation lighting correction relative gradients image enhancement;lighting image reconstruction computer vision degradation image generation filtering layout face recognition image recognition image restoration;image restoration;computer vision;lighting correction;image enhancement;lighting variation relative gradients image lighting correction computer vision image enhancement lighting condition image reconstruction;image colour analysis;image reconstruction;relative gradients;lighting;image lighting correction;rendering computer graphics;lighting variation;image reconstruction image colour analysis image enhancement;lighting condition	A fundamental problem in computer vision is to enhance images under various lighting conditions. This paper proposes an approach to lighting correction through reconstructing the image based on relative gradients. Compared with the absolute gradients as exemplified in the originally acquired image, the relative gradients are more stable under the variation of lighting. Extensive experiments have been carried out over images under a wide range of lighting variations. Comparisons with popular lighting correction methods are presented.	autostereogram;computer vision;experiment;gradient;solver	Zujun Hou;Wei-Yun Yau	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495453	iterative reconstruction;image restoration;computer vision;computer science;lighting;poisson's equation;computer graphics (images)	Vision	56.188105823590504	-56.60071685757669	102266
e2b2cb8419737c908cd747f0916dae1447b0180a	an efficient dehazing method for edge enhancement by using entropy-map	time complexity;consumer electronics;image texture edge detection filtering theory image colour analysis image enhancement;image edge detection;image color analysis;image dehazing efficient dehazing method edge enhancement entropy map complex texture regions entropy information incorrect transmission map transmission map ycbcr color space threshold value averaging filter;entropy;flowcharts;image edge detection entropy image color analysis flowcharts conferences consumer electronics time complexity;conferences	In this paper, we propose a novel dehazing method for effectively enhancing complex texture regions by using entropy information. The proposed method improves a drawback of an existing method due to estimating an incorrect transmission map. The proposed method consisted of two steps. In the first step, for effectively estimating a transmission map, an entropy map is generated from YCbCr color space. Then, by applying a threshold value to the entropy map, an input image is divided into complex texture regions and others. In the second step, in order to generate a transmission map, an averaging filter is applied to complex texture regions. And two edge filters are applied to non-texture regions. Consequently, a correct transmission map preserving more details in complex texture regions is generated. Experimental results have shown that the proposed method achieves better performance of image dehazing than existing methods.	color space;edge enhancement;self-organizing map	Ki Tae Park;Yong Min Kim;Young Shik Moon	2015	2015 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2015.7066446	image texture;time complexity;computer vision;entropy;speech recognition;color image;flowchart;computer science;mathematics;algorithm;computer graphics (images)	Robotics	56.26819841852541	-64.36931009038577	102473
b91c4dc22fcddef6fea73b2476a81e17bd90ea74	use of linear diffusion in depth estimation based on defocus cue	computer vision;edge detection	Diffusion has been used extensively in computer vision. Most common applications of diffusion have been in low level vision problems like segmentation and edge detection. In this paper a novel application of the linear diffusion principle is made for the estimation of depth using the properties of the real aperture imaging system. The method uses two defocused images of a scene and the lens parameter setting as input and estimates the depth in the scene, and also generates the corresponding fully focused equivalent pin-hole image. The algorithm described here also brings out the equivalence of the two modalities, viz. depth from focus and depth from defocus for structure recovery.	algorithm;coefficient;computer vision;depth map;digital forensics framework (dff);edge detection;image segmentation;synthetic intelligence;turing completeness;virtual reality;viz: the computer game	Vinay P. Namboodiri;Subhasis Chaudhuri	2004			computer vision;pattern recognition;computer science;equivalence (measure theory);artificial intelligence;aperture;edge detection	Vision	56.77028619578999	-57.44321789395504	102607
559af4bc3aadd5a2bca07dafdfb77b320264dbff	fast decompression of textures using vector quantisation and wavelet decomposition		In order to make efficient use of memory for storage of game data, developers often use compression techniques. A common disadvantage of many widely used compression methods is that they require significant processing to reconstruct compressed data, and this can prohibit their use in mainstream gaming environments, where CPU time is in high demand.  This article presents an image compression method that combines the techniques of vector quantisation (VQ) and wavelet decomposition. Our VQ method is performed using an unsupervised self-organising neural network: the Scale Invariant Map, which has previously been used in image compression. The major advantage of this method is that image decompression can be performed very quickly. Wavelet decomposition is used to overcome some of the problems normally associated with VQ methods, and we show that our method provides lossy compression without blocky artefacts in reconstructed images.  A major advantage of this work is that it combines well with the technique of mip-mapping. Low-detail copies of textures can be decompressed quickly, making this a very useful technique for generating 3D scenes using many textures. Another strength of the method is that it supports progressive downloading, and is therefore suitable for streaming of game content over networks.	data compression;vector quantization;wavelet	Stephen J. McGlinchey	2005	Int. J. Intell. Games & Simulation		computer vision;simulation;computer science;computer graphics (images)	AI	65.50912621965938	-54.438235959492566	102799
2c263756544af29d84f1bd80acfc79cf0bab0051	image quality assessment for dibr synthesized views using elastic metric		Frames of free viewpoint video (FVV) synthesized with depth image-based rendering (DIBR) mainly contains special local artifacts like geometric distortions, in which the shape of objects may be stretched/bent. Human observers tend to perceive such local severe deformations instead of consistent shifting artifacts that penalized by most of the existing metrics. Elastic metric is capable of measuring the difference in stretching or bending between two curves, and thus is suitable for evaluating such geometric distortions. In this paper, an elastic metric based image quality assessment (EM-IQA) scheme is proposed by first selecting local distortion regions and then quantifying the deformations of curves. According to the experimental results on the IRCCyn/IVC DIBR image database, the proposed EM-IQA outperforms the state of the art metrics designed for synthesized images and obtains a gain of 6.97% in pearson correlation compared to the second best performing MP-PSNRreduced.	algorithm;amazon elastic compute cloud (ec2);artifact (software development);distortion;image quality;microwave;portable c compiler;speeded up robust features	Suiyi Ling;Patrick Le Callet	2017		10.1145/3123266.3123329	computer vision;bending;artificial intelligence;rendering (computer graphics);image quality;pearson product-moment correlation coefficient;computer science;mathematical optimization;distortion	Vision	59.15328477252857	-61.01337673389009	103033
00e7ad8de689c85de5da7cafba67c5b3e9ea7c25	cfa-based motion blur removal using long/short exposure pairs	kernel;image motion analysis;high dynamic range image cfa based motion blur removal method short exposure images noise suppression object motion blur motion pixel detection color filter array image low computational complexity;colored noise;image alignment;optical filters;pixel motion detection object detection colored noise filters sensor arrays cameras deconvolution kernel noise robustness;filters;motion blur color filter array cfa sensor noise modeling hdr imaging image alignment;color filter array cfa;boundary object;spatial filters;hdr imaging;noise robustness;high dynamic range imaging;computational modeling;motion blur;computational complexity;image colour analysis;feature extraction;color filter array;pixel;deconvolution;image denoising;object detection computational complexity filtering theory image colour analysis image denoising image motion analysis;sensor noise modeling;motion detection;sensor arrays;filtering theory;cameras;object detection;noise	This paper presents an efficient and effective motion blur removal method based on long and short exposure images. The two images are captured sequentially and motion pixels between the images are then robustly detected, with suppression of noise and prevention of artifacts around object boundaries. Object motion blur is removed and high quality image is obtained by merging the two images with taking into account the detected motion pixels. The proposed method is directly performed on the CFA (Color Filter Array) image which only has one color component per pixel. It has low computational complexity and low memory requirements. The proposed method also achieves a HDR (High Dynamic Range) image at the same time.	color filter array;color space;computational complexity theory;control flow analysis;display resolution;gaussian blur;high dynamic range;high-dynamic-range imaging;kinesiology;noise reduction;pixel;requirement;traffic enforcement camera;zero suppression	Pongsak Lasang;Chin Phek Ong;Sheng Mei Shen	2010	IEEE Transactions on Consumer Electronics	10.1109/TCE.2010.5505936	image restoration;computer vision;color filter array;kernel;colors of noise;feature extraction;computer science;noise;deconvolution;motion estimation;optical filter;mathematics;computational complexity theory;computational model;pixel;computer graphics (images)	Vision	57.692266705609846	-57.66376647674623	103187
b8ae436fbf74d62298cacd22fa7b1c97599bb98d	a hierarchical method of map-based stochastic diffusion and disparity estimation	minimisation;estimation theory;optimal method;disparity estimation;stochastic processes motion estimation broadcasting stochastic systems optimization methods image segmentation tracking image restoration bayesian methods markov random fields;maximum likelihood estimation;textureless regions hierarchical method map based stochastic diffusion disparity estimation optimization method potential function correspondence field line fields segmentation field hierarchical scheme potential space geometric relation neighborhood fields estimation performances occluded regions;stochastic processes;stereo image processing;potential function;stereo image processing maximum likelihood estimation stochastic processes estimation theory minimisation	This paper talks about a hierarchical approach on stochastic diffusion in the MAP-based estimation. Stochastic diffusion has been proposed for an optimization method to minimize the potential function in the MAP-based estimation, and showed good performances in the simultaneous estimations of correspondence, line, and segmentation fields. This paper applies stochastic diffusion to the MAP-based estimation of disparity and line fields in the hierarchical scheme. The proposed hierarchical method combines two successive approximations of the disparity field and potential space at the same time. This hierarchical method propagates not only the geometric relation but also interactions of neighborhood fields. The experimental results show that the proposed hierarchical stochastic diffusion decreases the memory and computational burden and improves the estimation performances in the occluded or textureless regions.	binocular disparity	Sang Hwa Lee;Yasuaki Kanatsugu;Jong-Il Park	2002		10.1109/ICIP.2002.1040007	stochastic process;computer vision;minimisation;mathematical optimization;mathematics;maximum likelihood;estimation theory;statistics	Vision	54.763045601572145	-56.366651775704305	103405
0c89e05c2a9e0d7490ba14c9e0ce84b74cf67ae6	perceptual quality improvement for synthesis imaging of chinese spectral radioheliograph	compressed sensing;solar radio astronomy;image reconstruction;aperture syntheis	Chinese Spectral Radioheliography can generate the images of the Sun with good spatial resolutions. It employs the Aperture Synthesis principle to image the Sun with plentiful solar radio activities. However, due to the limitation of the hardware, specifically the limited number of antennas, the recorded signal is extremely sparse in practice, which results in unsatisfied solar radio image quality. In this paper, we study the image reconstruction of Chinese Spectral RadioHeliograph (CSRH) by the aid of compressed sensing (CS) technique. In our proposed method, we adopt dictionary technique to represent solar radio images sparsely. The experimental results indicate that the proposed algorithm contributes both PSNR and subjective image quality improvements of synthesis imaging of CSRH markedly.	algorithm;compressed sensing;dictionary;image quality;iterative reconstruction;peak signal-to-noise ratio;sparse matrix	Long Xu;Lin Ma;Zhuo Chen;Yihua Yan;Jinjian Wu	2015		10.1007/978-3-319-24078-7_10	iterative reconstruction;computer vision;computer science;compressed sensing	Vision	67.46171878821752	-65.53788947201294	103516
cc6ae438c520f9046d233852e3aba84f516f7b73	a high quality fast inverse halftoning algorithm for error diffused halftones	nonlinear filters;filtering;c implementation;image storage;psnr;image processing;c language fir filters adaptive filters low pass filters image processing;finite impulse response filter;gray scale;c language;smoothing methods;adaptive filters;error diffused halftones;horizontal gradient;image edge detection;high quality fast inverse halftoning algorithm;computer errors image edge detection smoothing methods laboratories finite impulse response filter image storage gray scale filtering signal to noise ratio nonlinear filters;low pass filters;vertical gradient;fir filters;operations;signal to noise ratio;c implementation high quality fast inverse halftoning algorithm error diffused halftones separable 7 spl times 7 fir filter horizontal gradient vertical gradient operations image storage psnr subjective quality;subjective quality;computer errors;embedded software;separable 7 7 fir filter	|We present an inverse halftoning algorithm for error di used halftones. At each pixel, the algorithm applies a separable 7 7 FIR lter parameterized by the computed local horizontal and vertical gradients. All operations are entirely local; only 7 rows of image storage and fewer than 300 operations per pixel are required. The algorithm can be easily implemented in embedded software or hardware. We compare our algorithm with previously reported approaches, and show that it delivers comparable PSNR and subjective quality at a fraction of the computation and memory requirements. A C implementation of the algorithm is available at http://www.ece.utexas.edu/~bevans/ projects/inverseHalftoning.html.	algorithm;computation;computer hardware;embedded software;finite impulse response;gradient;peak signal-to-noise ratio;pixel;requirement	Thomas D. Kite;Niranjan Damera-Venkata;Brian L. Evans;Alan C. Bovik	1998		10.1109/ICIP.1998.723317	computer vision;speech recognition;image processing;computer science;finite impulse response	Graphics	55.0471060661108	-65.94908744751451	103529
e1def6adcb06cd71bb9daccfc99625183fd8c4b7	a new single image dehazing method with msrcr algorithm	msrcr algorithm;dehazing;single image;physical model	In this paper, we propose a new single image dehazing method using a Multi-Scale Retinex with Color Restoration algorithm. The overall dehazing process involves three parts, the atmospheric light value calculation, transmission map estimation, and optimization after dehazing. In addition to a subjective evaluation, image quality was also evaluated objectively using two indicators, Average Gradient, and Information Entropy, in order to consider the detail of the image texture, and image dynamic range. The experimental results show that our algorithm can effectively improve the image quality degraded by foggy weather, is fast, and can retain image details effectively.	algorithm;autostereogram;dynamic range;entropy (information theory);gradient;image quality;image texture;mathematical optimization	Jin-Bao Wang;Ning He;Ke Lu	2015		10.1145/2808492.2808511	computer vision;simulation;geography;computer graphics (images)	Vision	58.74885855019081	-62.05654723952385	103763
8ff93fa6e52b1ac53cde7c1331d7aba58998cc81	a motion adaptive wavelet-denoising for frame-rate up-conversion	mirrors;symmetric optical flow frame rate up conversion wavelet zerotree;interpolation;prediction algorithms;speech;video signal processing image denoising image motion analysis image sequences interpolation;interpolation motion adaptive wavelet denoising wavelet zerotree based shrinkage algorithm optical flow estimation video frame rate up conversion frame denoising pixel location temporal motion path;estimation;noise reduction;noise reduction prediction algorithms estimation interpolation mirrors speech	This study introduces a method that employs a wavelet zerotree-based shrinkage algorithm motion-adaptively-combined with optical flow estimation for video frame rate up-conversion. In the method, an optical flow estimation technique is used to predict and insert frames between existing ones, and then, the predicted frames are denoised by using a specific wavelet-based algorithm, where each pixel location is examined independently through its own temporal motion path. The denoising was performed by shrinking zero-tree footprints to remove temporal oddities. The resulting video was observed to be smoother and more fluent as compared to optical flow - only interpolation.	algorithm;coefficient;embedded system;motion compensation;motion interpolation;noise reduction;online and offline;optical flow;pixel;requirement;server (computing);server-side;video denoising;wavelet	Sukru Gorgulu;Ömer Nezih Gerek	2016	2016 24th Signal Processing and Communication Application Conference (SIU)	10.1109/SIU.2016.7496180	computer vision;speech recognition;pattern recognition;motion estimation;mathematics;video denoising	Vision	58.43572657032465	-55.432066756028924	103977
723eca932dac6b1009fb30fddf4fb3fd067e844f	controllable image illumination enhancement with an over-enhancement measure		The quality of images or videos that suffer from exposure distortions can be enhanced using histogram equalization or retinex methods. However, the relationship between the visual quality and the degree of enhancement is an inverted U-shaped function with a peak point, and many existing methods have parameters that have no clear relationship with image quality. We introduce a controllable illumination enhancement system, where the degree of enhancement can be adjusted using a single parameter. We then propose an over-enhancement measure, Lightness Order Measure (LOM), which quantifies the unnaturalness based on a local inversion of lightness order. We explore the relationship between the peak point and LOM in a subjective test. The results indicate that LOM reduces content dependency compared to existing methods. Our subjective test also evaluates the image quality of our enhancement, and demonstrates the effectiveness of our method.	distortion;histogram equalization;image quality	Chen Bai;Amy R. Reibman	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451321	computer vision;image quality;artificial intelligence;lightness;pattern recognition;distortion;color constancy;computer science;histogram equalization	Robotics	60.96359927070541	-61.89299170767885	104038
d0b84ee0c3c01387b5050e1f2f24573ee7055a98	a 2d wigner distribution-based multisize windows technique for image fusion	high resolution;articulo;image fusion;universiteitsbibliotheek;multifocus;wigner distribution;space frequency;performance assessment	We present a scheme for image fusion based on a 2D implementation of the Wigner Distribution (WD) combined with a multisize windows technique. The joint space–frequency distribution provided by the WD can be managed as a measure of saliency that indicates which regions among different sources (channels) should be preserved. However such a saliency measure varies significantly according to the local analysis (window) in which the WD is calculated. Hence, large windows provide high resolution and robustness against possible noise present in channels and small windows provide accurate localization. The multisize windows technique combines the saliency measures of different windows taking advantage of the benefit contributed by each size. The performance assessment was conducted in artificial multifocus images under different noise exposures as well as real multifocus scenarios. 2007 Elsevier Inc. All rights reserved.	image fusion;image resolution;microsoft windows;wigner quasiprobability distribution	Rafael Redondo;Sylvain Fischer;Filip Sroubek;Gabriel Cristóbal	2008	J. Visual Communication and Image Representation	10.1016/j.jvcir.2007.06.009	computer vision;image resolution;computer science;wigner distribution function;mathematics;image fusion;computer graphics (images)	AI	60.955017996217805	-65.50940998850004	104113
3870e5010c75b0e49480f4ab29e913d188e24166	shape from mixed polarization		Shape from Polarization (SfP) estimates surface normals using photos captured at different polarizer rotations. Fundamentally, the SfP model assumes that light is reflected either diffusely or specularly. However, this model is not valid for many real-world surfaces exhibiting a mixture of diffuse and specular properties. To address this challenge, previous methods have used a sequential solution: first, use an existing algorithm to separate the scene into diffuse and specular components, then apply the appropriate SfP model. In this paper, we propose a new method that jointly uses viewpoint and polarization data to holistically separate diffuse and specular components, recover refractive index, and ultimately recover 3D shape. By involving the physics of polarization in the separation process, we demonstrate competitive results with a benchmark method, while recovering additional information (e.g. refractive index).	algorithm;benchmark (computing);holism;normal (geometry);polarization (waves);polarizer	Vage Taamazyan;Achuta Kadambi;Ramesh Raskar	2016	CoRR		polarizer;optics;refractive index;separation process;polarization (waves);specular reflection;physics	Vision	59.43533948372144	-52.27542725044373	104133
bcaacae0728c6e6eba2279afaa6ebf2eb4b4cf7d	complete multi-view reconstruction of dynamic scenes from probabilistic fusion of narrow and wide baseline stereo	narrow baseline stereo;video signal processing image reconstruction optimisation probability stereo image processing;optimisation;mincut problem;multiview wide baseline stereo reconstruction technique;probability;video signal processing;probabilistic fusion;3d videos;layout image reconstruction videos stereo image processing cameras surface reconstruction robustness stability predictive models fuses;original probabilistic framework;stereo correspondence problems;mincut problem dynamic scenes complete multiview reconstruction probabilistic fusion narrow baseline stereo 3d videos multiview wide baseline stereo reconstruction technique stereo correspondence problems stereo photo consistency image content stability original probabilistic framework multiview structure from motion;multiview structure from motion;correspondence problem;motion capture;3d model;three dimensional displays;feature extraction;image reconstruction;stereo image processing;stereo photo consistency;image content stability;robustness;probabilistic logic;3d video;structure from motion;cameras;dynamic scenes complete multiview reconstruction;videos;dynamic scenes	This paper presents a novel approach to achieve accurate and complete multi-view reconstruction of dynamic scenes (or 3D videos). 3D videos consist in sequences of 3D models in motion captured by a surrounding set of video cameras. To date 3D videos are reconstructed using multiview wide baseline stereo (MVS) reconstruction techniques. However it is still tedious to solve stereo correspondence problems: reconstruction accuracy falls when stereo photo-consistency is weak, and completeness is limited by self-occlusions. Most MVS techniques were indeed designed to deal with static objects in a controlled environment and therefore cannot solve these issues. Hence we propose to take advantage of the image content stability provided by each single-view video to recover any surface regions visible by at least one camera. In particular we present an original probabilistic framework to derive and predict the true surface of models. We propose to fuse multi-view structure-from-motion with robust 3D features obtained by MVS in order to significantly improve reconstruction completeness and accuracy. A min-cut problem where all exact features serve as priors is solved in a final step to reconstruct the 3D models. In addition, experimental results were conducted on synthetic and challenging real world datasets to illustrate the robustness and accuracy of our method.	3d modeling;baseline (configuration management);bayesian approaches to brain function;cut (graph theory);free viewpoint television;global optimization;iterative reconstruction;mathematical optimization;maxima and minima;minimum cut;motion capture;np-completeness;photo-consistency;stereoscopy;structure from motion;synthetic intelligence	Tony Miu Tung;Shohei Nobuhara;Takashi Matsuyama	2009	2009 IEEE 12th International Conference on Computer Vision	10.1109/ICCV.2009.5459384	iterative reconstruction;computer vision;structure from motion;motion capture;simulation;feature extraction;computer science;probability;probabilistic logic;correspondence problem;robustness;computer graphics (images)	Vision	55.0674662087078	-53.86182980423844	104136
1a06bee906e4f4ae48cca400867bdf8e14f07035	overt visual attention for free-viewing and quality assessment tasks: impact of the regions of interest on a video quality metric	gaze;evaluation performance;evaluation image;control de calidad;image coding;performance evaluation;image processing;data compression;video signal processing;movimiento ocular;evaluacion prestacion;localization;video compression;procesamiento imagen;localizacion;mirada;region interes;video quality metric;qualite image;atencion visual;traitement image;observador;codage image;video coding;regard;observateur;quality assessment;compression image;evaluation subjective;localisation;senal video;signal video;image compression;codage video;region of interest;image quality;eye movement;image sequence;poursuite cible;oculomotor;visual features;controle qualite;traitement signal video;video signal;full reference;evaluacion imagen;secuencia imagen;calidad imagen;image evaluation;attention visuelle;compresion dato;region interet;eye tracking;target tracking;quality control;mouvement oculaire;subjective evaluation;visual attention;free viewing and quality tasks;observer;video quality assessment;sequence image;compression donnee;interest region;evaluacion subjetiva;compresion imagen	The aim of this study is to understand how people watch a video sequence during free-viewing and quality assessment tasks. To this end, two eye tracking experiments were carried out. The video dataset is composed of ten original video sequences and fifty impaired video sequences (5 levels of impairments obtained by a H.264 video compression). A first experiment consisted in recording eye movements in a free-viewing task. The ten original video sequences were used. The second experiment concerned an eye tracking experiment in a context of a subjective quality assessment. Eye movements were recorded while observers judged on the quality of the fifty impaired video sequences. The comparison between gaze allocations indicates the quality task has a moderate impact on the visual attention deployment. This impact increases with the presentation number of impaired video sequences. The locations of regions of interest remain highly similar after several presentations of the same video sequence, suggesting that eye movements are still driven by the low level visual features after several viewings. In addition, the level of distortion does not significantly alter the oculomotor behavior. Finally, we modified the pooling of an objective full-reference video quality metric by adjusting the weight applied on the distortions. This adjustment depends on the visual importance (the visual importance is deduced from the eye tracking experiment realized on the impaired video sequences). We observe that a saliency-based distortion pooling does not significantly improve the performances of the video	bottom-up parsing;computation;computational model;data compression;distortion;expect;experiment;eye tracking;h.264/mpeg-4 avc;high- and low-level;map;performance;region of interest;relevance;software deployment;top-down and bottom-up design	Olivier Le Meur;Alexandre Ninassi;Patrick Le Callet;Dominique Barba	2010	Sig. Proc.: Image Comm.	10.1016/j.image.2010.05.006	data compression;subjective video quality;computer vision;simulation;image processing;computer science;algorithm;statistics;computer graphics (images)	Vision	63.20138620265654	-63.01660877006533	104174
4563f32a59c770bca13d08e8f25d5d776d1db24d	a novel quality image fusion assessment based on maximum codispersion		In this paper, we present a novel objetive measure for image fusion based on the codispersion quality index, following the structure of Piella’s metric. The measure quantifies the maximum local similarity between two images for many directions using the maximum codispersion quality index. This feature is not commonly assessed by other measures of similarity between images. To vizualize the performance of the maximum codispersion quality index we suggested two graphical tools. The proposed fusion measure is compared to image structural similarity based metrics of the state-of-art. Different experiments performed on several databases show that our metric is consistent with human visual evaluation and can be applied to evaluate different image fusion schemes.	image fusion	Silvina Pistonesi;Jorge Martinez;Silvia María Ojeda;Ronny O. Vallejos	2015		10.1007/978-3-319-25751-8_46	pattern recognition;graphical tools;artificial intelligence;computer vision;structural similarity;computer science;image fusion	Vision	61.761620173739615	-64.24963832810971	104364
d9b533de30e4f717b4b576ea2b3832431d48f84b	learning sparse representation for no-reference quality assessment of multiply distorted stereoscopic images	stereo image processing distortion three dimensional displays measurement image quality visualization databases;binocular combination blind no reference multiply distorted stereoscopic image sparse representation	Binocular combination under different distortion types poses a great challenge to three-dimensional image quality assessment (3D-IQA). However, the research works on 3D-IQA with multiple distortion types are very limited. In this paper, we first construct a new multiply distorted stereoscopic image database (NBU-MDSID), which is composed of 270 multiply distorted stereoscopic images and 90 singly distorted stereoscopic images that are corrupted simultaneously and independently by blurring, JPEG compression, and noise injection. We then propose a new multimodal blind metric for quality assessment of multiply distorted stereoscopic images. Inspired by multimodal sparse representation framework, modality-specific dictionaries and the corresponding projection matrices are learned from the singly distorted training database at the training stage, and the testing stage only needs to estimate the quality score based on the reconstruction errors. Experimental results demonstrate the effectiveness of our blind metric.	3d computer graphics;binocular vision;dictionary;distortion;image quality;modality (human–computer interaction);multimodal interaction;sparse approximation;sparse matrix;stereoscopy	Feng Shao;Weijun Tian;Weisi Lin;Gangyi Jiang;Qionghai Dai	2017	IEEE Transactions on Multimedia	10.1109/TMM.2017.2685240	computer vision;artificial intelligence;pattern recognition;computer science;distortion;stereoscopy;visualization;matrix (mathematics);image quality;sparse approximation;quality score;jpeg	Vision	62.49996990439445	-64.96855694036107	104472
eb6da073b723e31f047cd01f54aaf15ae4acbcf1	a centroid algorithm for stabilization of turbulence-degraded underwater videos	measurement;image restoration;surface reconstruction;image reconstruction;image registration;surface waves;videos	This paper addresses the problem of stabilizing underwater videos with non-uniform geometric deformations or warping due to a wavy water surface. It presents an improved method to correct these geometric deformations of the frames, providing a high-quality stabilized video output. For this purpose, a non-rigid image registration technique is employed to accurately align the warped frames with respect to a prototype frame and to estimate the deformation parameters, which in turn, are applied in an image dewarping technique. The prototype frame is chosen from the video sequence based on a sharpness assessment. The effectiveness of the proposed method is validated by applying it on both synthetic and real- world sequences using various quality metrics. A performance comparison with an existing method confirms the higher efficacy of the proposed method.	algorithm;align (company);charge-coupled device;distortion;gaussian blur;image noise;image registration;jumbo frame;motion estimation;pixel;prototype;synthetic intelligence;turbulence	Kalyan Kumar Halder;Manoranjan Paul;Murat Tahtali;Sreenatha G. Anavatti;M. Manzur Murshed	2016	2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)	10.1109/DICTA.2016.7796985	iterative reconstruction;image restoration;computer vision;simulation;surface reconstruction;surface wave;computer science;image registration;mathematics;measurement;computer graphics (images)	Robotics	57.51209255794803	-58.04035740737099	104495
5a810aafdae620e626cb5aa25a0fe2d109395289	underwater video dehazing based on spatial-temporal information fusion	dark channel prior;underwater video dehazing;guided filter;spatial temporal fusion	In this paper, a novel multidimensional underwater video dehazing method is presented to restore and enhance the underwater degraded videos. Videos in the underwater suffer from medium scattering and light absorption. The absorption of light traveling in the water makes the underwater hazing videos different from the atmosphere hazing videos. In order to dehaze the underwater videos, a spatial---temporal information fusion method is proposed which includes two main parts. One is transmission estimation, which is based on the correlation between the adjacent frames of videos to keep the color consistency, where fast tracking and the least square method are used to reduce the influence of camera and object motions and water flowing. Another part is background light estimation to keep consistent atmospheric light values in a video. Extensive experimental results demonstrate that the proposed algorithm can have superior haze removing and color balancing capabilities.		Chunmei Qing;Feng Yu;Xiangmin Xu;Wenyou Huang;Jianxiu Jin	2016	Multidim. Syst. Sign. Process.	10.1007/s11045-016-0407-2	computer vision;simulation	HPC	57.64677537523982	-59.61362370580483	104550
af10893c27ca171c6dea9abe98bcbf5fa6034563	3-d shape recovery of hybrid reflectance surface using indirect diffuse illumination	light scattering stereo image processing image reconstruction reflectivity light sources;surface reflectance function;instruments;electronic mail;disk type light source;reflectivity;light scattering;photometric stereo method;shape reconstruction hybrid reflectance surface indirect diffuse illumination 3d shape recovery photometric stereo method surface reflectance function disk type light source hemisphere slant tilt angle intensity images algorithm 3d reference table needle map;shape recovery;needle map;surface reconstruction;algorithm;indirect diffuse illumination;hemisphere;reflectivity lighting surface reconstruction light sources geometrical optics solid modeling shape control electronic mail instruments photometry;3d reference table;photometry;image reconstruction;shape reconstruction;reflection model;solid modeling;stereo image processing;hybrid reflectance surface;tilt angle;shape control;lighting;3d shape recovery;slant;intensity images;light sources;geometrical optics	To recover 3-D shapes, a new algorithm which is based on the hybrid reflectance model and the indirect diffuse illumination method is presented. The photometric stereo method (PSM) is modified with the indirect diffuse illumination method (IDIM). We derive the surface reflectance function for a disk-type light source and make the 3-D reference table generated by applying the derived surface reflectance function to the hemisphere. The slant and tilt angle at each point of the intensity images which are the inputs for the algorithm is determined from the 3-D reference table. Finally, the 3-D shape of objects is recovered from the needle map.	diffuse reflection	Tae-eun Kim;Jong-Soo Choi	1995		10.1109/ICIP.1995.537488	iterative reconstruction;geometrical optics;computer vision;surface reconstruction;photometry;lighting;reflectivity;light scattering;solid modeling;diffuse reflectance infrared fourier transform	Vision	59.79250467495997	-53.12865074049535	104552
1c74a18fd442a5c880737b19467537ed9a6d00be	a focus measure for light field rendering	rendu image;focal plane;depth from focus;image processing;restitucion imagen;filters;procesamiento imagen;image multiple;synthetic image light field rendering free viewpoint image synthesization multiviewpoint image scene structure focal plane focus measurement image enhancement;imagen multiple;focal planes;traitement image;multiple image;image enhancement;plan focal;image reconstruction;light field rendering;image rendering;plano focal;focusing rendering computer graphics image generation filters layout cameras psnr shape image resolution pixel;filters image enhancement focal planes image reconstruction;depth map	Light field rendering is a fundamental method for synthesizing free-viewpoint images from a set of multiviewpoint images. In the simplest case, the scene structure is approximated by a simple plane: a focal plane. This approximation leads to focus-like effects on synthetic images where the focused depth is determined by the focal plane. A serious problem is that the range of the focused depth is too small in most practical cases. In this paper, we propose a focus measure that is specialized for synthetic images by light field rendering. When a set of differently-focused images is generated at a given viewpoint, the proposed focus measure enables us to obtain a depth map and an all in-focus image. Our approach has some remarkable differences from other related techniques, such as depth-from-stereo and depth-from-focus methods. Experimental results show that the proposed method effectively enhances PSNR of the final synthetic images.	approximation algorithm;depth map;focal (programming language);light field;peak signal-to-noise ratio;synthetic data;synthetic intelligence	Keita Takahashi;Akira Kubota;Takeshi Naemura	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1421604	iterative reconstruction;computer vision;cardinal point;image processing;computer science;mathematics;depth map;computer graphics (images)	Robotics	59.13468431547251	-59.717401182940215	104710
87a7badc56db02280c2e134c9ea48e79e26c0f5f	interactive multi-label segmentation of rgb-d images	caner hazirbas;members;hazirbas	We propose a novel interactive multi-label RGB-D image segmentation method by extending spatially varying color distributions [14] to additionally utilize depth information in two different ways. On the one hand, we consider the depth image as an additional data channel. On the other hand, we extend the idea of spatially varying color distributions in a plane to volumetrically varying color distributions in 3D. Furthermore, we improve the data fidelity term by locally adapting the influence of nearby scribbles around each pixel. Our approach is implemented for parallel hardware and evaluated on a novel interactive RGB-D image segmentation benchmark with pixel-accurate ground truth. We show that depth information leads to considerably more precise segmentation results. At the same time significantly less user scribbles are required for obtaining the same segmentation accuracy as without using depth clues.	3d computer graphics;benchmark (computing);channel (communications);channel (digital image);disk image;ground truth;image segmentation;information retrieval;multi-label classification;pixel	Julia Diebold;Nikolaus Demmel;Caner Hazirbas;Michael Möller;Daniel Cremers	2015		10.1007/978-3-319-18461-6_24	computer vision;ground truth;pixel;rgb color model;image segmentation;artificial intelligence;computer science;communication channel;segmentation	Vision	57.4395351605819	-60.41056618039137	104778
1a251ee50858fa7576dfe0960948e12bd1e13898	what is a good model for depth from defocus?	point spread functions;blur equalization technique;image restoration approximation theory gaussian processes;relative blur;deconvolution;deconvolution depth from defocus point spread functions relative blur blur equalization technique;deconvolution based approaches depth estimation defocused images blur equalization technique bet blur kernels inverse approach gaussian approximation;kernel apertures cameras lenses shape optical diffraction diffraction;depth from defocus	Different models for estimating depth from defocused images have been proposed over the years. Typically two differently defocused images are used by these models. Many of them work on the principle of transforming one or both of the images so that the transformed images become equivalent. One of the most common models is to estimate the relative blur between a pair of defocused images and compute depth from it. Another model known as the Blur Equalization Technique (BET) works by blurring both images by an appropriate pair of blur kernels. The inverse approach is to deblur both images by an appropriate pair of blur kernels. In this paper we compare the performance of these models to find under what conditions they work best. We show that the common approach of using the Gaussian approximation of the relative blur kernel performs worse than a more general approximation of the relative blur kernel. Furthermore, we show that despite the reduction in signal content in BET, it works well in most circumstances. Finally, the performance of deconvolution based approaches depends on a large part on the shape of the blur kernel and is more appropriate for the coded aperture setup.	approximation;coded aperture;deconvolution;gaussian blur;kernel (operating system)	Fahim Mannan;Michael S. Langer	2016	2016 13th Conference on Computer and Robot Vision (CRV)	10.1109/CRV.2016.61	computer vision;mathematical optimization;computer science;deconvolution;gaussian blur;mathematics	Vision	55.76621912047797	-58.07350088364325	105018
d74b769e9e789f57046072c6934f4a88b2b999a5	image fusion with no gamut problem by improved nonlinear ihs transforms for remote sensing	geophysical image processing;improved nonlinear ihs inihs color space;image fusion;multispectral image;image fusion rgb cube color transformations color space multispectral image panchromatic image remote sensing nonlinear ihs transforms;remote sensing geophysical image processing image colour analysis image fusion;image colour analysis;panchromatic image clipping gamut problem image fusion improved nonlinear ihs inihs color space multispectral image;remote sensing;clipping;panchromatic image;gamut problem;article;image color analysis image fusion transforms color computational modeling surface waves educational institutions	An image fusion method must ideally preserve both the detail of the panchromatic image and the color of the multispectral image. Existing image fusion methods incur the gamut problem of creating new colors which fall out of the RGB cube. These methods solve the problem by color clipping which yields undesirable color distortions and contrast reductions. An improved nonlinear IHS (intensity, hue, saturation; iNIHS) color space and related color transformations are proposed in this paper to solve the gamut problem without appealing to color clipping. The iNIHS space includes two halves, one being constructed from the lower half of the RGB cube by RGB to IHS transformations, and the other from the upper half of the RGB cube by CMY to IHS transformations. While incurring no out-of-gamut colors, desired intensity substitutions and additions in substitutive and additive image fusions, respectively, are all achievable, with the saturation component regulated within the maximum attainable range. Good experimental results show the feasibility of the proposed method.	color space;distortion;image fusion;multispectral image;nonlinear system;utility functions on indivisible goods	Chun-Liang Chien;Wen-Hsiang Tsai	2014	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2013.2243157	color histogram;image texture;multispectral image;false color;rgb color model;computer vision;feature detection;hsl and hsv;color depth;color image;image gradient;binary image;rgb color space;clipping;clipping;color balance;color space;image fusion;remote sensing;computer graphics (images)	Robotics	58.55730723605124	-61.97441880880842	105099
d2d12ce8719792fc1bfa10075ea8af79cb22fff0	intra prediction of depth picture with plane modeling				Dong-seok Lee;Soon-kak Kwon	2018	Symmetry	10.3390/sym10120715		Vision	62.99957281310728	-54.85308494533207	105142
d9a2181bc2f38d7cf0829cbc97b887d62b64c90c	satellite image compression by directional decorrelation of wavelet coefficients	image coding;directional decorrelation;satellites image coding decorrelation wavelet coefficients dictionaries discrete wavelet transforms wavelet transforms principal component analysis polynomials niobium;wavelet transforms geophysical signal processing image coding principal component analysis remote sensing;satellite image compression;satellite applications image coding wavelet transforms decorrelation discrete transforms;satellite applications;wavelet transforms;wavelet transform;discrete transforms;geophysical signal processing;computational complexity;principal component analysis;remote sensing;bandelet transform satellite image compression directional decorrelation wavelet coefficients wavelet transform;satellite image;decorrelation;bandelet transform;wavelet coefficients	This paper presents a satellite image compression scheme based on a post-processing of the wavelet transform of images. The bandelet transform is a directional post-processing of wavelet coefficients. Thanks to a low computational complexity, this transform is a good candidate for future on-board satellite image compression systems. First, we analyze the ability of the bandelets to exploit directional correlations between wavelet coefficients. This study leads to an improved post-processing with a better decorrelation of adjacent wavelet coefficients in the vertical or in the horizontal direction taking into account the wavelet subband orientations. To perform even better decorrelation, bases are also build by principal component analysis (PCA). This results in an improved compression performance without increasing the computational complexity.	bandelet (computer science);coefficient;computational complexity theory;decorrelation;image compression;on-board data handling;principal component analysis;video post-processing;wavelet transform	Xavier Delaunay;Marie Chabert;Vincent Charvillat;Géraldine Morin;Rosario Ruiloba	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4517829	wavelet;computer vision;second-generation wavelet transform;continuous wavelet transform;computer science;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;wavelet transform	Robotics	66.13601234401224	-64.92910832993974	105475
29bc39c5514727c01ffb8a6329e6ac2cd105bfe2	kinect depth map based enhancement for low light surveillance image	video surveillance;image sensors;image enhancement;video surveillance image enhancement image sensors;perceptual quality kinect depth map based enhancement low light surveillance image high noise level low dynamic range visual quality low light image enhancement methods 2d cues depth information depth based image enhancement methods depth based methods normal light image local depth perception kinect depth based enhancement algorithm depth level similarity nonlocal means denoising depth aware contrast stretching;surveillance noise reduction dynamic range image edge detection heuristic algorithms noise image enhancement;enhancement kinect depth image	High noise level from darkness and low dynamic range are two characteristics of low light surveillance image that severely degrade the visual quality. Traditional low light image enhancement methods merely use the 2D cues without the depth information of the scene. Recently, the depth based image enhancement methods are proposed to enhance the depth perception of the image. However, these depth based methods are focus on the normal light image and only enhance the local depth perception. In this paper, based on the characteristics that the depth map captured by Kinect is less affected by low light condition than color image, we propose a Kinect depth based enhancement algorithm to enlarge the dynamic range and meanwhile to enhance the depth perception for the low light surveillance image. In our algorithm, firstly, the depth level similarity is incorporated into the non-local means denoising to remove the noises while better preserve object edges. Then, the depth aware contrast stretching is performed to enlarge the dynamic range and meanwhile to enhance both globe and local depth perception for low light surveillance image. Experimental results on low light surveillance images show that our proposed algorithm achieves better perceptual quality than previous work.	algorithm;color image;depth map;depth perception;dynamic range;high-dynamic-range rendering;image editing;kinect;light pen;noise (electronics);noise reduction;non-local means;normalization (image processing)	Jinhui Hu;Ruimin Hu;Zhongyuan Wang;Yan Gong;Mang Duan	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738225	computer vision;image sensor;multimedia;computer graphics (images)	Vision	58.17812931845122	-60.93656195604624	105535
eaa74807a99f713552018d3123d4eef3eb7e8634	logarithmic profile mapping and retinex edge preserving for restoration of low illumination images	color;image restoration;brightness;image edge detection;image color analysis;optimization;lighting	Contents carried in an image are valuable information sources for many scientific and engineering applications. However, if the image is captured under low illumination conditions a large portion of the image appears dark and this heavily degrades the image quality. In order to solve this problem, a restoration algorithm is developed here that transforms the low input brightness to a higher value using a logarithmic mapping function. The mapping is further refined by a linear weighting with the input to reduce the un-necessary amplification at regions with high brightness. Moreover, fine details in the image are preserved by applying the Retinex principle to extract and then re-insert object edges. Results from experiments using low and normal illumination images have shown satisfactory performances with regard to the improvement in information contents and the mitigation of viewing artifacts.	algorithm;circuit restoration;experiment;image quality;mathematical optimization;performance;pixel;weight function	Ngai Ming Kwok;Haiyan Shi;Gu Fang;Stephen Ching-Feng Lin;Chin Yeow Wong;San Chi Liu;Shilong Liu;Md. Arifur Rahman	2016	2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)	10.1109/CISP-BMEI.2016.7852711	image restoration;computer vision;feature detection;color image;image gradient;image processing;computer science;lighting;mathematics;optics;brightness;computer graphics (images)	Vision	58.307967203962434	-61.83503007674665	105598
6e7d235178dc1ade48644e9ffd307681c8338ea8	on-chip digital noise reduction for integrated cmos cameras	system design;algorithm;data storage;chip;signal to noise ratio;system on a chip;system on chip;image quality;implementation;noise reduction;image sensor;simulation;filtering;cmos image sensor;computer hardware;denoising	We propose an on-line noise reduction system especially designed for noisy CMOS image sensors. Image sequences from CMOS sensors in general are corrupted by two types of noise, temporal noise and fixed pattern noise (FPN). It is shown how the FPN component can be estimated from a sequence. We studied the theoretical performance of two different approaches called direct and indirect FPN estimation. We show that indirect estimation gives superior performance, both theoretically and by simulations. The FPN estimates can be used to improve the image quality by compensating it. We assess the quality of the estimates by the achievable SNR gains. Using those results a dedicated filtering scheme has been designed to accomplish both temporal noise reduction and FPN correction by applying a single noise filter. It allows signal gains of up to 12dB and provides a high visual quality of the results. We further analyzed and optimized the memory size and bandwidth requirements of our scheme and conclude that it is possible to implement it in hardware. The required memory size is 288kByte and the memory access rate is 70MHz. Our algorithm allows the integration of noisy CMOS sensors with digital noise reduction and other circuitry on a system-on-chip solution.© (2003) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	cmos;noise reduction	Markus Rullmann;Jens-Uwe Schluessler;René Schüffny	2003			system on a chip;median filter;image noise;computer vision;telecommunications;computer science;noise measurement;noise reduction;image sensor	EDA	60.57865900345039	-57.55448275806324	105736
264c27b621a455c3c90aa93d75c9a7c06e320cf4	scale multiplication in odd gabor transform domain for edge detection	scale multiplication;exponential distribution;adaptive thresholding;gabor transform;edge detection;adaptive threshold;natural images;optimization problem;gabor filter	In this paper, we propose an adaptive edge detection technique based on scale multiplication in odd Gabor transform domain (ESMG). With adjacent scale multiplication in odd Gabor transform domain, a sharpened edge response output is obtained, which can more effectively resist the inverse influence from noise contamination on the performance of edge detector. Based on odd Gabor filter with single scale, it is shown that Rayleigh distribution can be feasibly adopted to model the real pdf of edge response. Thus the pdf of sharpened edge response output can be approximately modeled by an exponential distribution since there exists strong correlation between two edge response outputs with two adjacent scale factors. In determining the threshold for the sharpened edge response, an adaptive strategy is applied, in which the nonlinear relation of the threshold with the mean and variance of exponential distribution is exploited. Moreover, an optimization problem is finally formulated to find the adaptive adjustment factor. The experimental results on both synthetic and real world natural images show that our scheme is robust and takes on good edge detection performance. 2006 Elsevier Inc. All rights reserved.	edge detection;gabor filter;mathematical optimization;nonlinear system;optimization problem;portable document format;rayleigh–ritz method;synthetic intelligence;time complexity	Zhenfeng Zhu;Hanqing Lu;Yao Zhao	2007	J. Visual Communication and Image Representation	10.1016/j.jvcir.2006.10.001	exponential distribution;gabor transform;optimization problem;computer vision;mathematical optimization;combinatorics;discrete mathematics;edge detection;gabor–wigner transform;computer science;mathematics;thresholding;canny edge detector	Vision	54.368919416649156	-65.16464642034265	105921
03d0d7b9e966d10718d1e85f5ca7be70e48f58ad	research on depth estimation method of light field imaging based on big data in internet of things from camera array		In recent years, optical field imaging technology has received extensive attention in the academic circle for its novel imaging characteristics of shooting first and focusing later, variable depth of field, variable viewpoint, and so on. However, the existing optical field acquisition equipment can only acquire a limited number of discrete angle signals, so image aliasing caused by under sampling of optical field angle signals reduces the quality of optical field images. Based on the camera array system as a platform, this paper studies the optical field imaging and depth estimation method based on the Big Data in Internet of Things obtained from camera array around the angle sampling characteristics of the optical field data set, and has achieved some innovative research results in the following aspects. On the basis of analyzing the characteristics of different depth clues in the optical field data set, a depth estimation method combining parallax method and focusing method is proposed. First, this paper analyzes the disparity clues and focus clues contained in the multi-view data set and the light field refocusing image set of the camera array, respectively, and points out the differences and relationships between the two depth clues extraction methods in the light field sampling frequency domain space, that is, the disparity method focuses on the energy concentration characteristics near the frequency domain spatial angle axis, while the focus method focuses on the high frequency proportion of energy distribution on the angle axis. Then, the weighted linear fusion method based on image gradient is used to fuse the two calculation results, which improves the accuracy and robustness of depth estimation. Finally, the results of depth estimation experiments on different sets of scenes show that compared with the method based on a single depth cue, the method in this paper has higher accuracy in depth calculation in discontinuous areas of scene depth and similar texture areas.	aliasing;apache axis;big data;binocular disparity;experiment;image gradient;imaging technology;internet of things;light field;parallax;sampling (signal processing)	Yue Wu	2018	IEEE Access	10.1109/ACCESS.2018.2870394	frequency domain;depth of field;light field;computer vision;image gradient;imaging technology;sampling (signal processing);aliasing;optical field;computer science;distributed computing;artificial intelligence	Visualization	67.01620830103325	-60.44102308920467	105948
1ef9c25ddd73db3827395d016be8b82629fa0939	a novel adaptive image zooming scheme via weighted least-squares estimation	arbitrary integer and wls aiz scheme;weighted least squares estimation;refinement strategy;adaptive interpolation refinement strategy weighted least squares estimation arbitrary integer an wls aiz scheme;xuexia zhong guorui feng jian wang wenfei wang wen si 加权最小二乘估计 图像缩放 自适应 算法 加权最小二乘法 图像像素 图像插值 人力资源 a novel adaptive image zooming scheme via weighted least squares estimation;adaptive interpolation	A critical issue in image interpolation is preserving edge detail and texture information in images when zooming. In this paper, we propose a novel adaptive image zooming algorithm using weighted least-square estimation that can achieve arbitrary integer-ratio zoom (WLS-AIZ) For a given zooming ratio n, every pixel in a low-resolution (LR) image is associated with an n × n block of high-resolution (HR) pixels in the HR image. In WLS-AIZ, the LR image is interpolated using the bilinear method in advance. Model parameters of every n×n block are worked out through weighted least-square estimation. Subsequently, each pixel in the n × n block is substituted by a combination of its eight neighboring HR pixels using estimated parameters. Finally, a refinement strategy is adopted to obtain the ultimate HR pixel values. The proposed algorithm has significant adaptability to local image structure. Extensive experiments comparing WLS-AIZ with other state of the art image zooming methods demonstrate the superiority of WLS-AIZ. In terms of peak signal to noise ratio (PSNR), structural similarity index (SSIM) and feature similarity index (FSIM), WLS-AIZ produces better results than all other image integer-ratio zoom algorithms.	acdsee;algorithm;bicubic interpolation;bilinear filtering;experiment;high-resolution scheme;image resolution;lr parser;least squares;peak signal-to-noise ratio;pixel;refinement (computing);structural similarity	Xue-xia Zhong;Guorui Feng;Jian Wang;Wenfei Wang;Wen Si	2015	Frontiers of Computer Science	10.1007/s11704-015-4179-x	mathematical optimization;statistics	Vision	58.884584009974745	-65.65886820354235	105964
766c9562de4518108e5c5cdca74808f8f76896a2	adjoint-driven russian roulette and splitting in light transport simulation	light transport;russian roulette;splitting;zero variance schemes;importance sampling	While Russian roulette (RR) and splitting are considered fundamental importance sampling techniques in neutron transport simulations, they have so far received relatively little attention in light transport. In computer graphics, RR and splitting are most often based solely on local reflectance properties. However, this strategy can be far from optimal in common scenes with non-uniform light distribution as it does not accurately predict the actual path contribution. In our approach, like in neutron transport, we estimate the expected contribution of a path as the product of the path weight and a pre-computed estimate of the adjoint transport solution. We use this estimate to generate so-called weight window which keeps the path contribution roughly constant through RR and splitting. As a result, paths in unimportant regions tend to be terminated early while in the more important regions they are spawned by splitting. This results in substantial variance reduction in both path tracing and photon tracing-based simulations. Furthermore, unlike the standard computer graphics RR, our approach does not interfere with importance-driven sampling of scattering directions, which results in superior convergence when such a technique is combined with our approach. We provide a justification of this behavior by relating our approach to the zero-variance random walk theory.	computer graphics;importance sampling;path tracing;precomputation;rapid refresh;round-robin scheduling;sampling (signal processing);simulation;variance reduction;window function	Jirí Vorba;Jaroslav Krivánek	2016	ACM Trans. Graph.	10.1145/2897824.2925912	mathematical optimization;simulation;importance sampling;mathematics;optics;algorithm;splitting;statistics	Graphics	64.33203225740485	-53.50763815669411	105982
15748ae1967bad975a371cca66cc01e5da0fec4d	log acts as a good feature in the task of image quality assessment	databases;distortion;image quality	In the previous work, the LoG (Laplacian of Gaussian) signal that is the earliest stage output of human visual neural system was suggested to be useful in image quality assessment (IQA) model design. This work considered that LoG signal carried crucial structural information of IQA in the position of its zero-crossing and proposed a Non-shift Edge (NSE) based IQA model. In this study, we focus on another aspect of the properties of the LoG signal, i.e., LoG whitens the power spectrum of natural images. Here our interest is that: when exposed to unnatural images, specifically distorted images, how does the HVS whitening this type of signals? In this paper, we first investigate the whitening filter for natural image and distorted image respectively, and then suggest that the LoG is also a whitening filter for distorted images to some extent. Based on this fact, we deploy the LOG signal in the task of IQA model design by applying two very simple distance metrics, i.e., the MSE (mean square error) and the correlation. The proposed models are analyzed according to the evaluation performance on three subjective databases. The experimental results validate the usability of the LoG signal in IQA model design and that the proposed models stay in the state-of-the-art IQA models. © (2014) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.	image quality	Xuanqin Mou;Wufeng Xue;Congmin Chen;Lei Zhang	2014		10.1117/12.2038982	image quality;computer vision;simulation;distortion;telecommunications;data mining;optics	Vision	61.38413525355661	-65.16129065806717	105992
e09d1f096ae402411a8614b7bdba5481929e3e29	local stereo matching under radiometric variations		Stereo matching is an active research area in computer vision for decades. Most of the existing stereo matching algorithms assume that the corresponding pixels have the same intensity or color in both images. But in real world situations, image color values are often affected by various radiometric factors such as exposure and lighting variations. This paper introduces a robust stereo matching algorithm for images captured under varying radiometric conditions. In this paper, histogram equalization and binary singleton expansion are performed as preprocessing step for local stereo matching. For the purpose of eliminating the discrepancy of illumination between reference image and corresponding image in stereo pair, the histogram equalization is first explored to remove the global discrepancy. As the second step, binary singleton expansion is performed to reduce noise and normalize histogram results for window cost computation efficient. Afterwards, local pixel matching on preprocessed stereo images is performed with Sum of Absolute Difference (SAD) on intensity and gradient. Finally, the final disparity map is obtained by left-right consistency checking and filtering with mean shift segments. Experimental results show that the proposed algorithm can reduce illumination differences and improve the matching accuracy of stereo image pairs effectively.	algorithm;binocular disparity;computation;computer stereo vision;computer vision;discrepancy function;gradient;histogram equalization;metric;map;mean shift;pixel;preprocessor;stereoscopy	Tin Tin San;Nu War	2017	2017 18th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)	10.1109/SNPD.2017.8022728	pixel;filter (signal processing);template matching;computer stereo vision;computer science;histogram matching;histogram equalization;computer vision;mean-shift;histogram;artificial intelligence	Vision	57.72491874059001	-61.33382421636886	106103
5d92c9244640da3788c07df2972c732d0d27a31f	statistics of natural fused image distortions		The capability to automatically evaluate the quality of long wave infrared (LWIR) and visible light images has the potential to play an important role in determining and controlling the quality of a resulting fused LWIR-visible image. Extensive work has been conducted on studying the statistics of natural LWIR and visible light images. Nonetheless, there has been little work done on analyzing the statistics of fused images and associated distortions. In this paper, we study the natural scene statistics (NSS) of fused images and how they are affected by several common types of distortions, including blur, white noise, JPEG compression, and non-uniformity (NU). Based on the results of a separate subjective study on the quality of pristine and degraded fused images, we propose an opinion-aware (OA) fused image quality analyzer, whose relative predictions with respect to other state-of-the-art metrics correlate better with human perceptual evaluations.	circuit complexity;distortion;gaussian blur;image quality;jpeg;scene statistics;white noise	David Eduardo Moreno-Villamarin;Hernan D. Benítez-Restrepo;Alan C. Bovik	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952355	spectrum analyzer;artificial intelligence;image quality;transform coding;pattern recognition;statistics;computer science;jpeg;white noise;computer vision;distortion;scene statistics;image fusion	Vision	61.79565795474906	-64.04031417975577	106934
28ddeda42671180790b5519c235aa1bc5f82e870	multi-layer light field display characterisation	optical variables measurement;system performance multilayer light field display characterisation light field 3d displays hologram like image geometrical optical techniques light interference cascaded display layers;image resolution;two dimensional displays;three dimensional displays;image color analysis;image quality;solid modeling;image resolution three dimensional displays image color analysis image quality optical variables measurement two dimensional displays solid modeling;subjective test light field display depth of field usaf resolution chart viewing angle moire fringing;three dimensional displays geometrical optics image resolution	Light field 3D displays, where a hologram-like image is produced by using geometrical optical techniques as opposed to interference of light, require different measurement procedures to conventional stereoscopic displays for the measurement of certain parameters. This paper covers methods that are particularly applicable to multi-layer light field displays using two or more cascaded display layers where the images are produced by computationally-intensive algorithms. As the overall system performance is dependent on capture and computation as well as display hardware, we have developed methods that allow for all the components in the complete chain. In addition to describing these we also cover other selected measurement techniques here.	algorithm;computation;holography;interference (communication);layer (electronics);light field;stereoscopy	Phil Surman;Shizheng Wang;Kien Seng Ong;Xiao Wei Sun;Junsong Yuan;Yuanjin Zheng	2016	2016 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON)	10.1109/3DTV.2016.7548964	computer vision;display resolution;geography;optics;computer graphics (images)	HCI	64.13288439890671	-59.59321233843901	106995
00a1e95be62b3f597d7673c6ea4a7f2c0095d6d5	context-based coherent surface completion	hole filling;surface completion	We introduce an algorithm to synthesize missing geometry for a given triangle mesh that has “holes.” Similarly to previous work, the algorithm is context based in that it fills the hole by synthesizing geometry that is similar to the remainder of the input mesh. Our algorithm goes further to impose a coherence objective. A synthesis is coherent if every local neighborhood of the filled hole is similar to some local neighborhood of the input mesh. This requirement avoids undesired features such as can occur in context-based completion. We demonstrate the algorithm's ability to fill holes that were difficult or impossible to fill in a compelling manner by earlier approaches.	algorithm;coherence (physics);triangle mesh	Gur Harary;Ayellet Tal;Eitan Grinspun	2014	ACM Trans. Graph.	10.1145/2532548	mathematical optimization;computer science;mathematics;geometry	Graphics	58.63287056973817	-55.450469752458694	107166
312949faa1d31980e2085674458695597e7eeb78	a new haze image database with detailed air quality information and a novel no-reference image quality assessment method for haze images	weather and air quality information;decision support systems indexes;haze image database;visual databases air quality;mean opinion score;image quality assessment;image quality assessment haze image database weather and air quality information mean opinion score;haze database haze image database detailed air quality information no reference image quality assessment method haze situations weather information mean opinion score mos haze severity subjective evaluation objective descriptions subjective descriptions image quality degradation iqa method	In this paper, we propose a new standard haze image database with nearly all kinds of haze situations. Our database includes haze-free images as well as different levels and situations of haze images, such as snowy and extremely serious haze images. Our database also records the related weather and air quality information. Moreover, the database offers a mean opinion score (MOS) for each image as the subjective evaluation of the haze severity. Our database provides ample haze images with various objective and subjective descriptions, and has significantly potential scientific value. Since haze is a reason leading to the degradation of the image quality, obtaining the quality of haze images is also necessary and meaningful. In this paper, a novel no-reference image quality assessment (IQA) method for haze images is also proposed. Our method analyzes the model of haze images to evaluate the quality. The experimental results on the haze database show that our method is consistent with the subjective evaluation.	elegant degradation;image quality	Yibing Zhan;Rong Zhang;Qian Wu;You Wu	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7471845	mean opinion score;computer vision;computer science	Vision	62.334008091167185	-63.74831522588582	107223
49ade5e0426f1efefe6a071333c8a5788c366b30	a novel iris image quality evaluation based on coarse-to-fine method	biometrics;coarse-to-fine method;iris image quality evaluation	An effective image evaluation algorithm is vital for iris recognition system. This paper proposes a coarse-to-fine quality evaluation method to iris image. Five criterions are adopted in this paper, which are variance, gradient, edge strength, fuzzy entropy, information entropy, for iris image coarse evaluation. In fine quality evaluation phase, which mainly focus on evaluation of spatial location and effective regional. The experimental results show that our proposed method are able to meet the need of practical iris recognition system. © Springer International Publishing 2013.	image quality	Ying Chen;Yuanning Liu;Xiaodong Zhu;Xiaoxu Zhang;Haiying Xu;Guang Huo;Ning Deng	2013		10.1007/978-3-319-02961-0_44	fuzzy logic;computer vision;entropy (information theory);iris recognition;image quality;biometrics;artificial intelligence;computer science	Vision	60.34207194273041	-65.70610400795827	107369
1b9926d85cbcc7ef202e08275b5f134bc269cd0b	seeing beyond lambert's law	diffuse reflectance	Lambert's model for diiuse reeection is extensively used in computational vision. For several real-world objects, the Lambertian model can prove to be a very inaccurate approximation to the diiuse component. While the brightness of a Lambertian surface is independent of viewing direction, the brightness of a rough diiuse surface increases as the viewer approaches the source direction. A comprehensive model is developed that predicts reeectance from rough diiuse surfaces. Experiments have been conducted on real samples, such as, plaster, clay, and sand. The reeectance measurements obtained are in strong agreement with the reeectance predicted by the proposed model.	approximation;computer vision;experiment;lambertian reflectance;viewing cone	Michael Oren;Shree K. Nayar	1994		10.1007/BFb0028360	computer science;diffuse reflection	Vision	59.95003698791427	-52.1911007463826	107428
9ef40e79429ae3674250967509d3b37dd9062fff	evaluation of image fusion performance with visible differences	performance measure;predictive modelling;image fusion;evaluation method;performance metric;human visual system;process evaluation	Multisensor signal-level image fusion has attracted considerable research attention recently. Whereas it is relatively straightforward to obtain a fused image, e.g. a simple but crude method is to average the input signals, assessing the performance of fusion algorithms is much harder in practice. This is particularly true in widespread “fusion for display” applications where multisensor images are fused and the resulting image is presented to a human operator. As recent studies have shown, the most direct and reliable image fusion evaluation method, subjective tests with a representative sample of potential users are expensive in terms of both time/effort and equipment required. This paper presents an investigation into the application of the Visible signal Differences Prediction modelling, to the objective evaluation of the performance of fusion algorithms. Thus given a pair of input images and a resulting fused image, the Visual Difference Prediction process evaluates the probability that a signal difference between each of the inputs and the fused image can be detected by the human visual system. The resulting probability maps are used to form objective fusion performance metrics and are also integrated with more complex fusion performance measures. Experimental results indicate that the inclusion of visible differences information in fusion assessment yields metrics whose accuracy, with reference to subjective results, is superior to that obtained from the state of the art objective fusion performance measures.	algorithm;image fusion;map	Vladimir S. Petrovic;Costas S. Xydeas	2004		10.1007/978-3-540-24672-5_30	computer vision;simulation;program evaluation;computer science;machine learning;pattern recognition;predictive modelling;image fusion;human visual system model	Vision	62.04126891005779	-63.906380119273294	107778
484b398cc8e4c197a76bb9e82c01a2c7cfa2d177	comparison between popular image fusion approaches with the consideration of their influence on later segmentation outcomes	wavelet transform image fusion multi resolution segmentation pca transform quickbird image;panchromatic bands;image analysis processes;image segmentation;multi resolution segmentation;remote sensing techniques;image fusion;ecognition image fusion panchromatic bands spatial resolution multispectral bands remote sensing techniques image analysis processes quickbird image multiresolution segmentation;remote sensing image fusion image segmentation;wavelet transform;quickbird image;principal component analysis;remote sensing;transforms;image segmentation image fusion principal component analysis transforms spatial resolution remote sensing;ecognition;multispectral bands;image analysis;multi spectral;multiresolution segmentation;pca transform;multi resolution;spectral resolution;spatial resolution	Image Fusion is the basic way to complement the panchromatic bands which occupies precise spatial resolution with the multi-spectral bands which contains abundant spectral details. By the courtesy of rapid development of remote sensing techniques, to the same area, there are lots of images with different spatial and spectral resolutions, which boost the necessity of image fusion. There have been various approaches for image fusion available now, most of which emphasize only on the visual effect while ignoring its fusion consequence to the later image analysis processes (such as segmentation, classification, etc). The paper intends to propose some suggestions on how to choose the fusion procedures, based on the comparison of popular image fusion methodologies applied to Quickbird image with 0.6m spatial resolution. The multi-resolution segmentation approach provided by Definiens' eCognition has been used in the final segmentation stage to ensure the optimal results).	definition;image analysis;image fusion;statistical classification;visual effects	Ding Chen;Xiaoping Lin	2010	2010 18th International Conference on Geoinformatics	10.1109/GEOINFORMATICS.2010.5567506	computer vision;geography;segmentation-based object categorization;pattern recognition;scale-space segmentation;remote sensing	Robotics	68.03461193736723	-63.536561185533756	107842
a2844fe1d19fc9b68b058905c6a84f00f8105d2e	chroma replacing and adaptive chroma blending for subpixel-based downsampling	image color analysis rendering computer graphics image edge detection distortion measurement image resolution psnr gold;image resolution;chroma replacing downsampled image subpixel rendering effect removal color fringing artifact removal apparent resolution subpixel based downsampling adaptive chroma blending;image colour analysis;rendering computer graphics image colour analysis image resolution;rendering computer graphics	Subpixel-based downsampling generates images with higher apparent resolution with the expense of annoying color-fringing artifacts near strong edges. In this paper we propose two methods that find a balance in the tradeoff of apparent resolution and color-fringing artifacts. The first method is called Chroma Replacing in which the color-fringing artifacts are completely removed but the subpixel rendering effect is also removed. The second one is called Chroma Blending in which only the color-fringing artifacts that are strong enough to be noticed are removed, and also the subpixel rendering effect is retained. We also propose two objective measures for measuring the similarity of downsampled image to the original image. Experiment results show that the proposed methods are effective in removing color-fringing artifacts, without harming the high apparent resolution.	aliasing;alpha compositing;artifact (software development);decimation (signal processing);experiment;peak signal-to-noise ratio;pixel;probabilistic data association filter;purple fringing;structural similarity;subpixel rendering;video post-processing;zoomed video port	Ketan Tang;Oscar C. Au;Lu Fang;Yuanfang Guo;Jiahao Pang	2013	2013 IEEE 15th International Workshop on Multimedia Signal Processing (MMSP)	10.1109/MMSP.2013.6659290	computer vision;image resolution;computer science;subpixel rendering;multimedia;computer graphics (images)	Visualization	58.66530883044829	-61.03791251485371	107895
10d1749a33ad7c8830f4a00746e8df87c5d9cc60	fixed-pattern noise modeling and removal in time-of-flight sensing	noise contamination removal;gaussian noise;photonics;fixed pattern noise fpn;time of flight tof;tof sensing;photonic mixer device pmd;time measurement;tof measurement;sensors;gaussian processes;time of flight tof denoising fixed pattern noise fpn low sensing ls environment photonic mixer device pmd;spatial variables measurement;pollution measurement;measurement uncertainty;ls condition;low sensing condition;subsequent gaussian denoising fixed pattern noise modeling time of flight sensing fpn photonic mixture device tof sensing depth estimation low sensing condition ls condition noise contamination removal tof measurement;time measurement gaussian processes sensors spatial variables measurement;noise reduction;noise reduction sensors photonics gaussian noise pollution measurement measurement uncertainty delays;fpn;denoising;time of flight sensing;depth estimation;fixed pattern noise modeling;photonic mixture device;delays;low sensing ls environment;subsequent gaussian denoising	In this paper, we discuss the modeling and removal of fixed-pattern noise (FPN) in photonic mixture devices employing the time-of-flight (ToF) principle for range measurements and scene depth estimation. We present a case that arises from low-sensing (LS) conditions caused by either external factors related to scene reflectivity or internal factors related to the power and operation mode of the sensor or both. In such a case, the FPN becomes especially dominating and invalidates previously adopted noise models, which have been used for removal of other noise contaminations in ToF measurements. To tackle LS cases, we propose a noise model specifically addressing the presence of FPN and develop a relevant FPN removal procedure. We demonstrate, by experiments with synthetic and real-world data, that the proper modeling and removing of FPN is substantial for the subsequent Gaussian denoising and yields accurate depth maps comparable to the ones obtainable in normal operating mode.	experiment;fixed-pattern noise;gaussian blur;least squares;map;noise reduction;synthetic data	Mihail Georgiev;Robert Bregovic;Atanas P. Gotchev	2016	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2015.2494622	computer vision;electronic engineering;telecommunications;computer science;engineering;noise reduction;physics;statistics	Vision	67.95547539299054	-60.912781027453335	108011
e1949ee1ace5b71c1ac03404e1220d666fd00879	subpixel phase-based image registration using savitzky-golay differentiators in gradient-correlation			gradient;image registration;pixel	Amir HajiRassouliha;Andrew J. Taberner;Martyn P. Nash;Poul M. F. Nielsen	2018	Computer Vision and Image Understanding	10.1016/j.cviu.2017.11.003		Vision	58.862769028071455	-64.51650134549004	108200
fb4c64049c900079a02a06e0e680582705461482	noise and visual perception [instrumentation notes]	contrast enhanced;band pass filters;video signal processing;spatial filters;noise measurement;band pass;sensitivity;visualization;image enhancement;visual perception band pass filters image denoising image enhancement medical image processing spatial filters video signal processing vision defects;vision defects;noise measurement sensitivity visualization visual perception text recognition;medical image processing;spatial filtering;visual impairment;visual perception;image denoising;image denoising visual perception noise text reading visual impairments image enhancement videos spatial filtering spatial frequencies depressed preceptor naturally occurring low contrast enhancement wide band enhancement band pass enhancement mpeg compression domain;text recognition;spatial frequency	The inability to read text and to distinguish images is a primary complaint of people with visual impairments. Several attempts have been made to develop methods (simple and complex) of enhancing images and videos for the visually impaired. The first approaches used dedicated spatial filtering to enhance those spatial frequencies that are not detectable by a depressed preceptor at their naturally occurring low contrast [1] - [5]. A few modifications of this contrast enhancement approach have been investigated by applying wide-band enhancement by superposition of high contrast outlines over the images as well as band pass enhancement implemented in the MPEG compression domain [6], [7].	color vision;moving picture experts group	Bruno Ando	2012	IEEE Instrumentation & Measurement Magazine	10.1109/MIM.2012.6174580	computer vision;speech recognition;computer science;band-pass filter;multimedia;physics	Vision	64.26442761362195	-60.65922577257789	108737
5f26756442d76fb887e6d05a6601b65064c92df3	a flexible fpga implementation for illuminance–reflectance video enhancement	video and image enhancement;real time;illuminance reflectance;fpga;recursive median filter;retinex	Digital cameras, new generation phones, commercial TV sets and, in general, all modern devices for image acquisition and visualization can benefit from algorithms for image enhancement suitable to work in real time and preferably with limited power consumption. Among the various methods described in the scientific literature, Retinex-based approaches are able to provide very good performances, but unfortunately they typically require a high computational effort. In this article, we propose a flexible and effective architecture for the real-time enhancement of video frames, suitable to be implemented in a single FPGA device. The video enhancement algorithm is based on a modified version of the Retinex approach. This method, developed to control the dynamic range of poorly illuminated images while preserving the visual details, has been improved by the adoption of a new model to perform illuminance estimation. The video enhancement parameters are controlled in real time through an embedded microprocessor which makes the system able to modify its behavior according to the characteristics of the input images, and using information about the surrounding light conditions.	algorithm;ambient occlusion;closed-circuit television;digital camera;dynamic range;embedded system;field-programmable gate array;image editing;microprocessor;performance;real-time clock;scientific literature;simulation;videotelephony;web page;world wide web	Stefano Marsi;Giovanni Ramponi	2011	Journal of Real-Time Image Processing	10.1007/s11554-011-0203-z	embedded system;computer vision;real-time computing;simulation;computer science;color constancy;field-programmable gate array;computer graphics (images)	Graphics	63.35202538128624	-59.36495137986379	108837
454d46692cec32ddc28aae18bf58b0ed7976f7ae	a method for correcting the leaning of ar two-dimensional codes based on lsm		In this paper, it is inevitable that the AR two-dimensional codes obtained by the mobile device will have a tendency towards the mobile augmented reality application. A two-dimensional code tilts correction method based on least square method is proposed. After binarization preprocessing, the target two-dimensional code inclination angle is obtained by least-squares method. After further correction by bilinear interpolation, satisfactory results can be achieved. Restore the image. The experimental results show that the method can be quickly and effectively identify the target two-dimensional code, and then the corresponding three-dimensional model to enhance the reality display.		Chen Miao;Shufen Liu;Zhilin Yao	2017		10.1007/978-3-319-74521-3_2	bilinear interpolation;augmented reality;mobile device;computer vision;least squares;computer science;artificial intelligence	Theory	55.23349032419981	-60.57635693450531	108843
33b915476f798ca18ae80183bf40aea4aaf57d1e	face illumination manipulation using a single reference image by adaptive layer decomposition	least squares approximations;edge preserving filter face illumination manipulation face illumination transfer face illumination normalization adaptive layer decomposition;smoothing methods;face recognition;image colour analysis;algorithms computer graphics face humans image enhancement image interpretation computer assisted imaging three dimensional lighting numerical analysis computer assisted reference values reproducibility of results sensitivity and specificity signal processing computer assisted subtraction technique;smoothing methods face recognition image colour analysis least squares approximations;face illumination manipulation skin color identification structure shadow effects contrast light face delighting nonnormal lighting face large scale layer gradient values adaptive smoothing parameters wls filter weighted least squares filter lightness layers normal lighting face illumination effects human face adaptive layer decomposition single reference image	This paper proposes a novel image-based framework to manipulate the illumination of human face through adaptive layer decomposition. According to our framework, only a single reference image, without any knowledge of the 3D geometry or material information of the input face, is needed. To transfer the illumination effects of a reference face image to a normal lighting face, we first decompose the lightness layers of the reference and the input images into large-scale and detail layers through weighted least squares (WLS) filter with adaptive smoothing parameters according to the gradient values of the face images. The large-scale layer of the reference image is filtered with the guidance of the input image by guided filter with adaptive smoothing parameters according to the face structures. The relit result is obtained by replacing the largescale layer of the input image with that of the reference image. To normalize the illumination effects of a non-normal lighting face (i.e., face delighting), we introduce similar reflectance prior to the layer decomposition stage by WLS filter, which make the normalized result less affected by the high contrast light and shadow effects of the input face. Through these two procedures, we can change the illumination effects of a non-normal lighting face by first normalizing the illumination and then transferring the illumination of another reference face to it. We acquire convincing relit results of both face relighting and delighting on numerous input and reference face images with various illumination effects and genders. Comparisons with previous papers show that our framework is less affected by geometry differences and can preserve better the identification structure and skin color of the input face.	acclimatization;color space;face;gradient;illumination (image);lambertian reflectance;least squares;normalize;paper;population parameter;shading;skin pigmentation;smoothing (statistical technique);anatomical layer	Xiaowu Chen;Hongyu Wu;Xin Jin;Qinping Zhao	2013	IEEE Transactions on Image Processing	10.1109/TIP.2013.2271548	facial recognition system;computer vision;computer science;computer graphics (images)	Vision	58.474306925586376	-62.34690958234588	108882
04589315fcfdeafc9fe03ee39bb40c9a91087925	eigenwavelet: hyperspectral image compression algorithm	eigenvalues and eigenfunctions;hyperspectral imagery;defense;compression algorithm;texture;long range dependence;optimisation;image coding;spectral redundancy extraction;data compression;spectral response;hyperspectral image compression;aviris data eigen wavelet hyperspectral image compression defense remote sensing non isotropy non stationarity short range dependence texture long range dependence spectral response integerizable eigendecomposition optimization spectral redundancy extraction wavelet based encoding contextual arithmetic encoding;eigen wavelet;information content;scaling up;image texture;non isotropy;wavelet transforms;short range dependent;arithmetic codes;redundancy;wavelet based encoding;feature extraction;remote sensing;aviris data;long range dependent;integerizable eigendecomposition;short range dependence;arithmetic codes data compression image coding remote sensing military computing eigenvalues and eigenfunctions wavelet transforms image texture spectral analysis feature extraction redundancy optimisation;compression ratio;optimization;spectral analysis;hyperspectral imaging;contextual arithmetic encoding;hyperspectral image;hyperspectral imaging image coding;non stationarity;military computing	Summary form only given. The increased information content of hyperspectral imagery over multispectral data has attracted significant interest from the defense and remote sensing communities. We develop a mechanism for compressing hyperspectral imagery with no loss of information. The challenge of hyperspectral image compression lies in the non-isotropy and non-stationarity that is displayed across the spectral channels. Short-range dependence is exhibited over the spatial axes due to the finite extent of objects/texture on the imaged area, while long-range dependence is shown by the spectral axis due to the spectral response of the imaged pixel and transmission medium. A secondary, though critical, challenge is one of speed. In order to be of practical interest, a good solution must be able to scale up to speeds of the order of 20 MByte/s. We use an integerizable eigendecomposition along the spectral channel to optimally extract spectral redundancies. Subsequently, we apply wavelet-based encoding to transmit the residuals of eigendecomposition. We use contextual arithmetic encoding implemented with several innovations that guarantee speed and performance. Our implementation attains operating speeds of 550 kBytes of raw imagery per second, and achieves a compression ratio of around 2.7:1 on typical AVIRIS data. This demonstrates the utility and applicability of our algorithm towards realizing a deployable hyperspectral image compression system.	algorithm;data compression;image compression	Sridhar Srinivasan;Laveen N. Kanal	1999		10.1109/DCC.1999.785707	data compression;full spectral imaging;computer vision;computer science;hyperspectral imaging;pattern recognition;mathematics;statistics	Vision	67.91979363041733	-62.581640126496424	108963
e93acfb3f6656bbdcb00cf123eb2875f41311aa0	3d hierarchical optimization for multi-view depth map coding		Depth data has a widespread use since the popularity of high resolution 3D sensors. In multi-view sequences, depth information is used to supplement the color data of each view. This article proposes a joint encoding of multiple depth maps with a unique representation. Color and depth images of each view are segmented independently and combined in an optimal Rate-Distortion fashion. The resulting partitions are projected to a reference view where a coherent hierarchy for the multiple views is built. A Rate-Distortion optimization is applied to obtain the final segmentation choosing nodes of the hierarchy. The consistent segmentation is used to robustly encode depth maps of multiple views obtaining competitive results with HEVC coding standards.	coherence (physics);depth map;distortion;encode;high efficiency video coding;image resolution;mathematical optimization;multiview video coding;rate–distortion optimization;rate–distortion theory;scene graph;sensor	Marc Maceira;David Varas;Josep Ramon Morros;Javier Ruiz Hidalgo;Ferran Marqués	2017	Multimedia Tools and Applications	10.1007/s11042-017-5409-z	computer science;computer vision;coding (social sciences);pattern recognition;artificial intelligence;encoding (memory);hierarchy;rate–distortion optimization;depth map;segmentation	Vision	56.041195610183735	-55.477578989159895	109131
19654ee502cc62b4968fb9c071aea495a91ffbcd	on the performance of disparity-based weighting technique applied to 3d image quality assessment	irccyn ivc 3d images quality database disparity based weighting technique 3d image quality assessment stereosocopic image quality assessment;3d and multi view image performance evaluation objective evaluation techniques;visual databases stereo image processing;image quality stereo image processing psnr three dimensional displays correlation decision support systems prediction algorithms	This papers describes the performance of objective algorithms when they are applied for stereosocopic image quality assessment. Specifically, the authors ascertained the performance of the objective algorithms after the combination with an weighting by means of the disparity. The IRCCyN-IVC 3D Images Quality database was used to verify the performance of the objective algorithms. Indeed, the obtained results suggest a significative enhance of the prediction capacity of the objective algorithms when they were combined with the disparity information contained in the original image signal.	algorithm;binocular disparity;chip carrier;coefficient;context tree weighting;distortion;image quality;jpeg 2000;stereoscopy	José Vinícius de Miranda Cardoso;Carlos Danilo Miranda Regis;Marcelo Sampaio de Alencar	2014	2014 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting	10.1109/BMSB.2014.6873462	image quality;image texture;computer vision;digital image processing;multimedia;information retrieval;standard test image	Arch	62.549493668950454	-64.12742965420685	109161
4edb24b409c4f2224a3cfff006f1fc3d4e16311a	quantization error reduction in depth maps	optimisation;image resolution;optimization depth maps 3d processing quantization error;visual perception image enhancement image registration image resolution mean square error methods optimisation;word length 8 bit image enhancement view synthesis shape registration mssim mean structural similarity index mse evaluation mean square error evaluation depth quantization error reduction optimization approach image resolution human perception cardboard effect 3d video processing system depth map;quantization signal three dimensional displays optimization noise sensors spatial resolution solid modeling;image enhancement;image registration;mean square error methods;visual perception	Since most depth maps are quantized to 8-bit numbers in current 3D video systems, the induced cardboard effects can disturb human perception. Moreover, depth maps with larger resolution suffer more from the quantization error. Therefore, this paper proposes an optimization approach to reduce the depth quantization error with well-preserved structure of the depth maps. The experimental results demonstrate that the proposed approach can successfully recover the structure characteristics from the quantized depth maps. Evaluation in mean square error (MSE) and mean structural similarity index (MSSIM) also strongly support our theory and algorithm. Through enhancing the quality of the depth maps from the very beginning, this work can benefit most 3D processing applications, such as 3D modeling, shape registration, and view synthesis.	3d modeling;8-bit;algorithm;depth map;mathematical optimization;mean squared error;quantization (signal processing);structural similarity;view synthesis	Ku-Chu Wei;Yung-Lin Huang;Shao-Yi Chien	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6637910	computer vision;image resolution;visual perception;computer science;image registration;theoretical computer science;mathematics;computer graphics (images)	Robotics	60.38060226385644	-62.04501007721701	109242
29b3c490c0c3d6978bbe7b8bb3b1dbf41bbdbc81	exemplar-based image inpainting base on structure construction	index terms—image inpainting;bézier curves;color segmentation;mean shift segmentation;bezier curves;indexing terms;mean shift;curve fitting	Image inpainting technique is that repairs damaged area or remove areas in an image. In order to deal with this kind of problems, not only a robust image inpainting algorithm should be used, technique of structure generation is also needs to adopt in inpainting procedure. In this paper, we extend an exemplar-based image inpainting method by incorporating Bézier curves to construct missing edge information. Before we apply our inpainting procedure to a damaged image, mean shift segmentation is used to extract contours of each region. After the structure information of an image is determined, destroyed contours will be connected in curve fitting process and damaged regions will be inpainted by using exemplar-based inpainting method. The experimental results show that our proposed method could deal with several kinds of pictures well.	algorithm;bézier curve;curve fitting;inpainting;mean shift	Jason C. Hung;Chun-Hong Huang;Yi-Chun Liao;Nick C. Tang;Ta-Jen Chen	2008	JSW		computer vision;index term;mean-shift;computer science;bézier curve;pattern recognition;statistics;curve fitting;computer graphics (images)	Vision	54.422805389731124	-60.60366008874117	109245
cf63d3e9cd61142995ece025f447777328de11d4	adaptive wavelet rendering	sphere packing;high dimensionality;monte carlo sampling;depth of field;computational conformal geometry;circle packing;monte carlo integration;global illumination;architectural geometry;computational differential geometry;ray tracing;supporting structures;monte carlo;freeform surface	Effects such as depth of field, area lighting, antialiasing and global illumination require evaluating a complex high-dimensional integral at each pixel of an image. We develop a new adaptive rendering algorithm that greatly reduces the number of samples needed for Monte Carlo integration. Our method renders directly into an image-space wavelet basis. First, we adaptively distribute Monte Carlo samples to reduce the variance of the wavelet basis’ scale coefficients, while using the wavelet coefficients to find edges. Working in wavelets, rather than pixels, allows us to sample not only image-space edges but also other features that are smooth in the image plane but have high variance in other integral dimensions. In the second stage, we reconstruct the image from these samples by using a suitable wavelet approximation. We achieve this by subtracting an estimate of the error in each wavelet coefficient from its magnitude, effectively producing the smoothest image consistent with the rendering samples. Our algorithm renders scenes with significantly fewer samples than basic Monte Carlo or adaptive techniques. Moreover, the method introduces minimal overhead, and can be efficiently included in an optimized ray-tracing system.	adaptive sampling;algorithm;approximation;coefficient;compression artifact;curse of dimensionality;display resolution;distributed ray tracing;edge detection;frequency analysis;gaussian blur;global illumination;graphics interface;ibm notes;image plane;importance sampling;light transport theory;mint;monte carlo integration;monte carlo method;overhead (computing);pixel;ray tracing (graphics);rendering (computer graphics);siggraph;sampling (signal processing);smoothing;spatial anti-aliasing;stationary process;wavelet transform	Ryan S. Overbeck;Craig Donner;Ravi Ramamoorthi	2009	ACM Trans. Graph.	10.1145/1618452.1618486	quasi-monte carlo method;mathematical optimization;mathematics;geometry;monte carlo integration;statistics;monte carlo method;computer graphics (images)	Graphics	64.65517575138091	-53.4332866383707	109257
e566277088fa2bd66467ee0ba478cd5cb2302e9e	pan-sharpening based on improvement of panchromatic image to minimize spectral distortion	pan sharpening;spectral distortion;multispectral;fusion panchromatic	In this paper, we propose a novel method to enhance the pan-sharpening result of low-resolution multispectral images (MS) and high-resolution panchromatic images (Pan) by minimizing the spectral distortion engendered by the fusion process. In fact, spectral distortion is the most significant problem in many pansharpening techniques, due to the non linearity between Pan and MS images. In this method, an improvement of the Pan image is performed in order to enhance the correlation between Pan and MS images before pan-sharpening process. The proposed method is applied as a preprocessing by fusing the intensity image derived from MS image with the original Pan to get an improved Pan image which could be more correlated with MS image. And later, the pan-sharpening is applied on both MS and the improved Pan using any pan-sharpening technique. Simulation results of proposed method are compared in four different techniques, such as: Generalized IHS, DWT, Brovey and HPF. It has been observed that simulation results of this method preserves more spectral information and gets better visual quality compared with earlier reported techniques using original Pan.	discrete wavelet transform;distortion;high performance fortran;image resolution;multispectral image;preprocessor;simulation	Arwa Abdelkrim;Zhaoxiang Zhang;Qingjie Liu	2014		10.1007/978-3-662-45643-9_13	computer vision;geography;cartography;remote sensing	Robotics	66.55327897029326	-65.5906919786962	109454
93f8bb8eb3e563110883a2d9a006ed48f3b27ff7	implementing an anisotropic texture filter	image filtering;computer graphics;computer graphics accelerators;texture mapping;anisotropic texture filters;computer graphic;performance improvement;rendering system;high end computing;software implementation	Texture mapping is an important operation in high-quality computer graphics applications. The principles of image filtering are well established and understood and several high-quality texture filtering algorithms have been developed. However, these have tended to be either off-line software implementations or based on high-end computer graphics rendering systems. Current generation PC-based graphics acceleration utilises texture pre-filtering techniques based upon an isotropic filter kernel. The most common implementations are those based on MIP-map texture storage and bilinear or trilinear interpolation. Such filters give a reduction in image aliasing at the expense of introducing blurring. As acceleration hardware performance improves, tolerance of such compromises is falling leading to the adoption of “anisotropic” filtering solutions. In this paper we present a detailed description of a low-cost implementation of an anisotropic filtering unit for use within a contemporary graphics pipeline. The filter is shown to produce improved visible results for only a modest increase in hardware cost.		Jon P. Ewins;Marcus D. Waller;Martin White;Paul F. Lister	2000	Computers & Graphics	10.1016/S0097-8493(99)00159-4	s3 texture compression;texture mapping;computer vision;graphics pipeline;2d computer graphics;simulation;rendering;computer science;texture mapping unit;real-time computer graphics;texture atlas;texture memory;computer graphics;texture compression;texture filtering;software rendering;anisotropic filtering;computer graphics (images)	Vision	66.51023306238727	-53.035993686894884	109484
543c715112b02ebcb9239e437f4f53326246c163	smart compositing: a real-time content-adaptive blending method for remote visual collaboration	groupware;video signal processing;overlay frame smart compositing real time content adaptive blending method remote visual collaboration overlapped video frames pixel wise adaptive blending factor map saturation information computational complexity overlay content saturated color;collaboration;image edge detection image color analysis real time systems streaming media visualization collaboration computational complexity;visualization;visual collaboration video compositing content adaptive;image edge detection;streaming media;computational complexity;image color analysis;image colour analysis;visual collaboration;content adaptive;video signal processing computational complexity groupware image colour analysis real time systems;video compositing;real time systems	This paper proposes a content-adaptive blending method, smart compositing, for displaying two overlapped video frames on the same screen while preserving the readability of both. A pixel-wise adaptive blending factor map is generated according to the edge and saturation information of the content of only the overlay frame. Using this blending factor map, regions of the overlay frame with edges or saturated color are assigned to be more opaque and the remaining regions are assigned to be more transparent. A halo is also created around the edges of the overlay content which enhances the edges and disambiguates them from the underlying frame. The proposed method is suitable for overlaying many different types of content (e.g. drawings, slides, texts, and pictures) and does not require any information (e.g., an opacity mask) from the application which generates the content. This method has low computational complexity and has been implemented in real-time.	alpha compositing;computational complexity theory;image;pixel;real-time clock;real-time computing;real-time locating system;visual collaboration	Wei Hong;April Slayden Mitchell;Mitchell D. Trott	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288378	computer vision;visualization;computer science;multimedia;computational complexity theory;compositing;computer graphics (images);collaboration	Robotics	59.01108456661359	-54.6099881224191	109635
0219da93179ce9f1c263965c6fc2b80562260e42	toward robust estimation of specular flow.	robust estimator	Specular flow is an important class of optical flow whose utility in visual tasks has gained much interest in contemporary vision research. Unfortunately, however, reliably estimation of specular flow from image sequences is an open question that was never addressed formally before. In this paper we first argue that existing optical flow algorithms are incapable of reliable specular flow estimation due to their typical regularization criteria that conflict the unique and singular structure of specular flows. We show these discrepancies both qualitatively and using quantitative evaluation based on a firstof-its-kind benchmark dataset with ground truth specular flow data. We then suggest to generalize the popular optical flow variational framework using spatial weighting of different regularizers, and we propose new regularization terms that correspond better to the expected singularities of specular flows. Finally, we show how these contributions significantly improves specular flow estimation.	algorithm;benchmark (computing);ground truth;optical flow;variational principle	Yair Adato;Todd E. Zickler;Ohad Ben-Shahar	2010		10.5244/C.24.22	robust statistics;computer science	Vision	55.50788968329927	-53.53350714388294	109776
2dd7816e07fef599ad96b25a7e0bb2a54fc27853	use of colored reflectors for negation or highlighting of scanned color information on film-based cielab-coded optical logic gate models	boolean operation;color computing;color code		logic gate	Kiyoshi Moritaka;Tomonori Kawano	2013	JACIII	10.20965/jaciii.2013.p0799	computer vision;theoretical computer science;high color;algorithm	EDA	63.225635228464135	-55.38089596190455	109978
40057ef6d3cc3a532789a02e7822928be4a7b642	graphics for serious games: infinitex: an interactive editing system for the production of large texture data sets	compositing;visual quality;production process;technology and engineering;graphics hardware;texture streaming;version management;itec;just in time;content creation;real time application	Recent advancements in graphics hardware have made the use of texture streaming methods feasible for realtime applications. Using these methods, not only texture resolution and detail can be increased up to gigapixel resolution, but when used together with well authored textures these techniques can offer dramatically improved visual quality. However, systems aiding in texture data production itself have received a lot less attention than the streaming and rendering problem. When using current production methods in a texture streaming environment, these methods tend to break down and reduce artist efficiency to the point where the technology is no longer used to its full potential. In this paper we describe the details behind our Infinitex system. Infinitex is a texture creation and editing system that allows the users, i.e. artists, to produce large texture data in an intuitive and interactive way. Our system goes beyond a simple editor, as it incorporates the whole production process from the initial empty environment until the final finished product and addresses all the challenges that arise along the way when producing gigabytes of texture data. In particular, we will focus on versioning, management, continuity, and security. We show how our system, through the use of just-in-time tile generation, offers interactive editing and management operations while meeting all the other constraints imposed on the system. & 2010 Elsevier Ltd. All rights reserved.	approximation algorithm;centralized computing;database;deployment environment;gamebryo;gigabyte;gigapixel image;graphics hardware;graphics processing unit;image resolution;just-in-time compilation;multi-user;open world;pixel;production system (computer science);real-time clock;real-time computing;requirement;robustness (computer science);scott continuity;server (computing);streaming algorithm;texture mapping;version control	Charles-Frederik Hollemeersch;Bart Pieters;Aljosha Demeulemeester;Frederik Cornillie;Bert Van Semmertier;Erik Mannens;Peter Lambert;Piet Desmet;Rik Van de Walle	2010	Computers & Graphics	10.1016/j.cag.2010.09.012	computer vision;simulation;computer science;artificial intelligence;operating system;scheduling;multimedia;texture atlas;texture memory;graphics hardware;compositing;computer graphics (images)	Graphics	66.69470407895074	-52.3913254602942	110169
76adaee74309c5d87f67b62e277f932a1010b8d7	scene-adaptive single image dehazing via opening dark channel model	colour distortion;haze degraded image;odcm;atmosphere scattering model;opening dark channel model;scene adaptive single image dehazing;domain transform filter;guide filter;image depth information;minimum channel image	Many traditional dark channel prior based haze removal schemes often suffer from the colour distortion and generate halo artefacts in the remote scenes. To tackle these issues, the authors present an efficient scene-adaptive single image dehazing approach via opening dark channel model (ODCM). First, the authors detect the image depth information and separate it into close view and distant view. Then, an ODCM is proposed to optimise the whole atmospheric veil, in which the values of close view are regularised by a minimum channel image while the distant parts are estimated by an appropriate lower constant. Accordingly, the transmission map can be further optimised by guide filter and smoothed by domain transform filter. Finally, the haze degraded image can be well restored by the atmosphere scattering model. The extensive experiments have shown that the proposed image dehazing approach has significantly increased the perceptual visibility of the scene and achieved a better colour fidelity visually.	autostereogram;channel (communications)	Xin Liu;He Zhang;Yuan Yan Tang;Ji-Xiang Du	2016	IET Image Processing	10.1049/iet-ipr.2016.0138	computer vision;computer graphics (images)	Vision	57.680996459642564	-59.78909567747512	110266
6db3e31d746671ae44f83b92b379d621148db9c3	real-time stereo matching on cuda using an iterative refinement method for adaptive support-weight correspondences	iterative refinement;real time systems approximation methods stereo vision complexity theory accuracy graphics processing unit iterative methods;image matching;cuda;iterative methods;stereo image processing image matching iterative methods;real time stereo matching adaptive support weights cuda iterative refinement;stereo image processing;adaptive support weights;real time stereo matching;middlebury stereo benchmark cuda iterative refinement method adaptive support weight correspondences high quality real time stereo matching computer vision semiautomated robotic surgery teleimmersion 3 d video surveillance two pass approximation adaptive support weight aggregation two pass method probabilistic framework matching cost minimization iterative processing parallel high performance graphics hardware compute unified device architecture computing engine	High-quality real-time stereo matching has the potential to enable various computer vision applications including semi-automated robotic surgery, teleimmersion, and 3-D video surveillance. A novel real-time stereo matching method is presented that uses a two-pass approximation of adaptive support-weight aggregation, and a low-complexity iterative disparity refinement technique. Through an evaluation of computationally efficient approaches to adaptive support-weight cost aggregation, it is shown that the two-pass method produces an accurate approximation of the support weights while greatly reducing the complexity of aggregation. The refinement technique, constructed using a probabilistic framework, incorporates an additive term into matching cost minimization and facilitates iterative processing to improve the accuracy of the disparity map. This method has been implemented on massively parallel high-performance graphics hardware using the Compute Unified Device Architecture computing engine. Results show that the proposed method is the most accurate among all of the real-time stereo matching methods listed on the Middlebury stereo benchmark.	algorithmic efficiency;approximation;benchmark (computing);binocular disparity;cuda;closed-circuit television;computer stereo vision;computer vision;graphics hardware;iterative method;iterative refinement;real-time clock;real-time transcription;real-time web;refinement (computing);robot;semiconductor industry;utility functions on indivisible goods	Jedrzej Kowalczuk;Eric Psota;Lance C. Pérez	2013	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2012.2203200	computer vision;computer science;theoretical computer science;mathematics;iterative method;computer graphics (images)	Vision	55.60644280201503	-55.65437090581932	110373
2a650b906bcfb8b6ea378b0a71bc543859307b87	a new noise removing algorithm	randomness of noise;patches sample;image denoise;extended ssd quad tree split;detail preserve;noise removal	In this paper, we propose a new algorithm for removing Gaussian noise from natural images while preserving important details for further processing. Similar patches appear frequently in an image. Our algorithm is based on this high redundancy in natural images. Our initial step is to perform an image segmentation by a constrained quad-tree-split-and-merge technique. Then for each patch, using an extended SSD, we identify all similar patches in the image and recalculate the patch by summing up a weighted copy of each similar patch. Since the mean of white noise is zero, the final image will be noise free and important details are preserved. Experimental comparisons of Wiener Filter, Wavelet Threshold Methods and our method will be carried out by using different types of images. The result shows that our method is more efficient in both noise removing and detail preserving.	algorithm;extended precision;image segmentation;quadtree;solid-state drive;wavelet;white noise;wiener filter	Dong-Liang Bi;Wei Guo;Aidong Xu	2006	Int. J. Image Graphics	10.1142/S0219467806002446	gradient noise;gaussian noise;median filter;computer vision;value noise;machine learning;pattern recognition;mathematics;salt-and-pepper noise	ML	57.82864691017779	-65.61127510069572	110584
b3cac3f8479a8020ffca86350243f32a8416e141	reconstruction method of missing texture using error reduction algorithm	error reduction;fourier transform;reconstruction algorithms fourier transforms image reconstruction image restoration iterative algorithms computer errors discrete fourier transforms information science information retrieval phase estimation;phase retrieval;phase estimation error reduction algorithm missing texture reconstruction method phase retrieval methods fourier transform magnitude estimation selection scheme;image texture;phase estimation image texture image reconstruction fourier transforms;phase estimation;image reconstruction;fourier transforms	This paper presents a novel reconstruction method of missing textures using an error reduction algorithm which is one of phase retrieval methods. The proposed method estimates the Fourier transform magnitude of the missing area from another area whose texture is similar in the obtained image. In order to realize this, a novel approach that monitors the errors caused by the error reduction algorithm is introduced into the selection scheme of the similar texture. Further, the proposed method estimates the phase of the target area by using the error reduction algorithm modified for the texture reconstruction and can restore the missing area accurately. Some experimental results show that the proposed method achieves more accurate restoration than that of the traditional methods.	algorithm;circuit restoration;phase retrieval	Takahiro Ogawa;Miki Haseyama;Hideo Kitajima	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1530233	fourier transform;computer vision;pattern recognition;mathematics;phase correlation;adaptive-additive algorithm;statistics	Vision	57.58620350006273	-65.79877888850244	110626
46e6d9eb9f8888eedf7199a46398fe5111735d29	towards a quality metric for dense light fields		Light fields become a popular representation of three-dimensional scenes, and there is interest in their processing, resampling, and compression. As those operations often result in loss of quality, there is a need to quantify it. In this work, we collect a new dataset of dense reference and distorted light fields as well as the corresponding quality scores which are scaled in perceptual units. The scores were acquired in a subjective experiment using an interactive light-field viewing setup. The dataset contains typical artifacts that occur in light-field processing chain due to light-field reconstruction, multi-view compression, and limitations of automultiscopic displays. We test a number of existing objective quality metrics to determine how well they can predict the quality of light fields. We find that the existing image quality metrics provide good measures of light-field quality, but require dense reference light-fields for optimal performance. For more complex tasks of comparing two distorted light fields, their performance drops significantly, which reveals the need for new, light-field-specific metrics.	distortion;experiment;flicker (screen);gaussian blur;image quality;image scaling;light field;mask (computing);multiscopy;parallax;resampling (statistics);rollover (key);sparse matrix;synthetic intelligence;visual artifact	Vamsi Kiran Adhikarla;Marek Vinkler;Denis Sumin;Rafal Mantiuk;Karol Myszkowski;Hans-Peter Seidel;Piotr Didyk	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.396	iterative reconstruction;computer vision;image quality;pattern recognition;resampling;computer science;artificial intelligence;distortion	Vision	60.978035315582794	-55.50226891859169	110696
89f27f46be80d4a45f41e6778048551a3c250b55	structural videotext regions completion with temporal-spatial consistency	interpolation;image motion analysis;spatial temporal consistency;video signal processing;videoconference;temporal spatial consistency;pervasive computing;video gradient interpolation approach temporal spatial consistency structural videotext video content motion videotext scrolling videotexts canny edge map spatial texture region completion;real time;canny edge map;spatial texture region completion;video sequences;layout;computer networks;image texture;videotext removal video completion edge completion spatial temporal consistency;video content;structural videotext;scrolling videotexts;voting;edge completion;genetic algorithms;humans;motion videotext;text recognition;video completion;video gradient interpolation approach;videotext removal;video signal processing image motion analysis image texture interpolation;videoconference humans video sequences voting computer networks pervasive computing interpolation text recognition layout genetic algorithms	With the rapid speed of spreading video content, some videotext exists in the video content. Usually some videotext is the advertisements and it is not needed for some people. Most video completion methods deal with the object removing or broken regions occurring when the object is undesired or original video is broken. But few methods can handle the completion of the original region occupied by motion videotext well since the complex background of motion videotext. Most existing algorithm took a lot computation time for video completion. However, the completed region should be generated as soon as possible based on the real-time consideration. In this paper, we propose a fast and regular structural videotext region completion method to recover the original structural region occupied by the scrolling videotexts. We utilize the characteristic of the canny edge map of frames to form structural edge in the occupied region with temporal consistency. The spatial texture region completion is carried out by our proposed video gradient interpolation approach. We have completed the whole system and the experimental results show that all of the horizontal and vertical scrolling videotexts can be completed well.	algorithm;canny edge detector;computation;digital video;gradient;interpolation;jumbo frame;pixel;real-time clock;scrolling;simulation;time complexity;videotex	Tsung-Han Tsai;Chih-Lun Fang	2008	2008 IEEE International Conference on Sensor Networks, Ubiquitous, and Trustworthy Computing (sutc 2008)	10.1109/SUTC.2008.45	image texture;layout;computer vision;genetic algorithm;voting;interpolation;computer science;multimedia;videoconferencing;ubiquitous computing;computer network;computer graphics (images)	Vision	54.23570482258818	-60.25935552957098	110702
9a73b1342f9d152def4f46cc9d65d9923a836058	estimating the paddy spike yield using fractal dimension	image processing;digital camera;fractal dimension;color image	Paddy is one of major crops grown in China. Fractal dimension is an important parameter used to describe geometrical characteristics of many natural objects. The objective of this study is to investigate mathematical relationships between the fractal dimension and the yield of the paddy spike based on the fractal theory and the image processing technologies. The color images of paddy spikes were taken with a digital camera, then they were transacted to the gray using image processing. Fractal dimensions of the spikes were computed using the Box-Counting Method based on the gray images. The results indicate that there is a significant linear relationship between the fractal dimension and the yield.	digital camera;fractal dimension;image processing	Hongju Gong;Hua Li;Haiming Yu;Changying Ji	2008		10.1007/978-1-4419-0213-9_2	computer vision;fractal transform;computer graphics (images)	Graphics	64.98111532865732	-61.283157744208815	110750
cd2eb5f730358740988a0b2174162a5cd9293466	efficient sparse voxel octrees	sparse voxel octrees;compact data structure;octree;ray tracing sparse voxel octrees voxel representations feature rich geometry gpu compact data structure ray cast acceleration smooth surface compact encoding variable radius postprocess filtering technique shading attribute discrete sampling triangle based representations;feature rich geometry;octrees geometry image color analysis graphics processing unit arrays rendering computer graphics approximation methods;approximation method;shading attribute discrete sampling;efficient algorithm;volumetric image representation;geometry;computational geometry;computer graphic equipment;gpu;ray tracing computational geometry computer graphic equipment coprocessors filtering theory image representation octrees;coprocessors;voxel;computer graphic;cuda;arrays;ray cast acceleration;variable radius postprocess filtering technique;image color analysis;image representation;ray tracing;graphic processing unit;approximation methods;cuda voxel octree ray tracing volumetric image representation gpu;rendering computer graphics;graphics processing unit;smooth surface compact encoding;data structure;triangle based representations;voxel representations;filtering theory;octrees;open source;ray casting;discrete sampling	In this paper, we examine the possibilities of using voxel representations as a generic way for expressing complex and feature-rich geometry on current and future GPUs. We present in detail a compact data structure for storing voxels and an efficient algorithm for performing ray casts using this structure. We augment the voxel data with novel contour information that increases geometric resolution, allows more compact encoding of smooth surfaces, and accelerates ray casts. We also employ a novel normal compression format for storing high-precision object-space normals. Finally, we present a variable-radius postprocess filtering technique for smoothing out blockiness caused by discrete sampling of shading attributes. Based on benchmark results, we show that our voxel representation is competitive with triangle-based representations in terms of ray casting performance, while allowing tremendously greater geometric detail and unique shading information for every voxel. Our voxel codebase is open sourced and available at http://code.google.com/p/efficient-sparse-voxel-octrees/.	algorithm;benchmark (computing);compression;data structure;graphics processing unit;homeomorphism (graph theory);maximum voxel;open-source software;ray casting;sampling (signal processing);shading;smoothing (statistical technique);software feature;sparse matrix	Samuli Laine;Tero Karras	2011	IEEE transactions on visualization and computer graphics	10.1109/TVCG.2010.240	ray tracing;computer vision;data structure;computational geometry;computer science;theoretical computer science;ray casting;geometry;voxel;coprocessor;octree;computer graphics (images)	Visualization	67.58360019149477	-52.396246134671124	110809
304412a849e6ec260f15ad42d8205c43bcdea54f	detecting image splicing using geometry invariants and camera characteristics consistency	spliced image classification method;crf estimation;computational geometry;large data sets;image classification;null;image authentication;response function;statistical analysis;single channel;cross fitting error;single channel image digital image tampering image authentication spliced image classification method geometry invariant camera response function crf estimation cross fitting error statistical classifier camera characteristic consistency;statistical analysis cameras computational geometry image classification;single channel image;camera characteristic consistency;digital image;splicing watermarking computational geometry layout digital images authentication computer vision digital cameras image classification pixel;cameras;statistical classifier;digital image tampering;camera response function;geometry invariant	Recent advances in computer technology have made digital image tampering more and more common. In this paper, we propose an authentic vs. spliced image classification method making use of geometry invariants in a semi-automatic manner. For a given image, we identify suspicious splicing areas, compute the geometry invariants from the pixels within each region, and then estimate the camera response function (CRF) from these geometry invariants. The cross-fitting errors are fed into a statistical classifier. Experiments show a very promising accuracy, 87%, over a large data set of 363 natural and spliced images. To the best of our knowledge, this is the first work detecting image splicing by verifying camera characteristic consistency from a single-channel image	computer vision;conditional random field;digital image;experiment;frequency response;pixel;semiconductor industry;sensor;statistical classification;verification and validation	Yu-Feng Hsu;Shih-Fu Chang	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262447	computer vision;contextual image classification;computational geometry;computer science;theoretical computer science;pattern recognition;mathematics;digital image;statistics	Vision	54.42734063586201	-61.35175930986373	110850
b8ae7b97ff837d54991da223af5a2f593b6a7dd7	computational inexpensive two step auto white balance method	detecteur image;reflectivity;facteur reflexion;implementation;balance de blancos;image sensors;chromaticity;reflectance;percepcion visual;image reconstruction;balance blancs;perception visuelle;chromaticite;color correction;visual perception;cromaticidad;detector imagen;computer hardware;white balance;implementacion;hardware implementation;image sensor;human visual perception;coeficiente reflexion	The chromaticity of an acquired image reconstructed from a Bayer pattern image sensor is heavily dependent on the scene illuminant and needs color corrections to match human visual perception. This paper presents a method to 'white balance' an image that is computationally inexpensive for hardware implementation, has reasonable accuracy without the need of storing the full image, and is aligned to the current technical development of the field. The proposed method introduces the use of a 2D chromaticity diagram of the image to extract information about the resultant scene reflectance. It assumes that the presence of low-saturated colors in the scene will increase the probability of retrieving accurate scene color information.	color balance;computation	Sergio Goma;Milivoje Aleksic	2006		10.1117/12.643926	computer vision;image sensor;reflectivity;color balance;optics;physics;computer graphics (images)	EDA	62.30587091878696	-58.67846904624412	110917
13a2a032c56a47d82bae5083ec8b39a34477c4da	multiview stereoscopic video hole filling considering spatiotemporal consistency and binocular symmetry for synthesized 3d video	filling three dimensional displays color visualization image color analysis stereo image processing optimization;visual discomfort hole filling synthesized 3d video binocular symmetry temporal consistency	This paper proposes a new hole-filling method with spatiotemporal consistency and binocular symmetry for synthesized 3D videos in view extrapolation. Disocclusion regions in the synthesized views at virtual viewpoints result in regions with missing content. These regions will be referred to as hole regions. To provide the high-quality synthesized 3D videos via 3D display, the hole regions need to be filled considering the characteristics of human visual perception. From the perceptual point of view, binocular asymmetry between synthesized left- and right-eye videos (i.e., stereo pair video) is one of the most important factors that induce visual discomfort in stereoscopic viewing. In addition, binocular symmetry without temporal consistency between temporally neighboring frames could cause visual discomfort by annoying flickering artifacts. In this paper, to maintain the spatiotemporal consistency and binocular symmetry in synthesized 3D videos at multiple virtual viewpoints, we propose a global optimization-based hole-filling method using the information from the already filled adjacent view and previous frame. Furthermore, to reduce the computational cost of the global optimization, we propose a label propagation method, which propagates reliable labels used in the adjacent view and previous frame to the target image to be filled. The performance of the proposed method has been evaluated by objective assessments of 3D image quality, temporal consistency, and computational efficiency. In addition, subjective assessment is conducted for measuring the visual comfort and overall quality. The experimental results proved that the proposed method provides hole filling results with spatiotemporal consistency and binocular symmetry.	algorithmic efficiency;binocular vision;computation;computational complexity theory;extrapolation;global optimization;horner's method;image quality;integral imaging;mathematical optimization;parallax;software propagation;stereo display;stereoscopy;time complexity	Hak Gu Kim;Yong Man Ro	2017	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2016.2515360	computer vision;pattern recognition;artificial intelligence;computer science;visual discomfort;flicker;image quality;stereoscopy;visualization;asymmetry;stereo display;visual perception	Vision	57.64491449259127	-55.75758736886758	110919
56c5de7481114892a148b0b719d82edd6ae88766	autocorrelation-based interlaced to progressive format conversion	edge preserving;blackman harris windowed sinc filter;format conversion;video deinterlacing	Abstract To generate a high resolution image from a low resolution one, interpolation plays a crucial role. However, conventional interpolation methods including edge-based interpolation methods have some drawbacks such as the limited number of edge directions, imprecise edge detection, and inefficient interpolation. To overcome these shortcomings, we propose a new edge-directed interpolation method, which has three aims: various edge directions, reliable edge detection, and outstanding interpolation. Since the number of candidate edge directions in the proposed method is flexible, we can use several edges included in the high resolution image. To accurately determine the edge direction, we use autocorrelation of neighboring pixels for candidate directions based on the duality between a high resolution image and its corresponding low resolution image. For the interpolation step, we utilize a Blackman–Harris windowed-sinc weighted average filter where we use correlation values obtained in the edge detection step as weights. Experimental results show that the proposed method outperforms conventional methods in terms of both the subjective and the objective results.	autocorrelation;complexity;edge detection;interlaced video;interpolation;lr parser;pixel;simulation;sinc function;window function;x86	Joohyeok Kim;Gwanggil Jeon;Jechang Jeong	2014	Digital Signal Processing	10.1016/j.dsp.2014.01.002	demosaicing;computer vision;mathematical optimization;bilinear interpolation;stairstep interpolation;bicubic interpolation;mathematics;nearest-neighbor interpolation;multivariate interpolation;statistics;image scaling	Graphics	58.2456436752134	-64.87164030405687	110993
2f63f2339e2ad6d4d221df449c384c9e6b05a447	suppression of sampling moire in color printing by spline-based least-squares prefiltering	image processing;bivariate splines;least squares approximation;low pass filter;hexagonal lattices;technology and engineering;least square;sampling moire;resampling;color printing	Many image processing systems, including those for printing applications, need sampling conversions for the representation of an image from one lattice to another. For example in the case of printing, classical halftoning requires new sample values on the halftone lattice. Although often considered as a straightforward procedure, resampling can cause so-called sampling moire due to aliasing. These artifacts are often very noticeable and as such undesirable, in particular for high-quality printing. In color printing, each color separation uses its own halftone lattice. Therefore, moire patterns will not only display an unexpected new frequency and orientation, but also influence the color appearance itself. These artifacts are frequently encountered in commercial (even high-quality) printing since the interpolation algorithms used in RIPs are simple (e.g., bilinear interpolation) and do not take into account the nature of the target lattice. Approaches such as simple low-pass filtering unacceptably blur the edges, while manual selective smoothing by an operator is very time-consuming. This paper proposes an optimal prefilter which is based on the least-squares linear resampling paradigm. Our approach requires proper discrete/continuous models, i.e., for both the source and the target lattices, and computes the associated reconstruction function which minimizes the error between the representations in the continuous domain. The reconstruction function jointly takes into account the Nyquist areas of every color separation using a novel hexagonal spline model resulting into an optimal prefilter before halftoning. Experimental results show that after prefiltering, the images are much less prone to moire while not suffering from noticeable edge blurring. 2002 Elsevier Science B.V. All rights reserved.	algorithm;aliasing;approximation;bilinear filtering;digital camera;gaussian blur;image processing;image resolution;image scanner;interpolation;least squares;low-pass filter;print job;printing;programming paradigm;resampling (statistics);sampling (signal processing);smoothing;spline (mathematics);zero suppression	Dimitri Van De Ville;Wilfried Philips;Ignace Lemahieu;Rik Van de Walle	2003	Pattern Recognition Letters	10.1016/S0167-8655(02)00332-X	computer vision;image processing;computer science;machine learning;mathematics;least squares;statistics;computer graphics (images)	Graphics	58.84723232102619	-60.160412818674416	111206
195634b4cef3f197ba19a9391573802fdb1a7ac3	hardware implementation of the median-rational hybrid filters for colour images	median filter;nonlinear filter	Median-Rational Hybrid Filter (MRHF) was introduced recently as a new class of nonlinear filters and applied to image filtering problems. It has been shown that the MRHF have the inherent property that on smooth areas they provide good noise attenuation whereas on changing areas the noise attenuation is traded for a good response to the change. Moreover, they act in small window and few number of operations, resulting in simple and fast filter structures. We present in this paper a hardware implementation of the MRHF which exploits in an effective way the features and the robustness of both median filters and rational filters. This architecture is suitable for the u s in real time due to its reduced hardware complexity in applications where size and cost are of critical significance.	data structure;field-programmable gate array;nonlinear system;real-time clock;utility functions on indivisible goods	Lazhar Khriji;Moncef Gabbouj;Giuseppe Bernacchia;Giovanni L. Sicuranza	1999			architecture;nonlinear filter;computer hardware;median filter;mathematics;filter design;weighted median;rational function;compromise;adaptive filter	Robotics	57.86655024452957	-64.80695176834193	111224
fa8960ea807df495d3ea9874c97bce803cf85d21	depth estimation from a monocular view of the outdoors	geophysical image processing;3d image;monocular image;solid modelling approximation theory geophysical image processing image colour analysis image fusion image resolution image segmentation;image segmentation;image resolution;multi resolution analysis;3d imaging;edge detection;image fusion;dark channel;three dimensional;approximation theory;object segmentation;computational savings three dimensional depth estimation single monocular outdoor image dark channel multiresolution analysis image location object segmentation color saturation depth information approximation final depth map;dark channel 3d image depth estimation monocular image;estimation;image edge detection;three dimensional displays;image color analysis;image colour analysis;pixel;pixel three dimensional displays image resolution image color analysis estimation image edge detection image segmentation;ground truth;depth estimation;depth map;solid modelling	This study presents an algorithm for estimating three-dimensional (3D) depth from a single monocular outdoor image. This algorithm combines three major components: the initial depth prior, the dark channel prior, and multi-resolution analysis. The initial depth prior assigns an initial depth to each object based on its location in the image. The object segmentation in this step is based on edge, hue, and saturation. The dark channel prior cooperates with color saturation to generate approximate depth information. Combining the depth maps of both initial depth prior and dark channel prior generates depth maps of three resolutions. Fusing these maps together across different resolutions produces the final depth map. Experimental results show that the proposed method achieves a considerable computational savings compared to existing methods while retaining a depth map close to the ground truth.	approximation algorithm;bilateral filter;computation;depth map;ground truth;mobile device;multiresolution analysis;object-based language	Tien-Ying Kuo;Yi-Chung Lo	2011	IEEE Transactions on Consumer Electronics	10.1109/TCE.2011.5955227	stereoscopy;computer vision;computer science;computer graphics (images)	Vision	57.37770314864242	-56.59182164226184	111341
eca0aa271a38b68a09515f3e1678056e1ded0d27	using gaze information to improve image difference metrics	gaze;man;eye;4230;etude experimentale;0130c;weighting;metrics;mirada;observers;ponderacion;journal article;algorithme;regard;observateur;eyes;peer reviewed;perception visuelle;algorithms;visual perception;4266;ponderation;oeil;metrique;homme	We have used image difference metrics to measure the quality of a set of images to know how well they predict perceived image difference. We carried out a psychophysical experiment with 25 observers along with a recording of the observers gaze position. The image difference metrics used were CIELAB ∆Eab, S-CIELAB, the hue angle algorithm, iCAM and SSIM. A frequency map from the eye tracker data was applied as a weighting to the image difference metrics. The results indicate an improvement in correlation between the predicted image difference and the perceived image difference.	algorithm;eye tracking;structural similarity	Marius Pedersen;Jon Y. Hardeberg;Peter Nussbaum	2008		10.1117/12.764468	computer vision;peer review;visual perception;artificial intelligence;weighting;optics;metrics	Vision	62.54255460421013	-62.47662917272596	111533
bd3266bfabe6d4336dccba76415ef078750b3f04	development of an xyz digital camera with embedded color calibration system for accurate color acquisition	color space;color difference	Acquisition of accurate colors is important in the modern era of widespread exchange of electronic multimedia. The variety of device-dependent color spaces causes troubles with accurate color reproduction. In this paper we present the outlines of accomplished digital camera system with device-independent output formed from tristimulus XYZ values. The outstanding accuracy and fidelity of acquired color is achieved in our system by employing an embedded color calibration system based on emissive device generating reference calibration colors with user-defined spectral distribution and chromaticity coordinates. The system was tested by calibrating the camera using 24 reference colors spectrally reproduced from 24 color patches of the Macbeth Chart. The average color difference (CIEDE2000) has been found to be ΔE = 0.83, which is an outstanding result compared to commercially available digital cameras.	digital camera;xyz file format	Maciej Kretkowski;Ryszard Jablonski;Yoshifumi Shimodaira	2010	IEICE Transactions		color gradient;color histogram;false color;rgb color model;computer vision;color filter array;icc profile;color model;color quantization;color depth;colorimetry;computer science;high color;accuracy and precision;color balance;color space;statistics;computer graphics (images)	Graphics	62.7044384649753	-58.57905033906719	111708
6de36a9fa43d499e3f20f1e4af829be31cb8b7af	a possible neural mechanism for computing shape from shading	shape from shading;image formation;natural images;line drawings	A simple neural mechanism that recovers surface shape from image shading is derived from a simplified model of the physics of image formation. The mechanism's performance is surprisingly good even when applied to complex natural images, and is even able to extract significant shape information from some line drawings.	emoticon;illumination (image);image formation;line drawing algorithm;linear model;photometric stereo;preprocessor;shading	Alex Pentland	1989	Neural Computation	10.1162/neco.1989.1.2.208	computer vision;photometric stereo;computer science;image formation;computer graphics (images)	ML	56.442197658054454	-52.35893520970438	111731
1422e363a4124ca5eff1269029488ed856ab563c	a bayesian method for probable surface reconstruction and decimation	construccion arquitectura tecnologia ambiental;computacion informatica;measurement;learning;remeshing;grupo de excelencia;decimation;surface reconstruction;bayesian method;probabilistic model;bayesian;ciencias basicas y experimentales;smoothing;surface model;theory;denoising;tecnologias;computer animation;fairing;measurement noise	We present a Bayesian technique for the reconstruction and subsequent decimation of 3D surface models from noisy sensor data. The method uses oriented probabilistic models of the measurement noise and combines them with feature-enhancing prior probabilities over 3D surfaces. When applied to surface reconstruction, the method simultaneously smooths noisy regions while enhancing features such as corners. When applied to surface decimation, it finds models that closely approximate the original mesh when rendered. The method is applied in the context of computer animation where it finds decimations that minimize the visual error even under nonrigid deformations.	approximation algorithm;computer animation;decimation (signal processing)	James Diebel;Sebastian Thrun;Michael Brünig	2006	ACM Trans. Graph.	10.1145/1122501.1122504	computer vision;speech recognition;bayesian probability;computer science;mathematics;statistics;computer graphics (images)	Graphics	57.91801631947813	-52.14760091044231	112014
2902d8dc1875be1b4581dca4bed3d80f8aeb2c5e	color demosaicing using multi-frame super-resolution	high resolution;synthetic image sequence color demosaicing multiframe super resolution imaging sensor fabrication process signal to noise ratio frame rate pixel size spatial resolution signal processing techniques image quality high resolution color image cmos sensor color filter array iterative super resolution single output image color image planes;image quality;image color analysis signal resolution interpolation spatial resolution sensors signal processing algorithms;super resolution;signal to noise ratio;iterative methods cmos image sensors filtering theory image colour analysis image resolution image segmentation image sequences interpolation;image sensor;color image	Despite the considerable advances in the fabrication processes of imaging sensors, their performance remains limited by theoretical constraints that compromise the achievable sensitivity, frame rate, signal to noise ratio, pixel size and spatial resolution. Therefore, signal processing techniques are needed to improve the final image quality. In this paper, we present a framework for producing a high-resolution color image directly from a sequence of images captured by a CMOS sensor that is overlaid with a color filter array. The method is based on iterative super-resolution that separately filters the color image planes, interpolates and combines these into a single output image. We present experimental results using synthetic image sequence as well as real data from CMOS sensors. The results confirm the potential of this technique to achieve superior image quality.	active pixel sensor;algorithm;cmos;color filter array;color image;demosaicing;image quality;image resolution;interpolation;iterative method;motion estimation;signal processing;signal-to-noise ratio;super-resolution imaging;synthetic data	Mejdi Trimeche	2008	2008 16th European Signal Processing Conference		image quality;color co-site sampling;demosaicing;color histogram;image texture;image restoration;image noise;rgb color model;computer vision;color filter array;electronic engineering;feature detection;image resolution;color image;image gradient;binary image;image processing;digital image processing;mathematics;sub-pixel resolution;histogram equalization;computer graphics (images)	Vision	59.29463500152219	-58.92301950778659	112120
2a492b6ba5eae520d3c107f001ac72e8b0fee001	image registration and change detection under rolling shutter motion blur	cameras three dimensional displays distortion solid modeling sensor arrays kernel;aerial imaging rolling shutter motion blur change detection image registration	In this paper, we address the problem of registering a distorted image and a reference image of the same scene by estimating the camera motion that had caused the distortion. We simultaneously detect the regions of changes between the two images. We attend to the coalesced effect of rolling shutter and motion blur that occurs frequently in moving CMOS cameras. We first model a general image formation framework for a 3D scene following a layered approach in the presence of rolling shutter and motion blur. We then develop an algorithm which performs layered registration to detect changes. This algorithm includes an optimisation problem that leverages the sparsity of the camera trajectory in the pose space and the sparsity of changes in the spatial domain. We create a synthetic dataset for change detection in the presence of motion blur and rolling shutter effect covering different types of camera motion for both planar and 3D scenes. We compare our method with existing registration methods and also show several real examples captured with CMOS cameras.	algorithm;cmos;deblurring;defense in depth (computing);distortion;estimated;experiment;gaussian blur;image formation;image rectification;image registration;mathematical optimization;motion compensation;movie projector;rolling hash;shutter device component;sparse matrix;synthetic intelligence;unified framework;anatomical layer;registration - actclass	Vijay Rengarajan;A. N. Rajagopalan;Rangarajan Aravind;Guna Seetharaman	2017	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2016.2630687	motion blur;artificial intelligence;computer science;image restoration;rolling shutter;computer vision;pattern recognition;change detection;motion interpolation;image formation;distortion;image registration	Vision	54.963849205684156	-53.29036183349782	112244
0ac6bee272fe6af9b1843cf26aabafbface19473	improving the spatial resolution of imaging instruments using software		In order to overcome spatial resolution limitations associated with physical sensor limitations when using smallsats and cubesats, we utilize an image processing technology referred to as Super-Resolution (SR). In general, software approaches are increasingly considered in connection with smaller satellites for which size, mass and power constraints limit the sensor capabilities. Being able to perform hardware vs. software trades might enable more capabilities for a lower cost. This paper describes recent experiments conducted to optimize the spatial enhancement of acquired observations using multiple sub-pixel shifted low resolution image.	experiment;image processing;image resolution;imaging instruments;pixel;super-resolution imaging	Mareboyana Manohar;Jacqueline Le Moigne;Philip Dabney	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518234	image processing;iterative reconstruction;remote sensing;computer vision;artificial intelligence;computer science;superresolution;software;image resolution	Robotics	67.35353442431939	-61.842489037119904	112349
d68f306db56004882dfd7e24937f24276834cf76	visual distortion assessment with emphasis on spatially transitional regions	optical distortion;analisis imagen;high pass filter;edge detection;signal distortion;humans error correction image edge detection decoding discrete cosine transforms low pass filters distortion measurement visual system video compression image coding;distorsion signal;distortion measurement;qualite image;video quality experts group;visual quality;filtre frequence spatiale;video coding;filtre passe haut;percepcion visual;human visual system;image quality;50 hz visual perception distortion assessment spatially transitional regions human visual system picture quality perceptual model visual quality decompressed images decompressed video perceptually disturbing artefacts edge impairments blurring false edges ringing rippling effects blockiness;filtro frecuencia espacial;perception visuelle;image analysis;visual perception;evaluation;spatial frequency filter;calidad imagen;evaluacion;analyse image;human perception;distortion measurement visual perception video coding edge detection optical distortion;filtro paso alto;transition region;distorsion senal	It is known that the human visual system (HVS) does not pay equal attention to each error and even region in judging picture quality. In this paper, we combine a perceptual model with an integrated detection of the spatially transitional regions in visual distortion evaluation to better match the HVS perception to visual quality. For decompressed images or video, the spatially transitional regions are the regions where major perceptually disturbing artefacts caused by edge impairments (mainly due to blurring and locations where the edge information is not adequately represented) and the presence of false edges (mainly due to blockiness and the presence of strong rippling effects of ringing) usually occur. Such regions are efficiently detected based on a single two-dimensional spatial high-pass filter in our work. Good correlation between the proposed method and the human perception has been demonstrated with the full set of 50-Hz video quality expert group test data.	distortion;experiment;human visual system model;image quality;relevance;ringing (signal);rippling;test data;wavelet	Ee Ping Ong;Weisi Lin;Zhongkang Lu;Susu Yao;Minoru Etoh	2004	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2004.825574	image quality;computer vision;image analysis;edge detection;visual perception;computer science;evaluation;high-pass filter;human visual system model;perception;computer graphics (images)	Vision	62.49605792212764	-62.186318611494215	112505
69fe469e66c323dba397788f9764a187a2142ab7	reconstruction of spectra using empirical basis functions	computer graphic;image synthesis;red green and blue	Physically-based image synthesis requires measured spectral quantities for illuminants and reflectances as part of the virtual scene description to compute trustworthy lighting simulations. When spectral distributions are not available, a method to reconstruct spectra from color triplets needs to be applied. A comprehensive evaluation of the practical applicability of previously published approaches in the context of realistic rendering is still lacking. Thus, we designed three different comparison scenarios typical for computer graphic applications to evaluate the suitability of the methods to reconstruct illumination and reflectance spectra. Furthermore, we propose a novel approach applying empirical mean spectra as basis functions to reconstruct spectral distributions. The mean spectra are derived from averaging sets of typical red, green, and blue spectra. This method is intuitive, computationally inexpensive, and achieved the best results for all scenarios in our evaluation. However, reconstructed spectra are not unrestrictedly applicable in physically-based rendering where reliable synthetic images are crucial.	basis function;rca spectra 70	Jakob Bärz;Tina Hansen;Stefan Müller	2010		10.1007/978-3-642-17289-2_56	computer vision;simulation;computer science;computer graphics (images)	ML	61.45334450675635	-52.12999676746871	112531
e221738929b4d66853b8abc02f929c0302b060a0	gamut mapping through perceptually-based contrast reduction		In this paper we present a spatial gamut mapping algorithm that relies on a perceptually-based variational framework. Our method adapts a well-known image energy functional whose minimization leads to image enhancement and contrast modification. We show how by varying the importance of the contrast term in the image functional we are able to perform gamut reduction. We propose an iterative scheme that allows our algorithm to successfully map the colors from the gamut of the original image to a given destination gamut while preserving the colors’ perception and texture close to the original image. Both subjective and objective evaluation validate the promising results achieved via our proposed framework.	algorithm;cheryl fuerte;code;color management;distortion;experiment;goto;human visual system model;image editing;intel gma;iteration;iterative method;many-one reduction;variational principle	Syed Waqas Zamir;Javier Vazquez-Corral;Marcelo Bertalmío	2013		10.1007/978-3-642-53842-1_1	computer vision;mathematics;computer graphics (images)	Vision	57.61491464892649	-61.62964936679934	112567
b5c3fd10025806cfe24e9375daf6df1a411b68df	interacting with large 3d datasets on a mobile device	image resolution;computer graphics;volume rendering;mobile volume rendering;visualization;graphics processing units;interactive states;interactive frame rates;mobile handsets;volume measurement;rendering computer graphics;ray casting	A detail-on-demand scheme can alleviate both memory and GPU pressure on mobile devices caused by volume rendering. This approach allows a user to explore an entire dataset at its native resolution while simultaneously constraining the texture size being rendered to a dimension that does not exceed the processing capabilities of a portable device. This scheme produces higher-quality, more focused images rendered at interactive frame rates, while preserving the native resolution of the dataset.	graphics processing unit;mobile device;native resolution;silo (dataset);volume rendering	Chris Schultz;Mike Bailey	2016	IEEE Computer Graphics and Applications	10.1109/MCG.2016.99	computer vision;tiled rendering;scientific visualization;image-based modeling and rendering;visualization;image resolution;3d rendering;rendering;computer science;parallel rendering;ray casting;real-time computer graphics;multimedia;real-time rendering;texture memory;computer graphics;sub-pixel resolution;alternate frame rendering;volume rendering;software rendering;3d computer graphics;computer graphics (images)	Visualization	67.21185989343387	-52.76831227117225	112981
5397ec4d1903efef1a09d3449f125dfde5ac026a	efficient hpr-based rendering of point clouds	topology;kernel;approximation algorithms;point based rendering;computational geometry;surface reconstruction point cloud visibility point based rendering;surface reconstruction;arrays;point cloud visibility;computational complexity;time complexity hpr operator based rendering point clouds visibility information estimation gpu implementation convex hull algorithm;graphics processing units;surface reconstruction rendering computer graphics approximation algorithms arrays kernel graphics processing units topology;rendering computer graphics;rendering computer graphics computational complexity computational geometry graphics processing units	Recently, Katz et al. have shown how visibility information for a point cloud may be estimated by the so-called HPR operator. In a nutshell, the operator consists of a simple transformation of the cloud followed by a convex hull computation. Since convex hulls take O(n log n) time to compute in the worst case, this method has been considered impractical for real-time rendering of medium to large point clouds. In this paper, we describe a GPU implementation of an approximate convex-hull algorithm that runs in O(n+k) time, where k is a parameter of the method. Experiments show that the method is suitable for real-time rendering and partial reconstruction of point clouds with millions of points.	approximation algorithm;best, worst and average case;computation;convex hull;experiment;graphics processing unit;point cloud;proteomics;real-time clock;real-time computing	Renan Machado e Silva;Claudio Esperança;Antonio A. F. Oliveira	2012	2012 25th SIBGRAPI Conference on Graphics, Patterns and Images	10.1109/SIBGRAPI.2012.26	computer vision;tiled rendering;image-based modeling and rendering;3d rendering;rendering;computer science;theoretical computer science;rendering equation;parallel rendering;real-time computer graphics;alternate frame rendering;volume rendering;software rendering;computer graphics (images)	Robotics	67.52005494351428	-52.32173366466258	113010
f128880f245addf88eaacda168805a6e423b8bd2	boundary artifact reduction in view synthesis of 3d video: from perspective of texture-depth alignment	background noise;3d video background noise boundary artifact foreground erosion texture depth alignment view synthesis;boundary artifact reduction scheme;color;pixel noise measurement image color analysis cameras merging color visualization;foreground erosion;noise measurement;boundary artifact;depth image based view synthesis;boundary artifact reduction scheme texture depth alignment depth image based view synthesis 3d video next generation broadcasting stereo matching algorithm;visualization;stereo matching;view synthesis;three dimensional displays;image color analysis;pixel;stereo image processing;texture depth alignment;merging;next generation;next generation broadcasting;stereo matching algorithm;depth map;next generation networks;data consistency;3d video;three dimensional displays next generation networks stereo image processing;cameras	3D Video (3DV) with depth-image-based view synthesis is a promising candidate of next generation broadcasting applications. However, the synthesized views in 3DV are often contaminated by annoying artifacts, particularly notably around object boundaries, due to imperfect depth maps (e.g., produced by state-of-the-art stereo matching algorithms or compressed lossily). In this paper, we first review some representative methods for boundary artifact reduction in view synthesis, and make an in-depth investigation into the underlying mechanisms of boundary artifact generation from a new perspective of texture-depth alignment in boundary regions. Three forms of texture-depth misalignment are identified as the causes for different boundary artifacts, which mainly present themselves as scattered noises on the background and object erosion on the foreground. Based on the insights gained from the analysis, we propose a novel solution of suppression of misalignment and alignment enforcement (denoted as SMART) between texture and depth to reduce background noises and foreground erosion, respectively, among different types of boundary artifacts. The SMART is developed as a three-step pre-processing in view synthesis. Experiments on view synthesis with original and compressed texture/depth data consistently demonstrate the superior performance of the proposed method as compared with other relevant boundary artifact reduction schemes.	algorithm;compression artifact;computer stereo vision;data compression;effective method;experiment;jagged array;map;pixel;preprocessor;ringing artifacts;smart;smoothing;view synthesis;zcam;zero suppression	Yin Zhao;Ce Zhu;Zhenzhong Chen;Dong Tian;Lu Yu	2011	IEEE Transactions on Broadcasting	10.1109/TBC.2011.2120730	computer vision;next-generation network;visualization;computer science;noise measurement;mathematics;background noise;multimedia;data consistency;pixel;depth map;computer graphics (images)	Vision	58.10832521979497	-59.9520354514978	113203
c0a8873a45180caea1e2b43acc95916bffa8dd41	adaptive four-dot median filter for removing 1-99% densities of salt-and-pepper noise in images		In﻿order﻿to﻿accelerate﻿denoising﻿and﻿improve﻿the﻿denoising﻿performance﻿of﻿the﻿current﻿median﻿filters,﻿ an﻿Adaptive﻿Four-dot﻿Median﻿Filter﻿(AFMF)﻿for﻿image﻿restoration﻿is﻿proposed﻿in﻿this﻿article.﻿AFMF﻿ is﻿not﻿only﻿very﻿efficient﻿and﻿fast﻿in﻿logic﻿execution,﻿but﻿also﻿it﻿can﻿restore﻿the﻿corrupted﻿images﻿ with﻿1–99%﻿densities﻿of﻿salt-and-pepper﻿noise﻿ to﻿ the﻿satisfactory﻿ones.﻿Without﻿any﻿complicated﻿ operation﻿for﻿noise﻿detection,﻿it﻿intuitively﻿and﻿simply﻿distinguishes﻿impulse﻿noises,﻿while﻿keeping﻿ the﻿noise-free﻿pixels﻿intact.﻿Only﻿the﻿uncorrupted﻿pixels﻿of﻿the﻿four-dot﻿mask﻿in﻿adaptive﻿filtering﻿ windows﻿are﻿used﻿for﻿the﻿adoption﻿of﻿candidates﻿for﻿median﻿finding,﻿whatever﻿filtering﻿window﻿size﻿ is.﻿Furthermore,﻿the﻿adoption﻿of﻿recursive﻿median﻿filters﻿leads﻿to﻿denoising﻿performance﻿improvement﻿ and﻿faster﻿filtering.﻿The﻿simple﻿logic﻿of﻿the﻿proposed﻿algorithm﻿obtains﻿significant﻿milestones﻿on﻿the﻿ fidelity﻿of﻿a﻿restored﻿image.﻿Relevant﻿experimental﻿results﻿on﻿subjective﻿visualization﻿and﻿objective﻿ digital﻿measure﻿validate﻿the﻿robustness﻿of﻿the﻿proposed﻿filter. KeywoRDS Image Denoising, Image Restoration, Impulse Noise, Median Filter, Salt-and-pepper Noise		Xin-Ming Zhang;Qiang Kang;Jinfeng Cheng;Xia Wang	2018	JITR	10.4018/JITR.2018070104	salt-and-pepper noise;data mining;median filter;computer vision;artificial intelligence;computer science	Vision	56.29559824373082	-65.48212839151688	113355
2562a82dc182104862ad1be749204f9d53a114bb	virtual view synthesis using multi-view video sequences	stereo matching multiview video sequence virtual view synthesis algorithm inverse warping occluded region detection forward warping technique;video signal processing image matching image sequences stereo image processing;forward warping;psnr;video sequences information technology cameras psnr layout bidirectional control government belief propagation;video signal processing;multi view video;image matching;video sequences;inverse warping stereo matching view synthesis multi view video forward warping;stereo matching;view synthesis;inverse warping;pixel;stereo image processing;coherence;signal processing algorithms;rendering computer graphics;cameras;image sequences	A virtual view synthesis algorithm using multi-view video sequences, which inherits the advantages of both the forward warping and the inverse warping, is proposed in this work. First, we use the inverse warping to synthesize a virtual view without holes from the nearest two views. Second, we detect occluded regions in the synthesized view based on the uniqueness constraint. Then, we refine the occluded regions using the information in the farther views based on the forward warping technique. Simulation results demonstrate that the proposed algorithm provides significantly higher PSNR performances than the conventional inverse warping scheme.	algorithm;peak signal-to-noise ratio;performance;simulation;view synthesis	Il-Lyong Jung;Taeyoung Chung;Kwanwoong Song;Chang-Su Kim	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5413529	image warping;computer vision;coherence;peak signal-to-noise ratio;computer science;multimedia;pixel;computer graphics (images)	Visualization	57.2969089782135	-55.51370163885573	113458
e8b10a795ec1da47564cf769128cbe816a967fed	a survey of gpu-based volume rendering of unstructured grids	unstructured grid;volume rendering;real time rendering;visual communication	Real-time rendering of large unstructured meshes is a major research goal in the scientific visualization community. While, for regular grids, texture-based techniques are well-suited for current Graphics Processing Units (GPUs), the steps necessary for rendering unstructured meshes are not so easily mapped to current hardware. This paper reviews volume rendering algorithms and techniques for unstructured grids aimed at exploiting high-performance GPUs. We discuss both the algorithms and their implementation details, including major shortcomings of existing approaches. Resumo: A visualização volumétrica de grandes malhas não estruturadas é uma das principais metas da comunidade de visualização científica. Enquanto que em grades regulares o uso de técnicas baseadas em textura são adequadas para as Unidades de Processamento Gráfico (GPUs) atuais, os passos necessários para exibir malhas não estruturas não são diretamente mapeadas para o hardware atual. Este artigo revisa algoritmos e técnicas de visualização volumétrica que exploram GPUs de alta performance. São discutidos tanto os algoritmos como seus detalhes de implementação, incluindo as principais dificuldades das abordagens atuais.	algorithm;bibliothèque de l'école des chartes;em intermediate language;graphics processing unit;real-time transcription;scientific visualization;unstructured grid;volume rendering	Cláudio T. Silva;João Luiz Dihl Comba;Steven P. Callahan;Fábio F. Bernardon	2005	RITA		rendering (computer graphics);computer vision;artificial intelligence;volume rendering;scientific visualization;computer science;computer graphics (images);graphics;real-time rendering;polygon mesh;unstructured grid	HPC	67.47805925972338	-54.1743923741461	113723
4bc68bbdcb0a1195396d66cd972b9f040312ad6a	color correction for tone reproduction		High dynamic range images require tone reproduction to match the range of values to the capabilities of the display. For computational reasons as well as absence of fully calibrate d imagery, rudimentary color reproduction is often added as a po stprocessing step rather than integrated into the tone reprod uction algorithm. However, in the general case this currently requ ires manual parameter tuning, although for some global tone repr oduction operators, parameter settings can be inferred from the tone curve. We present a novel and fully automatic saturatio n correction technique, suitable for any tone reproduction o perator, which exhibits better color reproduction than the stat e-ofthe-art and we validate its comparative effectiveness thro ugh psychophysical experimentation. Introduction Recent advances in both capture and display technologies al low images of a much wider dynamic range to be photographed, manipulated and displayed, better capturing the light of na tural scenes and giving artists unparalleled freedom. Unlike pre valent consumer imaging pipelines though, no high dynamic ran ge (HDR) standard has yet emerged defining the precise range, fo rmat or encoding to be used. As such, HDR data often needs to be compressed for display on most current displays, a process k nown as tonemapping [15, 2]. The aim of this paper is to preserve the appearance and information content of the image as much as possible while ensurin g that it can be displayed on the chosen display device. To achi eve that, tonemapping algorithms typically operate on the lumi nance of the image with little to no consideration for the color inf ormation present, leading to noticeable changes in the color app e rance of the image, as shown in Figure 1. Commonly, tone compressed images acquire an over-saturated appearance when only the l uminance channel is processed [12, 18]. Image appearance models, which can be seen as tone reproduction operators with integrated color appearance manage ment [7, 9, 16], aim to reproduce color appearance, but they are de signed with calibrated applications in mind and often come a t the cost of higher computational complexity due to spatially va rying processing. Despite their accuracy, these factors can limi t the r general applicability. Some solutions exist for correcting saturation mismatches after tone reproduction [12, 18]. This leads to computational ly efficient correction, although we have observed that existing methods tend to create hue and luminance artefacts. Moreover, th ey require manual parameter selection which is strongly image and tone reproduction operator dependent. Recently, a psychop hysical study was conducted for defining an automatic model to der ive the parameters necessary for such corrections, but only all ows parameters to be predicted when the tone compression or expans ion function is global [12]. Instead, we propose a new approach for correcting saturatio n mismatches after dynamic range compression. We base our alg orithm on insights from color science and on the observation t hat the amount of desaturation can be inferred from the non-line ar ty applied by the tone curve, irrespective of whether the tone r eproduction operator was spatially varying or not. As such, o ur approach is parameter-free and agnostic to the operator use d for mapping the dynamic range of the image or video. We find that our algorithm reproduces saturation significantly better t han the current state-of-the-art. Related Work Differences in viewing conditions may result in significant mismatches in perceived color, which can be attributed to id iosyncrasies of the human visual system. To ensure that the appear ance of a scene is correctly reproduced on a display, many issues w ill have to be taken into account, all broadly belonging to the fie ld of color reproduction [8]. Image appearance models can be us ed to reproduce images as a human observer would see them under given viewing conditions [5, 16]. Such algorithms can be con figured to yield calibrated color reproduction, and therefore do not require color post-processing. However, measurements of s cene and display conditions are needed as inputs to image appeara nce models so that the human visual response can be accurately pr edicted. This requires specialist equipment such as photome ters. These algorithms also tend to be computationally expensive , further limiting their use to offline processing. Dynamic range mismatches between scenes and display devices are therefore typically handled by tone reproduction operators. In essence, most of these algorithms focus on one dimen s on of the color gamut, namely compression along the luminance d irection [15, 2]. Appearance effects are often ignored, lead ing to images which may appear too saturated. This problem can be mi tigated by combining tone reproduction and color appearance lgorithms [1]. However, this solution still requires calibr ated data and measured viewing conditions to drive the color appearan ce component. A more common approach to saturation reproduction is to post-process the tone-mapped image, manually adjusting sa turation to levels that appear plausible. Perhaps the most wellknown technique for color correction involves the adjustment of c lor values by means of a power function, according to user parameter p ∈ [0,1] [18]. Given an original high dynamic range im-	algorithm;algorithmic efficiency;analysis of algorithms;color space;computational complexity theory;cyclic redundancy check;display device;eve;fo (complexity);han unification;high dynamic range;high-dynamic-range imaging;image editing;lumi masking;naruto shippuden: clash of ninja revolution 3;numerical aperture;online and offline;pipeline (computing);range imaging;scene statistics;self-information;tone mapping;video post-processing;visual computing	Tania Pouli;Alessandro Artusi;Francesco Banterle;Ahmet Oguz Akyüz;Hans-Peter Seidel;Erik Reinhard	2013			computer vision;artificial intelligence;computer science;pixel;high dynamic range;rgb color space;hue;human visual system model;gamut;color correction;tone reproduction	Graphics	61.10682854003232	-61.14596437653157	113805
3a8aba567a5db65e68c742a29f0cc4aa66c35b6b	variance soft shadow mapping	and texture;soft shadow;shadowing;algorithms;i 3 7 computer graphics three dimensional graphics and realism color;shading	Abstract#R##N##R##N#We present variance soft shadow mapping (VSSM) for rendering plausible soft shadow in real-time. VSSM is based on the theoretical framework of percentage-closer soft shadows (PCSS) and exploits recent advances in variance shadow mapping (VSM). Our new formulation allows for the efficient computation of (average) blocker distances, a common bottleneck in PCSS-based methods. Furthermore, we avoid incorrectly lit pixels commonly encountered in VSM-based methods by appropriately subdividing the filter kernel. We demonstrate that VSSM renders high-quality soft shadows efficiently (usually over 100 fps) for complex scene settings. Its speed is at least one order of magnitude faster than PCSS for large penumbra.		Baoguang Yang;Zhao Dong;Jieqing Feng;Hans-Peter Seidel;Jan Kautz	2010	Comput. Graph. Forum	10.1111/j.1467-8659.2010.01800.x	computer vision;shading;simulation;computer science;shadow mapping;algorithm;computer graphics (images)	NLP	64.7391273567499	-52.126857175982984	113898
c2e410d4be994b253112adb7231b075fa6180986	color image enhancement based on single-scale retinex with a jnd-based nonlinear filter	nonlinear filters;histograms;filtering;histogram equalization color image enhancement single scale retinex jnd based nonlinear filter;degradation;reflectivity;image converters;color;nonlinear filter;color nonlinear filters lighting histograms reflectivity degradation filtering image converters shape image enhancement;white light;image enhancement;shape;image colour analysis;color image enhancement;lighting;nonlinear filters image colour analysis image enhancement;just noticeable difference;single scale retinex;jnd based nonlinear filter;histogram equalization;color image	In this paper, we propose a color image enhancement based on the single-scale retinex (SSR) with a just noticeable difference (JND)-based nonlinear filter. In the proposed method, an input RGB color image is transformed into an HSV color image. Under the assumption of white-light illumination, the S and V component images are enhanced. In the enhancement of the V component image, the illumination is first estimated using the JND-based nonlinear filter. The output V component image is then obtained by subtracting some portion of the log signal of the estimated illumination from the log signal of the input V component image. The histogram modeling is next applied to the output V component image. The S component image is enhanced in proportion to the enhanced ratio of the V component image. Finally an output RGB color image is obtained from the enhanced V and S component images along with the original H component image. Experimental results show that the proposed method yields better performance of color enhancement over the conventional histogram equalization and SSR for test color images.	color image;histogram equalization;illumination (image);image editing;nonlinear system	Doo-Hyun Choi;Ick Hoon Jang;Mi Hye Kim;Nam Chul Kim	2007	2007 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2007.378664	filter;demosaicing;just-noticeable difference;nonlinear filter;color histogram;rgb color model;computer vision;hsl and hsv;degradation;color normalization;color image;image gradient;binary image;shape;computer science;lighting;histogram;mathematics;reflectivity;color balance;histogram equalization;computer graphics (images)	Vision	58.777459323822065	-62.43502910456138	113965
a7d174676683f373035415b3eb9ca1e0d8a58cc4	a two-step image inpainting algorithm using tensor svd		In this paper, we present a novel exemplar-based image inpainting algorithm using the higher order singular value decomposition (HOSVD). The proposed method performs inpainting of the target image in two steps. At the first step, the target region is inpainted using HOSVD-based filtering of the candidate patches selected from the source region. It helps to propagate the structure and color smoothly in the target region and restrict to appear unwanted artifacts. But a smoothing effect may be visible in the texture regions due to the filtering. In the second step, we recover the texture by an efficient heuristic approach using the already inpainted image. The experimental results show the superiority of the proposed method compared to the state of the art methods.	algorithm;artifact (software development);circuit restoration;heuristic;inpainting;singular value decomposition;smoothing	Mrinmoy Ghorai;Sekhar Mandal;Bhabatosh Chanda	2014		10.1007/978-3-319-16631-5_5	computer vision;artificial intelligence;higher-order singular value decomposition;filter (signal processing);pattern recognition;inpainting;tensor;computer science;markov random field;smoothing;algorithm;singular value decomposition;texture synthesis	Vision	56.30408754296988	-59.9891927510679	114012
d8d6bf5fdd978852c14d18e0ad89b07d9b482465	gray level requantization	analisis imagen;quantization;optimisation;cuantificacion;optimizacion;imagen nivel gris;quantification;image niveau gris;pattern recognition;image analysis;optimization;reconnaissance forme;reconocimiento patron;grey level image;analyse image	A distance measure between pictures, that enables comparison of images with different gray level ranges, is described. With such measure halftoning can be treated as an optimization problem. Based on this measure, an algorithm for reducing the number of gray levels of a picture with minimal visual degradation is developed. The algorithm can even produce a binary halftone of the picture, which is needed for many hard copy devices. The complexity of the algorithm is high, and it is not recommended as a daily halftoning method. However, the method provides a well-defined mathematical approach for comparing images. D 1988 Academic	algorithm;elegant degradation;grayscale;mathematical optimization;optimization problem	Michael Werman;Shmuel Peleg	1988	Computer Vision, Graphics, and Image Processing	10.1016/0734-189X(88)90044-8	computer vision;image analysis;quantization;computer science;computer graphics (images)	Vision	59.774024922616505	-63.51446318151746	114119
85150d205ee3b40181f820d610a1c9b207b04f8c	an adaptive random-valued impulse noise reduction method based on noise ratio estimation in highly corrupted images	median filters adaptive filters image denoising impulse noise;impulse noise;noise reduction signal to noise ratio psnr adaptive filters filtering pixel image edge detection samarium image restoration chaotic communication;adaptive filters;switching based median filters adaptive random valued impulse noise reduction method noise ratio estimation highly corrupted images adaptive edge preserved median filtering adaptive threshold estimation iterative psnr checking strategy;image denoising;median filters	In this paper, we propose a novel random-valued impulse-noise reduction method by adaptive edge-preserved median filtering, called AEPMF, with adaptive threshold and noise-ratio estimation. Generally, a pixel is always very similar to its horizontal and vertical neighbors and thus such a characteristic can be used to estimate the noise-ratio of a corrupted image for deriving appropriate thresholds. To substantially reduce noises, AEPMF uses iterative PSNR-checking strategy in which if the PSNR of the previously filtered image is lower than a threshold, next filtering process is executed. Experimental results manifest that the proposed AEPMF method is more robust and effective than other switching-based median filters and achieves above 20% in average PSNR improvement rate when the corruption ratio is above 30%.	iterative method;median filter;newton's method;noise reduction;peak signal-to-noise ratio;pixel	Chao-Ho Chen;Chao-Yu Chen;Chin-Hsing Chen	2007	Third International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP 2007)	10.1109/IIH-MSP.2007.71	adaptive filter;median filter;image noise;computer vision;impulse noise;computer science;mathematics;statistics;salt-and-pepper noise	Robotics	56.70467267159411	-65.63377895348049	114153
0fe41cc88bf685cbb12b02c7b0f338854babbc3a	a depth-guided inpainting scheme based on foreground depth-layer removal for high quality 2d to 3d video conversion	2d to 3d conversion		inpainting	Jangwon Choi;Yoonsik Choe;Yong-goo Kim	2013	IEICE Transactions		computer science;2d to 3d conversion	Vision	57.63182013940687	-54.999682992658975	114250
5dc457ff903bdf6610c5d6a3f9eea1d1ea9ecc13	renoir - a dataset for real low-light image noise reduction		Many modern and popular state of the art image denoising algorithms are trained and evaluated using images corrupted by artificial noise. These trained algorithms and their evaluations on synthetic data may lead to incorrect conclusions about their performances on real noise. In this paper we introduce a benchmark dataset of uncompressed color images corrupted by natural noise due to low-light conditions, together with spatially and intensity-aligned low noise images of the same scenes. The dataset contains over 120 scenes and more than 400 images, including both 16-bit RAW images and 8-bit BMP pixel and intensity-aligned images from 2 digital cameras (Canon S90 and Canon T3i) and a mobile phone (Xiaomi Mi3). We also introduce a method for estimating the true noise level in each of our images, since even the low noise images contain a small amount of noise. Finally, we exemplify the use of our dataset by evaluating four denoising algorithms: Active Random Field, BM3D, Bilevel MRF optimization, and Multi-Layer Perceptron. We show that while the Multi-Layer Perceptron and Bilevel MRF algorithms work as well as or even better than BM3D on synthetic noise, they lag behind on our dataset.	16-bit;8-bit;acoustic radiation force;algorithm;bmp file format;benchmark (computing);bilevel optimization;digital camera;exemplification;ground truth;markov random field;mathematical optimization;mobile phone;multilayer perceptron;noise (electronics);noise reduction;peak signal-to-noise ratio;performance;pixel;quad flat no-leads package;raw image format;structural similarity;synthetic data	Josue Anaya;Adrian Barbu	2018	J. Visual Communication and Image Representation	10.1016/j.jvcir.2018.01.012	image noise;random field;perceptron;pattern recognition;computer vision;artificial intelligence;noise reduction;lag;artificial noise;computer science	Vision	58.68068132705125	-57.82211685532871	114305
eefedd5ab8fa2043375eb9afbd149142f9216061	projectibles: optimizing surface color for projection	projection mapping;radiometric compensation	Typically video projectors display images onto white screens, which can result in a washed out image. Projectibles algorithmically control the display surface color to increase the contrast and resolution. By combining a printed image with projected light, we can create animated, high resolution, high dynamic range visual experiences for video sequences. We present two algorithms for separating an input video sequence into a printed component and projected component, maximizing the combined contrast and resolution while minimizing any visual artifacts introduced from the decomposition. We present empirical measurements of real-world results of six example video sequences, subjective viewer feedback ratings, and we discuss the benefits and limitations of Projectibles. This is the first approach to combine a static display with a dynamic display for the display of video, and the first to optimize surface color for projection of video.	algorithm;high dynamic range;image resolution;lcd projector;printing;video projector;visual artifact	Brett R. Jones;Rajinder Sodhi;Pulkit Budhiraja;Kevin Karsch;Brian P. Bailey;David A. Forsyth	2015		10.1145/2807442.2807486	rgb color model;computer vision;computer science;video tracking;multimedia;computer graphics (images)	HCI	60.468662216052415	-55.55010722148985	114478
bc5c54be16e7b46c275b719e61e0f02e5877f5dd	multi-scale analysis of odd gabor transform for edge detection	exponential distribution;adaptive thresholding;adaptive threshold determining method;gabor transform;edge detection;adaptive edge detection technique;gabor filter template;multiscale analysis;gabor filters;gabor filter;image edge detection gabor filters detectors estimation error robustness computer vision fourier transforms convolution laboratories pattern recognition;error estimation;transforms;estimation error;multi scale analysis;transforms edge detection exponential distribution gabor filters;exponential distribution multiscale analysis gabor transform adaptive edge detection technique error estimation gabor filter template adaptive threshold determining method	This paper proposes an adaptive edge detection technique based on multi-scale analysis of odd Gabor transform. In order to obtain the odd Gabor transform based edge response efficiently, an improved scheme is suggested, which can avoid the inverse influence brought by the estimation error of gradient direction on the construction of odd Gabor filter templates. To sharpen the edge response, an adjacent scales multiplication in odd Gabor transform domain is introduced. Moreover an adaptive threshold determining method related to the exponential distribution property of sharpened edge response is also presented. The experimental results on both synthetic and real world images show that our scheme is robust and has good edge detection performance	edge detection;gabor filter;gradient;synthetic intelligence;time complexity	Zhenfeng Zhu;Hanqing Lu;Yao Zhao	2006	First International Conference on Innovative Computing, Information and Control - Volume I (ICICIC'06)	10.1109/ICICIC.2006.312	exponential distribution;gabor transform;computer vision;s transform;edge detection;gabor–wigner transform;computer science;pattern recognition;mathematics;thresholding;gabor wavelet;statistics	Vision	54.12723502925567	-65.12921644199439	114632
f178150d230801e8bd5069cda107c270491cf9c5	evolutive parametric approach for specular correction in the dichromatic reflection model	luminosity correction;legendre polynomial;dichromatic reflection model;reflection model;diffuse;specular;evolution strategy;legendre polynomials;image modeling;region detection	Assuming the dichromatic image model we propose a global reduction of specularity effects by means of parametric illumination gradient images obtained by fitting 2D Legendre polynomials to the specular component of the images. Fitting is done applying a ( i¾? +  μ ) Evolution Strategy. The method could be applied to static robotic monitoring in teams of robots, where the illumination gradient image could be computed once and applied to successive frames until the illumination conditions change drastically. The method could be useful for the detection of image regions with different chromatic properties.		Ramón Moreno;Alicia D'Anjou;Manuel Graña	2008		10.1007/978-3-540-87656-4_82	computer vision;legendre polynomials;computer science	NLP	53.85018728022971	-55.94865100114137	114707
6705621de537ec5415880a42cd87822d63abd014	a novel optimal fuzzy system for color image enhancement using bacterial foraging	intensification;degradation;fuzzy systems color microorganisms image enhancement degradation entropy fuzzy logic genetic algorithms apertures shape;fuzzy contrast;underexposed image;contrast degradations color image enhancement optimal fuzzy system fuzzy logic technique image underexposed regions image overexposed regions original color composition parametric sigmoid function luminance component enhancement fuzzy contrast visual factors bacterial foraging algorithm triangular membership functions gaussian membership functions;fuzzifier;triangular membership functions;gaussian processes;visual factor contrast factor entropy exposure fuzzifier image enhancement intensification overexposed image underexposed image;image overexposed regions;color;hsv color space;bacterial foraging algorithm;visual factor;fuzzy set theory;parametric sigmoid function;fuzzy logic;objective function;visual factors;luminance component enhancement;image enhancement;contrast degradations;contrast factor;shape;image underexposed regions;image colour analysis;optimal fuzzy system;color image enhancement;membership function;genetic algorithm;genetic algorithms;overexposed image;entropy;original color composition;fuzzy logic technique;power law;image enhancement fuzzy logic fuzzy set theory gaussian processes image colour analysis;microorganisms;exposure;fuzzy systems;fuzzy system;apertures;gaussian membership functions;color image	A new approach is presented for the enhancement of color images using the fuzzy logic technique. An objective measure called exposure has been defined to provide an estimate of the underexposed and overexposed regions in the image. This measure serves as the dividing line between the underexposed and overexposed regions of the image. The hue, saturation, and intensity (HSV) color space is employed for the process of enhancement, where the hue component is preserved to keep the original color composition intact. A parametric sigmoid function is used for the enhancement of the luminance component of the underexposed image. A power-law operator is used to improve the overexposed region of the image, and the saturation component of HSV is changed through another power-law operator to recover the lost information in the overexposed region. Objective measures like fuzzy contrast and contrast and visual factors are defined to make the operators adaptive to the image characteristics. Entropy and the visual factors are involved in the objective function, which is optimized using the bacterial foraging algorithm to learn the parameters. Gaussian and triangular membership functions (MFs) are chosen for the underexposed and overexposed regions of the image, respectively. Separate MFs and operators for the two regions make the approach universal to all types of contrast degradations. This approach is applicable to a degraded image of mixed type. On comparison, this approach is found to be better than the genetic algorithm (GA)-based and entropy-based approaches.	color image;color space;fuzzy control system;fuzzy logic;genetic algorithm;image editing;loss function;optimization problem;sigmoid function	Madasu Hanmandlu;Om Prakash Verma;Nukala Krishna Kumar;Muralidhar Kulkarni	2009	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2009.2016371	computer vision;genetic algorithm;computer science;artificial intelligence;machine learning;mathematics;fuzzy control system	Vision	56.34975109080924	-64.15535818044059	114788
6d6d7c0f59a9687fe44af491741ad214f6b21c47	exposing photographic splicing by detecting the inconsistencies in shadows		As sophisticated photo editing software is increasingly available and the widespread use of multimedia social network, the reliability of digital images becomes more and more important. Photographic splicing, herein defined as a cut-and-paste of image regions from one image onto another image, is difficult to be detected due to the absence of a reference object. To carry out such forensic analysis, we present a novel shadow-based method, with which the fake shadow of the composites can be detected. We show how to estimate the shadow scale factors with a shadow removal technique and, further, how to estimate the growth rate of the penumbra width (GRPW). Inconsistencies in the shadows are then used as evidence of tampering. Compared with other shadow-based forensic methods, the proposed method can not only deal with the problem of shadow cloning in the same image, but also expose the fakery containing the real shadow, which benefit from the estimation of shadow scale factors and GRPW. Comparison results obtained from the splicing forgery detection database verify the ability of our approach.		Bin Yang;Xingming Sun;Xianyi Chen;Jianjun Zhang;Xu Li	2015	Comput. J.	10.1093/comjnl/bxu146	computer vision;computer graphics (images)	Theory	63.5866637914148	-58.16993963482542	114814
5cce1e4cb31417079a171687b7e29f8b6d7d0e4e	cross-scale reference-based light field super-resolution		Light fields suffer from a fundamental resolution tradeoff between the angular and the spatial domain. In this paper, we present a novel cross-scale light field super-resolution approach (up to  $\text{8}\times$ resolution gap) to super-resolve low-resolution (LR) light field images that are arranged around a high-resolution (HR) reference image. To bridge the enormous resolution gap between the cross-scale inputs, we introduce an intermediate view denoted as single image super-resolution (SISR) image, i.e., super-resolving LR input via single image based super-resolution scheme, which owns identical resolution as HR image yet lacks high-frequency details that SISR scheme cannot recover under such significant resolution gap. By treating the intermediate SISR image as the low-frequency part of our desired HR image, the remaining issue of recovering high-frequency components can be effectively solved by the proposed high-frequency compensation super-resolution (HCSR) method. Essentially, HCSR works by transferring as much as possible the high-frequency details from the HR reference view to the LR light field image views. Moreover, to solve the nontrivial warping problem that induced by the significant resolution gaps between the cross-scale inputs, we compute multiple disparity maps from the reference image to all the LR light field images, followed by a blending strategy to fuse for a refined disparity map; finally, a high-quality super-resolved light field can be obtained. The superiority of our proposed HCSR method is validated on extensive datasets including synthetic, real-world and challenging microscope scenes.	alpha compositing;angularjs;autostereogram;binocular disparity;frequency compensation;high-resolution scheme;image resolution;lr parser;light field;map;super-resolution imaging;synthetic intelligence	Mandan Zhao;Gaochang Wu;Yipeng Li;Xiangyang Hao;Lu Fang;Yebin Liu	2018	IEEE Transactions on Computational Imaging	10.1109/TCI.2018.2838457	artificial intelligence;iterative reconstruction;mathematics;computer vision;microscope;image warping;light field;microscopy;superresolution;image resolution	Vision	56.99927690859517	-57.167279692708306	115009
7017a7157e45bbc2e0203439401a8865e5a75ff1	fusion-based image de-fogging using dual tree complex wavelet transform		Images captured in foggy weather are severely degraded, which influences the tracking and recognition of objects present in those images. Therefore, restoring the true scene using a foggy image is important. In this paper, an effective fusion-based foggy image restoration technique by using dual tree complex wavelet transform (DT-CWT) has been proposed. Minimum color channel and the dark channel of a foggy image are constructed. Low and high pass components of both these channels are fused to obtain a transmission map. Dark channel is estimated by minimum preserving down sampling approach which improves the computational efficiency of the de-fogging process. Since DCP-based de-fogging techniques suffer from halo artifacts and darkness, proposed technique improves the overall contrast and the halo artifact regions in a time efficient way. To make the de-fogging results look uniformly bright, an adaptive post processing technique is applied on the de-fogged images. Comparative experiments with existing stat...	complex wavelet transform	Isha Kansal;Singara Singh Kasana	2018	IJWMIP	10.1142/S0219691318500546	mathematics;mathematical optimization;image restoration;high-pass filter;decimation;computer vision;fogging;artificial intelligence;channel (digital image);communication channel;complex wavelet transform	Vision	57.91066879852226	-60.96005081126396	115480
63285592fe2842fdbfa33b3d4c5802179a25f1d4	fast depth from defocus from focal stacks	shape from defocus;depth from defocus	We present a new depth from defocus method based on the assumption that a per pixel blur estimate (related with the circle of confusion), while ambiguous for a single image, behaves in a consistent way when applied over a focal stack of two or more images. This allows us to fit a simple analytical description of the circle of confusion to the different per pixel measures to obtain approximate depth values up to a scale. Our results are comparable to previous work while offering a faster and flexible pipeline.	approximation algorithm;autostereogram;box blur;focal (programming language);gaussian blur;glossary of computer graphics;pipeline (computing);pixel;the circle (file system)	Stephen W. Bailey;Jose I. Echevarria;Bobby Bodenheimer;Diego Gutierrez	2014	The Visual Computer	10.1007/s00371-014-1050-2	computer vision;computer science;mathematics;geometry	Vision	58.610921311473625	-52.67905331174114	115567
1c348b727e5fd946cb29490c9b8164399ca8e50a	new methods for digital halftoning and inverse halftoning	look up table;halftones;direct binary search;human visual system;displays;image quality;tree structure;algorithms;diffusion	Halftoning is the rendition of continuous-tone pictures on bi-level displays. Here we first review some of the halftoning algorithms which have a direct bearing on our paper and then describe some of the more recent advances in the field. Dot diffusion halftoning has the advantage of pixel-level parallelism, unlike the popular error diffusion halftoning method. We first review the dot diffusion algorithm and describe a recent method to improve its image quality by taking advantage of the Human Visual System function. Then we discuss the inverse halftoning problem: The reconstruction of a continuous tone image from its halftone. We briefly review the methods for inverse halftoning, and discuss the advantages of a recent algorithm, namely, the Look Up Table (LUT)Method. This method is extremely fast and achieves image quality comparable to that of the best known methods. It can be applied to any halftoning scheme. We then introduce LUT based halftoning and tree-structured LUT (TLUT)halftoning. We demonstrate how halftone image quality in between that of error diffusion and Direct Binary Search (DBS)can be achieved depending on the size of tree structure in TLUT algorithm while keeping the complexity of the algorithm much lower than that of DBS.© (2001) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.		Murat Mese;Palghat P. Vaidyanathan	2001		10.1117/12.452998	computer vision;computer science;theoretical computer science;error diffusion;computer graphics (images)	HCI	58.968383216327496	-63.72983350438323	115580
708e974aa28b70e8af0533d61bbfff2778493d42	depth from optical turbulence	optical distortion;image restoration optical turbulence hot surfaces desert terrains roads summer shimmering image distortion image blurring long range structure from motion nonhomogenous turbulence height varying turbulence wave propagation;high temperature;wave propagation image restoration optical distortion turbulence;image restoration;symposia;distortion;computational modeling;roads;optical properties;algorithms;refractive index;clear air turbulence;surface waves;cameras optical distortion computational modeling surface waves roads refractive index;wave propagation;outdoor;images;cameras;turbulence	Turbulence near hot surfaces such as desert terrains and roads during the summer, causes shimmering, distortion and blurring in images. While recent works have focused on image restoration, this paper explores what information about the scene can be extracted from the distortion caused by turbulence. Based on the physical model of wave propagation, we first study the relationship between the scene depth and the amount of distortion caused by homogenous turbulence. We then extend this relationship to more practical scenarios such as finite extent and height-varying turbulence, and present simple algorithms to estimate depth ordering, depth discontinuity and relative depth, from a sequence of short exposure images. In the case of general non-homogenous turbulence, we show that a statistical property of turbulence can be used to improve long-range structure-from-motion (or stereo). We demonstrate the accuracy of our methods in both laboratory and outdoor settings and conclude that turbulence (when present) can be a strong and useful depth cue.	algorithm;circuit restoration;depth perception;distortion;experiment;gradient;ibm notes;image restoration;microsoft research;reflections of signals on conducting lines;scene graph;software propagation;structure from motion;super-resolution imaging;turbulence	Yuandong Tian;Srinivasa G. Narasimhan;Alan J. Vannevel	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6247682	turbulence;image restoration;computer vision;distortion;wave propagation;surface wave;refractive index;computational model;clear-air turbulence	Vision	57.78796118568718	-59.467173213321814	115601
a4330bd1fb2a045468108a63e326a162505b41eb	real-time line scan extraction from infrared images using the wedge method in industrial environments	infrared cameras;surface temperature;real time;scanners;infrared imaging;feature extraction;hough transforms;temperature measurement;infrared;real time image processing;calibration;cameras;radiation;inverse problems	Infrared imaging is based on the measurement of radiation of an object and its conversion to temperature. A vital parameter of the conversion procedure is emissivity, which defines the capability of a material to absorb and radiate energy. For most applications, emissivity is assumed to be constant. In applications measuring the temperature of objects with high emissivity, this is not problematic, as slight variations in the chosen emissivity value cause only minor changes in the resulting surface temperatures. However, when emissivities are low, as in steel strips, considering emissivity as a constant can lead to significant errors in temperature measurement. To overcome problems generated by variations in emissivity, one solution is to measure temperature where the steel strip forms a wedge, acting as a cavity. In the deepest part of the wedge, emissivity is sufficiently close to one. This work presents a real time image processing system to acquire infrared line scans for steel strips using the wedge method. The proposed system confronts two challenges: extracting infrared line scans in real time from the deepest part of the wedge in rectangular infrared images, and translating pixels from the line scan into real-world units. © 2010 SPIE and IS&T. [DOI: 10.1117/1.3514741]	image processing;marathon trilogy;pixel;radiate;real-time clock;strips;timeline	Rubén Usamentiaga;Julio Molleda;Daniel F. García;Luis Pérez;Guillermo Vecino	2010	J. Electronic Imaging	10.1117/1.3514741	radiation;computer vision;calibration;infrared;temperature;feature extraction;temperature measurement;inverse problem	Robotics	60.47337865316784	-53.49021450800781	115653
6b5c3bc6efea98d504f3cedd84a2ea7cdf949773	a new framework for retinex-based colour image enhancement using particle swarm optimisation	wavelet energy;multiscale retinex;pso;msr;parameter tuning;particle swarm optimisation;colour image enhancement	A new approach for tuning the parameters of MultiScale Retinex (MSR) based color image enhancement algorithm using a popular optimization method, namely, Particle Swarm Optimization (PSO) is presented in this paper. The image enhancement using MSR scheme heavily depends on parameters such as Gaussian surround space constant, number of scales, gain and offset etc. Selection of these parameters, empirically and its application to MSR scheme to produce inevitable results are the major blemishes. The method presented here results in huge savings of computation time as well as improvement in the visual quality of an image, since the PSO exploited maximizes the MSR parameters. The objective of PSO is to validate the visual quality of the enhanced image iteratively using an effective objective criterion based on entropy and edge information of an image. The PSO method of parameter optimization of MSR scheme achieves a very good quality of reconstructed images, far better than that possible Copyright c © 2009 Inderscience Enterprises Ltd. ar X iv :1 40 9. 40 46 v1 [ cs .C V ] 1 4 Se p 20 14 2 M. C Hanumantharaju et al. with the other existing methods. Finally, the quality of the enhanced color images obtained by the proposed method are evaluated using novel metric, namely, Wavelet Energy (WE). The experimental results presented show that color images enhanced using the proposed scheme are clearer, more vivid and efficient.	algorithm;color image;computation;display resolution;entropy (information theory);image editing;matlab;mathematical optimization;multiscale modeling;particle swarm optimization;program optimization;time complexity;wavelet	M. C. Hanumantharaju;M. Ravishankar;D. R. Ramesh Babu;V. N. Manjunath Aradhya	2014	CoRR	10.1504/IJSI.2014.060241	computer vision;mathematical optimization;machine learning;mathematics	Vision	59.39690759506225	-65.37916768260747	115731
dd5543e1dafd3f3f0ca4a23e2d2e97b3ab05a240	optimal recovery of depth from defocused images using an mrf model	layout design for disassembly focusing cameras simulated annealing computational modeling lenses space technology testing stereo vision;simulated annealing;parameter estimation image reconstruction;image reconstruction;map estimation;parameter estimation;depth recovery focused image optimal recovery defocused images simulated annealing map estimates	A MAP-MRF based scheme is proposed for simultaneous recovery of the depth and the focused image of a scene from two defocused images. The space-variant blur parameter and the focused image of the scene are both modeled as MRFs and their MAP estimates are obtained using simulated annealing. The performance of the proposed scheme is tested on synthetic as well as real data and the estimates of the depth are found to be better than that of existing window-based techniques.	dhrystone;gaussian blur;markov random field;simulated annealing	A. N. Rajagopalan;Subhasis Chaudhuri	1998		10.1109/ICCV.1998.710846	iterative reconstruction;computer vision;mathematical optimization;simulated annealing;mathematics;estimation theory;statistics	Vision	57.33700547314337	-57.53035228179881	115806
d774a00aa63cde0070dfaacb7b55c243ce3ff0e1	a new image structural similarity metric based on k-l transform	会议论文	Recently, structural similarity image metric (SSIM) becomes the most popular model for image quality assessment (IQA). The idea behind SSIM is that natural images are highly structured, and estimate a general similarity of the image pairs from luminance, contrast and structure comparison. A novel similarity measure based on K-L transform is presented in this paper. It combines edge and texture components to provide a hierarchical description of image structure. We validate the performance of our algorithm with an extensive subjective study involving two sets of compressed images, the JPEG and the JPEG2000 images at the LIVE website. The experimental results show that the obtained quality metric had a high correlation with the subjective measure and outperforms SSIM.	structural similarity	Cheng Jiang;Fen Xiao;Xiaobo He	2014		10.1007/978-3-662-45643-9_17	structural similarity;image quality;luminance;similarity measure;jpeg;artificial intelligence;correlation;jpeg 2000;human visual system model;pattern recognition;mathematics	Vision	62.31753150966499	-64.43373520530601	115808
f10470815d78cca1499c2558faae13cdbb2bfb5a	depth from a single image through user interaction	depth cues;institute;image;init;viewing algorithms;picture image generation;imaging;i 4 8 image processing and computer vision;new;i 3 3 computer graphics;technologies;scene analysis	In this paper we present a method to obtain a depth map from a single image of a scene by exploiting both image content and user interaction. Assuming that regions with low gradients will have similar depth values, we formulate the problem as an optimization process across a graph, where pixels are considered as nodes and edges between neighbouring pixels are assigned weights based on the image gradient. Starting from a number of userdefined constraints, depth values are propagated between highly connected nodes i.e. with small gradients. Such constraints include, for example, depth equalities and inequalities between pairs of pixels, and may include some information about perspective. This framework provides a depth map of the scene, which is useful for a number of applications.	approximation algorithm;autostereogram;depth map;eurographics;gaussian blur;glossary of computer graphics;golem (ilp);gradient descent;high- and low-level;image gradient;interaction;international association of privacy professionals;mathematical optimization;pixel;social inequality;synthetic intelligence;tamagotchi;verve	Angeles López;Elena Garces;Diego Gutierrez	2014		10.2312/ceig.20141109	computer vision;feature detection;image-based modeling and rendering;image processing;computer science;digital image processing;real-time computer graphics;multimedia;automatic image annotation;computer graphics (images)	Vision	57.20377294193752	-56.091618369378544	115872
5655e1fedff7d35c2c148516f08e5d9b92dd9b88	the perceptual color space of digital image display terminals	color space;digital image	The geometric properties of the set of colors that can be displayed on the TV monitor of a digital image processing terminal are discussed in the framework of the Commission Internationale de l'Eclairage (CIE) 1976 (L* u* v*) color space. Quantitative results are presented for the HACIENDA image processing system. The use of lightness, chroma, and hue angle for the representation of multi-band images is briefly discussed.	color space;digital image	Antonio Santisteban	1983	IBM Journal of Research and Development	10.1147/rd.272.0127	color histogram;false color;rgb color model;computer vision;color model;color quantization;hsl and hsv;color image;binary image;image processing;computer science;digital image processing;color difference;multimedia;color balance;color space;digital image;computer graphics (images)	HCI	61.668953853241476	-61.097467670724356	115876
3409a1da6f3ee860a3a803fafd18e77a2097b089	angular variations of reflectance and fluorescence from paper - the influence of fluorescent whitening agents and fillers	annan fysik;other physics topics	It has earlier been shown that light reflected from the body of paper exhibit anisotropic behavior. On the other hand, fluorescence emission is often assumed to be distributed in a Lambertian manner. The angular behavior of light reflected and fluoresced from paper is examined using measurements from a spectral goniophotometer. The angular dependency of the radiance factors was measured for a range of excitation wavelengths. Moreover, the influence of fillers and fluorescent whitening agents (FWA) on the anisotropy was studied. The measurements show that the anisotropy of the total radiance factor of paper decreases when an increasing amount of FWA is added to the paper. The same effect was also observed when an increased amount of filler was added to the paper. In addition, it was shown that the presence of fillers reduce the effect of the FWA. The results show that in comparison to the anisotropy of the total radiance factor from the paper samples, the anisotropy of the fluorescence alone is negligible. Hence, for paper samples continues top of page 2 Technologies in Digital Photo Fulfillment: TDPF2013 Importance of Illumination Rendering Index in Image Capture and Printing Ron Kubara, Noritsu Koki Works Company Ltd. (Canada) Abstract: Traditionally, the color balance of illumination in Degrees Kelvin has been of significant importance due to color sensitivity of film. Today, digital cameras including camera phones have an automatic white balance setting that reduces the importance of illumination color balance over film cameras. However, with the relatively fast pace of change of fluorescent and now light emitting diode (LED) illumination including camera strobes, the quality of light measured by Rendering Index (Ra) may be of more importance than Color Balance (Kelvin). Combining low rendering index values of some artificial light sources with the large color gamuts of inkjet printers versus traditional silver halide printers, increases the effects of color inconsistency and metamerism. While there is no immanent solution, it is important for imaging professionals to be aware this growing concern. Traditionally, the color balance of illumination in Degrees Kelvin has been of significant importance due to color sensitivity of film. Today, digital cameras including camera phones have an automatic white balance setting that reduces the importance of illumination color balance over film cameras. However, with the relatively fast pace of change of fluorescent and now light emitting diode (LED) illumination including camera strobes, the quality of light measured by Rendering Index (Ra) may be of more importance than Color Balance (Kelvin). Combining low rendering index values of some artificial light sources with the large color gamuts of inkjet printers versus traditional silver halide printers, increases the effects of color inconsistency and metamerism. While there is no immanent solution, it is important for imaging professionals to be aware this growing concern. Including Videos in Photo Books Reiner Fageth, CEWE COLOR AG & Co. OHG (Germany) Abstract: This paper describes how videos can be implemented into printed photo books. More than half of the consumers take videos with DSCs, the other half with camcorders, smartphones and other devices. Therefore consumers making photo books are a great target group to offer a service implementing videos. Using the CEWE PHOTOBOOK software a consumer can select scenes (frames) of the video and have it printed together with a QR code in the product. After receiving the product, the QR code can be scanned with any smartphone or tablet and the movie will be displayed on the mobile device. continues bottom right of page 2 To view the full papers of these abstracts for no fee go to This paper describes how videos can be implemented into printed photo books. More than half of the consumers take videos with DSCs, the other half with camcorders, smartphones and other devices. Therefore consumers making photo books are a great target group to offer a service implementing videos. Using the CEWE PHOTOBOOK software a consumer can select scenes (frames) of the video and have it printed together with a QR code in the product. After receiving the product, the QR code can be scanned with any smartphone or tablet and the movie will be displayed on the mobile device. continues bottom right of page 2 To view the full papers of these abstracts for no fee go to www.imaging.org/ist/publications/reporter/index.cfm * These papers were presented at CIC20, held November 12-16, 2012, in Los Angeles, CA. To view the full papers of these abstracts for no fee go to www.imaging.org/ist/publications/reporter/index.cfm * These papers were presented at the International Symposium on Technologies in Digital Photo Fulfillment conference, held January 8-9, 2013, in Las Vegas, NV. INSIDE THIS ISSUE Highlighted Abstracts: CIC20 and TDPF 2013 1 CIC20 Report. . . . . . . . . . . . . . . . . . . . . . . . 3 TDPF 2013 Report . . . . . . . . . . . . . . . . . . . . 4 Standards Update. . . . . . . . . . . . . . . . . . . . . 6 Vol. 28, No. 1 January March 2013 HIGHLIGHTED PAPERS FROM RECENT CONFERENCES	angularjs;book;camera phone;color balance;decorrelation;digital camera;diode;goto;halide;lambertian reflectance;mobile device;nv network;printing;qr code;smartphone;star filler;tablet computer;vacuum fluorescent display	Niklas Johansson;Mattias Andersson	2012			photometric stereo;computer science;optoelectronics;bidirectional reflectance distribution function;optics;diffuse reflectance infrared fourier transform	Graphics	62.44796619328917	-60.50017217308434	116019
b8ad0abd266edb68f81700d96d43b5d442b1ab63	high-dynamic-range image reproduction methods	fuzzy reasoning;knowledge based systems;tone reproduction preprocessing algorithms;image processing;high dynamic range image reproduction;fuy reasoning;reproduction (copying);lighting;digital images;tone reproduction;segmentation;anchoring;image reproduction;high dynamic range images;high-dynamic-range images;lightness perception;image proce ssing;illumination	The high dynamic range of illumination may cause serious distortions and problems in the viewing and further processing of digital images. Important information can be hidden in the highly or extremely lowly illuminated parts. This paper deals with the reproduction of such images and introduces two new tone reproduction preprocessing algorithms which may help in developing the hardly viewable or nonviewable features and content of the images.	algorithm;computational complexity theory;digital image;distortion;high dynamic range;preprocessor;real-time clock	Annamária R. Várkonyi-Kóczy;András Rövid	2007	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2007.899923		Vision	59.07794018680196	-61.246714699710886	116207
5a46dc52eb0cd4e6770d11583df476870a0cb877	robust image and video dehazing with visual artifact suppression via gradient residual minimization	contrast enhancement;artifact suppression;video dehazing;image dehazing	MATLAB Code and more results: http://web.engr.illinois.edu/~cchen156/dehaze.html Image dehazing aims to remove the haze on the image and recover the original scene. Most existing image dehazing methods tend to boost local image contrast as well as the image artifacts. In this work, we propose a new method to jointly recover the haze-free image while explicitly minimizing possible visual artifacts.	gradient;matlab;visual artifact;zero suppression	Chen Chen;Minh N. Do;Jue Wang	2016		10.1007/978-3-319-46475-6_36	artificial intelligence;residual;computer science;computer vision;visual artifact;smoothing;minification;aliasing	Vision	56.777730257962354	-59.92008557434532	116366
b6b43bba82365d22bf73302ed021a14ff175bfb0	ultra-eye: uhd and hd images eye tracking dataset	high definition video visualization image resolution conferences multimedia communication tv tracking;visual perception gaze tracking high definition television object tracking;uhd images;human fixation;dataset eye tracking human fixation visual attention uhd images;eye tracking;visual attention;fixation points ultra eye uhd image eye tracking dataset ultrahigh definition displays uhd tv human visual perception viewing strategies human visual attention visual saliency uhd content 4k uhd images eye tracking information eye tracking experiments content selection fixation density maps;dataset	Due to the recent advances in ultra high definition (UHD) displays, UHD TV may replace HD TV in a near future. However, little is known about the effect of UHD content on human visual perception, specifically, on human visual attention, viewing strategies, and visual saliency. To help studying these properties of the human visual system and their dynamics when HD content is replaced with UHD content, a publicly accessible dataset is proposed, which is composed of 41 4K UHD and HD images with corresponding eye tracking information. The eye tracking information includes the fixation points and fixation density maps measured during extensive subjective experiments. In this paper, we describe the dataset in details, including the strategy for content selection, the eye-tracking experiments, and the computation of the fixation density maps.	computation;experiment;eye tracking;map	Hiromi Nemoto;Philippe Hanhart;Pavel Korshunov;Touradj Ebrahimi	2014	2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX)	10.1109/QoMEX.2014.6982284	computer vision;eye tracking;computer science;multimedia;data set;statistics;computer graphics (images)	Vision	64.24157167699346	-62.1806322335447	116405
41fd7d5754a955b9ddf2c203ddc36249f1564164	re-illuminating single images using albedo estimation	appearance;shape from shading;image processing;virtual reality;re illumination;parameter selection;albedo estimation;prediction	Predicting the appearance of a scene under novel lighting conditions is of growing interest at the convergence of vision, graphics and virtual reality. In this paper, we develop a method for appearance prediction from a single image using the apparatus of shape from shading (SFS). We re-visit the reflectance estimation process first proposed by Blake (Graphics Image Process. 32 (1985) 314), and develop a novel approach to parameter selection within the Blake method based on the quality of images which can be produced by re-illuminating the recovered needle-map. Combining Blake's method with recent advances in SFS is demonstrated to yield significant improvements in the appearance prediction of real images under varying lighting conditions.	autostereogram	Philip L. Worthington	2005	Pattern Recognition	10.1016/j.patcog.2004.11.015	computer vision;simulation;photometric stereo;prediction;image processing;computer science;virtual reality;statistics;computer graphics (images)	Vision	56.07445854579874	-53.77612104289586	116620
b08d905d0957dc5c72d99a0e6094d357af463bd9	on the estimation of spectral data: a genetic algorithm approach	identification problem;sensor systems;linear independence;image processing;predictability measure;light sensor sensitivity estimation;reflectivity;integral equations;least squares approximation;input signals;spectral data estimation;data engineering;image sensors;generalized cross validation;computer vision;image data;light sensor sensitivity estimation spectral data estimation genetic algorithm image data ill posed problem solid state light sensors input signals color signals linear independent equations identification problem extended generalized cross validation measure input data image processing computer vision predictability measure;solid state circuits;solid state lighting;a priori knowledge;prediction theory;linear independent equations;extended generalized cross validation measure;image colour analysis;input data;ill posed problem;image colour analysis spectral analysis genetic algorithms prediction theory optical sensors parameter estimation;genetic algorithm;genetic algorithms;informatics;optical sensors;parameter estimation;spectral analysis;gain measurement;genetic algorithms integral equations image sensors least squares approximation informatics reflectivity gain measurement data engineering solid state circuits sensor systems;color signals;solid state light sensors	Spectral data estimation from image data is an ill-posed problem since (i) due to the integral nature of solid-state light sensors the same output can be obtained from an infinity of input signals and (ii) color signals are spectrally smooth in nature and therefore limit the number of linear independent equation that can be formulated for the identification problem. To enable the solution of these problems most methods relay on exact a priori knowledge, such as smoothness and modality, to formulate hard constraints. In this paper a new method based on an extended generalized cross-validation measure is introduced for this type of problems. The solution is obtained with a genetic algorithm that maximizes its prediction ability. The method does not require exact a priori knowledge on the solution, since it is able to extract this information from the input data.	cross-validation (statistics);genetic algorithm;modality (human–computer interaction);relay;sensor;well-posed problem	Paulo Carvalho;Amâncio Santos;Bernardete Ribeiro;António Dourado	2001		10.1109/ICIP.2001.959183	computer vision;mathematical optimization;genetic algorithm;image processing;computer science;machine learning;mathematics;statistics	Robotics	66.55972366981716	-63.540241194336666	116651
dea60ed6d5a34b12700fa695b9e47835cb1b6bc4	effect of training and test datasets on image restoration and super-resolution by deep learning		Many papers have recently been published on image restoration and single-image super-resolution (SISR) using different deep neural network architectures, training methodology, and datasets. The standard approach for performance evaluation in these papers is to provide a single “average” mean-square error (MSE) and/or structural similarity index (SSIM) value over a test dataset. Since deep learning is data-driven, performance of the proposed methods depends on the size of the training and test sets as well as the variety and complexity of images in them. Furthermore, the performance varies across different images within the same test set. Hence, comparison of different architectures and training methods using a single average performance measure is difficult, especially when they are not using the same training and test sets. We propose new measures to characterize the variety and complexity of images in the training and test sets, and show that our proposed dataset complexity measures correlate well with the mean PSNR and SSIM values obtained on different test data sets. Hence, better characterization of performance of different methods is possible if the mean and variance of the MSE or SSIM over the test set as well as the size, resolution and complexity measures of the training and test sets are specified.	architecture as topic;artificial neural network;autostereogram;best, worst and average case;biological neural networks;circuit restoration;deep learning;image restoration;mean squared error;paper;peak signal-to-noise ratio;performance evaluation;sample variance;scientific publication;silo (dataset);structural similarity;super-resolution imaging;test data;test set	Ogun Kirmemis;A. Murat Tekalp	2018	2018 26th European Signal Processing Conference (EUSIPCO)	10.23919/EUSIPCO.2018.8552961	image restoration;deep learning;artificial neural network;structural similarity;superresolution;test data;artificial intelligence;image resolution;pattern recognition;test set;computer science	ML	62.77661735301683	-65.80554336929993	116953
21a7dbe8f6160384fcc665dfb36a70c3fbc2bf37	hue-preserving multiscale retinex for color images	image restoration;image color analysis color lighting channel estimation histograms image restoration approximation methods;image colour analysis;image restoration image colour analysis;hue preserving transformation enhancement multiscale retinex color image;natural image hue preserving multiscale retinex hue msr color image restoration process color image processing rgb color space hue preserving transformation color checker	In this paper, we propose a hue-preserving multi-scale retinex (MSR) for color images. In a color image restoration process, MSR is applied to each channel (i.e., red, green and blue) independently. Since MSR is included logarithm operation and local illumination of each channel is estimated independently, the hue of retinex output is different from the original (input) image. Hue preservation is necessary for color image processing. Distortion may occur if hue is not preserved. It is known that two operations, scaling and shifting, are hue-preserving operations in RGB color space. In the proposed method, we approximate the relationship between input pixel value and MSR output pixel value by the hue-preserving transformation form which is a combination of scaling and shifting. The efficiency of the proposed method is shown by using color checker and natural images.	approximation algorithm;circuit restoration;color image;color space;distortion;image processing;image restoration;image scaling;list of common shading algorithms;pixel	Kohichi Izumino;Yoshikatsu Hoshi;Akira Taguchi	2013	2013 International Symposium on Intelligent Signal Processing and Communication Systems	10.1109/ISPACS.2013.6704590	demosaicing;color histogram;rgb color model;computer vision;color model;hsl and hsv;color depth;color image;image gradient;binary image;geography;chromaticity;color balance;optics;color space;computer graphics (images)	Robotics	58.49043771939312	-61.96280154390913	117304
5a6722d8c18856414d2817dd148736522e188fab	scast: wireless video multicast scheme based on segmentation and softcast		"""Analog transmission schemes for wireless video multicast, especially SoftCast, have gained great attention recently. SoftCast can overcome the """"cliff effect"""" by delivering the linear transformations of video pixels, which is unavoidable for digital schemes. Considering the fact that different parts of image#x002F;video have diverse importances to the human perception, in this paper, a new analog video multicast scheme named SCAST is proposed. SCAST is based on SoftCast and the image segmentation technology, designed to improve the video subjective visual quality. Firstly, SCAST employs the Otsu segmentation algorithm to decompose the video source into the foreground part and the background part. Then a power allocation method is presented to provide strong protection for the foreground part, which is the region of interest (ROI) of video for the human visual system (HVS). The two parts are both encoded in the analog way by SoftCast and mapped into a complex signal before transmitted. Results show that the proposed scheme SCAST outperforms SoftCast, H.264#x002F;AVC and WSVC in both the metrics, i.e., peak signal-to-noise ratio (PSNR) and structural similarity (SSIM), which reflect the objective and subjective video qualities separately. Especially, SCAST performs excellently under bad channel condition."""	algorithm;analog transmission;cliff effect;human visual system model;image segmentation;multicast;otsu's method;peak signal-to-noise ratio;pixel;region of interest;simulation;structural similarity;video	Yuanyuan Li;Zhexin Li;Yu Liu;Yumei Wang	2017	2017 IEEE Wireless Communications and Networking Conference (WCNC)	10.1109/WCNC.2017.7925618	computer vision;telecommunications;computer science;multimedia;computer graphics (images)	Vision	57.18299429434384	-62.589077856198855	117464
cb8f1f317727c7f887f35ec8b5b11581de341711	just noticeable difference estimation for screen content images	electronic mail;image coding;measurement;visualization;image edge detection adaptation models visualization electronic mail image coding discrete cosine transforms measurement;image edge detection;discrete cosine transforms;parametric edge modeling just noticeable difference screen content image;adaptation models	We propose a novel just noticeable difference (JND) model for a screen content image (SCI). The distinct properties of the SCI result in different behaviors of the human visual system when viewing the textual content, which motivate us to employ a local parametric edge model with an adaptive representation of the edge profile in JND modeling. In particular, we decompose each edge profile into its luminance, contrast, and structure, and then evaluate the visibility threshold in different ways. The edge luminance adaptation, contrast masking, and structural distortion sensitivity are studied in subjective experiments, and the final JND model is established based on the edge profile reconstruction with tolerable variations. Extensive experiments are conducted to verify the proposed JND model, which confirm that it is accurate in predicting the JND profile, and outperforms the state-of-the-art schemes in terms of the distortion masking ability. Furthermore, we explore the applicability of the proposed JND model in the scenario of perceptually lossless SCI compression, and experimental results show that the proposed scheme can outperform the conventional JND guided compression schemes by providing better visual quality at the same coding bits.	acclimatization;algorithmic efficiency;behavior;body dysmorphic disorders;cns disorder;codec;computation (action);data compression;distortion;experiment;fda quality metrics terminology;inspiration function;just-noticeable difference;lossless compression;mathematical optimization;rate–distortion optimization;algorithm	Shiqi Wang;Yuming Fang;Weisi Lin;Siwei Ma;Wen Gao	2016	IEEE Transactions on Image Processing	10.1109/TIP.2016.2573597	computer vision;visualization;computer science;mathematics;multimedia;measurement;statistics;computer graphics (images)	Vision	62.421716261317435	-64.21413228097975	117783
df1bf6cdac33ae3b0ec28d21f18bc165b2cc195c	compression-based template matching	image coding;data compression;local centroids compression based template matching textual image compression image compression pattern comparison uncertainty context based compression model entropy model noise susceptibility registration errors quad tree decomposition;image sequences data compression image coding;template matching;working paper;image coding libraries pattern matching computer science optical character recognition software entropy context modeling pixel image reconstruction optical losses;image sequences	The first step is to extract the “marks,” or sets of connected black pixels, from the image. Marks correspond approximately to individual characters, except that characters with disconnected parts like “2’ and “j“ are represented by pairs of marks, one for the body and the other for the dot. (Due to the adaptive compression process that follows, this incurs a negligible penalty.) To build the library in the second step, the marks are clustered into self-similar categories. The intention is that all marks that represent a particular character fall into the same class. This effect is achieved by checking each new mark against the library and adding it as a new symbol if it is sufficiently different from all previous clusters. The final step compresses the image by coding the library symbols and then coding the sequence of symbols that correspond to the marks on the page. Because many marks in the image map to any one character, the mark that is stored in the library is an average of the original marks in the category. The use of a single representative introduces a small amount of noise in the reconstructed image, and an extra step can be used to make the compression lossless if desired. The result is an effective losslessflossy method for compressing textual images. A critical requirement in the second step is the ability to match marks accurately. This is really a process of pattern comparison, and is often called “template matching.” Template matching in textual image compression differs from the matching required in an optical character recognition (OCR) system because there is no need to identify the actual character to which any particular mark corresponds. Nevertheless, it is important to match each new occurrence of a mark consistently with the cluster to which it belongs, because compression will deteriorate if the matching operation introduces errors. Matching procedures generally work by aligning the two images in the position where we think the best match will be made, and then examining an error map, which is the bitwise exclusive-OR of the images. The most effective methods employ heuristics that are tailored	adaptive compression;approximation algorithm;bitwise operation;cross entropy;encode;exclusive or;heuristic (computer science);horner's method;image compression;image map;lossless compression;lossy compression;optical character recognition;pixel;self-similarity;string (computer science);template matching;unsupervised learning;virtual screening	Stuart Inglis;Ian H. Witten	1994		10.1109/DCC.1994.305918	data compression;lossy compression;color cell compression;computer vision;speech recognition;template matching;block truncation coding;image compression;computer science;pattern recognition;mathematics;lossless compression;context-adaptive binary arithmetic coding;texture compression;statistics	Vision	54.24350566228818	-59.20188331598964	117829
af3b4e3f78063a021c04c3783aa5001e5e6fe231	the fast multilevel fuzzy edge detection of blurry images	charge coupled image sensors;image thresholds;image edge detection pixel robot vision systems charge coupled image sensors cameras data mining mobile robots charge coupled devices vibrations image resolution;blurry images;image segmentation;vibrations;image resolution;image contrast enhancement;edge detection;blurry image;image restoration;mobile robots;image segmentation edge detection feature extraction fuzzy logic image enhancement image restoration;fmfed;data mining;charge coupled devices;fuzzy logic;image threshold;fast multilevel fuzzy edge detection algorithm;image enhancement;image edge detection;feature extraction;pixel;image thresholds blurry images edge detection fuzzy enhancement;fast multilevel fuzzy enhancement algorithm;robot vision systems;fuzzy enhancement;cameras;feature extraction fast multilevel fuzzy edge detection algorithm fmfed blurry image image contrast enhancement fast multilevel fuzzy enhancement algorithm fmfe image threshold;fmfe	To realize the fast and accurate detection of the edges from the blurry images, the fast multilevel fuzzy edge detection (FMFED) algorithm is proposed. The FMFED algorithm first enhances the image contrast by means of the fast multilevel fuzzy enhancement (FMFE) algorithm using the simple transformation function based on two image thresholds. Second, the edges are extracted from the enhanced image by the two-stage edge detection operator that identifies the edge candidates based on the local characteristics of the image and then determines the true edge pixels using the edge detection operator based on the extremum of the gradient values. Experimental results demonstrate that the FMFED algorithm can extract the thin edges and remove the false edges from the image, which leads to its better performance than the Sobel operator, Canny operator, traditional fuzzy edge detection algorithm, and other multilevel fuzzy edge detection algorithms	algorithm;canny edge detector;edge detection;gradient;grayscale;image editing;maxima and minima;pixel;sobel operator;software bug	Jinbo Wu;Zhou-Ping Yin;Youlun Xiong	2007	IEEE Signal Processing Letters	10.1109/LSP.2006.888087	fuzzy logic;mobile robot;image restoration;computer vision;edge detection;image resolution;image gradient;feature extraction;computer science;machine learning;vibration;deriche edge detector;pattern recognition;mathematics;image segmentation;canny edge detector;marr–hildreth algorithm;sobel operator;pixel	Vision	56.01319821301441	-64.84367125067753	117884
fe907b65c86513ac20aa90bda3c487594275cae2	temporal resolution vs. visual saliency in videos: analysis of gaze patterns and evaluation of saliency models	frame rate;saliency model;quality of experience;temporal scalability;eye tracking;perception	Temporal scalability of videos refers to the possibility of changing frame rate adaptively for efficient video transmission. Changing the frame rate may alter the spatial location that the viewers pay attention in the scene, which in turn significantly influences human's quality perception. Therefore, in order to effectively exploit the temporal scalability in applications, it is necessary to understand the relationship between frame rate variation and visual saliency. In this study, we answer the following three research questions: (1) Does the frame rate influence the overall gaze patterns (in an average sense over subjects)? (2) Does the frame rate influence the inter-subject variability of the gaze patterns? (3) Do the state-of-the-art saliency models predict human gaze patterns reliably for different frame rates? To answer the first two questions, we conduct an eye-tracking experiment. Under a free viewing scenario, we collect and analyze gaze-paths of human subjects watching high-definition (HD) videos having a normal or low frame rate. Our results show that both the average gaze-path and subject-wise variability of the gaze-path are influenced by frame rate variation. Then, we apply representative state-of-the-art saliency models to the videos and evaluate their performance by using the gaze pattern data collected from the eye-tracking experiment in order to answer the third question. It is shown that there exists a trade-off relation between accuracy in predicting the gaze pattern and robustness to frame rate variation, which raises necessity of further research in saliency modeling to simultaneously achieve both accuracy and robustness. HighlightsFirst, we study effects of video frame rate on visual attention via eye-tracking.We observe significant influence of video frame rate on average gaze patterns.We observe significant influence of frame rate on subject-wise changes in eye-paths.Second, visual saliency models are evaluated for videos with different frame rates.Trade-off of saliency prediction accuracy and consistency over frame rates is shown.		Manri Cheon;Jong-Seok Lee	2015	Sig. Proc.: Image Comm.	10.1016/j.image.2015.05.010	computer vision;simulation;eye tracking;computer science;multimedia;perception;frame rate	Vision	64.0400875376551	-63.35822310673416	118050
8b1e9eff7d92122adc1298dca6a93c79363fedfb	line sampling for direct illumination	i 3 7 computer graphics;ray tracing;three dimensional graphics and realism	Computing direct illumination efficiently is still a problem of major significance in computer graphics. The evaluation involves an integral over the surface areas of the light sources in the scene. Because this integral typically features many discontinuities, introduced by the visibility term and complex material functions, Monte Carlo integration is one of the only general techniques that can be used to compute the integral. In this paper, we propose to evaluate the direct illumination using line samples instead of point samples. A direct consequence of line sampling is that the two-dimensional integral over the area of the light source is reduced to a one-dimensional integral. We exploit this dimensional reduction by relying on the property that commonly used sampling patterns, such as stratified sampling and low-discrepancy sequences, converge faster when the dimension of the integration domain is reduced. We show that, while line sampling is generally more computationally intensive than point sampling, the variance of a line sample is smaller than that of a point sample, resulting in a higher order of convergence.	computer graphics;converge;discrepancy function;global illumination;low-discrepancy sequence;monte carlo integration;monte carlo method;nearest-neighbor interpolation;rate of convergence;sampling (signal processing);stratified sampling	Niels Billen;Philip Dutré	2016	Comput. Graph. Forum	10.1111/cgf.12948	ray tracing;computer vision;simulation;computer science;computer graphics (images)	Graphics	64.54025019350955	-53.445121810240266	118077
33bea63a259aece43c477c61d90f5e4711b1d3b6	purkinje images: conveying different content for different luminance adaptations in a single image	purkinje illusion;scotopic vision;photopic vision;perceptually based rendering;i 3 3 computer graphics picture image generation viewing algorithms	Providing multiple meanings in a single piece of art has always been intriguing to both artists and observers. We present Purkinje images, which have different interpretations depending on the luminance adaptation of the observer. Finding such images is an optimization that minimizes the sum of the distance to one reference image in photopic conditions and the distance to another reference image in scotopic conditions. To model the shift of image perception between day and night vision, we decompose the input images into a Laplacian pyramid. Distances under different observation conditions in this representation are independent between pyramid levels and pixel positions and become matrix multiplications. The optimal pixel colour can be found by inverting a small, per-pixel linear system in real time on a GPU. Finally, two user studies analyze our results in terms of the recognition performance and fidelity with respect to the reference images.		Sami Arpa;Tobias Ritschel;Karol Myszkowski;Tolga K. Çapin;Hans-Peter Seidel	2015	Comput. Graph. Forum	10.1111/cgf.12463	computer vision;mathematics;scotopic vision;photopic vision;computer graphics (images)	Vision	61.32323599267884	-60.75234258719	118189
91b16d5e7953617579de164ebaaf945e831afb97	a new approach to interactive viewpoint selection for volume data sets	harris interest point detection;viewpoint selection;principal component analysis	Automatic viewpoint selection algorithms try to optimize the view of a data set to best show its features. They are often based on information theoretic frameworks. Although many algorithms have shown useful results, they often take several seconds to produce a result because they render the scene from a variety of viewpoints and analyze the result. In this article, we propose a new algorithm for volume data sets that dramatically reduces the running time. Our entire algorithm takes less than a second, which allows it to be integrated into real-time volume-rendering applications. The interactive performance is achieved by solving a maximization problem with a small sample of the data set, instead of rendering it from a variety of directions. We compare performance results of our algorithm to state-of-the-art approaches and show that our algorithm achieves comparable results for the resulting viewpoints. Furthermore, we apply our algorithm to multichannel volume data sets.		Han Suk Kim;Didem Unat;Scott B. Baden;Jürgen P. Schulze	2013	Information Visualization	10.1177/1473871612467631	computer science;artificial intelligence;theoretical computer science;machine learning;data mining;statistics;principal component analysis	Visualization	67.8626543870748	-56.272231990352736	118228
c358e8f00165d777597a4ef72efe5a3a1df0fb2e	uniform color scale applications to computer graphics	computer graphic	People who are engaged with computerized image processing are often worried by computer-processed results that were not anticipated. These results often occur in color displayed images. The reason is that the color difference measured in red, green, and blue (RGB) space is not perceived linearly by human eyes. The uniform color scale is adopted as an approximation to the human visual system, On this basis, data conversions are proposed that enable the matching of computer image processing to human sensing. A pseudo-color selection method and optimal color TV display characteristics are also derived from the conversion formula between the RGB and Uniform Color Scale spaces.	computer graphics	Johji Tajima	1983	Computer Vision, Graphics, and Image Processing	10.1016/S0734-189X(83)80046-2	color gradient;color histogram;false color;rgb color model;computer vision;icc profile;color model;color quantization;hsl and hsv;color normalization;color depth;color image;computer science;rgb color space;high color;multimedia;color balance;color space;web colors;computer graphics (images)	Graphics	61.8475421764414	-60.6306330922571	118374
202097b666c19bec05b7a8331a13fa9675f3d866	super-resolution of images: algorithms, principles, performance	super resolution	A new facet of image restoration research has begun to emerge in recent years: super-resolution of images, which we define as the processing of an image so as to recover object information from beyond the spatial frequency bandwidth of the optical system that formed the image. Simple Fourier analysis would indicate that superresolution is not possible. Therefore, it is important to reconcile this simplistic view with the existing algorithms that have been demonstrated to achieve super-resolution. In this article, we consider some of the algorithms that have demonstrated super-resolution and discuss the common principles that they share which makes it possible for them to recover some of the lost bandwidth of the object. We also consider the question of super-resolution performance, which is the measure of how much lost bandwidth can be recovered from a super-resolution algorithm, and how the performance is related to the algorithm principles that allow super-resolution to occur. we conclude with examples of super-resolution.	algorithm;bandwidth (signal processing);circuit restoration;fourier analysis;image restoration;super smash bros.;super-resolution imaging	Bobby R. Hunt	1995	Int. J. Imaging Systems and Technology	10.1002/ima.1850060403	computer vision;simulation;computer science;artificial intelligence;superresolution	ML	60.317256561331426	-58.44166222093556	118466
3fbc6f6fbb8a7a6ac69455dbd52c3b8632682f3a	specularity removal using dark channel prior	chromaticity;specularity detection;dark channel prior;specular-to-diffuse mechanism;specularity removal	The reflectance of inhomogeneous objects can be described as a linear combination of diffuse and specular reflection components. Most computer vision algorithms assume that visually observable surfaces consist only of diffuse reflection. The existence of specular reflection can be misleading to these computer vision algorithms. A new algorithm  dark channel prior based specularity removal is proposed for separating specular and diffuse reflection components on colorful surfaces from a single input image. The dark channel prior is applied to detect the specular pixels in the image. The maximum diffuse chromaticity of the diffuse pixels is propagated to their neighboring specular pixels after specularity have been detected. Specularity removal can be achieved by using the specular-to-diffuse mechanism. The experimental results show that the proposed algorithm obtain comparable results as the state-of-the-art reflection components separation methods with the merit of being computationally more efficient.	algorithm;computer vision;diffuse reflection;observable;pixel;specularity	Beiji Zou;Xiaoyun Zhang;Shenghui Liao;Lei Wang	2013	J. Inf. Sci. Eng.		computer vision;diffuse reflection	Vision	56.45635187290966	-59.21657032673108	118484
eed59983e6d93ab0565d474e9cc73cab19426e72	depth estimation and inpainting with an unconstrained camera	depth estimation	Unrestricted camera motion and the ability to operate over a range of lens parameters are often desirable when using an off-the-shelf camera. Variations in intrinsic and extrinsic parameters induce defocus and pixel motion, both of which relate to scene structure. We propose a depth estimation approach by elegantly coupling the motion and defocus cues. We further advocate a natural extension of our framework for inpainting both depth and image, using the motion cue. Unlike traditional inpainting, our approach also considers defocus blur. This ensures that the image inpainting is coherent with respect to defocus. We use the belief propagation method in our estimation approach, which also handles occlusions and uses the color image segmentation cue.	belief propagation;camera resectioning;coherence (physics);color depth;color image;emoticon;gaussian blur;image segmentation;inpainting;parallax;pixel;software propagation;stationary process	Arnav V. Bhavsar;A. N. Rajagopalan	2010		10.5244/C.24.84	computer vision;computer science;mathematics;computer graphics (images)	Vision	55.422561426798964	-53.1458366923491	118622
130cbf143d804ae1c5794d4023ae72a401e51510	locating the source of topological error in reconstructed 3d models	image tridimensionnelle;4230;image processing;modelo 3 dimensiones;0705;selected works;modele 3 dimensions;0130c;imagerie;three dimensional model;laser scanner;parameterization;surface reconstruction;traitement image;parametrizacion;reconstruction image;reconstruction surface;imagery;3d model;4230w;image reconstruction;laser scanners;tridimensional image;bepress;etalonnage;imagineria;scanning;4230v;calibration;parametrisation;imagen tridimensional	Although range scanning technology has offered great improvements to digital model creation in recent years, it has also introduced some new concerns. Specifically, recent work shows that topological errors such as tiny handles can significantly lower the overall quality of range-scanned models for down-stream applications (such as simplification and parameterization). In this paper we present our investigation into the source of this topological error in the range scanning process, and our methods to alleviate the error. We concentrated our investigation of the scanning process on: (1) signal noise or calibration error in the laser scanner (resulting in bad data points) and (2) error during the model reconstruction phase. We found that by modifying the surface reconstruction phase of the range scanning process, we were able to reduce the amount of topological noise in the resulting 3D model by up to 60 percent.	3d modeling;3d scanner;data point;level of detail;noise (electronics)	Eric Firestone;Craig Povey;Zoë J. Wood	2008		10.1117/12.766863	parametrization;computer vision;calibration;simulation;surface reconstruction;image processing;optics	Graphics	57.7670377662868	-52.38623664681345	118846
5b22162d8436ca86e7c37be989e124dc0aa8a4e7	perceptual video quality assessment and analysis using adaptive content dynamics				Mohammed A. Aabed	2017			pevq;video quality;perception;distortion;subjective video quality;computer science;multimedia;computer vision;artificial intelligence	Vision	63.06345884991737	-63.344285118954154	119039
0d426de0f9a6761336243c79f1ede8fdc13d3131	multiscale algorithm for reconstructing videos from streaming compressive measurements	video	We propose a multiscale, iterative algorithm for reconstructing video signals from streaming compressive measurements. Our algorithm is based on the observation that, at the imaging sensor, many videos should have limited temporal bandwidth due to the spatial lowpass filtering that is inherent in typical imaging systems. Under modest assumptions about the motion of objects in the scene, this spatial filtering prevents the temporal complexity of the video from being arbitrarily high. Thus, even though streaming measurement systems may measure a video thousands of times per second, we propose an algorithm that only involves reconstructing a much lower rate stream of “anchor frames.” Our analysis of the temporal complexity of videos reveals an interesting tradeoff between the spatial resolution of the camera, the speed of any moving objects, and the temporal bandwidth of the video. We leverage this tradeoff in proposing a multiscale reconstruction algorithm that alternates between video reconstruction and motion estimation as it produces finer resolution estimates of the video.	algorithm;image sensor;iterative method;low-pass filter;motion estimation;system of measurement	Jae Young Park;Michael B. Wakin	2013	J. Electronic Imaging	10.1117/1.JEI.22.2.021001	computer vision;video;computer science;theoretical computer science;computer graphics (images)	Vision	59.64238257288126	-56.747273325861705	119117
5d584c402e2ac1c47df3c8be0734db4f102fba5c	dense texture flow visualization using data-parallel primitives				Mark Kim;Scott Klasky;David Pugmire	2018		10.2312/pgv.20181095	theoretical computer science;computer science;flow visualization	Visualization	68.13875303146867	-52.74729800891654	119271
2e9d3b34fb18ef5b80a646f15b48e71aa510a33c	adaptive dynamic range imaging: optical control of pixel exposures over space and time	motion analysis;image recognition;optical control;object recognition;controllable liquid crystal light modulator;video surveillance;high resolution;image formation;image resolution;scene dynamics;computer graphics;real time control;optical control computer vision image recognition image resolution motion estimation object detection cameras computer graphics brightness;rate adaptation;optical domain;brightness values;adaptive dynamics;motion estimation;effective optical transmittance;spatial light modulator;scene point;detector irradiance;video monitoring;color ccd detector;controllable optical attenuator;computer vision;radiance value;digital cameras;brightness;navigation;video monitoring adaptive dynamic range imaging optical control pixel exposures radiance value scene point optical domain image formation spatial light modulator real time control algorithm transmittance function scene radiance video rate adaptive dynamic range camera color ccd detector controllable liquid crystal light modulator tracking navigation brightness values computer vision color channel computer graphics digital cameras motion estimation object recognition multiple image detectors controllable optical attenuator detector irradiance effective optical transmittance spatiotemporal attenuator control scene dynamics attenuator blurring automatic gain control lcd attenuator video surveillance;video rate adaptive dynamic range camera;range image;machine vision;adaptive dynamic range imaging;dynamic range;real time control algorithm;pixel exposures;scene radiance;color channel;attenuator blurring;liquid crystal;programmable control adaptive control dynamic range optical imaging optical control pixel optical modulation layout cameras detectors;lcd attenuator;cameras;tracking;object detection;spatiotemporal attenuator control;automatic gain control;transmittance function;multiple image detectors	This paper presents a new approach to imaging that significantly enhances the dynamic range of a camera. The key idea is to adapt the exposure of each pixel on the image detector, based on the radiance value of the corresponding scene point. This adaptation is done in the optical domain, that is, during image formation. In practice, this is achieved using a spatial light modulator whose transmittance can be varied with high resolution over space and time. A real-time control algorithm is developed that uses acquired images to automatically adjust the transmittance function of the spatial modulator. Each captured image and its corresponding transmittance function are used to compute a very high dynamic range image that is linear in scene radiance. We have implemented a video-rate adaptive dynamic range camera that consists of a color CCD detector and a controllable liquid crystal light modulator. Experiments have been conducted in scenarios with complex and harsh lighting conditions. The results indicate that adaptive imaging can have a significant impact on vision applications such as monitoring, tracking, recognition, and navigation. 1 High Dynamic Range Imaging A typical real-world scene has an enormous range of brightness (radiance) values. Digital cameras used in computer vision and computer graphics generally provide 8 bits of brightness information per color channel, at each pixel. That is, within each color channel, all radiance values in the scene are mapped to one of 256 image brightness values, irrespective of the complexity of the scene. Computer vision problems, such as the perception of shape, the estimation of motion, and the recognition of objects, are often under-constrained and thus inherently hard to solve. The low quality of image data produced by today’s cameras makes each of these problems even harder. This fact is now widely accepted; the problem of acquiring “high dynamic range” (HDR) images has drawn the attention of researchers in vision and graphics. This paper presents a novel approach to HDR imaging, called adaptive imaging, that produces unprecedented dynamic ranges. Before presenting this approach, a quick review of previous work is in order. All of the previous methods seek to improve the dynamic range of an imaging system either by trading-off temporal or spatial resolution of the image detector, or by using multiple detectors. Temporal Exposure Change: The most popular approach to HDR imaging is to sequentially capture multiple images of the scene using different exposures (for examples, see [7], [3], [11], [22], [18].) Low exposures provide useful data in bright scene regions and high exposures in dark scene regions. Therefore, the acquired images can be fused to obtain a single high quality image. In [12], [4], [13], and [17], this approach is taken a step further; the radiometric response of the imaging system is also recovered from the acquired images. The above methods are of course suited only to static scenes; the imaging system, the scene objects, and their radiances must remain constant during the sequential capture of images under different exposures. Mosaicing with Spatially Varying Filter: Recently, the concept of generalized mosaicing [19], [1] was introduced where a spatially varying neutral density filter is rigidly attached to the camera. When this imaging system is rotated, each scene point is observed under different exposures. The captured images are fused to obtain a wide-angle mosaic that has very high dynamic range. As with temporal exposure change, the disadvantage here is that the scene must remain constant while the imaging system rotates. Multiple Image Detectors: The scene/camera motion restrictions faced by the above methods can be remedied by using multiple imaging systems. This approach has been taken by several investigators (for examples, see [5], [18]). Beam splitters are used to generate multiple copies of the optical image of the scene. Each copy is measured by an image detector whose exposure is preset by using an optical attenuator or by adjusting the exposure time of the detector. This approach has the advantage of producing HDR images in real time. The disadvantage is that it is more expensive as it requires multiple detectors and precision optics for good alignment between the acquired images. Multiple Sensor Elements Within a Pixel: A rather novel approach to HDR imaging uses a custom 1 Proceedings of the Ninth IEEE International Conference on Computer Vision (ICCV 2003) 2-Volume Set 0-7695-1950-4/03 $17.00 © 2003 IEEE detector (see [8] and [9] for examples) where each detector cell includes two sensing elements (potential wells) of different sizes (and hence sensitivities). When the detector is exposed to the scene, two measurements are made within each cell and they are combined on-chip before the image is read out. This approach is expensive as it requires a sophisticated detector to be fabricated. Furthermore, spatial resolution is reduced by a factor of two since the two potential wells take up roughly the same space as two pixels in a conventional detector. Spatially Varying Pixel Exposures: In [15], the notion of spatially varying pixel sensitivities was introduced. The idea is to assign different (fixed) exposures to neighboring pixels on the image detector. When a pixel is saturated in the acquired image it is likely to have a neighbor that is not, and when a pixel produces zero brightness it is likely to have a neighbor that produces non-zero brightness. In short, there are no large “holes” of saturation or darkness in the acquired image and an HDR image can be reconstructed from it. In this case, one is really trading-off spatial resolution for brightness resolution. Smart Pixels: In [2], a novel solid state image sensor is described where each pixel on the device includes a computational element that measures the time it takes to attain full potential well capacity. Since the full-well capacity is the same for all pixels, the time to achieve it is proportional to scene radiance. This approach is attractive but faces the challenge of scaling to high spatial resolution while keeping fabrication costs under control. In this category, also lie other custom detectors that are designed to adjust the exposure of each pixel (or a set of pixels) independently (see [6], [20]). As we shall see shortly, the development of such devices is really complementary to the adaptive imaging technique we propose in this paper. 2 Adaptive Dynamic Range Imaging We now present the basic principles underlying adaptive dynamic range (ADR) imaging and show why it can achieve significant improvements in dynamic range over the existing HDR methods described in section 1. Then, we will describe a few different optical configurations that can be used to implement such an imaging system. 2.1 Pixels with Adaptive Exposures We use the term adaptive “imaging” rather than “detector” or “sensor” as the adaptation we propose takes place in optical domain, that is, during image formation. The effect of the adaptation is a change in the incident irradiance of each pixel, based on the scene 1Note that this is different from automatic gain control (AGC) or auto-exposure control (AEC) features found in commercial detector element attenuator element	algorithm;automatic gain control;channel (digital image);charge-coupled device;computer graphics;computer vision;display resolution;high dynamic range;high-dynamic-range imaging;iccv;image formation;image resolution;image scaling;image sensor;metric;modulation;ncsa mosaic;optical attenuator;pixel;potential well;range imaging;real-time locating system;solid-state drive;spatial light modulator;transmission coefficient	Shree K. Nayar;Vlad Branzoi	2003		10.1109/ICCV.2003.1238624	computer vision;image resolution;machine vision;computer science;computer graphics (images)	Vision	61.711749639058894	-55.93361420735043	119298
6be30227f72718b218d21772b159ea2c29bd7a87	camouflage texture evaluation using a saliency map	detection probability;saliency detection;camouflage evaluation;focus of attention	Camouflage effect evaluation and examination is an important procedure in digital camouflage pattern design as it is helpful in improving the objectivity and effectiveness thereof. Based on the human vision mechanism, we propose a new method, which creates a saliency map of the input image that can quantitatively evaluate the degree to which the target and surrounding background differ with respect to color brightness and space distribution. Then, we use this saliency map to evaluate the effect of camouflage design. Experimental results demonstrate that the saliency map is an effective approach for evaluating camouflage design.	artificial neural network;objectivity/db	Feng Xue;Cui Guoying;Richang Hong;Jing Gu	2014	Multimedia Systems	10.1007/s00530-014-0368-y	computer vision;simulation	AI	60.468905523926765	-62.885466414990184	119494
2e51c49636d03b958ee02db80d2b4f1a765a00ce	fast bilateral filter for hdr imaging	tone mapping;image processing;dynamic range reduction;acceleration;approximation;hdr;human visual system;bilateral filter	Bilateral filtering is a method often used in image processing applications. It is specifically useful for HDR algorithms. A novel approach to a fast and close approximation of bilateral filtering is presented. The method is designed especially with a focus on HDR image conversion into a normal color space processing. This paper presents the methods itself, describes the sources of acceleration and discusses the results of the method.		Michal Seeman;Pavel Zemcík;Roman Juránek;Adam Herout	2012	J. Visual Communication and Image Representation	10.1016/j.jvcir.2011.07.012	acceleration;computer vision;tone mapping;image processing;computer science;approximation;bilateral filter;human visual system model;computer graphics (images)	Vision	59.53360199326328	-60.488026275276845	119583
2fc261a2f3b261926c8320a54370f5b5f9184f15	on optical phase shift profilometry based on dual tree complex wavelet transform	image reconstruction noise measurement noise wavelet coefficients solid modeling;profilometry;phase unwrapping dual tree complex wavelet transform profilometry;digital camera;phase shift;dual tree complex wavelet transform;trees mathematics;complex wavelet transform;noise measurement;oriented 2d dual tree complex wavelet transform;noisy fringe images;wavelet transforms;noisy fringe images optical phase shift profilometry parallel fringe patterns digital camera 3d shape reconstruction quality oriented 2d dual tree complex wavelet transform;image reconstruction;solid modeling;optical phase shift profilometry;parallel fringe patterns;3d shape reconstruction quality;real time application;wavelet coefficients;cameras;wavelet transforms cameras image reconstruction trees mathematics;noise;phase unwrapping	In optical phase shift profilometry, parallel fringe patterns are projected onto an object and the deformed fringes are captured using a digital camera. It is of particular interest because it enables reconstruction of the 3D shape of the object using just a few image captures, which facilitates real time applications. However, when using the approach in real life environment, it is noticed that the noise in the captured images can greatly affect the reconstruction quality. In this paper, we firstly analyze why the noisy fringe images can best be analyzed using the oriented 2D dual tree complex wavelet transform. We then suggest an effective yet simple method for enhancing the noisy fringe images. Both the simulation and experiment results show that the new approach can give good performance in reconstruction with fringe images even at high noise level.	algorithm;complex wavelet transform;digital camera;noise (electronics);real life;simulation	Tai-Chiu Hsung;Daniel Pak-Kong Lun	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5653341	iterative reconstruction;computer vision;profilometer;noise measurement;noise;mathematics;phase;solid modeling;wavelet transform;computer graphics (images)	Robotics	57.90843421354488	-57.34767155451425	119599
580ff8be471e0589aa4109a33733fae233ba0f7a	a maximum flow approach to the volumetric reconstruction problem		We present a 3D reconstruction technique based on the maximum-flow formulation. Starting with a set of calibrated images, we globally search for the most probable 3D model given the photoconsistency and the spatial continuity constraints. This search is done radially from the center of the reconstruction volume; therefore imposing a radial topology. The fact that cameras are arbitrarily positioned around the scene presents challenges for managing occlusion, especially when applying global smoothing. We solve this problem by proposing an iterative occlusion management mechanism, and a new way of looking at surface smoothing and discontinuities that takes photoconsistency into account. Experiments show that our method is relatively fast and robust when dealing with simple objects, even in noisy conditions.	3d reconstruction;algorithm;experiment;hidden surface determination;iteration;maximum flow problem;minimum cut;radial (radio);reconstruction conjecture;reflections of signals on conducting lines;robustness (computer science);scott continuity;smoothing;voxel	Catherine Proulx;Sébastien Roy	2005		10.5244/C.19.14	computer vision;mathematical optimization;mathematics;geometry	Vision	56.840197080619056	-52.17874860756565	119656
ae8ea832cf78bb27d97fbcbb77973fcac289c823	curvature consistency improves local shading analysis	local shading analysis;curvature consistency;shape;geometry;noise shaping;surface reconstruction;differential geometry;image analysis;robustness;image reconstruction;computer vision	This paper describes how a surface reconstruction algorithm, based on minimizing the variation of surface curvature, can be used to stabilize and correct the results of local shading analysis. What is novel about this approach is that it is viewpoint independent and applicable to any process that can provide estimates of local surface orientation. The assumptions used in formulating the minimization are derived from standard differential geometry. When applied as a second stage of processing after local shading analysis, the algorithm can recover a close approximation of the true surface orientation under realistic assumptions about image noise. Results are presented that show the performance of the algorithm on synthetic and real data. In particular, they demonstrate how this form of reconstruction can compensate for some of the shape distortion incurred in local shading analysis. e MZ	algorithm;approximation;dhrystone;distortion;image noise;shading;sharp mz	Frank P. Ferrie;Jean Lagarde	1992	CVGIP: Image Understanding	10.1016/1049-9660(92)90009-R	computer vision;mathematical optimization;mathematics;geometry	Vision	54.67662448734963	-55.93384325329828	119696
7484f55c80b584d4f2922d912b363d202d81bef0	multi-foveation filtering	image coding;image processing;image coding filtering theory;multi viewer eye sensitivity;contrast sensitivity;multi foveation filtering;foveated image processing;cut off frequency map;video coding;sensitivity;multiresolution contrast sensitivity foveated image processing foveation foveation filtering;first order;foveation;pixel;cutoff frequency image coding filtering spatial resolution programmable control telecommunication computing computed tomography multimedia computing computer vision costs;multiresolution;approximation methods;foveation filtering;cut off frequency map multi foveation filtering multi viewer eye sensitivity;encoding;cutoff frequency;filtering theory;real time systems	We present a method for computing a function of average multi-viewer eye sensitivity based on the Geisler & Perry contrast threshold formula, and, from this, the cut-off frequency map (as used in foveation filtering) that is optimal in the sense of discarding frequencies in least-noticeable-first order. Existing approaches usually solve the multi-viewer foveation problem as a number of single-viewer foveations, effectively taking collective sensitivity to be the maximum of the individual viewer eye sensitivities. This has inherent problems such as over-sensitivity to outliers which are not problems with the proposed approach. Furthermore, the proposed approach can be employed in the infinite-viewer (probability-based) scenario without additional cost.	texture filtering	Tim Popkin;Andrea Cavallaro;David Hands	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4959672	computer vision;image processing;sensitivity;computer science;cutoff frequency;first-order logic;pixel;encoding;computer graphics (images)	Robotics	60.26635143938876	-59.77471388699406	119835
12726a48683b6feb34d86b3be447bf3b1d1e8be1	"""comparison between an hvs inspired linear filter and the bilateral filter in performing """"vision at a glance"""" through smoothing with edge preservation"""	receptive field;spatial filter;bilateral filter;preprocessing;vision at a glance	"""We propose that the Magno (M)-channel filter, belonging to the extended classical receptive field (ECRF) model, provides us with """"vision at a glance"""", by performing smoothing with edge preservation. We compare the performance of the M-channel filter with the well-known bilateral filter in achieving such """"vision at a glance"""" which is akin to image preprocessing in the computer vision domain. We find that at higher noise levels, the M-channel filter performs better than the bilateral filter in terms of reducing noise while preserving edge details. The M-channel filter is also significantly simpler and therefore faster than the bilateral filter. Overall, the M-channel filter enables us to model, simulate and arrive at a better understanding of some of the initial mechanisms in visual pathway, while simultaneously providing a fast, biologically inspired algorithm for digital image preprocessing."""	bilateral filter;human visual system model;smoothing	Debjyoti Bhattacharjee;Ashish Bakshi;Kuntal Ghosh	2015	Int. J. Image Graphics	10.1142/S0219467815500151	edge-preserving smoothing;computer vision;simulation;kernel adaptive filter;computer science;machine learning;bilateral filter;preprocessor;receptive field;spatial filter;computer graphics (images)	Vision	56.18276837048402	-63.32183199469947	119900
733ac95079e6331eee2d21009e019fd9ea048a8a	local quasi-monte carlo exploration		In physically-based image synthesis, the path space of light transport paths is usually explored by stochastic sampling. The two main families of algorithms are Monte Carlo/quasi-Monte Carlo sampling and Markov chain Monte Carlo. While the former is known for good uniform discovery of important regions, the latter facilitates efficient exploration of local effects. We introduce a hybrid sampling technique which uses quasi-Monte Carlo points to achieve good stratification in both stages: we use the Halton sequence to generate initial seed paths and rank-1 lattices for local exploration. This method avoids the hard problem of introducing QMC sequences into MCMC while still stratifying samples both globally and locally. We propose perturbation strategies that prefer dimensions close to the camera, facilitating efficient reuse of transport path suffixes. This framework provides maximum control of the sampling scheme by the programmer, which can be hard to achieve with Markov chain-based methods. We show that local QMC exploration can generate results on par with state of the art light transport sampling methods, while providing more uniform convergence, improving temporal consistency.	algorithm;crystal structure;dimensions;eurographics;lambda lifting;markov chain monte carlo;monte carlo method;numerous;petrosal sinus sampling;programmer device component;quantum monte carlo;quasi-monte carlo method;rendering (computer graphics);reuse (action);sampling (signal processing);sampling - surgical action;stratification;video for windows	Lorenzo Tessari;Johannes Hanika;Carsten Dachsbacher	2017		10.2312/sre.20171196	monte carlo method;monte carlo integration;quasi-monte carlo method;halton sequence;mathematical optimization;hybrid monte carlo;markov chain monte carlo;rejection sampling;computer science;markov chain	ML	63.947355692062246	-52.93099126078818	120247
e88f9d928dc27ba6d408b1ac65702717e8f4e815	robust and fast motion estimation for video completion		A motion estimation method for completing a video with large and consecutive damage is introduced. It is principally based on sparse matching and interpolation. First, SIFT, which is robust to arbitrary motion, is used to efficiently obtain sparse correspondences in neighboring frames. To ensure these correspondences are uniformly distributed across the image, a fast dense point sampling method is applied. Then, a dense motion field is generated by interpolating the correspondences. An efficient weighted explicit polynomial fitting method is proposed to achieve spatially and temporally coherent interpolation. In the experiment, quantitative measurements were conducted to show the robustness and effectiveness of the proposed method.	coherence (physics);motion estimation;motion field;motion interpolation;nearest-neighbor interpolation;polynomial;sampling (signal processing);scale-invariant feature transform;sparse matrix	Shaodi You;Robby T. Tan;Rei Kawakami;Katsushi Ikeuchi	2013			computer vision;mathematical optimization;discrete mathematics;quarter-pixel motion;motion estimation;mathematics	Vision	54.997938611482674	-55.01943698848773	120257
0d2b55460f48cbcdecea656efc5dc345356e635e	intellectual property-based lossless image compression for camera systems [hardware matters]	image coding;intellectual property;computed tomography;cameras;distance measurement;wavelet coefficients	This article presents a novel framework for intellectual property (IP)- based lossless image compression used for camera systems. The proposed approach presents two subframeworks: a) two-dimensional (2-D) Haar wavelet transformation (HWT)- based forward pixel calculator IP for image compression and b) 2-D HWTbased inverse pixel calculator IP for image decompression. Each framework is capable of fully compressing and decompressing images through onestage computation during forward and inverse transformations. The proposed approach has been tested on images of critical data sets of medical applications [e.g., computed tomography (CT) images and satellite applications, including NASA images]. Results of compressed and decompressed images along with compression efficiency through the proposed framework for IP-based lossless image compression are also reported in this article.	autonomous robot;ct scan;capacitor plague;computation;data compression;emi;earthbound;emoticon;haar wavelet;image compression;lossless compression;my girlfriend is a cyborg;pixel;serdes;tomography;wavelet transform	Anirban Sengupta;Dipanjan Roy	2018	IEEE Consumer Electronics Magazine	10.1109/MCE.2017.2743239	image compression;calculator;computed tomography;multimedia;computation;pixel;computer science;computer vision;data set;haar wavelet;artificial intelligence;lossless compression	Vision	66.35178019593437	-61.05503361543134	120289
37ddbe8e65476e7e282d1e87a3fd16a1adf2e57e	a framework for transient rendering	transient light transport;bidirectional path tracing;importance sampling;progressive photon mapping;transient rendering	Recent advances in ultra-fast imaging have triggered many promising applications in graphics and vision, such as capturing transparent objects, estimating hidden geometry and materials, or visualizing light in motion. There is, however, very little work regarding the effective simulation and analysis of transient light transport, where the speed of light can no longer be considered infinite. We first introduce the transient path integral framework, formally describing light transport in transient state. We then analyze the difficulties arising when considering the light's time-of-flight in the simulation (rendering) of images and videos. We propose a novel density estimation technique that allows reusing sampled paths to reconstruct time-resolved radiance, and devise new sampling strategies that take into account the distribution of radiance along time in participating media. We then efficiently simulate time-resolved phenomena (such as caustic propagation, fluorescence or temporal chromatic dispersion), which can help design future ultra-fast imaging devices using an analysis-by-synthesis approach, as well as to achieve a better understanding of the nature of light transport.	graphics;light transport theory;path integral formulation;sampling (signal processing);simulation;software propagation;speech coding;transient state	Adrian Jarabo;Julio Marco;Adolfo Muñoz;Raul Buisan;Wojciech Jarosz;Diego Gutierrez	2014	ACM Trans. Graph.	10.1145/2661229.2661251	computer vision;importance sampling;mathematics;photon mapping;optics;physics;statistics;computer graphics (images)	Graphics	63.628605525736106	-52.957370931098204	120291
bebdc4ff435c1588fe1cdd481f0df16a94176993	a cost-effective and robust edge-based blur metric based on careful computation of edge slope	cost effectiveness	This letter presents a novel edge-based blur metric that averages the ratios between the slopes and heights of edges. The metric computes the edge slopes more carefully, i.e., by averaging the edge gradients. The effectiveness of the proposed metric is confirmed by experiments with motion or Gaussian blurred real images and comparison with existing edge-based blur metrics.	computation;gaussian blur	Hanhoon Park;Hideki Mitsumine;Mahito Fujii	2011	IEICE Transactions		computer vision;cost-effectiveness analysis;topology;mathematics;geometry	Vision	57.62614528559814	-60.642946085723985	120432
69fb2909cf9508b9455081505cba010e49bf5703	a novel framework for enhancement of the low lighting video		A novel and effective framework for the enhancement of low lighting images is proposed in this paper. The novel framework presents an optimized de-haze algorithm on inverted images to enhance the low-dynamic-range images which optimizes the complicated process of computing the parameters A and t(x). The improved gamma correction is used to enhance the image contrast for providing better visual performance. For the noise signal is magnified simultaneously, the guided filter has been adapted for the noise reduction with the green channel as the guided image. The framework can real-time work with its low computational complexity. The luminance enhancement part reveals the relationship between the Retinex and the de-haze algorithm.	algorithm;channel (digital image);computational complexity theory;edge enhancement;gamma correction;kalman filter;noise reduction;real-time clock;stream processing	Jianhua Pang;Sheng Zhang;Wencang Bai	2017	2017 IEEE Symposium on Computers and Communications (ISCC)	10.1109/ISCC.2017.8024714	visualization;noise (signal processing);noise reduction;computational complexity theory;computer science;gamma correction;computer vision;color constancy;artificial intelligence;luminance;communication channel	Vision	58.559077716646904	-62.03143894464261	120540
721f52156165d07e01dba70dbb0460ceaae0868f	compression of the layered depth image	transformation ondelette;theorie vitesse distorsion;image tridimensionnelle;compression algorithm;evaluation performance;rate distortion;jpeg 2000 compression layered depth image compression image representation image rendering two dimensional image 2d image ldi pixel pixels array ldi compression algorithm multiple layers depth coding rate distortion model benchmark compression tools jpeg 2000 mpeg 4;modele empirique;image coding;performance evaluation;realite virtuelle;image processing;data compression;realidad virtual;evaluacion prestacion;layered depth image ldi;virtual reality;procesamiento imagen;traitement image;rate distortion theory;codage image;compression image;image compression;layered depth image;image representation;empirical model;tridimensional image;modelo empirico;rendering computer graphics image coding image representation data compression rate distortion theory;transformacion ondita;rendering computer graphics;wavelet;wavelet transformation;imagen tridimensional;image coding rendering computer graphics layout cameras pixel signal processing algorithms geometry two dimensional displays mpeg 4 standard compression algorithms;compresion imagen	A layered depth image (LDI) is a new popular representation and rendering method for objects with complex geometries. Similar to a two-dimensional (2-D) image, the LDI consists of an array of pixels. However, unlike the 2-D image, an LDI pixel has depth information, and there are multiple layers at a pixel location. We develop a novel LDI compression algorithm that handles the multiple layers and the depth coding. The algorithm records the number of LDI layers at each pixel location, and compresses LDI color and depth components separately. For LDI layer with sparse pixels, the data is aggregated and then encoded. An empirical rate-distortion model is used to optimally allocate bits among different components. Compared with the benchmark compression tools such as JPEG-2000 and MPEG-4, our scheme improves the compression performance significantly.		Jiangang Duan;Jin Li	2003	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/TIP.2003.809010	data compression;computer vision;image processing;computer science;mathematics;virtual reality;multimedia;statistics;computer graphics (images)	Visualization	61.01431346504121	-54.74125755057586	120550
363641a2444cb5f663484d03eee079cbaa88bdc8	inherently edge-preserving depth-map coding without explicit edge detection and approximation	residual coding;psnr;decoding;video coding image reconstruction image texture;depth map coding;quantization signal;3d hevc standard edge preserving depth map coding 3d video coding view synthesis scene analysis 3d object reconstruction depth images sharp edges smooth large regions depth coding techniques intra coding mode approximated edge modelling inter coded block residuals texture components pixel domain motion data edge preserving intra coding techniques;vectors;image edge detection;pixel based scalar quantization;approximation methods;pixel based scalar quantization depth map coding residual coding multiview plus depth;multiview plus depth;encoding;quantization signal image edge detection encoding decoding vectors approximation methods psnr	In emerging 3D video coding, depth has significant importance in view synthesis, scene analysis, and 3D object reconstruction. Depth images can be characterized by sharp edges and smooth large regions. Most of the existing depth coding techniques use intra-coding mode and try to preserve edges explicitly with approximated edge modelling. However, edges can be implicitly preserved as long as the transformation is avoided. In this paper, we have demonstrated that inherent edge preserving encoding of inter-coded block residuals, uniformly quantized at pixel domain using motion data from associated texture components, is more efficient than explicitly edge preserving intra-coding techniques. Experimental results show that the proposed technique have achieved superior image quality of synthesized views against the new 3D-HEVC standard. Lossless applications of the proposed technique has achieved on average 66% and 23% bit-rate savings against 3D-HEVC with negligible quantization and perceptually unnoticeable view synthesis, respectively.	3d computer graphics;3d film;approximation algorithm;data compression;depth perception;edge detection;high efficiency video coding;image quality;pixel;sharp mz;stereoscopic video coding;view synthesis	Shampa Shahriyar;M. Manzur Murshed;Mortuza Ali;Manoranjan Paul	2014	2014 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)	10.1109/ICMEW.2014.6890592	computer vision;peak signal-to-noise ratio;computer science;theoretical computer science;coding tree unit;pattern recognition;mathematics;context-adaptive binary arithmetic coding;encoding;statistics	Vision	57.38195716526638	-62.553218889289894	120572
00528f1ca0c41e166587f0d16c7f9f2c7384e303	image synthesis using adjoint photons	image synthesis;global illumination;monte carlo method;adjoint monte carlo methods	The most straightforward image synthesis algorithm is to follow photon-like particles from luminaires through the environment. These particles scatter or are absorbed when they interact with a surface or a volume. They contribute to the image if and when they strike a sensor. Such an algorithm implicitly solves the light transport equation. Alternatively, adjoint photons can be traced from the sensor to the luminaires to produce the same image. This “adjoint photon” tracing algorithm is described, and its strengths and weaknesses are discussed, as well as details needed to make adjoint photon tracing practical.	algorithm;rendering (computer graphics)	R. Keith Morley;Solomon Boulos;Jared M. Johnson;David Edwards;Peter Shirley;Michael Ashikhmin;Simon Premoze	2006		10.1145/1143079.1143109	mathematical optimization;calculus;mathematics;geometry;global illumination;statistics;monte carlo method	Graphics	63.54496167832437	-52.36887987312287	120581
44b970dce08987404dc61e9fbf731868b19a5977	specularity removal for enhancing face recognition	3d shape;albedo;image matching;image matching face recognition image enhancement;image matching errors face recognition enhancement specularity removal face images lambertian component estimation source image 3d shape recognition methods;image enhancement;face recognition;shape;estimation;face recognition lighting reflectivity image recognition image reconstruction shape measurement laboratories national electric code surveillance surface reconstruction;three dimensional displays;image reconstruction;diffuse reflectance;face;specularity;lighting;lambertian;structural properties;lambertian face recognition specularity albedo 3d shape	This paper proposes a new method for removing specularity from face images so that albedo (diffuse reflectivity) can be accurately estimated. Our method utilizes common structural properties of face images to estimate the Lambertian component (including shadows) without using albedo, then specularity is separated as the positive component of the difference between the estimated Lambertian component and the source image. Experimental results show that face-recognition performance is significantly improved by applying our new algorithm. Numerous previous face-recognition methods use 3D shape and albedo of an enrolled image for face recognition under variable pose and illumination conditions. However albedo used by the previous methods contained a residual of specularity from the enrolled image, which produced matching errors. Our algorithm is effective for solving this problem.	algorithm;diffuse reflection;facial recognition system;lambertian reflectance;specularity	Rui Ishiyama;Masato Tsukada	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5413993	iterative reconstruction;facial recognition system;face;computer vision;lambert's cosine law;estimation;shape;lighting;mathematics;diffuse reflection;albedo;computer graphics (images)	Vision	53.96981095801742	-56.191788553640585	120597
f449e1709d1479611a861146c8ec41eb3f37a044	quality evaluation of tourmaline red based on uniform color space		Based on the uniform color space $$\hbox {CIE}\,1976\hbox {L}^{{*}}\hbox {a}^{{*}}\hbox {b}^{{*}},$$ CIE 1976 L ∗ a ∗ b ∗ , the red colors of 310 tourmalines were conducted with the research of color colorimetry. In terms of the quantitative analysis of color indexes such as lightness, chroma and color hue, we put forward the eight-class five-level systems for quality evaluation of tourmaline red by K-Means cluster analysis. The systems were sorted from superior to inferior: Fancy Vivid, Fancy Intense, Fancy Deep, Fancy and Fancy Dark. Through Fisher discriminant, the accuracy was proved to be up to 98.7%. Furthermore, we analyzed the effects of different standard light sources (D $$_{65},$$ 65 , A and CWF) on tourmaline red in detail. It was pointed out that D $$_{65}$$ 65 could better reflect the color of non-self-luminous object, which was suitable for quality evaluation of tourmaline red. Light source A significantly improved the chroma and hue of tourmaline red for red tourmaline display. Light source CWF reduced the chroma of tourmaline red, making the hue significantly deviated from the red. Therefore, CWF was not suitable for tourmaline red lighting. Finally, the work discussed the effects of non-color background lightness and color background on tourmaline red to derive the following results: (1) in the process of non-color background lightness transformation, the lightness and chroma of tourmaline red were sensitive and changed significantly, with high synchronism; whereas, the color hue has unobvious change. The higher lightness and chroma of tourmaline red led to larger effects of non-color background lightness transformation to visual lightness and saturation. It was thus proved that the non-color background was suitable for quality evaluation of tourmaline red. (2) In the process of color background transformation, color metal alloy background has significantly higher lightness (average lightness 83.81) and chroma (average chroma 39.21) than tourmaline red, with obvious orange hue (average hue angle 74.48). Therefore, color metal alloy background significantly enhanced the lightness and chroma of tourmaline red, thus greatly contributing to quality evaluation of tourmaline red. However, color metal alloy background made orange hue obvious, namely strengthening mixed hue of non-mainstream red, which was not conducive to quality evaluation of tourmaline red. Therefore, color metal alloy background was only suitable for inlaid jewelry material or gemstone display rather than quality evaluation.	chroma subsampling;cluster analysis;color space;computability in europe;computer graphics lighting;k-means clustering;linear discriminant analysis;luminous studio;ncsa mosaic;resident evil	Ying Guo	2017	Cluster Computing	10.1007/s10586-017-1091-1	real-time computing;gemstone;colorimetry;lightness;computer science;hue;color space;mineralogy;tourmaline	Graphics	60.987925344435034	-62.035784619583296	120609
4e28ec2437b65a14066ec35a0d0d7369678159db	time-of-flight sensor fusion with depth measurement reliability weighting	depth map upsampling time of flight active brightness sensorfusion super resolution 3d video scene depth;media and communication technology;brightness reliability weight measurement spatial resolution noise cameras signal resolution;signalbehandling;medieteknik;double weighted tsr proposal time of flight sensor fusion depth measurement reliability weighting function three dimensional television 3dtv high quality view synthesis autostereoscopic multiview display tof sensor fusion stereovision analysis depth upsampling error method weighted error energy minimization approach temporal information time of flight active brightness signal;signal processing;weighing minimisation reliability sensor fusion spatial variables measurement	Accurate scene depth capture is essential for the success of three-dimensional television (3DTV), e.g. for high quality view synthesis in autostereoscopic multiview displays. Unfortunately, scene depth is not easily obtained and often of limited quality. Dedicated Time-of-Flight (ToF) sensors can deliver reliable depth readings where traditional methods, such as stereovision analysis, fail. However, since ToF sensors provide only limited spatial resolution and suffer from sensor noise, sophisticated upsampling methods are sought after. A multitude of ToF solutions have been proposed over the recent years. Most of them achieve ToF superresolution (TSR) by sensor fusion between ToF and additional sources, e.g. video. We recently proposed a weighted error energy minimization approach for ToF super-resolution, incorporating texture, sensor noise and temporal information. For this article, we take a closer look at the sensor noise weighting related to the Time-of-Flight active brightness signal. We determine a depth measurement reliability function based on optimizing free parameters to test data and verifying it with independent test cases. In the presented double-weighted TSR proposal, depth readings are weighted into the upsampling process with regard to their reliability, removing erroneous influences in the final result. Our evaluations prove the desired effect of depth measurement reliability weighting, decreasing the depth upsampling error by almost 40% in comparison to competing proposals.	3d television;autostereoscopy;digital video;display resolution;energy minimization;estimation theory;image noise;lu decomposition;sensor;stereopsis;super-resolution imaging;test case;test data;time-of-flight camera;traffic sign recognition;upsampling;verification and validation;view synthesis;weight function	Sebastian Schwarz;Mårten Sjöström;Roger Olsson	2014	2014 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON)	10.1109/3DTV.2014.6874759	computer vision;simulation;computer science;computer graphics (images)	EDA	58.59702217802317	-57.58534739757192	120808
656ee9265f1a4d768a5827c9c565c4cb392d3adf	multi-objective differential evolution algorithm for underwater image restoration	degradation;measurement;scattering;image restoration;mode algorithm multiobjective differential evolution algorithm underwater image restoration underwater image processing image formation represent variables scattering absorption coefficient evolutionary optimization algorithms;mathematical model;image restoration mathematical model optimization measurement degradation scattering cameras;optimization;image restoration evolutionary computation image representation;cameras	Underwater image processing area has been considered an important topic within the last decades with important achievements. This kind of images are essentially characterized by their poor visibility because light is exponentially attenuated as it travels in the water and the scenes result poorly contrasted and hazy. On the other hand, image restoration takes into account the influence of the environment on the image in order to achieve an image with an improved quality. This technique consist of inverting the physical model of image formation. That model contains parameters which represent variables such as coefficients of absorption, scattering, among others. In this case, the quality of the restored image depends on the correct estimation of these parameters. In this work, an approach based on evolutionary optimization algorithms is proposed, for restoring underwater images by estimating the model parameters, and using two metrics for quality assessment. The degradation in the images has been simulated by using an image formation model. Results show that image restoration based on a Multi-Objective Differential Evolution (MODE) algorithm achieves images with good contrast and sharpness, being even better than the original image.	algorithm;circuit restoration;coefficient;differential evolution;elegant degradation;image formation;image processing;image restoration;mathematical optimization;stereopsis	Camilo Sánchez-Ferreira;Helon V. H. Ayala;Leandro dos Santos Coelho;Daniel Munoz;Mylène C. Q. Farias;Carlos H. Llanos	2015	2015 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2015.7256898	image restoration;computer vision;mathematical optimization;feature detection;degradation;mathematical model;scattering;anisotropic diffusion;top-hat transform;measurement;statistics	Vision	57.51915962573138	-58.670762364835106	120914
ff8a98d7004f0b00383d97ae432f92f593195d1c	extended corrected-moments illumination estimation		A remarkably simple color constancy method was recently developed, based in essence on the Gray-Edge method, i.e., the assumption that the mean of color-gradients in a scene (or colors themselves, in a Gray-World setting) are close to being achromatic. However this new method for illuminant estimation explicitly includes the important notions that (1) we cannot hope to recover illuminant strength, but only chromaticity; and (2) that a polynomial regression from image moment vectors to chromaticity triples should be based not on polynomials but instead on the roots of polynomials, in order to release the regression from absolute units of lighting. In this paper we extend these new image moments in several ways: by replacing the standard expectation value mean used in the moments by a Minkowski p-norm; by going over to a float value for the parameter p and carrying out a nonlinear optimization on this parameter; by considering a different expectation value, generated by using the geometric mean. We show that these strategies can drive down the median and maximum error of illumination estimates. Introduction Colors in images result from the combination of illumination, surface reflection, and camera sensors plus the effects of the imaging and display pipeline [13]. In general, the human visual system is capable of filtering out the effects of the illumination source when observing a scene – a psychophysical phenomenon denoted color constancy (CC). In many computer vision or image processing problems, researchers have often made use of some variety of CC as a pre-processing step to either generate data that is relatively invariant to the illuminant, or on the other hand to ensure that the captured color of the scene changes appropriately for different illumination conditions. The computer science goal in the color constancy task is to estimate the illumination, or at least the chromaticity – color without magnitude. Remarkably, the recent Corrected Moments illumination estimation due to Finlayson [6] does overall best in terms of illumination accuracy, and moreover produces results that reduce the maximum error in estimation. The latter property is important and desired: a camera manufacturer wishes to generate no images at all that produce strange colors, in any situation. The objective we aim at, here, falls within the scenario of a camera company (or smartphone producer) providing a CC algorithm with their equipment. In this sense, a training phase would be acceptable since the resulting algorithm adheres only to a single camera – the images we consider are not “unsourced” in the sense that come from the web or other unknown source: instead, they come from a known camera. In this paper we re-examine Finlayson’s Corrected Moments method [6] with a view to simple extensions which we find further improve the illumination estimates delivered by the method. These simple extensions do not greatly affect the good timeand space-complexity of the method, yet yield better results, thus surpassing the best results to date. Here we extend the Corrected-Moments approach in three ways. Specifically, we begin by incorporating Minkowski-norm moments into Corrected-Moments illumination estimation. Then we show how to incorporate the Zeta-Image [5] approach to illuminant estimation within the Corrected-Moments method. Finally we devise a float-parameter optimization scheme to deliver the best performance for each dataset situation. The paper is organized as follows. In Section [Related Work] we discuss related works that form the scaffold for the present work. In Section [Corrected Moments] we review the corrected moments approach proposed by [6]. In the Section [Minkowski Norm and Geometric Mean in Corrected Moments Method] we propose novel moments to be used in the Corrected-Moments approach, plus a new optimization scheme. We compare results for the proposed moments with results obtained previously by exhaustively considering different estimators applied to 4 standard datasets. Related Work Gray-World and Gray-Edge In experiments and tables of results below, note that we compare results with the best to date, state-of-the-art methods. However, in fact the method in [6] is based on very simple algorithms, so we begin the discussion with these. The simplest illumination estimation algorithm is the Gray-World algorithm [3], which assumes that the average reflectance in a scene is achromatic. Thus the illumination color may be estimated by simply taking the global average over pixel values. More specifically, in each color channel k = 1..3, the gray-world estimate of light color is given by E(Rk), where E(·) is expectation value and Rk is RGB color. That is, Gray-World states that E(Rk) = 1 N ∑ N i=1 R i k, with N being the number of pixels. Intuitively, Gray-World will obviously fail if a scene is insufficiently colorful. For example, an image of a gold coin that takes up most of the pixels will generate a very poor illumination estimate; and if we move the white point to R = G = B = 1, or use a more careful white-point camera balance (see, e.g., [11]) then our image will likely end up containing a coin that looks gray rather than gold. A more recent but almost as simple algorithm is the GrayEdge method, which asserts that the average of reflectance differences in a scene is achromatic [16]. With this assumption, the illumination color is estimated by computing the average color derivative in the image, E(||∇Rk||), where ∇ is the gradient field pair {∂/∂x,∂/∂y}. The Gray-Edge assumption originated from the empirical observation that the color derivative probability distribution for images forms a relatively regular, ellipsoid-like shape, with the long axis coinciding with the illumination color [16]. The expectation value for the kth color channel is then estimated by ĉk = √√√√ N ∑ i=1 ∣∣∣∣∂Rik ∂x ∣∣∣∣ 2 + ∣∣∣∣∂Rik ∂y ∣∣∣∣ 2	algorithm;apache axis;channel (digital image);color balance;computer science;computer vision;dspace;expectation value (quantum mechanics);experiment;global illumination;gradient;human visual system model;image moment;image processing;mathematical optimization;minkowski addition;nonlinear programming;nonlinear system;norm (social);pixel;polynomial;preprocessor;sensor;smartphone;t-norm	Xiaochuan Chen;Mark S. Drew;Ze-Nian Li;Graham D. Finlayson	2016			mathematics	Vision	55.66110802240567	-58.0039320554561	120928
98c3ba6632dcefb565e5cca67e170276a5b7bbde	learning custom color transformations with adaptive neighborhoods	look up table;color management;ridge regression;estimation method;gold;smoothing;image quality;transform theory;video;color image	bstract. Custom color transformations for images or video can be earned from a small set of sample color pairs by estimating a ook-up table (LUT) to describe the enhancement and storing the UT in an International Color Consortium profile, which is a standard ool for color management. Estimating an accurate LUT from a small et of sample color pairs is challenging. Local linear and ridge reression are tested on six definitions of neighborhoods for twenty olor enhancements and twenty-five color images. Excellent results ere obtained with local ridge regression over proposed enclosing eighborhoods, including a variant of Sibson’s natural neighbors. he evaluation of the different estimation methods for this task comared the fidelity of the learned color enhancement to the original ample color pairs and the presence of objectionable artifacts in nhanced images. These metrics show that enclosing neighboroods are promising adaptive neighborhood definitions for local lassification and regression. © 2008 SPIE and IS&T. DOI: 10.1117/1.2955968	color management;consortium;on-off keying	Maya R. Gupta;Eric K. Garcia;Andrey Stroilov	2008	J. Electronic Imaging	10.1117/1.2955968	gold;image quality;color histogram;computer vision;icc profile;color quantization;video;color normalization;color depth;color image;lookup table;computer science;transform theory;color balance;color space;tikhonov regularization;statistics;smoothing;computer graphics (images)	Vision	59.71100225079798	-62.73926936879474	121106
7cb0b71153232f74530232779b95faa82da572bc	removal of high density salt and pepper noise through modified decision based unsymmetric trimmed median filter	high density salt noise removal;ief;median filters image colour analysis image denoising image enhancement image restoration;modified decision based algorithm;unsymmetrical trimmed median filter;gray scale restoration;psnr;high density;median filter;modified decision based unsymmetric trimmed median filter;edge detection;image restoration;salt and pepper noise;color image restoration;materials;noise measurement;trimmed median value;ief pepper noise removal high density salt noise removal modified decision based unsymmetric trimmed median filter gray scale restoration color image restoration trimmed median value modified decision based algorithm mdba progressive switched median filter psmf peak signal to noise ratio psnr image enhancement factor;pepper noise removal;image enhancement;progressive switched median filter;filtering algorithms;image edge detection;image colour analysis;peak signal to noise ratio;image enhancement factor;pixel;pixel filtering algorithms noise measurement materials psnr image edge detection;mdba;unsymmetrical trimmed median filter median filter salt and pepper noise;psmf;image denoising;pepper;median filters;color image	A modified decision based unsymmetrical trimmed median filter algorithm for the restoration of gray scale, and color images that are highly corrupted by salt and pepper noise is proposed in this paper. The proposed algorithm replaces the noisy pixel by trimmed median value when other pixel values, 0's and 255's are present in the selected window and when all the pixel values are 0's and 255's then the noise pixel is replaced by mean value of all the elements present in the selected window. This proposed algorithm shows better results than the Standard Median Filter (MF), Decision Based Algorithm (DBA), Modified Decision Based Algorithm (MDBA), and Progressive Switched Median Filter (PSMF). The proposed algorithm is tested against different grayscale and color images and it gives better Peak Signal-to-Noise Ratio (PSNR) and Image Enhancement Factor (IEF).	action message format;ca gen;circuit restoration;grayscale;image editing;median filter;peak signal-to-noise ratio;peterson's algorithm;pixel;salt (cryptography);salt-and-pepper noise	S. Esakkirajan;T. Veerakumar;A. N. Subramanyam;C. H. PremChand	2011	IEEE Signal Processing Letters	10.1109/LSP.2011.2122333	median filter;computer vision;speech recognition;peak signal-to-noise ratio;computer science;salt-and-pepper noise	Vision	56.946159804315236	-65.33591735878733	121641
0530f1d4d2283676e1f20bfb100753b164a44313	flat-region detection and false contour removal in the digital tv display	object detection digital television flat panel displays image enhancement image segmentation image sequences;histograms;quantization;contrast enhanced;memory management;image segmentation;digital tv;video enhancement processing;bit depth extension;digital television;liquid crystal displays;false contour removal;digital tv liquid crystal displays image segmentation image edge detection entropy quantization histograms memory management object detection research and development;physical characteristic;image enhancement;research and development;video image sequence;image edge detection;flat panel displays;image sequence;video image sequence digital tv display false contour removal video enhancement processing flat region detection image segmentation bit depth extension;flat region detection;entropy;histogram equalization;digital tv display;object detection;image sequences	Bit-depth reduction in digital displays results in false contours in the image. Moreover some of video enhancement processing in the digital TV display, such as histogram equalization, contrast enhancement, and increasing sharpness, etc., make false contours more visible. Bit-depth reduction comes from various display limitations such as video memory constraints, physical characteristics of the display, display drivers, and coarse MPEG quantization, etc. Daly, SJ et al., (2004). We present an efficient method for detecting and segmenting flat-region in the image, and a technique for bit-depth extension to effectively remove false contours. We have simulated bit-depth reduction followed by video enhancement processing that cause false contours with various video image sequences including simple patterns. Our result shows that false contours are effectively removed in the flat-region in the image while the sharpness of object edges is preserved	attribute clash;contour line;false precision;histogram equalization;moving picture experts group;sensor;video card	Wonseok Ahn;Jae-Seung Kim	2005	2005 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2005.1521677	computer vision;digital television;computer science;multimedia;computer graphics (images)	Visualization	59.739004723224326	-59.68235420547483	121647
a33fbe9857c1adb8e512669cf433ccd717dff2fd	shading extraction and correction for scanned book images	smoothing methods document image processing image scanners piecewise constant techniques;image scanners;piecewise constant techniques;reflectivity;shading correction;image restoration;books reflectivity optical character recognition software image quality shape lighting image processing image restoration optical distortion image converters;books;smoothing methods;smooth function scanned book image image shading extraction image shading correction document page scanning reflectance function piecewise constant illumination function;document image processing;image analysis;optical model;shading correction document image processing image restoration optical model	When one scans document pages from a bound book, shading artifacts are commonly occurred in the book spine area. In this letter, we propose a general-purpose method for image shading correction based on an assumption that the reflectance function of the page surface is piecewise constant and the illumination function is smooth. The proposed method is able to completely correct more general types of shading artifacts which are nonuniformly distributed along the book spine. Comparison experiments on a synthetic and a variety of real scanned book images demonstrate the feasibility and effectiveness of the proposed method.	experiment;general-purpose modeling;shading;synthetic intelligence	Gaofeng Meng;Nanning Zheng;Shaoyi Du;Yonghong Song;Yuanlin Zhang	2008	IEEE Signal Processing Letters	10.1109/LSP.2008.2002929	image quality;image restoration;computer vision;feature detection;shading;image analysis;binary image;image processing;computer science;digital image processing;reflectivity;computer graphics (images)	Vision	60.69523595817214	-53.8300800202956	121730
6a3a7e3a2865032f3e8637a5a1877155d461c347	free iris and focus image generation by merging multiple differently focused images based on a three-dimensional filtering	filtering;tecnologia electronica telecomunicaciones;three dimensional blur;three dimensional;focus;image generation;tecnologias;frequency domain;grupo a	This paper describes a method of free iris and focus image generation based on transformation integrating multiple differently focused images. First, we assume that objects are defocused by a geometrical blurring model. And we combine acquired images on certain imaging planes and spatial information of objects by using a convolution of a three-dimensional blur. Then, based on spatial frequency analysis of the blur, we design three-dimensional filters that generate free iris and focus images from the acquired images. The method enables us to generate not only an all-in-focus image corresponding to an ideal pin-hole iris but also various images, which would be acquired with virtual irises whose sizes are different from the original one. In order to generate a certain image by using multiple differently focused images, especially very many images, conventional methods usually analyze focused regions of each acquired image independently and construct a depth map. Then, based on the map, the regions are merged into a desired image with some effects. However, generally, it is so difficult to conduct such depth estimation robustly in all regions that these methods cannot prevent merged results from including visible artifacts, which decrease the quality of generated images awfully. In this paper, we propose a method of generating desired images directly and robustly from very many differently focused images without depth estimation. Simulations of image generation are performed utilizing synthetic images to study how certain parameters of the blur and the filter affect the quality of generated images. We also introduce pre-processing that corrects the size of acquired images and a simple method for estimating the parameter of the three-dimensional blur. Finally, we show experimental results of free iris and focus image generation from real images.	texture filtering	Kazuya Kodama;Akira Kubota	2007	IEICE Transactions	10.1093/ietisy/e90-1.1.191	filter;shift-and-add;three-dimensional space;computer vision;image formation;frequency domain;focus;computer graphics (images)	Vision	58.4034767024534	-59.93834997210452	121823
414f2bd67345e2f5c7da32c8453bc0d9d89dad7f	a survey of projection methods for direct volume rendering	projection method		volume rendering	Peter Kulka	1998			volume rendering;3d rendering;artificial intelligence;computer vision;projection method;computer science	Visualization	67.37353605123981	-54.21486722655929	122255
7f3f099278a1c5b0a09650ccbbe689d0573cb6ad	digital holographic three-dimensional video displays	digital holograms;three dimensional television;light field;digital holographic three dimensional video displays;digital holography;spatial light modulator;light emitting diode;three dimensional television holographic displays spatial light modulators three dimensional displays;three dimensional;spatial light modulators;light emitting diode illumination digital holographic three dimensional video displays digital holograms spatial light modulators;research paper;optical imaging;holography image reconstruction three dimensional displays holographic optical components optical imaging video image processing image color analysis;three dimensional displays;image color analysis;image reconstruction;spatial light modulators slms;3dtv;holographic optical components;holography;holographic displays;3dtv digital holography holographic video spatial light modulators slms 3 d displays;light emitting diode illumination;holographic video;spatiallight modulators slms;video image processing;3 d displays	Holography aims to record and regenerate volume filling light fields to reproduce ghost-like 3-D images that are optically indistinguishable from their physical 3-D originals. Digital holographic video displays are pixelated devices on which digital holograms can be written at video rates. Spatial light modulators (SLMs) are used for such purposes in practice; even though it is desirable to have SLMs that can modulate both the phase and amplitude of the incident light at each pixel, usually amplitude-only or phase-only SLMs are available. Many laboratories have reported working prototypes using different designs. Size and resolution of the SLMs are quite demanding for satisfactory 3-D reconstructions. Space-bandwidth product (SBP) seems like a good metric for quality analysis. Even though moderate SBP is satisfactory for a stationary observer with no lateral or rotational motion, the required SBP quickly increases when such motion is allowed. Multi-SLM designs, especially over curved surfaces, relieve high bandwidth requirements, and therefore, are strong candidates for futuristic holographic video displays. Holograms are quite robust to noise and quantization. It is demonstrated that either laser or light-emitting diode (LED) illumination is feasible. Current research momentum is increasing with many exciting and encouraging results.	3d reconstruction;diode;holography;illumination (image);image noise;lateral thinking;light field;oled;pixel;pixelation;quantization (signal processing);ray (optics);requirement;sbp;software prototyping;spatial light modulator;stationary process	Levent Onural;Fahri Yaras;Hoonjong Kang	2011	Proceedings of the IEEE	10.1109/JPROC.2010.2098430	iterative reconstruction;three-dimensional space;computer vision;light field;optoelectronics;optical imaging;optics;holography;holographic display;physics;light-emitting diode	Vision	61.66848185695359	-55.75798218327788	122405
38e62a31354637f727ec9e8cc37c25f28a4e1b9d	inverse error function trajectories for image reconstruction*this material is based upon work supported by the national science foundation under grant no. 1662029		Capturing clear images while a camera is moving fast, is integral to the development of mobile robots that can respond quickly and effectively to visual stimuli. This paper proposes to generate camera trajectories, with position and time constraints, that result in higher reconstructed image quality. The degradation in of an image captured during motion is known as motion blur. Three main methods exist for mitigating the effects of motion blur: (i) controlling optical parameters, (ii) controlling camera motion, and (iii) image reconstruction. Given control of a camera's motion, trajectories can be generated that result in an expected blur kernel or point-spread function. This work compares the motion blur effects and reconstructed image quality of three trajectories: (i) linear, (ii) polynomial, and (iii) inverse error. Where inverse error trajectories result in Gaussian blur kernels. Residence time analysis provides a basis for characterizing the motion blur effects of the trajectories.		Rohan Katoch;Beatriz Fusaro;Jun Ueda	2018	2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2018.8594315	motion blur;artificial intelligence;computer vision;kernel (linear algebra);iterative reconstruction;image quality;computer science;error function;trajectory;mobile robot;gaussian blur	Robotics	56.05811745782718	-58.09681493217629	122472
c594a9f0512db8356ff396f19cc839f567c5bf8e	anaglyph stereo without ghosting	and texture;real time;gpu;level of detail;shadowing;many view;multi view;i 3 7 computer graphics color;shading	Anaglyph stereo provides a low-budget solution to viewing stereoscopic images. However, it may suffer from ghosting and bad color reproduction. Here we address the first issue. We present a novel technique to perceptually calibrate an anaglyph stereoscopic system and to use the calibration to eliminate ghosting from the anaglyph image. We build a model based on luminance perception by the left and right eyes through the anaglyph glasses. We do not rely on power spectra of a monitor or on transmission spectra of anaglyph glasses, but show how the five parameters of our model can be captured with just a few measurements within a minute. We present how full color, half color, and gray anaglyphs can be rendered with our technique and compare them to the traditional method.	anaglyph 3d;blackwell (series);color;dynamic range;eurographics;naivety;rollover (key);stereoscopy	Harald Sanftmann;Daniel Weiskopf	2011	Comput. Graph. Forum	10.1111/j.1467-8659.2011.01984.x	computer vision;shading;computer science;level of detail;computer graphics (images)	Graphics	61.66637555512529	-54.95014201011154	122550
6ce1550eb55d519a26ee5ea6167cd77433ea49c6	naturalization of screen content images for enhanced quality evaluation				Xingge Guo;Liping Huang;Ke Gu;Leida Li;Zhili Zhou;Lu Tang	2017	IEICE Transactions		computer vision;computer science;bicubic interpolation;multimedia;algorithm;computer graphics (images)	HCI	62.2315895110032	-56.607200624446406	122946
42b461a1812b825c65abab17f4cfffa78bff0b70	least squares sub-pixel registration refinement using area sampler model	high resolution;sub pixel translation;low resolution;linear least square;image registration;least square;least squares solution;super resolution	Super-resolution applications require sub-pixel registrations of low resolution images to be almost exact due to the deterioration caused by inaccurate image registration. A linear-least-squares technique is proposed to refine sub-pixel translation parameters, which can be employed when the images are registered but just where there is not enough sub-pixel accuracy. In the technique, it is assumed that low resolution pixels are obtained by area sampling high resolution pixel field which have twice the density of their low resolution correspondents. Using this downsampling schema, a set of equations is formed. Assumed geometry and layout provide a constraint set to be used with the equation set. The sub-pixel translations are then found using least-squares-solution-with-equality-constraints. The method is shown to improve the registration accuracy.	decimation (signal processing);image registration;image resolution;linear least squares (mathematics);pixel;sampling (signal processing);super-resolution imaging	Erol Seke;Kemal Özkan	2006	Journal of Mathematical Imaging and Vision	10.1007/s10851-006-3600-3	computer vision;image resolution;computer science;mathematics;sub-pixel resolution;statistics;computer graphics (images)	Vision	54.71283490795123	-54.82003760801617	123059
e69771d4d5b72e08dbd72571c16dd5b0e859a328	an fir image interpolation filter design method based on properties of human vision	image sampling;minimization;least squares approximations;interpolation;human vision;human vision properties;convolution;weighted least square;finite impulse response filter;image interpolation;perceptually weighted least square filter;low pass filter;satisfiability;finite impulse response filter interpolation design methodology low pass filters filtering theory image sampling humans least squares methods frequency response convolution;frequency response;interference suppression;filter design;statistical analysis;digital filters;low pass filters statistical analysis frequency response interpolation fir filters digital filters image sampling least squares approximations interference suppression;image operation;low pass filters;humans;fir image interpolation filter design method;fir filters;ripple response;close up view;edges;frequency response fir image interpolation filter design method human vision properties image operation image enlargement close up view sampling theory perceptually weighted least square filter minimization ripple response edges;least squares methods;filtering theory;image enlargement;design methodology;sampling theory	Image interpolation is an important image operation. It is commonly used in image enlargement to obtain a close-up view of the detail of an image. From sampling theory, an ideal low-pass filter can be used for image interpolation. But, ripples appear around image edges which are annoying to a human viewer. We introduce a new FIR image interpolation filter known as a perceptually weighted least square (PWLS) filter which is designed using both sampling theory and properties of human vision. The goal of this design approach is to minimize the ripple response around edges of the interpolated images and to best satisfy frequency response constraints. The interpolation results using the proposed approach are substantially better than those resulting from replication or bilinear interpolation, and are at least as good as and possibly better than that of cubic convolution interpolation. 1. IDEAL LOW-PASS INTERPOLATION FILTER Based on the sampling theorem, for a band limited signal, a classical method similar to 1-D interpolation [5] can be used for image interpolation. That is, for an L by L interpolation, an M x N image is interlaced by rows and columns of zeros to obtain an L M x L N image, which is then filtered with an ideal low-pass filter H(f1 I f 2 ) which has the point spread function sin(rm/L) sin(rn/L) r m / L r n / L h(m, n) = 1 m n = f l , f 2 , . . . (2)	bandlimiting;bilinear filtering;column (database);convolution;cubic function;filter design;finite impulse response;frequency response;interlaced video;low-pass filter;nyquist–shannon sampling theorem;ripple effect;sampling (signal processing);whittaker–shannon interpolation formula	Hong Chen;Gary E. Ford	1994		10.1109/ICIP.1994.413739	spline interpolation;demosaicing;computer vision;mathematical optimization;bilinear interpolation;low-pass filter;stairstep interpolation;finite impulse response;bicubic interpolation;mathematics;nearest-neighbor interpolation;multivariate interpolation;trilinear interpolation;image scaling	Vision	60.43303431319263	-59.3305281760376	123144
0c1f3efcbb21411da5a473d704fc4b00a19a2191	two remarks on multidimensional texture analysis	texture;three dimensional images;elevation data;texture analysis	This note points out that texture analysis techniques are applicable, in principle, to three-dimensional data arrays (such as obtained, e.g., from CT reconstructions) and to arrays of range (or terrain elevation) data.		David G. Morgenthaler;Chengye Wang;Azriel Rosenfeld	1982	Pattern Recognition Letters	10.1016/0167-8655(82)90021-6	bidirectional texture function;image texture;computer vision;texture;texture compression;texture filtering;computer graphics (images)	Vision	64.49655331515585	-55.6436681019357	123199
3937837b448eeef37e1ee56c74fd6d0702ba1344	estimating the spectral sensitivity of a digital sensor using calibration targets	usu;color constancy;digital camera;chip;response function;evolution strategies;colorimetry;evolution strategy;spectral sensitivity;camera calibration;constraints	A digital sensor which is used inside a digital camera usually responds to a range of wavelengths. The response of the sensor is proportional to the product of the irradiance falling onto the sensor and the sensitivity of the sensor integrated over all wavelengths. Knowledge of the sensor's response function is important for colorimetry and the research area of color constancy. Such data may not always be available from the manufacturer of the camera. The sensitivity of the imaging device is a result of the hardware properties of theimaging chip, the lens and filters used, and the post-processing done by the processor contained inside the camera. We will be using an evolution strategy to obtain the sensor response curves of a camera given a single image of a calibration target.	autostereogram;digital camera;evolution strategy;frequency response;image sensor;video post-processing	Marc Ebner	2007		10.1145/1276958.1277095	chip;computer vision;camera auto-calibration;camera resectioning;colorimetry;spectral sensitivity;computer science;image sensor;evolution strategy;camera interface;color constancy	Vision	61.736532028265565	-58.16434000482091	123267
5b4fb897500b9d04358540a9d053f9e9293e0d3d	efficient method for face region quality enhancement in low bit rate video coding	image segmentation;region segmentation;video coding;bit rate video coding filters image quality focusing image coding image segmentation image reconstruction shape video sequences;image enhancement;adaptive filters;adaptive filtering face region quality enhancement low bit rate video coding face region image quality adaptive weighting factors multi background region approach channel bit rate face region segmentation method face focused filter;image quality;adaptive filters video coding image enhancement image segmentation filtering theory;filtering theory	This paper presents an efficient method for the enhancement of face region image quality. The benefit of in complex computation, preserving background region necessary information and no blocking effect make this method outperforms current existing method. Furthermore, this method provides adaptive weighting factors and multi background region approach that allows to control the effect of the filter as a function of available channel bit rate. An improved face ’region segmentation method is also addressed in this paper. Simulation results confirmed that the proposed method has significantly improved image quality of face region (up to 2.65 dB) and reduced a large number of skipped frames in comparison with ITU-T H.263+ video coding test model 10 (TMN10).	blocking (computing);computation;data compression;image quality;simulation	I. Adiono;Tsuyoshi Isshiki;Dongju Li;Hiroaki Kunieda	2002		10.1109/APCCAS.2002.1115061	image quality;adaptive filter;image texture;computer vision;speech recognition;computer science;multimedia;region growing;image segmentation;scale-space segmentation	Vision	57.437262380959325	-63.35847493129563	123316
8d9ebc70996eb80b97528bbdd94892da0955a532	colorimetric- and spectral-based printing: a simple comparison	printing;mismatching;metamerie;color rendering;estudio comparativo;fatiga color;qualite image;observador;etude comparative;observateur;desadaptacion;rendu couleur;metamerism;image quality;comparative study;colorimetry;impression;calidad imagen;metamerismo;desadaptation;point of view;impresion;observer	The advantage of spectral-based print reproduction with respect to colorimetric approaches is the reduction of metamerism between originals and printed colors. From a theoretical point of view, metamerism-free reproductions have a considerable advantage over traditional prints, since the visual match, traditionally established for a reference illuminant and observer, holds under any viewing conditions. In practice, such advantage implicitly assumes that a comparison between the original and the corresponding reproduction can be established, and that an observer exists to discover the mismatch. Under this hypothesis, we assume that the reproduction problem concerns single color patches, and perform a comparison between colorimetric and multispectral approaches to its solution.	color;multispectral image;printing;stellar classification;uncontrolled format string	Silvia Zuffi	2005		10.1117/12.586960	image quality;metamerism;computer vision;color rendering index;colorimetry;comparative research;observer;quantum mechanics	Robotics	62.121553692551934	-60.511812739434816	123521
d72cf979940a65c4fe201ba435c535c031d97ae9	perceived overall contrast and quality of the tone scale rendering for natural images	tone reproduction;natural images;digital photography;imaging system;visual system;natural scenes;imaging systems	Past research has demonstrated the complexity of perceived contrast as an attribute of natural images. This attribute is affected by the tone reproduction characteristics in an imaging system, the observer's viewing environment, and the scene itself. Development of digital photography with new tools to affect tone reproduction prompts the necessity for further insights into parameters that influence the perception of contrast to computationally predict and optimize this attribute for a large variety of individual images. To elucidate the relationship between perceived overall contrast and image properties, we performed a series of experiments where observers estimated perceived overall contrast, defined as an integrated impression of difference in lightness, for images of natural scenes presented on a monitor screen. The ratings were used to develop a computational prediction based on assessment of features, which hypothetically could be used by the subjects' visual system when evaluating perceived overall contrast.© (2002) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.		Elena A. Fedorovskaya	2002		10.1117/12.469508	computer vision;geography;multimedia;computer graphics (images)	Vision	62.28596203225583	-62.94048619372245	123565
d6af17f79583ddf1a448fa0a523ff5f43dafe076	dual nonlinear correlation applied to textured and colour object recognition	optoelectronics;object recognition;nonlinear dynamics;nonlinear filter;pattern recognition;spectrum;frequency domain	Dual nonlinear correlation (DNC) is a general operation in optical pattern recognition involving linear and nonlinear filtering methods. DNC also allows to apply new non-symmetric operators to both the analyzed scene channel and to the reference target channel. A third nonlinearity introduced in the frequency domain allows the control of the region of the spectrum where the DNC is applied. The implementation of the DNC is carried out in a sole filterless optoelectronic processor based on a two-step joint transform correlator assisted by computer. Experimental conditions related to camera and spatial light modulator features have an influence on the method performance. We present some applications of the DNC to textured and color pattern recognition with variable discrimination capability.	outline of object recognition	Elisabet Pérez;María Sagrario Millán;Katarzyna Chalasinska-Macukow	1998			computer vision;telecommunications;mathematics;optics	Vision	66.25509476875847	-60.70227411297856	123578
0d44c1656cc19cd36991e57577ed7e394a86ee07	fast, robust image registration for compositing high dynamic range photographs from hand-held exposures	difference operator;image registration;high dynamic range	In this paper, we present a fast, robust, and completely automatic method for translational alignment of hand-held photographs. The technique employs percentile threshold bitmaps to accelerate image operations and avoid problems with the varying exposure levels used in high dynamic range (HDR) photography. An image pyramid is constructed from grayscale versions of each exposure, and these are converted to bitmaps which are then aligned horizontally and vertically using inexpensive shift and difference operations over each image. The cost of the algorithm is linear with respect to the number of pixels and effectively independent of the maximum translation. A three million pixel exposure can be aligned in a fraction of a second on a contemporary microprocessor using this technique.	algorithm;align (company);autoregressive integrated moving average;bitmap;bitwise operation;compositing;grayscale;high dynamic range;image registration;manifold alignment;microprocessor;mobile device;pixel;pyramid (image processing);range imaging;sensor	Greg Ward	2003	J. Graphics, GPU, & Game Tools	10.1080/10867651.2003.10487583	computer vision;simulation;computer science;image registration;computer graphics (images)	Vision	58.034950187674056	-53.078830153314605	123645
fcc5d69847efcd9f4644063ece3fea6ac3879554	cloud image resolution enhancement method using loss information estimation	image resolution;image interpolation;multimedia systems;image generation;computational complexity;image quality;resolution enhancement;resolution enhancement technique;satellite image;error estimate	Image resolution enhancement techniques are required in various multimedia systems for image generation and processing. The main problem is an artifact such as blurring in image resolution enhancement techniques. Specially, cloud image have important information which need resolution enhancement technique without image quality degradation. To solve the problem, we propose error estimation and image resolution enhancement algorithm using low level interpolation. The proposed method consists of the following elements: error computation, error estimation, error application. Our experiments obtained the average PSNR 1.11dB which is improved results better than conventional algorithm. Also we can reduce more than 92% computation complexity. The proposed algorithm may be helpful for applications such as satellite image and cloud image.	image resolution	Won-Hee Kim;Jong-Nam Kim	2010		10.1007/978-3-642-17641-8_15	image quality;image restoration;computer vision;feature detection;image resolution;image processing;digital image processing;sub-pixel resolution;remote sensing;computer graphics (images)	Vision	58.89451009756508	-60.51523731703605	123647
a88de2082d3cd0b9931fcb218f0de57a143403e5	comparing image resamplers via a model of the human vision system		Interpolating images is a very common task but many interpolation filters are designed using error metrics that, in comparison to the known human vision system, are fairly crude. In this paper a model of the human vision system is used to produce a quality score that is useful for comparing the error s due to image resampling. It is tested with a number of image resamplers. We conclude that such a model produces a measure of image quality that is superior to signal-to-error ratio, but that further refinements are necess ary.		Richard Harvey;Stephen King;Richard V. Aldridge;J. Andrew Bangham	1997			artificial intelligence;computer science;computer vision;machine vision;human visual system model	Vision	62.105839129144115	-63.044779480053954	123773
599f409fcdef985993181971b257263f2ca4b969	blotches correction and contrast enhancement for old film pictures	rational unsharp masking;contrast enhancement;contrast enhanced;mathematical morphology;image resolution;motion pictures;motion compensation;cinematography;film properties;high resolution images;frame alignment method;morphological operation;motion estimation;image restoration;correction vector;old film pictures;mathematical operators;dark regions;noise reduction blotches correction contrast enhancement old film pictures old picture restoration frame alignment method deblotching algorithm nonlinear approach global motion compensation logarithmic search correction vector morphological operators pixel maps multilevel median filter motion estimation rational unsharp masking high resolution images film properties sharpening operator dark regions bright regions;image resolution cinematography motion compensation motion estimation image enhancement image restoration mathematical operators mathematical morphology median filters filtering theory;image enhancement;image restoration motion pictures filters motion estimation noise reduction 1f noise additive noise motion compensation motion detection image resolution;pixel maps;sharpening operator;noise reduction;nonlinear approach;old picture restoration;multilevel median filter;deblotching algorithm;logarithmic search;global motion compensation;morphological operators;bright regions;global motion;filtering theory;median filters;blotches correction	A complete system for old picture restoration is described: in particular, a frame alignment method, a simple deblotching algorithm and a nonlinear approach for contrast enhancement are proposed. The frame alignment uses a global motion compensation, with a logarithmic search for the correction vector. The blotch removal step consists of two stages. Blotches are rst detected using two suitable morphological operators; the resulting pixel maps are merged and then validated using an estimate of the motion. Secondly, a multilevel median lter is used to replace sets of pixels identi-ed as blotches. For contrast enhancement we use a rational unsharp masking technique which is able to eeectively treat the high resolution images. To comply with the lm properties the sharpening operator acts strongly in the dark regions and weakly in the bright ones. It can also reduce noise in homogeneous areas.	algorithm;circuit restoration;global motion compensation;image resolution;map;mathematical morphology;nonlinear system;pixel;unsharp masking	Livio Tenze;Giovanni Ramponi;Sergio Carrato	2000		10.1109/ICIP.2000.899795	image restoration;computer vision;mathematical morphology;image resolution;computer science;motion estimation;noise reduction;mathematics;cinematography;motion compensation;global motion compensation;computer graphics (images)	Vision	57.06162278340406	-62.591303629030726	123803
e97e901bd13ef9a432bb0cda4c7b240b374ec64d	fast pixel-size-based large-scale enlargement and reduction of image: adaptive combination of bilinear interpolation and discrete cosine transform	evolutionary programming;process integration;discrete cosine transform;large scale;image enhancement;matrices;algorithms;image scaling;curve fitting;wavelets;evolutionary computing	A fast pixel-size-based very large-scale enlargement and reduction of image via an adaptive combination of bilinear interpolation and discrete cosine transform (DCT) is proposed to improve the quality and speed of the image zooming process. The proposed algorithm combined with DCT and bilinear interpolation is proposed to adaptively process different blocks of an image with different characteristics for a very large-scale image enlargement and reduction. Moreover, if quality is the main concern factor, the evolutionary programming is applied to further improve the quality of reduction algorithm of an image by tuning the frequency coefficient matrix of the reduced image. Finally, for dealing with arbitrary pixel-size-based zooming of an image, a new algorithm is also proposed. © 2011 SPIE and IS&T. [DOI: 10.1117/1.3603937]	algorithm;bicubic interpolation;bilinear filtering;coefficient;discrete cosine transform;evolutionary programming;expectation propagation;image scaling;pixel;run time (program lifecycle phase)	Shu-Mei Guo;Chih-Yuan Hsu;Guo-Ching Shih;Chia-Wei Chen	2011	J. Electronic Imaging	10.1117/1.3603937	evolutionary programming;wavelet;computer vision;mathematical optimization;discrete mathematics;computer science;discrete cosine transform;mathematics;algorithm;matrix;process integration;statistics;curve fitting;evolutionary computation;image scaling	Vision	59.07396181397776	-65.28653377941886	123972
69f41a69722c7cd64fc79f2e2f18ec0012a81ec4	object learning and convex cardinal shape composition		This work mainly focuses on a novel segmentation and partitioning scheme, based on learning the principal elements of the optimal partitioner in the image. The problem of interest is characterizing the objects present in an image as a composition of matching elements from a dictionary of prototype shapes. The composition model allows set union and difference among the selected elements, while regularizing the problem by restricting their count to a fixed level. This is a combinatorial problem addressing which is not in general computationally tractable. Convex cardinal shape composition (CSC) is a recent relaxation scheme presented as a proxy to the original problem. From a theoretical standpoint, this paper improves the results presented in the original work, by deriving the general sufficient conditions under which CSC identifies a target composition. We also provide qualitative results on who well the CSC outcome approximates the combinatorial solution. From a computational standpoint, two convex solvers, one supporting distributed processing for large-scale problems, and one cast as a linear program are presented. Applications such as multi-resolution segmentation and recovery of the principal shape components are presented as the experiments supporting the proposed ideas.	cobham's thesis;dictionary;distributed computing;experiment;linear programming relaxation;prototype;proxy server	Alireza Aghasi;Justin K. Romberg	2016	CoRR		computer vision;mathematical optimization;computer science;machine learning;mathematics;algorithm;statistics	ML	54.56700029447307	-53.74988613066707	124067
3d10567813f256261f4687fe319762e2b77d14e6	image super-resolution reconstruction utilizing the combined method of k-svd and ramp		A new image super-resolution reconstruction (SRR) method, combined a modified K-means based Singular Value Decomposition (K-SVD) and Regularized Adaptive Matching Pursuit (RAMP) algorithm is proposed in this paper. In the modified K-SVD algorithm, the maximum sparsity is considered. First, the K-SVD denoising model is first to preprocess the Low Resolution (LR) images. And then, the high-resolution (HR) and LR images are both trained by the RAMP based K-SVD algorithm. The LR and HR dictionaries are also classed by the K-mean method. In test, a human-made and real LR image, namely millimeter wave (MMW) image are respectively used to testify our method proposed. Further, compared our image SRR method with methods of the basic K-SVD and RAMP, experimental results testified the validity of our method proposed.	k-svd;singular value decomposition;super-resolution imaging	Li Shang;Tao Liu;Zhan-Li Sun	2014		10.1007/978-3-319-09333-8_51	artificial intelligence;pattern recognition;matching pursuit;noise reduction;singular value decomposition;computer science;k-svd;sparse approximation;superresolution	Vision	67.13768073503141	-66.08869285190241	124099
c744a73c0c906e86eb6782d25edc765c17d03a6f	a relaxed factorial markov random field for colour and depth estimation from a single foggy image	graph theory;scattering equations cost function markov processes computer vision atmospheric modeling meteorology;markov processes graph theory hessian matrices image colour analysis image representation;image colour analysis;image representation;sparse cholesky factorisation methods relaxed factorial markov random field colour estimation depth estimation single foggy image albedo reovery scattering theory atmospheric vision model fmrf depth layer relaxation labelling problems sparse representations graph laplacian hessian matrices;markov processes;hessian matrices	In this paper, we present a method to recover the albedo and depth from a single image. To this end, we depart from the scattering theory in the atmospheric vision model used elsewhere for defogging and dehazing. We then view the image as a relaxed factorial Markov random field (FMRF) of albedo and depth layers. This leads to a formulation which, for each of the layers in the FMRF, is akin to relaxation labelling problems. Moreover, we can obtain sparse representations for the graph Laplacian and Hessian matrices involved. This implies that global minima for each of the layers can be estimated efficiently via sparse Cholesky factorisation methods. We illustrate the utility of our method for depth and albedo recovery making use of real world data and compare against other techniques elsewhere in the literature.	autostereogram;cholesky decomposition;hessian;laplacian matrix;linear programming relaxation;markov chain;markov random field;maxima and minima;monoid factorisation;relaxation labelling;scattering theory;sparse matrix	Lawrence Mutimbu;Antonio Robles-Kelly	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738073	mathematical optimization;combinatorics;discrete mathematics;graph theory;mathematics;markov process;statistics	Vision	54.04658748451236	-54.16671986789568	124222
e324d5be2029cc248fe0ae9acadb6f247049aa5b	suppression of noise amplification during colour correction	digital camera;image restoration;low pass filter;image sensors;solid state image sensors noise amplification suppression colour correction digital photography low pass filters cmos image sensor digital cameras;interference suppression;digital photography;colour photography;cmos image sensor;colored noise color low pass filters equations filtering digital cameras noise reduction australia digital photography cmos image sensors;image colour analysis;low pass filters;cameras;image restoration image colour analysis interference suppression colour photography cameras image sensors low pass filters	This paper relates to correcting colours in digital photography. Three low pass filters are used to prevent noise amplification during colour correction. It is well known that a straightforward application of a low pass filter reduces the sharpness of an image. In this paper, the filtering is applied in a non-conventional way which naturally compensates the blur introduced by the filtering. This compensation is achieved by combining datapom all the colour channels, thus preserving the original sharpness of the image. This method is particularly useful, but not limited to, CMOS image sensor based digital cameras.	cmos;color;digital camera;digital photography;filter (signal processing);gaussian blur;image sensor;low-pass filter;zero suppression	Igor Kharitonenko;Sue Twelves;Chaminda Weerasinghe	2002	IEEE Trans. Consumer Electronics	10.1109/TCE.2002.1010126	image restoration;image noise;computer vision;digital photography;electronic engineering;computational photography;dark-frame subtraction;image resolution;low-pass filter;computer science;digital image processing;image sensor format;image sensor;digital imaging;digital camera back	Vision	60.30913619854236	-58.66669650210862	124280
0f6bbe9afab5fd61f36de5461e9e6a30ca462c7c	a duality based approach for realtime tv-l1 optical flow	paper;image processing;real time;numerical scheme;flow field;nvidia geforce 7900 gtx;nvidia;graphic processing unit;total variation;optical flow;opengl;variational method;cg;nvidia geforce 7800 gts	Variational methods are among the most successful approaches to calculate the optical flow between two image frames. A particularly appealing formulation is based on total variation (TV) regularization and the robust L norm in the data fidelity term. This formulation can preserve discontinuities in the flow field and offers an increased robustness against illumination changes, occlusions and noise. In this work we present a novel approach to solve the TV-L formulation. Our method results in a very efficient numerical scheme, which is based on a dual formulation of the TV energy and employs an efficient point-wise thresholding step. Additionally, our approach can be accelerated by modern graphics processing units. We demonstrate the real-time performance (30 fps) of our approach for video inputs at a resolution of 320 × 240 pixels.	active contour model;cuda;calculus of variations;computer graphics;correctness (computer science);gradient;graphics processing unit;numerical analysis;opengl;optical flow;pixel;real-time clock;streaming media;thresholding (image processing);video clip	Christopher Zach;Thomas Pock;Horst Bischof	2007		10.1007/978-3-540-74936-3_22	computer vision;simulation;computer science;computer graphics (images)	Graphics	56.7199335662416	-56.91494426240451	124445
8e1f99cca9d3e8d49cc090f23c997efe87c69804	image fusion algorithm for impulse noise reduction	filtering;image denoising median filter image fusion impulse noise;impulse noise image denoising image fusion;median filter;sensors;impulse noise reduction;impulse noise;image fusion;image sensors;noise measurement;psnr value;noise reduction;remote sensing;pixel;mutual information;image fusion algorithm;psnr value image fusion algorithm impulse noise reduction fidelity factor;medical application;image denoising;real time application;fidelity factor;noise;image fusion noise reduction psnr remote sensing medical services biomedical equipment pixel image sensors sensor fusion testing	This paper introduces the concept of image fusion of noisy images for impulse noise reduction. Image fusion is the process of combining two or more images into a single image while retaining the important features of each image. Multiple image fusion is an important technique used in military, remote sensing and medical applications. This paper introduces an image fusion technique for impulse noise reduction, where the fused image will combine the uncorrupted pixels of the noisy images obtained from different sensors. A fidelity factor for fusion of denoised images is also proposed in this paper. Our technique is evaluated and tested with different standard images by finding the mutual information and PSNR value between the recovered and original images. As the simulation results demonstrate, this method is capable of producing better results compared to individually denoised and fused images. This method is algorithmically simple and can be used for combining any number of images and can be used in real time applications.	algorithm;autostereogram;image fusion;impulse noise (audio);mutual information;noise reduction;peak signal-to-noise ratio;pixel;sensor;simulation	S. Indu;Chaveli Ramesh	2009	2009 International Conference on Advances in Recent Technologies in Communication and Computing	10.1109/ARTCom.2009.125	filter;median filter;image restoration;image noise;computer vision;image processing;impulse noise;computer science;sensor;noise measurement;noise;pattern recognition;noise reduction;image sensor;mutual information;image fusion;pixel	Robotics	59.64842437741898	-64.28195664674203	124450
c2b1c47b1cf4c9299a79ae5f361819acb606547b	desire: discontinuous energy seam carving for image retargeting via structural and textural energy functionals	desire visual quality content aware image retargeting discontinuous energy seam carving image retargeting structural energy functional textural energy functional;image texture;energy functional seam carving discontinuous texture structure;visualization optimization computational modeling silicon nonlinear distortion image color analysis		retargeting;seam carving	Akshaya Kumar Mishra;Christian Scharfenberger;Parthipan Siva;Fan Li;Alexander Wong;David A. Clausi	2015		10.1109/ICIP.2015.7351492	image texture;computer vision;computer science;computer graphics (images)	Vision	56.45425530677482	-61.89566576667972	124486
77ca07af10baedd2efd47672ed9959469bf3ad3f	tone mapping with contrast preservation and lightness correction in high dynamic range imaging		In real-world environments, the human visual system perceives a wide range of luminance in a scene. However, the full range of tones in a high dynamic range (HDR) scene cannot be displayed on standard display devices due to their low dynamic range (LDR). Therefore, tone mapping is necessary to faithfully display a HDR scene on an LDR display device. Existing tone mapping methods have problems because details and contrast in a scene are not preserved faithfully, and they also distort the colors in a scene. Thus, we propose a tone mapping method for preserving the detail in an HDR scene using a weighted least squares filter, which preserves the global contrast in a scene by using a competitive learning neural network, before applying a tone reproduction operator to preserve the color without shifting the lightness. According to the Helmholtz–Kohlrausch effect, the perception of brightness depends on the lightness, chroma, and hue of a color. For example, the luminance of pixels with specific colors such as red and blue is low in an HDR scene. The proposed method corrects the lightness of pixels according to the color (i.e., lightness, chroma, and hue) of a tone-mapped image. Experimental results with several test sets of images demonstrated that the proposed tone mapping method with contrast preservation and lightness correction is more suitable for dynamic range compression than existing tone mapping methods, while it also preserves the color of a scene in an effective way.	dynamic range;high-dynamic-range imaging;range imaging;tone mapping	Baek-Kyu Kim;Rae-Hong Park;SoonKeun Chang	2016	Signal, Image and Video Processing	10.1007/s11760-016-0942-1	computer vision;tone mapping;lightness;computer graphics (images)	Robotics	61.051295670394445	-61.10394052818595	124540
6346e59cf7e335e632c4c28e3f5448303e010e64	cost adaptive window for local stereo matching		We present a novel stereo block-matching algorithm which uses adaptive windows. The shape of the window is selected to minimize the matching cost. Such a window might be the less distorted by the disparity function and thus the optimal one for matching. Moreover, we introduce a coarse-to-fine strategy to limit the number of ambiguous matches and reduce the computational cost. The proposed approach performs as state of the art local matching methods.	algorithmic efficiency;binocular disparity;block-matching algorithm;computation;computer stereo vision;experiment;image rectification;microsoft windows;pixel;time complexity	Julia Navarro;Antoni Buades	2017		10.5220/0006100503690376	computer vision	Vision	54.47634229641937	-57.542509392522	124593
9aca539da7110ef7084c74b328e174d964b5c526	enhancement of shape perception by surface reflectance transformation	distribution function	1 Hewlett-Packard Laboratories, 1501 Page Mill Rd, Palo Alto, Ca. 94304 2 School of Religion, University of Southern California, Taper Hall of Humanities, Rm 328, University of Southern California, Los Angeles, Ca., 90089-0355. * To whom all correspondence should be addressed/ Email: malzbend@hpl.hp.com The visual system interprets the shape of an object through a number of cues such as shading, occlusion and motion parallax. We present a simple imaging method that can provide improved perception of the surface shape of objects by enhancing shading cues. This allows physical samples such as ancient artifacts and fossils to be photographed with a conventional digital camera and then redisplayed with a new set of material properties more conducive to the interpretation of surface shape and detail.	certificate authority;circa;digital camera;email;hidden surface determination;palo;parallax;shading	Thomas Malzbender	2004			parallax;computer graphics (images);computer vision;digital camera;artificial intelligence;polynomial texture mapping;perception;computer science;palo;shading;reflectivity	Vision	63.6441985086963	-56.23213274485898	124666
7016f0e701473c646ddfefefa4a80b3bb6f01464	demosaicking method for multispectral images based on spatial gradient and inter-channel correlation		Multispectral images have been studied in various fields such as remote sensing and sugar content prediction in fruits. One of the systems that captures multispectral images uses a multispectral filter array based on a color filter array. In this system, demosaicking processing is required because the captured multispectral images are mosaicked. However, demosaicking is more difficult for multispectral images than for RGB images owing to the low density between the observed pixels in multispectral images. Therefore, we propose a demosaicking method for multispectral images based on spatial gradient and inter-channel correlation. Experimental results demonstrate that our proposed method outperforms the existing methods and is effective.	demosaicing;gradient	Shu Ogawa;Kazuma Shinoda;Madoka Hasegawa;Shigeo Kato;Masahiro Ishikawa;Hideki Komagata;Naoki Kobayashi	2016		10.1007/978-3-319-33618-3_17	pixel;multispectral image;computer vision;artificial intelligence;interpolation;pattern recognition;demosaicing;rgb color model;computer science;color filter array;correlation;communication channel	Vision	67.8222067690772	-65.68059675456836	124683
45248574d5ed761c8d886a4d92bece3f8d82d339	pansharpening using regression of classified ms and pan images to reduce color distortion	land cover low resolution panchromatic image ratio enhancement pansharpening method component substitution pansharpening method multispectral images nonlinear spectral response satellite sensors weighted summation multispectral bands local areas gray value difference synthetic panchromatic image high resolution panchromatic image k means algorithm multiple regression summation weights color distortion;terrain mapping geophysical image processing image resolution image segmentation land cover regression analysis remote sensing;image fusion;classification;remote sensing image color analysis image fusion image resolution satellites indexes vegetation mapping;remote sensing;remote sensing classification image fusion pansharpening;pansharpening	The synthesis of low-resolution panchromatic (Pan) image is a critical step of ratio enhancement (RE) and component substitution (CS) pansharpening methods. The two types of methods assume a linear relation between Pan and multispectral (MS) images. However, due to the nonlinear spectral response of satellite sensors, the qualified low-resolution Pan image cannot be well approximated by a weighted summation of MS bands. Therefore, in some local areas, significant gray value difference exists between a synthetic Pan image and a high-resolution Pan image. To tackle this problem, the pixels of Pan and MS images are divided into several classes by k-means algorithm, and then multiple regression is used to calculate summation weights on each group of pixels. Experimental results demonstrate that the proposed technique can provide significant improvements on reducing color distortion.	approximation algorithm;distortion;image resolution;k-means clustering;multispectral image;nonlinear system;pixel;sensor;synthetic intelligence	Qizhi Xu;Yun Zhang;Bo Li;Lin Ding	2015	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2014.2324817	computer vision;image resolution;biological classification;geology;image fusion;physics;remote sensing	Vision	67.8304627231874	-65.24033350085517	124684
6bc00a80c6967cbd07ead3a2ad97800e69e59036	perceiving translucent materials	light transport;computer graphic;human skin;subsurface scattering;perception;visual system;psychophysics	Many common materials, including fruit, wax and human skin, are somewhat translucent. What makes an object look translucent or opaque? Here we use a recently developed computer graphics model of subsurface light transport [Jensen, et al., 2001] to study the factors that determine perceived translucency. We discuss how physical factors, such as light-source direction can alter the apparent translucency of an object, finding that objects are perceived to be more translucent when illuminated from behind than in front. We also study the role of a range of image cues, including colour, contrast and blur, in the perception of translucency. Although we learn a lot about images of translucent materials, we find that many simple candidate sources of information fail to predict how translucent an object looks. We suggest that the visual system does not rely solely on these simple image statistics to estimate translucency: the relevant stimulus information remains to be discovered.	computer graphics;gaussian blur;jensen's inequality;scene statistics;subsurface scattering	Roland W. Fleming;Henrik Wann Jensen;Heinrich H. Bülthoff	2004		10.1145/1012551.1012575	psychology;cognitive psychology;computer vision;subsurface scattering;visual system;optics;communication;perception;psychophysics;computer graphics (images)	Graphics	63.583914848683314	-56.68662135594634	124945
6bbe73c329dc0b185d0eb662061e10cfc31ffd65	on-line blind unmixing for hyperspectral pushbroom imaging systems		In this paper, the on-line hyperspectral image blind unmixing is addressed. Inspired by the Incremental Non-negative Matrix Factorization (INMF) method [2], we propose an on-line NMF which is adapted to the acquisition scheme of a pushbroom imager. Because of the non-uniqueness of the NMF model, a minimum volume constraint on the endmembers is added allowing to reduce the set of admissible solutions. This results in a stable algorithm yielding results similar to those of standard off-line NMF methods, but drastically reducing the computation time. The algorithm is applied to wood hyperspectral images showing that such a technique is effective for the on-line prediction of wood piece rendering after finishing.	algorithm;computation;image sensor;non-negative matrix factorization;online and offline;time complexity	Ludivine Nus;Sebastian Miron;David Brie	2018	2018 IEEE Statistical Signal Processing Workshop (SSP)	10.1109/SSP.2018.8450702	rendering (computer graphics);acquisition scheme;computation;matrix decomposition;hyperspectral imaging;algorithm;computer science;non-negative matrix factorization	Vision	66.5439384222033	-57.32176653733938	125168
be070847068aef0d85ae1f656d0c9d7ec4246f02	two-pass robust component analysis for cloud removal in satellite image sequence		Due to the inevitable existence of clouds and their shadows in optical remote sensing images, certain ground-cover information is degraded or even appears to be missing, which limits analysis and utilization. Thus, cloud removal is of great importance to facilitate downstream applications. Motivated by the sparse representation techniques which have obtained a stunning performance in a variety of applications, including target detection, anomaly detection, and so on; we propose a two-pass robust principal component analysis (RPCA) framework for cloud removal in the satellite image sequence. First, a plain RPCA is applied for initial cloud region detection, followed by a straightforward morphological operation to ensure that the cloud region is completely detected. Subsequently, a discriminative RPCA algorithm is proposed to assign aggressive penalizing weights to the detected cloud pixels to facilitate cloud removal and scene restoration. Significantly superior to currently available methods, neither a cloud-free reference image nor a specific algorithm of cloud detection is required in our method. Experiments on both simulated and real images yield visually plausible and numerically verified results, demonstrating the effectiveness of our method.	algorithm;anomaly detection;circuit restoration;display resolution;downstream (software development);iterative reconstruction;numerical analysis;pixel;robust principal component analysis;sparse approximation;sparse matrix	Fei Wen;Yongjun Zhang;Zhi Gao;Xiao Ling	2018	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2018.2829028	artificial intelligence;robustness (computer science);mathematics;iterative reconstruction;computer vision;robust principal component analysis;pixel;anomaly detection;sparse approximation;cloud computing;real image	Vision	56.43586757808807	-59.870712545007564	125390
85d433b2f0f9521fb192b57329fcba3f6a37a8cf	a video camera system with enhanced zoom tracking and auto white balance	automatic white balance;focusing;charge coupled image sensors;light sources video cameras optical tracking brightness cathode ray tube displays;color components video camera system zoom tracking auto white balance robust automatic focus automatic white balance system memory reduction crt monitor light source luminance intensity compensation;layout;crt monitor;cameras focusing light sources layout frequency lenses robustness table lookup temperature charge coupled image sensors;auto white balance;automatic exposure;automatic focus;brightness;light source;optical tracking;luminance intensity compensation;video cameras;lenses;color components;robustness;video camera system;temperature;frequency;table lookup;zoom tracking;cameras;light sources;robust automatic focus;system memory reduction;cathode ray tube displays	This paper presents an advanced video camera system with robust automatic focus (AF), automatic white-balance (AWB), and enhanced zoom tracking. The proposed system can achieve accurate zoom tracking with significantly reduced system memory. It can also find accurate in-focus state even when the camera shoots at a CRT monitor or a light source. The proposed AWB technique compensates the luminance intensity of color components without degrading the image.	color balance	Yoon Kim;June-Sok Lee;Aldo W. Morales;Sung-Jea Ko	2002	IEEE Trans. Consumer Electronics	10.1109/TCE.2002.1037024	layout;cathode ray tube;computer vision;temperature;frequency;digital zoom;lens;brightness;robustness;computer graphics (images)	Vision	59.782864111731776	-58.37670508294857	125797
5e730243d5a21be3fbb6af4fda1320b81775b9d9	shape from focus through laplacian using 3d window	two dimensional images;2d windows;high pass filter;three dimensional structure reconstruction;high frequency component detection;laplacian method computer vision three dimensional structure reconstruction two dimensional images 3d window shape from focus camera thin lens gaussian law high pass filter 2d windows high frequency component detection depth map;gaussian law shape from focus 2d window 3d window;2d window;frequency measurement;shape laplace equations focusing frequency image reconstruction lenses optical filters computer vision layout cameras;high pass filters;three dimensional;computer vision;gaussian law;3d window;laplace equations;shape;laplace transforms;three dimensional displays;image reconstruction;pixel;object detection computer vision high pass filters image reconstruction laplace transforms;shape from focus;laplacian method;thin lens gaussian law;depth map;3d structure;high frequency;object detection;point of interest;camera;image sequences	One of the fundamental objectives of computer vision is to reconstruct a three-dimensional (3D) structure of objects from two-dimensional (2D) images. The basic idea of image focus is that objects at different distances from a lens are focused at different distances. Shape from Focus (SFF) is the problem of reconstructing the depth of the scene changing actively the optics of the camera until the point of interest is in focus. The point in focus gives information about its depth through the thin lens Gaussian law. An effective focus measure operator should be a high-pass filter. Usually, the variation of frequency components are not enough that focus measure could be computed pixel-wise, therefore, sum of pixels in small 2D windows are used for detecting the high frequency components. In this paper, we propose to use 3D windows instead of 2D windows for detecting the high frequency components in the images. The proposed algorithm using 3D window gives better depth map than the previous algorithms using 2D windows.	algorithm;computer vision;depth map;filter bank;microsoft windows;pixel;point of interest;sensor	Youngeun An;Gwangwon Kang;Il-Jung Kim;Hyunsook Chung;Jong-An Park	2008	2008 Second International Conference on Future Generation Communication and Networking	10.1109/FGCN.2008.139	computer vision;mathematics;optics;computer graphics (images)	Vision	59.69559248871843	-54.88704361599689	125879
ca626c18ca184b88b57ab30d78df5e24039a8bba	spherical sampling and color transformations	spherical sampling;sampling technique;sharp sensors;ivrg;chromatic adaptation	In this paper, we present a spherical sampling technique that can be employed to find optimal sensors for trichromatic color applications. The advantage over other optimization techniques is that it assures a global minimum is found, and that not only one, but a set of solutions is retained if so desired. The sampling technique is used to find all possible RGB sensors that exhibit favorable chromatic adaptation transform (CAT) behavior when tested on Lam’s corresponding color data set, subject to a CIE ∆E94 error criterion. We found that there are a number of sensors that meet the criterion, and that the Bradford, Sharp, and CMCCAT2000 sensors are not unique.	computability in europe;lam/mpi;mathematical optimization;maxima and minima;sampling (signal processing);sensor	Graham D. Finlayson;Sabine Süsstrunk	2001			sampling;computer vision;mathematical optimization;chromatic adaptation;mathematics;optics;statistics	Visualization	60.640729844545675	-59.80531776426438	126192
175140318e938d86a7e6c81e3ddd66d2a136f7bf	edge enhancement in clustered-dot dithering	edge enhancement;image resolution;edge detection;rule based;image enhancement;smoothing methods;natural scenes halftoned image clustered dot dithering automated edge enhancement gradient based edge detection input image presmoothing edge map postprocessing local adjustments dithering threshold values grayscale resolution spatial resolution rule based compensation pixel patterns threshold modifications experimental results text line art;clustering algorithms pixel image edge detection spatial resolution image resolution laboratories gray scale art layout dispersion;is success;natural scenes;spatial resolution;smoothing methods edge detection image enhancement image resolution	We describe an automated edge enhancement procedure that operates in conjunction with clustered-dot dithering. The goal is not to make weak edges more noticeable, but rather to give the strong edges in the halftoned image a sharper and more natural appearance. Our technique uses a well-known gradient-based edge detection scheme, augmented by pre-smoothing of the input image and post-processing of the resulting edge map. Enhancement is accomplished by means of local adjustments to the dithering threshold values, which result in a tradeoff of grayscale resolution for improved spatial resolution in the vicinity of edges. A rule-based compensation scheme is used to identify and eliminate objectionable pixel patterns caused by the threshold modifications. Experimental results show that our method is successful in improving the appearance of text, line art, and natural scenes rendered by clustered-dot dithering.	dither;edge detection;edge enhancement;gradient;grayscale;logic programming;pixel;smoothing;video post-processing	Rick A. Vander Kam;Ping Wah Wong	1996		10.1109/ICIP.1996.559634	rule-based system;computer vision;image resolution;binary image;computer science;mathematics;edge enhancement;computer graphics (images)	Vision	56.31500956916918	-63.77849662036021	126262
9338ee225711cf6fc18bc0516519f9217380d626	low-complexity video quality assessment using temporal quality variations	video databases;video quality assessment vqa machine learning spatial quality temporal quality variations;video signal processing;computer model;video quality;low complexity;drntu engineering computer science and engineering;video signal processing computational complexity learning artificial intelligence video databases;journal article;visualization distortion measurement quality assessment optical distortion machine learning video sequences;quality assessment;machine learning;computational complexity;human visual system;image quality assessment;full reference;ground truth;cross database tests low complexity video quality assessment vqa schemes temporal quality variations human visual system hvs image quality assessment temporal factors computational complexity worst case pooling strategy spatial quality variations temporal axis machine learning video databases full reference algorithm;learning artificial intelligence;video database;video quality assessment	Objective video quality assessment (VQA) is the use of computational models to evaluate the video quality in line with the perception of the human visual system (HVS). It is challenging due to the underlying complexity, and the relatively limited understanding of the HVS and its intricate mechanisms. There are three important issues that arise in objective VQA in comparison with image quality assessment: 1) the temporal factors apart from the spatial ones also need to be considered, 2) the contribution of each factor (spatial and temporal) and their interaction to the overall video quality need to be determined, and 3) the computational complexity of the resultant method. In this paper, we seek to tackle the first issue by utilizing the worst case pooling strategy and the variations of spatial quality along the temporal axis with proper analysis and justification. The second issue is addressed by the use of machine learning; we believe this to be more convincing since the relationship between the factors and the overall quality is derived via training with substantial ground truth (i.e., subjective scores). Experiments conducted using publicly available video databases show the effectiveness of the proposed full-reference (FR) algorithm in comparison to the relevant existing VQA schemes. Focus has also been placed on demonstrating the robustness of the proposed method to new and untrained data. To that end, cross-database tests have been carried out to provide a proper perspective of the performance of proposed scheme as compared to other VQA methods. The third issue regarding the computational costs also plays a key role in determining the feasibility of a VQA scheme for practical deployment given the large amount of data that needs to be processed/analyzed in real time. A limitation of many existing VQA algorithms is their higher computational complexity. In contrast, the proposed scheme is more efficient due to its low complexity without jeopardizing the prediction accuracy.	algorithm;apache axis;best, worst and average case;computational complexity theory;computational model;database;digital video;distortion;experiment;ground truth;hoc (programming language);human visual system model;image quality;machine learning;nonlinear system;resultant;software deployment	Manish Narwaria;Weisi Lin;Anmin Liu	2012	IEEE Transactions on Multimedia	10.1109/TMM.2012.2190589	computer simulation;computer vision;simulation;ground truth;computer science;video quality;machine learning;data mining;human visual system model;computational complexity theory	Vision	64.22290358405733	-63.86669669357755	126561
83c27017dfee39ef0b1491a4c7a713e91d1acaa4	line-preserving hole-filling for 2d-to-3d conversion	stereoscopy;2d to 3d conversion;triangle mesh;video plus depth;image warping	Many 2D-to-3D conversion techniques rely on image-based rendering methods in order to synthesize 3D views from monoscopic images. This leads to holes in the generated views due to previously occluded objects becoming visible for which no texture information is available. Approaches attempting to alleviate the effects of these artifacts are referred to as hole-filling. This paper proposes a method which determines a non-uniform deformation of the stereoscopic view such that no holes are visible. Additionally, an energy term is devised, which prevents straight lines in the input image from being bent due to the non-uniform image warp. This is achieved by constructing a triangle mesh, which approximates the depth map of the input image and by integrating a set of detected lines into it. The line information is incorporated into the underlying optimization problem in order to prevent bending of the lines. The evaluation of the proposed algorithm on a comprehensive dataset with a variety of scenes shows that holes are efficiently filled without obvious background distortions.	2d to 3d conversion;algorithm;depth map;distortion;mathematical optimization;optimization problem;space-filling tree;stereoscopy;triangle mesh	Nils Plath;Lutz Goldmann;Alexander Nitsch;Sebastian Knorr;Thomas Sikora	2014		10.1145/2668904.2668931	computer vision;mathematics;engineering drawing;computer graphics (images)	Graphics	58.75974462029123	-54.29894049555625	126578
5a1a56798cff9fe687850c2f883abef615937c69	high-quality brightness enhancement functions for real-time reverse tone mapping	bilateral filtering;tone mapping;real time;brightness enhancement functions;reverse tone mapping;experience base	This paper presents an automatic technique for producing high-quality brightness-enhancement functions for real-time reverse tone mapping of images and videos. Our approach uses a bilateral filter to obtain smooth results while preserving sharp luminance discontinuities, and can be efficiently implemented on GPUs. We demonstrate the effectiveness of our approach by reverse tone mapping several images and videos. Experiments based on HDR visible difference predicator and on an image distortion metric indicate that the results produced by our method are less prone to visible artifacts than the ones obtained with the state-of-the-art technique for real-time automatic computation of brightness enhancement functions.	algorithm;artifact (software development);be file system;bilateral filter;color;computation;distortion;graphics processing unit;image processing;real-time clock;tone mapping;variable data printing	Rafael Pacheco Kovaleski;Manuel Menezes de Oliveira Neto	2009	The Visual Computer	10.1007/s00371-009-0327-3	computer vision;tone mapping;computer science;bilateral filter;computer graphics (images)	Graphics	58.96984547733331	-61.01131208167878	126779
2ebcdaff16b064bed68b77027397cd94d3c90454	holographic correloscopy—unconventional holographic techniques for imaging a three-dimensional object through an opaque diffuser or via a scattering wall: a review	strongly scattering layer holographic correloscopy holographic techniques three dimensional object opaque diffuser scattering wall unconventional holography coherence holography photon correlation holography remote imaging digital holography dual reference holography 3 d imaging;optical correlation holography light scattering;holography coherence optical interferometry holographic optical components optical imaging optical diffraction;scattering coherence correlation holography imaging;optical imaging;optical diffraction;coherence;holographic optical components;holography;optical interferometry	Techniques of unconventional holography, called holographic correloscopy, for imaging a three-dimensional (3-D) object through an opaque diffuser or via a scattering wall are reviewed. This paper is not intended to be a general overview. Instead, it will focus and shed light on less-known techniques termed coherence holography, photon-correlation holography, remote imaging digital holography, and dual-reference holography. This paper will introduce how they perform 3-D imaging through a diffuser or via a scattering wall with a thin but strongly scattering layer.	coherence (physics);data acquisition;digital holography;holographic display;image resolution;image sensor;monochrome;parallel computing;spatial anti-aliasing;subsurface scattering	Mitsuo Takeda;Alok Kumar Singh;Dinesh Narayana Naik;Giancarlo Pedrini;Wolfgang Osten	2016	IEEE Transactions on Industrial Informatics	10.1109/TII.2015.2503641	digital holographic microscopy;coherence;interferometry;optical imaging;holographic interferometry;holography	Visualization	64.04112925884112	-57.118722258070775	126962
aecf982494a2debb2bba751cef84b2f4c588f9b0	visible difference predicator for high dynamic range images	image colour analysis computer vision;contrast sensitivity;visual quality;high dynamic range imaging;computer vision;rendering system;image colour analysis;imaging system high dynamic range images visible difference predicator high dynamic range data visual difference predictor automatic visual quality assessment;high dynamic range;dynamic range layout image generation humans liquid crystal displays image coding rendering computer graphics cathode ray tubes computer displays image storage	Since new imaging and rendering systems commonly use physically accurate lighting information in the form of high-dynamic range data, there is a need for an automatic visual quality assessment of the resulting images. In this work we extend the visual difference predictor (VDP) developed by Daly to handle HDR data. This let us predict if a human observer is able to perceive differences for a pair of HDR images under the adaptation conditions corresponding to the real scene observation.	distortion;elegant degradation;high dynamic range;high-dynamic-range imaging;kerrison predictor;ptc integrity;variable data printing;variable data publishing	Rafal Mantiuk;Karol Myszkowski;Hans-Peter Seidel	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1400750	computer vision;image-based modeling and rendering;rendering;computer science;multimedia;computer graphics (images)	Robotics	61.39125826999468	-60.97720764032685	126999
05b1473f941e0812fb090280eb360be4d5dac5ab	discrete stationary wavelet transform based saliency information fusion from frequency and spatial domain in low contrast images		Abstract Due to degraded visibility and low signal-to-noise ratio properties, traditional saliency detection models face great challenges toward low contrast images. In this circumstance, it is difficult to extract effective visual features to describe saliency information. To cope with this problem, this paper proposes a salient object detection model utilizing efficient features both from frequency domain and spatial domain in low contrast images. The discrete stationary wavelet transform (DSWT) is used to fuse the saliency information from frequency and spatial domain. The input image is firstly converted into HSV color space, where each color channel is transformed into frequency domain to adjust the amplitude spectrum by a median filter. Then, a superpixel-level feature extraction is utilized to generate saliency map from both local and global spatial information. Finally, the frequency and spatial domain saliency maps are fused via DSWT to obtain the final result. Experiments are carried out on three public datasets containing visible light condition and our low contrast image dataset to demonstrate the effectiveness of the proposed saliency detection model over other ten state-of-the-art saliency models.		Nan Mu;Xin Xu;Xiaolong Zhang;Xiaoli Lin	2018	Pattern Recognition Letters	10.1016/j.patrec.2018.02.002	artificial intelligence;computer vision;mathematics;frequency domain;feature extraction;hsl and hsv;object detection;pattern recognition;median filter;stationary wavelet transform;spatial analysis;channel (digital image)	Vision	56.95698123864533	-64.0095292058658	127097
56fe9f95ea690ca94f2aa3a1e599b1d8768776d5	temporal post-processing method for automatically generated depth maps		Methods of automatic depth maps estimation are frequently used for 3D content creation. Such depth maps often contains errors. Depth filtering is used to decrease the noticeability of the errors during visualization. In this paper, we propose a method of temporal post-processing for automatically generated depth maps. Filtering is performed using color and motion information from the source video. A comparison of the results with test ground-truth sequences using the BI-PSNR metric is presented.	algorithm;color;depth map;display resolution;energy minimization;ground truth;image segmentation;peak signal-to-noise ratio;video post-processing	Sergey Matyunin;Dmitriy Vatolin;Maxim Smirnov	2011			computer science	Vision	57.86166628477704	-58.27605446373512	127170
5969108b834feab8a172cc0dbbbe1d379ba4e7a4	computing optical flow with physical models of brightness variation	image sequences optical flow brightness motion estimation illumination 2d images total least squares;least squares approximations;time dependent;image motion analysis;illumination;least squares approximations image sequences motion estimation brightness;heat transport;2d images;optical computing physics computing image motion analysis brightness motion estimation least squares approximation infrared imaging lighting sea surface infrared heating;optical flow estimation;optical computing;motion estimation;least squares approximation;physics computing;brightness;sea surface;infrared imaging;infrared heating;total least squares;optical flow;physics based brightness variation;lighting;physical model;image sequences	ÐAlthough most optical flow techniques presume brightness constancy, it is well-known that this constraint is often violated, producing poor estimates of image motion. This paper describes a generalized formulation of optical flow estimation based on models of brightness variations that are caused by time-dependent physical processes. These include changing surface orientation with respect to a directional illuminant, motion of the illuminant, and physical models of heat transport in infrared images. With these models, we simultaneously estimate the 2D image motion and the relevant physical parameters of the brightness change model. The estimation problem is formulated using total least squares (TLS), with confidence bounds on the parameters. Experiments in four domains, with both synthetic and natural inputs, show how this formulation produces superior estimates of the 2D image motion. Index TermsÐOptical flow, physics-based brightness variation, total least squares.	experiment;linear system;numerical analysis;optical flow;ordinary least squares;synthetic intelligence;total least squares	Horst W. Haussecker;David J. Fleet	2001	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.927465	computer vision;computer science;lighting;statistics	Vision	55.024529302143954	-52.885623277835556	127317
968a55e72fa04b74d71a14990d6137eee2f1e512	a noise attribute thresholding method for document image binarization	noise attribute thresholding method;image gray level histogram;nat method;feature extraction document image processing noise;document image binarization;histograms feature extraction network address translation image segmentation image processing noise figure character recognition;noise attribute features;feature extraction;simple noise model noise attribute thresholding method document image binarization document processing global thresholding methods image gray level histogram prominent peaks nat method noise attribute features feature extraction;global thresholding methods;document image processing;document processing;noise;simple noise model;prominent peaks	A new thresholding method, called Noise Attribute Thresholding method (NAX), for document processing is presented in this paper. This method has overcome a daficulty encountered b y many conventional global thresholding methods which utilize the image gray level histogram. The dificulty is that some objects of interest in the image do not form prominent peaks in the histogram. The NAT method utilizes the noise attribute features extracted from the image. These fea tures are based on a simple noise model and are independent of the strength of the signals (objects) in the image. Experimental results show that this method is very effective.	document processing;grayscale;thresholding (image processing)	Hon-Son Don	1995		10.1109/ICDAR.1995.598983	median filter;computer vision;feature detection;speech recognition;document processing;feature extraction;computer science;noise;machine learning;pattern recognition;balanced histogram thresholding;thresholding;image histogram	Vision	56.018357370721546	-64.60145074870455	127323
425014c594ef880ae6ab2a00209302865409015d	non-photorealistic rendering with spot colour	non photorealistic rendering;image abstraction;segmentation;qa75 electronic computers computer science;colour	Colour is an important aspect of art. Not only does it give richness to images, but it always provides a means to highlight certain objects. This idea of spot colour has been used extensively in both fine art and commercial illustrations. Many non-photorealistic rendering (NPR) algorithms produce grayscale or monochromatic images with low saturations. In this paper we introduce the idea of spot colour to NPR and propose a simple and automatic algorithm to add spot colour to these rendering styles. The hue is thresholded into colour layers and the most appropriate layer is automatically determined based on factors such as layer region shape and salience. We also consider using an edge-based criterion to colourise the background, which is an effective means of making the foreground stand out. We demonstrate the effectiveness of our approach by adding spot colour to a diverse set of NPR styles.	algorithm;grayscale;monochrome;non-photorealistic rendering	Paul L. Rosin;Yu-Kun Lai	2013		10.1145/2487276.2487280	computer vision;computer science;colour look-up table;multimedia;computer graphics (images)	Vision	60.536226122054885	-60.55749817780599	127368
1bda2cf6d06c31f3c0456507d3d055261d0242fa	on learning the visibility for joint importance sampling of low-order scattering	participating media;online expectation maximization;importance sampling;light transport simulation	Volumetric path tracing relies on importance sampling to stochastically construct light transport paths from an emitter to the sensor. Existing techniques incrementally sample path vertices or segments with respect to the local scattering property incorporating the geometry and scattering terms. Thus the joint probability density for drawing a path results in a product of the conditional densities each for a local sampling decision. We present a joint path sampling technique that additionally accounts for the spatially varying visibility due to transmittance and occlusion along a double scattering path. The directional density is formulated as a Gaussian mixture model being fitted to single scattered radiance by the online expectation–maximization algorithm. It is first trained with samples oblivious to the visibility, then incrementally consumes an arbitrary number of samples being drawn from the actual scene. The resulting density in turn guides the directional sampling decision for both isotropic and anisotropic scattering. We demonstrate the benefit of our approach by integrating it into the unidirectional path tracing algorithm. The image noise is effectively reduced, even while rendering the heterogeneous participating media in the presence of complex opaque surfaces.	expectation–maximization algorithm;image noise;image scaling;importance sampling;lambert's cosine law;linear discriminant analysis;mixture model;multidimensional scaling;portable document format;rate of convergence;sampling (signal processing);stepwise regression;volumetric path tracing	Guo Zhou;Dengming Zhu;Ting Li;Zhaoqi Wang;Yongquan Zhou	2017	Neurocomputing	10.1016/j.neucom.2016.09.086	computer vision;simulation;importance sampling;mathematics	ML	53.86726521533371	-54.61989274542628	127548
188bd602c9719e52efca0874d5a49076db4c4877	content aware video quality prediction model for hevc encoded bitstream		In this paper, a novel content based video quality prediction model for High Efficiency Video Coding (HEVC) encoded video stream is proposed, which takes into account the quantization parameter (QP) and the newly proposed content type classification (CTC) metric. The CTC metric is derived by combining different types of information extracted from the encoded video sequences: temporal and spatial complexity, the standard deviation of the bitrate and the value of quantized transform coefficients. This metric can establish a logarithmic relationship with the quality of the video sequence, which is evidenced by extensive experimental results. The experimental results demonstrate that the proposed prediction model can achieve better correlation between the actual PSNR and the predicted PSNR in the training and testing process, and outperforms the other existing prediction methods in terms of accuracy. Furthermore, subjective testing results also show a good consistency between the proposed prediction metric and the subjective rankings.	bitstream;coefficient;deep learning;experiment;high efficiency video coding;peak signal-to-noise ratio;quantization (signal processing);streaming media	Yongfang Wang;Kanghua Zhu;Jian Wu;Yun Zhu	2017	Multimedia Tools and Applications	10.1007/s11042-017-4574-4	computer science;artificial intelligence;computer vision;pattern recognition;coding (social sciences);logarithm;quantization (signal processing);standard deviation;video quality;bitstream;quantization (physics);content type	Vision	63.23432504408254	-64.42352408600806	127840
5bdd03e74487712f4b037f790e770921a5b0276d	combined depth and outlier estimation in multi-view stereo	moving object;kernel;generic model;layout hidden markov models stereo vision pixel brightness noise level kernel cameras inference algorithms rendering computer graphics;psi_visics;layout;brightness;mean field;hidden markov models;noise level;spatial correlation;pixel;stereo vision;multi view stereo;inference algorithms;rendering computer graphics;em algorithm;free energy;cameras	In this paper, we present a generative model based approach to solve the multi-view stereo problem. The input images are considered to be generated by either one of two processes: (i) an inlier process, which generates the pixels which are visible from the reference camera and which obey the constant brightness assumption, and (ii) an outlier process which generates all other pixels. Depth and visibility are jointly modelled as a hiddenMarkov Random Field, and the spatial correlations of both are explicitly accounted for. Inference is made tractable by an EM-algorithm, which alternates between estimation of visibility and depth, and optimisation of model parameters. We describe and compare two implementations of the E-step of the algorithm, which correspond to the Mean Field and Bethe approximations of the free energy. The approach is validated by experiments on challenging real-world scenes, of which two are contaminated by independently moving objects.	approximation;bethe–salpeter equation;cobham's thesis;expectation–maximization algorithm;experiment;generative model;mathematical optimization;pixel	Christoph Strecha;Rik Fransens;Luc Van Gool	2006	2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)	10.1109/CVPR.2006.78	layout;computer vision;spatial correlation;kernel;expectation–maximization algorithm;computer science;stereopsis;mean field theory;machine learning;brightness;pixel;hidden markov model;statistics;computer graphics (images)	Vision	53.99922643908615	-53.964145553584935	127963
f2da6962f96f634d14917089dbe584bf2101dc8b	a robust quality metric for color image quality assessment	quality metric;image coding;quality assessment;visual representation;human visual system;image representation;full reference;visual databases image coding image representation;robustness color quality assessment image coding humans transform coding visual system degradation psychology laboratories;high performance;error pooling visual color image quality metric coding schemes human visual system image representation image database image coding reference image;color image;visual databases	In this paper, we propose a visual color image quality metric with full reference image for the evaluation of coding schemes. This metric is based on human visual system properties in order to get the best correspondence with human judgements. Contrary to some others objective criteria, it doesn't use any information on the type of degradations introduced by coding schemes. We use two main stages: the first one in order to compute visual representation of images (based on results from psychophysics experiments conducted in our laboratory) and the second in order to pool errors between visual representation of two images. We propose a new approach for this pooling stage based on the density of errors and their structure. We also show the interest of such pooling method. We compare results of the metric with human judgments on a database of 140 images distorted with 3 types of compression schemes (JPEG, JPEG2000 and a ROI-based algorithm). High performances are obtained leading to assure that the metric is robust, so this approach constitutes an alternative useful tool to PSNR for coding image searchers.	color image;image quality	Patrick Le Callet;Dominique Barba	2003		10.1109/ICIP.2003.1246992	image quality;computer vision;feature detection;color image;computer science;theoretical computer science;human visual system model;information retrieval	Robotics	62.30785584479455	-63.762561241165024	128298
0811ea5112fd3ebca9d29d62bb16a81b31dec014	motion denoising with application to time-lapse photography	motion analysis;time scale;image motion analysis;video signal processing;time lapse video resynthesis;video rerendering;motion denoising;long term evolution;stylized jerkiness;three dimensional displays;motion analysis motion denoising time lapse photography stylized jerkiness video rerendering time lapse video resynthesis;noise reduction;transforms;spatiotemporal phenomena;noise reduction equations mathematical model three dimensional displays spatiotemporal phenomena transforms optimization;mathematical model;optimization;image denoising;long term change;video signal processing image denoising image motion analysis rendering computer graphics;rendering computer graphics;time lapse photography	Motions can occur over both short and long time scales. We introduce motion denoising, which treats short-term changes as noise, long-term changes as signal, and re-renders a video to reveal the underlying long-term events. We demonstrate motion denoising for time-lapse videos. One of the characteristics of traditional time-lapse imagery is stylized jerkiness, where short-term changes in the scene appear as small and annoying jitters in the video, often obfuscating the underlying temporal events of interest. We apply motion denoising for resynthesizing time-lapse videos showing the long-term evolution of a scene with jerky short-term changes removed. We show that existing filtering approaches are often incapable of achieving this task, and present a novel computational approach to denoise motion without explicit motion analysis. We demonstrate promising experimental results on a set of challenging time-lapse sequences.	algorithm;belief propagation;casio loopy;motion estimation;noise reduction;rendering (computer graphics);software propagation;while	Michael Rubinstein;Ce Liu;Peter Sand;Frédo Durand;William T. Freeman	2011	CVPR 2011	10.1109/CVPR.2011.5995374	computer vision;simulation;computer science;noise reduction;mathematical model;computer graphics (images)	Vision	59.53862229699636	-56.29883631575531	128455
12e5c91cdd0ed9a1a3828415f2f2f2a25e9c8eb6	automatic 2d to stereoscopic video conversion for 3d tvs		In this paper we present a novel technique for automatically converting 2D videos to stereoscopic. Uniquely, the proposed approach leverages the strengths of Deep Learning to address the complex problem of depth estimation from a single image. A Convolutional Neural Network is trained on input RGB images and their corresponding depths maps. We reformulate and simplify the process of generating the second camera's depth map and present how this can be used to render an anaglyph image. The anaglyph image was used for demonstration only because of the easy and wide availability of red/cyan glasses however, this does not limit the applicability of the proposed technique to other stereo forms. Finally, we present preliminary results and discuss the challenges.	anaglyph 3d;autostereogram;convolutional neural network;deep learning;depth map;stereoscopy	Xichen Zhou;Bipin C. Desai;Charalambos Poullis	2017	2017 3DTV Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON)	10.1109/3DTV.2017.8280410	convolutional neural network;computer graphics (images);artificial intelligence;deep learning;stereoscopy;computer vision;computer science;2d to 3d conversion;cyan;rgb color model;anaglyph 3d;depth map	Vision	58.74620097535798	-53.04376443736437	128546
81c46d7ea68857f82fe9895f8bdb09b947838b35	perceptual photometric seamlessness in projection-based tiled displays	tiled display;construccion arquitectura tecnologia ambiental;computacion informatica;color calibration;grupo de excelencia;digital camera;variational problem;optimization problem;graphics hardware;ciencias basicas y experimentales;image quality;dynamic range;high dynamic range;tecnologias;tiled displays;projection based displays	Arguably, the most vexing problem remaining for multi-projector displays is that of photometric (brightness) seamlessness within and across different projectors. Researchers have strived for <i>strict photometric uniformity</i> that achieves identical response at every pixel of the display. However, this goal typically results in displays with severely compressed dynamic range and poor image quality.  In this article, we show that strict photometric uniformity is not a requirement for achieving photometric seamlessness. We introduce a general goal for photometric seamlessness by defining it as an optimization problem, balancing <i>perceptual uniformity</i> with <i>display quality</i>. Based on this goal, we present a new method to achieve <i>perceptually seamless high quality displays</i>. We first derive a model that describes the photometric response of projection-based displays. Then we estimate the model parameters and modify them using perception-driven criteria. Finally, we use the graphics hardware to reproject the image computed using the modified model parameters by manipulating only the projector inputs at interactive rates.  Our method has been successfully demonstrated on three different practical display systems at Argonne National Laboratory, made of 2 × 2 array of four projectors, 2 × 3 array of six, projectors, and 3 × 5 array of fifteen projectors. Our approach is efficient, automatic and scalable---requiring only a digital camera and a photometer. To the best of our knowledge, this is the first approach and system that addresses the photometric variation problem from a perceptual stand point and generates truly seamless displays with high dynamic range.	circuit complexity;digital camera;display resolution;graphics hardware;high dynamic range;image quality;machine perception;mathematical optimization;movie projector;optimization problem;photometric stereo;pixel;scalability;seamless3d;video projector	Aditi Majumder;Rick L. Stevens	2005	ACM Trans. Graph.	10.1145/1037957.1037964	image quality;optimization problem;computer vision;dynamic range;simulation;computer science;optics;graphics hardware;computer graphics (images)	Graphics	61.23557493813679	-60.83236175452991	128720
89e9a97e3a30f807730b5ccea8303ee566ca1e31	video painting based on a stabilized time-varying flow field	painting;video abstraction;video signal processing computer graphic equipment coprocessors image colour analysis rendering computer graphics;time varying;flow based filtering;color variation;video stylization;video signal processing;color;3d feature flow construction;nonphotorealistic rendering;computer graphic equipment;stabilized time varying flow field;particle based video stylization technique;coprocessors;three dimensional;computer graphic;3d vectors;painterly rendering;spatiotemporal video cube;flow field;three dimensional displays;image color analysis;three dimensional displays pixel painting image color analysis rendering computer graphics color coherence;image colour analysis;pixel;coherence;gpu implementation;nonphotorealistic rendering video painting stabilized time varying flow field 3d feature flow construction video stylization 3d vectors spatiotemporal video cube color variation particle based video stylization technique gpu implementation;rendering computer graphics;painterly rendering nonphotorealistic rendering flow based filtering video abstraction;video painting	We present a method for constructing 3D feature flow from video and its application to video stylization. Our method extracts smoothly aligned 3D vectors that describe the smallest variation of colors within a spatiotemporal video cube, and thus effectively preserves both spatial and temporal coherence in a relatively inexpensive manner. As an application of this flow field we present a particle-based video stylization technique to rerender the video in a feature enhancing, painterly style. Our method consists of per-pixel operations and is suitable for GPU implementation, which enables real-time video stylization.	alignment;coherence (physics);collision detection;color;graphics processing unit;hl7publishingsubsection <operations>;pixel;real-time clock;smoothing	Jong-Chul Yoon;In-Kwon Lee;Henry Kang	2012	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2011.47	computer vision;coherence;painting;computer science;video tracking;multimedia;pixel;computer graphics (images)	Visualization	58.51533389060454	-54.278608435253915	128801
bf2c70dc99614a11a49fd76d202148df4e1520dd	double compression detection in mpeg-4 videos based on block artifact measurement with variation of prediction footprint	会议论文	In this paper, we propose a novel scheme to detect double MPEG-4 compression with block artifact analysis. An adaptive measurement of block artifact in decompressed frames is proposed and then combined with the Variation of Prediction Footprint (VPF) in an effective way. Based on such measurement, periodic analysis is used to detect double compression. The proposed scheme is verified on several publically available standard videos and compared with the state-of-the-art method. Experimental results demonstrate that it has more robust detection capability.	compression artifact	Peisong He;Tanfeng Sun;Xinghao Jiang;Shilin Wang	2015		10.1007/978-3-319-22053-6_84	computer vision;speech recognition;computer science;computer graphics (images)	Vision	58.139255270448515	-58.32367007372333	129035
8364f3956988e978161d70f07aaec266e086be85	multi-frame super resolution using refined exploration of extensive self-examples		The multi-frame super resolution (SR) problem is to generate high resolution (HR) images by referring to a sequence of low resolution (LR) images. However, traditional multi-frame SR methods fail to take full advantage of the redundancy in LR images. In this paper, we present a novel algorithm using a refined example-based SR framework to cope with this problem. The refined framework includes two innovative points. First, based upon a thorough study of multi-frame and single frame statistics, we extend the single frame examplebased scheme to multi-frame. Instead of training an external dictionary, we search for examples in the image pyramids of the LR inputs, i.e., a set of multi-resolution images derived from the input LRs. Second, we propose a new metric to find similar image patches, which not only considers the intensity and structure features of a patch but also adaptively balances between these two parts. With the refined framework, we are able to make the utmost of the redundancy in LR images to facilitate the SR process. As can be seen from the experiments, it is efficient in preserving structural features. Experimental results also show that our algorithm outperforms state-of-the-art methods on test sequences, achieving the average PSNR gain by up to 1.2dB.	algorithm;dictionary;display resolution;experiment;image resolution;jaggies;lr parser;peak signal-to-noise ratio;super-resolution imaging	Wei Bai;Jiaying Liu;Mading Li;Zongming Guo	2013		10.1007/978-3-642-35725-1_37	simulation;algorithm	Vision	55.32715915964595	-58.274486752529924	129153
9e568b13ea51ca3c669186624566f672eb547857	a multi-ink color-separation algorithm maximizing color constancy	ink;color constancy	Current color-printing technologies may use three or more inks, e.g., CMY, CMYK, CMYKcm, CMYKGO, CMYKRGB. When the number of inks exceeds three, there is the usual color-management one-to-many mapping problem. Because the spectral properties of many modern inks are optimized for maximum color gamut and in some cases, black ink may not be used for pictorial images, many prints have poor color constancy. Changes in lighting dramatically change color balance, particularly for neutrals. An algorithm was developed for multi-ink printing in which the one-to-many mapping problem was overcome by selecting ink combinations with the best color constancy between illuminants F11 and D50. The algorithm was tested using a pigmented-ink inkjet proofing printer. CMYKGO prints color-separated using this algorithm were compared with a generic ICC profile for CMYKcm prints. The CMYK inks were common to both prints. The new algorithm improved color constancy significantly.	algorithm;color balance;color management;image;one-to-many (data model);printer (computing);printing	Yongda Chen;Roy S. Berns;Lawrence A. Taplin;Francisco H. Imai	2003			computer vision;color printing;color management;artificial intelligence;primary color;color balance;computer science;color constancy;algorithm;icc profile;subtractive color;color space	Graphics	59.21991258800901	-62.36610207655222	129220
d3c9cf8de1882347899f8738c401d1e7fae79afe	video super-resolution for mixed resolution stereo	binocular suppression spatio temporal markov random field disparity;video coding data compression markov processes stereo image processing;data compression;video super resolution 3d markov network spectral consistency temporal consistency spatial consistency stereo correspondence mixed resolution stereo video lost frequency information 3d percept visual system compression method mixed resolution stereoscopic coding;video coding;stereo image processing;markov processes;spatial resolution markov random fields cameras three dimensional displays visualization	Mixed resolution stereoscopic coding is a compression method that reduces the bandwidth of stereo video by transmitting one of the two views at a lower resolution. While the visual system weights the sharper view more heavily resulting in a nearly fully sharp 3D percept, certain applications require, or would benefit from, having both views at full resolution. In this paper, we present a super-resolution method that recovers the lost frequency information in mixed resolution stereo video by exploiting stereo correspondence while emphasizing spatial, temporal, and spectral consistency. Our method is distinguished by operating on video modeled as a 3D Markov network, whereas previous approaches focused solely on images.	markov chain;markov random field;stereoscopy;super-resolution imaging;transmitter	Ankit K. Jain;Truong Q. Nguyen	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738199	data compression;stereo cameras;computer vision;computer science;mathematics;markov process;statistics;multiview video coding;computer graphics (images)	Vision	59.765375112216255	-56.90634824962352	129277
a25c1a4a6bf8d25c655d4a204f101f59778ef90a	fast alignment of digital images using a lower bound on an entropy metric	entropy;image registration;quantisation (signal);trees (mathematics);alignment metric;digital images false alignment;entropy metric;metric favors edge alignment;minimal spanning tree entropy estimate;registration algorithm;successively refined quantization	We propose a registration algorithm based on successively refined quantization and an alignment metric derived from a minimal spanning tree entropy estimate. The metric favors edge alignment, is fast to compute, and compares well in experiments with competing approaches.	algorithm;digital image;experiment;file spanning;minimum spanning tree	Mert R. Sabuncu;Peter J. Ramadge	2004	2004 International Conference on Image Processing, 2004. ICIP '04.		computer vision;entropy;mathematical optimization;combinatorics;topology;entropy estimation;computer science;image registration;minimum spanning tree;mathematics;upper and lower bounds;digital image	Robotics	53.88741641509168	-58.7658616459769	129299
1bee86c2336cd19b86a24b59da19819e1ae2b552	universal 4d multiplexing of layered disparity image sequences for pixel and voxel based display devices	3d displays;image tridimensionnelle;image processing;display devices;0130c;disparity;stereoscopy;traitement image;disparidad;multiplexing;voxel;algorithme;combinatorial problem;probleme combinatoire;multiplexage;problema combinatorio;matrices;dispositif affichage;pixel;image sequence;stereoscopie;tridimensional image;algorithms;3d representation;estereoscopia;4230v;disparite;3d display;sequence image;imagen tridimensional;image sequences	An algorithm is presented to multiplex discrete disparity layers to a stack of screen images for different 3d display technologies. This approach enabled the description of a 3d display by just setting few parameters. After more than a century of intense 3d development there is a diversity of 3d displays. The representation of 3d information in these devices can be very different. Regardless of the diversity, all representation technologies can be described by a number of 2d images. The screen images are specific combinations of the image sequence. The combination rule by using the universal 4dimensional formula is adapted for a certain display type. The formula parameters are illustrated by samples for the main groups of 3d displays: stereoscopic, auto stereoscopic and volumetric. With the same input image sequence, samples are calculated for diverse output systems. Also, some matrices are presented to show the influence of various parameters. Furthermore it is demonstrated that the modification of some parameters can change the 3d representation without any modification of the device or the input images. Such effects can be used for the correction of the 3d impression in single 3d systems and multi display solutions (i.e. 3d walls).	binocular disparity;multiplexing;pixel;voxel	Armin Grasnick	2010		10.1117/12.838753	computer vision;stereo display;image processing;optics;computer graphics (images)	Vision	61.02565194284975	-54.852399665738936	129603
015a6e5abebb2c51af57c4d9cd9c81da6ab2cd08	entropy-based adaptive sampling	contrast;adaptive sampling;entropy;ray tracing;antialiasing;pixel colour;stochastic sampling.;decision tree;information theory	Ray tracing techniques need supersampling to reduce aliasing and/or noise in the final image. Since not all the pixels in the image require the same number of rays, supersampling can be implemented by adaptive subdivision of the sampling region, resulting in a refinement tree. In this paper we present a theoretically sound adaptive sampling method based on entropy, the classical measure of information. Our algorithm is orthogonal to the method used for sampling the pixel or for obtaining the radiance of the hitpoint in the scene. Results will be shown for our implementation within the context of stochastic ray tracing and path tracing. We demonstrate that our approach compares well to the ones obtained by using classic strategies based on contrast and variance.	adaptive sampling;algorithm;aliasing;carrier-to-noise ratio;distributed ray tracing;image noise;information theory;mathematical induction;path tracing;pixel;ray tracing (graphics);recursion;refinement (computing);sampling (signal processing);subdivision surface;supersampling	Jaume Rigau;Miquel Feixas;Mateu Sbert	2003			computer vision;artificial intelligence;supersampling;slice sampling;sampling (statistics);ray tracing (graphics);distributed ray tracing;adaptive sampling;computer science;aliasing;path tracing	Graphics	64.4411748774291	-53.72139058220514	129654
c4844aa24a2d6ff431bf2d7a071b0fff4cc681ca	real-time texton substitution for super resolution	art;video streaming;real time;haptics;wavelet transform;graphics hardware;super resolution;design tools	In this paper, texton substitution is implemented by graphics hardware to achieve super resolution of a video stream in realtime processing. The texton substitution method was proposed previously and evaluated for the super resolution of still images, and the textons were defined in wavelet space. In this paper, the wavelet transformation was accelerated by the graphics hardware [Hopf et al. 2001]], followed by two-dimensional filtering of three layers to produce a 10-dimensional vector of the texture at each pixel. The obtained textons were classified into a cluster based on the tree-based clustering technique by the graphics hardware using the pixel shader. This system was applied to the video stream of a face, and the experimental results show the effectiveness of the proposed system.	cluster analysis;graphics hardware;pixel;real-time clock;shader;streaming media;substitution (logic);substitution method;super-resolution imaging;texton;wavelet transform	Kenji Kamimura;Toshiya Nakaguchi;Norimichi Tsumura;Hideto Motomura;Katsuhiro Kanamori;Yoichi Miyake	2005		10.1145/1186954.1186998	computer vision;computer science;multimedia;haptic technology;sub-pixel resolution;graphics hardware;superresolution;wavelet transform;computer graphics (images)	Graphics	57.86761385552871	-55.19861269626416	129904
60c203f5f0c65fec24b527c6c0a284bed8e455a9	finding the aspect-ratio of an imaging system	image sampling;sampling nonuniformity;charge coupled image sensors;fundamental parameters;vertical distance;clocks;image converters;vertical spacing;camera lines;parallel lines;charge coupled devices;power spectrum;computer vision;imaging system;signal processing;image buffer;lenses;horizontal spacing;pixels;sensor array;digitization;computer vision calibration;video signals;cameras sensor arrays frequency image sampling lenses charge coupled image sensors signal processing clocks image converters charge coupled devices;frequency;photo elements;calibration;sensor arrays;cameras;parallel lines sampling nonuniformity video signals aspect ratio imaging system camera lines image buffer pixels digitization vertical spacing vertical distance photo elements sensor array horizontal spacing power spectrum;aspect ratio	Aspect-ratio is a fundamental parameter of an imaging system. It determines the extent of nonuniformity in sampling. Video signal guarantees a one-to-one match between the camera lines and the lines in the image buffer. The horizontal arrangement of pixels, however, undergoes a resampling due to the digitization process. The vertical spacing between lines is given by the vertical distance of the photo elements on the sensor array. The relationship between the vertical and horizontal spacing is determined by the aspect-ratio. We propose a technique that uses power spectrum of the image of two sets of parallel lines to determine the aspect-ratio of the system.	one-to-one (data model);pixel;resampling (statistics);sampling (signal processing);spectral density	Ali R. Bani-Hashemi	1991		10.1109/CVPR.1991.139673	computer vision;aspect ratio;calibration;computer science;frequency;signal processing;parallel;lens;spectral density;sensor array;pixel	Vision	61.11860606764244	-58.41709218491246	129952
1bc661e2fc0fc43e31a5efde4d47294d042a1107	measurement of compression-induced temporal artifacts in subjective and objective video quality assessment	4230;methode mesure;quality measurement;0130c;imagerie;video quality;temporal filtering;imagery;quality assessment;measuring methods;temporal artifacts;paired comparison;temporal processing;subjective experiment;imagineria;video;video quality assessment	Temporal pooling and temporal defects are the two di erences between image and video quality assessment. Whereas temporal pooling has been the object of two recent studies, this paper focuses on the rarely addressed topic of compression-induced temporal artifacts, such as mosquito noise. To study temporal aspects in subjective quality assessment, we compared the perceived quality of two versions of a mosquito noise corrector: one purely spatial and the other spatio-temporal. We set up a paired-comparison experiment and choose videos whose compression mainly creates temporal artifacts. Results proved the existence of a purely temporal aspect in video quality perception. We investigate the correlation between subjective results from the experiment and three video metrics (VQM, MOVIE, VQEM), as well as two temporally-pooled image metrics (SSIM and PSNR). SSIM and PSNR metrics nd the corrected sequences of better quality than the compressed ones but do not distinguish spatial and spatio-temporal processings. The confrontation of those results with the VQM and Movie objective metrics show that they do not account for this type of defects. A detailed study highlights that either they do not detect them or the response of their temporal component is masked by the one of their spatial components.	compression artifact;peak signal-to-noise ratio;structural similarity;video	Claire Mantel;Patricia Ladret;Thomas Kunlin	2011		10.1117/12.871597	pairwise comparison;subjective video quality;computer vision;simulation;video;computer science;video quality;multimedia	Vision	63.07785885187623	-63.09682653871666	130061
c663304e5796560a912e563315ebe1cdb192c245	chromablur: rendering chromatic eye aberration improves accommodation and realism		Computer-graphics engineers and vision scientists want to generate images that reproduce realistic depth-dependent blur. Current rendering algorithms take into account scene geometry, aperture size, and focal distance, and they produce photorealistic imagery as with a high-quality camera. But to create immersive experiences, rendering algorithms should aim instead for perceptual realism. In so doing, they should take into account the significant optical aberrations of the human eye. We developed a method that, by incorporating some of those aberrations, yields displayed images that produce retinal images much closer to the ones that occur in natural viewing. In particular, we create displayed images taking the eye's chromatic aberration into account. This produces different chromatic effects in the retinal image for objects farther or nearer than current focus. We call the method ChromaBlur. We conducted two experiments that illustrate the benefits of ChromaBlur. One showed that accommodation (eye focusing) is driven quite effectively when ChromaBlur is used and that accommodation is not driven at all when conventional methods are used. The second showed that perceived depth and realism are greater with imagery created by ChromaBlur than in imagery created conventionally. ChromaBlur can be coupled with focus-adjustable lenses and gaze tracking to reproduce the natural relationship between accommodation and blur in HMDs and other immersive devices. It may thereby minimize the adverse effects of vergence-accommodation conflicts.		Steven A. Cholewiak;Gordon D. Love;Pratul P. Srinivasan;Ren Ng;Martin S. Banks	2017	ACM Trans. Graph.	10.1145/3130800.3130815	chromatic scale;computer vision;focal length;rendering (computer graphics);computer graphics (images);artificial intelligence;aperture;chromatic aberration;accommodation;computer science;human eye;lens (optics);optics	Graphics	61.66586939609445	-52.839689102129206	130331
8b73ae0825aa724773d54dccc88c83cc8aae4b0b	fast image/video contrast enhancement based on wthe	contrast enhancement;cumulative distribution function;contrast enhanced;weighted thresholded histogram equalization;image video contrast enhancement;probability;video signal processing image enhancement probability;video signal processing;cumulative distribution function contrast enhancement histogram histogram equalization;probability distribution function image video contrast enhancement weighted thresholded histogram equalization wthe;histogram;image enhancement;helium histograms distribution functions image segmentation probability distribution process control programmable control adaptive control distributed computing biomedical image processing;fast imaging;probability distribution function;wthe;histogram equalization	We present a fast and effective method for image contrast enhancement based on weighted and thresholded histogram equalization (WTHE). In our proposed method, the probability distribution function of an image is modified by weighting and thresholding before the histogram equalization (HE) is performed. We show that such an approach provides a convenient and effective mechanism to control the enhancement process while being adaptive to various types of images. We also discuss applications of the proposed method in video enhancement	effective method;histogram equalization;thresholding (image processing)	Qing Wang;Rabab Kreidieh Ward	2006	2006 IEEE Workshop on Multimedia Signal Processing	10.1109/MMSP.2006.285326	computer vision;probability density function;cumulative distribution function;histogram matching;machine learning;pattern recognition;probability;balanced histogram thresholding;histogram;mathematics;adaptive histogram equalization;histogram equalization;statistics	Vision	56.22797334338046	-64.53182507266835	130636
4f8effc22f540be62e0073a0a8bde74977f44ad4	multispectral imaging with optical bandpass filters: tilt angle and position estimation	inclination;mirrors;miroir;band pass filters;sensors;optical filters;0130c;angle inclinaison;mesure position;imagerie;angular measurement;stereoscopy;filtre passe bande;distortion;color filter;imagery;video cameras;mesure angle;bandpass filters;imaging;position measurement;4279c;camera video;filtro colorado;filtre colore;stereoscopie;formation image;etalonnage;estereoscopia;imagineria;parameter estimation;estimation parametre;refraction;imagen color;4230v;calibration;image couleur;cameras;filtre optique;multispectral imaging;color image;glasses	Optical bandpass filters play a decisive role in multispectral imaging. Various multispectral cameras use this type of color filter for the sequential acquisition of different spectral bands. Practically unavoidable, small tilt angles of the filters with respect to the optical axis influence the imaging process: First, by tilting the filter, the center wavelength of the filter is shifted, causing color variations. Second, due to refractions of the filter, the image is distorted geometrically depending on the tilt angle. Third, reflections between sensor and filter glass may cause ghosting, i.e., a weak and shifted copy of the image, which also depends on the filter angle. A method to measure the filter position parameters from multispectral color components is thus highly desirable. We propose a method to determine the angle and position of an optical filter brought into the optical path in, e.g., filter-wheel multispectral cameras, with respect to the camera coordinate system. We determine the position and angle of the filter by presenting a calibration chart to the camera, which is always partly reflected by the highly reflective optical bandpass filter. The extrinsic parameters of the original and mirrored chart can then be estimated. We derive the angle and position of the filter from the coordinates of the charts. We compare the results of the angle measurements to a ground truth obtained from the settings of a high-precision rotation table and thus validate our measurement method. Furthermore, we simulate the refraction effect of the optical filter and show that the results match quite well with the reality, thus also confirming our method.	multispectral image	Johannes Brauers;Til Aach	2009		10.1117/12.804985	medical imaging;computer vision;telecommunications;band-pass filter;optics;filter design;prototype filter;polarizing filter;physics;remote sensing;m-derived filter	Robotics	61.71774425265313	-59.04449147919056	130835
2fe45270c1ad17499daf128e17729e3a83a42c86	rred indices: reduced reference entropic differencing for image quality assessment	databases;reduced reference quality entropy image information image quality natural scene statistics perceptual approaches;approximation algorithms algorithm design and analysis signal processing algorithms visualization quality assessment entropy databases;approximate algorithm;image processing;approximation algorithms;natural scene statistics;entropy difference rred indices reduced reference entropic differencing reduced reference image quality assessment natural image approximation distorted image wavelet coefficients;image database;image information;natural images;algorithm visualization;wavelet transforms entropy image processing natural scenes;wavelet transforms;visualization;quality assessment;signal processing;image quality;image quality assessment;entropy;perceptual approaches;point of view;signal processing algorithms;reduced reference quality;algorithm design;reduced reference;algorithm design and analysis;natural scenes	We study the problem of automatic “reduced-reference” image quality assessment (QA) algorithms from the point of view of image information change. Such changes are measured between the reference- and natural-image approximations of the distorted image. Algorithms that measure differences between the entropies of wavelet coefficients of reference and distorted images, as perceived by humans, are designed. The algorithms differ in the data on which the entropy difference is calculated and on the amount of information from the reference that is required for quality computation, ranging from almost full information to almost no information from the reference. A special case of these is algorithms that require just a single number from the reference for QA. The algorithms are shown to correlate very well with subjective quality scores, as demonstrated on the Laboratory for Image and Video Engineering Image Quality Assessment Database and the Tampere Image Database. Performance degradation, as the amount of information is reduced, is also studied.	algorithm;approximation;autoregressive integrated moving average;categories;chroma subsampling;coefficient;computation (action);database;distortion;elegant degradation;image quality;image resolution;mean squared error;qr decomposition;quinolinic acid;risk assessment;robust random early detection;software quality assurance;wavelet;emotional dependency	Rajiv Soundararajan;Alan C. Bovik	2012	IEEE Transactions on Image Processing	10.1109/TIP.2011.2166082	algorithm design;computer vision;image processing;computer science;theoretical computer science;signal processing;data mining;algorithm	Vision	61.93046047757884	-65.03225002128082	130907
1ea85b1f1787a149f316df141d185cf62f9a1abd	digital image forensics using em algorithm	spectrum;frequency spectrum;expectation maximization;expectation maximization algorithm;digital image forensics;digital image;em algorithm	Digital image forensics has become a very important research topic. This paper proposes a method to detect the forgery of digital image by (1) computing the interpolated coefficient for the images using expectation-maximization (EM) algorithm, (2) generating the probability map, (3) obtaining the frequency spectrum of the probability map, (4) determining whether an image has been tampered based on the periodicity characteristics of the spectrum. The experimental results show that our approach is effective to detect three different image forgeries: (a) air-brush or brush strokes, (b) different blurring filters, and (c) composite image taken from different cameras.		Tim-kun Lin;Chung-Lin Huang	2009		10.1007/978-3-642-10467-1_94	image restoration;computer vision;binary image;expectation–maximization algorithm;computer science;theoretical computer science;machine learning;digital image processing;pattern recognition;mathematics	Vision	54.808946029955365	-61.69590163584606	131030
1172fdb41355a377de4fd86f62b324ee23a17c87	topographic mapping of astronomical light curves via a physically inspired probabilistic model	cost function;posterior probability;topographic map;probabilistic model;topographic mapping;eclipsing binary stars;map estimation;physical model;expectation maximisation;eclipsing binaries	We present a probabilistic generative approach for constructing topographic maps of light curves from eclipsing binary stars. The model defines a low-dimensional manifold of local noise models induced by a smooth non-linear mapping from a low-dimensional latent space into the space of probabilistic models of the observed light curves. The local noise models are physical models that describe how such light curves are generated. Due to the principled probabilistic nature of the model, a cost function arises naturally and the model parameters are fitted via MAP estimation using the Expectation-Maximisation algorithm. Once the model has been trained, each light curve may be projected to the latent space as the the mean posterior probability over the local noise models. We demonstrate our approach on a dataset of artificially generated light curves and on a dataset comprised of light curves from real observations.	expectation–maximization algorithm;experiment;hydrogen darkening;loss function;map;mathematical optimization;nonlinear system;refinement (computing);starflight;statistical model;topography	Nikolaos Gianniotis;Peter Tiño;Steve Spreckley;Somak Raychaudhury	2009		10.1007/978-3-642-04274-4_59	topographic map;generative topographic map;optics;physics;statistics;remote sensing	ML	54.090923707590804	-53.92181539388899	131133
47428eb92a4e830869cf29f3d60b0734f1b019cd	robust stereo matching using adaptive normalized cross-correlation	adaptive normalized cross correlation measurement stereo matching algorithm image color values color consistency assumption radiometric variation factor illumination direction factor illuminant color factor imaging device change factor;image color values;stereo image processing image colour analysis image matching;color consistency assumption;degradation;robustness radiometry cameras geometry layout lighting degradation color image databases costs;illumination;image databases;imaging device change factor;color;image matching;adaptive normalized cross correlation measurement;gamma correction stereo matching color radiometric variation illumination camera exposure;geometry;image indexing;layout;radiometry;stereo matching;image colour analysis;stereo image processing;illumination direction factor;illuminant color factor;algorithms color image enhancement image processing computer assisted pattern recognition automated photogrammetry;radiometric variation;robustness;radiometric variation factor;lighting;stereo matching algorithm;camera exposure;gamma correction;cameras;color image;normalized cross correlation	A majority of the existing stereo matching algorithms assume that the corresponding color values are similar to each other. However, it is not so in practice as image color values are often affected by various radiometric factors such as illumination direction, illuminant color, and imaging device changes. For this reason, the raw color recorded by a camera should not be relied on completely, and the assumption of color consistency does not hold good between stereo images in real scenes. Therefore, the performance of most conventional stereo matching algorithms can be severely degraded under the radiometric variations. In this paper, we present a new stereo matching measure that is insensitive to radiometric variations between left and right images. Unlike most stereo matching measures, we use the color formation model explicitly in our framework and propose a new measure, called the Adaptive Normalized Cross-Correlation (ANCC), for a robust and accurate correspondence measure. The advantage of our method is that it is robust to lighting geometry, illuminant color, and camera parameter changes between left and right images, and does not suffer from the fattening effect unlike conventional Normalized Cross-Correlation (NCC). Experimental results show that our method outperforms other state-of-the-art stereo methods under severely different radiometric conditions between stereo images.	area striata structure;color;computer stereo vision;cross-correlation;extraction;imaging device;impacted tooth;metric;mutual information;neural correlates of consciousness;pattern matching;physical object;population parameter;requirement;video tracking;algorithm;registration - actclass	Yong Seok Heo;Kyoung Mu Lee;Sang Uk Lee	2011	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2010.136	computer stereo vision;computer vision;computer science;lighting;mathematics	Vision	54.26195223305835	-56.1892622912322	131228
924826e41263702e34045d5b5890b64a057eaa95	adaptive texture energy measure method	adaptive image process;texture energy measure;machine vision;feature extraction	Recent developments in image quality, data storage, and computational capacity have heightened the need for texture analysis in image process. To date various methods have been developed and introduced for assessing textures in images. One of the most popular texture analysis methods is the Texture Energy Measure (TEM) and it has been used for detecting edges, levels, waves, spots and ripples by employing predefined TEM masks to images. Despite several successful studies, TEM has a number of serious weaknesses in use. The major drawback is; the masks are predefined therefore they cannot be adapted to image. A new method, Adaptive Texture Energy Measure Method (aTEM), was offered to overcome this disadvantage of TEM by using adaptive masks by adjusting the contrast, sharpening and orientation angle of the mask. To assess the applicability of aTEM, it is compared with TEM. The accuracy of the classification of butterfly, flower seed and Brodatz datasets are 0.08, 0.3292 and 0.3343, respectively by TEM and 0.0053, 0.2417 and 0.3153, respectively by aTEM. The results of this study indicate that aTEM is a successful method for texture analysis.	advanced visualization studio;computer data storage;futures studies;image quality;sensor;texture filtering;texture mapping;variable shadowing	Ömer Faruk Ertugrul	2014	CoRR	10.11648/j.ijiis.20140302.11	image texture;computer vision;machine vision;feature extraction;computer science;machine learning	Vision	60.35260675281832	-65.71672252336568	131692
6731e89ce0c4155128c3888017d6d8d9ad87624f	measurement of a container crane spreader under bad weather conditions by image restoration	degradation;bad weather conditions;position measurement anti sway bad weather conditions container crane degradation model image restoration;freight handling;degradation model;measurement systems;measurement system;image restoration meteorology cameras containers estimation cranes degradation;image restoration;trolleys;trolleys cameras containers cranes feature extraction freight handling image restoration measurement systems;container crane;estimation;feature extraction;light intensity;anti sway;position measurement;weather condition;cranes;reference mark image extraction container crane spreader measurement accuracy bad weather condition image restoration antisway system cargo handling camera based measurement system crane trolley;meteorology;cameras;containers	Recently, anti-sway systems for container crane spreaders have played an important role in efficiently handling cargo at terminals. A camera-based measurement system for the anti-sway systems, which consists of two reference marks on the upper surface of the spreader and a camera mounted on the crane's trolley, has been developed. However, the quality of the reference marks in images taken in bad weather such as mist, fog, and rain is degraded, and consequently, the measurement accuracy also deteriorates. To preserve measurement accuracy in bad weather, a rapid method for image restoration is needed. We propose a method that restores the quality of reference marks in images by modeling the degradation of the light intensity. The two constants in the model are estimated from the variances and averages of the intensities in two reference mark images extracted from two images taken at different heights of the spreader. The height of the spreader, as measured by the system, is also fed back into the restoration. The measurement accuracy attained by the method under simulated bad weather conditions is compared with the accuracy achieved in normal conditions. It is found that measurement accuracy is preserved, with only small errors of up to 1.39 mm under a swaying motion. The processing time of the method is 0.83 ms on average, which does not affect the measurement period of the system.	circuit restoration;distance fog;elegant degradation;image restoration;system of measurement	Hideki Kawai;Young-Bok Kim;Yongwoon Choi	2012	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2011.2161830	computer vision;simulation;computer science;engineering;system of measurement;forensic engineering;physics;statistics	Vision	65.50726184874513	-57.85377615870028	131952
e85a874be135655aff4d52189655a4d50ff38651	reflection removal for stele images via sparse signal decomposition	shape;image edge detection;mathematical model;signal resolution;robustness;tv;conferences	Steles are most important historical records of human civilizations, and most of them are collected by museums. Stele images are usually captured with flash compensation to the low-light condition of museum, resulting in annoying reflection spots due to the non-lambertian reflectance of polished stone steles. Observing that the reflection spots are bright at the center but fade away from the center, we propose modeling reflection spots using the sum of Gaussian (SoG) functions. As the content of steles is often characters or drawings, the textures are mainly contours and edges, which suggest a sparse distribution on the gradient domain. Based on these observation, we propose a signal decomposition model for the removal of reflection spots, decomposing the stele image into three components: total variation (TV) component, SoG component, and reference component. The derived optimization is minimized via an alternating direction method (ADM). Experimental results show that our method achieves promising reflection removing results compared with other two segmentation algorithms.	algorithm;contour line;diffuse reflection;gradient;lambertian reflectance;mathematical optimization;polynomial;reflection (computer graphics);sparse matrix	Jingyu Yang	2015	2015 Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2015.7457908	computer vision;telecommunications;shape;mathematical model;robustness;computer graphics (images)	Vision	59.30606069153993	-53.573496597254476	132140
964b82cb50972f332faa942ccc6160ac44fee8b2	context-based filtering of document images	filtering;condition dependence;enhancement;optical character recognition;document images;context based statistical modeling;statistical model;image enhancement;image quality;compression;noise removal	Two statistical context-based filters are introduced for the enhancement of binary document images for compression and recognition. The Simple Context Filter unconditionally changes uncommon pixels in low information contexts, whereas the Gain-Loss Filter changes the pixels conditionally depending on whether the gain in compression outweighs the loss of information. The filtering methods alleviate the loss in compression performance caused by digitization noise while preserving the image quality measured as the OCR accuracy. The Gain-Loss Filter reaches approximately the compression limit estimated by the compression of the noiseless digital original.	image quality;optical character recognition;pixel	Eugene I. Ageenko;Pasi Fränti	2000	Pattern Recognition Letters	10.1016/S0167-8655(00)00011-8	data compression;image quality;filter;statistical model;computer vision;speech recognition;computer science;pattern recognition;optical character recognition;compression;statistics	Vision	57.64900888049345	-65.23588330592601	132355
2288f7537def24cb73309a15230e25c71a5f4d29	hole filling through photomontage		To fill holes in photographs of structured, man made environments, we propose a technique which automatically adjusts and clones large image patches that have similar structure. These source patches can come from elsewhere in the same image, or from other images shot from different perspectives. Two significant developments of this work are the ability to automatically detect and adjust source patches whose macrostructure is compatible with the hole region, and alternately, to interactively specify a user’s desired search regions. In contrast to existing photomontage algorithms which either synthesize microstructure or require careful user interaction to fill holes, our approach handles macrostructure with an adjustable degree of automation.	algorithm;artifact (software development);coherence (physics);color histogram;cut (graph theory);image rectification;interactive media;iterative method;library (computing);mathematical optimization;pixel;vanishing point	Marta Wilczkowiak;Gabriel J. Brostow;Ben Tordoff;Roberto Cipolla	2005		10.5244/C.19.52	library science;engineering;multimedia;world wide web	Graphics	58.784568909076384	-54.035950633909486	132393
13063b9ee44676764061ce74b39d4b9b56e99146	learning-based super-resolution of 3d face model	image sampling;image resolution;image resolution image reconstruction face recognition predictive models laplace equations computer science image databases image generation target recognition prediction algorithms;usf humanid 3d face database 3d face model learning based super resolution higher resolution image resampling cylindrical representation;face recognition;image representation;image sampling image resolution image representation face recognition visual databases;face modeling;super resolution;visual databases	Super resolution technique could produce a higher resolution image than the originally captured one. However, nearly all super-resolution algorithms arm at 2D images. In this paper, we focus on generating the 3D face model of higher resolution from one input of 3D face model. In our method, the 3D face models firstly are all regularized via resampling in cylindrical representation. The super resolution then performs in the regular domain of cylindrical coordinate. The experiments using USF HumanID 3D face database of 137 3D face models are carried out, and demonstrate the presented algorithm is promising.	3d film;algorithm;experiment;resampling (statistics);super-resolution imaging	Shiqi Peng;Gang Pan;Zhaohui Wu	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1530072	facial recognition system;computer vision;image resolution;computer science;machine learning;pattern recognition;three-dimensional face recognition;sub-pixel resolution;superresolution	Vision	56.00759699647471	-54.33844142666513	132408
5a4e376f9655d80d075e4fab53ecc50a11c881d5	least square based view synthesis prediction for multi-view video coding	multi view video coding;leastsquare;view synthesis;free view tv	In the applications of Free View TV, pre-estimated depth information is available to synthesize the intermediate views as well as to assist texture video coding. Existing view synthesis prediction schemes generate virtual view picture only from interview pictures. However, there are many types of signal mismatches caused by depth errors, camera heterogeneity or illumination difference across views, and these mismatches decrease the prediction capability of virtual view picture. In this paper, we propose a least square based view synthesis prediction method to enhance the prediction capability of virtual view picture. This method integrates least square estimation with backward warping to synthesize the virtual view picture, which not only utilizes the adjacent views information but also the temporal information. Experiments show that the proposed method reduces the bitrate by up to 23% relative to the multi-view video coding standard, and about 16% relative to the conventional view synthesis prediction method.	data compression;view synthesis	Jinhui Hu;Ruimin Hu;Zhongyuan Wang;Mang Duan;Rui Zhong;Zhen Han	2012		10.1007/978-3-642-34778-8_22	computer vision;simulation;computer science;multimedia	Vision	58.18389913655389	-55.76292596192935	132493
b37d9168fa001a9e85d84d041384ee2fbe68facb	illuminant color estimation method based on pseudo-detection of specular reflection		White balance (WB) is the correction process to remove unrealistic color casts caused by an illuminant color. For high-quality image correction, it is important to accurately estimate the illuminant color. In this study, we propose an accurate illuminant color estimation method for WB based on pseudo-detection of specular reflection. In the proposed method, pixels which are brighter than their neighborhood and their neighboring darker pixels are found at first. Then, the averages of their pixel values are calculated respectively, and then the specular reflection is detected approximately as the difference of the average values. Finally, the illuminant color is estimated as the specular reflection itself. The accurate estimation capability of the proposed method is verified through a series of comparative experiments with typical WB methods.	experiment;pixel	Daiki Moriyama;Yoshiaki Ueda;Takanori Koga;Noriaki Suetake;Eiji Uchino	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451423	computer vision;standard illuminant;color balance;brightness;pixel;artificial intelligence;computer science;specular reflection;colors of noise	Robotics	56.21185802132991	-60.156596030132356	132553
5cc5ae59908ac7a68153bd73640e382867d476c4	blockiness and blurriness measurement for video quality assessment				Muhammad Tahir Qadri	2012				Metrics	61.97249998160008	-62.86866364249967	132598
615f6c3622c615305ca5e013e9e7dc34720f0403	a fusion-based blind image quality metric for blurred stereoscopic images		Blur is certainly one of the most encountered and the most annoying degradation types in image. It is due to several causes such as compression, motion, filtering and so on. In order to estimate the quality of this kind of degraded images, several metrics have been proposed in the literature. In this paper, we focus our attention on stereoscopic images and we propose a fusion-based blind stereoscopic image quality metric for blur degradation. In order to characterize the considered degradation type, some relevant features are first computed. Note that these features are extracted from a cyclopean image (CI) derived from the stereoscopic image. The final index quality is given by combined all features through a Support Vector Machine (SVM) model used as a regression tool. The 3D LIVE and the IEEE image databases have been used to evaluate our method. The achieved performance has been compared to the state-of-the-art.	data compression;database;elegant degradation;filter (signal processing);gaussian blur;image quality;stereoscopy;support vector machine	Aladine Chetouani	2017	2017 International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)	10.1109/ATSIP.2017.8075530	image restoration;automatic image annotation;cyclopean image;image processing;image quality;feature extraction;image texture;computer vision;artificial intelligence;feature detection (computer vision);computer science;pattern recognition	Vision	61.76991432615171	-64.37840785633743	132743
3c7b6fd46ffc5c5f30220d009999d82b9b1b68f9	color enhancement based on the anchoring theory	color enhancement method anchoring theory multimedia systems color reproduction perceptual quality backlight scaled images liquid crystal display lightness perception anchoring theory;liquid crystal displays;multimedia systems;multimedia systems image colour analysis image enhancement liquid crystal displays;image enhancement;image colour analysis;image color analysis degradation adaptation models predictive models liquid crystal displays psychology brightness	Providing consistent viewing experience across different reproduction conditions is an important issue for multimedia systems in the real world. In this paper, we propose a method to improve the perceptual quality of color reproduction for backlight-scaled images displayed on a liquid crystal display. Supported by the anchoring theory of lightness perception developed in psychology, this method is able to enhance the color appearance of images even when the backlight is only 5% of the original intensity. The goal is to make the appearance of the resulting images as close as possible to the original images illuminated with full backlight. The concept behind the proposed color enhancement method is general enough for many other applications. The effectiveness of the method is verified by subjective experiments.	backlight;color;computational complexity theory;experiment;liquid-crystal display;real-time clock	Kuang-Tsu Shih;Homer H. Chen	2013	2013 IEEE 15th International Workshop on Multimedia Signal Processing (MMSP)	10.1109/MMSP.2013.6659280	rgb color model;computer vision;computer science;operating system;liquid-crystal display;multimedia;computer graphics (images)	Robotics	60.51420696326069	-61.18821601240179	132762
dc615cd01ba86ec7221c16898088c53426b171ac	efficient high dynamic range texture compression	texture;color space;image;graphics hardware;hdr;image quality metric;image quality;high dynamic range;compression;hardware implementation	We present a novel compression method for high dynamic range (HDR) textures, targeted for future graphics hardware. Identifying that the existing solutions provide either very high image quality or very simple hardware implementation, we aim to achieve both features in a single solution.  Our approach improves upon an existing technique by incorporating a simple chrominance coding that allows overall image quality on par with the state of the art, but at a substantially lower encoding and decoding complexity. The end result is what we believe to be an excellent compromise between image quality and efficiency of hardware implementations.  We evaluate our compression method using common test images and established HDR image quality metrics. Additionally, we complement these results with error measurements in the CIE L*a*b* color space in order to separately assess the quality of luminance and chrominance information.	color space;computability in europe;graphics hardware;high dynamic range;image quality;texture compression	Kimmo Roimela;Tomi Aarnio;Joonas Itäranta	2008		10.1145/1342250.1342282	image quality;computer vision;simulation;computer science;image;texture;color space;texture compression;graphics hardware;compression;computer graphics (images)	Graphics	61.966943584135244	-63.05384448916741	132945
2eaa8c99e8494d77578a9d99cc328ed52fe8c7d2	relativistic ultrafast rendering using time-of-flight imaging	ultrafast optics;time scale;time of flight;time resolved imaging;time resolved;streak sensor;signal to noise ratio;relativistic effects	We capture ultrafast movies of light in motion and synthesize physically valid visualizations. The effective exposure time for each frame is under two picoseconds (ps). Capturing a 2D video with this time resolution is highly challenging, given the low signal-to-noise ratio (SNR) associated with ultrafast exposures, as well as the absence of 2D cameras that operate at this time scale. We re-purpose modern imaging hardware to record an average of ultrafast repeatable events that are synchronized to a streak tube, and we introduce reconstruction methods to visualize propagation of light pulses through macroscopic scenes.	signal-to-noise ratio;software propagation	Andreas Velten;Di Wu;Adrian Jarabo;Belén Masiá;Christopher Barsi;Everett Lawson;Chinmaya Joshi;Diego Gutierrez;Moungi Bawendi;Ramesh Raskar	2012		10.1145/2343045.2343100	time of flight;simulation;streak camera;relativistic quantum chemistry;signal-to-noise ratio;quantum mechanics;computer graphics (images)	Vision	62.77337313789305	-52.70937113779381	132962
658d6a6465fde593ecd8336588aac027688a1b63	focus measurement on programmable graphics hardware for all in-focus rendering from light fields	focusing;data transmission;paper;image processing;directx 9;interactive rendering focus measurement programmable graphics hardware photorealistic images virtual reality graphics processing units numerical computation image processing light field rendering image based rendering in focus rendering directx 9 optimization data transmission latency video memory system memory;realistic images rendering computer graphics computer graphic equipment virtual reality image processing;photorealistic images;light field;focus measurement;computer graphic equipment;virtual reality;ati;data communication;optimization problem;image generation;system memory;interactive rendering;data transmission latency;graphics processing units;numerical computation;light field rendering;graphics hardware rendering computer graphics focusing virtual reality central processing unit image processing delay data communication image generation;ati radeon 9800 pro;graphic processing unit;realistic images;programmable graphics hardware;optimization;computer science;image based rendering;rendering computer graphics;video memory;in focus rendering;3d graphics and realism;graphics;central processing unit;directx;hardware;rendering	This paper deals with a method for interactive rendering of photorealistic images, which is a fundamental technology in the field of virtual reality. Since the latest graphics processing units (GPUs) are programmable, they are expected to be useful for various applications including numerical computation and image processing. This paper proposes a method for focus measurement on light field rendering using a GPU as a fast processing unit for image processing and image-based rendering. It is confirmed that the proposed method enables interactive all in-focus rendering from light fields. This is because the latest DirectX 9 generation GPUs are much faster than CPUs in solving optimization problems, and a GPU implementation can eliminate the latency for data transmission between video memory and system memory. Experimental results show that the GPU implementation outperforms its CPU implementation.	central processing unit;computation;computer graphics;directx;graphics hardware;graphics processing unit;image processing;light field;mathematical optimization;numerical analysis;video ram (dual-ported dram);virtual reality	Kaoru Sugita;Keita Takahashi;Takeshi Naemura;Hiroshi Harashima	2004	IEEE Virtual Reality 2004	10.1109/VR.2004.39	optimization problem;computer vision;tiled rendering;image-based modeling and rendering;3d rendering;computer hardware;rendering;image processing;computer science;graphics;light field;central processing unit;parallel rendering;real-time computer graphics;virtual reality;directx;real-time rendering;texture memory;alternate frame rendering;volume rendering;software rendering;data transmission;computer graphics (images)	Visualization	66.95673105378324	-52.93751832902648	133015
61ba6a48b1441ae8a0c3f6874b32a24d88dc1e6b	an adaptive spatial filter for early depth test	adaptive filters;engines;history;testing;filtering;hardware;mask;bandwidth;pipelines;spatial filtering;memory bandwidth	In this paper, we extend our previous work which is a new method for early depth test in a 3D rendering engine. We add a filter stage to the rasterizer in the 3D rendering engine, in an attempt to identify and avoid the occluded pixels. This filtering block determines if a pixel is hidden by a certain plane. The plane is a mask having the history of a pixel's appearances in front of it. If a pixel is hidden by the plane, it can be removed. The filter adaptively updates its position from an initial position in order to find a maximum rejection ratio. The simulation results show that the filter reduces the number of pixels to the next stage up to 71.7%. As a result, 67% of memory bandwidth is saved with simple extra hardware in the 3D rendering engine.	3d rendering;layout engine;memory bandwidth;pixel;rasterisation;rejection sampling;simulation;z-buffering	Chang-Hyo Yu;Lee-Sup Kim	2004	2004 IEEE International Symposium on Circuits and Systems (IEEE Cat. No.04CH37512)		adaptive filter;computer vision;tiled rendering;real-time computing;rendering;computer science;alternate frame rendering;memory bandwidth;spatial filter;computer graphics (images)	Arch	65.85048113021195	-52.47907915279448	133110
8ea10c1b6001c435920843765acac69f7ab116ac	a virtual wiper - restoration of deteriorated images by using multiple cameras	image restoration;interference suppression;random noise;image registration virtual wiper deteriorated image restoration multiple cameras outdoor environment adherent noises lens protecting glass noise elimination;random noise image restoration interference suppression image registration cameras feature extraction;feature extraction;image registration;image compositing;image restoration cameras layout working environment noise robot vision systems glass lenses surveillance automotive components interpolation;cameras	In this paper, we propose a new method for the restoration of deteriorated images by using multiple cameras. In outdoor environment, it is often the case that scenes taken by the cameras are hard to see because of adherent noises on the surface of the lens-protecting glass of the cameras. Our proposed method analyses multiple camera images describing the same scene, and synthesizes an image in which adherent noises are eliminated. key words: adherent noise, noise elimination, image restoration, image compositing, multiple cameras	circuit restoration;compositing;image restoration	Atsushi Yamashita;Masayuki Kuramoto;Toru Kaneko;Kenjiro T. Miura	2003		10.1109/IROS.2003.1249637	image restoration;computer vision;feature extraction;computer science;engineering;image registration;optics;computer graphics (images)	Vision	57.7001362807098	-57.584482567094504	133119
51fab52d13eb0d09395135d159b17db704d5c80b	improving piecewise-linear registration through mesh optimization	piecewise linear;mesh optimization;triangular mesh;sum of squares;mutual information;normalized cross correlation	Piecewise-linear methods accomplish the registration by dividing the images in corresponding triangular patches, which are individually mapped through affine transformations. For this process to be successful, every pair of corresponding patches must lie on projections of a 3D plane surface; otherwise, the registration may generate undesirable artifacts, such as broken lines, which diminish the registration quality. This paper presents a new technique for improving the registration consistency by automatically refining the topology of the corresponding triangular meshes used by this method. Our approach iteratively modifies the connectivity of the meshes by swapping edges. For detecting the edges to be swapped, we analyze the local registration consistency before and after applying the action, employing for that the mutual information (MI ), a metric for registration consistency significantly more robust than other well-known metrics such as normalized cross correlation (NCC ) or sum of square differences (SSD). The proposed method has been successfully tested with different sets of test images, both synthetic and real.	cross-correlation;mutual information;neural correlates of consciousness;paging;sensor;solid-state drive;synthetic intelligence;triangle mesh;triangulated irregular network	Vicente Arévalo;Javier González	2007		10.1007/978-3-540-72849-8_16	mathematical optimization;combinatorics;discrete mathematics;piecewise linear function;triangle mesh;cross-correlation;laplacian smoothing;explained sum of squares;mutual information;statistics	Vision	53.81227766992061	-58.31026216471336	133133
462e6c8c4d14c983a973707085e7e322d8b5d6a7	dibr-synthesized image quality assessment via statistics of edge intensity and orientation		Depth-image-based-rendering (DIBR) is a popular technique for view synthesis. The rendering process mainly introduces artifacts around edges, which leads to degraded quality. This letter proposes a DIBR-synthesized image quality metric by measuring the Statistics of both Edge Intensity and Orientation (SEIO). The Canny operator is first used to detect edges. Then the gradient maps are calculated, based on which the intensity and orientation of the edge pixels are computed for both the reference and synthesized images. The distance between the two intensity histograms and that between the two orientation histograms are computed. Finally, the two distances are pooled to obtain the overall quality score. Experimental results demonstrate the advantages of the presented method. key words: quality evaluation, DIBR, edge intensity, edge orientation	canny edge detector;edge enhancement;gradient;image quality;map;pixel;view synthesis	Yu Zhou;Leida Li;Ke Gu;Zhaolin Lu;Beijing Chen;Lu Tang	2017	IEICE Transactions		computer science;artificial intelligence;pattern recognition;computer vision;image quality	Vision	58.110971695406036	-62.28817393443191	133161
9ca646a13dadf81fece60bad0a2f89e1423290b5	regularizing phase-based stereo	phase difference;disparity estimation;gabor filters pixel frequency phase estimation stereo image processing phase measurement markov random fields lattices image generation computer graphics;markov random field;computer graphic;gabor filter;image reconstruction phase based stereo images complex valued gabor filter phase differences disparity map smoothness constraint markov random fields probabilistic lattice model regularization;image reconstruction;lattice model	Wavelet-based techniques proved to be a promising approach for estimating the disparity between two stereo images. The complex-valued Gabor f ilter responses reduce the ambiguity of raw image intensities, and their phase differences between left and right image provide a direct measure for the disparities. Experience shows that such phasebased measurements are reliable near edges but yield poor results between them. To improve these phase-based disparity estimations, a regularization scheme is proposed which directly compares possible matching pairs. Unreliable regions are f illed by using a simple smoothness constraint. In the spirit of Markov random f ields, we propose a probabilistic lattice model which describes the complete disparity distribution instead of representing only a single conf iguration. Experimental results are presented for an artif icial image pair generated by computer graphics.	algorithm;binocular disparity;computer graphics;depth map;distortion;experiment;lattice model (physics);markov chain;matrix regularization;raw image format;scott continuity;sparse matrix;wavelet	Thorsten Fröhlinghaus;Joachim M. Buhmann	1996		10.1109/ICPR.1996.546067	iterative reconstruction;computer vision;mathematical optimization;lattice model;pattern recognition;mathematics	Vision	54.973291475139405	-55.286733458991385	133167
d965dc8bbeece032036546f84529021742c52fe5	color palette reduction and enhancement techniques	naval planning;data compression;platforms;luminance;augmentation;missions;data bases;colors;digital maps;marine corps aircraft;indexes;contrast;naval research;pixels;algorithms;optical storage;tools;reprints;images;graphics;ratios;aircraft;charts	"""index in the algebraically-defined palette. Unforntmately, color degradation becomes very apparent when the reThe Naval Research Laboratory's Map Data Formatting duced palette reaches about half its original size. SpecifiFacility is developing a seamless, compressed, digital cally, when the reduced palette becomes small (i.e., under database ofthe world called the Compressed Aeronautical 128 colors), significant color shifts appear. However, Chart (CAC). The CAC consists of scanned, aeronautical algebraic remapping is a viable method of creating a chart images. The CAC's primary purpose is to support reduced color set for displaying CAC data on 8-bit color a Navy and Marine Corps aircraft mission planning and systems that reserve more than 16 colors. Algebraic digital moving map systems. However, other users on remapping preserves much of the color information in the i various platforms have expressed interest in using the CAC data when the reduced palette is at least half the size 0 CAC. Therefore, the CAC must be capable of adapting to of the original CAC palette. The CAC color palette the graphics capabilities of other platforms.' consists of 240 colors, leaving only 16 colors available to 4 The CAC uses a set of custom color palettes that the graphic display. consists of 240 distinct colors. Each pixel from the An enhancement technique was developed to help precompressed data at full color (24-bit) indexes to its compensate for color shifts that tend to blend similar closest color palette entry as decided by its Euclidean colors. A contrast stretch algorithm (CSA) was used. The distance. Custom color palettes tend to maintain color CSA maps all palette colors that are within 10% of pure integrity between the CAC and the original data. black (RGB=O,0,0) to pure black and 10% of pure white Since graphics capabilities vary from platform to plat(RGB---25,255,255) to pure white. National Television form, the ability to reduce and remap a color palette is a Standards Committee (NTSC) luminance values are calcuuseful tool. This paper describes the technique chosen to lated for a palette's colors. The CSA is then applied to the remap the CAC color palettes to the desired, reduced set of luminance values. A ratio of the CSA luminance values to colors. An enhancement technique that uses a contrast the original luminance values is calculated. The square stretch algorithm is also described. The enhancement is root of this ratio is used to scale the palette colors, resulting achieved by first calculating a given palette's luminance in a brighter (i.e., higher contrast) image. The function of (Y) values. Then, a ratio of the contrast stretch luminance the square root is to create greater contrast at low ratios as :4 to the original luminance value is derived for each color in compared to lower contrast at higher ratios which tends to a given color palette. better separate low intensity colors. Introduction Color Palette Reduction The CAC database is a library of compressed Defense The initial thrust of color palette reduction was to produce Mapping Agency (DMA) Arc Digitized Raster Graphics an acceptable product for the Army by reducing the CAC (ADRG) images. CAC is available on CDROM as a palettes from 240 to 16 colors. An outgrowth from this standard DMA product. ADRG is compressed to the CAC effort was to algebraically remap a reduced set of colors to in order to meet specifications of the digital moving map adapt to 8-bit color systems requiring a large number of systems on-board the F/A-I8 and AV8B aircraft. Howreserved slots in a user-loadable palette. ever, the CAC has also been used by the National Security The authors have developed a method of creating an Agency, Federal Emergency Management Agency and algebraic palette, based upon shades ofred, green, and blue other mission planning and geographic information sys(RGB). The algebraic palette consists of the centroid tems. values of sub-rectangles formed in RGB space. For exIn order to increase its usage, the graphics requirement ample, a palette based on four red shades, four green of the CAC needs to be flexible. This issue was first shades, and eight blue shades produces 128 colors (4x4x8 addressed with the Army's requirement for a 4-bit (16 color vectors). Eight bits of color in CAC provide 256 color) compressed map database. Work began to develop shades. In this example, sub-rectangles in RGB space are a method of displaying the CAC in 4-bit color without formed by dividing the red and green vectors by four (256/ having to recompress the source data. A simple method of 4 = 64) and the blue vector by eight (256/8 = 32). This remapping the CAC palette colors to an algebraicallyvectoring process produces sub-rectangles of (64x64x32 defined palette in RGB space was chosen. This method shades) where the centroid value for each sub-rectangle allows the user to define a number of shades of red, green, becomes an entry in the algebraic color palette. andblue to find the total number of algebraic palette colors For example, a Sun Spare Server 300 with 8-bit color to be built. The overhead in this method ofcolor reduction graphics, reserves, by default, 24 slots in a palette. Thereis a remapping array. The remapping array reindexes the fore, the user-loadable palette cannot exceed 232 entries. original color palette entry in the CAC data to the new Since the CAC palettes may have as many as 240 entries, J 78.--S&TandS1D 'j Color Imaging Conference: Transforms & Trarsponablisy of Color (1993) V -1 7?.o --e CAC must be remapped to a smaller color palette to be ically-defined palette of 216 colors (6x6x6 color vectors). displayed on this system. With the algebraic remap, the A remap array is then created to map each of the CAC's number of palette colors is user-selectable. For example, original 240 palette colors to the reduced 216 color palette 7 the user might choose to divide the RGB cube into six levels via Euclidean distance caldculations. Thus, when the CAC each of red, green, and blue. In other words, divide each data is displayed, a pixel's color value of 237 may, for color vector (red, green, and blue) into six equal increexample, be mapped to 215 in the remap array (see Figure ments, each of which consists of 42 shades (256/6 = 42). 1). The RGB value in the algebraic palette at entry 216 is, The centroid ofeach cube (42x42x42 shades) is defined as in turn, displayed for that pixel on the screen. the algebraic color palette entry, resulting in an algebraS# 1I R I B -T--B# """"-l R I G [ JB FiI _# II S1 0 0 0 21 21 21 1 1 2 8 22 12 2 63 21 21 2 1 3 30 15 45 3 21 63 21 3 4 4 48 29 20 4 21 21 63 4 2 237 235 218 188 213 189 231 231 237 215 238 216 223 226 214 231 189 231 238 216 239 226 205 245 215 231 231 189 239 214 240 255 255 255 216 231 231 231 240 216 Original 240 entry Algebraic 216 entry Remap Array CAC Color Palette Color Palette Figure 1. Palette Remapping based on RGB Euclidean Distances Color Palette Enhancement original luminance over the """"stretched' luminance values. Each palette color (RGB) is then scaled based upon the * While the CAC custom color palettes are designed to square root of its original/stretched luminance ratio. The maintain the original source data color, some loss does square root of the ratio is used to increase separation occur. This loss results during color compression from 24between palette colors with small ratios and decrease bit to 8-bit color. 2 Enhancement of the color palette is a separation at high ratios (see Figure 2). quick and simple means of altering the appearance of the displayed image without modifying the CAC data itself Comment Calculate luminance values and determine high and Enhancing the CAC palette brightens the image, and restors low thresholds. some ofthe color loss. One negative side effect ofenhancDo i = L, 240 * OriginalLuminance (i) 0.3R(i) + 0.59G3(i) + 0.1 IB(i) ing an image in this way is the possiblity of enhancing the If(OriginalLuminance (i) < (. 10 x 255)) then noise. The least possible amount ofnoise would best serve Number ofThresholdLow=Number ofThresholdLow+1 the needs ofa pilot using the CAC with an on-board moving OriginalLuminance (i) = 0 map display system. Threrefore, this enhancement techEndif nique may not be appropriate for use in the cockpit. HowIf (Original Luminance (i) > (255 -(.10 x 255))) then ever, the enhancement is useful for creating brighter imNumber ofThresholdHi = NumberofThresholdHi + 1 ages for mission planning and hardcopy output. Original-Luminance (i) = 0 The enhancement technique described here uses the Endif CSA. First, the NTSC luminance value (Y) of each palette Enddo color is calculated, based on equation (1).3 Sort OriginalLuminance (240) ! Sort in ascending order NewMax = 240 Number Of ThresholdHi NewMin = Numberof_Threshold Low Y = 0.30R + 0.59G + 0.1 II B () StepSize = 256.0 / (NewMax NewMin) The resultant luminance values are sorted by increasStepValue = 0.0 ing intensity. Thresholds are set to 10% to map the Comment: Calculate CSA luminance values. corresponding luminance values to either pure black or Do i = NewMin, NewMax pure white. The step size is found by dividing 256 (i.e., the StretchedLuminance (i) = StepValue number of shades in 8-bit color) by the luminance values StepValue = StepValue + StepSize that are not within the thresholds. Enddo A""""stretched"""" luminance array is then calculatedbased upon the step size. Ratios are calculated between the IS& TandSID s Color Imaging Conference: Transforms & Transporiabalaty of Color (1993)--179 Comment: Calculate ratio between original and CSA luminance the larger sub-rectangles. The enhancement techmique values, apply square root of ratio to palette colors. generates greater contrast in the CAC. This technique may Doi 1, 240 be applied to either the original or algebraically-defined ati)-Floal(Strethed-Lumnnance(i)iOriginal-Luminance(i)) color palette. The appearance ofnoise due to the enhance. ScalinFwactl"""	24-bit;4-bit;8-bit color;algorithm;cd-rom;certified server validation;color depth;common access card;direct memory access;elegant degradation;euclidean distance;geographic information system;image noise;inferring horizontal gene transfer;linear algebra;map;ntsc;on-board data handling;overhead (computing);palette (computing);pixel;raster graphics;resultant;seamless3d;sorting;source data;thrust;web colors	Michael E. Trenchard;Lancelot M. Riedlinger;Stephanie A. Myrick;Marlin L. Gendron	1993			data compression;database index;rgb color model;computer vision;simulation;digital mapping;optical storage;color depth;contrast;computer science;graphics;chart;ratio;luminance;color cycling;pixel;statistics;computer graphics (images)	Graphics	59.73773078009989	-62.691473426187876	133597
5b1c67b6d54c8bcb31fd4c9c33bb4c71c312998a	a no-reference metric for demosaicing artifacts that fits psycho-visual experiments	signal image and speech processing;settore inf 01 informatica;quantum information technology spintronics	The present work concerns the analysis of how demosaicing artifacts affect image quality and proposes a novel no-reference metric for their quantification. This metric that fits the psycho-visual data obtained by an experiment analyzes the perceived distortions produced by demosaicing algorithms. The demosaicing operation consists of a combination of color interpolation (CI) and anti-aliasing (AA) algorithms and converts a raw image acquired with a single sensor array, overlaid with a color filter array, into a full-color image. The most prominent artifact generated by demosaicing algorithms is called zipper. The zipper artifact is characterized by segments (zips) with an On–Off pattern. We perform psycho-visual experiments on a dataset of images that covers nine different degrees of distortions, obtained using three CI algorithms combined with two AA algorithms. We then propose our no-reference metric based on measures of blurriness, chromatic and achromatic distortions to fit the psycho-visual data. With this metric demosaicing algorithms could be evaluated and compared. Introduction Image quality is difficult to assess correctly for a number of reasons [1]. Firstly, image quality is perceptual by nature. This makes it hard to measure it in a standardized way and allows for personal preferences. Secondly, it can vary widely between different application domains. Due to its perceptual nature, image quality should be evaluated through a subjective assessment, and quality metrics should be designed to fit quality judgments collected by psycho-visual experiments. The efficiency of studies that involve people’s judgments is very low compared to a computerized objective study. Nevertheless, to validate automated approaches, psycho-visual scaling studies are insurmountable for image quality research [2,3]. We are interested in developing a pool of no-reference (NR) metrics to automatically assess the performance of the algorithms composing the image generation pipeline of digital cameras. In particular, we are interested in defining these metrics so that they fit the psycho-visual data. A general three-step approach to design and develop these types of metrics has been given by Bartleson [4]: *Correspondence: gasparini@disco.unimib.it 1Department of Informatics, Systems and Communication, viale Sarca 336, University of Milano-Bicocca, 20126 Milano, Italy Full list of author information is available at the end of the article 1. Identification of perceptual dimensions (attributes) of quality. 2. Determination of relationships between attribute scale values and objective, image based measures. 3. Combination of attribute scale values to predict overall image quality. To define a no-reference image quality metric is therefore needed to design a good psycho-visual experiment. Ideally, we should be able to generate a dataset of distorted images where the distortion can be controlled by a proper defect-generating process. In this way the collected data can be easily related to the considered distortion. In particular, what we would like to obtain is a monotone behavior of the perceived quality with respect to the increase of the distortion. Several kinds of defects can affect digital images. They can be roughly divided into [5]: • physical defects, such as out of focus, motion blur, noise, etc. • digital defects introduced by the processing pipeline, such as demosaicing, compression, etc. For physical defects the procedure adopted to generate the distorted images used within the experiments could be a simulation of the physical process, while in the case © 2012 Gasparini et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Gasparini et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:123 Page 2 of 15 http://asp.eurasipjournals.com/content/2012/1/123 of digital defects the procedure should apply the corresponding algorithm(s) within the pipeline. Note that each of these distortion processes can vary with respect to one or more parameters. Within this context, in this paper we address the problem of how demosaicing artifacts affect image quality. The demosaicing operation converts a raw image acquired with a single sensor array, overlaid with a color filter array, into a full-color image. The most prominent artifact generated by demosaicing algorithms is called zipper. The zipper artifact is characterized by segments (zips) with an On–Off pattern. The quality of rendered images depends on the perception of the zipper artifact that can also affect the sharpness. The perception of this artifact also depends on image content. We here propose a no-reference metric to assess image quality in case of demosaicing artifact that combines measures of blurriness (intended as lack of sharpness), chromatic and achromatic distortions and fits the psychovisual data. Several full-reference metrics exist for this kind of artifact [6], while the literature is poor in noreference ones. Some no-reference sharpnessmetrics [7,8] could be adopted, but they can not take into account typical chromatic and achromatic zipper effects. Liu et al. [9] have recently presented a no-reference method for CFA demosaicing based on double interpolation and have evaluated several demosaicing algorithms. However this metric has not been correlated with psycho-visual experiments. In this work we have generated a dataset with different degrees of zipper artifacts by applying a combination of three different CI algorithms with two AA algorithms. These algorithms have been applied to a set of reference images having different visual contents. More demosaicing and/or anti aliasing algorithms could have been used. However lengthy psycho-visual tests are not reliable, and we have preferred to not reduce the number of test images. This paper is organized as follows. In Demosaicing section we briefly describe the demosaicing process, while in Psycho-visual setup section we describe how we have generated the dataset utilized during our tests and the psycho-visual experiments that we have conducted to rank the chosen algorithms. From the analysis of the experimental data (detailed in Data analysis section), we propose our novel no-reference metric, described in Noreference metric for Demosaicing section, based on measures of blurriness, chromatic and achromatic distortions. In Metric parameter estimation section we report details of the regression we have proposed to fit the subjective data and we compare our metric with a reference one [9]. All the psycho-visual data presented and the corresponding distorted images are available at http://www.ivl.disco. unimib.it/. Finally, in Section Methods we report details on the testing methodology adopted here. Demosaicing To produce a color image there should be at least three color samples at each pixel location. The more expensive solution consists in using a color filter in front of each sensor, generating three full-channel color images. Thus, many modern cameras use a color filter array (CFA) in front of the sensor so that only one color is measured at each pixel. This means that to reconstruct the fullresolution image, the missing two color values at each pixel should be estimated. This process, known as demosaicing [10] is generally composed of a CI algorithm followed by an AA algorithm to reduce possible artifacts. Among various CFA patterns, the Bayer pattern was the most popular choice [11]. The Bayer array measures the green image on a quincunx grid and the red and blue images on rectangular grids, obtaining 1/2 of the pixels for the green channel, and 1/4 for both the blue and the red channels, as depicted in Figure 1. The most prominent artifact generated by demosaicing algorithms is called zipper. The zipper effect refers to abrupt or unnatural changes of color differences between neighboring pixels, manifested as an “On–Off” pattern [6]. In Figure 2 an example of an original image and two different demosaiced versions are reported. As can be seen from Figure 2b, where a typical example of demosaiced image is shown, the zipper artifacts are both chromatic and achromatic. On the other hand, demosaicing algorithms that try to mitigate this On–Off pattern, significantly blur the image (Figure 2c). Figure 1 Bayer pattern array. The array of filters of the Bayer pattern. Gasparini et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:123 Page 3 of 15 http://asp.eurasipjournals.com/content/2012/1/123 Figure 2 Original image and two different demosaiced versions. (a) Original image. (b) An example of demosaicing: the artifacts introduced can be distinguished into achromatic and chromatic zipper. (c) A different demosaicing: the image is visibly blurred to mitigate the On–Off pattern. Several algorithms for demosaicing were developed in the literature [12-17], and some of them are proprietary. A survey of these methods was presented by Li et al. [18]. Several methods deal with content adaptive demosaicing, based on an edge detection mechanism [19-21]. Recently Rehman and Shao [22] have presented a demosaicingmethod using optimised filters, based on a training process and well-defined content classification. We have here considered nine different demosaicing algorithms obtained combining three CI algorithms with two anti aliasing (AA) algorithms. The three CI algorithms adopted here are: • Bilinear interpolation [18]: it is the simplest demosaicing algorithm and acts as a benchmark; the missing values on the three channels are computed by linear interpolation independently. • ST1: proposed by Smith [23], it performs an isotropic interpolation that includes a non-linear step that minimizes the energy of aliasing artifacts. • ST2: proposed by Guarnera et al.	acutance;algorithm;aliasing;application domain;bayer filter;benchmark (computing);bilinear transform;box counting;channel (digital image);color filter array;color image;demosaicing;digital camera;digital image;distortion (economics);eurasip journal on advances in signal processing;edge detection;emoticon;experiment;fits;gaussian blur;glossary of computer graphics;image quality;image scaling;informatics;linear interpolation;missing data;newton's method;noise reduction;nonlinear system;pattern language;pixel;pokémon red;raw image format;ringing artifacts;seagate st1;simulation;software bug;spatial anti-aliasing;springer (tank);statistical classification;monotone	Francesca Gasparini;Fabrizio Marini;Raimondo Schettini;Mirko Guarnera	2012	EURASIP J. Adv. Sig. Proc.	10.1186/1687-6180-2012-123	demosaicing;computer vision;mathematics;computer graphics (images)	Graphics	60.96638327551999	-63.50884269282333	133855
7d2c7aa76a3008025005842da61fe0e60e7043d5	direct shape from texture using a parametric surface model and an adaptive filtering technique	perspective projection;shape surface texture frequency estimation adaptive filters information filtering information filters iterative algorithms data mining rate distortion theory solid modeling;filtering theory image texture parameter estimation;image texture;surface geometry;density estimation;texture images iterative shape from texture surface depth information curved object texture distortion surface geometry accurate surface shape textural irradiance equation final reconstructed shape estimation;parameter estimation;parametric surface;adaptive filter;filtering theory	In this research, we propose a new iterative shape from texture (SFT) algorithm which extracts accurate surface depth information of a curved object covered with fairly homogeneous texture directly. The shape information can be inferred from the rate of texture distortion depicted in an image, and therefore the modeling of the projection and surface geometry as well as the estimation of local texture variation are crucial in obtaining accurate surface shape of an object. By introducing semi-perspective projection camera model and a parametric surface model, we establish a new SFT problem formulation called the textural irradiance equation which relates the local texture density called textural intensity to finite surface parameters. Moreover, by adopting an adaptive multiscale filtering scheme for local texture density estimation, in which the scale or frequency band of a local edge filter is chosen adaptively according to the local shape information, we greatly enhance the accuracy of the estimation of the projected local texture densities, and the final reconstructed shape. We demonstrate the performance of the proposed algorithm by the test with several synthetic and real texture images.	adaptive filter	Kyoung Mu Lee;C.-C. Jay Kuo	1998		10.1109/CVPR.1998.698637	adaptive filter;bidirectional texture function;image texture;computer vision;mathematical optimization;perspective;density estimation;computer science;parametric surface;pattern recognition;mathematics;estimation theory;texture compression;texture filtering;projective texture mapping;statistics	Vision	53.8379040290853	-65.48462226167588	133874
fba26578988ac431458bebad2b5996fed81027ca	automatic jpeg compression using a color visual model.	visual modeling	JPEG compression is extensively used in digital cameras, internet, and image databases. The amount of compression can be adjusted by scaling the quantization table or Qtable. In many cases, an iterative process is used to achieve optimum compression having a smaller file size, but still visually lossless at the intended display and viewing distance. This process is very time consuming for large image databases since human observers are used to judge the image quality. We present an automatic method to achieve the optimum compression using a color visual difference model (CVDM). The CVDM output is a map of the visible differences between reference and distorted images. In order to use the model in automatic compression, a single number JPEG artifact score was derived from the visual difference map to be used as a merit function. A subjective experiment was conducted to find the best merit function for JPEG artifacts. The subjective experiment also derives an acceptance criterion for JPEG artifacts. We found that the 99-percentile provides the best correlation with the subjective results; thus it was used as the JPEG artifact score in the automatic compression. In the compression process, the compressed images were evaluated with the visual model. Based on the predicted artifact score, the selected Q-table was scaled up or down so that the artifact score was close to the acceptance criteria derived in the subjective experiment.	compression artifact;database;digital camera;image quality;image scaling;iteration;jpeg;lossless compression;ringing artifacts;visual modeling	Xiao-Fan Feng;Scott J. Daly	2003			computer vision;jpeg;human visual system model	Vision	61.55833814943744	-63.23178919975815	134566
ea7698b622ffd15febacea53e983719c001d7795	wavelet-based depth map estimation for light field cameras	wavelet transforms;brightness;estimation;optimization;cameras;conferences	Conventional depth estimation methods for light field cameras often fail to estimate sharp object boundaries. We propose a depth estimation method using a wavelet-based matching cost calculation and an estimation optimization, which can estimate sharp object boundaries. Our method first calculates matching costs on four subband images derived with a wavelet transform, and then combines the costs based on matching confidence. After an initial depth estimation by using the matching cost, we improve estimation by an optimization process. Experimental results demonstrated that our proposed method provides more plausible estimation with sharper object boundaries.	depth map;light field;mathematical optimization;wavelet transform	Kazu Mishiba;Yuji Oyamada;Katsuya Kondo	2016	2016 IEEE 5th Global Conference on Consumer Electronics	10.1109/GCCE.2016.7800376	computer vision;mathematical optimization;mathematics;statistics	Vision	54.76501552260752	-57.44634915335219	134677
36d58c2042406a36d7d525b5507f2a85fbf9c03f	iterative soft color-shrinkage for color-image denoising	wavelet transforms image colour analysis image denoising iterative methods;colored noise;noise reduction colored noise additive noise gaussian noise digital cameras color wavelet transforms wavelet domain low frequency noise noise cancellation;cross correlation;color space;color;inter channel color cross correlations iterative soft color shrinkage color image denoising signal dependent noise removal digital color camera wavelet transform;color image processing;wavelet transforms;iterative methods;shrinkage;wavelet transform;image color analysis;image colour analysis;noise reduction;image denoising;non expansive mapping;denoising;non expansive mapping color image processing denoising wavelet transform shrinkage;color image	To remove signal-dependent noise of a digital color camera, we present a new soft color-shrinkage scheme for color-image denoising in a wavelet transform domain. The classic soft-shrinkage scheme works well for monochrome-image denoising; to utilize inter-channel color cross-correlations, a noisy image undergoes the color-transformation from the RGB to the luminance-and-chrominance color space, and the luminance and the chrominance components are separately denoised. However, this color-denoising approach cannot cope with actual signal-dependent noise. To utilize the noise's signal-dependencies, we present an iterative soft color-shrinkage scheme where the inter-channel color cross-correlations are directly utilized in the RGB color space, and theoretically study its convergence property. Our color-shrinkage scheme alleviates denoising artifacts, and improves picture quality of denoised images.	color image;color space;cross-correlation;image quality;iterative method;monochrome;noise reduction;simulation;wavelet transform	Takahiro Saito;Nobuhiro Fujii;Takashi Komatsu	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414246	computer vision;computer science;noise reduction;mathematics;video denoising;wavelet transform;computer graphics (images)	Robotics	57.717347789341765	-65.72198315020731	135199
4aedfba5452b53dbcfd7922da84dfb92283819f1	blind single image super resolution with low computational complexity		This paper proposes a single image super resolution algorithm with the aim of satisfying three desirable characteristics, namely, high quality of the produced images, adaptability to image contents and unknown blurring conditions used to generate given input images, and low computational complexity. After the given input image is up-scaled using a conventional reconstruction operator, the missing high frequency components estimated from lower resolution versions of the input image are added for improved quality and, moreover, the amount of the high frequency components to be added is adaptively determined. No computationally intensive operation is involved in the whole process, which makes the method computationally cheap. Experimental results show that the proposed method yields good subjective and objective image quality consistently across different blurring conditions and contents, and operates fast in comparison to existing state-of-the-art algorithms. In addition, it is also demonstrated that the proposed method can be used in combination with the existing algorithms in order to improve further their performance in terms of image quality.	algorithm;autostereogram;computational complexity theory;display resolution;image quality;robustness (computer science);super-resolution imaging	Won-Hee Kim;Jong-Seok Lee	2016	Multimedia Tools and Applications	10.1007/s11042-016-3396-0	computer vision;mathematical optimization;simulation	Vision	59.719169296753115	-61.778302924190626	135552
873b695750b2972a31f9373ffd8ac13560213968	stochastic super-resolution image reconstruction	bilateral filtering;gibbs sampler;sample generation;bayesian inference;low resolution;general techniques;image acquisition;markov chain monte carlo;image reconstruction;super resolution;high resolution imager;bilateral filter;image modeling;metropolis hastings algorithm;exhaustive search	The objective of super-resolution (SR) imaging is to reconstruct a single higher-resolution image based on a set of lower-resolution images that were acquired from the same scene to overcome the limitations of image acquisition process for facilitating better visualization and content recognition. In this paper, a stochastic Markov chain Monte Carlo (MCMC) SR image reconstruction approach is proposed. First, a Bayesian inference formulation, which is based on the observed low-resolution images and the prior high-resolution image model, is mathematically derived. Second, to exploit the MCMC sample-generation technique for the stochastic SR image reconstruction, three fundamental issues are observed as follows. First, since the hyperparameter value of the prior image model controls the degree of regularization and intimately affects the quality of the reconstructed high-resolution image, how to determine an optimal hyperparameter value for different low-resolution input images becomes a very challenging task. Rather than exploiting the exhaustive search, an iterative updating approach is developed in this paper by allowing the value of hyperparameter being simultaneously updated in each sample-generation iteration. Second, the samples generated during the so-called burn-in period (measured in terms of the number of samples initially generated) of the MCMC-based sample-generation process are considered unreliable and should be discarded. To determine the length of the burn-in period for each set of low-resolution input images, a time-period bound in closed form is mathematically derived. Third, image artifacts could be incurred in the reconstructed high-resolution image, if the number of samples (counting after the burn-in period) generated by the MCMC-based sample-generation process is insufficient. For that, a variation-sensitive bilateral filter is proposed as a 'complementary' post-processing scheme, to improve the reconstructed high-resolution image quality, when the number of samples is insufficient. Extensive simulation results have clearly shown that the proposed stochastic SR image reconstruction method consistently yields superior performance.	iterative reconstruction;super-resolution imaging	Jing Tian;Kai-Kuang Ma	2010	J. Visual Communication and Image Representation	10.1016/j.jvcir.2010.01.001	computer vision;econometrics;mathematical optimization;feature detection;computer science;machine learning;mathematics;bilateral filter;statistics	Vision	56.2153500698562	-58.803981165549914	135611
6d8a522d4d6a9f8b0ee3cceb5e2a7a52a4d43294	evaluation of two principal approaches to objective image quality assessment	challenging task;image quality assessment;computational model;human visual system;objective image;quality assessment;visible differences;human perception;important advantage;human perceptual property;principal approaches;new assessment methodology;structural similarity index;visualization;computer science;structural similarity;testing;utility computing;data visualisation;mean opinion score;information visualization;computational modeling;image compression;region of interest;data compression;indexation;predictive models;computer model;image quality;visual system	Nowadays, it is evident that we must consider human perceptual properties to visualize information clearly and efficiently. We may utilize computational models of human visual systems to consider human perception well. Image quality assessment is a challenging task that is traditionally approached by such computational models. Recently, a new assessment methodology based on structural similarity has been proposed. We select two representative models of each group, the visible differences predictor and the structural similarity index, for evaluation. We begin with the description of these two approaches and models. We then depict the subjective tests that we have conducted to obtain mean opinion scores. Inputs to these tests included uniformly compressed images and images compressed non-uniformly with regions of interest. Then, we discuss the performance of the two models, and the similarities and differences between the two models. We end with a summary of the important advantages of each approach.	computation;computational model;computer graphics;image quality;kerrison predictor;region of interest;semantic similarity;structural similarity;variable data printing;variable data publishing	Martin Cadík;Pavel Slavík	2004	Proceedings. Eighth International Conference on Information Visualisation, 2004. IV 2004.	10.1109/IV.2004.1320193	computer vision;computer science;artificial intelligence;data mining	Robotics	62.13452476315585	-64.39183295561408	135623
6ed5881148f9b3d7676189f0994978d31cb566f5	fuzzy perceptual watermarking for ownership verification	fuzzy logic;digital images.;watermarking;digital image;data fusion;human visual system;domain model;fuzzy system	An adaptive watermarking method based on the human visual system model and the fuzzy inference system in wavelet domain is proposed. Fuzzy logic is used for data fusion and builds a HVS model for spatial masking in wavelet domain. Modeling spatial masking is a complicated task and there is no single theoretical formulation to precisely compute the perceptual value for a corresponding wavelet coefficient. Fuzzy logic is used for data fusion and operates on the HVS model for spatial masking in wavelet domain. The fuzzy input variables (brightness, luminosity, texture) are computed for each wavelet coefficient in the image. The output of the fuzzy system is a single value which gives a perceptual value for each corresponding wavelet coefficient. The fuzzy based watermarks are robust to attacks and at the same time achieve a high level of imperceptibility.	coefficient;digital watermarking;fuzzy control system;fuzzy logic;high-level programming language;human visual system model;inference engine;wavelet transform	Mukesh C. Motwani;Frederick C. Harris	2009			neuro-fuzzy;fuzzy set operations;adaptive neuro fuzzy inference system;fuzzy logic;fuzzy associative matrix;defuzzification;machine learning;mathematics;fuzzy control system;fuzzy classification;artificial intelligence	Robotics	55.77812151313783	-64.86958443858346	135664
2b819bef80c02d5d4cb56f27b202535e119df988	image quality assessment based on gradient similarity	databases;image processing distortion gradient methods;information structure;image coding;image processing;edge detection;gradient similarity;transform coding;algorithms artifacts image enhancement image interpretation computer assisted pattern recognition automated quality control reproducibility of results sensitivity and specificity signal processing computer assisted subtraction technique;image quality assessment iqa;journal article;structural change;structural similarity ssim;drntu engineering computer science and engineering computing methodologies image processing and computer vision;distortion;visualization;image edge detection;human visual system hvs;human visual system;image quality;structural similarity ssim contrast masking gradient similarity human visual system hvs image quality assessment iqa;adaptive method;contrast masking;image quality assessment;gradient methods;image edge detection visualization image quality databases transform coding image coding white noise;scene understanding;contrast structural changes image quality assessment gradient similarity luminance changes;white noise;structural similarity	In this paper, we propose a new image quality assessment (IQA) scheme, with emphasis on gradient similarity. Gradients convey important visual information and are crucial to scene understanding. Using such information, structural and contrast changes can be effectively captured. Therefore, we use the gradient similarity to measure the change in contrast and structure in images. Apart from the structural/contrast changes, image quality is also affected by luminance changes, which must be also accounted for complete and more robust IQA. Hence, the proposed scheme considers both luminance and contrast-structural changes to effectively assess image quality. Furthermore, the proposed scheme is designed to follow the masking effect and visibility threshold more closely, i.e., the case when both masked and masking signals are small is more effectively tackled by the proposed scheme. Finally, the effects of the changes in luminance and contrast-structure are integrated via an adaptive method to obtain the overall image quality score. Extensive experiments conducted with six publicly available subject-rated databases (comprising of diverse images and distortion types) have confirmed the effectiveness, robustness, and efficiency of the proposed scheme in comparison with the relevant state-of-the-art schemes.	benchmark (computing);blinded;body dysmorphic disorders;computational complexity theory;computer performance;database;distortion;embedded system;embedding;experiment;gradient descent;human visual system model;image gradient;image processing;image quality;mathematical optimization;mathematics;pixel;published database;self-similarity;similarity learning;similarity measure;vetispiradiene synthase activity	Anmin Liu;Weisi Lin;Manish Narwaria	2012	IEEE Transactions on Image Processing	10.1109/TIP.2011.2175935	image quality;computer vision;transform coding;speech recognition;visualization;edge detection;distortion;image processing;computer science;structural similarity;structural change;white noise;human visual system model;computer graphics (images)	Vision	61.65594238354417	-64.98984063326918	135671
c428e40e2bcaa34edf8601ba1ea216e0bb23be9a	unsupervised corrections of unknown chromatic dominants using a brownian-path-based retinex algorithm	reflectivity;lamps;spectrometers;algorithms;tungsten;neon;light sources	cy. erapeca a t mbeble 200 Abstract. An experimental analysis of chromatic equalization based on a new implementation of the Retinex algorithm is presented. The experiments are carried out on a colored Mondrian patchwork illuminated with different commercial light sources and on synthetic images generated with a photometric ray tracer using different illuminants. Regarding the Mondrian patchwork, the spectral characteristics of the bulbs and the reflected light from each patch are measured using a commercial spectrometer. From the measured data, synthetic images of the patchwork with different illuminants are created and processed by the Retinex algorithm. The chromatic correction capabilities of the Retinex implementation have been measured and compared with unfiltered values and with the results of another Retinex implementation and classic color equalization algorithms. Results show that Retinex performs an unsupervised color correction without requiring any information about the spectral composition of the illuminant. © 2003 SPIE and IS&T. [DOI: 10.1117/1.1584051]	algorithm;brownian motion;cylinder-head-sector;experiment;mondrian olap server;patchwork;ray tracing (graphics);synthetic data;synthetic intelligence	Alessandro Rizzi;Daniele Marini;Luigi Rovati;Franco Docchio	2003	J. Electronic Imaging	10.1117/1.1584051	computer vision;neon;spectrometer;reflectivity;tungsten;computer graphics (images)	Robotics	64.93605784553374	-58.08738887376534	135837
3491bfacb33598b686e1f1fa640b1658a836e1fd	image dehazing algorithm based on atmosphere scatters approximation model	atmospheric optical;scattering approximate model;expanded jones matrix;dehazing algorithm;stokes laws	Due to the scattered light of suspended particles in the atmosphere, the images taken in the foggy day become gray and are lack of visibility. In order to unveil the clear images structures and colors, the author propose an algorithm based on atmosphere scatters approximation model, which adopts the extended Jones Matrix and Stokes Law to calculate approximate the transmission of light in the atmosphere, so as to eliminate some of the scattered light. Both the light intensity in the atmosphere and haze concentration are obtained by means of Dark Channel Prior, afterward the extinction function for light transmission is used for calculation to restore the foggy images. The experimental results show that the algorithm can not only effectively improve scenery visual effect under different condition of haze, and provide clear pictures for machine vision applications in the foggy day.	algorithm;approximation	Zhongyi Hu;Qing Liu;Shenghui Zhou;Mingjing Huang;Fei Teng	2012		10.1007/978-3-642-34500-5_20	visibility;haze;machine vision;matrix (mathematics);computer science;atmosphere;algorithm;communication channel;stokes' law	EDA	57.91602078124698	-59.93316802893312	135942
208cd4003e59f4e2f46858a67bd0865ed6b4ba07	challenges in digital imaging for artificial human vision	bionica;contenu image;image numerique;image content;eye;human vision;bionique;digital image processing;metodologia;image processing;bionics;procesamiento imagen;hombre;digital imaging;experience subjective;qualite image;traitement image;information content;methodologie;human vision and color perception;machine vision;image quality;imaging;imagen numerica;human;information rate;aparato visual;formation image;appareil visuel;calidad imagen;vision artificielle;formacion imagen;digital image;human vision system;scene understanding;methodology;subjective assessment;artificial vision;contenido imagen;experiencia subjetiva;visual system;subjective experience;homme;vision artificial;modulation	Several international research teams are currently developing artificial human vision systems that have the potential to restore some visual faculties to blind persons. Given the significant advancements from these teams, it is conceivable that the implantation of a safe a d useful prosthesis will occur soon, perhaps in the next 2-4 years. It is thus timely to suggest and demonstrate methods to increase the information content of such artificial vision systems. Several ideas are suggested in this paper, such as brightness modulation, range indication, importance mapping and the delivery of supplementary information, which will do much towards providing visual information comparable to that obtained via a normally functioning human eye but at far lower information rates. This paper briefly describes the framework of artificial vision systems and outlines basic considerations of digital image processing as applied to artificial vision systems. We describe the poor quality of anticipated images produced by these artificial vision systems and the need for enhancing the images to allow increased scene understanding. Several techniques are identified which could enhance the information content of images. We then describe our own research in this area, which aims to determines the performance envelope of useful low quality images associated with artificial vision systems. Our subjective assessment studies using representative test patterns have investigated how much information and what types of information are needed to recognize or perceive a scene. This testing has been to identify the most informative image processing operations which lead to better understanding of picture content.© (2001) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	digital imaging	Justin R. Boyle;Anthony J. Maeder;Wageeh W. Boles	2001		10.1117/12.429525	image quality;computer vision;simulation;bionics;qualia;self-information;visual system;machine vision;image processing;artificial intelligence;digital image processing;methodology;digital imaging;digital image;vision science;modulation	Vision	63.126554420327935	-61.873751764509095	135961
00038b9918086a7887fe13ce90dd92661cf7674f	structural similarity quality metrics in a coding context: exploring the space of realistic distortions	error concealment;video compression;hombre;contrast sensitivity;4266s;qualite image;error analysis;video coding;distortion;codificacion;senal video;signal video;subbanda;percepcion visual;subband;image quality;coding;human;perception visuelle;video signal;visual perception;calidad imagen;video;vision;human perception;sous bande;wavelets;structural similarity;codage;homme	Perceptual image quality metrics have explicitly accounted for human visual system (HVS) sensitivity to subband noise by estimating thresholds above which distortion is just-noticeable. A recently proposed class of quality metrics, known as structural similarity (SSIM), models perception implicitly by taking into account the fact that the HVS is adapted for extracting structural information (relative spatial covariance) from images. We compare specific SSIM implementations both in the image space and the wavelet domain. We also evaluate the effectiveness of the complex wavelet SSIM (CWSSIM), a translation-insensitive SSIM implementation, in the context of realistic distortions that arise from compression and error concealment in video transmission applications. In order to better explore the space of distortions, we propose models for typical distortions encountered in video compression/transmission applications. We also derive a multi-scale weighted variant of the complex wavelet SSIM (WCWSSIM), with weights based on the human contrast sensitivity function to handle local mean shift distortions.	data compression;distortion;error concealment;human visual system model;image quality;mean shift;structural similarity;wavelet	Alan C. Brooks;Thrasyvoulos N. Pappas	2006		10.1117/12.660611	data compression;image quality;wavelet;vision;computer vision;speech recognition;video;distortion;visual perception;computer science;structural similarity;multimedia;coding;perception	Vision	62.95834245883736	-63.696602051041694	136129
fe152231536ef8efb08684b548de29f198b41274	a fuzzy-bayesian approach to image expansion	interpolation;image segmentation;bayesian approach;low pass filters bayesian methods frequency image segmentation spline image reconstruction cameras image resolution samarium australia;bayes methods;inference mechanisms;image restoration;computer vision;region segmentation;fuzzy logic;gibbs energy function image expansion edge preserving interpolation digital images region segmentation fuzzy inference bayesian structure inverse problem;interpolation method;fuzzy inference;inverse problems computer vision image restoration fuzzy logic inference mechanisms bayes methods interpolation image segmentation;digital image;inverse problems	This paper presents a fuzzy edge preserving interpolation method for digital images t o reduce the effect of jaggedness and blurring artifacts along the high contrast edges. A high subjective performance is achieved by combining two techniques, a region segmentation method and a fuzzy inference method based on the bayesian	digital image;fuzzy logic;interpolation	Mustafa Sakalli;Hong Yan;Alan M. N. Fu	1999		10.1109/IJCNN.1999.833502	fuzzy logic;image restoration;computer vision;bayesian probability;interpolation;computer science;inverse problem;machine learning;pattern recognition;mathematics;image segmentation;digital image	Vision	56.55665209791	-65.17297219270246	136316
3a701f6f29a3a96812a269b77657e3477fae448d	high quality panoramic image generation using multiple panoramic annular lens images	pal image;image quality deterioration;image quality degradation;panoramic expansion;image resolution;panorama;low spatial resolution;surveillance;surveillance system;high quality panoramic image generation;traffic control;integrated optics;intelligent camera system;multiple image;omnidirectional camera;optical imaging;general methods;high quality;omnidirectional camera system;surveillance image resolution lenses;multiple image omnidirectional camera system high quality expansion panorama;image quality;pixel;field of view;lenses;image quality deterioration high quality panoramic image generation panoramic annular lens image intelligent camera system surveillance system compactness omnidirectional camera system low spatial resolution panoramic expansion image quality degradation pal image zoom ratio;panoramic image;compactness;pixel cameras lenses optical imaging spatial resolution integrated optics;panoramic annular lens image;zoom ratio;cameras;expansion;spatial resolution	Surveillance with intelligent camera systems is widely used in traffic control, driving assistance and surgery. One of the most essential specifications for surveillance systems is compactness. Therefore, in these applications with large field of view, we can avoid the use of multiple camera systems or mechanically controlled cameras. It is well known that omnidirectional camera systems using panoramic annular lens (PAL) have many advantages. Nevertheless, because of low spatial resolution of angle of elevation for the vertical direction of captured image by using PAL, the image after panoramic expansion usually has a disadvantage such as image quality degradation. To overcome this disadvantage, high quality panoramic image generation method is essential. Consequently, we propose the high quality panoramic image generation method using multiple PAL images. By synthesizing multiple PAL images for different zoom ratio, we can obtain fine quality panoramic image that improve image quality deterioration.	display resolution;elegant degradation;glossary of computer graphics;image quality;kinetic data structure;omnidirectional camera;pal	Satoshi Araki;Kei Maeda;Keiji Shibata;Yuukou Horita	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5652454	image quality;computer vision;image resolution;computer graphics (images)	Robotics	59.75531013408875	-59.72250692916954	136431
b88702dabe89f811382f8e89ce076222680e81f9	attentive generative adversarial network for raindrop removal from a single image		Raindrops adhered to a glass window or camera lens can severely hamper the visibility of a background scene and degrade an image considerably. In this paper, we address the problem by visually removing raindrops, and thus transforming a raindrop degraded image into a clean one. The problem is intractable, since first the regions occluded by raindrops are not given. Second, the information about the background scene of the occluded regions is completely lost for most part. To resolve the problem, we apply an attentive generative network using adversarial training. Our main idea is to inject visual attention into both the generative and discriminative networks. During the training, our visual attention learns about raindrop regions and their surroundings. Hence, by injecting this information, the generative network will pay more attention to the raindrop regions and the surrounding structures, and the discriminative network will be able to assess the local consistency of the restored regions. This injection of visual attention to both generative and discriminative networks is the main contribution of this paper. Our experiments show the effectiveness of our approach, which outperforms the state of the art methods quantitatively and qualitatively.		Rui Qian;Robby T. Tan;Wenhan Yang;Jiajun Su;Jiaying Liu	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00263	computer vision;generative grammar;discriminative model;local consistency;artificial intelligence;pattern recognition;adversarial system;visualization;computer science;lens (optics)	Vision	56.60098474148596	-59.470233986283155	136451
5903aac7fc316c31a180c4f9bdb7dadfe7df7ac1	objective quality assessment of image retargeting based on line distortion	loss measurement;image segmentation;distortion measurement;layout;distortion;quality assessment;shape	With the proliferation of mobile devices, research on image retargeting is becoming ever more important. However, there is little work on image retargeting quality assessment despite its importance. In this work, we focus on evaluating retargeting quality based on line distortion. Generally, image retargeting results in content loss and shape distortion. Line segments, which are fundamental image structures, are hence discarded or distorted in retargeted images. As a result, we formulate a retargeting quality index consisted of three line distortion measures: line loss, line artifact and line rotation. To test its performance, we have validated it on the public dataset RetargetMe. Experimental results demonstrate that our method outperforms many existent ones and line distortion is a good indicator of retargeting quality.	algorithm;distortion;experiment;mobile device;retargeting;seam carving	Yichi Zhang;King Ngi Ngan	2016	2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2016.7844616	layout;computer vision;distortion;shape;computer science;multimedia;image segmentation	Robotics	59.85977943095803	-60.80616776521033	136863
d6ff9227cd70921722d6af71f07ade1607b9de00	a new automated quality assessment algorithm for night vision image fusion	perceptual quality evaluation method;perceptual quality;automated quality assessment;contrast sensitivity function;night vision image fusion;saliency map;contrast preservation map;image fusion;evaluation method;government;prediction algorithms;quality assessment night vision image fusion humans visual system image quality government image sensors prediction algorithms mutual information;image sensors;local contrast map;quality assessment;night vision;human visual system;image quality;visual perception image fusion night vision;mutual information;visual perception;humans;saliency map automated quality assessment night vision image fusion human visual system contrast sensitivity function perceptual quality evaluation method image quality local contrast map contrast preservation map;visual system	In this paper we propose a perceptual quality evaluation method for image fusion which is based on human visual system (HVS) models. Our method assesses the image quality of a fused image using the following steps. First the source and fused images are filtered by a contrast sensitivity function (CSF) after which a local contrast map is computed for each image. Second, a contrast preservation map is generated to describe the relationship between the fused image and each source image. Finally, the preservation maps are weighted by a saliency map to obtain an overall quality map. The mean of the quality map indicates the quality for the fused image. Experimental results compare the predictions made by our algorithm with human perceptual evaluations for several different parameter settings in our algorithm. For some specific parameter settings, we find our algorithm provides better predictions, which are more closely matched to human perceptual evaluations, than the existing algorithms.	algorithm;human visual system model;image fusion;image quality;map;psychoacoustics	Yin Chen;Rick S. Blum	2007	2007 41st Annual Conference on Information Sciences and Systems	10.1109/CISS.2007.4298361	image quality;computer vision;visual system;visual perception;mathematics;multimedia;mutual information;image fusion;government;statistics;computer graphics (images)	Vision	62.63312318084188	-63.76380100405969	136998
674877962b90754643d06ea3c27c510c0617a101	fast automated stopping-time and edge-strength estimation for anisotropic diffusion	stopping time estimation;image processing edge detection image denoising;robust scale parameter;robust scale parameter fast automated stopping time edge strength estimation anisotropic diffusion iterative image smoothing process automatic noise adaptive stopping time estimator;edge preservation denoising anisotropic diffusion stopping time estimation edge strength;image processing;edge detection;anisotropic magnetoresistance psnr noise reduction noise robustness smoothing methods gaussian noise robust stability statistics error correction shape control;fast automated stopping time;indexing terms;anisotropic diffusion;edge strength;iterative image smoothing process;scale space;edge strength estimation;stopping time;image denoising;denoising;automatic noise adaptive stopping time estimator;edge preservation	Anisotropic diffusion (ATD) is an edge-oriented, scale-space based, and iterative image-smoothing process. Two main challenges of ATD are how to automatically stop the iterative process, so to avoid blurring, and how to determine the scale (or edge-strength) parameter, so to best differentiate between edge and noise. In this paper, we propose 1) an automatic noise-adaptive stopping-time estimator and 2) a robust scale parameter (or edge strength) estimator. With these two novel estimators, our adaptive ATD method effectively reduces noise (high PSNR gain) and preserves structures (significantly less blurring) than conventional ATD.	anisotropic diffusion;iteration;iterative method;peak signal-to-noise ratio;scale space;smoothing	Eva Rifkah;Aishy Amer	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4517764	computer vision;mathematical optimization;scale space;edge detection;index term;image processing;stopping time;computer science;pattern recognition;noise reduction;mathematics;anisotropic diffusion;statistics	Vision	56.12192670794794	-65.96504692626154	137298
9df9e596c44c8d928470d52b6656abed12e15afd	block matching algorithms for motion estimation - a comparison study			algorithm;motion estimation	Abir Jaafar Hussain;Liam Knight;Dhiya Al-Jumeily;Paul Fergus;Hani Hamdan	2014		10.1007/978-3-319-04960-1_32		Vision	56.63970069547202	-55.01944962411153	137386
712946cd2122fbc76343d0ef7198cc26100afa7f	perceptual video quality evaluation using fuzzy inference system	takagi sugeno model;perceptual quality;learning algorithm;subjective test data;video signal processing;video compression;visual masking error;video quality;inference mechanisms;testing;distortion measurement;fuzzy systems humans video compression visual system engines inference algorithms image quality distortion measurement testing takagi sugeno model;perceptual video quality evaluation;contrast distortion;distortion;engines;image quality;perceptual quality metric;fuzzy inference system;distortion video signal processing fuzzy systems inference mechanisms learning artificial intelligence;inference algorithms;humans;inference engine;learning artificial intelligence;subjective test data perceptual video quality evaluation fuzzy inference system perceptual quality metric image quality inference engine visual masking error blurring distortion contrast distortion learning algorithm;blurring distortion;visual system;fuzzy systems;visual masking	In this paper, we present a perceptual quality metric for evaluating image and video quality. The metric is based on a fuzzy inference system with Takagi-Sugeno's inference engine. Three visually important factors including visual masking error, blurring distortion and contrast distortion are used as the inputs to the inference system. Through learning algorithm with the subjective test data, the inference system can predict video quality with high accuracy and monotonicity.	algorithm;distortion;fuzzy logic;inference engine;test data;video	Susu Yao;Weisi Lin;Zhongkang Lu;Ee Ping Ong;Xiaokang Yang	2004	2004 IEEE International Symposium on Circuits and Systems (IEEE Cat. No.04CH37512)	10.1109/ISCAS.2004.1328892	data compression;image quality;subjective video quality;computer vision;visual system;distortion;adaptive neuro fuzzy inference system;computer science;video quality;machine learning;pattern recognition;mathematics;software testing;inference engine;fuzzy control system	Embedded	63.11657041751933	-64.23606315449123	137421
1d9ce4a61e8d67ce82506d8b9ec95604f31d4897	a novel image filtering approach for sensor fingerprint estimation in source camera identification	digital cameras;estimation;receiver operating characteristic image filtering approach sensor fingerprint estimation source camera identification photo response nonuniformity noise imaging sensors denoising method prnu noise extraction spatial domain filtering pixel to pixel correlation image database;fingerprint recognition;noise forensics fingerprint recognition estimation correlation digital cameras;correlation;image filtering cameras digital forensics feature extraction image denoising;forensics;noise;h600 electronic and electrical engineering	Photo-response non-uniformity (PRNU) noise has been well established as a reliable fingerprint of imaging sensors for source camera identification and other forensic applications. In this paper, we introduce a novel denoising method for PRNU noise extraction that considerably outperforms the existing techniques. The rationale is to use as little as one adjacent pixel in spatial domain filtering to suppress the pixel-to-pixel correlation in the estimation of the (supposedly white) PRNU noise. Experimental results are presented based on an image database of ten cameras of different models and makes. These results are embodied in the receiver operating characteristic (ROC) of source camera identification.	circuit complexity;data compression;design rationale;filter (signal processing);fingerprint;noise reduction;pixel;receiver operating characteristic;sensor;white noise	Mustafa Al-Ani;Fouad Khelifi;Ashref Lawgaly;Ahmed Bouridane	2015	2015 12th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)	10.1109/AVSS.2015.7301808	image noise;computer vision;estimation;speech recognition;computer science;noise;forensic science;correlation;fingerprint recognition;statistics	Vision	54.78162750116866	-62.11584645074743	137425
36906d003a0ce40a17e3932dda55fa07f6bd799f	underwater image quality enhancement through composition of dual-intensity images and rayleigh-stretching	noise reduction underwater image quality enhancement dual intensity images rayleigh stretching water medium physical property low contrast blur inhomogeneous lighting color diminishing contrast correction technique von kreis hypothesis rayleigh distribution color correction technique hue saturation value color model hsv color model color component image color performance qualitative analysis quantitative analysis;entropy improvement;sonar imaging image colour analysis image denoising image enhancement image restoration;entropy improvement underwater image processing contrast and color enhancement noise reduction;noise reduction;contrast and color enhancement;underwater image processing	Quality of underwater image is poor due to the environment of water medium. The physical property of water medium causes attenuation of light travels through the water medium, resulting in low contrast, blur, inhomogeneous lighting, and color diminishing of the underwater images. This paper extends the methods of enhancing the quality of underwater image. The proposed method consists of two stages. At the first stage, the contrast correction technique is applied to the image, where the image is applied with the modified Von Kreis hypothesis and stretching the image into two different intensity images at the average value with respects to Rayleigh distribution. At the second stage, the color correction technique is applied to the image where the image is first converted into hue-saturation-value (HSV) color model. The modification of the color component increases the image color performance. Qualitative and quantitative analyses indicate that the proposed method outperforms other state-of-the-art methods in terms of contrast, details, and noise reduction.	color space;dual;gaussian blur;image quality;noise reduction;physical phenomenon or property;rayleigh–ritz method;stage level 1;stage level 2;hue;travel	Ahmad Shahrizan Abdul Ghani;Nor Ashidi Mat Isa	2014		10.1109/ICCE-Berlin.2014.7034265	demosaicing;color histogram;image restoration;computer vision;image gradient;binary image;geography;color balance;optics;histogram equalization;computer graphics (images)	Vision	58.64901122085482	-62.13135562543484	137585
9c10c44950e223edbd9620cfa2c3adf4919477fc	lossless image compression utilizing reference points coding	lossless image compression;reference point;data storage;image compression	This paper proposes a lossless image compression method utilizing the neighboring pixels to determine the reference point values. The proposed method scans every pixel row by row and assigns a 2-bit reference point value to each pixel by comparing its intensity value to the neighboring pixels' intensity values. The intensity value will be stored to a new file only when the comparison fails to find a neighborhood pixel with the same intensity value. The compression is achieved as only the information of 2-bit reference point values for all pixels and certain intensity values are required for storage. The suggested method is tested on various types of images and the results show that it performs well for most of the images.© (2011) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.	image compression;lossless compression	Yi-Fei Tan;Wooi-Nee Tan;Kae-Yann Tan	2011		10.1117/12.896159	data compression;lossy compression;color cell compression;computer vision;image compression;computer science;theoretical computer science;lossless compression;computer graphics (images)	Vision	56.19074731913347	-62.84801740136995	137803
8d57aad83f4241f6c9327943da7f693f47c6ef0c	a feature point based scheme for unsupervised video object segmentation in stereoscopic video sequences	image segmentation;mpeg 4;satisfiability;data mining;depth map	The video coding standard MPEG-4 is enabling content-based functionalities by the introduction of video object planes (VOP’s) which represent semantically meaningful objects. In this paper, a novel fast, unsupervised semantic segmentation scheme is presented for stereoscopic sequences, which utilizes the provided depth information. Each stereo pair is first analyzed and the disparity field and occluded areas are estimated. Then a multiresolution implementation of the RSST segmentation algorithm is applied to the depth map for extracting the depth segments. For each depth segment, except the last, feature points are generated on its contour and a motion geometric space (MGS) for every initial point is defined. Afterwards one point per MGS is selected, which satisfies predefined intensity and curvature constraints so that the object boundaries are accurately extracted. Experimental results are presented to indicate the good performance of the proposed scheme on real life stereoscopic video sequences.	algorithm;binocular disparity;data compression;depth map;modelling of general systems;real life;stereoscopy;video coding format	Klimis S. Ntalianis;Nikolaos D. Doulamis;Anastasios D. Doulamis;Stefanos D. Kollias	2000			computer vision;computer science;multimedia;image segmentation;mpeg-4;depth map;satisfiability;computer graphics (images)	Vision	57.26786518847325	-55.0727698071447	137861
1f90a8a8b6113f4b02aafc794bc9c04e98d06743	snow removal-a noise-stripping process for picture signals	gaussian noise;performance evaluation;snow;signal detection;additive noise;testing;tv distortion;smoothing methods;signal processing;humans;tv;snow signal processing tv testing humans additive noise gaussian noise performance evaluation signal detection smoothing methods	It is quite often possible to separate “picture” from “noise” in a television image. We do this when we view television under unfavorable conditions. This ability of the human observer suggests that completely automatic operation might at least partially separate noise and picture. A nonstationary, nonlinear operation has been found which selectively removes moderate amounts of additive Gaussian noise from a received picture signal. Running tests are performed upon the signal to detect the presence of perceptually significant picture detail in a number of different categories. Depending upon the test results, a selection is made from a number of available smoothing filter modes to maximize the suppression of noise without picture blurring. No preparatory operation is required at the transmitter, so that the technique is compatible with existing picture transmission systems. The process is applicable to conventional broadcast television, and could in principle be incorporated in home receivers to improve reception in fringe areas.	image noise;nonlinear system;smoothing;terrestrial television;transmitter;utility functions on indivisible goods;zero suppression	R. E. Graham	1962	IRE Trans. Information Theory	10.1109/TIT.1962.1057690	gradient noise;gaussian noise;median filter;noise;snow;colors of noise;noise;value noise;telecommunications;computer science;noise measurement;signal processing;noise;noise;mathematics;software testing;noise floor;stochastic resonance;detection theory	Graphics	60.26224611650936	-58.90458678573967	137890
842fa8869048444db57d951849d7dd72ee380d01	depth map super-resolution via extended weighted mode filtering	refined adaptive support window;noise suppression;super resolution;weighted mode filter	Depth maps captured by the range imaging sensors such as ToF (time of flight) camera and Kinect are stuck with limited spatial resolution and varieties of noises, which makes it difficult to be directly applied to 3D scene analysis. In this paper, we address these issues via an extended weighted mode filter (EWMF). In view of the impressive feature of the noise-aware filter in noise suppression, the proposed method synergistically combines standard weighted mode filter (WMF) and the noise aware filter to achieve a better noise suppression performance. Different from conventional filtering-based methods with a fixed support window, a refined adaptive support window (RASW) is designed. The proposed filter with RASW can well capture local structure details better. Experimental results demonstrate that the proposed method outperforms several state-of-the-art super-resolution techniques in terms of bad pixel rate and root mean square error.	bilateral filter;defective pixel;depth map;kinect;lr parser;mean squared error;multipoint ground;nato architecture framework;range imaging;sensor;super-resolution imaging;synergy;zero suppression	Mingliang Fu;Weijia Zhou	2016	2016 Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2016.7805430	adaptive filter;computer vision;kernel adaptive filter;computer science;root-raised-cosine filter;control theory;filter design;salt-and-pepper noise;superresolution	Vision	58.155655521744976	-57.8747945263399	137981
c258d3c8d4c2ebc2185426c8b96dc306f30fa467	defect pixel interpolation for lossy compression of camera raw data	image processing;digital cameras;image quality;algorithms;cameras	The image processing pipeline of a traditional digital camera is often limited by processing power. A better image quality could be generated only if more complexity was allowed. In a raw data workflow most algorithms are executed off-camera. This allows the use of more sophisticated algorithms for increasing image quality while reducing camera complexity. However, this requires a major change in the processing pipeline: a lossy compression of raw camera images might be used early in the pipeline. Subsequent off-camera algorithms then need to work on modified data. We analyzed this problem for the interpolation of defect pixels. We found that a lossy raw compression spreads the error from uncompensated defects over many pixels. This leads to a problem as this larger error cannot be compensated after compression. The use of high quality, high complexity algorithms in the camera is also not an option. We propose a solution to this problem: Inside the camera only a simple and low complexity defect pixel interpolation is used. This significantly reduces the compression error for neighbors of defects. We then perform a lossy raw compression and compensate for defects afterwards. The high complexity defect pixel interpolation can be used off-camera. This leads to a high image quality while keeping the camera complexity low.	algorithm;critical graph;data compression;digital camera;display resolution;distortion;image processing;image quality;interpolation;lossy compression;pixel;raw image format;software bug	Michael Schöberl;Joachim Keinert;Jürgen Seiler;Siegfried Fößel;André Kaup	2012		10.1117/12.907910	image quality;computer vision;simulation;image processing;image compression;computer science;computer graphics (images)	Graphics	58.632651331291896	-59.21137806536463	138014
adeb00b2ce84d2819a61bba6767648bcd4aa8cc3	a perceptual quality assessment metric using temporal complexity and disparity information for stereoscopic video	stereoscopic video;measurement;conference;training;subjective quality assessment;visualization;pearson correlation coefficient value stereoscopic video perceptual quality assessment metric temporal complexity disparity information visual discomfort fatigue problems 3d video human perception visual comfort subjective quality assessments temporal variance disparity variation intra frames inter frames disparity distribution frame boundary areas objective quality assessment metric linear regression;quality assessment;stereo image processing quality assessment visualization tv three dimensional displays training measurement;visual fatigue;three dimensional displays;visual perception regression analysis stereo image processing;stereo image processing;visual perception;regression analysis;tv;visual discomfort;stereoscopic video no reference subjective quality assessment visual discomfort visual fatigue;no reference	Visual discomfort/fatigue problems in 3D video have become an important issue. In this paper, we examine the factors that affect human perception of depth and visual comfort from stereoscopic video. For this, we conduct subjective quality assessments from which we extract four factors - temporal variance, disparity variation in intra-frames, disparity variation in inter-frames and disparity distribution of frame boundary areas. Based on these four factors, we design a no-reference stereoscopic video quality perception model (SV-QPM) as an objective quality assessment metric for stereoscopic video. The proposed SV-QPM does not require depth map but utilize the disparity information by simple estimation, and the model parameters are estimated based on linear regression. The experimental results show that our proposed model exhibits high consistency with subjective quality assessment results in terms of a Pearson correlation coefficient value of 0.808, and the prediction performance exhibits good consistency with zero outlier ratio value.	binocular disparity;coefficient;depth map;singular value decomposition;stereoscopy;systemverilog;video	Kwangsung Ha;Munchurl Kim	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6116176	subjective video quality;computer vision;simulation;visualization;visual perception;computer science;video quality;mathematics;multimedia;regression analysis;measurement;statistics	Robotics	63.35753849832205	-63.883058390399384	138015
b49ef8f8a65fc163cc4c60d4e18c41f328374544	interactive hyper spectral image rendering on gpu		In this paper, we describe a framework focused on spectral images rendering. The rendering of a such image leads us to three major issues: the computation time, the footprint of the spectral image, and the memory consumption of the algorithm. The computation time can be drastically reduced by the use of GPUs, however, their memory capacity and bandwidth (compared to their compute power) are limited. When the spectral dimension of the image will raise, the straightforward approach of the Path Tracing will lead us to high memory consumption and latency problems. To overcome these problems, we propose the DPEPT (Deferred Path Evaluation Path Tracing) which consists in decoupling the path evaluation from the path generation. This technique reduces the memory latency and consumption of the Path Tracing. It allows us to use an efficient wavelength samples batches parallelization pattern to optimize the path evaluation step and outperforms the straightforward approach when the spectral resolution of the simulated image increases.	algorithm;cas latency;computation;coupling (computer programming);graphics processing unit;high memory;parallel computing;path tracing;time complexity	Romain Hoarau;Eric Coiro;Sébastien Thon;Romain Raffin	2018		10.5220/0006549800710080	rendering (computer graphics);computer vision;computer science;artificial intelligence	Graphics	67.31956756362479	-52.117280341993	138093
36837a2c99ce142f504e359926cf49d3be251a89	disparity map enhancement based stereo matching method using optical flow		Estimating disparity from stereo images is a core subject in computer vision. However, the poorly-textured and ambiguous surfaces cannot be matched consistently using the conventional stereo matching method, in other words, the disparity values exist many errors produced by these noises. To solve these problems, based on the characteristics that the optical flow has good robustness to low texture and deep discontinuity, we propose a novel stereo disparity map enhancement approach to improve the accuracy of disparity values in the low texture as well as deep discontinuity regions, meanwhile generate a high-quality disparity map, through performing efficient fusions between the ELAS disparity map and the optical flow image. Experimental results show that the enhanced disparity map obtained by the proposed approach can decrease the bad pixel rate by 2.6% on average compared with the ELAS method, and display accurate and consistent structures robustly.	binocular disparity;computer stereo vision;computer vision;defective pixel;optical flow;reflections of signals on conducting lines	Chunhui Zhao;Bin Fan;Jinwen Hu;Zhiyuan Zhang;Quan Pan;Xiaoxu Wang	2018	2018 IEEE 14th International Conference on Control and Automation (ICCA)	10.1109/ICCA.2018.8444334	pixel;control theory;robustness (computer science);computer vision;engineering;optical flow;discontinuity (linguistics);artificial intelligence	Vision	56.1027876021256	-57.38806583297446	138139
acf272bc11f15900ef40985d5a7edcc483d45243	colorization for monochrome image with texture		Colorization is a computerized process of adding color to a monochrome image. The authors have developed colorization algorithms which propagate colors from seeded color pixels. Since those algorithms are constructed based on a region growing approach, failure colorization occurs at the place where a luminance changes intensely such as edge and texture. Although we developed, in the previous work, a partitioning algorithm for preventing the error propagation at edge, numerous color seeds were required for accurate colorization of the image with texture. This paper presents a new algorithm for colorization with texture by blending seeded colors. In our algorithm, the color can be estimated depending on the Euclidean distance and the luminance distance between each pixel to be colorized. It is shown that the proposed approach can be successfully applied to the images with texture by sowing a small number of color seeds. Introduction Colorization is a computerized process that adds color to a black and white print, movie and TV pro-gram, supposedly invented by Wilson Markle. It was initially used in 1970 to add color to footage of the moon from the Apollo mission. The demand of adding color to monochrome images such as BW movies and BW photos has been increasing. For example, in the amusement field, many movies and video clips have been colorized by human’s labor, and many monochrome images have been distributed as vivid images. In other fields such as archaeology dealing with historical monochrome data and security dealing with monochrome images by a crime prevention camera, we can imagine easily that colorization techniques are useful. A luminance value of a monochrome image can be calculated uniquely by a linear combination of RGB color components. However, searching for the RGB components from a luminance value poses conversely an ill-posed problem, because there is several corresponding color to one luminance value. Due to these ambiguous, human interaction usually plays a large role in the colorization process. The correspondence between a color and a luminance value is determined through common sense (green for grass, blue for the ocean) or by investigation. Even in the case of pseudo-coloring, where the mapping of luminance values to a set of RGB components is automatic, the choice of the color-map is purely subjective. Since there are a few industrial software products, those technical algorithms are generally not available. However, by operating those software products, it turns out that humans must meticulously hand-color each of the individual image subjectivity. There also exist a few patents for colorization. However, those approaches depend on heavy human operation. Recently, simple colorization algorithms have been proposed by a few research groups. In 2002, one of the authors proposed a colorization algorithm in which a small number of color seeds were sown on a monochrome image and the remaining pixels are colorized by propagating seeds’ color to adjacent pixels. The algorithm has been improved in Refs.5-9. In the same year, Welsh et al. colorize a monochrome image by transferring color from a reference color image with a stochastic matching. The concept of transferring color from one image to another image was inspired by work in Ref.11. In the Welsh’s method, the source image, which is the same kind of image as a monochrome image, is prepared and the colorization is performed by color matching between both pictures. After that, Levin et al. mark a monochrome image with some color scribbles and adjacent pixels are colorized by formulating and solving an optimization problem. Those conventional algorithms are very simple and work well as an intuitive impression, especially for the image which can be segmented to a few large regions with the same chrominance components. However, it was difficult to perform accurate colorization for texture. In order to obtain an accurate colorized result for a texture image, Horiuchi’s algorithm requires many color seeds. In the case of Welsh’s algorithm, a specific reference image is required and Levin’s algorithm requires many color scribbles. This study aims to develop a new colorization algorithm for monochrome images with texture. This paper organized as follows: Section 2 presents our conventional algorithm and shows the problem for an image with texture. Section 3 presents the proposed colorization algorithm, and Section 4 demonstrates experiments. Finally, we conclude with a discussion in Section 5. Conventional Colorization by Propagating Seeded Colors Independently The most advanced colorization algorithm for still monochrome images by the authors' works can be shown in Ref.7. In this section, the conventional algorithm is explained briefly and a problem about texture is shown. Let ) , ( y x I = be a pixel in an input monochrome image and let { } p p p p y x S 1 ) , ( = = be a set of color seeds, where P is the total number of the seeds. The color seeds, which are color pixels strictly, are given manually as a prior knowledge by a user. The position of the seeds and their color are determined by the user. Note that the color must be chosen with keeping the luminance of the original monochrome pixels. We present our method in CIELAB color space, each monochrome pixel I is transformed (a) Position of seven color seeds on the monochrome image. (b) Colorized image. Figure 1. A colorized result by the method in Ref.7. into the luminance signal ) (I L . Each color seeds p S is also transformed into ) ( ), ( ), ( p p p S b S a S L , respectively. In Ref.7, each pixel I is colorized by )) ( ( )), ( ( ), ( I f b I f a I L in CIELAB color space. The function ) (⋅ f selects a color seeds which have the minimum Euclidean distance, and defined as:       − = 2 min ) ( p p p S I S I f (1) where 2 ⋅ means the Euclidean distance in the X-Y image space. Figure 1 shows an example of colorization by using the algorithm in Ref.7. Figure 1(a) shows an input monochrome image and the position of color seeds expressed by red circles. Each seeds were sown at the center of the circle. In this example, seven seeds were sown on the monochrome image by the user. Figure 1(b) shows the colorized results. Better result was obtained. Figure 2 shows another example. The image consists of texture such as petals and trees. By sowing five color seeds as shown in Fig.2(a), a failure colorized result was obtained as shown in Fig.2(b). In order to obtain more accurate result, the user has to sow color seeds for each small region in the texture. In actual application, it is impossible to sow numerous seeds on each region. Reference 7 also proposed a partitioning algorithm to prevent the error propagation at edge. However, it is difficult to determine a threshold of partition. Even if the user can set the partition, failure estimation will be occurred after collapsing the partition. Moreover, the method produces visible artifacts of block distortion. In order to solve the problem, we propose a new colorization algorithm by blending seed color in the next section. Proposed Colorization by Blending Many Seed Color Decision of the Chrominance Components In our algorithm, we use two properties of natural images. The first property is that pixels with similar luminance values should have similar colors. This property was used for solving colorization problem in Levin’s algorithm. The second property is that near pixels should have similar colors. This property was used in Horiuchi’s algorithm. In the proposed method, we express those (a) Position of five color seeds on the monochrome image. (b) Colorized image. Figure 2. A failure example of colorization by the method in Ref.7. Figure 4. Gamut mapping on a*-b* plane by clipping. two properties by distances NED and NLD as follows. (NED: Normalized Euclidean distance) We define the first distance ] 1 , 0 [ ) , ( 1 ∈ p S I d between I and p S	algorithm;alpha compositing;clipping (computer graphics);color image;color management;color space;disk partitioning;distortion;emoticon;euclidean distance;experiment;graph coloring;human body weight;mathematical optimization;maxima and minima;monochrome;optimization problem;pixel;propagation of uncertainty;rate–distortion theory;region growing;software propagation;statement of work;texture mapping;the circle (file system);tree (data structure);video clip;well-posed problem	Takahiko Horiuchi;Hiroaki Kotera	2005			computer science;computer vision;artificial intelligence;chrominance;color image;image texture;rgb color model;monochrome;gamut;region growing;color space	Vision	59.289096166220524	-61.333036103036434	138165
b1951f4229133594bdb72b28d53ad1b43e78e42f	rectification of figures and photos in document images using bounding box interface	minimization;optimisation;interpolation;alternating optimization scheme;image segmentation;document images;digital cameras;photo rectification;image edge detection;optimisation document image processing image segmentation interpolation;bounding box interface;figure rectification;interpolation method;pixel;boundary interpolation method;perspective distortion removal;document image processing;image segmentation digital cameras interpolation lighting image reconstruction optical character recognition software robustness books image quality hardware;optimization;perspective distortion removal figure rectification photo rectification document images bounding box interface image segmentation alternating optimization scheme boundary interpolation method	This paper proposes an algorithm for the segmentation and rectification of figures and photos in document images. The algorithm requires just a rough user-provided bounding box for the objects in a single-view image. On receiving the user's bounding box, it takes about 1–2 seconds to segment and rectify mega-pixel sized figures. The main feature of the algorithm is a novel segmentation method that exploits the properties of printed figures. Specifically, a set of boundary candidates is generated using the properties, and the optimal boundary in the set is found by using an alternating optimization scheme. This segmentation result is further refined so that it is well localized to the true boundary. In addition to our segmentation method, we also propose a new boundary interpolation method for the rectification of segmented figures. The method improves the quality of output by largely removing perspective distortions compared to conventional boundary interpolation methods. Experimental results on a variety of images show that the method is efficient, robust, and easy to use.	algorithm;distortion;image rectification;image scanner;interpolation;mathematical optimization;minimum bounding box;pixel;printed circuit board;printing;rectifier;robustness (computer science)	Hyung Il Koo;Nam Ik Cho	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2010.5540071	computer vision;interpolation;computer science;theoretical computer science;image segmentation;pixel;computer graphics (images)	Vision	57.2205176448799	-57.105810447714084	138297
8ad10d3a356edcc70fe1cede6f6d505de612bfeb	content-weighted mean-squared error for quality assessment of compressed images	image content;image quality assessment iqa;image compression;mean squared error mse	Image quality assessment (IQA) has been intensively studied, especially for the full-reference (FR) scenario. However, only the mean-squared error (MSE) is widely employed in compression. Why other IQA metrics work ineffectively? We first sum up three main limitations including the computational time, portability, and working manner. To address these problems, we then in this paper propose a new content-weightedMSE (CW-MSE)method to assess the quality of compressed images. The design principle of our model is to use adaptive Gaussian convolution to estimate the influence of image content in a block-based manner, thereby to approximate the human visual perception to image quality. Results of experiments on six popular subjective image quality databases (including LIVE, TID2008, CSIQ, IVC, Toyama and TID2013) confirm the superiority of our CWMSE over state-of-the-art FR IQA approaches.	approximation algorithm;convolution;database;experiment;gaussian blur;image quality;jpeg;mean squared error;software portability;time complexity	Ke Gu;Shiqi Wang;Guangtao Zhai;Siwei Ma;Xiaokang Yang;Wenjun Zhang	2016	Signal, Image and Video Processing	10.1007/s11760-015-0818-9	computer vision;simulation;image compression;computer science;data mining	AI	62.73911546374815	-64.99140313484408	138540
0d861b6a26bb946e0667da2905ddf70e9d60eb17	distributed gradient-domain processing of planar and spherical images	poisson equation;streaming multigrid;panoramas;dynamic range;distributed solver;digitized sky survey;screened poisson equation;spherical parameterization	Gradient-domain processing is widely used to edit and combine images. In this article we extend the framework in two directions. First, we adapt the gradient-domain approach to operate on a spherical domain, to enable operations such as seamless stitching, dynamic-range compression, and gradient-based sharpening over spherical imagery. An efficient streaming computation is obtained using a new spherical parameterization with bounded distortion and localized boundary constraints. Second, we design a distributed solver to efficiently process large planar or spherical images. The solver partitions images into bands, streams through these bands in parallel within a networked cluster, and schedules computation to hide the necessary synchronization latency. We demonstrate our contributions on several datasets including the Digitized Sky Survey, a terapixel spherical scan of the night sky.	computation;digitized sky survey;distortion;gigapixel image;gradient;planar (computer graphics);seamless3d;solver;synchronization (computer science)	Michael M. Kazhdan;Dinoj Surendran;Hugues Hoppe	2010	ACM Trans. Graph.	10.1145/1731047.1731052	screened poisson equation;computer vision;mathematical optimization;dynamic range;computer science;theoretical computer science;poisson's equation;geometry;optics;quantum mechanics;computer graphics (images)	Graphics	66.5599381484108	-53.5800705795826	138567
8ae9a98b304c3cf62474460600c905c31d1bdeaa	gray scale cct compensation of mobile phone lcd			computational complexity theory;liquid-crystal display;mobile phone	In-Ho Song;Bong-Soo Kim;Sung-Hak Lee;Eun-Su Kim;Soo-Wook Jang;Kyu-Ik Sohng	2005			grayscale;embedded system;mobile phone;liquid-crystal display;computer science	Vision	61.12651877736056	-57.88653146968337	138895
e64f31cb0af0ee7910b6d379451b8c8c7b7c34f3	blue noise sampling with a pbf-based method	controllable;efficience;blue noise sampling;position based fluids	Inspired by the constant density constraints in position-based fluids (PBF), we propose a novel blue noise sampling algorithm. We formulate and solve a set of sampling points' positional constraints to enforce constant density, which is similar to the PBF. Points converge to distribute evenly and stochastically in space. Fourier spectral analysis of the converged points' distribution shows that it has great blue noise spectrum. By adjusting a single parameter, our method can generate blue noise samplings from Capacity Constrained Voronoi Tessellations (CCVT) to Lloyd's relaxation, and it can also trade off the noise and aliasing of samplings. We utilize the grid-based signed distance field to represent sampling regions, which makes our method fit for general dimensions. Varying the gird size can adjust the number of points. We put points at sampling boundary to solve the points deficiency. Adaptive sampling is achieved with a sampling density function. Experimental results show our method is efficient, stable and controllable in both two-dimensional (2D) plane sampling, and adaptive sampling. Moreover, it is suitable for image stippling.	adaptive sampling;aliasing;angular defect;colors of noise;converge;distance transform;linear programming relaxation;lloyd's algorithm;sampling (signal processing)	Xiuyu Zheng;Junjun Si;Shuaifu Dai	2016		10.1145/2949035.2949055	mathematical optimization;combinatorics;slice sampling;mathematics;statistics	Vision	64.11169214969601	-54.15148860005666	138903
4ca103a950df1d1d29013e8915e5717d8008377b	interpolation and zooming techniques utilizing edge-weighted adaptive filtering for color filter array	image zooming;color characteristics;image interpolation;red green and blue;image quality;color filter array;adaptive filter;spatial information;color image	A single sensor provided with a color filter array (CFA) format looks like a mosaic image. Each pixel in the image contains a primary color, red, green and blue, arranged alternately. Most previous interpolation and zooming techniques for CFA image utilize the gradient and the spatial information of the neighborhood color pixels around the interpolation positions. However, their characteristics of imaging color are not reconsidered. This paper proposes a new idea for CFA image interpolation and zooming techniques. The color characteristics such as the edge-weighted adaptive filtering (EWAF) and the edge modification are used to interpolate and spatially enlarge the images in order to reconstruct the original full-color images. The proposed method yields both objective and subjective image quality measurement. The experimental results show improved visualization of the images over the conventional methods. The evaluated performance is higher PSNR 1-2 dB.	adaptive filter;color filter array;gradient;image quality;interpolation;peak signal-to-noise ratio;pixel	Thanawat Sornnen;Woralak Kongdenfha;Werapon Chiracharit;Kosin Chamnongthai	2010		10.1007/978-3-642-15696-0_40	image quality;color co-site sampling;demosaicing;adaptive filter;color histogram;false color;rgb color model;computer vision;color filter array;color quantization;hsl and hsv;color depth;color image;image gradient;binary image;bayer filter;computer science;stairstep interpolation;high color;mathematics;spatial analysis;color balance;image scaling;computer graphics (images)	Vision	58.50494190608213	-64.62917732929668	138925
e2c3a16e5b961ba32ef7784caf2fc7e943a8d2a3	on the assessment of the quality of textures in visual media	databases;testing image databases visual databases application software quality assessment spatial databases pixel transform coding image coding image restoration;textures quality;quality metric;image coding;psnr;image processing;measurement;image databases;application software;computer graphics;next generation video coding;video processing;image restoration;testing;transform coding;computer graphic;image texture;human subjects;texture images;video coding;visualization;quality assessment;pixel;spatial databases;mean opinion score;next generation;full reference;texture images textures quality visual media quality assessment next generation video coding image restoration computer graphics mean opinion scores;wireless sensor networks;visual media;quality management;mean opinion scores;quality management image restoration image texture;visual databases	Textures are being extensively used in next-generation video coding, image restoration and computer graphics wherein the stochastic and perceptual properties of textures are exploited. Given the prominence of texture content in image and video processing applications, texture quality assessment and objective quality metrics geared specifically towards texture are of particular importance. This paper outlines an extensive subjective study in which a total of 340 distorted images extracted from 10 reference images were evaluated by a group of 20 human subjects. The subjective Mean Opinion Scores (MOS) thus obtained from the study were used to evaluate several existing mainstream full reference quality metrics with the help of a recently developed quality assessment framework (IVQUEST). Based on the results obtained, it can be concluded that traditional objective metrics do not perform well on texture images and there is a need for specialized texture quality metrics.	circuit restoration;computer graphics;data compression;distortion;image restoration;performance evaluation;video processing	Milind S. Gide;Lina J. Karam	2010	2010 44th Annual Conference on Information Sciences and Systems (CISS)	10.1109/CISS.2010.5464741	mean opinion score;image texture;subjective video quality;image restoration;computer vision;quality management;application software;transform coding;wireless sensor network;visualization;peak signal-to-noise ratio;image processing;computer science;multimedia;software testing;video processing;computer graphics;pixel;measurement;computer graphics (images)	Graphics	62.436939351593	-63.97089789204904	139207
210ba72ae00e29ccd5bd1a87fcbb2dd2dff14e6d	3d-visualization techniques, including samples and applications	visual perception data visualisation digital photography stereo image processing;art;sar imagery;3d imaging;3d visualization;computer graphics;virtual 3d visualization techniques;high resolution imaging;photography;forensics photography documentation cameras head art spatial resolution australia computer graphics high resolution imaging;sar imagery virtual 3d visualization techniques 3d images forensic 3d photography stereoview collection 3d niche photography telescope rod lite platform aerial photography;3d nichephoto graphy;digital photography;data visualisation;3d nichephoto;stereoview collection;mathematical 3dmodel geo visualization 3d sar 3d nichephoto graphy 3d rod camera lite;stereo image processing;3d sar;3d rod camera lite;3d niche photography;telescope rod lite platform;geo visualization;visual perception;head;3d images;graphy;forensic 3d photography;mathematical 3dmodel;cameras;forensics;aerial photography;documentation;australia;spatial resolution	"""This paper deals with new aspects of 3D- visualization techniques as state-of the-art tools for recording, documentation, interpretation. To give an idea of the outstanding documentary value of 3D-images and of its broad range of applications, masterpieces for stereoviews and for forensic 3Dphotography are initially presented. Since these samples represent huge stereoview collections, this paper also deals with international 3D- imagearchives. First time gaining """"3D-niche-photography"""" based on a telescope-rod-Lite-platform is introduced. Finally a new way to derive terrain-heights from image-parallax-values is shown for aerial photography and for SAR-imagery, as well as a completed list of virtual 3D-visualization methodology"""	adobe flash lite;aerial photography;documentation;interpretation (logic);niche blogging;parallax;stereoscopy	Walter Schuhr;Erich Kanngieser	2006	International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)	10.1109/CGIV.2006.5	computer vision;geography;remote sensing;computer graphics (images)	Visualization	63.65878159388615	-57.33778621167829	139230
fed9e53f321041a8062e3a2f630f58d6e7f39caf	an adaptive nonlocal regularized shadow removal method for aerial remote sensing images	geophysical image processing;remote sensing image color analysis land surface image edge detection compounds adaptation models image reconstruction;spatially adaptive aerial images nonlocal nl operators shadow removal soft shadow;remote sensing;soft shadow approach adaptive nonlocal regularized shadow removal method aerial remote sensing images urban scenes natural images nonlocal operators traditional binary hard shadow shadow free image shadow free results;remote sensing geophysical image processing	Shadows are evident in most aerial images with high resolutions, particularly in urban scenes, and their existence obstructs the image interpretation and the following application, such as classification and target detection. Most current shadow removal methods were proposed for natural images, whereas shadows in remote sensing images show distinct characteristics. We have therefore analyzed the characteristics of shadows in aerial images, and in this paper, we propose a new shadow removal method for aerial images, using nonlocal (NL) operators. In the proposed method, the soft shadow is introduced to replace the traditional binary hard shadow. NL operators are used to regularize the shadow scale and the updated shadow-free image. Furthermore, a spatially adaptive NL regularization is introduced to handle compound shadows. The combination of the soft shadow and NL operators yields satisfying shadow-free results, preserving textures and holding regular color. Different types of shadowed aerial images are employed to verify the proposed method, and the results are compared with two other methods. The experimental results confirm the validity of the proposed method and the advantage of the soft-shadow approach.	aerial photography;aharonov–bohm effect;manifold regularization;nl (complexity);nonlocal lagrangian;shadow volume	Huifang Li;Liangpei Zhang;Huanfeng Shen	2014	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2012.2236562	computer vision;shadow and highlight enhancement;shadow mapping;optics;physics;remote sensing	Vision	67.59971444234839	-65.58362747042416	139625
549f1a021e59cd21ad20c32091636d55813eaa5b	halftone moiré due to imager distortion	printing;diseno circuito;posicionamiento;metodo vectorial;halftones;asymmetry;imageur;raster;circuit design;specification;scanneur;imager;asymetrie;escaner;scanner;printer;distortion;positioning;scanners;imaging system;especificacion;vector method;imaging;trame;imprimante;impression;impresora;asimetria;formation image;methode vectorielle;conception circuit;formacion imagen;impresion;imaging systems;trama;positionnement	Individual halftone color separations must possess a low degree of distortion to avoid undesirable moiré in the overlays that produce the process colors. Achieving low relative distortion requires precise registration between the exposure devices used to write the halftone separations. However, optical and mechanical errors within the multiple Raster Output Scanners (ROS’s) or image bars of a printer result in differences in the trajectory and placement of the exposure spots among color planes. In this paper, color halftone moiré due to ROS errors is analyzed using a frequency vector representation of color halftones. We analyze three forms of process-direction distortion: skew, shear, and bow. Each distortion is inspired from a practical printing system (i.e. while shear and bow are observed in ROS systems, skew is observed in image bar imaging systems). The frequency vector formalism is used to derive bounds on distortion for a classical halftone screen configuration (square cell equal frequency halftones at 15◦, 45◦, and 75◦). The bounds are examined for distortion of one halftone screen and the analysis can be readily applied to distortion of multiple screens. The bounds can be used to develop specifications for imaging components in the design of a ROS or image bar imaging system.	color;dr-dos;distortion;formal system;image sensor;lateral thinking;printer (computing);printing	Orhan Bulan;Robert P. Loce;Beilei Xu	2010		10.1117/12.838867	medical imaging;computer vision;stochastic screening;raster graphics;distortion;telecommunications;circuit design;optics;specification;asymmetry	Robotics	62.84381665198582	-60.012312426565906	139632
65a9ca5216682a8914995dfc281a206d63383ddb	one scan shadow compensation and visual enhancement of color images	histograms;filtering;image coding;image processing;data compression;color space;ycbcr color space scan shadow compensation visual enhancement dynamic range compression still color images recursive filtering technique contrast stretching technique statistical measures logarithmic image processing model image representation rgb color space;recursive filters;color image processing;color image processing dynamic range compression shadow compensation recursive filtering logarithmic model;statistical analysis data compression image coding image colour analysis image enhancement image representation recursive filters;image enhancement;statistical analysis;shadow compensation;logarithmic model;image color analysis;image colour analysis;image representation;pixel;color dynamic range image coding filtering image processing humans histograms filters digital cameras heuristic algorithms;dynamic range;low light;recursive filtering;dynamic range compression;color image	The paper presents a new technique of efficient dynamic range compression and shadow compensation for still color images. The proposed method enhances low light areas while preserving the colors and details, without generating visual artifacts. The approach is based on recursive filtering and contrast stretching techniques, driven by statistical measures of the image and implemented under a logarithmic image processing model. The implementation can be used for any image represented in the RGB or YCbCr color spaces.	color space;dynamic range;image compression;image processing;normalization (image processing);recursion (computer science);shadow volume;visual artifact	Felix Albu;Constantin Vertan;Corneliu Florea;Alexandru Drimbarean	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414437	data compression;filter;color histogram;false color;rgb color model;computer vision;dynamic range;dynamic range compression;color quantization;color depth;color image;image processing;computer science;high color;histogram;mathematics;color balance;color space;pixel;computer graphics (images)	Robotics	58.7380655680528	-62.69467266401503	139657
7a4774ae43223e28d5c7e34d2b6faad81031e9b3	designing effective inter-pixel information flow for natural image matting		We present a novel, purely affinity-based natural image matting algorithm. Our method relies on carefully defined pixel-to-pixel connections that enable effective use of information available in the image and the trimap. We control the information flow from the known-opacity regions into the unknown region, as well as within the unknown region itself, by utilizing multiple definitions of pixel affinities. This way we achieve significant improvements on matte quality near challenging regions of the foreground object. Among other forms of information flow, we introduce color-mixture flow, which builds upon local linear embedding and effectively encapsulates the relation between different pixel opacities. Our resulting novel linear system formulation can be solved in closed-form and is robust against several fundamental challenges in natural matting such as holes and remote intricate structures. While our method is primarily designed as a standalone natural matting tool, we show that it can also be used for regularizing mattes obtained by various sampling-based methods. Our evaluation using the public alpha matting benchmark suggests a significant performance improvement over the state-of-the-art.	affinity analysis;algorithm;arc diagram;benchmark (computing);information flow (information theory);jean;linear system;matte display;nonlinear dimensionality reduction;pixel;refinement (computing);sampling (signal processing)	Yagiz Aksoy;Tunç Ozan Aydin;Marc Pollefeys	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.32	pattern recognition;pixel;robustness (computer science);artificial intelligence;computer vision;information flow (information theory);computer science;linear system	Vision	54.61785481839239	-53.76898623944855	139888
581e5db2afbcbd42fedb16da7125883317d56624	depth-map super-resolution for asymmetric stereo images	image coding;image resolution;depth map superresolution average psnr gain low resolution color image stereo color plus depth image mixed resolution coding asymmetric stereo images;multiview super resolution mixed resolution depth map;stereo image processing image coding image colour analysis image resolution;image resolution color computer architecture image coding interpolation video coding decoding;image colour analysis;stereo image processing	We propose a mixed-resolution coding architecture for stereo color-plus-depth images, where encoding is performed at a low resolution, except for one of the color images. Super-resolution methods are proposed for the depth maps and for the low-resolution color image at the decoder side. Experiments are carried out for several real and synthetic images and reveal a reduction in complexity at the encoder associated with average PSNR gains of the super-resolved low-resolution color image with respect to regular interpolation.	color image;depth map;encoder;image resolution;interpolation;peak signal-to-noise ratio;super-resolution imaging;synthetic intelligence	Diogo C. Garcia;Camilo C. Dorea;Ricardo L. de Queiroz	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6637911	demosaicing;color histogram;image texture;stereo camera;computer vision;feature detection;image resolution;color image;image gradient;binary image;image processing;computer science;digital image processing;sub-pixel resolution;image formation;computer graphics (images)	Vision	58.83761217447717	-56.8117116925334	140215
2e089cded3ff66dbd25c4115c1ee4811bd140242	dual lookup table algorithm: an enhanced method of displaying 16-bit gray-scale images on 8-bit rgb graphic systems	image features;personal computer;real time;red green blue;graphics system;graphics hardware;region of interest;animal imaging;lookup table	Most digital radiologic images have an extended contrast range of 9 to 13 bits, and are stored in memory and disk as 16-bit integers. Consequently, it is difficult to view such images on computers with 8-bit red-green-blue (RGB) graphic systems. Two approaches have traditionally been used: (1) perform a one-time conversion of the 16-bit image data to 8-bit gray-scale data, and then adjust the brightness and contrast of the image by manipulating the color palette (palette animation); and (2) use a software lookup table to interactively convert the 16-bit image data to 8-bit gray-scale values with different window width and window level parameters. The first method can adjust image appearance in real time, but some image features may not be visible because of the lack of access to the full contrast range of the image and any region of interest measurements may be inaccurate. The second method allows “windowing” and “leveling” through the full contrast range of the image, but there is a delay after each adjustment that some users may find objectionable. We describe a method that combines palette animation and the software lookup table conversion method that optimizes the changes in image contrast and brightness on computers with standard 8-bit RGB graphic hardware—the dual lookup table algorithm. This algorithm links changes in the window/level control to changes in image contrast and brightness via palette animation. The purpose of the algorithm is to use palette animation to mimic changes in image appearance performed by the software lookup table method after the window width and window level parameters have changed. The algorithm combines the advantages of both methods: rapid manipulation of image brightness and contrast by palette animation, and the ability to window and level on the full 16-bit image data using the software lookup table. This algorithm may be useful for applications that display 16-bit radiologic images on computers with standard 8-bit RGB graphic systems.	16-bit;8-bit;algorithm;animation;color cycling;computer;computers;dual;grayscale;interactive media;kernel bandwidth;lookup table;palette (computing);radiology;real-time computing;region of interest;sea-blue histiocyte syndrome;brightness;width	Thurman Gillespy;Alan H. Rowberg	1994	Journal of Digital Imaging	10.1007/BF03168474	rgb color model;computer vision;lookup table;computer hardware;computer science;graphics hardware;feature;computer graphics (images);region of interest	Graphics	60.95806075588321	-60.9792362723145	140408
ba3e69ca38a19694370df17dac9d635353cff526	screening and moiré suppression in printing and its analysis by fourier transform	diagrama moare;moire pattern;image processing;fourier transform;interferencia optica;performance;procesamiento imagen;optical interference;traitement image;fourier transformation;diagramme moire;imaging;transformation fourier;characterization;interference optique;formation image;formacion imagen;rendimiento;caracterisation;caracterizacion;transformacion fourier	Moir6 p a t t e r n s are o f t e n observed i n p r i n t i n g when t h e image i s reproduced as a b i l e v e l image wi th h a l f t o n e . A Four i e r t ransform a n a l y s i s of screening a p i c t u r e by an o p t i c a l method o r an ordered d i t h e r method is p resen ted . Conventional moirgsuppressing techniques are d i scussed conside r i n g t h e human v i s u a l system. The moire f r i n g e s are explained as a l i a s i n g and v i s i b l e moire f r i n g e s are e s p e c i a l l y due t o highfrequency s p e c t r a wi th high power. t h i s viewpoint, a sc reen ing method i s proposed which u s e s t h e Four i e r t ransform f o r moire suppression. This method has t h e addit i o n a l advantage t h a t any p i t c h and any screen a n g l e may be s e l e c t e d . From	printing;taito l system;zero suppression	Yoshiharu Morimoto;Yasuyuki Seguchi;Masao Okada	1990	Systems and Computers in Japan	10.1002/scj.4690210210	fourier transform;image processing;mathematics	AI	62.829622478111844	-59.952386155723914	140440
732fd9b8e6fee19793021b99724c2c04374a75fb	highly accurate image reconstruction for multimodal noise suppression using semisupervised learning on big data		"""Impulse noise corruption in digital images frequently occurs because of errors generated by noisy sensors or communication channels, such as faulty memory locations in devices, malfunctioning pixels within a camera, or bit errors in transmission. Although recently developed big data streaming enhances the viability of video communication, visual distortions in images caused by impulse noise corruption can negatively affect video communication applications. In addition, <inline-formula><tex-math notation=""""LaTeX"""">${{sparsity}}$</tex-math></inline-formula>, <inline-formula><tex-math notation=""""LaTeX"""">${{density}}$</tex-math></inline-formula>, and <inline-formula> <tex-math notation=""""LaTeX"""">${{multimodality}}$</tex-math></inline-formula> in large volumes of noisy images have often been ignored in recent studies, whereas these issues have become important because of the increasing viability of video communication services. To effectively eliminate the visual effects generated by the impulse noise from the corrupted images, this study proposes a novel model that uses a devised cost function involving semisupervised learning based on a large amount of corrupted image data with a few labeled training samples. The proposed model qualitatively and quantitatively outperforms the existing state-of-the-art image reconstruction models in terms of the denoising effect."""	big data;digital image;distortion;impulse noise (audio);iterative reconstruction;loss function;multimodal interaction;noise reduction;noise shaping;pixel;semi-supervised learning;sensor;visual effects;zero suppression	Jia-Li Yin;Bo-Hao Chen;Ying Li	2018	IEEE Transactions on Multimedia	10.1109/TMM.2018.2820910	pixel;digital image;computer vision;computer science;artificial intelligence;iterative reconstruction;pattern recognition;bit error rate;noise reduction;noise measurement;impulse noise;communication channel	Vision	58.75574056639437	-58.273326670210295	140504
0e0667a43a35a6ba4f2df955d0c9633d6555f712	dctune perceptual optimization of compressed dental x-rays	data transmission;dentistry;optimisation;medical imagery;data compression;optimizacion;video compression;medical science;radiography;scanners;dent;pixels;tecnica;imagerie medicale;x ray radiography;diente;optimization;digital data;imageneria medical;compresion dato;video data;radiographie rx;color photography;quality control;technique;images;compression donnee;tooth;radiografia rx;x ray imagery;x rays	This is a brief report on research on the subject of DCTune optimization of JPEG compression of dental x-rays. DCTune is a technology for optimizing DCT quantization matrices to yield maximum perceptual quality for a given bit-rate, or minimum bit-rate for a given perceptual quality. In addition, the technology provides a means of setting the perceptual quality of compressed imagery in a systematic way. We optimized matrices for a total of 20 images at two resolutions (150 and 300 dpi) and four bit-rates (0.25, 0.5, 0.75, 1.0 bits/pixel), and examined structural regularities in the resulting matrices. We also conducted some brief psychophysical studies to validate the DCTune quality metric and to demonstrate the visual advantage of DCTune compression over standard JPEG.	discrete cosine transform;jpeg;mathematical optimization;pixel;quantization (image processing)	Andrew B. Watson;Mathias Taylor;Robert Borthwick	1997		10.1117/12.273914	computer vision;engineering;optics;computer graphics (images)	Graphics	61.870375098060556	-62.922173368258115	140515
40d0399dc5063bcfa7f2f73fc16296f60c0ff359	noise reduction and adaptive contrast enhancement for local tone mapping	discrete wavelet transforms;contrast enhancement;tone mapping;high dynamic range image;decomposed images;compressed luminance;image coding;discrete wavelet transform;data compression;multiscale subbands;tone mapped image;color;subband decomposition;adaptive contrast enhancement;adaptive saturation control parameter;image color analysis image coding noise reduction filtering algorithms noise color algorithm design and analysis;image enhancement;filtering algorithms;image color analysis;tone mapping bilateral filter contrast enhancement high dynamic range noise reduction subband decomposition;noise reduction;noise reduction method;computer simulation noise reduction method adaptive contrast enhancement local tone mapping high dynamic range image compressed luminance multiscale subbands discrete wavelet transform decomposed images bilateral filter soft thresholding tone mapped image adaptive saturation control parameter;soft thresholding;bilateral filter;high dynamic range;image enhancement data compression digital simulation discrete wavelet transforms filtering theory image coding;computer simulation;algorithm design and analysis;filtering theory;local tone mapping;digital simulation;noise	In this paper, we propose a noise reduction method and an adaptive contrast enhancement for local tone mapping (TM). The proposed local TM algorithm compresses the luminance of high dynamic range (HDR) image and decomposes the compressed luminance of HDR image into multi-scale subbands using the discrete wavelet transform. For noise reduction, the decomposed images are filtered using a bilateral filter and soft-thresholding. And then, the dynamic ranges of the filtered subbands are enhanced by considering local contrast using the modified luminance compression function. Finally, the color of the tone-mapped image is reproduced using an adaptive saturation control parameter. We generate the tone-mapped image using the proposed local TM. Computer simulation with noisy HDR images shows the effectiveness of the proposed local TM algorithm in terms of visual quality as well as the local contrast. It can be used in various displays with noise reduction and contrast enhancement.	algorithm;bilateral filter;computer simulation;discrete wavelet transform;haar wavelet;high dynamic range;high-dynamic-range imaging;high-dynamic-range rendering;horseland;image noise;image quality;lh (complexity);ll parser;noise reduction;one-way compression function;rendering (computer graphics);soft error;thresholding (image processing);tone mapping;video post-processing;zero suppression	Ji Won Lee;Rae-Hong Park;SoonKeun Chang	2012	IEEE Transactions on Consumer Electronics	10.1109/TCE.2012.6227463	computer simulation;computer vision;electronic engineering;tone mapping;computer science;mathematics;computer graphics (images)	Visualization	57.78571874888759	-64.12491077264869	141210
13199cab8c38c809254150118e73200072c19b3c	joint ihs and variational methods for pan-sharpening of very high resolution imagery	geophysical image processing;image resolution;transforms feature extraction geophysical image processing geophysical techniques image fusion image resolution;image fusion;gradient descend flow image fusion pan sharpening ihs fusion variational fusion;remote sensing image color analysis image fusion spatial resolution transforms optimization;feature extraction;transforms;ihs bt ihs method variational method very high resolution imagery pan sharpening approach panchromatic image multispectral image image merging intensity component ihs transform energy functional spatial detail extraction pan image spectral signature preservation optimization solution ikonos dataset quickbird dataset fusion model extended fast ihs efihs awlp;geophysical techniques	A new pan-sharpening approach is proposed for merging the panchromatic image and the multi-spectral image by jointly using IHS and variational method. The substitute image of the intensity component of IHS transform is the optimization solution of the proposed energy functional. The functional consists of three energy terms. The first term extracts the spatial details from the Pan image and injects the details into the MS image. The second term preserves the spectral signature of the MS image by requiring that the low-resolution version of the substitute image should be close to the intensity component. The third term guarantees the smoothness of the optimization solution. The proposed method is validated on the IKONOS and QuickBird dataset. In comparison with the fusion models including extended fast IHS (eFIHS), AWLP and IHS-BT, the prospective fusion results are presented by the proposed model.	calculus of variations;image resolution;mathematical optimization;multispectral image;prospective search	Zeming Zhou;Pinglv Yang;Yuanxiang Li;Wen Yin;Lin Jiang	2013	2013 IEEE International Geoscience and Remote Sensing Symposium - IGARSS	10.1109/IGARSS.2013.6723354	computer vision;feature detection;image resolution;feature extraction;computer science;machine learning;image fusion;remote sensing	Vision	67.54020938848356	-66.00915191200959	141596
af17442228e39396dd0555572e6c380041231c44	the highly lose image inpainting method based on human vision	data transmission;image inpainting method;human vision;image resolution;visual communication;noise interference;image multiresolution analysis;image restoration;image texture;wavelet transforms;wavelet transform;image color analysis;image colour analysis;image data transmission;humans interference image storage propagation losses data communication image restoration image texture frequency wavelet analysis wavelet transforms;image inpainting;image data storage;multi resolution;wavelet transforms image colour analysis image resolution image restoration image texture visual communication;image multiresolution analysis image inpainting method human vision noise interference image data transmission image data storage image restoration wavelet transformation image texture image color analysis;spatial frequency;wavelet transformation;principal component	Currently, noise interference and data loss are two major problems that affect the processing results of image data transmission and storage. In order to restore damaged image data effectively, we propose a novel image inpainting technique based on wavelet transformation. The primary feature of our proposed technique is to separate the given image into two principal components which encompass image texture and color respectively. Then, according to the distinctive qualities of the given image, various image inpainting methods are adopted to perform image repair. By taking advantage of the separation of an image into its individual frequency components, we use the multi-resolution characteristics of wavelet transform, from the lowest spatial-frequency layer to the higher one, to analyze the image from global-area to local-area progressively. In order to substantiate the effectiveness of our proposed image inpainting method, we employed various images subject to high noise interference and/or extensive data loss or distortion. The experimental results were perfect, even if the distortion portions of the repaired images were higher than 90%	distortion;image texture;inpainting;interference (communication);wavelet transform	Ching-Tang Hsieh;Eugene Lai;Ping-Da Yang;Yen-Liang Chen	2006	First International Conference on Innovative Computing, Information and Control - Volume I (ICICIC'06)	10.1109/ICICIC.2006.363	image quality;image warping;image texture;image restoration;computer vision;feature detection;binary image;image processing;computer science;pattern recognition;mathematics;top-hat transform;wavelet transform;computer graphics (images)	Vision	57.29153068874604	-62.46111554764897	141626
8342ac9bf451801be517d8787491b08f02f55b12	directional super-resolution by means of coded sampling and guided upsampling	interpolation;light emitting diodes;light stage systems directional super resolution guided upsampling guided super resolution technique image correspondences depth estimation best matching multidimensional patches directional images iterative guided upsampling sampling code estimation lightfield camera arrays;estimation;sensor arrays cameras image coding image resolution image sampling iterative methods;cameras spatial resolution estimation interpolation lighting light emitting diodes;lighting;cameras;spatial resolution	We present a simple guided super-resolution technique for increasing directional resolution without reliance on depth estimation or image correspondences. Rather, it searches for best-matching multidimensional (4D or 3D) patches within the entire captured data set to compose new directional images that are consistent in both the spatial and the directional domains. We describe algorithms for guided upsampling, iterative guided upsampling, and sampling code estimation. Our experimental results reveal that the outcomes of existing light-field camera arrays and lightstage systems can be improved without additional hardware requirements or recording effort simply by realignment of cameras or light sources to change their sampling patterns.	algorithm;iterative method;light field;requirement;sampling (signal processing);super-resolution imaging;upsampling	David C. Schedl;Clemens Birklbauer;Oliver Bimber	2015	2015 IEEE International Conference on Computational Photography (ICCP)	10.1109/ICCPHOT.2015.7168365	computer vision;mathematics;optics;computer graphics (images)	Robotics	59.7358338226796	-58.00532959244312	141835
447f6b9033506a6546af30ca6514827faaef3922	reconstruction of high-resolution facial image using recursive error back-projection	reconstruction of high resolution facial image based on a recursive error back projection;vol 31 no 1 b;joeng seon park;한국정보과학회 2004년도 봄 학술발표논문집 제31권 제1호 b;korea information science society;한국정보과학회;seong whan lee	This paper proposes a new method of reconstructing highresolution facial image from a low-resolution facial image using a recursive error back-projection of example-based learning. A face is represented by a linear combination of prototypes of shape and texture. With the shape and texture information about the pixels in a given lowresolution facial image, we can estimate optimal coefficients for a linear combination of prototypes of shape and those of texture by solving least square minimization. Then high-resolution facial image can be reconstructed by using the optimal coefficients for linear combination of the high-resolution prototypes. Moreover recursive error back-projection is applied to improve the accuracy of high-resolution reconstruction. An error back-projection is composed of estimation, simulation, and error compensation. The encouraging results of the proposed method show that our method can be used to improve the performance of the face recognition by applying our method to enhance the low-resolution facial images captured at visual surveillance systems.	coefficient;facial recognition system;image resolution;pixel;recursion (computer science);simulation	Jeong-Seon Park;Seong-Whan Lee	2004		10.1007/978-3-540-25948-0_9	computer vision;speech recognition;computer science;pattern recognition	Vision	57.661854056374395	-56.369179241643565	141841
1b3ca9a9dbb5ec118cfd9211958302313e8d987c	fast computation of seamless video loops	video textures;blend aware consistency;cinemagraphs	Short looping videos concisely capture the dynamism of natural scenes. Creating seamless loops usually involves maximizing spatiotemporal consistency and applying Poisson blending. We take an end-to-end view of the problem and present new techniques that jointly improve loop quality while also significantly reducing processing time. A key idea is to relax the consistency constraints to anticipate the subsequent blending, thereby enabling looping of low-frequency content like moving clouds and changing illumination. We also analyze the input video to remove an undesired bias toward short loops. The quality gains are demonstrated visually and confirmed quantitatively using a new gradient-domain consistency metric. We improve system performance by classifying potentially loopable pixels, masking the 2D graph cut, pruning graph-cut labels based on dominant periods, and optimizing on a coarse grid while retaining finer detail. Together these techniques reduce computation times from tens of minutes to nearly real-time.	algorithmic efficiency;alpha compositing;computation;computational complexity theory;control flow;cut (graph theory);end-to-end principle;gradient;graph cuts in computer vision;mobile device;motion estimation;pixel;real-time clock;seamless3d;shake;smartphone;variable shadowing	Jing Liao;Mark Finch;Hugues Hoppe	2015	ACM Trans. Graph.	10.1145/2816795.2818061	computer vision;real-time computing;simulation;computer science;theoretical computer science;machine learning;computer graphics (images)	Graphics	58.861871282356695	-55.04494765645672	141876
e7aca73c3a0459d6fee7ff41508977e15f88c18b	color enhancement with adaptive illumination estimation for low-backlighted displays	lighting reflectivity table lookup image color analysis brightness estimation image edge detection;adaptive illumination estimation image decomposition color saturation low backlighted display	In this paper, we propose an image enhancement system to maintain human visual perception for low-backlighted LCD or LED displays. Adopting the low-backlight mode can save electrical power and extend battery life. First, we examine the relationship between the image and backlight to maintain visual perceptual quality. Then, we use an adaptive illumination estimation to decompose the image intensity into an illumination layer and a reflectance layer with multiscale and parallel perspectives. We then refer to the given backlight level and our derived image-backlight relationship to compensate the illumination layer and meanwhile enhance the reflectance layer. Finally, we also boost the color saturation. Experiments using both numerical blind/referenceless image spatial quality evaluator and subjective mean opinion score demonstrate that our system outperforms existing systems.	backlight;image editing;interpreter (computing);liquid-crystal display;numerical analysis	Soo-Chang Pei;Chih-Tsung Shen	2017	IEEE Transactions on Multimedia	10.1109/TMM.2017.2688924	computer science;mean opinion score;artificial intelligence;computer vision;backlight;brightness;visual perception;liquid-crystal display;reflectivity	Vision	59.9687990830295	-62.21089967189942	142060
6574f3f2ab75cb2b06df13c521f5a2a8760acb59	application of three dimensional shape from image focus in lcd/tft displays manufacturing	tft displays;two dimensional images;image focus;shape focusing thin film transistors three dimensional displays liquid crystal displays manufacturing image reconstruction cameras filters glass;optics;optical filters;tft lcd panels;three dimensional shape;2d images;lcd color filters;auto grinding equipment;liquid crystal displays;2d window;lcd tft displays manufacturing;three dimensional object structure;indexing terms;optical filters computer vision electronic equipment manufacture image reconstruction liquid crystal displays;three dimensional;3d object structure;computer vision;lcd color filters displays manufacturing tft displays lcd displays image focus three dimensional shape three dimensional object structure 3d object structure computer vision two dimensional images 2d images optics thin lens gaussian law 2d window camera mounted microscope auto grinding equipment;protrusions;protrusions three dimensional shape image focus lcd tft displays manufacturing computer vision shape from focus thin lens gaussian law camera mounted microscope tft lcd panels;image reconstruction;shape from focus;displays manufacturing;thin lens gaussian law;electronic equipment manufacture;3d structure;3d shape reconstruction;optical filters cameras computer vision image reconstruction liquid crystal displays;cameras;point of interest;camera mounted microscope;lcd displays	One of the fundamental objectives of computer vision is to reconstruct a three-dimensional (3D) structure of objects from two-dimensional (2D) images. Shape from focus (SFF) is the problem of reconstructing the depth of the scene changing actively the optics of the camera until the point of interest is in focus. The point in focus gives information about its depth through the thin lens Gaussian law. Previous SFF algorithms use 2D window for finding the best focused points. In this paper, we propose to use 3D window for finding the best focused points. The new SFF algorithm is applied on the camera mounted microscope for developing an auto-grinding equipment of LCD color filters. In order to develop thin and bright displays, TFT-LCD is used nowadays. In the manufacturing process of TFT-LCD panels, cells and color filters are put together. The protrusion which is generated by combining the two unequal sizes (difference in microns) of glasses in the construction of LCD panels causes a big problem. Therefore, the protrusion is cut to the proper height before the two glasses are put together. Usually, the number of cells in a typical TFT/LCD display is in millions. The time for the checking and cutting of protrusions greatly affects the cost of manufacturing. We, therefore, propose the use of the proposed SFF algorithms for the said problem for fast and accurate process.	algorithm;computer vision;gaussian blur;lcd television;microsoft windows;point of interest;thin-film-transistor liquid-crystal display	Muhammad Bilal Ahmad;Tae-Sun Choi	2007	2007 Digest of Technical Papers International Conference on Consumer Electronics	10.1109/TCE.2007.339492	computer vision;computer science;engineering;liquid-crystal display;computer graphics (images)	Robotics	59.85764258625999	-54.60178450210244	142158
9193859e34a5f692c2da16df6eb874feb7b89471	subjective and objective quality assessment for images with contrast change	databases image quality measurement entropy histograms psnr quality assessment;visual databases entropy image enhancement statistical analysis;order statistics contrast changed image database mean opinion score mos image quality assessment iqa entropy;image enhancement;statistical analysis;objective quality assessment mainstream image quality assessment methods tid2008 databases csiq databases image histograms order statistics entropies reduced reference image quality metric mos mean opinion scores natural images cid2013 database contrast changed image database iqa image processing contrast enhancement subjective quality assessment;entropy;visual databases	It is widely known that, for most natural images, appropriate contrast enhancement can usually lead to improved subjective quality. Despite of its importance to image processing, contrast change has largely been overlooked in the current research of image quality assessment (IQA). To fill this void, in this paper we first report a new and dedicated contrast-changed image database (CID2013). The CID2013 database is composed of four hundred contrast-changed images of fifteen original natural images and the mean opinion scores (MOSs) recorded from twenty-two inexperienced viewers. We then proposed a novel reduced-reference image quality metric for contrast-changed images (RIQMC) using entropies and order statistics of the image histograms. Experimental results on the CID2013, TID2008, and CSIQ databases demonstrate that the proposed RIQMC metric outperforms some mainstream image quality assessment methods.	database;experience;image histogram;image processing;image quality	Ke Gu;Guangtao Zhai;Xiaokang Yang;Wenjun Zhang;Min Liu	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738079	image quality;subjective video quality;computer vision;entropy;computer science;data mining;information retrieval;statistics	Vision	62.15032653362417	-63.87036876276673	142342
d80d2542831daade23a6f8d1678d4f0f31889ab7	subjective and objective quality evaluation of compressed point clouds		The increasing availability of point cloud data in recent years is demanding high performance compression solutions. Naturally, methods to perform objective quality assessment of compressed point clouds are also very much needed, namely metrics to measure the geometry distortion of point clouds when positioning errors are present. This is a rather challenging problem since this 3D representation format is unstructured and it is typically not directly visualized. In this context, the objective of this paper is to perform subjective and objective quality assessment of point clouds degraded by compression artifacts and to evaluate the correlation of the most popular objective quality metrics with human perception. In this work, subjective experiments conducted at Instituto Superior Técnico (IST) are described with point clouds compressed with two different but yet promising solutions, one based on the octree representation of the 3D space and another based on the rather popular graph transform. As far as the authors know, this is the first study of this type made available and should have a key role on the future development and evaluation of point cloud coding solutions.1	codec;compression artifact;distortion;experiment;octree;point cloud	Alireza Javaheri;Catarina Brites;Fernando da Cruz Pereira;João Ascenso	2017	2017 IEEE 19th International Workshop on Multimedia Signal Processing (MMSP)	10.1109/MMSP.2017.8122239	computer science;computer vision;artificial intelligence;point cloud;codec;octree;distortion;graph;compression artifact	Visualization	63.58629010491165	-64.62579207849821	142416
9a6f365cbab00c979df36c2def5b64ef0fa8f74a	blind image quality assessment in the complex frequency domain		In this paper, we propose a no-reference (NR) image quality assessment (IQA) metric that operates in the complex frequency domain. A set of features are developed to model the natural scene statistics without depending on any specific visual distortion. The proposed approach relies on a statistical analysis of the transformed image, involving the importance of the phase and magnitude provided by the underlying complex coefficients. We further investigate the correlation between the different image spatial-frequency resolutions, i.e., representations under different scales and orientations in order to extract the directional features and energy distributions of an image. The validation of the NR metric is performed on a variety of challenging IQA databases and the obtained results show good correlation with subjective scores. Besides, the obtained performance is highly competitive compared to the top-performing NR IQA metrics.	coefficient;database;distortion;image quality;noise reduction;scene statistics	Kais Rouis;Mohamed-Chaker Larabi;Jamel Belhadj Tahar	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296385	computer vision;frequency domain;artificial intelligence;magnitude (mathematics);image quality;visualization;pattern recognition;feature extraction;computer science;distortion;scene statistics;correlation	Robotics	61.85831935232468	-64.95642086922834	142852
58816d45bb682fc1e4f36c211f7826a4a4eac7ae	snapshot spectral imaging via compressive random convolution	coded aperture spectral snapshot imaging;spectroscopy;surveillance image reconstruction remote sensing spectroscopy;detectors;tissue spectroscopy;wide area airborne surveillance;random convolution snapshot spectral imaging;compressed sensing;coherent illumination;multispectral imaging compressed spectral imaging coherent illumination compressed sensing;surveillance;convolution;psnr spectral image cube reconstructions;fpa measurement;rcssi;spatial light modulator;compressive random convolution;image reconstruction;remote sensing;multispectral images;imaging;compressed spectral imaging;spectrally dispersed coherent sources;psnr spectral image cube reconstructions compressive random convolution wide area airborne surveillance remote sensing tissue spectroscopy coded aperture spectral snapshot imaging cassi focal plane array fpa measurement aperture code random convolution snapshot spectral imaging rcssi spectrally dispersed coherent sources;spectral imaging;reconstruction algorithm;aperture code;cassi;dispersion;focal plane array;apertures;multispectral imaging;imaging image reconstruction detectors dispersion convolution apertures compressed sensing	Spectral imaging is of interest in many applications, including wide-area airborne surveillance, remote sensing, and tissue spectroscopy. Coded aperture spectral snapshot imaging (CASSI) provides an efficient mechanism to capture a 3D spectral cube with a single shot 2D measurement. CASSI uses a focal plane array (FPA) measurement of a spectrally dispersed, aperture coded, source. The spectral cube is then attained using a compressive sensing reconstruction algorithm. In this paper, we explore a new approach referred to as random convolution snapshot spectral imaging (RCSSI). It is based on FPA measurements of spectrally dispersed coherent sources that have been randomly convoluted by a spatial light modulator. The new method, based on the theory of compressive sensing via random convolutions, is shown to outperform traditional CASSI systems in terms of PSNR spectral image cube reconstructions.	airborne ranger;algorithm;coded aperture;coherence (physics);compressed sensing;convolution;modulation;peak signal-to-noise ratio;randomness;snapshot (computer storage);spatial light modulator;staring array	Yao Wu;Gonzalo R. Arce	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5946769	full spectral imaging;multispectral image;computer vision;spectroscopy	Vision	68.16456155233885	-61.60234469939557	142882
e994b672fccce134f7c873f0f404a6f8ad024639	a hardware-accelerated patch search engine for image completion	search engine;paper;image processing;search engines;texture synthesis;hardware accelerated patch search engine;gpu accelerated patch search engine;search strategy;hardware accelerator;search engines image restoration pixel diffusion processes convergence computer graphics partial differential equations cybernetics streaming media filling;image texture;patch based best fit searching strategy;image linear structure;image texture hardware accelerated patch search engine image completion gpu accelerated patch search engine patch based best fit searching strategy image linear structure;image reconstruction;search engines image reconstruction image texture;algorithms;image completion;exhaustive search	This paper proposes a GPU-accelerated patch search engine that efficiently Alls the unknown regions of an image caused by replacement or removal of part of the foreground. Previous approaches, such as inpainting and texture synthesis, are either fast, but not applicable for small-scale regions, or slow, but fills large regions with good quality. The algorithm in this paper is based on the patch-based best-fit searching strategy, in which a partly-known patch is filled by searching the known part of the image for a patch of pixels closely matching the known neighbors. This keeps the linear structure and texture of the image. Each patch is represented as a stream and processed in parallel in the GPU. We found that the exhaustive searching strategies used in previous work are the main cause of inefficiency; most matches are located in the neighborhood of the target patch. Inspired by this spatial continuity, we develop a very efficient search engine compared with previous work.	algorithm;brute-force search;curve fitting;graphics processing unit;inpainting;patch (computing);pixel;scott continuity;texture synthesis;web search engine	Yi Lin	2006	2006 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2006.384749	computer vision;image processing;computer science;theoretical computer science;search engine;computer graphics (images)	Robotics	55.96140987167282	-60.48572315429975	142904
7d3eef31086bd0ab0c51fcc1e6af84f63cc50584	a standard portrait image and image quality assessment			image quality	Kunihiko Kanafusa;Keiichi Miyazaki;Hiroshi Umemoto;Kazuhiko Takemura;Hitoshi Urabe	2000			computer vision;image quality;portrait;artificial intelligence;computer science	Vision	61.360812961661196	-62.39310551650582	143073
7a223f51443be6715843aff7aacaf7e83712ef93	a novel fractional filter design and cross-term elimination in wigner distribution		The second and the third sentences of the abstract are changed and the shorter abstract is given as follows. To recover the nonstationary signal in complicated noise environment without distortion, a novel general design of fractional filter is proposed and applied to eliminate the Wigner cross-term. A time-frequency binary image is obtained from the time-frequency distribution of the observed signal and the optimal separating lines are determined by the support vector machine (SVM) classifier where the image boundary extraction algorithms are used to construct the training set of SVM. After that, the parameters and transfer function of filter can be determined by the parameters of the separating lines directly in the case of linear separability or line segments after the piecewise linear fitting of the separating curves in the case of nonlinear separability. Without any prior knowledge of signal and noise, this method can meet the reliability and universality simultaneously for filter design and realize the global optimization of filter parameters by machine learning even in the case of strong coupling between signal and noise. Furthermore, it could completely eliminate the cross-term in Wigner-Ville distribution (WVD) and the time-frequency distribution we get in the end has high resolution and good readability even when autoterms and cross-terms overlap. Simulation results verified the efficiency of this method.	filter design;modified wigner distribution function	Jiexiao Yu;Kaihua Liu;Liang Zhang;Peng Luo	2015	IJDSN	10.1155/2015/189308	mathematical optimization;machine learning;filter design	EDA	54.18790523706698	-63.26402880181012	143108
af99cfd4d1a33a4ac60b3ecc025fc5dafb87d5f9	the fractal images evaluation model based on artificial neural network	fractals;color psychological effect fractal image evaluation model artificial neural network color design;neural nets;color design;fractal images;training;psychology;neural nets fractals image colour analysis;indexes;artificial neural networks;color psychological effect;image color analysis;image colour analysis;indexation;evaluation model fractal images artificial neural network;fractal image evaluation model;humans;evaluation model;image color analysis artificial neural networks fractals training indexes psychology humans;artificial neural network	Whether the fractal images can be applied to design, depends on the degree satisfaction of users for images. In images, colors are the main visual factors of affecting emotion; it is used as the determinant in fractal images evaluation. The evaluation of color design is nonstatistical; it is easy to bring evaluating deviation because of personal bias. Using artificial neural network, the color psychological effect is simulated, constructs the evaluation index system in affective features for fractal images and gives evaluation model in some fields, achieves a good result.	artificial neural network;color;fractal	Ling Feng;Jing Liu	2010	2010 Sixth International Conference on Natural Computation	10.1109/ICNC.2010.5583301	psychology;computer vision;fractal analysis;artificial intelligence;machine learning	Robotics	61.68538913285836	-63.36178303312414	143135
71a0dcb19a8de4d0772c3bef17193aeab3687f9c	simultaneous frame-rate up-conversion of image and optical flow sequences		We develop a variational method for the frame-rate up-conve rsion of optical flow fields, in which we combine motion coherency in an image sequence and the smoothness of t he temporal flow field. Since optical flow vectors define the motion of each point in an image, we can cons truct interframe images from low frame-rate image sequences using flow field vectors. The algorithm produ ces both interframe images and optical flow fields from a set of successive images in a sequence.	algorithm;calculus of variations;optical flow	Shun Inagaki;Hayato Itoh;Atsushi Imiya	2015		10.5220/0005296800680075	digital image correlation	Vision	56.27414324987897	-55.52274836123343	143161
f13088ee0332761d680650c2e3412bcca60ec3c6	multi-camera imaging, coding and innovative display: techniques and systems	image coding	Recently, the importance of three-dimensional (3D) imaging technology has been significantly increased. With its capability of offering richer experience, 3D imaging systems are replacing traditional systems in many different areas of entertainment, education, industry, and business. Especially, multi-camera imaging is a practical approach to construct 3D imaging systems, since multi-view images or videos can provide different views of the same scene, offering interactivity as well as 3D perception. However, in spite of the advantages, there are various challenges that should be overcome to deploy multi-camera imaging techniques in practical visual communication systems. All aspects of image processing systems should be redesigned to communicate multi-view images efficiently: acquisition, processing, representation, coding, and display. First, the acquisition step requires the calibration of multiple cameras and the post-processing to compensate for various discrepancies, such as color and illumination differences. Second, multi-view images can be processed to provide depth information, which is necessary to reconstruct 3D data. Third, it is essential to develop efficient tools to represent and compress multi-view images or videos compactly, which require a huge amount of storage space and transmission bandwidth in general. Last, innovative 3D display technologies should be developed to enable viewers to experience full 3D parallax without restrictions. We solicited state-of-the-art approaches and technical solutions in the area of multi-camera imaging, coding, and 3D display. More than 50 papers were submitted, and 18 papers have been selected among them to be included in this special issue. These selected papers address various aspects of multi-camera image processing systems. First, we have two papers on the color correction of multiview images and videos. In the paper ‘‘Iterative Colour Correction of Multicamera Systems Using Corresponding Feature Points,” Tehrani et al. propose an automatic correction scheme, which can correct image colors obtained from dense or sparse multi-camera systems to obtain the average color pattern. In the paper ‘‘Fast Color Correction for Multi-View Video by Modelling Spatio-Temporal Variation,” Shao et al. propose a fast color correction scheme that exploits spatio-temporal relationships in multi-view videos to provide fast and accurate correction performance. Next, we have four papers on stereo matching and 3D reconstruction. In the paper ‘‘Robust Stereo Matching with Improved Graph and Surface Models and Occlusion Handling,” Oh and Kuo propose an efficient stereo matching algorithm, which uses a graph model to estimate disparity values and detect occlusion regions. Their algorithm also uses a surface model to improve the accuracy	3d computer graphics;3d reconstruction;algorithm;binocular disparity;color;computer stereo vision;display device;image processing;imaging technology;interactivity;parallax;sparse matrix;stereo display;video post-processing	Minh N. Do;Chang-Su Kim;Karsten Müller;Masayuki Tanimoto;Anthony Vetro	2010	J. Visual Communication and Image Representation	10.1016/j.jvcir.2010.05.001	computer vision;computer science;multimedia	Graphics	57.77461493973836	-54.39738052209979	143362
6820e617876cefb8d145da102d2d94c2be8d7fbc	maximum entropy light source placement	ai and behavior;animation techniques	Several methods have been proposed to help the user to position lighting sources for a given view of a 3d-scene. In the first kind of methods the user defines a desired illumination through highlight and shadow locations for which the lighting system optimizes the light positions. The more automatic approaches are based on perceptual image metrics. [Marks et al. 1997] define an image metric that measures how different two images are perceived. A collection of maximally different images is presented to the user for selection. [Shacked and Lischinski 2001] define a perceptual based image quality metric composed of six contributing terms, for which the user has to specify weights before the system searches for a locally optimal light source placement.	illumination (image);image quality;local optimum;maximum entropy spectral estimation	Stefan Gumhold	2002	IEEE Visualization, 2002. VIS 2002.	10.1145/1242073.1242228	computer vision;simulation;computer science	Visualization	60.57729497495813	-61.841307825184415	143421
256c181414b29efd4e1c3e935af43294c78dc912	enhancing face recognition at a distance using super resolution	age factors;high resolution;image resolution;biometrics;low resolution;qa75 electronic computers computer science;face recognition;feature extraction;signature;super resolution;iris;high frequency;qa76 computer software	The characteristics of surveillance video generally include low-resolution images and blurred images. Decreases in image resolution lead to loss of high frequency facial components, which is expected to adversely affect recognition rates. Super resolution (SR) is a technique used to generate a higher resolution image from a given low-resolution, degraded image. Dictionary based super resolution pre-processing techniques have been developed to overcome the problem of low-resolution images in face recognition. However, super resolution reconstruction process, being ill-posed, and results in visual artifacts that can be visually distracting to humans and/or affect machine feature extraction and face recognition algorithms. In this paper, we investigate the impact of two existing super-resolution methods to reconstruct a high resolution from single/ multiple low-resolution images on face recognition. We propose an alternative scheme that is based on dictionaries in high frequency wavelet subbands. The performance of the proposed method will be evaluated on databases of high and low-resolution images captured under different illumination conditions and at different distances. We shall demonstrate that the proposed approach at level 3 DWT decomposition has superior performance in comparison to the other super resolution methods.	algorithm;closed-circuit television;database;dictionary;discrete wavelet transform;facial recognition system;feature extraction;image resolution;preprocessor;super-resolution imaging;visual artifact;well-posed problem	Nadia Al-Hassan;Sabah Jassim;Harin Sellahewa	2012		10.1145/2361407.2361429	computer vision;speech recognition;computer science;three-dimensional face recognition;sub-pixel resolution;computer graphics (images)	Vision	60.702480087588974	-64.95383467334418	143451
0a7a4ed076cb3f49d366b6cb870adb9266a8a9d6	machine learning and directional switching median-based filter for highly corrupted images		In this paper, two-stage machine learning-based noise detection scheme has been proposed for identification of salt-and- pepper impulse noise which gives excellent detection results for highly corrupted images. In the first stage, a window of size $$3\times 3$$ is taken from image and some other features of this window are used as input to neural network. This scheme has distinction of having very low missed detection (MD) and false positives rates. In the second stage, decision tree-based algorithm (J48) is applied on some well-known statistical parameters to generate rules for noise detection. These noise detection methods give promising results for identification of noise from highly corrupted images. A modified version of switching median filter (directional weighted switching median filter) is proposed for noise removal. Performance of noise detector is measured using MD and false alarm FA. Filtering results are compared with state-of-the-art noise removal techniques in terms of peak signal-to-noise ratio and structural similarity index measure. Extensive experiments are performed to show that the proposed technique gives better results than state-of-the-art noise detection and filtering methods.	algorithm;artificial neural network;decision tree;experiment;filter (signal processing);impulse noise (audio);machine learning;median filter;molecular dynamics;peak signal-to-noise ratio;pixel;structural similarity	Sohail Masood Bhatti;Ayyaz Hussain;M. Arfan Jaffar;Anwar M. Mirza;Tae-Sun Choi	2012	Knowledge and Information Systems	10.1007/s10115-012-0549-y	gradient noise;gaussian noise;median filter;image noise;speech recognition;value noise;computer science;noise measurement;machine learning;pattern recognition;salt-and-pepper noise	ML	56.702412992388474	-65.93610315453569	143646
055855f99e350aa0eec7a0314adee5dff7cf78b9	robust image registration with illumination, blur and noise variations for super-resolution	image degradation;local phase;illumination variation;blur kernel;blur variation;blur parameter;super resolution reconstruction algorithm;noise robustness image registration lighting image resolution degradation reconstruction algorithms parameter estimation image reconstruction error analysis band pass filters;indexing terms;image registration image reconstruction;band pass;image transformation;image reconstruction;blur kernel robust image registration illumination variation blur variation noise variation super resolution reconstruction algorithm blur parameter parameter estimation image degradation image transformation;image registration;registration;robust image registration;error rate;noise variation;super resolution;parameter estimation;reconstruction algorithm;local phase registration super resolution	Super-resolution reconstruction algorithms assume the availability of exact registration and blur parameters. Inaccurate estimation of these parameters adversely affects the quality of the reconstructed image. However, traditional approaches for image registration are either sensitive to image degradations such as variations in blur, illumination and noise, or are limited in the class of image transformations that can be estimated. We propose an accurate registration algorithm that uses the local phase information, which is robust to the above degradations. We derive the theoretical error rate of the estimates in presence of non-ideal band-pass behavior of the filter and show that the error converges to zero over iterations. We also show the invariance of local phase to a class of blur kernels. Experimental results on images taken under varying conditions clearly demonstrates the robustness of our approach.	algorithm;gaussian blur;image registration;iteration;super-resolution imaging	Himanshu Arora;Anoop M. Namboodiri;C. V. Jawahar	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4517856	iterative reconstruction;image restoration;computer vision;index term;word error rate;computer science;image registration;gaussian blur;band-pass filter;estimation theory;superresolution;computer graphics (images)	Vision	55.65623215474942	-58.14689895864928	143648
05222916aa074c0394e2078f46756d0a00a16c57	joint trilateral filtering for depth map super-resolution	local gradient information rgb d sensors color image visual analysis tasks joint trilateral filtering algorithm depth map super resolution problems bilateral filtering high resolution image;image resolution;image resolution filtering theory image colour analysis;image colour analysis;image edge detection image color analysis joints color spatial resolution interpolation;filtering theory	Depth map super-resolution is an emerging topic due to the increasing needs and applications using RGB-D sensors. Together with the color image, the corresponding range data provides additional information and makes visual analysis tasks more tractable. However, since the depth maps captured by such sensors are typically with limited resolution, it is preferable to enhance its resolution for improved recognition. In this paper, we present a novel joint trilateral filtering (JTF) algorithm for solving depth map super-resolution (SR) problems. Inspired by bilateral filtering, our JTF utilizes and preserves edge information from the associated high-resolution (HR) image by taking spatial and range information of local pixels. Our proposed further integrates local gradient information of the depth map when synthesizing its HR output, which alleviates textural artifacts like edge discontinuities. Quantitative and qualitative experimental results demonstrate the effectiveness and robustness of our approach over prior depth map upsampling works.	algorithm;bilateral filter;cobham's thesis;color image;depth map;gradient descent;high-resolution scheme;image resolution;pixel;sensor;super-resolution imaging;upsampling	Kai-Han Lo;Yu-Chiang Frank Wang;Kai-Lung Hua	2013	2013 Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2013.6706444	computer vision;image resolution;color image;image gradient;computer science;computer graphics (images)	Vision	57.06381718031275	-58.27622510481764	144053
22292311172ce2f0f92a9b4a88a2abbd84340327	distortion minimization and continuity preservation in surface pasting	continuity;surface pasting;least squares;distortion reduction;b-spline;tensor product;least square;hierarchical model	Surface pasting is a hierarchical modeling technique capable of adding local details to tensor product B- spline surfaces without incurring significant computa- tional costs. In this paper, we describe how the continuity conditions of this technique can be improved through the use of least squares fitting and the application of some general B-spline continuity properties. More importantly, we address distortion issues inherent to the standard past- ing technique by using an alternative mapping of the in- terior control points.	distortion;scott continuity	Rick Leung;Stephen Mann	2003			distortion;minification;hierarchical database model;least squares;mathematical optimization;tensor product;mathematics	HCI	54.823744497914994	-55.97223124625243	144200
1fd06fb375fcfa7d6cd67c0221f0fd9f99f7bb3c	perceptual correction for colour grading using sensor transformations and metameric data	human vision;human vision system;human perception	We present a method of colour shade grading for industrial inspection of surfaces, the differences of which are at the threshold of human perception. This method converts the input data from the electronic sensor to the corresponding data as they would have been viewed using the human vision system. Then their differences are computed using a perceptually uniform colour space, thus approximating the way the human experts would grade the product. The transformation from the electronic sensor to the human sensor makes use of synthetic metameric data to determine the transformation parameters. The method has been tested using real data.	approximation;classical xy model;color space;distortion;glossy display;machine vision;residential gateway;sensor;synthetic data;transformation matrix;x–y plotter	Constantinos Boukouvalas;Maria Petrou	1998	Machine Vision and Applications	10.1007/s001380050094	computer vision;perception	Robotics	61.87544827323791	-59.81223438997721	144221
c9c4acd0e9c0ba3cee17ced4200d6a7b4d83dde2	content-adaptive focus configuration for near-eye multi-focal displays	measurement;depth blending;light field;three dimensional displays focal planes optimisation stereo image processing;content visual quality content adaptive focus configuration near eye multifocal displays stereoscopic 3d displays near eye light field displays uniformly spaced focal plane configuration dioptric space focal plane configuration optimization;multi focal;visualization;optical imaging;three dimensional displays measurement optical imaging visualization optical sensors retina adaptive optics;three dimensional displays;retina;content adaptation;optical sensors;content adaptation light field depth blending multi focal near eye display;near eye display;adaptive optics	Near-eye multi-focal (light field) displays are known to be more advantageous than conventional stereoscopic 3D displays, because they can alleviate the vergence-accommodation conflict. While stereoscopic 3D displays are restricted to projecting images on a single focal plane, near-eye light field displays can form multiple focal planes and volumetrically render 3D data. Most existing near-eye light field displays use simplistic, uniformly spaced focal plane configuration (in dioptric space). In this paper, we present a novel technique that optimizes the focal plane configuration based on characteristics of the content to be rendered. We show that this technique can significantly improve the perceived visual quality of content visualized on such displays.	focal (programming language);light field;stereo display;stereoscopy;vergence	Wanmin Wu;Patrick Llull;Ivana Tosic;Noah Bedard;Kathrin Berkner;Nikhil Balram	2016	2016 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2016.7552965	computer vision;visualization;computer science;light field;optical imaging;adaptive optics;measurement;computer graphics (images)	Visualization	61.46123944159045	-55.41132388064652	144315
be2c666d8f746a8fefe10a556f62f87f9c85cdfe	blur in human vision and increased visual realism in virtual environments	theoretical model;human vision;virtual reality;computer graphic;probabilistic model;human visual system;model matching;virtual environment	A challenge for virtual reality (VR) applications is to increase the realism of an observer’s visual experience. For this purpose the variation of the blur an observer experiences in his/her vision, while he/she focuses on a particular location, can be mimicked by blurring the VR computer graphics based on a model of the blur. The blur in human vision is the result of a combination of optical and neural vision processes; namely optical refraction, non-uniform retinal sampling, and cortical magnification. Due to the complexity of the phenomenon, apparently no theoretical model of the blur has been published. In this work we model the combined effect by means of a probabilistic model of the human visual system. The results from the models match common vision experience verifying the validity of the underlying theoretical considerations. The implementation of the model for increased realism in virtual reality is illustrated by means of a rendering of a virtual reality scene, which is processed for two different acts of focusing.	computer graphics;gaussian blur;sampling (signal processing);statistical model;theory;verification and validation;virtual reality	Michael S. Bittermann;I. Sevil Sariyildiz;Özer Ciftcioglu	2007		10.1007/978-3-540-76858-6_14	statistical model;computer vision;simulation;computer science;virtual machine;virtual reality;human visual system model;statistics;computer graphics (images)	Visualization	62.62946680135329	-60.03279964703044	144372
8a3e2a5047865ee31bcdcb0f8e6200505a6597f5	sparse image and signal processing - wavelets, curvelets, morphological diversity		If you get the printed book in on-line book store, you may also find the same problem. So, you must move store to store and search for the available there. But, it will not happen here. The book that we will offer right here is the soft file concept. This is what make you can easily find and get this sparse image and signal processing wavelets curvelets morphological diversity by reading this site. We offer you the best product, always and always.	online and offline;online book;printing;signal processing;sparse matrix;wavelet	Jean-Luc Starck;Fionn Murtagh;Mohamed-Jalal Fadili	2010		10.1117/1.3518456		Web+IR	67.73835969664785	-62.53866528778429	144571
551782cefc054583a6e80b285aa7d0eab06b8e6e	perception-based histogram equalization for tone mapping applications	histograms;brightness;visualization;histograms dynamic range visualization brightness real time systems encoding image edge detection;image edge detection;perception based histogram equalization compression artifacts image brightness image contrast luminance range code words pq function perceptual quantizer function ceiling function dim images noise free images noise amplification artifact banding tmos tone mapping operators standard dynamic range displays hdr displays hdr content high dynamic range content tone mapping applications;dynamic range;image denoising brightness image coding;perceptual encoding hdr tone mapping histogram equalization;encoding;real time systems	Due to the ever increasing commercial availability of High Dynamic Range (HDR) content and displays, backward compatibility of HDR content with Standard Dynamic Range displays is currently a topic of high importance. Over the years, a significant amount of Tone Mapping Operators (TMOs) have been proposed to adapt HDR content to the restricted capabilities of SDR displays. Among them, the Histogram Equalization (HE) is considered to provide good results for a wide set of images. However, the naïve application of HE results either in banding artifacts or noise amplification when the HDR image has large unified areas (i.e. sky). In order to differentiate relevant information from noise in a uniform background, or in dark areas, the authors proposed a ceiling function. Their method results in noise-free but dim images. In this paper we propose a novel ceiling function which is based on the Perceptual Quantizer (PQ) function. Our method uses as threshold the number of code-words that PQ assigns on a luminance range in the original HDR image and the corresponding number of code-words in the resulting SDR image. We limit the number of code-words on SDR to be equal or less than the HDR. The saved code-words during the ceiling operation are redistributed to increase the contrast as well as the brightness of the final image. Results shows that provided SDR images are noise-free and brighter than the one obtained with prior HE operators. Finally since the proposed method is a Global TMO, it is thereby of low complexity and suitable for real time applications.	algorithm;backward compatibility;colour banding;compression artifact;data compression;etsi satellite digital radio;flicker (screen);floor and ceiling functions;high dynamic range;histogram equalization;quantization (signal processing);standard dynamic range;tone mapping	Stelios E. Ploumis;Ronan Boitard;Mahsa T. Pourazad;Panos Nasiopoulos	2016	2016 Digital Media Industry & Academic Forum (DMIAF)	10.1109/DMIAF.2016.7574892	computer vision;electronic engineering;computer science;adaptive histogram equalization;computer graphics (images)	Graphics	61.40885995213966	-61.67680094962219	144703
fee4adc4aecd704e04bd21f3509255ee650e8400	adaptive image contrast enhancement algorithm for point-based rendering	image contrast enhancement;distortion;image quality;algorithms	Surgical simulation is a major application in computer graphics and virtual reality, and most of the existing work indicates that interactive real-time cutting simulation of soft tissue is a fundamental but challenging research problem in virtual surgery simulation systems. More specifically, it is difficult to achieve a fast enough graphic update rate (at least 30 Hz) on commodity PC hardware by utilizing traditional triangle-based rendering algorithms. In recent years, point-based rendering (PBR) has been shown to offer the potential to outperform the traditional triangle-based rendering in speed when it is applied to highly complex soft tissue cutting models. Nevertheless, the PBR algorithms are still limited in visual quality due to inherent contrast distortion. We propose an adaptive image contrast enhancement algorithm as a postprocessing module for PBR, providing high visual rendering quality as well as acceptable rendering efficiency. Our approach is based on a perceptible image quality technique with automatic parameter selection, resulting in a visual quality comparable to existing conventional PBR algorithms. Experimental results show that our adaptive image contrast enhancement algorithm produces encouraging results both visually and numerically compared to representative algorithms, and experiments conducted on the latest hardware demonstrate that the proposed PBR framework with the postprocessing module is superior to the conventional PBR algorithm and that the proposed contrast enhancement algorithm can be utilized in (or compatible with) various variants of the conventional PBR algorithm. © 2015 SPIE and IS&T [DOI: 10.1117/1.JEI.24.2.023033]	approximation algorithm;computation;computational model;computer graphics;distortion;experiment;goto;graphics pipeline;human visual system model;image editing;image quality;noise reduction;numerical analysis;pbr theorem;real-time clock;real-time computing;real-time locating system;real-time transcription;simulation;surgery simulator;virtual reality	Shaoping Xu;Peter Xiaoping Liu	2015	J. Electronic Imaging	10.1117/1.JEI.24.2.023033	image quality;computer vision;simulation;distortion;computer science;computer graphics (images)	Graphics	59.29243131766884	-60.65847329823873	144737
033e93c0c3a4bd0260a5a1d8e6cdc7c2840ee153	improving color reproduction accuracy on cameras		One of the key operations performed on a digital camera is to map the sensor-specific color space to a standard perceptual color space. This procedure involves the application of a white-balance correction followed by a color space transform. The current approach for this colorimetric mapping is based on an interpolation of pre-calibrated color space transforms computed for two fixed illuminations (i.e., two white-balance settings). Images captured under different illuminations are subject to less color accuracy due to the use of this interpolation process. In this paper, we discuss the limitations of the current colorimetric mapping approach and propose two methods that are able to improve color accuracy. We evaluate our approach on seven different cameras and show improvements of up to 30% (DSLR cameras) and 59% (mobile phone cameras) in terms of color reproduction error.		Hakki Can Karaimer;Michael S. Brown	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00674	interpolation;computer vision;artificial intelligence;digital camera;colorimetry;computer science;color space	Vision	60.087314183347836	-61.00050129772098	144991
ee29f010f54ae9aef5735ed9ff675a8ef504a966	super-multiview content with high angular resolution: 3d quality assessment on horizontal-parallax lightfield display	full reference 3d quality assessment;pooling of multiple metrics;electricity;electronic imaging singal processing;subjective evaluation;lightfield display	The advent of glasses-free 3D super-multiview (SMV) displays has opened up new avenues for experiencing 3D content. Accordingly, perceptual quality assessment of such content assumes significance. In this context, we present the results of subjective and objective quality assessment of static 3D SMV content, conducted with twenty subjects on a glasses-free horizontal-parallax 3D lightfield display. First, we create a multiview image dataset using three real objects, with high angular resolution. The display system generates 3D views from fixed subsets of consecutive images. Next, we extend the standard guidelines of subjective assessment to our evaluation environment. In the course, we make recommendations on acquiring SMV data as well as conducting subjective evaluation. Subsequently, we observe that the existing 2D full-reference (FR) quality metrics, applied to individual 3D views, are inadequate for 3D quality assessment. To fill the gap, we propose a 3D FR objective quality metric. For every 3D view, the proposed metric combines spatial information from each constituent image and angular information (depth cues) from consecutive images. Finally, we show that the proposed metric correlates significantly with subjective scores, outperforming existing 2D metrics. The efficacy of pooling spatial and angular information highlights the fact that angular information plays a crucial role in 3D perception. Graphical abstractDisplay Omitted HighlightsA high-angular-resolution multiview dataset is created to produce 3D content.The 3D content is evaluated by 20 subjects on a glasses-free 3D lightfield display.Standard subjective evaluation guidelines are extended to 3D quality assessment.The proposed 3D objective quality metric leverages the spatio-angular nature of 3D.Proposed objective scores and the subjective scores correlate significantly.	angularjs;parallax	Roopak Tamboli;Balasubramanyam Appina;Sumohana S. Channappayya;Soumya Jana	2016	Sig. Proc.: Image Comm.	10.1016/j.image.2016.05.010	computer vision;simulation;data mining;electricity	HCI	64.07499180309773	-63.39292894237386	145050
be7041dad0fa1d31ccf347a716c0ad13ec55ba2b	robust generation of high dynamic range images	moving object;prediction method;image processing bayes methods;signal generators;adaptive thresholding;high dynamic range images;image processing;ghosting artifacts;predictive value;bayes methods;layout;ldr;high dynamic range imaging;bayes estimator robust generation high dynamic range images high dynamic range hdr low dynamic range ldr;robustness dynamic range layout bidirectional control pixel signal generators prediction methods image processing cameras entropy;bayes estimator;robust generation;prediction methods;hdr;similarity high dynamic range collaborative image processing ghosting artifacts;pixel;bidirectional control;similarity;dynamic range;robustness;entropy;low dynamic range;high dynamic range;cameras;collaborative image processing;noise;dynamic scenes	A robust scheme is proposed to generate an anti-ghosting high dynamic range (HDR) image from a set of low dynamic range (LDR) images with different exposure times. Three major contributions of this paper are 1) a bi-directional prediction method; 2) an adaptive threshold for the classification of pixels; 3) Bayes estimator based methods for the on-line updating of predicted values and the synthesis of pixels to fill in the regions of moving objects to preserve their dynamic ranges. The proposed scheme is suitable for both static and dynamic scenes.	bi-directional text;high dynamic range;high-dynamic-range rendering;ldraw;online and offline;pixel;rollover (key)	Zhengguo Li;Zijian Zhu;Shoulie Xie;Shiqian Wu;Susanto Rahardja	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495327	layout;computer vision;entropy;dynamic range;similarity;bayes estimator;image processing;computer science;noise;machine learning;thresholding;pixel;robustness;signal generator	Robotics	58.36344015579772	-58.833229858524625	145174
90074953aae4882b0a10fa9bec55e9f005486e73	the effect of using various gamut-mapping methods for color reproduction on the cotton fabric estimated by the category judgment method	cotton;color images reproduction;cotton fabrics;production engineering computing cotton fabrics image colour analysis image matching;color;image matching;gamut mapping;category judgment method;textile substrate gamut mapping methods cotton fabric category judgment method textile printing color images reproduction;observers;category judgment method electronic pre press technology gamut mapping color reproduction;production engineering computing;cotton fabrics textile technology color printing paper technology table lookup computer science software engineering cities and towns;image color analysis;image colour analysis;color reproduction;cotton fabric;linear model;fabrics;textile printing;textile substrate;substrates;textiles;electronic pre press technology;gamut mapping methods;color image	"""It was essential for color reproduction to achieve the goal of """"WYSIWYG"""" since the electronic pre-prepress technologies were adopted three decades ago. Gamut mapping technologies were often used as one of the primary control parameters for color reproduction. However, those technologies were widely used in the paper based printing, not in textile printing. In this study, three gamut-mapping methods: cut, linear, and non-linear were applied to color reproduction on the cotton fabric. Experimental data was collected and analyzed using a simple characterization of devices. Meanwhile, the effective degrees of color reproduction of the standard color images printed on the cotton fabric substrate were examined with Category Judgment Method. The results of the study indicated that the linear model of gamut mapping has the best performance in reproduction of color images on the textile substrate twill cotton fabric while the non-linear or clipping one the second."""	color management;linear model;nonlinear system;printing;wysiwyg	W. G. Kuo;Y. C. Wei;C. C. Chen	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.1320	computer vision;cotton;law;statistics;computer graphics (images)	Robotics	60.043951180890126	-62.78570242736805	145208
0db1f93d22fa98e670e6071b3bac964b2497b54e	a roughness measure for 3d mesh visual masking	roughness;human visual system;masking;curvature;visual masking;3d mesh	3D models are subject to a wide variety of processing operations such as compression, simplification or watermarking, which introduce slight geometric modifications on the shape. The main issue is to maximize the compression/simplification ratio or the watermark strength while minimizing these visual degradations. However few algorithms exploit the human visual system to hide these degradations, while perceptual attributes could be quite relevant for this task. Particularly, the Masking Effect defines the fact that a signal can be masked by the presence of another signal with similar frequency or orientation. In this context we introduce the notion of roughness for a 3D mesh, as a local measure of geometric noise on the surface. Indeed, a textured (or rough) region is able to hide geometric distortions much better than a smooth one. Our measure is based on curvature analysis on local windows of the mesh and is independent of the resolution/connectivity of the object. An application to Visual Masking is presented and discussed.	3d modeling;algorithm;data compression;digital watermarking;distortion;level of detail;mcgurk effect;microsoft windows;text simplification	Guillaume Lavoué	2007		10.1145/1272582.1272593	polygon mesh;computer vision;computer science;masking;curvature;optics;human visual system model	Vision	61.73506896076296	-60.05325477346179	145605
afe8dc77d3bc69bb1ad2b9f0c857a0d3c8ce9e9d	depth no-synthesis-error model for view synthesis in 3-d video	depth coding depth no synthesis error model 3d video view synthesis disparity adjustable stereoscopic video depth image based rendering depth information distortion dibr based view synthesis virtual view d nose profile;forward warping;filling;three dimensional;pixel cameras filling numerical models merging three dimensional displays encoding;video coding;view synthesis;numerical model;depth image based rendering dibr;three dimensional displays;pixel;stereo image processing;merging;video coding rendering computer graphics signal synthesis stereo image processing;3 d video 3dv;signal synthesis;depth image based rendering;numerical models;view synthesis 3 d video 3dv depth no synthesis error d nose model depth image based rendering dibr forward warping;rendering computer graphics;encoding;stereoscopic display;cameras;depth no synthesis error d nose model	Currently, 3-D Video targets at the application of disparity-adjustable stereoscopic video, where view synthesis based on depth-image-based rendering (DIBR) is employed to generate virtual views. Distortions in depth information may introduce geometry changes or occlusion variations in the synthesized views. In practice, depth information is stored in 8-bit grayscale format, whereas the disparity range for a visually comfortable stereo pair is usually much less than 256 levels. Thus, several depth levels may correspond to the same integer (or sub-pixel) disparity value in the DIBR-based view synthesis such that some depth distortions may not result in geometry changes in the synthesized view. From this observation, we develop a depth no-synthesis-error (D-NOSE) model to examine the allowable depth distortions in rendering a virtual view without introducing any geometry changes. We further show that the depth distortions prescribed by the proposed D-NOSE profile also do not compromise the occlusion order in view synthesis. Therefore, a virtual view can be synthesized losslessly if depth distortions follow the D-NOSE specified thresholds. Our simulations validate the proposed D-NOSE model in lossless view synthesis and demonstrate the gain with the model in depth coding.	1:1 pixel mapping;3d film;8-bit;binocular disparity;data compression;distortion;grayscale color map;integer (number);lossless compression;numerical analysis;pixel;simulation;smoothing (statistical technique);stereoscopy;view synthesis;zcam;benefit	Yin Zhao;Ce Zhu;Zhenzhong Chen;Lu Yu	2011	IEEE Transactions on Image Processing	10.1109/TIP.2011.2118218	three-dimensional space;computer vision;computer science;mathematics;multimedia;pixel;encoding;computer graphics (images)	Graphics	59.32539423149747	-55.579182425437786	145620
c37304258dc82e3456d15af66e83797032bc8d79	statistical characteristics of granular camera noise	television;luminance noise;equalizers;visual artifacts;preamplifiers;bruit granulaire;circuit noise;analisis estadistico;colored noise;chrominance noise;video signal processing;optical noise;luminance;noise reduction techniques statistical characteristics granular camera noise high quality tv visual artifacts digitally coded video sequences chrominance noise luminance noise spatially colored nature;video sequences;television cameras;reduccion ruido;spatially colored nature;digital cameras;interference suppression;senal video;statistical analysis;signal video;noise reduction techniques;chrominance;noise reduction;statistical characteristics;image sequence;analyse statistique;television interference;reduction bruit;video signal;digitally coded video sequences;image sequences statistical analysis interference suppression white noise video signal processing television cameras television interference;secuencia imagen;granular camera noise;tv;second order statistics;white noise;high quality tv;colored noise tv preamplifiers optical noise digital cameras noise reduction circuit optimization circuit noise equalizers video sequences;sequence image;circuit optimization;image sequences;luminancia	Granular camera noise is per se objectionable in high-quality TV and it is a source of bothersome visual artifacts with digitally coded video sequences. The first and second order statistical characteristics of this noise are investigated in this work. The results quantify intuitively expected notions, such as the relative unimportance of chrominance noise with respect to the luminance noise and the spatially colored nature of the noise. Noise reduction techniques can use, as general guidance rules, the indications of the presented analysis. >		Guido M. Cortelazzo;Gian Antonio Mian;Roberto Parolari	1994	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.340195	gradient noise;gaussian noise;image noise;computer vision;colors of noise;dark-frame subtraction;value noise;telecommunications;computer science;noise measurement;noise;mathematics;television;noise floor;noise;statistics;salt-and-pepper noise	EDA	59.25416835101784	-65.17139530166786	145761
8eb4e470ecf2f278feb6134d4f78e4b1e80f56f7	psychophysical measurement for perceptual image brightness enhancement based on image classification	tone mapping;etude experimentale;hombre;image classification;4266s;qualite image;percepcion visual;image quality;human;perception visuelle;visual perception;calidad imagen;vision;estudio experimental;homme	The purpose of this study is to examine the difference in perceptual brightness enhancement per image category through perceptual brightness measurement. Perceptual brightness is measured via psychophysical experiment and brightness enhancement is performed by TMF (Tone Mapping Function). The classification process is comprised of two steps. It is possible to classify histograms into six groups. The three different TMFs for each category selected using the criteria and TMF application strengths. A psychophysical experiment to measure perceptual brightness enhancement was carried out. The experiment was to determine the equal perceptual brightness point between an original image and the corresponding set of TMF images. The results showed that the mean luminance for each category is significantly different. The results from brightness matching indicate that perceptual brightness enhancement is dependent on image category. We can propose that image category should be considered for advanced brightness enhancement methods.	computer vision	Inji Kim;Wonhee Choe;Seong-Deok Lee	2006		10.1117/12.642017	image quality;vision;computer vision;contextual image classification;tone mapping;visual perception;artificial intelligence;optics;brightness	Vision	62.217161834008	-62.603105194814326	146267
c0b6e8869017d2bdb0bda3c883fad49863061b21	work stealing for time-constrained octree exploration: application to real-time 3d modeling	work stealing;video streaming;real time;3d model;categories and subject descriptors according to acm ccs c 1 2 processor architectures multiple data stream architectures multiprocessors i 4 5 image processing and computer vision reconstruction	This paper introduces a dynamic work balancing algorithm, based on work stealing, for time-constrained parallel octree carving. The performance of the algorithm is proved and confirmed by experimental results where the algorithm is applied to a real-time 3D modeling from multiple video streams. Compared to classical work stealing, the proposed algorithm enforces a relaxed width first octree carving that enables to stop computations at anytime while ensuring a balanced carving.	3d modeling;ambiguous name resolution;anytime algorithm;central processing unit;computation;computer graphics;experiment;graphics processing unit;octree;real-time clock;real-time transcription;streaming media;tree structure;work stealing	Luciano P. Soares;Clément Ménier;Bruno Raffin;Jean-Louis Roch	2007		10.2312/EGPGV/EGPGV07/061-068	parallel computing;real-time computing;computer science;theoretical computer science;operating system;programming language;algorithm;computer graphics (images)	Robotics	66.76469633201084	-52.15746720570169	146355
8a17b8b53484b82f27b6e0b92f0dc0ba7e869f94	visual information evaluation with entropy of primitive		In this paper, we overview the recent work on entropy of primitive (EoP), including its concept, design, extension, and mathematical analysis in evaluating the visual information of natural images. The design philosophy of EoP is establishing an entropy model that quantifies the visual information based on patch-level sparse representation, due to the close relationship between sparse representation and the hierarchical cognitive process of human perception. Furthermore, based on the concept and definition of EoP, we also demonstrate several applications, including just noticeable difference estimation and visual quality assessment. The future research directions of visual information evaluation are also envisioned, where we can perceive both promises and challenges.	cognition;sparse approximation;sparse matrix	Songchao Tan;Shurun Wang;Xiang Zhang;Shanshe Wang;Shiqi Wang;Siwei Ma;Wen Gao	2018	IEEE Access	10.1109/ACCESS.2018.2825368	iterative reconstruction;visualization;computer science;distributed computing;perception;machine learning;sparse approximation;cognition;just-noticeable difference;artificial intelligence	Vision	62.94369277018476	-63.44769412779516	146386
28a542ccd6093f5ec52428b30b4d1c09a6e4aa60	a new post-filter for removing the edge ringing artifacts in deconvolution images	image resolution deconvolution filtering theory image enhancement;enhancement;image coding;image resolution;optical filters;frequency domain analysis;artifact ringing enhancement deconvolution image;image;image restoration;image edge post filtering method edge ringing artifact image deconvolution;image enhancement;image edge detection;artifact;image quality;deconvolution;image edge detection deconvolution image restoration image coding frequency domain analysis image quality optical filters;filtering theory;ringing	The edge ringing artifacts often appear nearby the image edge for the deconvolution bias. A new post filtering is proposed to remove the edge ringing artifact in the deconvolution image. The experimental results validate the proposed edge ringing artifact filtering algorithms perform well.	algorithm;deconvolution;ringing artifacts	Bo-Xin Zuo;Xiang-yun Hu	2011	2011 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery	10.1109/CyberC.2011.71	image quality;image restoration;computer vision;image resolution;computer science;deconvolution;image;optical filter;edge enhancement;ringing;frequency domain	Robotics	58.7014469633035	-60.88576003122133	146389
b6527c20e2293eba0153421ffab1b91d6623005f	geometry-consistent light field super-resolution via graph-based regularization		Light field cameras capture the 3D information in a scene with a single exposure. This special feature makes light field cameras very appealing for a variety of applications: from post-capture refocus to depth estimation and image-based rendering. However, light field cameras suffer by design from strong limitations in their spatial resolution. Off-the-shelf super-resolution algorithms are not ideal for light field data, as they do not consider its structure. On the other hand, the few super-resolution algorithms explicitly tailored for light field data exhibit significant limitations, such as the need to carry out a costly disparity estimation procedure with sub-pixel precision. We propose a new light field super-resolution algorithm meant to address these limitations. We use the complementary information in the different light field views to augment the spatial resolution of the whole light field at once. In particular, we show that coupling the multi-view approach with a graph-based regularizer, which enforces the light field geometric structure, permits to avoid the need of a precise and costly disparity estimation step. Extensive experiments show that the new algorithm compares favorably to the state-of-the-art methods for light field super-resolution, both in terms of visual quality and in terms of reconstruction error.	algorithm;approximation algorithm;binocular disparity;convex optimization;experiment;free viewpoint television;graph - visual representation;large;license;light field;mathematical optimization;numerous;peak signal-to-noise ratio;pixel;quadratic equation;super-resolution imaging;uria aalge;wdfy2 wt allele	Mattia Rossi;Pascal Frossard	2018	IEEE Transactions on Image Processing	10.1109/TIP.2018.2828983	iterative reconstruction;rendering (computer graphics);computer vision;artificial intelligence;light field;regularization (mathematics);superresolution;mathematics;image resolution;linear programming;graph	Vision	57.323418142574646	-56.539184769132696	146598
3437eed45c7029f7f637580ea6a28e218bddd154	nonisometric surface registration via conformal laplace-beltrami basis pursuit		Surface registration is one of the most fundamental problems in geometry processing. Many approaches have been developed to tackle this problem in cases where the surfaces are nearly isometric. However, it is much more challenging to compute correspondence between surfaces which are intrinsically less similar. In this paper, we propose a variational model to align the Laplace-Beltrami (LB) eigensytems of two non-isometric genus zero shapes via conformal deformations. This method enables us compute to geometric meaningful point-to-point maps between non-isometric shapes. Our model is based on a novel basis pursuit scheme whereby we simultaneously compute a conformal deformation of a ’target shape’ and its deformed LB eigensytem. We solve the model using an proximal alternating minimization algorithm hybridized with the augmented Lagrangian method which produces accurate correspondences given only a few landmark points. We also propose a reinitialization scheme to overcome some of the difficulties caused by the non-convexity of the variational problem. Intensive numerical experiments illustrate the effectiveness and robustness of the proposed method to handle non-isometric surfaces with large deformation with respect to both noise on the underlying manifolds and errors within the given landmarks.	algorithm;align (company);alternating turing machine;augmented lagrangian method;basis pursuit;calculus of variations;conformal geometric algebra;discrete laplace operator;experiment;geometry processing;ibm notes;isometric projection;iterative closest point;lattice boltzmann methods;local convergence;map;numerical analysis;point-to-point protocol;variational principle	Stefan C. Schonsheck;Michael M. Bronstein;Rongjie Lai	2018	CoRR		manifold;mathematical analysis;mathematical optimization;deformation (mechanics);robustness (computer science);mathematics;conformal map;augmented lagrangian method;basis pursuit;laplace transform;geometry processing	Vision	55.3104839178703	-52.232229412883235	146653
333a019d81fcf853acfbf2a5c96da83146cb68ce	an accurate noise detector for image restoration	nonlinear filters;detectors;filtering;neodymium;nonlinear filters noise detector image restoration impulsive noise flag image median filters noise reduction filtering noisy images;median filter;flag image;impulse noise;filters;noisy images;image restoration;detectors image restoration filters noise reduction pixel filtering image edge detection neodymium nonlinear distortion digital images;nonlinear distortion;image edge detection;noise reduction;pixel;impulsive noise;image denoising;noise detector;noise reduction filtering;median filters image restoration impulse noise image denoising filtering theory;digital images;filtering theory;median filters	This paper proposes a new noise-detection method for restoration of images corrupted by impulsive noise. The proposed method consists of two stages. In the first stage, the pixels classified according to a new flag image are processed by different noise detectors. They are realized by using two median filters with different sizes of windows. In the second stage, each pixel once detected as an impulse in the first stage is verified by using a new system. According to the above stages, the proposed method can accurately detect the location of the impulsive noise and be effectively used as a preprocessor for noise reduction filtering. Experiments show that the proposed method can effectively detect impulsive noises in noisy images even when they are very highly corrupted.	circuit restoration;image restoration	Miki Haseyama;Keiko Kondo;Hideo Kitajima	2002		10.1109/ICIP.2002.1038025	filter;gradient noise;median filter;image restoration;image noise;computer vision;nonlinear distortion;detector;value noise;impulse noise;computer science;noise reduction;control theory;mathematics;neodymium;digital image;pixel;salt-and-pepper noise	Vision	56.43574927352723	-65.20745269707976	146988
4f1240321d5ffbf540c26c8ede314ec93cd43a81	learning video processing by example	image motion analysis;video signal processing;video sequences color educational institutions painting animation markov random fields cost function hidden markov models image segmentation streaming media;texture synthesis;video processing;image texture;learning by example video signal processing image sequences vectors image colour analysis image motion analysis image texture;motion blur;learning by example;vectors;image colour analysis;color correction;painting video processing learning by example input exemplars output exemplars pixel neighborhoods texture synthesis motion blur color correction;image sequences	We present an algorithm that approximates the output of an arbitrary video processing algorithm based on a pair of input and output exemplars. Our algorithm relies on learning the mapping between the input and output exemplars to model the processing that has taken place. We approximate the processing by observing that pixel neighborhoods similar in appearance and motion to those in the exemplar input should result in neighborhoods similar to the exemplar output. Since there are not many pixel neighborhoods in the exemplars, we use techniques from texture synthesis to generalize the output of neighborhoods not observed in the exemplars. The same algorithm is used to learn such processing as motion blur, color correction, and painting.	approximation algorithm;box blur;encode;feature vector;input/output;motion compensation;optical flow;pixel;sparse matrix;texture synthesis;video processing	Antonio Haro;Irfan A. Essa	2002		10.1109/ICPR.2002.1044771	image texture;computer vision;image processing;computer science;digital image processing;motion estimation;multimedia;video processing;texture synthesis;computer graphics (images)	Graphics	58.06923052005237	-54.295211539242715	147008
d3ea80e391bd6390d362c17810f5834fcad315da	joint demosaicking and zooming using moderate spectral correlation and consistent edge map	color difference;algorithms	The recently published joint demosaicking and zooming algorithms for single-sensor digital cameras all overfit the popular Kodak test images, which have been found to have higher spectral correlation than typical color images. Their performance perhaps significantly degrades on other datasets, such as the McMaster test images, which have weak spectral correlation. A new joint demosaicking and zooming algorithm is proposed for the Bayer color filter array (CFA) pattern, in which the edge direction information (edge map) extracted from the raw CFA data is consistently used in demosaicking and zooming. It also moderately utilizes the spectral correlation between color planes. The experimental results confirm that the proposed algorithm produces an excellent performance on both the Kodak and McMaster datasets in terms of both subjective and objective measures. Our algorithm also has high computational efficiency. It provides a better tradeoff among adaptability, performance, and computational cost compared to the existing algorithms. © 2014 SPIE and IS&T [DOI: 10.1117/1.JEI.23.4.043010]		Dengwen Zhou;Weiming Dong;Wengang Chen	2014	J. Electronic Imaging	10.1117/1.JEI.23.4.043010	computer vision;computer science;color difference;computer graphics (images)	Vision	59.574301784268464	-64.90668182883353	147022
37129c6e9ef63fa8414e015eb4a51dbd605be53f	hierarchical approach to the optimal design of camera spectral sensitivities for colorimetric and spectral performance	fabrication;sensors;digital color imaging;colorimetry;optimal design;spectral sensitivity;cameras;color image	The optimal design of spectral sensitivity functions for digital color imaging devices has been studied extensively. This paper analyzed the important requirements for designing sensor sensitivity functions. A hierarchical approach is proposed to the optimal design of camera spectral sensitivity functions by incorporating spectral fitting, colorimetric performance and noise. The approach is directly based on the filter fabrication parameters to avoid approximation deviation. A six-channel camera is designed via this approach, with the first three channels aiming at colorimetric performance and the total six channels for spectral performance.	approximation;mathematical optimization;mitchell corporation;optimal design;requirement	Shuxue Quan;Noboru Ohta;Roy S. Berns;Naoya Katoh	2003		10.1117/12.475437	computer vision;engineering;optics;computer graphics (images)	Vision	61.726889050844385	-58.33365003246614	147026
1dbb6d375464fa3fd1a63a158e2c08eac1e9ef8f	identifying common source digital camera from image pairs	authentication forensics camera identification sensor fingerprint;camera identification;authentication;digital camera;digital cameras fingerprint recognition joining processes image sensors colored noise object recognition pixel biosensors digital images detectors;statistical testing fingerprint identification image sensors;indexing terms;image sensors;sensor fingerprint common source digital camera identification image pairs test statistics two channel detector digital image verification device linking method;sensor;fingerprint;statistical testing;digital image;fingerprint identification;forensics	In this paper, we propose a method for verifying whether two digital images were obtained using the same digital camera. The method uses test statistics derived from a two-channel detector taking as input the noise residuals from both images. It is not assumed that the camera that took the images is available.	digital camera;digital image;verification and validation	Miroslav Goljan;Mo Chen;Jessica J. Fridrich	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379537	fingerprint;computer vision;digital photography;camera auto-calibration;image analysis;image resolution;computer science;digital image processing;image sensor;digital imaging;internet privacy;forensic science;camera interface;fingerprint recognition;digital image;computer graphics (images)	Robotics	54.477811864490036	-61.31161637154089	147168
402fdcd4c987bec76b31dc3d93d852e802f6cfe7	compressive sensing of light fields	image resolution bayes methods data compression image coding;compressed sensing;image coding;image resolution;data compression;coded aperture imaging;sensors;bayes methods;light field;bayesian methods;bayesian method;light fields;image acquisition;compressive sensing;image reconstruction;computational photography;tv;reconstruction algorithm;signal to noise ratio;cameras apertures image coding layout photography reconstruction algorithms spatial resolution lenses image resolution image reconstruction;cameras;bayesian methods compressive sensing camera design light field image acquisition randomly coded nonrefractive mask high spatial resolution signal to noise ratio;apertures;high spatial resolution;bayesian methods compressive sensing light fields coded aperture imaging computational photography	We propose a novel camera design for light field image acquisition using compressive sensing. By utilizing a randomly coded non-refractive mask in front of the aperture, incoherent measurements of the light passing through different regions are encoded in the captured images. A novel reconstruction algorithm is proposed to recover the original light field image from these acquisitions. Using the principles of compressive sensing, we demonstrate that light field images with high angular dimension can be captured with only a few acquisitions. Moreover, the proposed design provides images with high spatial resolution and signal-to-noise-ratio (SNR), and therefore does not suffer from limitations common to existing light-field camera designs. Experimental results demonstrate the efficiency of the proposed system.	algorithm;angularjs;compressed sensing;light field;randomness;signal-to-noise ratio	S. Derin Babacan;Reto Ansorge;Martin Luessi;Rafael Molina;Aggelos K. Katsaggelos	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414462	computer vision;computational photography;bayesian probability;computer science;compressed sensing	Robotics	59.73197752257507	-57.86574289144559	147195
413b2088b26af2e89d6176f42c2d03ceaf4be8fd	improving information perception from digital images for users with dichromatic color vision	chemical vapor deposition;technology;color blindness;digital imaging;teknik;engineering and technology;teknik och teknologier;color vision	Color vision deficiency (CVD) is the inability or limited ability to recognize colors and discriminate between them. A person with this condition perceives a narrower range of colors compared to a  ...	color vision;digital image	Omid Shayeghpour;Daniel Nyström;Sasan Gooran	2014		10.1117/12.2039132	computer vision;nanotechnology;chemical vapor deposition;digital imaging;color vision;optics;physics;computer graphics (images);technology	HCI	63.66155231920291	-57.238650468665085	147311
7ca077c2dc512af7f353192a6bd897e77c5710d1	quantifying the importance of cyclopean view and binocular rivalry-related features for objective quality assessment of mobile 3d video	signal image and speech processing;biometrics;pattern recognition;image processing and computer vision	3D video is expected to provide an enhanced user experience by using the impression of depth to bring greater realism to the user. Quality assessment plays an important role in the design and optimization of 3D video processing systems. In this paper, a new 3D image quality model that is specifically tailored for mobile 3D video is proposed. The model adopts three quality components, called the cyclopean view, binocular rivalry, and the scene geometry, in which the quality must be quantified. The cyclopean view formation process is simulated and its quality is evaluated using the three proposed approaches. Binocular rivalry is quantified over the distorted stereo pairs, and the scene quality is quantified over the disparity map. Based on the model, the 3D image quality can then be assessed using state-of-the-art 2D quality measures selected appropriately through a machine learning approach. To make the metric simple, fast, and efficient, final selection of the quality features is accomplished by also considering the computational complexity and the CPU running time. The metric is compared with several currently available 2D and 3D metrics. Experimental results show that the compound metric gives a significantly high correlation with the mean opinion scores that were collected through large-scale subjective tests run on mobile 3D video content.	binocular disparity;binocular vision;central processing unit;computational complexity theory;digital video;image quality;machine learning;mathematical optimization;mobile 3d graphics api;norm (social);stereoscopy;time complexity;user experience;video processing	Lina Jin;Atanas Boev;Karen O. Egiazarian;Atanas P. Gotchev	2014	EURASIP J. Image and Video Processing	10.1186/1687-5281-2014-6	subjective video quality;computer vision;simulation;computer science;video quality;archaeology;pattern recognition;multimedia;biometrics	Vision	63.61519910767882	-63.71867375699735	147697
289d26d2b92e6911b8736c20b3d1ade9ddcb73f4	a variational link of marking to color for color printing technologies: the color gain matrix for the simple estimation of print to print variation			item unique identification;printing;variational principle	Michael Sanchez;Martin Maltz	2005			computer vision;color printing;artificial intelligence;computer science;color balance;matrix (mathematics)	HCI	63.60059252037535	-56.08201629303226	147743
830d42bf3fb0b9fc35765f85add720867d9a7539	video inpainting of complex scenes	video textures;65k10;65c20;patch based inpainting;68u10;moving background;video inpainting	We propose an automatic video inpainting algorithm which relies on the optimisation of a global, patch-based functional. Our algorithm is able to deal with a variety of challenging situations which naturally arise in video inpainting, such as the correct reconstruction of dynamic textures, multiple moving objects and moving background. Furthermore, we achieve this in an order of magnitude less execution time with respect to the state-of-the-art. We are also able to achieve good quality results on high definition videos. Finally, we provide specific algorithmic details to make implementation of our algorithm as easy as possible. The resulting algorithm requires no segmentation or manual input other than the definition of the inpainting mask, and can deal with a wider variety of situations than is handled by previous work.	algorithm;as-easy-as;inpainting;mathematical optimization;run time (program lifecycle phase)	Alasdair Newson;Andrés Almansa;Matthieu Fradet;Yann Gousseau;Patrick Pérez	2014	SIAM J. Imaging Sciences	10.1137/140954933	computer vision;computer science;multimedia;inpainting;computer graphics (images)	Vision	56.63990608079982	-56.82505334938793	147806
75709af3dea9750ee3a32b5b1851efa70c2e05f7	background sprite generation using mpeg motion vectors	sprite generation;compressed domain;mpeg-4;video mosaics.	Sprite coding, accepted by the emerging MPEG-4 standard is a very efficient method for representing the background video object. Still this sprite generation is an open issue due to the foreground objects which obstructs the accuracy of camera motion estimation and blurs the generated sprite. In this paper we propose a method for constructing the background sprite with partial decoding of the MPEG stream. Initially the Independently Moving Objects (IMO) are segmented out from the background by clustering the pre-processed motion vectors of MPEG video. The camera motion parameters are obtained from the motion information corresponding to the background region which is used for frame alignment.	cluster analysis;computation;database;image segmentation;motion estimation;moving picture experts group;sparse matrix;sprite (computer graphics)	R. Venkatesh Babu;K. R. Ramakrishnan	2002			computer vision;pattern recognition;artificial intelligence;computer science;motion vector;video tracking;motion compensation;quarter-pixel motion;block-matching algorithm;motion estimation;multiview video coding;match moving	Vision	56.29021577974066	-55.22200238178635	147942
0a9ee678479350b1afc3a0f9e45b0189987e4a1f	gpu-based euclidean distance transforms and their application to volume rendering		We present discrete 2D and 3D distance transforms based on the vec- tor propagation algorithm by Danielsson. Like other vector propagation algo- rithms, the proposed method is close to exact, i.e., the error can be strictly bounded from above and is significantly smaller than one pixel. Our contribution is that the algorithm runs entirely on consumer class graphics hardware, thereby achieving a throughput of up to 96 Mpixels/s. Therefore, the proposed method can be used in a wide range of applications that rely on both high speed and high quality. The usability of our approach is demonstrated in the context of hardware- accelerated volumetric isosurface raycasting.	euclidean distance;graphics processing unit;volume rendering	Jens Schneider;Martin Kraus;Rüdiger Westermann	2009		10.1007/978-3-642-11840-1_16	computer vision;mathematical optimization;computer science;theoretical computer science;geometry;computer graphics (images)	Visualization	67.40272117150086	-52.32613293440605	148125
017df896a527120fb9ea036ae6039a6feffebf80	a cluster-based adaptive switching median filter	pattern clustering;psnr;image processing;impulses;impulse noise;image restoration;adaptive filters switches image restoration filtering algorithms educational institutions psnr;iterative methods;adaptive filters;filtering algorithms;clustering;image details clustering impulses;pattern clustering adaptive filters image processing impulse noise iterative methods median filters;random valued impulse noise filtering technique cluster based adaptive switching median filter clustering analysis linear function local image statistics iteration function corrupted image noise detector noisy pixels cluster based adaptive weighted median filter noise candidate value estimation psnr mae;switches;image details;median filters	This paper presents a cluster-based adaptive weight switching median filter. Clustering analysis and a linear function is combined to capture local image statistics. In term of the local information, an iteration function is constructed to subtract impulses from corrupted image and thus noise detector is defined. After the noisy pixels are identified, in order to keep image details as intact as possible, a cluster-based adaptive weighted median filter is proposed to estimate those noise candidates' values. Simulation results show that the proposed method provides better performance in term of PSNR and MAE than many existing random-valued impulse noise filtering techniques.	adaptive switching;impulse noise (audio);iteration;linear function;median filter;peak signal-to-noise ratio;pixel;scene statistics;simulation	Yunfan Wang;Zhu Zhu;Lei Miao;Xiaoguo Zhang;Xueyin Wang;Qing Wang	2013	2013 Seventh International Conference on Image and Graphics	10.1109/ICIG.2013.14	adaptive filter;median filter;image restoration;computer vision;mathematical optimization;kernel adaptive filter;peak signal-to-noise ratio;image processing;network switch;impulse noise;computer science;mathematics;iterative method;cluster analysis;salt-and-pepper noise	Robotics	56.49863791416155	-65.78605965934862	148170
3b17a0693878613ba2764dd4d71f8658ae9dc113	dense optical flow via robust data fusion	confidence measure;confidence region;global optimisation;data fusion;optical flow	Locally optimised differential methods for computing optical flow have the merit of being faster and more reliable under noise when compared to their globally optimised counterparts. However, they produce sparse flow estimates as a result of being unable to deal with local image regions of little texture. They are also particularly inefficient for regions whose change of feature-constancy function is highly nonlinear. In this study, we treat several limitations of the local approach in monochrome images. We present a robust H∞ data fusion-based framework to propagate flow information from high confidence regions to those suspected of poor quality estimates. The adopted data fusion engine is tolerant towards uncertainty and error inherited in the optical flow computation process. A new integrated confidence measure is also presented to predict the accuracy of the recovered flow across the image enabling the data fusion engine to work as an intelligent filling-in effect, not only when the intensity is problematic but also for other uncertain regions. Results demonstrate the significance of the proposed method.	optical flow	Mohd. Kharbat;Nabil Aouf	2011	Signal, Image and Video Processing	10.1007/s11760-010-0155-y	computer vision;simulation;computer science;confidence region;data mining;optical flow;sensor fusion	Vision	55.599684640971596	-57.4348093937013	148336
29607e5b6e258d37461c462e919f028672d4a7cc	impact of ccd size, pixel pitch, and anti-aliasing filter design on sharpness of digital camera print	filter design		aliasing;anti-aliasing filter;charge-coupled device;digital camera;dot pitch;filter design;pixel	Bruce H. Pillman	2000			digital camera;computer vision;dot pitch;anti-aliasing filter;digital camera back;artificial intelligence;camera resectioning;filter design;computer science	HCI	61.15443336859803	-58.03913145895366	148499
afa907c8b776967e637c07ad3b40934ef31d1daa	heightfield and spatially varying brdf reconstruction for materials with interreflections	and texture i 4 8 image processing and computer vision scene analysis shape;shadowing;i 3 7 computer graphics three dimensional graphics and realism color;shading	Photo-realistic reproduction of material appearance from images has widespread use in applications ranging from movies over advertising to virtual prototyping. A common approach to this task is to reconstruct the small scale geometry of the sample and to capture the reflectance properties using spatially varying BRDFs. For this, multi-view and photometric stereo reconstruction can be used, both of which are limited regarding the amount of either view or light directions and suffer from either lowor high-frequency artifacts, respectively. In this paper, we propose a new algorithm combining both techniques to recover heightfields and spatially varying BRDFs while at the same time overcoming the above mentioned drawbacks. Our main contribution is a novel objective function which allows for the reconstruction of a heightfield and high quality SVBRDF including view dependent effects. Thereby, our method also avoids both low and high frequency artifacts. Additionally, our algorithm takes interreflections into account allowing for the reconstruction of undisturbed representations of the underlying material. In our experiments, including synthetic and real-world data, we show that our approach is superior to state-ofthe-art methods regarding reconstruction error as well as visual impression. Both the reconstructed geometry and the recovered SVBRDF are highly accurate, resulting in a faithful reproduction of the materials characteristic appearance, which is of paramount importance in the context of material rendering.	algorithm;artifact (software development);bidirectional reflectance distribution function;blackwell (series);compiler;correspondence problem;display resolution;eurographics;experiment;heightmap;loss function;optimization problem;photometric stereo;synthetic intelligence	Roland Ruiters;Reinhard Klein	2009	Comput. Graph. Forum	10.1111/j.1467-8659.2009.01390.x	computer vision;shading;simulation;computer science;computer graphics (images)	Vision	58.69963320580116	-52.3036306751901	148771
0d0ec200c0ec1d473e1eba8e7a8bdcc65665ecb2	realistic rendering of birefringency in uniaxial crystals	polarization;crystals;computer graphic;rendering system;birefringence	In this article we derive the complete set of formulas needed to generate physically plausible images of uniaxial crystals. So far no computer graphics publication contains all the formulas one needs to compute the interaction of light with such crystals in a form that is useable by a graphics application, especially if a polarization-aware rendering system is being used.  This paper contains the complete derivation of the Fresnel coefficients for birefringent transparent materials, as well as for the direction cosines of the extraordinary ray and the Mueller matrices necessary to describe polarization effects. The formulas we derive can be directly used in a ray based renderer, and we demonstrate these capabilities in test scenes.	birefringence;coefficient;computer graphics;graphics software;mueller calculus;polarization (waves);rendering (computer graphics);usability	Andrea Weidlich;Alexander Wilkie	2008	ACM Trans. Graph.	10.1145/1330511.1330517	birefringence;computer vision;polarization;crystal;optics;computer graphics (images)	Graphics	62.805452769608756	-52.34520991823322	148773
b34c88af5de5e2f511ca5830c461cc689843085f	light beam tracing for multi-bounce specular and glossy transport paths	uctd;light beams;presentation;thesis;beam tracing	This paper describes a new extension to light beam tracing that includes glossy multi-bounce transport paths for more realistic rendering of caustics. A spherical Gaussian approximation of the glossy scatter distribution as well as Gauss' divergence theorem is used to develop an efficient solution that replaces the irradiance surface integral with a boundary line integral.	approximation;beam tracing;global illumination	Bernardt Duvenhage;Kadi Bouatouch;Derrick G. Kourie	2014		10.1145/2664591.2664610	cone tracing;computer vision;optics;beam tracing;global illumination;physics;computer graphics (images)	Graphics	63.28575095297802	-52.22013167666948	148831
affacbce5f90a0db949d841bcc22fd207792119d	robust optical flow in rainy scenes		Optical flow estimation in rainy scenes is challenging due to degradation caused by rain streaks and rain accumulation, where the latter refers to the poor visibility of remote scenes due to intense rainfall. To resolve the problem, we introduce a residue channel, a single channel (gray) image that is free from rain, and its colored version, a colored-residue image. We propose to utilize these two rain-free images in computing optical flow. To deal with the loss of contrast and the attendant sensitivity to noise, we decompose each of the input images into a piecewise-smooth structure layer and a high-frequency fine-detail texture layer. We combine the colored-residue images and structure layers in a unified objective function, so that the estimation of optical flow can be more robust. Results on both synthetic and real images show that our algorithm outperforms existing methods on different types of rain sequences. To our knowledge, this is the first optical flow method specifically dealing with rain. We also provide an optical flow dataset consisting of both synthetic and real rain images.	algorithm;benchmark (computing);elegant degradation;glossary of computer graphics;loss function;maximum flow problem;optical flow;optimization problem;synthetic data;synthetic intelligence;tree accumulation	Ruoteng Li;Robby T. Tan;Loong Fah Cheong	2018		10.1007/978-3-030-01267-0_18	computer vision;visibility;artificial intelligence;computer science;precipitation;real image;optical flow;communication channel	Vision	57.54109079521646	-59.538606713029814	148880
0dedfb1ffd1148a30ad5ef7331f7f87cd80e3eb0	mitigating discontinuities in segmented karhunen-loeve transforms	hyperspectral images karhunen loeve transform klt segmentation discontinuities;image coding;complexity theory;image segmentation;hyperspectral images;proceedings;segmentation;klt;discontinuities;covariance matrices;transforms;transforms image segmentation hyperspectral imaging covariance matrices image coding encoding complexity theory;hyperspectral imaging;encoding;karhunen loeve transform	The Karhunen-Loeve Transform (KLT) is a popular transform used in multiple image processing scenarios. Sometimes, the application of the KLT is not carried out as a single transform over an entire image. Rather, the image is divided into smaller spatial regions (segments), each of which is transformed by a smaller dimensional KLT. Such a situation may penalize the transform efficiency. An improvement for the segmented KLT, aiming at mitigating discontinuities arising on the edge of adjacent regions, is proposed in this paper. In the case of moderately varying image regions, discontinuities occur as the consequence of disregarded similarity between transform domains, as the order and sign of eigenvectors in the transform matrices are mismatched. In the proposed method, the KLT is adjusted to guarantee the best achievable similarity via the optimal assignment and sign correspondence for eigenvectors. Experimental results indicate that the proposed transform improves the similarity between transform domains, and reduces RMSE on the edge of adjacent regions. In consequence, images processed by the adjusted KLT present better cohesion and continuity between independently transformed regions.	ampersand;cohesion (computer science);image processing;scott continuity	Monika Stadnicka;Ian Blanes;Joan Serra-Sagristà;Michael W. Marcellin	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532751	computer vision;speech recognition;computer science;hyperspectral imaging;pattern recognition;mathematics;image segmentation;segmentation;karhunen–loève theorem;top-hat transform;encoding;statistics;classification of discontinuities	Vision	57.1037914804806	-62.564012674518764	148953
9b9cb16f6fa391456934bfd78bfad1915a7302eb	generation of high resolution image based on accumulated feature trajectory	multiple low resolution images;high resolution image generation;motion estimation image enhancement image registration image resolution;motion vector estimation high resolution image generation frame registration multiple low resolution images image restoration image quality enhancement frame memory hardware implementation multiframe super resolution method;high resolution;image processing;image resolution;frame registration;low resolution;motion estimation;image restoration;feature trajectory;image processing conferences;image enhancement;image generation;motion vector;image registration;image quality;registration;frame reconstruction super resolution feature trajectory motion estimation registration;frame memory;memory systems;multiframe super resolution method;super resolution;high resolution imager;motion vector estimation;image quality enhancement;hardware implementation;conferences;frame reconstruction	The proposed method creates a high-resolution(HR) image on the basis of the frame registration of multiple low-resolution(LR) images. Not only does the super-resolution(SR) method based on using multiple LR images generally enhance the restored HR image quality compared to that based on using a single LR image, but it also increases the complexity and frame memory for hardware implementation. In order to generate an HR image, the multi-frame SR method has to estimate all motion vectors(MVs) between the target LR image and all the reference LR images. Additionally, the total frame memories used for storing LR images have to be preset according to the number of all the reference LR images. Therefore, the proposed multi-frame SR method focuses on a real-time and low frame memory system, thereby reducing the number of motion estimation(ME) operations and the total frame memory required, and preserving the image quality in an HR image restoration. First, we classify the input LR image into a feature and a uniform region in order to reduce the frame memory because the performance of SR algorithms is predominantly affected by restoring a feature region rather than a uniform region. Accordingly, we only save and use the feature region of the multiple LR images and not the uniform region for restoring an HR image. Next, the MV of each feature is estimated frame-wise to reduce the complexity of ME, and these MVs are accumulated as the feature trajectories through multiple LR frames. In the proposed method, the ME operation is conducted once between the reference LR image and the target LR image, and the estimated MVs are linked to the feature trajectories. These accumulated feature trajectories are used for generating an HR image. Experimental results show that the proposed multi-frame SR method can reduce the complexity and frame memory to one-third, while the quality of the restored HR image is equal to that obtained by using the conventional SR methods.	algorithm;circuit restoration;complexity;image processing;image quality;image resolution;image restoration;lr parser;real-time clock	Yang Ho Cho;Kyu-Young Hwang;Ho-Young Lee;Du-Sik Park	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5653651	computer vision;speech recognition;image resolution;image processing;computer science;computer graphics (images)	Vision	55.01885609848969	-58.51474873916168	149155
131d28137a631aa95665cd25c953db8c7bed848f	efficient method of detecting globally blurry or sharp images	image degradation;degradation;efficient method;enhancement process;image motion analysis;prediction error;detection algorithms;information security;image restoration cameras image enhancement;sharp images;digital camera;image restoration;motion compensated;digital cameras;accuracy;image enhancement;estimation;discrete cosine transforms;information management;image quality;pixel;detection algorithm;image quality pixel digital cameras digital images degradation discrete cosine transforms image motion analysis information management information security detection algorithms;enhancement process sharp images blurry images detection efficient method digital cameras image degradation prediction error variance;digital image;digital images;cameras;blurry images detection;prediction error variance	In this paper we present a simple and efficient method for detecting the blurriness in the pictures. Recently, many digital cameras are equipped with auto-focusing functions to help the users take well-focused pictures. However, digital images can be degraded by limited contrast, inappropriate exposure, imperfection of auto-focusing or motion compensating devices, limited knowledge of amateur photographers, and so on. In order to detect blurry images for deleting them or making them go through an enhancement process automatically, a reliable measure of image degradation is needed. This paper presents a blurriness/sharpness detection algorithm based on the prediction-error variance, and demonstrates its feasibility by using extensive experiments. This method is fast, easy to implement and accurate. Regardless of the detection accuracy, the proposed method in this paper is not demanding on computation time.	algorithm;computation;digital camera;digital image;elegant degradation;experiment;sensor;time complexity	Elena Tsomko;Hyoung Joong Kim	2008	2008 Ninth International Workshop on Image Analysis for Multimedia Interactive Services	10.1109/WIAMIS.2008.28	computer vision;computer science;information security;multimedia;information management;digital image;statistics;computer graphics (images)	Vision	58.20059220123403	-60.82696682362909	149196
835c9ec6a65a7834f0eefdbd044ed0604b8f6780	camera-based document image mosaicing	image patches;high resolution;image segmentation;image patches camera based document image mosaicing camera captured document image registration;camera captured document;image registration;document image processing;camera based document image mosaicing;image segmentation document image processing image registration;image mosaicing;cameras image registration mice hardware laboratories educational institutions image resolution digital images satellites robustness	In this paper we present an image mosaicing method for camera-captured documents. Our method is unique in not restricting the camera position, thus allowing greater flexibility than scanner-based or fixed-camera-based approaches. To accommodate for the perspective distortions introduced by varying poses, we implement a two-step image registration process that relies on accurately computing the projectivity between any two document images with an overlapping area as small as 10%. In the overlapping area, we apply a sharpness based selection process to obtain seamless blending across the border and within. Experiments show that our approach can produce a very sharp, high resolution and accurate full page mosaic from small image patches of a document	addresses (publication format);algorithm;align (company);alpha compositing;computation (action);distortion;document mosaicing;experiment;image registration;image resolution;mosaic - computer software;ncsa mosaic;scanning systems;seamless3d;registration - actclass	Jian Liang;Daniel DeMenthon;David S. Doermann	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.352	image quality;image warping;homography;image texture;image restoration;computer vision;feature detection;image resolution;binary image;image processing;computer science;image registration;digital image processing;multimedia;image segmentation;automatic image annotation;digital image;standard test image;computer graphics (images)	Vision	57.87220995015504	-53.22956022634502	149372
50a9668b5be94967ed22b1afa24910915ed23b2f	influence of motion on contrast perception: supra-threshold spatio-velocity measurements	traitement signal;man;estimation mouvement;spatial frequencies;0130c;low pass;spatial variation;image optique;motion estimation;mesure vitesse;4266s;band pass;signal processing;frequence spatiale;variacion espacial;tâche appariement;tarea apareamiento;perception visuelle;optical images;variation spatiale;visual perception;velocity measurement;matching task;spatial frequency;homme	In this paper, a supra-threshold spatio-velocity CSF experiment is described. It consists in a contrast matching task with a methods of limits procedure. Results enable the determination of contrast perception functions which give, for given spatial and temporal frequencies, the perceived contrast of a moving stimulus. These contrast perception functions are then used to construct supra-threshold spatio-velocity CSF. As for supra-threshold CSF in spatial domain, it can be observed that CSF shape changes from band-pass behaviour at threshold to low-pass behaviour at supra-threshold, along spatial frequencies. However, supra-threshold CSFs have a band-pass behaviour along temporal frequency has threshold one. This means that if spatial variations can be neglected above the visibility threshold, temporal ones are still of primary importance.	low-pass filter;supra, inc.;threshold model;user-subjective approach;velocity (software development)	Sylvain Tourancheau;Patrick Le Callet;Dominique Barba	2007		10.1117/12.704574	computer vision;signal processing;spatial frequency;optics;physics	Vision	62.74104475097799	-61.82371929035314	149758
d898d9aadd6ea8345a40f32c5c9d922da6053f48	evaluation of tone-mapping operators for hdr video under different ambient luminance levels	t technology general;i 3 3 computer graphics picture image generation display algorithms;image and video processing;i 4 0 computer graphics general image displays;high dynamic range tone mapping	Since high dynamic range (HDR) displays are not yet widely available, there is still a need to perform a dynamic range reduction of HDR content to reproduce it properly on standard dynamic range (SDR) displays. The most common techniques for performing this reduction are termed tone-mapping operators (TMOs). Although mobile devices are becoming widespread, methods for displaying HDR content on these SDR screens are still very much in their infancy. While several studies have been conducted to evaluate TMOs, few have been done with a goal of testing small screen displays (SSDs), common on mobile devices. This#R##N#paper presents an evaluation of six state-of-the-art HDR video TMOs. The experiments considered three different levels of ambient luminance under which 180 participants were asked to rank the TMOs for seven tone-mapped HDR video sequences. A comparison was conducted between tone-mapped HDR video footage shown on an SSD and on a large screen SDR display using an HDR display as reference. The results show that there are differences between the performance of the TMOs under#R##N#different ambient lighting levels and the TMOs that perform well on traditional large screen displays also perform well on SSDs at the same given luminance level.		Miguel Melo;Maximino Bessa;Kurt Debattista;Alan Chalmers	2015	Comput. Graph. Forum	10.1111/cgf.12606	computer vision;computer science;multimedia;computer graphics (images)	HCI	61.580794448384395	-61.68038544028955	149993
680e3800ee789c133201e48febc0e80b1cf3893d	diagnostically lossless coding of x-ray angiography images based on background suppression		X-ray angiography images are widely used to identify irregularities in the vascular system. Because of their high spatial resolution and the large amount of images generated daily, coding of X-ray angiography images is becoming essential. This paper proposes a diagnostically lossless coding method based on automatic segmentation of the focal area using ray-casting and α-shapes. The diagnostically relevant Region of Interest is first identified by exploiting the inherent symmetrical features of the image. The background is then suppressed and the resulting images are encoded using lossless and progressive lossy-to-lossless methods, including JPEG-LS, JPEG2000, H.264 and HEVC. Experiments on a large set of X-ray angiography images suggest that our method correctly identifies the Region of Interest. When compared to the case of coding with no background suppression, the method achieves average bit-stream reductions of nearly 34% and improvements on the reconstruction quality of up to 20 dB-SNR for progressive decoding.	alpha shape;bitstream;focal (programming language);h.264/mpeg-4 avc;high efficiency video coding;jpeg 2000;lossless compression;lossy compression;ray casting;region of interest;signal-to-noise ratio;zero suppression	Zhongwei Xu;Joan Bartrina-Rapesta;Ian Blanes;Victor Sanchez;Joan Serra-Sagristà;Marcel García-Bach;Juan Francisco Muñoz	2016	Computers & Electrical Engineering	10.1016/j.compeleceng.2016.02.014	computer vision	Vision	58.28077637929363	-58.92160907745272	150033
b4815278a6c5d0d6769983f7afa05fa50c65f260	an efficient and effective blind camera image quality metric via modeling quaternion wavelet coefficients		Abstract As an extension of Discrete and Complex Wavelet Transform, Quaternion Wavelet Transform (QWT) has attracted extensive attention in the past few years, because it can provide better analytic representation for 2D images. The QWT of an image consists of four parts, i.e., one magnitude part and three phase parts. The magnitude is nearly shift-invariant, which characterizes features at any spatial location, and the three phases represent the structure of these features. This indicates that QWT is more powerful in representing image structures, and thus is suitable for image quality evaluation. In this paper, an efficient and effective Camera Image Quality Metric (CIQM) is proposed based on QWT, which is utilized to describe the intrinsic structures of an image. For an image, it is first decomposed by QWT with three scales. Then, for each scale, the magnitude and entropy of the subband coefficients, and natural scene statistics of the third phase are calculated. The magnitude is utilized to describe the generalized spectral behavior, and the entropy is used to encode the generalized information of distortions. Since the third phase of QWT is considered to be texture feature, the natural scene statistics of the third phase of QWT is used to measure structure degradations in the proposed method. All these features reflect the self-similarity and independency of image content, which can effectively reflect image distortions. Finally, random forest is utilized to build the quality model. Experiments conducted on three camera image databases and two multiply distorted image databases have proved that CIQM outperforms the relevant state-of-the-art models for both authentically distorted images and multiply distorted images.	coefficient;image quality;wavelet	Lijuan Tang;Leida Li;Kezheng Sun;Zhifang Xia;Ke Gu;Jiansheng Qian	2017	J. Visual Communication and Image Representation	10.1016/j.jvcir.2017.09.010	wavelet;computer vision;artificial intelligence;quaternion;random forest;mathematics;wavelet transform;pattern recognition;image quality;magnitude (mathematics);complex wavelet transform;scene statistics	Vision	61.677705896710094	-65.20154290069442	150337
b743abb4f71f7db9ee4d663db585d4b1a6b23762	study of non-linear mixing in hyperspectral imagery — a first attempt in the laboratory	geophysical image processing;reflectivity materials cameras laboratories reflection hyperspectral imaging atmospheric modeling;reflectivity;bilinear unmixing algorithm nonlinear spectral mixing hyperspectral image multiple reflection satellite borne airborne image material reflectance variability illumination angle brdf effect adjacency effect topographic effect image construction hyperspectral camera flat miniature scene linear mixing retrieval;materials;brdf and environmental effects non linear mixing hyperspectral camera laboratory;non linear mixing;hyperspectral camera;laboratory;atmospheric modeling;brdf and environmental effects;hyperspectral imaging;reflection;cameras;image retrieval cameras geophysical image processing;image retrieval	Non-linear spectral mixing occurs due to multiple reflections between different materials inside a single pixel. Studying such effects from satellite-borne or airborne images does not enable to control the parameters intervening in the mixing such as the variability of materials reflectance, viewing and illumination angles, BRDF, adjacency and topographic effects. Therefore, in this paper, we discuss the possibility to study non-linear mixing in the laboratory by the construction of a model imaged by a hyperspectral camera. We first study the case of a flat miniature scene; this experimental set-up enables to retrieve linear mixing results with a relative error of about 5–6%. Then, we reproduce the topographic effects responsible for the non-linearity of the mixing by adding some reliefs in the scene. We can then notice that the reflectance of the pixel is no more equal to the weighted average of the materials reflectance so that the non-linear effects due to reliefs are outlined. We finally apply the bilinear unmixing algorithm to improve unmixing results.	airborne ranger;algorithm;approximation error;bidirectional reflectance distribution function;bilinear filtering;bilinear transform;mixing (mathematics);nonlinear system;persistent vegetative state;pixel;reflection (computer graphics);simulation;spatial variability;topography	Pierre Huard;Rodolphe Marion	2011	2011 3rd Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)	10.1109/WHISPERS.2011.6080953	computer vision;geography;optics;remote sensing	Vision	65.90598053470521	-57.01266718006679	150351
86a989a8eb4c8c0f0e8a10dc7c896199fdd37167	pansharpening using total variation regularization	geophysical image processing;ikonos satellite images total variation regularization remote sensing complementary spectral resolution characteristics complementary spatial resolution characteristics multispectral image panchromatic image high resolution color image generation pansharpening method total variation variant minimization image fusion problem pixel colorization edge preservation;image resolution;image fusion;spatial resolution remote sensing satellites minimization image fusion noise;image colour analysis;remote sensing;multispectral images;artificial satellites;total variation;total variation pansharpening image fusion multispectral images;pansharpening;remote sensing artificial satellites geophysical image processing image colour analysis image fusion image resolution	In remote sensing, pansharpening refers to the technique that combines the complementary spectral and spatial resolution characteristics of a multispectral image and a panchromatic image, with the objective to generate a high-resolution color image. This paper presents a new pansharpening method based on the minimization of a variant of total variation. We consider the fusion problem as the colorization of each pixel in the panchromatic image. A new term concerning the gradient of the panchromatic image is introduced in the functional of total variation so as to preserve edges. Experimental results on IKONOS satellite images demonstrate the effectiveness of the proposed method.	color image;gradient;image resolution;multispectral image;pixel;total variation denoising	Xiyan He;Laurent Condat;Jocelyn Chanussot;Junshi Xia	2012	2012 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2012.6351611	image texture;multispectral image;computer vision;image resolution;optics;image fusion;total variation;physics;satellite;remote sensing	Robotics	67.63958699561904	-65.91024581894064	150766
2f3d9cdb657deace624c16f1fb1785f6298707c1	objective evaluation of light field rendering methods using effective sampling density	video signal processing computer vision numerical analysis rendering computer graphics sampling methods;free viewpoint video;selected works;video signal processing;computer graphic;computer vision;numerical simulation light field rendering method effective sampling density computer vision computer graphics free viewpoint video system geometric measurement lfr algorithm;numerical analysis;light field rendering;bepress;sampling methods;rendering computer graphics;subjective assessment;numerical simulation;rendering computer graphics electrostatic discharges interpolation cameras three dimensional displays mathematical model slabs	Light field rendering (LFR) is an active research area in computer vision and computer graphics. LFR plays a crucial role in free viewpoint video systems (FVV). Several rendering algorithms have been suggested for LFR. However, comparative evaluation of these methods is often limited to subjective assessment of the output. To overcome this problem, this paper presents a geometric measurement, Effective Sampling Density of the scene, referred to as effective sampling for brevity, for objective comparison and evaluation of LFR algorithms. We have derived the effective sampling for the well-known LFR methods. Both theoretical study and numerical simulation have shown that the proposed effective sampling is an effective indicator of the performance for LFR methods.	algorithm;computer graphics;computer vision;gibbs sampling;light field;numerical weather prediction;sampling (signal processing);simulation;whole earth 'lectronic link	Hooman Shidanshidi;Farzad Safaei;Wanqing Li	2011	2011 IEEE 13th International Workshop on Multimedia Signal Processing	10.1109/MMSP.2011.6093799	computer simulation;sampling;computer vision;simulation;image-based modeling and rendering;3d rendering;rendering;numerical analysis;computer science;real-time rendering;computer graphics;alternate frame rendering;software rendering;3d computer graphics;computer graphics (images)	Visualization	63.74785740627344	-52.78317021093394	150816
a70955bf7efccf380503536c3cf8def0c25b369a	efficient upsampling of natural images		We propose a novel method of efficient upsampling of a single natural image. Current methods for image upsampling tend to produce high-resolution images with either blurry salient edges, or loss of fine textural detail, or spurious noise artifacts. In our method, we mitigate these effects by modeling the input image as a sum of edge and detail layers, operating upon these layers separately, and merging the upscaled results in an automatic fashion. We formulate the upsampled output image as the solution to a non-convex energy minimization problem, and propose an algorithm to obtain a tractable approximate solution. Our algorithm comprises two main stages. 1) For the edge layer, we use a nonparametric approach by constructing a dictionary of patches from a given image, and synthesize edge regions in a higherresolution version of the image. 2) For the detail layer, we use a global parametric texture enhancement approach to synthesize detail regions across the image. We demonstrate that our method is able to accurately reproduce sharp edges as well as synthesize photorealistic textures, while avoiding common artifacts such as ringing and haloing. In addition, our method involves no training phase or estimation of model parameters, and is easily parallelizable. We demonstrate the utility of our method on a number of challenging standard test photos.	approximation algorithm;cobham's thesis;dictionary;energy minimization;image resolution;ringing (signal);ringing artifacts;upsampling	Chinmay Hegde;Oncel Tuzel;Fatih Murat Porikli	2015	CoRR		computer vision;mathematical optimization;mathematics;computer graphics (images)	Vision	56.106228461090026	-59.56964613134879	151292
79b67f0dc9f0f1166d66dc8839f87d61d49fb05d	advanced impulse detection based on pixel-wise mad	image processing;robust estimator;median filter;impulse noise;robust statistics;indexing terms;iterative methods;statistical analysis;filters pixel pulse width modulation noise robustness statistics neural networks filtering algorithms fuzzy reasoning design optimization optimization methods;statistics impulse noise removal variance estimation pixel wise mad filtering performance median filter;iterative methods median filters image processing impulse noise statistical analysis;median filters	A new method for impulse noise removal is presented, where a robust estimator of the variance, MAD (median of the absolute deviations from the median), is modified and used to efficiently separate noisy pixels from the image details. The algorithm is free of varying parameters, requires no previous training or optimization, and successfully removes all types of impulse noise. The pixel-wise MAD concept is straightforward, low in complexity, and achieves high filtering performance.	algorithm;impulse noise (audio);mad;mathematical optimization;pixel	V. Crnojevic;V. Senk;Zeljen Trpovski	2004	IEEE Signal Processing Letters	10.1109/LSP.2004.830117	median filter;robust statistics;image processing;computer science;machine learning;mathematics;statistics;salt-and-pepper noise	Vision	56.12616855983484	-65.95729534108536	151308
1df9f4e98d93312dc39f2c602e6a0c158d32a10d	motion-aware knn laplacian for video matting	pattern clustering;video signal processing;motion blur motion aware knn laplacian video matting nonlocal principle motion aware k nearest neighbor spatio temporally coherent cluster moving foreground pixels sparse user markup foreground color background color changing topologies illumination change fast motion;video signal processing pattern clustering;laplace equations vectors optical imaging image color analysis integrated optics optical scattering noise reduction	This paper demonstrates how the nonlocal principle benefits video matting via the KNN Laplacian, which comes with a straightforward implementation using motion-aware K nearest neighbors. In hindsight, the fundamental problem to solve in video matting is to produce spatio-temporally coherent clusters of moving foreground pixels. When used as described, the motion-aware KNN Laplacian is effective in addressing this fundamental problem, as demonstrated by sparse user markups typically on only one frame in a variety of challenging examples featuring ambiguous foreground and background colors, changing topologies with disocclusion, significant illumination changes, fast motion, and motion blur. When working with existing Laplacian-based systems, we expect our Laplacian can benefit them immediately with an improved clustering of moving foreground pixels.	cluster analysis;coherence (physics);color;feature vector;gaussian blur;k-nearest neighbors algorithm;laplacian matrix;nonlocal lagrangian;pixel;sparse matrix;web colors	Dingzeyu Li;Qifeng Chen;Chi-Keung Tang	2013	2013 IEEE International Conference on Computer Vision	10.1109/ICCV.2013.447	computer vision;pattern recognition;computer graphics (images)	Vision	56.40542060230479	-57.838278190105974	151389
8d9b278ccf8ed0e5e4e1703b77c2d27db0796495	color image quantization using weighted distortion measure of hvs color activity	image coding;image segmentation;visual quality;spatial masking effect color image quantization weighted distortion measure hvs color activity color palette optimal palette design image quality perception based distortion measure activity weighted distortion measure color visual sensitivity human visual system local image region hierarchical quantization algorithm computation time;quantisation signal;computational complexity;human visual system;image colour analysis;image quality;visual perception;color quantization distortion measurement weight measurement displays algorithm design and analysis image quality humans monitoring iterative algorithms;color quantization;image segmentation image colour analysis computational complexity quantisation signal image coding visual perception;color image quantization	Color quantization is to design a color palette having almost no noticeably perceived difference between original and quantized images. In this paper, to design such a palette, the color quantization algorithm is considered in two parts: the selection of a proper distortion measure and the design of an optimal palette. The proper selection of distortion measure in the quantization is important to image quality. Since the human eye is the final judge of image quality, it is desirable to use a perception-based distortion measure. Thus we developed an activity-weighted distortion measure considering color visual sensitivity and absorbtance of human visual system(HVS) depending on each color component in the local region of image. Then using the distortion measure, a hierarchical quantization algorithm is proposed. The algorithm consists of initial and subdivision steps both to reduce computation time and to minimize the distortion based on spatial masking effect of HVS. The experimental results show that the proposed algorithm shows better visual quality and less computation time comparing to the conventional algorithms.	algorithm;color image;color quantization;color space;computation;distortion;human visual system model;image quality;palette (computing);quantization (image processing);spatial anti-aliasing;subdivision surface;time complexity	Kyeong-Man Kim;Chae-Soo Lee;Eung-Joo Lee;Yeong-Ho Ha	1996		10.1109/ICIP.1996.561015	image quality;color histogram;computer vision;color quantization;color depth;color image;computer science;pattern recognition;mathematics;color balance;histogram equalization;dither;computer graphics (images)	Vision	58.698229311134284	-63.72273922627122	151624
9990a8f0b5553228f3793654a375618cdebce7ea	a model of network related qoe for 3d video	video streaming;lossy networks network related qoe model 3d video quality three dimensional video quality perceptual attributes image naturalness perceived depth comfort immersiveness 3d video streaming network parameters objective video quality measures subjective video quality measures stereoscopic video left and right views mos scores overall image quality prediction 3d video transmission;visual perception quality of experience stereo image processing video communication video streaming;quality of experience;streaming media stereo image processing packet loss quality assessment psnr;stereo image processing;visual perception;video communication	Three Dimensional video quality comprises a variety of perceptual attributes, including overall image quality, naturalness, presence, perceived depth, comfort, immersiveness, etc. This paper proposes a novel objective QoE model for 3D video streaming that takes into account network parameters (i.e. packet losses)and is based on the relationship between objective and subjective video quality measures. The investigation is concerned with stereoscopic video (left-and-right views). The analysis of the results indicate that the proposed QoE model has a strong correlation with MOS scores, hence can be effectively used in predicting the overall image quality of 3D videos transmitted over lossy networks.	image quality;lossy compression;network packet;peak signal-to-noise ratio;romeo;stereoscopy;streaming media;strongly correlated material	Ilias Politis;Lampros Dounis;Christos Tselios;Athanasios Kordelas;Tasos Dagiuklas;Andreas Papadakis	2012	2012 IEEE Globecom Workshops	10.1109/GLOCOMW.2012.6477776	video compression picture types;subjective video quality;computer vision;simulation;visual perception;computer science;video quality;video tracking;multimedia;video post-processing;pevq	Vision	63.48114731540098	-63.27818890214893	151712
f17d3cb0c5cb50188a93316466795c69d0b4b579	the effect of lightness scaling on the perceived color quality of compressed digital videos	quality attributes;digital signals;4230;data compression;etude experimentale;0130c;video compression;video quality;valor medio;qualite image;brightness;video coding;compression image;evaluation subjective;brillance;signal video;image compression;codage video;image quality;signal numerique;valeur moyenne;mean value;calidad imagen;4266;digital video;video signals;video;4230v;subjective evaluation;compression donnee;evaluacion subjetiva;compresion imagen	In this work, we studied how video compression and lightness scaling interact to affect the overall video quality and the color quality attributes. We examined three subjective attributes: perceived color preference, perceived color naturalness, and overall annoyance as digital videos were subjected to compression and lightness scaling. Psychophysical experiments were carried out in which naAÂ¯ve subjects made numerical judgments of the three subjective attributes. We found that preference and naturalness scores are concave down functions of mean lightness with an associated maximum, while annoyance scores are concave up with an associated minimum. As compression increases, both preference and naturalness scores decrease and vary less with mean lightness. Maximum preference, naturalness, and annoyance scores generally occur at similar mean lightness values. Preference, naturalness, and annoyance scores for individual videos, are approximated relatively well by Gaussian functions of mean lightness. Preference and naturalness scores decreases while annoyance scores increase as an S-shaped function of the logarithm of the total squared error. A three-parameter model is shown to provide a good description of how each attribute depends on lightness and compression for individual videos. Model parameters vary with video content.© (2008) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	image scaling	Chin Chye Koh;John M. Foley;Sanjit K. Mitra	2008		10.1117/12.767047	data compression;computer vision;simulation;multimedia	HCI	62.52203794825033	-63.00117842672652	151783
bf99399ccc8ed2161db53453270a9483f7fdd11b	severely noisy image segmentation via wavelet shrinkage using pso and fuzzy c-means	discrete wavelet transforms;image segmentation;noise measurement;clustering algorithms	The necessity of proposing algorithms that are effective in noisy image segmentation is clear in many real-world applications. This paper proposes a new algorithm for severely noisy image segmentation by looking at the proper choice of feature, and feature manipulation. We are using Discrete Wavelet Transformation (DWT) as a tool to provide our method with the proper feature, and then we manipulate it via wavelet shrinkage. Particle Swarm Optimization (PSO) is used to adaptively search for threshold values that produce the best segmentation results when applied in the wavelet shrinkage, and Fuzzy C-Means (FCM) is used as a fitness metric in PSO. The proposed method was tested on two different datasets being extremely contaminated with the common Gaussian noise. These tests indicate the superior performance and consistency of the proposed method in comparison to other state-of-the-art methods.	algorithm;coefficient;discrete wavelet transform;fuzzy cognitive map;image segmentation;particle swarm optimization;phase-shift oscillator	Saeed Mirghasemi;Peter Andreae;Mengjie Zhang;Ramesh Kumar Rayudu	2016	2016 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2016.7850051	computer vision;machine learning;segmentation-based object categorization;pattern recognition;cascade algorithm;mathematics;wavelet packet decomposition;image segmentation;scale-space segmentation	Vision	56.44368289990809	-65.52393965221589	151914
21fb6859084208643f62015dbf15e79c60d4adba	smart depth of field optimization applied to a robotised view camera	large format photography;depth of field;mathematical model;scheimpflug principle;computational photography;geometric model;optimal algorithm;constrained optimization problem	The great flexibility of a view camera allows the acquisition of high quality images that would not be possible any other way. Bringing a given object into focus is however a long and tedious task, although the underlying optical laws are known. A fundamental parameter is the aperture of the lens entrance pupil because it directly affects the depth of field. The smaller the aperture, the larger the depth of field. However a too small aperture destroys the sharpness of the image because of diffraction on the pupil edges. Hence, the desired optimal configuration of the camera is such that the object is in focus with the greatest possible lens aperture. In this paper, we show that when the object is a convex polyhedron, an elegant solution to this problem can be found. It takes the form of a constrained optimization problem, for which theoretical and numerical results are given. The optimization algorithm has been implemented on the prototype of a robotised view camera.	algorithm;constrained optimization;constraint (mathematics);display resolution;mathematical optimization;numerical analysis;optimization problem;polyhedron;prototype	Stéphane Mottelet;Luc de Saint Germain;Olivier Mondin	2011	Journal of Mathematical Imaging and Vision	10.1007/s10851-011-0306-y	scheimpflug principle;computer vision;computational photography;computer science;geometric modeling;mathematical model;depth of field;mathematics;computer graphics (images)	Vision	57.212715437129575	-52.1057274326275	151954
6288443cedb8c6225594eec38d04e3fcee20faf8	adaptive logarithmic mapping for displaying high contrast scenes	tone mapping;user preferences;power function;high contrast imaging;dynamic content;high dynamic range	We propose a fast, high quality tone mapping technique to display high contrast images on devices with limited dynamic range of luminance values. The method is based on logarithmic compression of luminance values, imitating the human response to light. A bias power function is introduced to adaptively vary logarithmic bases, resulting in good preservation of details and contrast. To improve contrast in dark areas, changes to the gamma correction procedure are proposed. Our adaptive logarithmic mapping technique is capable of producing perceptually tuned images with high dynamic content and works at interactive speed. We demonstrate a successful application of our tone mapping technique with a high dynamic range video player enabling to adjust optimal viewing conditions for any kind of display while taking into account user preference concerning brightness, contrast compression, and detail reproduction.	algorithm;binary logarithm;blackwell (series);computation;display resolution;dynamic web page;eurographics;experiment;gamma correction;graphics processing unit;high dynamic range;high-dynamic-range imaging;human visual system model;interactivity;interpolation;jack audio connection kit;loss function;nonlinear system;opengl;paul debevec;tone mapping;visual artifact	Frédéric Drago;Karol Myszkowski;Thomas Annen;Norishige Chiba	2003	Comput. Graph. Forum	10.1111/1467-8659.00689	computer vision;tone mapping;power function;computer science;dynamic web page;multimedia;computer graphics (images)	Graphics	61.1835685072693	-61.06700071723269	152302
83f3dc1e9ed9d22bb7381f82b115a9ac6037d258	efficient identification of arbitrary color filter array images based on the frequency domain approach	fourier transform;frequency spectrum;color filter array;mosaic images	For reducing the cost, most digital cameras are equipped with a CCD or CMOS sensor and a RGB color filter array (CFA) for each pixel to capture one primary color component, and hence produce a mosaic image. Suppose the input mosaic image without the CFA structure information, this paper presents a novel efficient method, consisting of a training-based scheme and an identification scheme, for identifying its CFA structure using the frequency domain approach. Initially, based on a set of training mosaic images with different CFA structures, a training-based scheme is proposed to build up the representative spectrum for every CFA structure. As the model maps, the constructed representative spectra can be reused in subsequent identification processes. The proposed identification scheme first constructs the representative spectrum of the header-less input mosaic image as the query map. Then, a matching scheme is proposed to identify the corresponding CFA structure of the query map from the model maps. Experimental results demonstrate that the proposed identification method has low computational cost and high identification accuracy merits for mosaic images without prior header information, when compared with the state-of-the-art spatial domain-based method by Chiu et al. HighlightsFor the mosaic image without the CFA structure information, we present a frequency domain method for identifying its CFA structure.The proposed method is consisted of a training-based scheme and an identification scheme.Experimental results demonstrate that the proposed identification method has low computational cost and high identification accuracy merits.	color filter array	Yong-Huai Huang;Kuo-Liang Chung;Tseng-Jung Lin	2015	Signal Processing	10.1016/j.sigpro.2015.03.023	fourier transform;computer vision;color filter array;frequency spectrum;electronic engineering;computer science;mathematics	Robotics	54.89003840480711	-62.41377200771806	152342
f74f5cb299c46d95a2e3cffe53afc45636ae9328	3d video assessment with just noticeable difference in depth evaluation	3d video assessment;depth perception assessment;quantization;3d video scene;image coding;data compression;depth perception depth images 3d video;video coding data compression image colour analysis stereo image processing;compression artifact 3d video assessment depth evaluation 3d video display system depth perception assessment video color depth representation auto stereoscopic display depth image compression just noticeable difference in depth 3d video scene;just noticeable difference in depth;three dimensional displays pixel image color analysis image coding quantization image analysis observers;observers;auto stereoscopic display;video coding;3d video display system;compression artifact;depth representation;depth perception;three dimensional displays;image color analysis;image colour analysis;pixel;stereo image processing;image analysis;depth evaluation;depth image compression;just noticeable difference;depth images;subjective evaluation;3d video;stereoscopic display;video color	The ability to provide a realistic perception of depth is the core added functionality of modern 3D video display systems. At present, there is no standard method to assess the perception of depth in 3D video. Existence of such methods would immensely enhance the progression of 3D video research. This paper focuses on the depth perception assessment in color plus depth representation of 3D video. In this paper, we subjectively evaluate the depth perceived by the users on an auto stereoscopic display, and analyze its variation with the impairments introduced during the compression of the depth images. The variation of the subjective perception of depth is explained based on another evaluation that is carried out to identify the Just Noticeable Difference in Depth (JNDD) perceived by the subjects. The JNDD corresponds to the sensitivity of the observers to the changes in depth in a 3D video scene. Even though only the effects of compression artifacts are considered in this paper, the proposed assessment technique, based on the JNDD values can be used in any future depth perception assessment work.	color gradient;compression artifact;depth perception;display device;stereoscopy	D. Varuna S. X. De Silva;Warnakulasuriya Anil Chandana Fernando;Gokce Nur;Erhan Ekmekcioglu;Stewart Worrall	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5653353	data compression;just-noticeable difference;computer vision;image analysis;quantization;depth perception;computer science;multimedia;compression artifact;pixel;statistics;computer graphics (images)	Visualization	62.76107675184529	-63.256655513919526	152349
2edaed11a65a0aa6d490f6b6b260a094800f9191	the detection of unresolved targets using the hough transform	time varying;image processing;linear filtering;frequency response;fir filter;pattern recognition;hough transform;target tracking;matched filter;line of sight;target acquisition	Clutter resulting from drift and vibration in a sensor line-of-sight and time-varying components in the background can seriously hinder the detection of faint, unresolved (i.e., point) targets moving against a background. These targets, however, produce well-defined patterns of motion referred to as tracks, whereas the effects of noise and clutter are more random. Frame-to-frame differencing can be used to reject clutter and enhance the detection of moving targets. These difference frames can then be combined recursively to form target tracks, The tracks of nonmaneuvering (i.e., constant velocity) targets appear as llne segments that can be detected using the Hough transform. In addition, the current position and heading of a target can be determined using a technique based on matched filtering. An example demonstrating the performance of this technique is also presented.	hough transform	Alan E. Cowart;Wesley E. Snyder;W. Howard Ruedger	1983	Computer Vision, Graphics, and Image Processing	10.1016/S0734-189X(83)80038-3	hough transform;computer vision;frequency response;speech recognition;image processing;computer science;finite impulse response;linear filter;matched filter;computer graphics (images)	Vision	54.19384465419109	-63.269317520624554	152460
e916f1c17da1e8ec68f1f15912384f6868b264c0	range image acquisition with a single binary-encoded light pattern	codificacion binaria;error correcting code;binary encoded light pattern;image processing;fault tolerant;illumination;codigo corrector error;saisie donnee;camera image;periodic structure;measurement system;procesamiento imagen;structured light;coding range image acquisition optical information processing computerised pattern recognition binary encoded light pattern identification illumination camera image pseudonoise sequences;computerised pattern recognition;depth;traitement image;light;optical information processing computerised pattern recognition encoding;binary coding;luz;error correction code;triangulacion;three dimensional system;optical information processing;toma dato;range image;identification;coding;profundidad;systeme 3 dimensions;sistema 3 dimensiones;triangulation;profondeur;depth map;code correcteur erreur;lumiere;encoding;vision;data acquisition;pseudonoise sequences;lighting layout cameras distortion measurement laboratories image coding fault diagnosis fault tolerance encoding prototypes;range image acquisition;codage binaire	The problem of strike identification in range image acquisition systems based on triangulation with periodically structured illumination is discussed. A coding scheme is presented based on a single fixed binary encoded illumination pattern, which contains all the information required to identify the individual strikes visible in the camera image. Every sample point indicated by the light pattern is made identifiable by means of a binary signature, which is locally shared among its closest neighbors. The applied code is derived from pseudonoise sequences, and it is optimized so that it can make the identification fault-tolerant to the largest extent. A prototype measurement system based on this coding principle is presented. Experimental results obtained with the measurement system are also presented. >	range imaging	Piet Vuylsteke;André Oosterlinck	1990	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.44402	computer vision;error detection and correction;image processing;computer science;mathematics;statistics	Vision	67.2671063068075	-60.03785412278614	152471
1d191c02736118bd3be4182104a59ded9fad38ce	detecting image forgeries using geometric cues	pattern recognition	This chapter presents a framework for detecting fake regions by using various methods including watermarking technique and blind approaches. In particular, we describe current categories on blind approaches which can be divided into five: pixel-based techniques, format-based techniques, camera-based techniques, physically-based techniques and geometric-based techniques. Then we take a second look on the geometric-based techniques and further categorize them in detail. In the following section, the state-of-the-art methods involved in the geometric technique are elaborated. INTRODUCTION Today's digital technology has begun to erode our trust on the integrity of the visual imagery since image editing software can generate highly photorealistic images (Farid, 2009). Doctored photographs are appearing with a growing frequency and sophistication in tabloid magazines, mainstream media outlets, political campaigns, photo hoaxes, evidences in a courtroom, insurance claims, and cases involving scientific fraud (Farid, 2009). With the rapid advancement in image editing software, photorealistic images will become increasingly easier to be generated and it becomes difficult for people to differentiate them from photographic images (Lyu & Farid, 2005). If we are to have any hope that photographs can hold the unique stature of being a definitive recording of events, we must develop technologies that can detect the tampered images. Therefore, authenticating the integrity of digital image's content has become particularly important when images are used as critical evidence in journalism and security surveillance applications. Over the past several years, the field of digital forensics has emerged to authenticate digital images by enforcing several authentication methods. The presence or absence of the watermark in interpolated images captured by the camera can be employed to establish the authenticity of digital color images. Digital watermarking (I.J. Cox & M.L. Miller & J.A. Bloom, 2002; H. Liu & J. Rao & X. Yao, 2008) has been proposed as a means to authenticate an image. However, a watermarking must be inserted at the time of recording, which would limit this approach to specially equipped digital cameras having no capabilities to add a watermarking at the time of image capture. Furthermore, the watermarking would be destroyed if the image is compressed and the ruin of watermark would make the method failed. Passive (nonintrusive) image forensics is regarded as the future direction. In contrast to the active methods, blind approaches need no prior information that is used in the absence of any digital watermarking or signature. Blind approaches can be roughly grouped into five categories (Farid, 2009): (1) pixel-based techniques that analyze pixel-level correlations arising from tampering. Efficient algorithms based on pixels have been proposed to detect cloned (B. Mahdian & S. Saic, 2007; A. Popescu & H. Farid, 2004; J. Fridrich & D. Soukal & J. Lukas, 2003), re-sampled (A. C. Popescu & H. Farid, 2005), spliced (T. T. Ng & S. F. Chang, 2004; T. T. Ng & S. F. Chang & Q. Sun, 2004; W. Chen, & Y. Shi, & W. Su, 2007) images.Statistical properties (H. Farid & S. Lyu, 2003; S. Bayram, & N. Memon, & M. Ramkumar, & B. Sankur, 2004) in natural images are also utilized; (2) format-based techniques detect tampering in lossy image compression: unique properties of lossy compression such as JPEG can be exploited for forensic analysis (H. Farid, 2008; J. Lukas & J. Fridrich, 2003; T. Pevny & J. Fridrich, 2008). (3) camera-based techniques exploit artifacts introduced by the camera lens, sensor or on-chip post-processing (J. Lukas, & J. Fridrich & M. Goljan, 2005; A. Swaminathan & M. Wu & K. J. Ray Liu, 2008). Models of color filter array (A. C. Popescu & H. Farid, 2005; S. Bayram & H. T. Sencar & N. Memon, 2005), camera response (Y. F. Hsu & S. F. Chang, 2007; Z. Lin & R. Wang & X. Tang & H.Y. Shum, 2005) and sensor noise (H. Gou & A. Swaminathan & M. Wu, 2007; M. Chen & J. Fridrich &M. Goljan & J. Lukas ,2008; J. Lukas, & J. Fridrich & M. Goljan, 2005) are estimated to infer the source digital cameras and reveal digitally altered images. Other work such as (A. Swaminathan & M. Wu & K. J. Ray Liu, 2008) trace the entire in-camera and post-camera processing operations to identify the source digital cameras and reveal digitally altered images using the intrinsic traces. (4) physically-based techniques model and detect anomalies using physical rules. For example, three dimensional interaction between physical objects, light, and the camera can be used as evidence of tampering (M.K. Johnson & H. Farid, 2005; M. K. Johnson & H. Farid, 2007). (5) geometric-based techniques make use of geometric constraints that are preserved or recovered from perspective views (M. K. Johnson & H. Farid, 2006; M. K. Johnson, 2007; W. Wang & H. Farid, 2008; W. Zhang & X. Cao & Z. Feng & J. Zhang & P. Wang, 2009; W. Zhang & X. Cao & J. Zhang & J.Zhu.&P. Wang, 2009). Several geometric-based techniques (M. K. Johnson & H. Farid, 2007; W. Wang & H. Farid, 2008; W. Zhang & X. Cao & Z. Feng & J. Zhang & P. Wang, 2009; M. K. Johnson & H. Farid, 2006) have been proposed in the field of image forgery detection. The estimation of internal camera parameters including principal point (M. K. Johnson & H. Farid, 2007) and skew (W. Wang & H. Farid, 2008) can be used as evidence of tampering. In (M. K. Johnson & H. Farid, 2007) the authors showed how translation in the image plane is equivalent to a shift of the principal point and differences in which can therefore be used as evidence of forgery. Wang and Farid (W. Wang & H. Farid, 2008) argued that the skew of the re-projected video is inconsistent with the expected parameter of an authentic video. The approach has the advantage that the re-projection can cause a non-zero skew in the camera intrinsic parameters, but there are also some drawbacks that it only applies to frames that contain a planar surface. Zhang et al. (W. Zhang & X. Cao & Z. Feng & J. Zhang & P. Wang, 2009) described a technique for detecting image composites by enforcing two-view geometrical constraints. The approach can detect fake regions efficiently on pictures at the same scene but requires two images correlated with H (planar homography) or F (fundamental matrix) constraints. Metric measurements can be made from a planar surface after rectifying the image. In (M. K. Johnson & H. Farid, 2006), the authors reviewed three techniques for the rectification of planar surfaces under perspective projection. They argued that knowledge of polygons of known shape, two or more vanishing points, and two or more coplanar circles can be used to recover the image to world transformation of the planar surface, thereby allowing metric measurements to be achieved on the plane. Each method in (M. K. Johnson & H. Farid, 2006) requires only one single image but fails in measurements for objects out of the reference plane. Wang et al. (G. Wang & Z. Hu & F. Wu & H. T. Tsui, 2005) show how to use the camera matrix and some available scene constraints to retrieve geometrical entities of the scene, such as height of an object on the reference plane, measurements on a vertical or arbitrary plane with respect to the reference plane, etc. The single view metrology using geometric constraints has been addressed in (A. Criminisi & I. Reid & A. Zisserman, 1999). The authors demonstrated that the affine 3D geometry of a scene may be measured from a single perspective image using the vanishing line of a reference plane and the vertical vanishing point. However, they are only concerned with measurements of the distance between the plane which is parallel to the reference plane and measurements on this plane. This chapter is organized as follows. After reviewing the background in section 2, the involved methods based on geometric technique are described in sections 3. The future research direction is given in section 4 and the final conclusions are drawn in section 5. BACKGROUND Photographic alterations have existed about as long as photography itself. However, before the digital age, such deceptions required mastery of complex and time-consuming darkroom techniques. Nowadays, anyone who has a little of computer skill can use powerful and inexpensive editing software to create tampered images as he or she likes. Therefore, as sophisticated forgeries appear with fast and alarming frequency, people’s belief in what they see has been eroded (H. Farid, 2009). A more recent example of photo tampering came to light in July 2008. Sepah News, the media arm of Iran’s Revolutionary Guard, celebrated the country’s military prowess by releasing a photo showing the simultaneous launch of four missiles. But only three of those rockets actually left the ground, a fourth was digitally added. The truth emerged after Sepah circulated the original photo showing three missiles in flight—but not before the faked image appeared on the front pages of the Chicago Tribune, the Financial Times, and the Los Angeles Times. Figure 1. A July 2008 photo shows four Iranian missiles streaking skyward. The right is the true image Sepah News replaced the faux photo with the original without explanation. Over the past few years, the field of digital-image forensics has emerged to challenge this growing problem and return some level of trust in photographs. Nearly every digital forgery starts out as a photo taken by a digital camera. The camera’s image sensor acts as the film. By using computer methods to look at the underlying patterns of pixels that make up a digital image, specialists can detect the often-subtle signatures of manipulated images that are invisible to the naked eye. Traditionally, watermarking is added into the images or video to give the validating information for image authentication. However, the watermarking can be easily destroyed in the process of image compression. Recently, digital blind techniq	3d projection;academy;acronis true image;authentication;autostereogram;camera matrix;camera resectioning;categorization;clutter;color filter array;compositing;computer science;computer vision;digital camera;digital electronics;digital image;digital imaging;digital video;digital watermarking;distortion;entity;entity–relationship model;farid f. abraham;financial times;fixed point (mathematics);fundamental matrix (computer vision);graphics software;holographic principle;homography (computer vision);homology (biology);human height;ieee transactions on information forensics and security;image compression;image editing;image noise;image plane;image processing;image rectification;image sensor;interpolation;iranian.com;jpeg;key;local interconnect network;lossy compression;pixel;rectifier;reference distance;region of interest;sensitivity and specificity;simplicial homology;the legend of zelda: skyward sword;tracing (software);type signature;vanishing point;video post-processing;virtual reality headset;wang and landau algorithm;watermark (data file);yao graph	Lin Wu;Yang Wang	2010	CoRR		computer vision;computer science;multimedia;computer graphics (images)	Vision	63.0703174486502	-57.64937474645976	152496
c6e29be24b3e1a0c05f01d93a668e69ca53aac65	specific chromatic errors: a quality assessment	quality mean opinion score color assessment;pattern clustering;image colour analysis;image color analysis multimedia communication hyperspectral imaging standards visualization quality assessment;pattern clustering hyperspectral imaging image colour analysis;color changes specific chromatic errors quality assessment hyperspectral images colors cie 1976 color space d65 standard illuminant k means algorithm color clusters chromatic coordinates mean opinion scores;hyperspectral imaging	A study on the perceived quality of images displayed with color changes is presented. Initially, two hyperspectral images colors were represented in the CIE 1976 (L*a*b*) color space under the D65 standard illuminant. The colors of each image were divided into four clusters with the application of the K-Means algorithm. A new set of images was created changing only one of the four color clusters. The color change results from the application of a predefined chromatic error to the chromatic coordinates (a*, b*) of all pixels in the cluster. Errors of 6, 9, 12 and 15 ΔE*ab units were applied, using two different directions for each color cluster. These images were displayed for individuals visualization, that were asked to rank their quality based on their naturalness. The Mean Opinion Scores was computed and allowed to test and quantify the sensitivity to specific color changes.	a* search algorithm;color space;computability in europe;k-means clustering;pixel;scientific visualization	Marco V. Bernardo;António M. G. Pinheiro;Manuela Pereira;Paulo Torrão Fiadeiro	2013	2013 Fifth International Workshop on Quality of Multimedia Experience (QoMEX)	10.1109/QoMEX.2013.6603192	color histogram;false color;computer vision;icc profile;color model;color quantization;color depth;color image;hyperspectral imaging;color difference;mathematics;color balance;color space;computer graphics (images)	Vision	62.25763281502855	-63.1908839086485	152498
ac92355aeff250e6469679321a5467e37f2f97dd	tampered image reconstruction with global scene adaptive in-painting		The objective of in-painting is to reconstruct the mislaid region of an image. This paper presents a new in-painting algorithm from the goodwill of Exemplar-based Greedy algorithms, which consist of two phases: making a decision of filling-in order and selection of good exemplars for the damaged regions. The proposed method overcomes these tribulations with the protection of edges, textures and also with lesser propagation error. This scheme upgrades the filling-in order that is based on the combination of priority terms, to encourage the early synthesis of linear structures. The subsequent contribution helps sinking the error propagation to an improved detection of outliers from the candidate patches. The proposed methodology is well suited in terms of both natural and artificial images with plausible output. This scheme dramatically outperforms earlier works in terms of both perceptual quality and computational efficiency.	iterative reconstruction	Ravi Subban;Muthukumar Subramanian;Pasupathi Perumalsamy;R. Seejamol;S. Gayathri Devi;Sankar Selvakumar	2014		10.1007/978-3-319-04960-1_24	inpainting;iterative reconstruction;goodwill;outlier;texture synthesis;propagation of uncertainty;greedy algorithm;artificial intelligence;pattern recognition;computer science	Vision	56.67105925758564	-59.931895835929765	152681
25ad17ab9886510dcf1d8d1e4a156b44a5b39222	am/fm halftoning: a method for digital halftoning through simultaneous modulation of dot size and dot placement	printing;moire patterns;halftones;frequency modulation;relative density;resistance;amplitude modulation;pulse width modulated;frequency modulated;algorithms;amplitude modulated;diffusion;high spatial resolution;spatial resolution;modulation	Conventionally, digital halftoning is accomplished by either changing the size of printed dots or changing the relative density of dots on the page. These two approaches are analogous to amplitude modulation (AM) or frequency modulation (FM) used in communications. A typical AM halftoning method, such as cluster dot screening, has very low computational requirements and good print stability. However, it suffers from low spatial resolution and Moiré artifacts. Alternatively, popular FM halftoning methods, such as error diffusion, can achieve high spatial resolution and are free of Moiré artifacts but lack the print stability required for electro-photographic printing. In this paper, we present a new class of halftoning algorithms that simultaneously modulate both the size and density of printed dots. We call this new class of algorithms AM/FM halftoning. The major advantages of AM/FM halftoning are: • Better stability in shadow area than dispersed dot methods through the formation of larger dot clusters. • Better Moiré resistance than clustered dot screens through irregular dot placement. • The ability to systematically optimize dot size and density to produce the best possible print quality at each gray level. A specific implementation of AM/FM halftoning is developed for use with electro-photographic printers having pulse width modulation (PWM) technology. We present results using dot size and dot density curves obtained through measurement-based optimization, and demonstrate that AM/FM halftoning achieves high spatial resolution, smooth halftone textures, good printing stability, and Moiré resistances.	am broadcasting;algorithm;display resolution;dots per inch;error diffusion;fm broadcasting;grayscale;iterative method;mathematical optimization;printing;pulse-width modulation;requirement	Zhen He;Charles A. Bouman	2002		10.1117/12.453002	electronic engineering;telecommunications;computer science;optics;error diffusion	Graphics	60.26684554830273	-60.21586504922504	152772
65096a825701b7fa966767f7d76725fd956c539c	high dynamic range scene realization using two complementary images	tone reproduction;tone mapping;local color mapping;high dynamic range;local tone mapping	Many existing tone reproduction schemes are based on the use of a single high dynamic range (HDR) image and are therefore unable to accurately recover the local details and colors of the scene due to the limited information available. Accordingly, the current study develops a novel tone reproduction system which utilizes two images with different exposures to capture both the local details and color information of the low- and high-luminance regions of a scene. By computing the local region of each pixel, whose radius is determined via an iterative morphological erosion process, the proposed system implements a pixel-wise local tone mapping module which compresses the luminance range and enhances the local contrast in the low-exposure image. And a local color mapping module is applied to capture the precise color information from the high-exposure image. Subsequently, a fusion process is then performed to fuse the local tone mapping and color mapping results to generate highly realistic reproductions of HDR scenes.		Ming-Chian Sung;Te-Hsun Wang;Jenn-Jier James Lien	2007		10.1007/978-3-540-76386-4_24	computer vision;tone mapping;computer science;computer graphics (images)	Vision	58.143650748628374	-61.253410774310275	152821
8606035ad0b8ad50b4d46c3ceb2766ee14a4eceb	fuzzy comprehensive evaluation method and its application in subjective quality assessment for compressed remote sensing images	chinese astronautic scientists;remote sensing image;image coding;data compression;quality assessment image coding remote sensing satellites image reconstruction transform coding image quality standards development signal to noise ratio signal resolution;spiht algorithm;jpeg2000 standard;evaluation method;compressed remote sensing images;image classification;fuzzy set theory;subjective quality assessment;quality assessment;geophysical signal processing;image reconstruction;remote sensing;remote sensing data compression fuzzy set theory geophysical signal processing image classification image coding image reconstruction;fuzzy comprehensive evaluation method;subjective assessment;chinese astronautic scientists fuzzy comprehensive evaluation method subjective quality assessment compressed remote sensing images image classification image reconstruction jpeg2000 standard spiht algorithm	Subjective assessment methods for compressed remote sensing images and their classification are introduced. A fuzzy comprehensive evaluation method is studied and applied in assessment for reconstructed remote sensing images which are compressed by JPEG2000 standard and improved SPIHT algorithm developed by Chinese astronautic scientists. Fuzzy comprehensive evaluation method is proved to be reliable through conducting a series of experiments and some useful conclusions are obtained.	aerial photography;algorithm;data compression;elegant degradation;experiment;image compression;image quality;image texture;jpeg 2000;set partitioning in hierarchical trees	Liang Zhai;Xinming Tang	2007	Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)	10.1109/FSKD.2007.321	data compression;iterative reconstruction;computer vision;contextual image classification;computer science;data mining;fuzzy set	Robotics	65.97352939486149	-64.99066173078374	152882
5643d286de46240097b28dcc455779a6ede70c98	a psychovisual threshold for generating quantization process in tchebichef moment image compression	q science general;tmt image compression;qa75 electronic computers computer science;tmt quantization;psychovisual threshold	A human visual system can hardly respond to small differences in image signals. A full colour image carries a certain amount of perceptual redundancy for the human eyes. The sensitivity human eye of the color image can be measured by a psychovisual threshold. The sensitivity of the human eye is useful for perceptual visual image in image compression. The quantization tables are obtained to determine psychovisual threshold that can be perceived visually significant by the human eye. This paper introduces the concept of psychovisual threshold into Tchebichef moment image compression. This paper will investigate the contribution of each moment coefficient to the image reconstruction. The error threshold from the contribution of its moments in image reconstruction will be the primitive of psychovisual threshold to an image. This paper presents a new technique to generate quantization table for an optimal TMT image compression based on psychovisual error threshold. The experimental results show that these new finer quantization tables provide a statistically better image quality output at lower average bit length of Huffman's code than previously proposed TMT quantization.	bit-length;coefficient;color image;data redundancy;huffman coding;human visual system model;image compression;image quality;iterative reconstruction;quantization (signal processing)	Ferda Ernawan;Nur Azman Abu;Nanna Suryana	2014	JCP	10.4304/jcp.9.3.702-710	computer vision;computer science;theoretical computer science;computer graphics (images)	Vision	61.975938828119354	-62.813952335744816	153009
e2d27eee64aaba305d6febef409c9b4015205e5b	fuzzy controlled fractal interpolation method for image resolution enhancement	image resolution		fractal compression;image resolution;interpolation	Nobuhito Matsushiro;Yoshiyuki Nakajima	1998			interpolation;nearest-neighbor interpolation;demosaicing;multivariate interpolation;stairstep interpolation;image scaling;mathematical optimization;computer vision;bilinear interpolation;bicubic interpolation;mathematics;artificial intelligence	Vision	58.38704216385215	-64.68850102161392	153164
97c264d120b4ccf2b954d89c94838b84e3affcb1	stereo image coding based on binocular energy modeling	visual presentation;image coding;binocular energy;stereoscopic coding;stereoscopic coder;image matching;image fusion;disparity;complex cells function;stereoscopic imaging technology;wavelet transforms cameras image coding image fusion image matching image sequences stereo image processing visual perception;complex wavelet transform;wavelet transforms;quality of experience;stereo image coding;visualization;retinal image fusion;computational modeling;images matching;human visual system;retina;stereo image processing;coding;next generation;mathematical model;stereo;mathematical function;visual perception;binocular energy model;bandelet transform stereo image coding stereoscopic imaging technology visual presentation cameras stereoscopic coder images matching binocular energy model complex cells function retinal image fusion visual cortex mathematical function complex wavelet transform;retinal imaging;bandelet transform;visual cortex;cameras;complex cell;human visual system stereoscopic coding bandelet transform binocular energy;image coding visualization retina mathematical model computational modeling continuous wavelet transforms;continuous wavelet transforms;image sequences	Stereoscopic imaging technologies are seen as the next generation of visual presentation, improving the quality of experience of the viewer. It uses two different sequences acquired from two regular cameras or from a regular camera with an additional specific depth camera. This means that the size of data is at least doubled. Thus, the coding process becomes very crucial. In this framework, we propose a stereoscopic coder based on visual properties. The matching of two images is computed by a binocular energy model based on the simple and complex cells functions allowing the fusion of both retinal images in the visual cortex. Mathematical functions were used to reproduce the behavior of these cells particularly complex wavelet transform (CWT) and bandelet transform. Our coder output is a disparity map, a residual image and the reference image. The innovative part of this work lies in a matching technique based on the binocular energy. The results are presented in comparative curves with one of the most known coder in literature.	bandelet (computer science);binocular disparity;binocular vision;complex wavelet transform;energy modeling;imaging technology;next-generation network;stereoscopy	Rafik Bensalma;Mohamed-Chaker Larabi	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5649232	binocular disparity;computer vision;visualization;visual perception;computer science;mathematical model;mathematics;coding;image fusion;human visual system model;computational model;stereophonic sound;function;statistics;wavelet transform;computer graphics (images)	Vision	64.61338532537897	-62.03140139605465	153730
193c1cf9e4736689f6e3cbec08baed5cc1090ea0	color gamut mapping and the printing of digital color images	three dimensional;reproductive system;digital image;color image	Principles and techniques useful for calibrated color reproduction are defined. These results are derived from a project to take digital images designed on a variety of different color monitors and accurately reproduce them in a journal using digital offset printing. Most of the images printed were reproduced without access to the image as viewed in its original form; the color specification was derived entirely from calorimetric specification. The techniques described here are not specific to offset printing and can be applied equally well to other digital color devices. The reproduction system described is calibrated using CIE tristimulus values. An image is represented as a set of three-dimensional points, and the color output device as a three-dimensional solid surrounding the set of all reproducible colors for that device, called its gamut. The shapes of the monitor and the printer gamuts are very different, so it is necessary to transform the image points to fit into the destination gamut, a process we call gamut mapping. This paper describes the principles that control gamut mapping. Included also are some details on monitor and printer calibration, and a brief description of how digital halftone screens for offset printing are prepared.	calibration (statistics);color management;digital image;output device;printer (computing);printing	Maureen C. Stone;William B. Cowan;John C. Beatty	1988	ACM Trans. Graph.	10.1145/46165.48045	color histogram;three-dimensional space;false color;rgb color model;computer vision;icc profile;color model;hsl and hsv;color depth;color image;computer science;rgb color space;high color;reproductive system;color balance;optics;color space;digital image;computer graphics (images)	Graphics	61.86684887058372	-53.64706922235242	153821
7a6a81449f03b2005b22ca27d1295c51a3792f80	anti-correlation digital halftoning by generalized russian roulette		A new class of digital halftoning algorithms is introduce Anti-correlation digital halftoning (ACDH) combines th idea of a well-known game, Russian roulette, with the s tistical approach to bilevel quantization of images. A re resentative of the class, serpentine anti-correlation d tal halftoning, is described and compared to error dif sion, ordered dither, and other important digital halfton algorithms. Digital halftoning means image quantizati by algorithms that exploit properties of the vision syste to create the illusion of continuous tone. Common pro lems often accompanying digital halftoning include co touring, correlated artifacts, edge enhancement, and pleasant boundary effects. Serpentine ACDH causes fe unwanted correlated artifacts and less contouring than benchmark algorithms. Two intensity distortion criter similar to those applied earlier to evaluate quality of no uniform sampling are used to demonstrate that serpen ACDH represents the average intensity remarkably w Unlike popular algorithms based on error diffusion, s pentine ACDH does not enhance edges. A simple pre cessing technique allows one to introduce edge enha ment if desired, while keeping it more isotropic than th of error diffusion. Serpentine ACDH does not cause s nificant boundary effects.	algorithm;benchmark (computing);distortion;dither;edge enhancement;error diffusion;field electron emission;ordered dithering;sampling (signal processing);sion's minimax theorem	Dmitri A. Gusev	1999			statistics;roulette;correlation;computer science	Graphics	59.64242027005142	-61.38598084270138	154013
f54df7967c2a61401e57ce5f18f08dc23f328685	computational time-lapse video	time lapse;non linear filtering;non uniform sampling;aliasing;dynamic program;summarization;camera simulation;computational photography;video;sampling methods	We present methods for generating novel time-lapse videos that address the inherent sampling issues that arise with traditional photographic techniques. Starting with video-rate footage as input, our post-process downsamples the source material into a time-lapse video and provides user controls for retaining, removing, and resampling events. We employ two techniques for selecting and combining source frames to form the output. First, we present a non-uniform sampling method, based on dynamic programming, which optimizes the sampling of the input video to match the user's desired duration and visual objectives. We present multiple error metrics for this optimization, each resulting in different sampling characteristics. To complement the non-uniform sampling, we present the  virtual shutter , a non-linear filtering technique that synthetically extends the exposure time of time-lapse frames.	computation	Eric P. Bennett;Leonard McMillan	2007	ACM Trans. Graph.	10.1145/1276377.1276505	aliasing;sampling;computer vision;computational photography;simulation;video;computer science;computer graphics (images)	Graphics	63.517932920692466	-53.808830891062044	154073
5abaaf9c222398caea40d62e45b81a847436b784	an adaptive gamma correction for image enhancement	signal image and speech processing;biometrics;pattern recognition;image processing and computer vision	Due to the limitations of image-capturing devices or the presence of a non-ideal environment, the quality of digital images may get degraded. In spite of much advancement in imaging science, captured images do not always fulfill users’ expectations of clear and soothing views. Most of the existing methods mainly focus on either global or local enhancement that might not be suitable for all types of images. These methods do not consider the nature of the image, whereas different types of degraded images may demand different types of treatments. Hence, we classify images into several classes based on the statistical information of the respective images. Afterwards, an adaptive gamma correction (AGC) is proposed to appropriately enhance the contrast of the image where the parameters of AGC are set dynamically based on the image information. Extensive experiments along with qualitative and quantitative evaluations show that the performance of AGC is better than other state-of-the-art techniques.	automatic gain control;digital image;experiment;gamma correction;image editing;norm (social)	Shanto Rahman;Mostafijur Rahman;Mohammad Abdullah-Al-Wadud;Golam Dastegir Al-Quaderi;Mohammad Shoyaib	2016	EURASIP J. Image and Video Processing	10.1186/s13640-016-0138-1	computer vision;image analysis;speech recognition;image processing;computer science;archaeology;digital image processing;pattern recognition;multimedia;biometrics	Vision	59.31059994269694	-62.804311462769434	154274
6ca066ce6579e747c002a61ae8fa7c00ce3cf442	performance characterization of fundamental matrix estimation under image degradation	performance measure;lossy compression;fundamental matrix;performance characterization;epipolar geometry;camera motion	The fundamental matrix represents the epipolar geometry between two images. We describe an algorithm for simultaneously estimating the fundamental matrix and corresponding points automatically from the two images. The performance of this algorithm is then assessed as the images are degraded by JPEG lossy compression. A number of performance measures are proposed and evaluated over image pairs corresponding to different camera motions and scene types.	algorithm;elegant degradation;epipolar geometry;fundamental matrix (computer vision);jpeg;lossy compression	Philip H. S. Torr;Andrew Zisserman	1997	Machine Vision and Applications	10.1007/s001380050051	lossy compression;triangulation;computer vision;eight-point algorithm;computer science;mathematics;geometry;fundamental matrix;essential matrix;epipolar geometry	Vision	55.18670379556562	-54.42337990816537	154399
68eee6c1e76eb22e3cd382409caae2f58e94b0dc	perceptual rasterization for head-mounted display image synthesis		t=0 t=0 t=0 t=0 t=1 t=1 t=1 t=1 t=0 t=.3 t=.6 t=1 Fig. 1. Perceptual rasterization is a generalization of classic rasterization to the requirements of HMDs such as foveation (top row) and rolling image formation (boom row). On a HMD, most pixels appear in the periphery (a). We rasterize images with continuously-varying pixel density (b). A zoom of the the foveated area shows how a common same-shading-eort image has aliasing (c), while our result benefits from higher pixel density, resulting in super-sampling (d). In common rasterization, each pixel on the display is eectively sampled at the same simulation time (t = 0 for the first frame (e) and t = 1 for the next frame (f)). When displayed on a “rolling” HMD display, where pixels are illuminated at dierent points in time, latency is introduced: the rightmost pixel is outdated by ca. 16ms. Our rolling rasterization (g) allows spatially-varying time: starting at t = 0 on the le of the image and increasing to 1 on the right.	aliasing;circa;computable function;distortion;emoticon;head-mounted display;image formation;overhead (computing);pixel density;rasterisation;requirement;sampling (signal processing);shading;simulation;specular highlight;supersampling	Tobias Ritschel;Sebastian Friston;Anthony Steed	2018	CoRR		rendering (computer graphics);computer vision;artificial intelligence;computer science;latency (engineering);pixel;image formation;latency (engineering);image warping;shading;pixel density	Graphics	62.95489634823944	-54.523673962860784	154571
9bbc3113e8f94cbadd073186323cde178333ceb9	a dominant-noise discrimination system for images corrupted by content-independent noises without a priori references	structural similarity index dominant noise discrimination system content independent noise dominant noise identification system additive white gaussian noise awgn random impulse noise intensity component impulse noise detector directional edge gaussian noise intensity detector lookup table;gaussian noise;additive white gaussian noise;blind iqm;impulse noise;awgn;noise type determination;noise type determination blind iqm;random noise;detectors awgn image edge detection noise measurement detection algorithms;table lookup awgn image denoising impulse noise random noise;indexation;success rate;a priori information;lookup table;image denoising;table lookup;structural similarity	A dominant-noise identification system used on images corrupted by content-independent noises without a priori information has been developed. As representative examples of content-independent noises additive white Gaussian noise (AWGN) and random impulse noise (RIN) are used. The system uses two noise detectors to estimate the intensity component of either AWGN or RIN in an image. For this purpose an impulse noise detector was developed based on directional edges [1] and a Gaussian noise intensity detector developed in [2] was implemented. Then in the second phase, lookup tables that are generated beforehand using the structural similarity index (SSIM) proposed in [3] are used to convert the detected noise intensities onto a common scale to compare the distortion caused by different noise types and identify which one is dominant. Simulations were conducted on over 300 images and results show that for images corrupted with moderate noise intensities the system's overall success rate for identifying the dominant noise is 7 out of 10 times.	additive white gaussian noise;computer simulation;dependability;distortion;image noise;image quality;impulse noise (audio);lookup table;relative intensity noise;selectivity (electronic);sensor;structural similarity;utility functions on indivisible goods	Robert Grou-Szabo;Tadashi Shibata	2011	2011 5th International Conference on Signal Processing and Communication Systems (ICSPCS)	10.1109/ICSPCS.2011.6140861	gradient noise;gaussian noise;image noise;electronic engineering;speech recognition;colors of noise;dark-frame subtraction;value noise;noise measurement;mathematics;noise floor;noise;statistics;salt-and-pepper noise	Robotics	54.944061863186256	-62.73879183403774	154604
aeb53c4b66dd6aac0f5cb7d459c22e89d2e7618b	nonlinear diffusion at your fingertips: theory and mobile applications	diffusion;mobile computing	Through affordable smartphones and lower costs for mobile internet access, mobile communication by moving and still pictures has become a commonplace. However, while most phones offer a camera for taking pictures, the image processing capabilities are relatively sparse. The challenges that arise with real-time image handling are not yet countered by complex techniques. Because of the limited computation power, simple algorithms were used. In this paper, we will show that the technique of nonlinear diffusion can be used in (near) real-time on most smartphone's GPUs if being implemented in an OpenGL ES shader. The presented implementation is then used for several image-related tasks, namely denoising, deblocking and upscaling directly on the phone. As the performance evaluation shows, anisotropic diffusion offers a nice framework for complex mobile image handling in several arising problem areas.	algorithm;anisotropic diffusion;computation;deblocking filter;graphics processing unit;image processing;internet access;mobile app;noise reduction;nonlinear system;opengl es;performance evaluation;real-time clock;real-time locating system;shader;smartphone;sparse matrix	Daniel Thuerck;Arjan Kuijper	2013	2013 8th International Symposium on Image and Signal Processing and Analysis (ISPA)		computer vision;computer science;multimedia;computer graphics (images)	Arch	66.66095901959079	-53.81408590072338	154616
083f308e06817e83c01c9b52a7950c8530e32508	panorama completion for street views	image completion panorama street views structure rectifying warp;qa76 computer software	This paper considers panorama images used for street views. Their viewing angle of 360° causes pixels at the top and bottom to appear stretched and warped. Although current image completion algorithms work well, they cannot be directly used in the presence of such distortions found in panoramas of street views. We thus propose a novel approach to complete such 360° panoramas using optimization-based projection to deal with distortions. Experimental results show that our approach is efficient and provides an improvement over standard image completion algorithms.	algorithm;distortion;mathematical optimization;pixel;rectifier;viewing angle	Zhe Zhu;Ralph R. Martin;Shi-Min Hu	2015	Computational Visual Media	10.1007/s41095-015-0008-2	computer vision;computer science;computer graphics (images)	Vision	58.40112409365849	-54.37752043311663	154856
3d4d2c54d3da30ac076f77f2831d84c229b8c8ed	lime: a method for low-light image enhancement	illumination light transmission;low light image enhancement;illumination estimation	When one captures images in low-light conditions, the images often suffer from low visibility. This poor quality may significantly degrade the performance of many computer vision and multimedia algorithms that are primarily designed for high-quality inputs. In this paper, we propose a very simple and effective method, named as LIME, to enhance low-light images. More concretely, the illumination of each pixel is first estimated individually by finding the maximum value in R, G and B channels. Further, we refine the initial illumination map by imposing a structure prior on it, as the final illumination map. Having the well-constructed illumination map, the enhancement can be achieved accordingly. Experiments on a number of challenging real-world low-light images are present to reveal the efficacy of our LIME and show its superiority over several state-of-the-arts.	algorithm;computer vision;effective method;experiment;illumination (image);image editing;pixel;smoothing;whole earth 'lectronic link;lime	Xiaojie Guo	2016		10.1145/2964284.2967188	computer vision;computer graphics (images)	AI	57.7409011416224	-60.84110992476624	154926
51b884a34e754013d8760e3bf287759c12134ef6	spectral gradient sampling for path tracing		Spectral Monte-Carlo methods are currently the most powerful techniques for simulating light transport with wavelengthdependent phenomena (e.g., dispersion, colored particle scattering, or diffraction gratings). Compared to trichromatic rendering, sampling the spectral domain requires significantly more samples for noise-free images. Inspired by gradient-domain rendering, which estimates image gradients, we propose spectral gradient sampling to estimate the gradients of the spectral distribution inside a pixel. These gradients can be sampled with a significantly lower variance by carefully correlating the path samples of a pixel in the spectral domain, and we introduce a mapping function that shifts paths with wavelength-dependent interactions. We compute the result of each pixel by integrating the estimated gradients over the spectral domain using a onedimensional screened Poisson reconstruction. Our method improves convergence and reduces chromatic noise from spectral sampling, as demonstrated by our implementation within a conventional path tracer. CCS Concepts •Computing methodologies → Ray tracing;	color;computer graphics;eurographics;image gradient;interaction;john d. wiley;metropolis light transport;monte carlo method;noise reduction;overhead (computing);path tracing;pixel;projection screen;ray tracing (graphics);redshift;sampling (signal processing);simulation	Victor Petitjean;Pablo Bauszat;Elmar Eisemann	2018	Comput. Graph. Forum	10.1111/cgf.13474	theoretical computer science;computer science;computer vision;artificial intelligence;sampling (statistics);path tracing	Graphics	64.2560652995784	-53.31374010408789	155304
7990ce851b5223ad2ace4d797ed82b2b75d8e0e1	an uneven illumination correction algorithm for optical remote sensing images covered with thin clouds	thin clouds;wavelet analysis;hsv transform;remote sensing images;uneven illumination	The uneven illumination phenomenon caused by thin clouds will reduce the quality of remote sensing images, and bring adverse effects to the image interpretation. To remove the effect of thin clouds on images, an uneven illumination correction can be applied. In this paper, an effective uneven illumination correction algorithm is proposed to remove the effect of thin clouds and to restore the ground information of the optical remote sensing image. The imaging model of remote sensing images covered by thin clouds is analyzed. Due to the transmission attenuation, reflection, and scattering, the thin cloud cover usually increases region brightness and reduces saturation and contrast of the image. As a result, a wavelet domain enhancement is performed for the image in Hue-Saturation-Value (HSV) color space. We use images with thin clouds in Wuhan area captured by QuickBird and ZiYuan-3 (ZY-3) satellites for experiments. Three traditional uneven illumination correction algorithms, i.e., multi-scale Retinex (MSR) algorithm, homomorphic filtering (HF)-based algorithm, and wavelet transform-based MASK (WT-MASK) algorithm are performed for comparison. Five indicators, i.e., mean value, standard deviation, information entropy, OPEN ACCESS Remote Sens. 2015, 7 11849 average gradient, and hue deviation index (HDI) are used to analyze the effect of the algorithms. The experimental results show that the proposed algorithm can effectively eliminate the influences of thin clouds and restore the real color of ground objects under thin clouds.	algorithm;color depth;color space;entropy (information theory);experiment;gradient;homomorphic filtering;tag cloud;thin client;wavelet transform	Xiaole Shen;Qingquan Li;Yingjie Tian;Linlin Shen	2015	Remote Sensing	10.3390/rs70911848	wavelet;computer vision;optics;statistics;remote sensing	Vision	66.30520708545211	-65.29663604695999	155329
96bf2c80058ac807aa2697e6719fa330fc0d12fa	superpixel-based disocclusion filling in depth image based rendering	superpixel segmentation superpixel based spatiotemporal disocclusion filling algorithm depth image based rendering video plus depth sequence video stream;video signal processing image segmentation rendering computer graphics;image color analysis image segmentation image edge detection filling rendering computer graphics interpolation extrapolation	We present a new super pixel-based spatio-temporal disocclusion filling algorithm for view synthesis in Depth Image Based Rendering. When rendering new views from a single video-plus-depth sequence, background regions get exposed in the virtual view that are occluded in the video stream. To fill these regions we propose to use texture information that is found both in the current and in temporally neighboring frames. Instead of using a patch-based search for texture to fill the holes, we propose a new super pixel-based filling method that exploits the property of super pixels being more meaningful sub-entities of an image than simple square patches. We present experiments with both real and synthetic sequences to evaluate our proposed method.	algorithm;align (company);entity;experiment;flicker (screen);image texture;optic axis of a crystal;pixel;ringing artifacts;stacking;streaming media;synthetic intelligence;view synthesis	Michael Schmeing;Xiaoyi Jiang	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.194	image texture;computer vision;tiled rendering;image-based modeling and rendering;3d rendering;rendering;computer science;multimedia;real-time rendering;alternate frame rendering;video post-processing;image-based lighting;computer graphics (images)	Vision	59.012772306976046	-54.42244083167242	155799
6eeb48c395a94a5aa3200f9383434ad07ebd3692	research and design of real-time human body video acquisition system	mathematical morphology;image motion analysis;video streaming;median filter method mixed reality motion model human body video acquisition system mrmm vfw image processing unit video stream frame difference pixel information parameter extraction quality mathematical morphology binary image virtual picture frame virtual video;real time;biological system modeling;virtual reality;image synthesis;motion capture;virtual reality streaming media solid modeling image color analysis biological system modeling real time systems computers;streaming media;image color analysis;image colour analysis;feature extraction;human body;solid modeling;realtime mixed reality motion capture image synthesis;mixed reality;video streaming feature extraction image colour analysis image motion analysis mathematical morphology;real time systems	The paper presents a Mixed Reality Motion Model (MRMM) based on the definition of Mixed Reality. Then a human body video acquisition system based on MRMM is carried out. Firstly, real-time human body video acquisition is realized by using VFW. The image processing unit processes the video stream frame-by-frame. It turns the current frame to binary image by Frames Difference; the parameters are extracted according to pixels' information and the characteristics of human body. Secondly, in order to identify the target region directly and improve the extraction quality, mathematical morphology is introduced in dealing with binary image. So the binary information is corrected to be more accurate. In the basis of the binary image, body image could be combined with the virtual picture frame by frame to form a new virtual video, including real person image and virtual background. Finally, median filter method is used to smooth the output video frame. The main function is to reduce the lays of frames. The experiment results are given to illustrate the algorithm effective and real-time.	algorithm;binary image;entry point;image processing;mathematical morphology;median filter;mixed reality;pixel;real-time clock;real-time computing;real-time locating system;rendering (computer graphics);salt-and-pepper noise;software testing;streaming media;velocity (software development);video for windows	Xiang Lin;Ying Li;Nannan Dong;Ning Li	2011	2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2011.6019813	video compression picture types;reference frame;residual frame;computer vision;motion capture;human body;mathematical morphology;image processing;feature extraction;computer science;video tracking;virtual reality;block-matching algorithm;mixed reality;multimedia;solid modeling;motion compensation;computer graphics (images)	Vision	57.37503797008023	-54.66146366927019	155950
0a74909f09117613bdfc591e43d371ccd14c2184	high - quality image interpolation based on multiplicative skeleton - texture separation	interpolation skeleton image edge detection generators image reconstruction image resolution additives;statistical analysis edge detection image restoration image texture interpolation;statistical resampling interpolation algorithm image interpolation multiplicative skeleton texture separation edge sharpness texture intensity picture quality skeleton image texture generator residual image interpolation method bounded variation function cartoon approximation super resolution deblurring oversampling method ringing artifacts standard linear interpolation algorithm reconstruction errors	This paper presents a high-quality interpolation approach that can adjust edge sharpness and texture intensity to reconstruct an image according to user's taste in picture quality. Our interpolation approach first resolves an input image I into its skeleton image U and its texture generator V and its residual image D such that I = U·V + D, and then interpolates each of the three components independently with a proper interpolation method suitable to each. The skeleton image is a bounded-variation function meaning a cartoon approximation of I, and interpolated with a super-resolution deblurring-oversampling method that interpolates sharp edges without producing ringing artifacts. The texture generator is an oscillatory function representing regular distinct textures, and interpolated with a standard linear interpolation algorithm. The residual image is a function representing irregular weak textures and reconstruction errors, and interpolated with a statistical re-sampling interpolation algorithm.	algorithm;approximation;bounded variation;computer simulation;deblurring;image quality;linear interpolation;oversampling;ringing artifacts;sampling (signal processing);super-resolution imaging	Takahiro Saito;Yuki Ishii;Yousuke Nakagawa;Takashi Komatsu	2006	2006 14th European Signal Processing Conference		spline interpolation;demosaicing;image texture;computer vision;mathematical optimization;bilinear interpolation;stairstep interpolation;bicubic interpolation;mathematics;geometry;nearest-neighbor interpolation;multivariate interpolation;trilinear interpolation;image scaling	Vision	58.424083244704384	-65.37796276632967	155999
46aa793c4ac5e309d1ea21a315cbbcb74333962c	haze removal method for natural restoration of images with sky		Abstract Most haze removal methods fail to restore long-shot images naturally, especially for the sky region. To solve this problem, we proposed a Fusion of Luminance and Dark Channel Prior (F-LDCP) method to effectively restore long-shot images with sky. The transmission values estimated based on a luminance model and dark channel prior model are fused together based on a soft segmentation. The transmission estimated from the luminance model mainly contributes to the sky region, while that from the dark channel prior for the foreground region. The airlight also is adjusted to adapt to real light by sky region detection. A user study and objective assessment comparison with a variety of methods on long-shot haze images demonstrate that our method retains visual truth and removes the haze effectively.	circuit restoration	Yingying Zhu;Gaoyang Tang;Xiaoyan Zhang;Jianmin Jiang;Qi Tian	2018	Neurocomputing	10.1016/j.neucom.2017.08.055	artificial intelligence;haze;mathematics;pattern recognition;sky;luminance;communication channel	Vision	57.927974419165615	-60.937322486738616	156062
e6a5871fdca1d2b3ea913a8db1065f61b68fcb38	an artificial bee colony algorithm for image contrast enhancement	contrast enhancement;artificial bee colony;image enhancement;grey level mapping;local global transformation	Image Enhancement is a crucial phase in almost every image processing system. It aims at improving both the visual and the informational quality of distorted images. Histogram Equalization (HE) techniques are the most popular approaches for image enhancement for they succeed in enhancing the image and preserving its main characteristics. However, using exhaustive approaches for histogram equalisation is an algorithmically complex task. These HE techniques also fail in offering good enhancement if not so good parameters are chosen. So, new intelligent approaches, using Artificial Intelligence techniques, have been proposed for image enhancement. In this context, this paper proposes a new Artificial Bee Colony (ABC) algorithm for image contrast enhancement. A grey-level mapping technique and a new image quality measure are used. The algorithm has been tested on some test images, and the comparisons of the obtained results with the genetic algorithm have proven its superiority. Moreover, the proposed algorithm has been extended to colour image enhancement and given very promising results. Further qualitative and statistical comparisons of the proposed ABC to the Cuckoo Search (CS) algorithm are also presented in the paper; not only for the adopted grey-level mapping technique, but also with using another common transformation, generally called the local/global transformation.	artificial bee colony algorithm	Amer Draa;Amira Bouaziz	2014	Swarm and Evolutionary Computation	10.1016/j.swevo.2014.01.003	computer vision;engineering;artificial intelligence;machine learning	AI	56.35527736402416	-64.84467737419283	156110
c37087712bb602b0f2844cfd93c90c4f65f69750	edge strength filter based color filter array interpolation	silicon;interpolation;edge detection;cpsnr color filter array interpolation digital camera beam splitter image data capture cfa interpolation image demosaicing algorithm subjective interpolation quality objective interpolation quality orientation free edge strength filter initial green channel interpolation color difference rule;demosaicing;orientation free edge filter;digital camera;channel estimation;arrays;color filter array cfa interpolation;image edge detection;image color analysis;image colour analysis;edge directed interpolation;color filter array;pixel;algorithms color colorimetry filtration image enhancement image interpretation computer assisted lighting reproducibility of results sensitivity and specificity;optical beam splitters;orientation free edge filter color filter array cfa interpolation demosaicing edge directed interpolation;optical beam splitters cameras filtering theory image colour analysis interpolation;filtering theory;cameras;pixel image color analysis interpolation image edge detection channel estimation arrays silicon;direct method	For economic reasons, most digital cameras use color filter arrays instead of beam splitters to capture image data. As a result of this, only one of the required three color samples becomes available at each pixel location and the other two need to be interpolated. This process is called Color Filter Array (CFA) interpolation or demosaicing. Many demosaicing algorithms have been introduced over the years to improve subjective and objective interpolation quality. We propose an orientation-free edge strength filter and apply it to the demosaicing problem. Edge strength filter output is utilized both to improve the initial green channel interpolation and to apply the constant color difference rule adaptively. This simple edge directed method yields visually pleasing results with high CPSNR.	algorithm;channel (digital image);color filter array;demosaicing;digital camera;image processing;interpolation imputation technique;pixel	Ibrahim Pekkucuksen;Yücel Altunbasak	2012	IEEE Transactions on Image Processing	10.1109/TIP.2011.2155073	direct method;demosaicing;computer vision;color filter array;edge detection;interpolation;bayer filter;computer science;root-raised-cosine filter;mathematics;silicon;pixel;computer graphics (images)	Vision	59.43284060237487	-59.65182354474159	156186
d97e986e2dd1363e30211bbb96736637c482a5c7	optimal estimation of spectral reflectance based on metamerism	reflectivity;color difference;reflectance estimation;metamerism;optimal spectral reflectance;multispectral imaging	In this paper, we proposed an accurate estimation method for spectral reflectance of objects captured in an image. The spectral reflectance is simply modeled by a linear combination of three basic spectrums of R, G, and B colors respectively, named as spectral reflective bases of objects, which are acquired by solving a linear system based on the principle of color metamerism. Some experiments were performed to evaluate the accuracy of the estimated spectral reflectance of objects. The average mean square error of 24 colors in Macbeth checker between we simulated and the measured is 0.0866, and the maximum is 0.310. In addition, the average color difference of the 24 colors is less than 1.5 under the D65 illuminant. There are 13 colors having their color difference values less than 1, and other 8 colors having the values during the range of 1 and 2. Only three colors are relatively larger, with the differences of 2.558, 4.130 and 2.569, from the colors of No. 2, No. 13, and No. 18 in Macbeth checker respectively. Furthermore, the computational cost of this spectral estimation is very low and suitable for many practical applications in real time.© (2012) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.		Tzren-Ru Chou;Wei-Ju Lin	2012		10.1117/12.907606	multispectral image;metamerism;computer vision;color difference;reflectivity;color balance;optics;remote sensing	Vision	60.44275610889095	-59.91609586051576	156238
c30a86f545987bf42e26bfd9414e263dde726d80	visualization of arbitrary-shaped 3d scenes on depth-limited 3d displays	data visualisation;three-dimensional displays;visual communication;arbitrary-shaped 3d scenes;data visualization;depth scaling method;depth-limited 3d display;dynamic dimension display system	We propose a depth scaling method that enables visualization of arbitrary-shaped 3D scenes on 3D displays. Most current 3D displays have a depth limitation, while the scene to be displayed has not. The trivial solutions as clipping or linear scaling of the scene's 3D bounding box suffer from nonoptimal utilization of the display's capabilities. Our approach uses spatially adaptive depth scaling that maximizes the perceptual 3D effect. From the original scene geometry, the topology and local depth ordering among objects are preserved, while depth linearity is disregarded. The scaling method applies to nearly all 3D displays, such as glasses-based, head-tracked, multiview, holographic and volumetric 3D displays. Subjective tests with the dynamic dimension display system show that our method significantly increases the perceptual 3D effect.	clipping (computer graphics);holography;image scaling;minimum bounding box;stereo display	André Reder	2004	Proceedings. 2nd International Symposium on 3D Data Processing, Visualization and Transmission, 2004. 3DPVT 2004.	10.1109/TDPVT.2004.1335416	computer vision;computer science;multimedia;computer graphics (images)	Visualization	61.475711300318984	-55.31009235507595	156728
4728e84d40e397b5816d397b42adbba4be7fd79d	usability report 1. internationalization i18n		A camera of this invention includes a photographing optical system, an imaging means, an optical path splitting means, a lens shutter, a focus detecting means, and a correcting means. The imaging means senses an optical image, formed by a light beam passing through the photographing optical system, on a predetermined photographing plane. The optical path splitting means splits at least a portion of a light beam passing through the photographing optical system. The lens shutter opens and closes the optical path of the photographing optical system. The focus detecting means receives the split light beam and detects the defocus amount of an image formation plane of the light beam with respect to a predetermined primary image formation plane. The correcting means corrects any deviation between an image formation plane of the photographing light beam imaged by the imaging means and the primary image formation plane.	internationalization and localization;usability	Jose Ramon Bobes-Bascaran;Mónica Vázquez-Goyarzu;Martin Gonzalez-Rodriguez;Carlos Tejo-Alonso	2008	e-Minds		shutter;software engineering;image formation;usability;business;light beam;optical path;optics	Logic	60.635400235572455	-53.44318406912419	156775
9ad77e839612dd675787ceea378d08153deff495	evaluating perceptually prefiltered video	objective quality metric perceptual prefiltering video image enhancement video coding human observer behavior subjective quality evaluation;human observer behavior;quality metric;perceived quality;video image enhancement;quality improvement;objective quality metric;video coding;visual perception filtering theory image enhancement video coding;image enhancement;quality evaluation;visual perception;perceptual prefiltering;bit rate humans monitoring video compression image coding multimedia communication testing codecs video signal processing performance evaluation;lts1;subjective evaluation;filtering theory;subjective quality evaluation	Perceptual prefiltering is the process of enhancing relevant portions of an image or of a video, and of simplifying contextual information in order to improve the perceived quality or the compression ratio. In this paper, we discuss the results of subjective quality evaluation experiments performed to assess the impact of perceptual prefiltering on video coding and we propose an objective quality metric that mimics the behavior of human observers. The predicted performance of the proposed metric is consistent with the subjective evaluation scores. Experimental results demonstrate that perceptual prefiltering leads to quality improvements by up to 10% at low bit rates	data compression;experiment	Olivier Steiger;Touradj Ebrahimi;Andrea Cavallaro	2005	2005 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2005.1521665	subjective video quality;computer vision;quality management;simulation;visual perception;computer science;video quality;multimedia	Robotics	62.99044878662	-63.690413050981476	156998
2b6dea6120e850089853e4b054ad719d4d9c5a88	understanding the impact of inter-lens and temporal stereoscopic video compression	stereoscopic video compression;steroscopic imaging	As we move toward more ubiquitous stereoscopic video, particularly with multiple (> 2) lenses, the need to understand the efficiency of compression will become increasingly important. In this paper, we explore the impact of spatial (between lenses) and temporal (over time) compression for stereoscopic video images. In particular, because stereoscopic images are taken at the same time, there is expected to be a high correlation between pixels in the horizontal direction due to the fixed nature of the multiple lenses. We propose a vertically reduced search window in order to take advantage of this correlation. Starting with multiple stereoscopic video sequences shot using a production studio 3D camera, we explore the effectiveness of temporal and inter-lens motion compensation for stereoscopic video. Furthermore, the experiments use exhaustive search to remove the effects of heuristic-based motion-compensation techniques.	brute-force search;data compression;experiment;heuristic;motion compensation;pixel;stereoscopy	Wu-chi Feng;Feng Liu	2012		10.1145/2229087.2229114	video compression picture types;computer vision;computer science;video tracking;multimedia;motion compensation;multiview video coding;computer graphics (images)	Vision	64.44731745442492	-62.70481700632837	157149
89976e30bb9c7d438e438abc0a0bcbce63f3e2b9	a quaternion-based switching filter for colour image denoising	switching filter;quaternion;non local means;colour image	An improved quaternion switching filter for colour image denoising is presented. It proposes a RGB colour image as a pure quaternion form and measures differences between two colour pixels with the quaternion-based distance. Further, in noisedetection, a two-stage detection method is proposed to determine whether the current pixel is noise or not. The noisy pixels are replaced by the vector median filter (VMF) output and the noise-free ones are unchanged. Finally, we combine the advantages of quaternion-based switching filter and non-local means filter to remove mixture noise. By comparing the performance and computing time processing different images, the proposed method has superior performance which not only provides the best noise suppression results but also yields better image quality compared to other widely used filters. & 2014 Elsevier B.V. All rights reserved.	biometrics;color image;image noise;image quality;median filter;netware loadable module;noise reduction;non-local means;pattern recognition;pixel;zero suppression	Gaihua Wang;Yang Liu;Tongzhou Zhao	2014	Signal Processing	10.1016/j.sigpro.2014.03.027	computer vision;electronic engineering;control theory;mathematics;non-local means;quaternion	Vision	56.8160756677423	-65.93054829997611	157216
345c5681f425adbf1cee5a080c4426a5fe0f5e5f	view synthesis for 3d video scene composition	image segmentation;image resolution;tv image resolution image segmentation;video signal processing image sequences;icp algorithm 3d video scene composition camera motion estimation view synthesis technique refined backward warping technique;tv	Scene composition is a method widely used in movie and TV production. Merging two sets of 3D videos into one is a very challenging task. There are several main issues on video composition. Our focus is compositing two sets of videos with different camera motion parameters. The key techniques are the camera motion estimation and view synthesis technique used to produce the synthesized motion-compensated background video. We propose a refined backward warping technique for view synthesis and adopt the ICP algorithm to calculate the camera motion parameters.	algorithm;compositing;depth map;iteration;motion estimation;scene graph;view synthesis	Amanda Wang;Chun-Liang Chien;Hsueh-Ming Hang	2015	2015 IEEE 17th International Workshop on Multimedia Signal Processing (MMSP)	10.1109/MMSP.2015.7340824	computer vision;image resolution;computer science;motion estimation;multimedia;image segmentation;computer graphics (images)	Vision	57.329736453034656	-54.78869968664738	157433
d7afd6b9dc1b150a2d6427c10e18220e2befcece	a model for calculating the potential iso speeds of digital still cameras based upon ccd characteristics.			charge-coupled device	Richard L. Baer;Jack M. Holm	1999			computer vision;simulation;computer graphics (images)	HCI	61.85246086305961	-57.22452059304997	157550
1f664621f4e3a958594d42d204e48e55aab7176b	photographic composite detection using two circles	cameras noise noise level forgery parallel processing robustness authentication;image processing;authentication;forgery;photography;focal length estimation photographic composite detection two circle image forgery technology digital photos;photographic process;noise level;photography image processing photographic process;robustness;shuyi zhu xiaochun cao handong zhao 复合检测 摄影 摄像机标定 法向量 复合材料 几何约束 单圈 共面 photographic composite detection using circles;parallel processing;cameras;noise	Image forgery technology has become popular for tempering with digital photos. In this work, we propose a new technique for detecting the photographic composites using two or more equal and coplanar circles. In particular, we describe how to (i) estimate the focal length based on the equality of the sizes of two coplanar circles, (ii) estimate the normal vector of the world circle plane on the basis of the estimated focal length. Inconsistencies in the angles among the normal vectors (each circle determines a normal vector) are used as evidence of tampering. To demonstrate the efficiency of the approach, we test our method on both synthetic and visually plausible composite images.	composite pattern;error analysis (mathematics);experiment;focal (programming language);normal (geometry);sensor;simulation;synthetic intelligence	Shuyi Zhu;Xiaochun Cao;Handong Zhao	2011	2011 Sixth International Conference on Image and Graphics	10.1109/ICIG.2011.144	parallel processing;computer vision;image processing;computer science;noise;photography;authentication;robustness;computer graphics (images)	Vision	55.01946259390086	-60.7705377983548	157753
33baf1f9ac475928565a34f256218e64cfa709bf	an evaluation of digital image forgery detection approaches.		With the headway of the advanced image handling software and altering tools, a computerized picture can be effectively controlled. The identification of image manipulation is vital in light of the fact that an image can be utilized as legitimate confirmation, in crime scene investigation, and in numerous different fields. The image forgery detection techniques intend to confirm the credibility of computerized pictures with no prior information about the original image. There are numerous routes for altering a picture, for example, resampling, splicing, and copymove. In this paper, we have examined different type of image forgery and their detection techniques; mainly we focused on pixel based image forgery detection techniques.	algorithm;authentication;digital image;pixel;resampling (statistics);time complexity	Abhishek Kashyap;Rajesh Singh Parmar;Megha Agarwal;Hariom Gupta	2017	CoRR		theoretical computer science;digital image;crime scene;headway;computer vision;pixel;artificial intelligence;computer science;software;credibility	Vision	63.76361744330158	-58.2397887974367	157840
14eeeea2665b66768a365934939df06b2ec967f2	noise bias compensation of tone mapped noisy image	estimation noise measurement psnr image quality probability density function;image;image filter hdr mapping noise;hdr;filter;mapping;high dynamic range image tone mapped noisy image denoising method zero mean image processing noise bias estimation tone mapping function noise bias compensation noisy pixel values biased noise variance hdr images nonlocal mean filter;noise;image filtering image denoising	This paper proposes a de-noising method especially for a biased noise. Even though a noise is assumed to have zero mean, an image processing such as tone mapping alters this property. Firstly, a precise estimation of the noise bias caused by a tone mapping function is derived considering discontinuity of a general form of the tone mapping. Secondly, a method of compensating the noise bias is introduced investigating estimation from observed noisy pixel values. Thirdly, the derived estimation is applied not only to compensate the noise bias, but also to reduce variance of the biased noise. Finally, significance of the proposed method is evaluated using HDR images in combination with a non-local mean filter.	high-dynamic-range imaging;image processing;mean squared error;pixel;reflections of signals on conducting lines;signal-to-noise ratio;tone mapping	Masahiro Iwahashi;Hitoshi Kiya	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025537	gradient noise;gaussian noise;median filter;image restoration;image noise;computer vision;speech recognition;colors of noise;dark-frame subtraction;value noise;filter;noise measurement;noise;image;noise;mathematics;non-local means;salt-and-pepper noise	Robotics	57.98153555066647	-65.50062255557808	157848
e754ec51c107d1af32cc704146ac9afeaf3c64bf	an accelerated superpixel generation algorithm based on 4-labeled-neighbors		Superpixels are perceptually meaningful atomic regions that could effectively improve efficiency of subsequent image processing tasks. Simple linear iterative clustering (SLIC) has been widely used for superpixel calculation due to outstanding performance. In this paper, we propose an accelerated SLIC superpixel generation algorithm using 4-labeled neighbor pixels called 4L-SLIC. The main idea of 4L-SLIC is that the labels are assigned to a portion of the pixels while the others that associated with certain cluster are restrained by adjacent four labeled pixels. In this way, the average number of distance calculated times of pixels are effectively reduced and the similarity between adjacent pixels ensures a better segmentation effect. The experimental results confirm that 4L-SLIC achieved a speed up of 25%–30% without declining accuracy sharply compared to SLIC. In contrast to the method published on CVIU 2016, 4L-SLIC has an acceptable increase in the cost of time, in the mean time, there is a significant ascension to the accuracy of the segmentation.	algorithm	Hongwei Feng;Fang Xiao;Qirong Bu;Feihong Liu;Lei Cui;Jun Feng	2017		10.1007/978-981-10-7299-4_45	acceleration;pixel;image processing;speedup;cluster analysis;algorithm;mathematics	EDA	54.08009665504841	-63.878566862235836	157947
6fc114d93333dc0e81e83deedb90418f9e8b11bf	color image enhancement with preservation of gamut range		Color image enhancement plays a key role in digital image processing more than grayscale image enhancement due to its wide color expressions with three perceptual attributes including hue, saturation and intensity of an image. It has been a popular research since many recent years but it is still an open issue in order to enhance the color for low intensity, blurred or incomplete images, etc. This paper proposes a color enhancement algorithm with an affine algorithm for enhancing illumination in gamut preserving color image enhancement. The proposed method firstly uses non-linear sigmoidal function for luminance enhancement while an improved histogram specification model is used to preserve gamut of an input image. The proposed algorithm is experimented with low intensity images obtained from standard image datasets which are compared with quite popular image enhancement methods. According to the experimental results, the proposed method produces less error rate than other models.	algorithm;color image;digital image processing;grayscale;histogram matching;image editing;nonlinear system;sigmoid function	Myint Myint Maw;Renu	2018	2018 IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS)	10.1109/ICIS.2018.8466453	digital image processing;digital image;grayscale;computer vision;color image;affine transformation;hue;gamut;artificial intelligence;luminance;mathematics	Vision	58.83780738400763	-62.3232823679302	158057
e6e2504e666e30660a2198393b0886a2150d93d5	image enhancement with blurred and noisy image pairs using a dual edge-preserving filtering technique	digital cameras;image enhancement	Taking a satisfactory image using a hand-held camera under dim lighting conditions is difficult because there is a tradeoff between exposure time and motion blur. If the image is taken with a short exposure time, the image is noisy, although it preserves sharp details. On the other hand, an image taken with a long exposure time suffers from motion blur but has good intensity. In this paper, we propose a two-image approach that adaptively combines a short-exposure image and a long-exposure image for the image stabilization function on digital cameras. Our method combines only the desirable properties of the two images to obtain a less-blurry and less-noisy image. Furthermore, the proposed approach uses a dual edge-preserving filtering technique for the edge areas of the long-exposure image, which are more affected by motion blur. © (2014) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.	edge enhancement;image editing	Yuushi Toyoda;Hiroyasu Yoshikawa;Masayoshi Shimizu	2014		10.1117/12.2037561	image quality;image warping;image texture;image restoration;computer vision;feature detection;image resolution;image gradient;binary image;image processing;gaussian blur;digital image processing;image subtraction;clipping;multimedia;motion field;top-hat transform;physics;image-based lighting;computer graphics (images)	Vision	58.17927867904641	-61.18391601304075	158271
9e5d9ad7071896b0756794fbc59c57d6b0c5749a	low-light auto-focus enhancement for digital and cell-phone camera image pipelines	digital camera;digital cameras pipelines focusing image enhancement image quality lenses light emitting diodes guidelines layout optoelectronic and photonic sensors;cell phone cameras;indexing terms;cell phone camera;low light auto focus;auto focus system;image enhancement;digital still cameras;mobile handsets cameras image enhancement;sharpness function quality;sharpness function;image enhancement preprocessing;mobile handsets;low light auto focus enhancement;low light;camera image pipelines;quality measures;peak in focus position;low light auto focus enhancement image enhancement preprocessing sharpness function peak in focus position auto focus system camera image pipelines cell phone camera digital camera;cameras	Images captured by a digital or cell-phone camera in low-light environments usually suffer from a lack of sharpness due to the failure of the camera's passive auto-focus (AF) system to locate the peak in-focus position of a sharpness function that is extracted from the image. In low-light, the sharpness function becomes flat, making it quite difficult to locate the peak.In this paper, a systematic approach is introduced to address the problem of low-light AF by performing computationally simple image enhancement preprocessing steps as part of the image pipeline. These enhancement steps elevate the sharpness function peak, leading to auto-focusing in low-light conditions. A sharpness junction quality measure along with experimental guidelines are presented for determining the most prominent enhancement steps for low-light AF. The implementation results on an actual digital camera platform are also shown to demonstrate the effectiveness of our solution.	anisotropic filtering;camera phone;digital camera;digital data;experiment;image editing;mobile phone;preprocessor;software deployment	Mark Gamadia;Nasser Kehtarnavaz;Katie Roberts-Hoffman	2007	IEEE Transactions on Consumer Electronics	10.1109/TCE.2007.381682	computer vision;electronic engineering;index term;computer science;engineering;camera phone;computer graphics (images)	Vision	59.762903835675665	-58.189840867824536	158425
1851d2a7713b969206cb196f6ca6e88f838c580b	a perceptual quality metric for 3d triangle meshes based on spatial pooling	mesh visual quality assessment;spatial pooling;statistical descriptors;support vector regression;visual masking	In computer graphics, various processing operations are applied to 3D triangle meshes and these processes often involve distortions, which affect the visual quality of surface geometry. In this context, perceptual quality assessment of 3D triangle meshes has become a crucial issue. In this paper, we propose a new objective quality metric for assessing the visual difference between a reference mesh and a corresponding distorted mesh. Our analysis indicates that the overall quality of a distorted mesh is sensitive to the distortion distribution. The proposed metric is based on a spatial pooling strategy and statistical descriptors of the distortion distribution. We generate a perceptual distortion map for vertices in the reference mesh while taking into account the visual masking effect of the human visual system. The proposed metric extracts statistical descriptors from the distortion map as the feature vector to represent the overall mesh quality. With the feature vector as input, we adopt a support vector regression model to predict the mesh quality score.We validate the performance of our method with three publicly available databases, and the comparison with state-of-the-art metrics demonstrates the superiority of our method. Experimental results show that our proposed method achieves a high correlation between objective assessment and subjective scores.	computer graphics;database;distortion;feature vector;statistical machine translation;support vector machine;triangle mesh	Xiang Feng;Wanggen Wan;Richard Y. D. Xu;Haoyu Chen;Pengfei Li;J. Alfredo Sánchez	2017	Frontiers of Computer Science	10.1007/s11704-017-6328-x	artificial intelligence;support vector machine;computer graphics;pooling;perceptual distortion;pattern recognition;distortion;polygon mesh;computer science;feature vector;human visual system model	Graphics	62.58275489415013	-64.44169379324961	158496
40387b08320fc483ae6e3f23bf83edacb7affde9	sports video retargeting	video retargeting;sports video;domain based analysis;content adaptation;aspect ratio	With the proliferation of diverse multimedia terminals, the request for elegantly retargeting videos to different display devices is evident, especially in sports. This demonstration presents a Sports Video Retargeting(SVR) technique, that utilized domain based structure parsing to build a semantic importance map for video retargeting. The system enables flexible and coherent aspect-ratio change of the output sports videos with a spatial-temporal 3D rectilinear grid framework, which are free from significant loss of information or distortion on salient and important regions. Results in various sports type have shown that SVR is promising for content adaptation on mobile media.	coherence (physics);content adaptation;distortion;mobile media;parsing;regular grid;retargeting	Liang Shi;Jinqiao Wang;Ling-yu Duan;Hanqing Lu	2009		10.1145/1631272.1631478	computer vision;aspect ratio;computer science;operating system;multimedia;computer graphics (images)	Vision	59.92505898696381	-55.68017648679011	158552
82ae3b46037d878030b78220a291658890a523cb	researches on image processing based on neural network technique for furnace frame	fast median filter;process burning coal powder;image noise elimination;filtering;furnaces;image segmentation;image processing;median filter;flames;neural nets;image processing neural networks furnaces fires monitoring powders boilers combustion signal processing filters;complex suspension combustion;furnace flame monitoring system;charge coupled devices;noise signals;artificial neural networks;monitoring system;furnace flame image segmentation neural network;special boundary detection algorithm image processing neural network technique furnace flame process burning coal powder complex suspension combustion noise signals furnace flame monitoring system fast median filter image noise elimination;pixel;object detection coal flames furnaces image denoising median filters neural nets;coal;special boundary detection algorithm;image denoising;furnace flame;boundary detection;fires;neural network technique;object detection;median filters;noise;neural network	Process burning coal powder in boiler is in very complex suspension combustion and very unstable, monitoring system image includes a lot of noise signals from difference hands. In this chapter, compose of furnace flame monitoring system is introduced, the new method called as fast median filter for elimination image noise and special boundary detection algorithm based on neural network technique for furnace flame are discussed in detail, processing for flame image is completed.	algorithm;artificial neural network;control theory;image noise;image processing;median filter;system image	Chenggang Zhen;Wei Liao	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.906	filter;median filter;flame;coal;computer science;noise;machine learning;image segmentation;artificial neural network;pixel	Robotics	55.801731335124586	-65.08675282507905	158876
bd60eeb6f89b82cc478dc227f7d11705954d2cea	seamless image stitching in the gradient domain	cost function;panoramic image	Image stitching is used to combine several individual images having some overlap into a composite image. The quality of image stitching is measured by the similarity of the stitched image to each of the input images, and by the visibility of the seam between the stitched images. In order to define and get the best possible stitching, we introduce several formal cost functions for the evaluation of the quality of stitching. In these cost functions, the similarity to the input images and the visibility of the seam are defined in the gradient domain, minimizing the disturbing edges along the seam. A good image stitching will optimize these cost functions, overcoming both photometric inconsistencies and geometric misalignments between the stitched images. This approach is demonstrated in the generation of panoramic images and in object blending. Comparisons with existing methods show the benefits of optimizing the measures in the gradient domain.	alpha compositing;gradient;image stitching;seamless3d	Anat Levin;Assaf Zomet;Shmuel Peleg;Yair Weiss	2004		10.1007/978-3-540-24673-2_31	computer vision;image stitching;computer science;loss function;computer graphics (images)	Vision	57.901519916876936	-56.10235032462533	159062
df9ae62dc43b3aa0dd54402d73ded6340bf66320	mirrorless interchangeable-lens light field digital photography camera system	photographic lenses;digital photography;cameras computer architecture photography mirrors focusing image generation;conventional digital dslr architecture mirrorless interchangeable lens light field digital photography camera system image information computational photography technique image quality designed system architecture scenario digital single lens reflex single integrated system;photographic lenses cameras digital photography;cameras	Camera system is expected to be evolved merging computational photography technique for having rich image information while maintaining image quality. The proposed camera system shows a newly designed system architecture scenario to have both high quality image performance and computational photography technique by combining digital single lens reflex (DSLR) based architecture and light field concept into a single integrated system. It starts from conventional digital DSLR architecture and enhances system features and reduces the problems with previous techniques.	computational photography;digital photography;digital single-lens reflex camera;display resolution;image quality;light field;music download;systems architecture;virtual camera system	Byung-Joon Baek;Hyeong Koo Lee;Youngjin Kim;TaeChan Kim	2013	2013 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2013.6486869	computer vision;digital photography;digital versus film photography;computational photography;angle of view;view camera;digital imaging;computer graphics (images)	Robotics	60.404907117547545	-55.329589633500156	159220
30d41404261ef34def92cb591dc81fe1ce3027cd	a method of printer modeling with the spectral reflectance curves using error back propagation	error back propagation;neural nets;printers;backpropagation;printers reflectivity color nonlinear distortion neural networks ink jet printing wavelength measurement charge coupled devices page description languages computer errors;backpropagation printers photoreflectance neural nets;spectral reflectance;color correction;cmy values printer modeling spectral reflectance curves error back propagation error backpropagation color distortions nonlinear component characteristics color correction techniques ink jet printer spectral reflectance xyz values l a b value;photoreflectance;point of view;color image;neural network	Most color imaging devices exhibit color distortions due to nonlinear component characteristics. Among color correction techniques, the most popular is device characterization. Generally there are two parts in printer characterization using a neural network, i.e. printer modeling and printer controller. This paper presents a method of printer modeling with spectral reflectance values using error back propagation. In experiments, we used an ink jet printer. First we print many color patches using known CMY values, and measured these patches with spectral reflectance values at wavelengths. The data set of the measured values and the input values were split into the validation set for assessing the performance of the trained network and the training sets. For training we took CMY values for the input nodes and reflectance values for the target values of the output nodes. The input layer of the network receives input as a CMY vector. The output represents spectral reflectance at certain wavelengths. To get a better performance, we change the network scheme. For the evaluation, we transformed spectral reflectance values to XYZ values. With this model, we can make a printer controller, the reverse of a printer model. Printer controller takes XYZ values for input and CMY for output. If the printer prints with controller CMY, we have the same L*a*b* between printed color patches and input value of the printer controller. From a global point of view, this total system represents a mapping from a target L*a*b* value to the output L*a*b* value of the printer.	backpropagation;printer (computing);software propagation	Hong-Kee Kim;Byoung-Ho Kang;Gyu-Seo Han;Jin-Seo Kim;Chang-Rak Yoon;Maeng-Sub Cho	1998		10.1109/ICSMC.1998.728155	computer vision;computer science;backpropagation;machine learning;artificial neural network	Vision	65.54601989482414	-60.02716613855172	159322
e7e5d94c008c6e43ff9d0f8a5f145c620aea55fe	archive film restoration based on spatiotemporal random walks	archive film restoration;motion information;novel restoration method;multiscale framework;previous scale;defective pixel;degraded pixel;spatiotemporal random walk;normal pixel;film restoration;real degraded image sequence;previous restoration work;random walk	We propose a novel restoration method for defects and missing regions in video sequences, particularly in application to archive film restoration. Our statistical framework is based on random walks to examine the spatiotemporal path of a degraded pixel, and uses texture features in addition to intensity and motion information traditionally used in previous restoration works. The degraded pixels within a frame are restored in a multiscale framework by updating their features (intensity, motion and texture) at each level with reference to the attributes of normal pixels and other defective pixels in the previous scale as long as they fall within the defective pixel’s random walk-based spatiotemporal neighbourhood. The proposed algorithm is compared against two state-of-the-art methods to demonstrate improved accuracy in restoring synthetic and real degraded image sequences.	algorithm;archive;circuit restoration;computation;defective pixel;software bug;synthetic intelligence	Xiaosong Wang;Majid Mirmehdi	2010		10.1007/978-3-642-15555-0_35	computer vision;computer graphics (images)	Vision	55.348046646901885	-58.82571590233188	159332
ab3d24e9b44c27f2e979c11a82814f4de54a3352	smartcolor: real-time color and contrast correction for optical see-through head-mounted displays	optical distortion;multimedia information systems artificial;color image color analysis multimedia communication real time systems adaptive optics optical distortion virtual reality user interfaces;user interfaces information interfaces and presentation multimedia information systems;multimedia information systems;computer graphics;color;virtual reality;software libraries colour computer graphics helmet mounted displays optical distortion;style guides;image processing computer assisted;ohmd optical see through head mounted displays real time color correction real time contrast correction color blending problem distortions transparent display material graphics library smartcolor show up on contrast;augmented;distortion;accuracy;evaluation methodology;image color analysis;multimedia communication;screen design;user interfaces ergonomics;and virtual realities;human engineering;algorithms;humans;information interfaces and presentation;user interfaces;adaptive optics;real time systems	Users of optical see-through head-mounted displays (OHMD) perceive color as a blend of the display color and the background. Color-blending is a major usability challenge as it leads to loss of color encodings and poor text legibility. Color correction aims at mitigating color blending by producing an alternative color which, when blended with the background, more closely approximates the color originally intended. In this paper we present an end-to-end approach to the color blending problem addressing the distortions introduced by the transparent material of the display efficiently and in realtime. We also present a user evaluation of correction efficiency. Finally, we present a graphics library called SmartColor showcasing the use of color correction for different types of display content. SmartColor uses color correction to provide three management strategies: correction, contrast, and show-up-on-contrast. Correction determines the alternate color which best preserves the original color. Contrast determines the color which best supports text legibility while preserving as much of the original hue. Show-up-on-contrast makes a component visible when a related component does not have enough contrast to be legible. We describe SmartColor's architecture and illustrate the color strategies for various types of display content.	3d computer graphics;algorithm;alpha compositing;augmented reality;binocular vision;case preservation;color space;computability in europe;countercurrent electrophoresis measurement;deploy;distortion;end-to-end principle;graphics library;hl7publishingsubsection <operations>;head and neck neoplasms;head-mounted display;linear function;morphologic artifacts;parallax;physical object;real-time clock;real-time computing;real-time transcription;retinal cone;ringing artifacts;sampling (signal processing);shader;software deployment;unique identifier;usability;user interface device component;user interface design;vertex	Juan David Hincapié-Ramos;Levko Ivanchuk;Srikanth Kirshnamachari Sridharan;Pourang Irani	2015	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2015.2450745	computer vision;color quantization;color normalization;color depth;distortion;computer science;accuracy and precision;virtual reality;multimedia;color balance;color space;computer graphics;user interface;adaptive optics;statistics;computer graphics (images)	Visualization	61.169675763081564	-61.056585203038665	159533
387086aad462fece116008be39d608f8d5b64ab0	does the concentration principle hold for photographic latent image formation?	image formation		image formation;latent image	Vitali V. Gavrik	1999				Vision	63.733339326213525	-56.0271258220217	159675
988fff95cc7675d3fce72f616f217eb4ed80de57	efficiency of dct-based denoising techniques applied to texture images		Textures or high-detailed structures contain information that can be exploited in pattern recognition and classification. If an acquired image is noisy, noise removal becomes an operation to improve image quality before further stages of processing. Among possible variants of denoising, we consider filters based on orthogonal transforms, in particular, on discrete cosine transform (DCT) known to be able to effectively remove additive white Gaussian noise (AWGN). Besides, we study a representative of nonlocal denoising techniques, namely, BM3D known as state-of-the-art technique based on DCT and similar patch search. We show that noise removal in texture images using the considered DCT-based techniques can distort fine texture details. To detect such situations and avoid texture degradation due to filtering, we propose to apply filtering efficiency prediction tests applicable to wide class of images. These tests are based on DCT coefficient statistic parameters and can be used for decision-making in relation to the use of the considered filters.	discrete cosine transform;noise reduction	Aleksey Rubel;Vladimir V. Lukin;Oleksiy B. Pogrebnyak	2014		10.1007/978-3-319-07491-7_27	texture filtering	Vision	57.156468661372934	-65.80702499876566	159795
964b951d83a176866ac1ffed6d8c0312958694e1	anomalous pixel replacement and spectral quality algorithm for longwave infrared hyperspectral imagery		Mercury cadmium telluride focal plane arrays for longwave infrared hyperspectral imagery are prone to dynamic anomalous pixels, which may result in degraded image quality and exploitation performance, including target detection and identification. Standard bad pixel replacement algorithms based on first and second order moments are too computationally inefficient for real-time processing. Here, we develop a computationally efficient spectral mixture replacement (SMR) algorithm using pre-defined universal subspaces for simultaneously identifying and replacing anomalous pixels. Furthermore, to quantify SMR performance, we derive a novel longwave spectral image quality algorithm by combining physics based modeling with Bayesian statistics. Our results reveal that an overall enhancement in spectral image quality of approximately 3% is achieved (averaged over ∼880 hyperspectral images). A significant fraction of the latter enhancement results from offset drift correction, indicating that accounting for temporal drift in detector nonuniformity plays a significant role in replacing bad pixels.	algorithm;algorithmic efficiency;defective pixel;image quality;mercury;real-time clock;shingled magnetic recording;staring array	Blake M. Rankin;Joshua B. Broadwater;Milton Smith	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8517461	pixel;infrared;computer vision;artificial intelligence;object detection;image quality;offset (computer science);detector;hyperspectral imaging;algorithm;mercury cadmium telluride;computer science	Vision	66.93367935934509	-62.482312103566855	159845
b042281980c2fae03921e90c728cbedc955acd24	a distortion-correction scheme for industrial machine-vision applications	vision system;distortion measurement digital images digital cameras machine vision satellites control systems polynomials robot vision systems production facilities lenses;vision ordenador;image numerique;correction;ease of use;experimental result;computer vision;corrections;distortion;locally affine correction scheme distortion correction scheme industrial machine vision industrial vision systems camera lens frame grabber combinations solid state cameras;machine vision;distorsion;imagen numerica;resultado experimental;correccion;factory automation;vision ordinateur;factory automation computer vision;digital image;resultat experimental	Digital images produced by industrial-grade cameras are subject to several kinds of spatial distortion. Machine vision systems on the factory floor possess characteristics that make it difficult to implement distortion-correction schemes that have been developed in other fields, such as satellite imagery. This paper discusses practical ways to measure and correct distortion in industrial vision systems, with an emphasis on ease of use. Study results show that substantial increases in accuracy can be gained.	distortion;machine vision;usability	David A. Butler;Patricia K. Pierson	1991	IEEE Trans. Robotics and Automation	10.1109/70.86085	computer vision;simulation;distortion;machine vision;computer science;mathematics;computer graphics (images)	Robotics	61.92886247077531	-57.47818456414419	160008
1e2c48815ee29d05b205f80fdef678d707cd3fcf	image deblurring via extreme channels prior		Camera motion introduces motion blur, affecting many computer vision tasks. Dark Channel Prior (DCP) helps the blind deblurring on scenes including natural, face, text, and low-illumination images. However, it has limitations and is less likely to support the kernel estimation while bright pixels dominate the input image. We observe that the bright pixels in the clear images are not likely to be bright after the blur process. Based on this observation, we first illustrate this phenomenon mathematically and define it as the Bright Channel Prior (BCP). Then, we propose a technique for deblurring such images which elevates the performance of existing motion deblurring algorithms. The proposed method takes advantage of both Bright and Dark Channel Prior. This joint prior is named as extreme channels prior and is crucial for achieving efficient restorations by leveraging both the bright and dark information. Extensive experimental results demonstrate that the proposed method is more robust and performs favorably against the state-of-the-art image deblurring methods on both synthesized and natural images.	academy;algorithm;bulk copy program;computer vision;deblurring;gaussian blur;ieee 1284;illumination (image);pixel;plaintext	Yanyang Yan;Wenqi Ren;Yuanfang Guo;Rui Wang;Xiaochun Cao	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.738	artificial intelligence;kernel (linear algebra);computer vision;pixel;image restoration;pattern recognition;motion blur;kernel density estimation;computer science;deblurring;communication channel;phenomenon	Vision	56.92422998001878	-58.371686231551095	160090
a1d0f767ccb767dc853856d68aa3809ba1dcaf64	interactive visualization of medical volume models in mobile devices	visualization on small displays;medical visualization;interaction on mobile devices;article	Interactive visualization of volume models in standard mobile devices is a challenging present problem with increasing interest from new application fields like telemedicine. The complexity of present volume models in medical applications is continuously increasing, therefore increasing the gap between the available models and the rendering capabilities in low-end mobile clients. New and efficient rendering algorithms and interaction paradigms are required for these small platforms. In this paper, we propose a transfer function-aware compression and interaction scheme, for client-server architectures with visualization on standard mobile devices. The scheme is block-based, supporting adaptive ray-casting in the client. Our two-level ray-casting allows focusing on small details on targeted regions while keeping bounded memory requirements in the GPU of the client. Our approach includes a transfer function-aware compression scheme based on a local wavelet transformation, together with a bricking scheme that supports interactive inspection and levels of detail in the mobile device client. We also use a quantization technique that takes into account a perceptive metrics of the visual error. Our results show that we can have full interaction with high compression rates and with transmitted model sizes that can be of the order of a single photographic image.	3d film;algorithm;brick (electronics);central processing unit;client (computing);client–server model;computation;data compression;gradient;graphics processing unit;haar wavelet;interactive visualization;level of detail;mobile device;on the fly;phong shading;point in polygon;preprocessor;quantization (signal processing);ray casting;region of interest;requirement;server (computing);transfer function;wavelet transform	Lázaro Campoalegre;Pere Brunet;Isabel Navazo	2012	Personal and Ubiquitous Computing	10.1007/s00779-012-0596-0	simulation;human–computer interaction;computer science;operating system;multimedia;computer graphics (images)	Visualization	67.48116373212396	-53.23862773295029	160135
990cb44d5aa7c9f1546531ccee83b4439d173af5	an efficient directional interpolated algorithm for video deinterlacing	interpolation;deinterlacing;interlaced	In this paper, we propose an efficient deinterlacing algorithm for the interpolation of interlaced images for digital display systems like LCD modules. Our method efficiently estimates the directional spatial correlations (horizontal, vertical, and diagonal directions) of neighboring pixels. The experiments on a variety of images and video sequences demonstrate that our proposed algorithm can accomplish better quantitative and visual quality than the FOI, ELA, and the low-complexity interpolation algorithm.	algorithm;deinterlacing;interpolation	Houng-Kuo Ku;Ting-Wei Hou	2009	IEICE Electronic Express	10.1587/elex.6.211	computer vision;electronic engineering;interpolation;computer science;deinterlacing;mathematics;computer graphics (images)	HCI	58.63172670981361	-59.01843954408469	160166
abf75b81b96ff2cd262086675deba55f907108db	benchmarking single-image dehazing and beyond		We present a comprehensive study and evaluation of existing single-image dehazing algorithms, using a new large-scale benchmark consisting of both synthetic and real-world hazy images, called REalistic Single-Image DEhazing (RESIDE). RESIDE highlights diverse data sources and image contents, and is divided into five subsets, each serving different training or evaluation purposes. We further provide a rich variety of criteria for dehazing algorithm evaluation, ranging from full-reference metrics to no-reference metrics and to subjective evaluation, and the novel task-driven evaluation. Experiments on RESIDE shed light on the comparisons and limitations of the state-of-the-art dehazing algorithms, and suggest promising future directions.		Boyi Li;Wenqi Ren;Dengpan Fu;Dacheng Tao;Dan Feng;Wenjun Zeng;Zhangyang Wang	2018	IEEE Transactions on Image Processing	10.1109/TIP.2018.2867951	task analysis;computer vision;artificial intelligence;benchmarking;mathematics;ranging;benchmark (computing)	Visualization	63.65531933711264	-64.73534901058298	160300
b309a2af7c36965554d1ffd4b9613e955cf16b25	rough sets attributes reduction based expert system in interlaced video sequences	database system;interpolation;image processing;expert systems;video signal processing;rough sets expert systems video sequences image processing reliability engineering power engineering and energy testing database systems decision making interpolation;interpolation rough sets attributes reduction expert system interlaced video sequences deinterlacing algorithm rough sets theory uncertain boundary regions image processing decision making;uncertain boundary regions;rough set theory;fuzzy control;rough sets attributes reduction;indexing terms;attribute reduction;deinterlacing algorithm;rough sets theory;interlaced video sequences;information system;rough set;computer simulation;is research;video signal processing decision making expert systems image sequences interpolation rough set theory;image sequences;expert system	A deinterlacing algorithm that is based on rough sets theory is researched and applied in this paper. The fundamental concepts of rough sets, with upper and lower approximations, offer a powerful means of representing uncertain boundary regions in image processing. However, there are a few studies that discuss the effectiveness of the rough sets concept in the field of engineering. Moreover, the studies involving deinterlacing systems that are based on rough sets have not been proposed yet. Thus, this paper proposes a deinterlacing method that will reliably confirm that the method being tested is the most suitable for the sequence, with almost perfect reliability. This proposed deinterlacing approach employs a size reduction of the database system, keeping only the essential information for the process. Decision making and interpolation results are presented. The results of computer simulations show that the proposed method outperforms a number of methods presented in the literature	algorithm;approximation;computer simulation;database;deinterlacing;expert system;feature selection;image processing;interlaced video;interpolation;rough set	Gwanggil Jeon;Donghyung Kim;Jechang Jeong	2006	IEEE Transactions on Consumer Electronics	10.1109/TCE.2006.273155	computer vision;rough set;computer science;machine learning;data mining;mathematics;expert system	DB	55.53543278658078	-63.89404005143742	160391
df8439a50db4c03006565572ac0960f265b9b7f6	a progressive refinement approach to aerial image registration using local transform perturbations	video image registration;spatial domain log polar techniques;aerial video surveillance system;video surveillance;gross translation estimation;video signal processing;rotational initial alignment error effects;scale effects;robust phase correlation;aerial image;image registration phase estimation hyperspectral imaging fourier transforms noise robustness layout parameter estimation video surveillance system testing virtual manufacturing;accuracy;affine transform parameters;aerial image registration;estimation;log polar origin location;geophysical signal processing;initial phase correlation;video signal processing affine transforms geophysical signal processing geophysical techniques image registration remote sensing;affine transformation;remote sensing;image registration;pixel;local transform perturbations;affine transforms;transforms;registration;progressive refinement;robustness;registration solution verity measure;sequential estimation;correlation;video;video registration phase correlation;local search;aerial video surveillance system progressive refinement aerial image registration local transform perturbations spatial domain log polar techniques video image registration log polar origin location robust phase correlation gross translation estimation affine transform parameters registration solution verity measure scale effects rotational initial alignment error effects initial phase correlation;phase correlation;geophysical techniques	Spatial domain log-polar approaches have demonstrated success for video image registration. However, the log-polar representation is sensitive to origin location. This drawback often necessitates performing a parameter sweep over log-polar origin location, which can be time-consuming. In this paper we present an alternative approach that is appropriate for small to moderate scale, rotational, and skew misalignments but allows large translational offset. We use a form of robust phase correlation to estimate the gross translation, then perform a local search over log-polar origin to fine tune the translation. We sequentially estimate affine transform parameters by maximizing a measure of registration solution verity. We also investigate the effect of scale and rotational initial alignment errors on the robustness of the initial phase correlation to estimate gross translation. We present results using video imagery acquired from a real aerial video surveillance system.	aerial photography;aerial video;closed-circuit television;image registration;local search (optimization);phase correlation;progressive refinement	Stephen P. DelMarco;Helen F. Webb;Victor Tom;Todd Jenkins	2008	IGARSS 2008 - 2008 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2008.4779191	sequential estimation;computer vision;estimation;simulation;video;image registration;local search;affine transformation;accuracy and precision;phase correlation;correlation;pixel;statistics;robustness;remote sensing	Vision	54.71069413427993	-55.370045722363166	160501
beb7c87e5b28136d8eeb2219b0736f11474780fe	performance measure of image and video quality assessment algorithms: subjective root-mean-square error	databases;algorithms;video	Evaluating algorithms used to assess image and video quality requires performance measures. Traditional performance measures (e.g., Pearson’s linear correlation coefficient, Spearman’s rank-order correlation coefficient, and root mean square error) compare quality predictions of algorithms to subjective mean opinion scores (mean opinion score/differential mean opinion score). We propose a subjective root-mean-square error (SRMSE) performance measure for evaluating the accuracy of algorithms used to assess image and video quality. The SRMSE performance measure takes into account dispersion between observers. The other important property of the SRMSE performance measure is its measurement scale, which is calibrated to units of the number of average observers. The results of the SRMSE performance measure indicate the extent to which the algorithm can replace the subjective experiment (as the number of observers). Furthermore, we have presented the concept of target values, which define the performance level of the ideal algorithm. We have calculated the target values for all sample sets of the CID2013, CVD2014, and LIVE multiply distorted image quality databases.The target values and MATLAB implementation of the SRMSE performance measure are available on the project page of this study.	algorithm	Mikko Nuutinen;Toni Virtanen;Jukka Häkkinen	2016	J. Electronic Imaging	10.1117/1.JEI.25.2.023012	subjective video quality;computer vision;video;computer science;video quality;theoretical computer science;multimedia;pevq;algorithm	Vision	62.512648062236025	-63.42187248476069	160695
e26c3d0e8fee75c3d5d4e7561d7e16a10ef24995	object shape morphing with intermediate reflectance properties	shape morphing;computer vision;color image analysis;reflectance measurement	Image morphing techniques can create a smooth transition between two images. However, one of the main weakness of the image morphing technique is that intermediate images in the transition often have physically incorrect shading such as highlights and shadows. Moreover, we cannot alter viewing and lighting conditions when creating the intermediate images. That is because those images are obtained by simply interpolating pixel intensities of the two 2D images without knowledge of 3D object shape and re ectance properties. In this context, 3D shape morphing techniques have a de nite advantage in that arbitrary viewing and illumination conditions can be used for creating new images. Unfortunately, previous 3D morphing techniques do not account for object surface re ectance properties or re ection models when generating intermediate images. This often results in undesired shading artifacts. In this paper, we consider a new approach for 3D shape and re ectance morphing of two real 3D objects. Our morphing method consists of two components: shape and re ectance property measurement, and smooth interpolation of those measured properties. The measured shape and re ectance parameters are used to compute intermediate shape and re ectance parameters. Finally, the computed shape and re ectance parameters are used to render intermediate images which represent a smooth transition between the two objects.	color image;interpolation;lambertian reflectance;morphing;pixel;range imaging;ringing artifacts;shading;shadow (os/2);shape context;specular highlight	Yoichi Sato;Imari Sato;Katsushi Ikeuchi	1997	International Journal of Shape Modeling	10.1142/S0218654397000082	computer vision;computer science;mathematics;computer graphics (images)	Vision	61.54927439718202	-53.254690998904216	160854
7f8ab95e508c975b2504a15c2716362f439cec5a	instant stippling on 3d scenes		In this paper, we present a novel real-time approach to generate high-quality stippling on 3D scenes. The proposed method is built on a precomputed 2D sample sequence called incremental Voronoi set with blue-noise properties. A rejection sampling scheme is then applied to achieve tone reproduction, by thresholding the sample indices proportional to the inverse target tonal value to produce a suitable stipple density. Our approach is suitable for stippling large-scale or even dynamic scenes because the thresholding of individual stipples is trivially parallelizable. In addition, the static nature of the underlying sequence benefits the frame-to-frame coherence of the stippling. Finally, we propose an extension that supports stipples of varying sizes and tonal values, leading to smoother spatial and temporal transitions. Experimental results reveal that the temporal coherence and real-time performance of our approach are superior to those of previous approaches.		Lei Ma;Jianwei Guo;Dong-Ming Yan;Hanqiu Sun;Yanyun Chen	2018	Comput. Graph. Forum	10.1111/cgf.13565	voronoi diagram;image processing;stippling;computer vision;computer science;tone reproduction;artificial intelligence;colors of noise;rejection sampling;thresholding;coherence (physics)	NLP	64.14686596192449	-53.75712304953218	161200
f213a2529a6a607644592ad19c1530ef386aa556	hedgehog labeling: view management techniques for external labels in 3d space	3d pole constraint hedgehog labeling view management techniques external labels 3d object space 3d geometric constraints;three dimensional displays;h 5 1 information interfaces and presentation multimedia information systems artificial augmented and virtual realities;three dimensional displays augmented reality real time systems;augmented reality;three dimensional displays layout cameras image resolution vectors force gears;real time systems	Annotations of objects in 3D environments are commonly controlled using view management techniques. State-of-the-art view management strategies for external labels operate in 2D image space. This creates problems, because the 2D view of a 3D scene changes over time, and temporal behavior of elements in a 3D scene is not obvious in 2D image space. We propose managing the placement of external labels in 3D object space instead. We use 3D geometric constraints to achieve label placement that fulfills the desired objectives (e.g., avoiding overlapping labels), but also behaves consistently over time as the viewpoint changes. We propose two geometric constraints: a 3D pole constraint, where labels move along a 3D pole sticking out from the annotated object, and a plane constraint, where labels move in a dominant plane in the world. This formulation is compatible with standard optimization approaches for labeling, but overcomes the lack of temporal coherence.	3d computer graphics;algorithm;ar (unix);augmented reality;automatic label placement;coherence (physics);concave function;display resolution;distortion;graph drawing;graphics pipeline;mathematical optimization;sonic the hedgehog 2;tuple space;usability testing	Markus Tatzgern;Denis Kalkofen;Raphaël Grasset;Dieter Schmalstieg	2014	2014 IEEE Virtual Reality (VR)	10.1109/VR.2014.6802046	computer vision;augmented reality;simulation;computer science;artificial intelligence;computer graphics (images)	Visualization	58.96446183100563	-54.329565836035655	161246
3c27a6f0d1e6f8963ad740f4d50c2e13ae11fb24	hue preserving-based approach for underwater colour image enhancement			color image;image editing	Guojia Hou;Zhenkuan Pan;Baoxiang Huang;Guodong Wang;Xin Luan	2018	IET Image Processing	10.1049/iet-ipr.2017.0359	computer vision;artificial intelligence;hue;underwater;mathematics;pattern recognition	Vision	59.530621668393	-62.19525279936666	161330
abd0cd6fa123cc726a8b834f8dcfe8395c4ead76	human visual system inspired saliency guided edge preserving tone-mapping for high dynamic range imaging		With the growing popularity of High Dynamic Range Imaging (HDR), the necessity for advanced tone-mapping techniques has greatly increased. We propose a Human Visual System (HVS)-inspired Saliency Guided Edge-preserving Tone Mapping method (HSGETM) that uses saliency region information of an HDR image as input to the guided filter for a base and detail image layer separation. After detail layer enhancement and base layer compression with constant weights, a new edge preserved tone mapped image is composed by adding the layers back together with saturation and exposure adjustments. The filter operation is faster due to the use of the guided filter which has O(N) time operation with N number of pixels. Experimental results demonstrate that the HSGETM has higher edge, and naturalness preserving capability, which is homologous to the HVS, as compared to other state-of-the-art tone mapping approaches.	dynamic range;high-dynamic-range imaging;human visual system model;kalman filter;pixel;range imaging;the filter;tone mapping	Nipu R. Barai;Matthew J. Kyan;Dimitrios Androutsos	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296435	pixel;computer vision;artificial intelligence;high-dynamic-range imaging;salience (neuroscience);tone mapping;feature extraction;computer science;naturalness;human visual system model;pattern recognition	Robotics	58.94964008058699	-64.24662163325434	161349
5c76fad23760d5341af1ec018174839f1edefd31	cepstral analysis based blind deconvolution for motion blur	image motion analysis;psnr;blind deconvolution;point spread function image restoration blind deconvolution cepstral analysis;image restoration;psf;image blurring;cepstral analysis;motion blur;cepstrum;cepstrum image restoration deconvolution cameras psnr;point spread function;blind image deconvolution;deconvolution;image motion analysis blind image deconvolution image blurring cepstral analysis image restoration point spread function psf;image restoration cepstral analysis image motion analysis;cameras	Camera shake during exposure blurs the captured image. Despite several decades of studies, image deconvolution to restore a blurred image still remains an issue, particularly in blind deconvolution cases in which the actual shape of the blur is unknown. Approaches based on cepstral analysis succeeded in restoring images degraded by a uniform blur caused by a camera moving straight in a single direction. In this paper, we propose to estimate, from a single blurred image, the point spread function (PSF) caused by a normal camera undergoing a 2D curved motion, and to restore the image. To extend the traditional cepstral analysis, we derive assumptions about the PSF effects in the cepstrum domain. In a first phase, we estimate several PSF candidates from the cepstrum of a blurred image and restore the image with a fast deconvolution algorithm. In a second phase, we select the best PSF candidate by evaluating the restored images. Finally, a slower but more accurate deconvolution algorithm recovers the latent image with the chosen PSF. We validate the proposed method with synthetic and real experiments.	algorithm;blind deconvolution;cepstrum;experiment;gaussian blur;latent image;shake;synthetic intelligence	Haruka Asai;Yuji Oyamada;Julien Pilet;Hideo Saito	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5651299	image restoration;computer vision;speech recognition;computer science;point spread function;mathematics;blind deconvolution	Vision	56.111857904761266	-57.97711564180838	161361
7746b4c1c1ba3a6777e83cedebbc6973c950b7a8	impsac: synthesis of importance sampling and random sample consensus	sampling importance resampling;monte carlo markov chain;monte carlo methods geometry motion estimation robustness cameras uncertainty gaussian approximation layout image coding;random sampling;bayesian methods;stereoscopic vision;epipolar geometry;posterior distribution;statistical analysis;gaussian distribution image reconstruction importance sampling statistical analysis;image reconstruction;mixture of gaussians;monte carlo markov chain estimator impsac importance sampling random sample consensus epipolar geometry recovery feature correspondence images deformation rotation arbitrarily close approximation posterior distribution pyramid processor image resolution ransac mcmc estimator sampling importance resampling sir;importance sampling;structure from motion;gaussian distribution	This paper proposes a new method for recovery of epipolar geometry and feature correspondence between images which have undergone a significant deformation, either due to large rotation or wide baseline of the cameras. The method also encodes the uncertainty by providing an arbitrarily close approximation to the posterior distribution of the two view relation. The method operates on a pyramid from coarse to fine resolution, thus raising the problem of how to propagate information from one level to another in a statistically consistent way. The distribution of the parameters at each resolution is encoded nonparametrically as a set of particles. At the coarsest level, a random sample consensus Monte Carlo Markov chain (RANSAC-MCMC) estimator is used to initialize this set of particles, the posterior can then be approximated as a mixture of Gaussians fitted to these particles. The distribution at a coarser level influences the distribution at a finer level using the technique of sampling-importance-resampling (SIR) and MCMC, which allows for asymptotically correct approximations of the posterior distribution. The estimate of the posterior distribution at the level above is being used as the importance sampling function to generate a new set of particles, which can be further improved by MCMC. It is shown that the method is superior to previous single resolution RANSAC-style feature matchers.	3d reconstruction;algorithm;baseline (configuration management);dirac comb;image registration;importance sampling;markov chain monte carlo;random sample consensus;resultant;sampling (signal processing)	Philip H. S. Torr;Colin Davidson	2003	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/TPAMI.2003.1182098	normal distribution;iterative reconstruction;sampling;econometrics;mathematical optimization;structure from motion;markov chain monte carlo;bayesian probability;importance sampling;stereopsis;mixture model;mathematics;posterior probability;statistics;epipolar geometry	Vision	54.0541891576226	-55.519369652976884	161606
db904c6fcc449be78d8bf81949e46fe3e2200b9f	improving the performance of k-means for color quantization	cluster algorithm;image processing;k means;data clustering;efficient implementation;clustering;color reduction;color quantization	Color quantization is an important operation with many applications in graphics and image processing. Most quantization methods are essentially based on data clustering algorithms. However, despite its popularity as a general purpose clustering algorithm, k-means has not received much respect in the color quantization literature because of its high computational requirements and sensitivity to initialization. In this paper, we investigate the performance of k-means as a color quantizer. We implement fast and exact variants of k-means with several initialization schemes and then compare the resulting quantizers to some of the most popular quantizers in the literature. Experiments on a diverse set of images demonstrate that an efficient implementation of k-means with an appropriate initialization strategy can in fact serve as a very effective color quantizer.	algorithm;cluster analysis;color quantization;distortion;experiment;graphics;image processing;k-means clustering;nearest neighbor search;quantization (signal processing);requirement;sourceforge	M. Emre Celebi	2011	Image Vision Comput.	10.1016/j.imavis.2010.10.002	computer vision;color quantization;image processing;computer science;theoretical computer science;machine learning;cluster analysis;linde–buzo–gray algorithm	AI	63.380966095149425	-65.847268659998	161672
d2d2f603cd57e3d06db08fdcf772dcdf3dbe9ab7	projection moire for 3d inspection of printed circuit boards	vision ordenador;moire patterns;formation image tridimensionnelle;data compression;visualizacion;3d imaging;phase shifting;digitizing;phase shift;numerisation;inspection;three dimensional;computer vision;visualization;cuantificacion vectorial;vector quantization;visualisation;numerizacion;vision ordinateur;compresion dato;printed circuit board;formacion imagen tridimensional;compression donnee;quantification vectorielle	We present a method of projection moire specially devised for the three-dimensional inspection of printed circuit boards. This method incorporates phase-shifting technique in analyzing moire fringes so as to achieve a fme resolution of 1 micron in height measurement. Further a synchronous grating translation scheme enhances the lateral measuring resolution by inherently removing the original pattern of the reference grating in resulting moire fringes. Finally we discuss the advantages of the proposed method using several measurement results performed on the various types of solder paste silk-screened on printed circuit boards.	lateral thinking;paste;printed circuit board;printing;projection screen;solder mask;syntax-directed translation	Seung-Woo Kim;Yi-Bae Choi;Jung-Taek Oh	1997		10.1117/12.269750	computer vision;electronic engineering;engineering;engineering drawing	EDA	62.70675680912655	-59.33331282890731	161906
08797c60eee4b076662db043f41b9e256470ec82	enhancing motion field with oa-filter and alternative measurement	video signal processing;computer graphics;motion estimation;flow visualisation;image enhancement;flow field sharpness occlusion aware filter video processing 3d reconstruction video tracking moving object detection motion field enhancment oa filter alternative flow measurement;image reconstruction;estimation optical imaging integrated optics image color analysis accuracy laplace equations adaptation models;video signal processing computer graphics filtering theory flow visualisation image enhancement image reconstruction motion estimation object detection;filtering theory;object detection	In this work, we introduce an alternative measurement to evaluate quality of flow field. We then present an alternative filter called OA-filter (Occlusion-Aware filter) which uses this measurement. It is integrated inside the solver to push the performance of the model in terms of accuracy and sharpness. The experiments show that the estimation result has been significantly improved. The method yields sharp flow field which is very useful for video processing, 3D reconstruction, video tracking, and moving object detection.	3d reconstruction;algorithm;experiment;kalman filter;mathematical model;motion estimation;motion field;object detection;real life;solver;synthetic intelligence;video processing;video tracking	Duc Dung Nguyen;Jae Wook Jeon	2012	Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)		iterative reconstruction;computer vision;image processing;computer science;video tracking;motion estimation;multimedia;computer graphics;computer graphics (images)	Robotics	57.24095537601365	-57.12818873075285	162171
f5621769199e2c341765efc6d8783abcb6640ee8	symmetric dynamic programming stereo using block matching guidance	dynamic programming;graph theory;stereo image processing dynamic programming graph theory image enhancement image matching image reconstruction;image matching;image reconstruction three dimensional displays dynamic programming heuristic algorithms stereo image processing complexity theory accuracy;image enhancement;image reconstruction;stereo image processing;image processing technique symmetric dynamic programming stereo matching block matching guidance block matching stereo matching local based bms local area matching techniques sdps semi global matching graph cuts stereo matching gcs global matching 3d reconstruction enhancement	In this paper, three stereo matching algorithms are investigated: Block Matching Stereo (BMS) represents local area matching techniques, Symmetric Dynamic Programming Stereo (SDPS) represents semi-global matching, and Graph Cuts Stereo (GCS) represents global matching. Both local and semiglobal methods are relatively fast and feasible for parallel implementation. On the other hand, global GCS obtains a higher stereo matching accuracy but it is very computationally expensive. Thus, we propose a technique that guides SDPS using pre-computed Local-based BMS's result which both restricts and guides the Dynamic Programming search for optimal profile related to BMS's signals. The technique is not only parallel-able for a faster processing speed but also self-repeatable for an enhancement of the 3D reconstruction. Our experimental results show that this proposed technique is superior to the Global Graph Cuts Stereo (GCS) in both speed and accuracy. As well as reconstructing relatively accurate results from the infamous Middlebury datasets, this method is also demonstrated to be fast, robust, and reliable in reconstructing high quality 3D scenes from real-life stereo images.	3d reconstruction;algorithm;analysis of algorithms;battery management system;computer stereo vision;cut (graph theory);display resolution;download;dynamic programming;precomputation;real life;semiconductor industry;server (computing);upload;web application	Minh Nguyen;Yuk Hin Chan;Patrice Delmas;Georgy L. Gimel'farb	2013	2013 28th International Conference on Image and Vision Computing New Zealand (IVCNZ 2013)	10.1109/IVCNZ.2013.6726997	iterative reconstruction;computer vision;mathematical optimization;template matching;graph theory;machine learning;dynamic programming;mathematics	Vision	56.12745708204686	-56.2618782739179	162201
18f222add929a3ac5de746e32e547e59518bf76e	optimization of image compression method based on fuzzy relational equations by overlap level of fuzzy sets	fuzzy relations;overlap of fuzzy sets.;image compression;fuzzy relational equations;color space;fuzzy set;human perception;design method;peak signal to noise ratio	 A design method of coders on YUV color space is proposed based on an overlap level of fuzzy sets, in order to optimize the image compression/reconstruction method based on fuzzy relational equations. In the YUV color representation of the original image, the Y plane contains more information than the U and V planes, in terms of human perception. Therefore, coders with different sizes (Y plane coders bigger than U and V planes) lead to more effective compression/reconstruction, where the appropriate coders for YUV planes can be constructed based on an overlap level of fuzzy sets. Through image compression/ reconstruction experiments using 100 typical images (extracted from Corel Gallery, Arizona Directory), it is confirmed that the peak signal to noise ratio of the proposed method increases at a rate of 7.1% ~ 13.2%,compared to the conventional method, when compression rates range from 0.0234 ~ 0.0938.	fuzzy set;image compression;overlap–add method	Hajime Nobuhara;Eduardo Masato Iyoda;Kaoru Hirota;Witold Pedrycz	2005		10.1007/10966518_12	fuzzy logic;crystallography;alkyl;fuzzy associative matrix;defuzzification;halogen;artificial intelligence;atom;pattern recognition;fuzzy number;mathematics;integer	Vision	54.61625202514447	-62.19035494770363	162233
1c0614bebb43f4492a709fbedb3030319d9ab11d	stereo reconstruction using high-order likelihoods	markov random field;high order likelihood;occlusion handling;stereo vision;high order cliques;graph cuts;global matching framework	Under the popular Markov random field (MRF) model, low-level vision problems are usually formulated by prior and likelihood models. In recent years, the priors have been formulated from high-order cliques and have demonstrated their robustness in many problems. However, the likelihoods have remained zeroth-order clique potentials. This zeroth-order clique assumption causes inaccurate solution and gives rise to undesirable fattening effect especially when window-based matching costs are employed. In this paper, we investigate high-order likelihood modeling for the stereo matching problem which advocates the dissimilarity measure between the whole reference image and the warped non-reference image. If the dissimilarity measure is evaluated between filtered stereo images, the matching cost can be modeled as high-order clique potentials. When linear filters and nonparametric census filter are used, it is shown that the high-order clique potentials can be reduced to pairwise energy functions. Consequently, a global optimization is possible by employing efficient graph cuts algorithm. Experimental results show that the proposed high-order likelihood models produce significantly better results than the conventional zerothorder models qualitatively as well as quantitatively. 2014 Elsevier Inc. All rights reserved.	algorithm;binocular disparity;central processing unit;computer stereo vision;correspondence problem;experiment;global optimization;graphics processing unit;high- and low-level;markov chain;markov random field;mathematical optimization;microsoft windows;parallel computing;pixel;reflections of signals on conducting lines;speedup;text simplification;unary operation	Ho Yub Jung;Haesol Park;In Kyu Park;Kyoung Mu Lee;Sang Uk Lee	2014	Computer Vision and Image Understanding	10.1016/j.cviu.2014.04.009	computer vision;mathematical optimization;cut;computer science;stereopsis;machine learning;pattern recognition;mathematics;statistics	Vision	54.67219009980855	-59.52666312755233	162316
97feaad305f239d5282c44bd567bb167867e0dde	transformation guided image completion	reflection image guided image completion transformation interactive image completion system midlevel structure translation glide guidance maps candidate shifts encoding rotation transformation scale transformation conjunction color matching cost;image matching image coding image colour analysis image enhancement;image coding;image matching;optimization shape image color analysis image segmentation surface treatment green products user interfaces;image enhancement;image colour analysis	In this paper, we describe a new interactive image completion system that allows users to easily specify various forms of mid-level structures in the image. Our system supports the specification of four basic symmetric types: reflection, translation, rotation, and glide. The user inputs are automatically converted into guidance maps that encode possible candidate shifts and, indirectly, local transformations of rotation and scale. These guidance maps are used in conjunction with a color matching cost for image completion. We show that our system is capable of handling a variety of challenging examples.	encode;glide os;mind map;reflection (computer programming)	Jia-Bin Huang;Johannes Kopf;Narendra Ahuja;Sing Bing Kang	2013	IEEE International Conference on Computational Photography (ICCP)	10.1109/ICCPhot.2013.6528313	image quality;image warping;image texture;computer vision;feature detection;template matching;color image;image gradient;binary image;image processing;digital image processing;mathematics;automatic image annotation;engineering drawing;computer graphics (images)	Robotics	56.162561899971124	-61.72763681093516	162584
36413e36cf1f7937554cb26f8b791da0948afe91	image sequences filtering using the adaptive number of frames	motion compensation;appropriate number of consecutive frames;motion estimation;adaptive weighted averaging;image sequence;noise variance	In this paper, we will propose a novel spatiotemporal filter that utilizes image sequences in order to remove noise. The consecutive frames include: current, previous and next noisy frames. The filter proposed in this paper is based upon the weighted averaging pixels intensity in image sequences. It utilizes the Appropriate Number of Consecutive Frames (ANCF) based on the noisy pixels intensity among the frames. The number of consecutive frames is adaptively calculated for each region in image and its value may change from one region to another region depending on the pixels intensity within the region. The weights are determined by a well-defined mathematical criterion, which is adaptive to the feature of spatiotemporal pixels of the consecutive frames. It is experimentally shown that the proposed filter can preserve image structures and edges under motion while suppressing noise, and thus can be effectively used in image sequences filtering. Most importantly, our proposed filter is independent of noise variance and only utilizes the intensity of pixels to suppress noise.		Mahmoud Saeidi	2007			computer vision;control theory;mathematics;bilateral filter;statistics	Vision	56.57065362504664	-59.37739273943508	162756
8318dbcdd2e10321c74cff47599c44e3ea59ec6a	increasing space-time resolution in video	low resolution;space time;temporal resolution;motion blur;space time analysis;super resolution;dynamic scenes	We propose a method for constructing a video sequence of high space-time resolution by combining information from multiple lowresolution video sequences of the same dynamic scene. Super-resolution is performed simultaneously in time and in space. By “temporal super-resolution” we mean recovering rapid dynamic events that occur faster than regular frame-rate. Such dynamic events are not visible (or else observed incorrectly) in any of the input sequences, even if these are played in “slow-motion”. The spatial and temporal dimensions are very different in nature, yet are inter-related. This leads to interesting visual tradeoffs in time and space, and to new video applications. These include: (i) treatment of spatial artifacts (e.g., motion-blur) by increasing the temporal resolution, and (ii) combination of input sequences of different space-time resolutions (e.g., NTSC, PAL, and even high quality still images) to generate a high quality video sequence.	box blur;display resolution;ntsc;pal;super-resolution imaging	Eli Shechtman;Yaron Caspi;Michal Irani	2002		10.1007/3-540-47969-4_50	computer vision;simulation;image resolution;computer science;temporal resolution;space time;superresolution;computer graphics (images)	Visualization	59.63676780810111	-56.447143496520475	162824
95f8e4f4643234b34ad6089d71e280984f417871	double-characters detection of nonlinear frequency modulated signals based on frft	nonparametric detection;median filter;fractional fourier transform;binary image;simulation experiment;polynomial phase signals;frequency modulated;double characters detector;multiple median filtering	In many practical applications, signals to be detected are unknown nonlinear frequency modulated (FM) and are corrupted by strong noise. The phase histories of the nonlinear FM signals are assumed to be unknown smooth functions of time, which are usually poorly modeled or cannot be modeled at all by a small number of parameters. Because of the lack of phase model, a nonparametric detection method is proposed based on successive fractional Fourier transform and double-characters detection. The detection process goes in three steps. First, an image is constructed by the fractional Fourier transforms of successive angles in one period. Then, the threshold procedure is utilized to transform the image into a binary image. After the multiple median filtering, the binary image is refined where the isolated noise pixels are removed. Finally, two complementary features are extracted from the refined image, and a double-characters detector is proposed to decide whether the target is present or not. The simulation experiments to three polynomial phase signals with different orders and a sinusoidal phase signal show that the proposed detection method is effective and robust.	binary image;experiment;fm broadcasting;fractional fourier transform;median filter;modulation;nonlinear system;pixel;polynomial;simulation	Shu-Wen Xu;Penglang Shui;Xiaochao Yang	2010	Science China Information Sciences	10.1007/s11432-010-4146-y	median filter;computer vision;mathematical optimization;binary image;computer science;fractional fourier transform;mathematics;statistics	Vision	54.47187778388731	-63.37255260217814	162826
874a55a0b4418b47d85836c11404455de500660d	innovative color interpolation using fuzzy logic and linear regression		 Fuzzy logic and linear regression method – The Fuzzy logic method • Based on the effect between pixels – To avoid color distortion around the edges – The linear regression method • To process the interaction effect among R, G and B channels – To eliminate false color, color moire, aliasing and color shift in the full color camera output – To show a very good quality 3 / 22	aliasing;distortion;fuzzy logic;interpolation;pixel	Jyh-jiun Lee;Yu-Sheng Tsai;Yi-Ching Liaw;Chia-Lun Chen;Chia-Hung Cheng;Shih-Chieh Chen;Yibin Lu	2005			interpolation;computer science;fuzzy logic;bilinear interpolation;linear regression;mathematical optimization;machine learning;artificial intelligence	Vision	58.268595173011796	-64.29715462583947	162843
264a2b5a3c2029c3dace5ec317be487124d27f15	image quality impact for eye tracking systems accuracy	image coding;codecs;systematics;gaze tracking;image quality;cameras;tracking	The accuracy of the eye tracking systems is a key indicator of data validity. Currently developed eye tracking systems can be configured to be used as remote wireless autonomous systems. In order to meet the required constraints of system's responsiveness and effectively use the available hardware, image compression techniques can be employed in order to reduce the amount of data needed to be sent via a physical transmission link. Since eye tracking systems are sensitive to input image details it is necessary to preserve as much details as possible. In this paper we are presenting results of accuracy gradation for an eye tracking system depending on the quality of the decompressed image. This image is used as input for the eye tracking system in order to find the minimal acceptable quality that leads to neglectable tracking systematic errors.	autonomous system (internet);eye tracking;image compression;image quality;responsiveness;tracking system	Pavel Morozkin;Marc Swynghedauw;Maria Trocan	2016	2016 IEEE International Conference on Electronics, Circuits and Systems (ICECS)	10.1109/ICECS.2016.7841226	computer vision;simulation;tracking system;computer science;computer graphics (images)	Robotics	64.81333807620207	-62.87301782620874	163172
552c4b3f4765358d28ae4331daafa3543b1a98eb	dual-layered light field display: maximizing image perceptibility		In this work, we propose a novel light field approximation method for multi-layer light field display. Our target light field display consists of two liquid crystal panels with a uniform back-light with no time multiplexing. LCD panels are not necessarily to be parallel. For wide angle of view configuration, we introduce quadratic penalization term to alleviate ghost effects. This creates perceptually improved approximation of light field and increases the possibility of usage in design with a wider field of view configuration.	approximation;layer (electronics);light field;liquid-crystal display;multiplexing	Ibragim R. Atadjanov;Seungkyu Lee	2017		10.1145/3102163.3102240	computer vision;perceptual system;artificial intelligence;computer science;liquid crystal;quadratic equation;liquid-crystal display;light field;angle of view;field of view;multiplexing	HCI	61.26642626129128	-55.46680804506778	163239
753150c7833222b451994ff2de921c9e97ff27da	restoration and enhancement of images degraded by light scattering and absorption		Author(s): Peng, Yan-Tsung | Advisor(s): Cosman, Pamela | Abstract: Images degraded by light scattering and absorption such as hazy, sandstorm, and underwater images often suffer from color distortion and low contrast because of light traveling through turbid media. This can prevent systems that operate outdoors in different lighting conditions from functioning properly, for example, video surveillance systems, autopilot systems and intelligent transportation systems, which include automatic license plate recognition, automatic traffic counting, etc. Therefore, it is desirable to develop an effective method to restore color and enhance contrast for these images. This thesis presents novel work to advance research on image restoration and enhancement for such images.To enhance or restore such a degraded image, the image formation model is often used to describe it as a ``clear image blended with an ambient light based on the scene transmission computed using the scene depth from the camera. The transmission describes the portion of the scene radiance which is not scattered or absorbed and which reaches the camera. By reversing the image formation process, one can attain the scene radiance from a degraded image, which is a ``clear image. However, it involves solving an ill-posed and under-constrained problem because we need to estimate both the ambient light and scene transmission from a single degraded image.To attack this problem, we proposed to use image blurriness to estimate ambient light and scene depth for underwater images. Furthermore, we extended it by combining light absorption and blurriness to estimate scene depth for underwater scenes in different lighting conditions and color tones. For any images degraded by light scattering and absorption, not limited to underwater ones, we proposed a generalization of the common dark channel prior approach for ambient light and transmission estimation. Additionally, adaptive color correction is incorporated into the image formation model for removing color casts while restoring contrast. Based on the experimental results, our proposed algorithms outperform, both subjectively and objectively, other state-of-the-art algorithms based on the image formation model.		Yan-Tsung Peng	2017			computer vision;image restoration;radiance;color correction;light scattering;effective method;distortion;image formation;artificial intelligence;absorption (electromagnetic radiation);computer science	Vision	57.70751381008354	-59.79281834784215	163364
4bee065d93ae2484baf837df51a3467f2a5403bb	perception-driven resizing for dynamic image sequences	image interpolation;image resizing;dynamic image sequence;feature descriptor;human visual perception	With the development of multimedia display devices, dynamic image sequence resizing, which can adapt image sequences to be displayed on devices with different resolutions, is becoming more important. However, existing approaches do not resize results from the viewpoint of the user. In this paper, we present a new resizing framework, which uses the feature descriptor technique and the image interpolation technique, that aims to improve the resizing quality of the important content perceived by user. To accomplish this, we use a coarse-to-fine detection approach to determine the important content of image sequences, and construct a partition interpolation model to improve the definitions of important content. By adopting a region energy protection approach we can obtain high quality image displays. Compared to representative algorithms in image resizing, our method can achieve satisfactory performance not only in terms of image visualization but also in terms of quantitative measures.	algorithm;display resolution;experiment;image scaling;interpolation;real-time clock;seam carving;visual descriptor	Lingling Zi;Junping Du;Lisha Hou;Xiangda Sun;Jangmyung Lee	2013	Comput. Sci. Inf. Syst.	10.2298/CSIS120731051Z	image quality;computer vision;feature detection;computer science;multimedia;image scaling;computer graphics (images)	Visualization	59.7143536184154	-61.602422802316795	163465
5af5f6644a9dea2e9c57c4c78a3512e84d7a8aca	atd model for color vision ii: applications	color vision	Using a slightly modified version of an earlier model, and using a new rule for simulating the effects of simultaneous or successive chromatic adaptation, excellent predictions are made for experimental data that offer especially strong challenges to models for chromatic adaptation.	color vision;simulation	S. Lee Guth	1994		10.1117/12.173844	computer vision;simulation;telecommunications;color vision;optics;physics;computer graphics (images)	ML	61.85930529351761	-53.320614411452624	163759
b8f6fd0892c24ffa0bb381b87372c917c5d2e50a	a content adaptive de-interlacing algorithm [video signal processing applications]	texture detector;interpolation;missing field interpolation;missing pixel interpolation;content adaptive de interlacing algorithm;video signal processing;edge detection;image converters;paper technology;directional correlation estimation;frequency estimation;video sequences;edge orientation adaptive search range;correlation methods;candidate directions consistency checking;image texture;cadi;gold;internet;current measurement;horizontal edge detector;vector matching;pixel;consistency checking;consecutive pixel combining;vertical edge detection;content adaptation;bandwidth;interlaced video;video format conversion;tv;image converters tv frequency estimation pixel current measurement gold paper technology video sequences bandwidth internet;candidate directions consistency checking horizontal edge detector texture detector content adaptive de interlacing algorithm cadi interlaced video progressive video video format conversion missing field interpolation vertical edge detection vector matching missing pixel interpolation edge orientation adaptive search range consecutive pixel combining directional correlation estimation;progressive video;correlation methods video signal processing edge detection image texture interpolation	This paper proposed a content adaptive de-interlacing algorithm (CADI) for conversion of interlaced video to progressive video. The proposed CADI algorithm interpolates the missing field based on the local image characteristics (smooth, horizontal edge, vertical edge and texture) and chooses the most appropriate de-interlacing algorithm. In addition, a novel vector matching algorithm is introduced for interpolating missing pixels in the horizontal edge region. It has three features to increase the accuracy of estimating edge directions: an adaptive search range for different edge orientations, combining consecutive pixels for directional correlation estimation within the same edge region and consistency checking along the candidate directions. Experimental results show that the proposed CADI algorithm outperforms existing algorithms in various video sequences, and produces high quality de-interlaced frames.	display resolution;genetic algorithm;integrated test facility;interlaced video;interpolation;pixel;progressive scan;signal processing;vii	Tak-Song Chong;Oscar C. Au;Wing-San Chau;Tai-Wai Chan	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1465737	progressive scan;gold;image texture;computer vision;electronic engineering;the internet;edge detection;interpolation;computer science;mathematics;bandwidth;pixel;computer graphics (images)	Vision	57.484645120820666	-55.15505502554377	163845
6788659f0ff5c58b78b47643278e05f118ffa14f	a new edge detection approach via neutrosophy based on maximum norm entropy		Abstract It is a quite important step to find object edges in applications such as object recognition, classification and segmentation. Therefore, the edge detection algorithm to be used directly influences the performance of these applications. In this study, a new edge detection method based on Neutrosophic Set (NS) structure via using maximum norm entropy (EDA-NMNE) is proposed. Many experts and intelligence systems, including the fuzzy system, do not satisfactorily succeed in resolving indeterminacies and deficiencies. However, in the NS approach, problems are solved by dividing them into True (T), False (F) and Indeterminacy (I) subsets. In addition, because the approach has a powerful algorithmic structure, NSu0027s conditions with indeterminate and missing situations can be solved successfully. In this study, object edges can be found successfully with the proposed approach because edges of the object are considered as indeterminate. Thus, using the proposed EDA-NMNE, a strong intelligent expert system-based edge finding software was designed. In our study, 5 different object edge detection results were obtained by converting 5 different types of entropy into NS-based edge detection software. Thus, edge detection analysis has been performed by experimenting many types of entropy that have not been previously used for NS edge detection. The Metric Figure of Merit (FOM), Peak Signal-to-Noise Ratio (PSNR) analysis, mean square error (MSE) were used in this experimental study. Based on these results, highest FOM and PSNR values are obtained by using NORM entropy. In addition, the proposed edge detection approach were compared with Edge Detection Approach via Canny (EDA-C), Edge Detection Approach via Sobel (EDA-S), Edge Detection Approach via Variation-Adaptive Ant Colony Optimization (EDA-VAACO) and Edge Detection Approach via Active Contour without Edge (EDA-ACWE). FOM and PSNR tests have been used to evaluate the edge detection results obtained via 5 different methods. The findings demonstrated that the performance of the proposed edge detection approach is more successful compared to other methods.	edge detection;principle of maximum entropy	Eser Sert;Derya Avci	2019	Expert Syst. Appl.	10.1016/j.eswa.2018.08.019	machine learning;ant colony optimization algorithms;active contour model;fuzzy control system;indeterminate;artificial intelligence;computer science;edge detection;pattern recognition;mean squared error;figure of merit;sobel operator	Vision	55.315826097973975	-64.76343143256678	164130
535866dc9430dc7a2a960486076db298017c1d4e	reorganized dct-based image representation for reduced reference stereoscopic image quality assessment	reduced reference rr;reorganized discrete cosine transform rdct;human visual system hvs;stereoscopic image quality assessment siqa	In this paper, a novel reduced reference (RR) stereoscopic image quality assessment (SIQA) is proposed by characterizing the statistical properties of the stereoscopic image in the reorganized discrete cosine transform (RDCT) domain. Firstly, the difference image between the left and right view images is computed. Afterwards, the left and right view images, as well as the difference image, are decomposed by block-based discrete cosine transform (DCT). The DCT coefficients are further reorganized into a three-level coefficient tree, resulting in ten RDCT subbands. For each RDCT subband, the statistical property of the coefficient distribution is modeled by the generalized Gaussian density (GGD) function. And the mutual information (MI) and energy distribution ratio (EDR) are employed to depict the statistical properties across different RDCT subbands. Moreover, EDR can further model the mutual masking property of the human visual system (HVS). By considering the GGD modeling behavior within each RDCT subband and MI together EDR characterizing behavior across RDCT subbands, the statistical properties of the stereoscopic image are fully exploited, including the left view, right view, and the difference image. Experimental results demonstrated that the statistical properties of the difference image can well represent the perceptual quality of the stereoscopic image, which outperforms the representative RR quality metrics for stereoscopic image and even some full reference (FR) quality metrics. By considering the left view, right view, and difference image together, the performances of the proposed RR SIQA can be further improved, which presenting a more closely relationship between the quality metric output and human visual perception. & 2016 Elsevier B.V. All rights reserved.	bluetooth;coefficient;discrete cosine transform;entity–relationship model;experiment;human visual system model;image quality;marginal model;mutual information;performance;round-robin scheduling;scene statistics;stereoscopy	Lin Ma;Xu Wang;Qiong Liu;King Ngi Ngan	2016	Neurocomputing	10.1016/j.neucom.2015.06.116	computer vision;speech recognition;mathematics;computer graphics (images)	Vision	61.71216302429768	-65.81129319682633	164416
38deb4fdc42b48a80935e6a466e05c1b26539792	codac: a compressive depth acquisition camera framework	compressed sensing;time of flight;optical radar image sensors optical modulation;single omnidirectional sensor codac compressive depth acquisition camera framework light detection and ranging systems lidar time of flight measurements scene raster scanning depth map acquisition spatial light modulator;laser radar;image sensors;time of flight compressed sensing depth maps lidar ranging;ranging;depth maps;lighting laser radar cameras spatial resolution photodetectors;optical radar;photodetectors;optical modulation;lighting;cameras;lidar;spatial resolution	Light detection and ranging (LIDAR) systems use time of flight (TOF) in combination with raster scanning of the scene to form depth maps, and TOF cameras instead make TOF measurements in parallel by using an array of sensors. Here we present a framework for depth map acquisition using neither raster scanning by the illumination source nor an array of sensors. Our architecture uses a spatial light modulator (SLM) to spatially pattern a temporally-modulated light source. Then, measurements from a single omnidirectional sensor provide adequate information for depth map estimation at a resolution equal that of the SLM. Proof-of-concept experiments have verified the validity of our modeling and algorithms.	algorithm;depth map;experiment;modulation;raster scan;sensor;spatial light modulator	Ahmed Kirmani;Andrea Colaco;Franco N. C. Wong;Vivek K. Goyal	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6289148	lidar;computer vision;computer science	Robotics	67.80621017891411	-61.240858344669846	164567
ba02f92f53e01c1854bec240eba14c691587b39e	fast non-iterative pca computation for spectral image analysis using gpu	parallel computing;spectral image analysis;gpu computation;213 sahko automaatio ja tietoliikennetekniikka elektroniikka;pca		computation;graphics processing unit;image analysis;iterative method	Jukka Antikainen;Markku Hauta-Kasari;Timo Jääskeläinen;Jussi Parkkinen	2010			parallel computing;theoretical computer science;computer graphics (images)	Vision	67.89824328618114	-54.49506731314619	164671
1df34f15dc4282a11206f59d030804002552dfb6	fuzzy logic recursive motion detection and denoising of video sequences	fuzzy membership function;sensors;fuzzy rules;logic;spatial filters;fuzzy logic;temporal filtering;motion blur;technology and engineering;noise reduction;image sequence;membership function;spatial filtering;denoising;video;motion detection;sliding window	We propose a fuzzy logic recursive scheme for motion detection and spatiotemporal filtering that can deal with the Gaussian noise and unsteady illumination conditions in both the temporal and spatial directions. Our focus is on applications concerning tracking and denoising of image sequences. We process an input noisy sequence with fuzzy logic motion detection to determine the degree of motion confidence. The proposed motion detector combines the membership of the temporal intensity changes, appropriately using fuzzy rules, where the membership degree of motion for each pixel in a 2-D sliding window is determined by a proposed membership function. Both the fuzzy membership function and the fuzzy rules are defined in such a way that the performance of the motion detector is optimized in terms of its robustness to noise and unsteady lighting conditions. We simultaneously perform tracking and recursive adaptive temporal filtering, where the amount of filtering is inversely proportional to the confidence in the existence of motion. Finally, temporally filtered frames are further processed by a proposed spatial filter to obtain a denoised image sequence. Our main contribution is a robust novel fuzzy recursive scheme for motion detection and temporal filtering. We evaluate the proposed motion detection algorithm using two criteria: (1) robustness to noise and to changing illumination conditions and (2) motion blur in temporal recursive denoising. Additionally, we make comparisons in terms of noise reduction with other state of the art video denoising techniques. © 2006 SPIE and IS&T. DOI: 10.1117/1.2201548	algorithm;color;fuzzy logic;fuzzy rule;gaussian blur;motion detector;noise reduction;norm (social);pixel;recursion;video denoising	Vladimir Zlokolica;Aleksandra Pizurica;Wilfried Philips;Stefan Schulte;Etienne E. Kerre	2006	J. Electronic Imaging	10.1117/1.2201548	computer vision;quarter-pixel motion;computer science;machine learning;motion estimation;noise reduction;control theory;mathematics;motion field	Vision	57.27590715133394	-63.79473271064615	164721
72d3c39362873e317d8a576df8cf10bb7f11b901	toward a perceptually uniform parameter space for filter transparency	color perception;visualization;transparency picker;transparency perception	Filter models of perceptual transparency relate to regularities in the retinal projections caused by light transmitting objects like clear liquids or glass and have been found to predict the color conditions for perceptual transparency more accurately than alternative models. An important but unsolved problem is how exactly the model parameters are related to the properties of the perceived transparent layer. We previously proposed a parametrization in terms of hue, saturation, overall transmittance and clarity of the filter that seems to capture important dimensions of the phenomenal impressions. However, these parameters are not independent and the corresponding scales are not perceptually uniform. Here, an invertible transformation of this parameter space is proposed that strongly mitigates these problems. This results in a more intuitively interpretable parameter set that seems well suited for the analysis of existing stimuli and the generation of transparent overlays with predefined perceptual properties. The latter property makes it suitable for graphics and visualization applications.	box counting;coefficient;cubic hermite spline;emoticon;graphics;image scaling;matlab;referential transparency;retinal implant;spline (mathematics);surround sound;transmitter	Franz Faul	2017	TAP	10.1145/3022732	computer vision;visualization;computer science;color vision;optics	ML	61.90183404252031	-60.13584731901465	164912
e93d9b3e2ef484e440e5dbcbab49637e0f963f1a	advances in digital image processing for document reproduction	digital image processing;pattern generation;spatial distribution;computational complexity;error correction;digital image	The properties of conventional, ordered dot-pattern generation techniques for bi-level halftone representation are examined and compared with the properties of error-diffusion-based, disordered dot-pattern-generation algorithms. The various processing steps necessary for adaptation of the disordered halftone pattern-generation technique to digital image hardcopy reproduction with non-ideal computer-output printing devices are described. It includes procedures for spatial distribution of thresholding errors, suppression of dot-density artifacts and compensation for dot overlap. These procedures represent the core of a Multiple-Error Correction Computation Algorithm called MECCA, the objective of which is to linearize the non-ideal printing process in order to minimize the loss or shift of tonal gradations. Finally, the performance of MECCA is compared with a conventional digital screening technique, and the various reproduction-quality versus computational-complexity trade-offs are discussed.	digital image processing	Peter Stucki	1984		10.1007/BFb0043458	computer vision;feature detection;image analysis;dynamic imaging;error detection and correction;analog image processing;template matching;binary image;image processing;computer science;theoretical computer science;digital signal processing;digital image processing;anti-aliasing;multimedia;computational complexity theory;automatic image annotation;top-hat transform;digital image	Graphics	59.27596451129149	-63.59053603050337	165073
0c54de272437457b8a9ca06935e5b3240b216fe4	adaptive scale selection for hierarchical stereo		Hierarchical stereo provides an efficient coarse-to-fine mechanism for disparity map estimation. However, common drawbacks of such an approach include the loss of high frequency structures not observable at coarse scale levels, as well as the unrecoverable propagation of erroneous disparity estimates through the scale space. This paper presents an adaptive scale selection mechanism to determine a suitable resolution level from which to begin the hierarchical depth estimation process for each pixel. The proposed scale selection mechanism allows us to robustly implement variable cost aggregation in order to reduce the variability of the photo-consistency measure across scale space. We also incorporate a weighted shiftable window mechanism to enable error correction during coarse-to-fine depth refinement. Experiments illustrate the effectiveness of our approach in terms of disparity accuracy, while attaining a computational efficiency compromise between full resolution and hierarchical disparity map estimation.	binocular disparity;computation;error detection and correction;map;observable;photo-consistency;pixel;refinement (computing);scale space;software propagation;spatial variability	Yi-Hung Jen;Enrique Dunn;Pierre Fite Georgel;Jan-Michael Frahm	2011		10.5244/C.25.95	computer vision;mathematical optimization;mathematics;statistics	Vision	55.45598917359129	-57.44914135773339	165085
c93c3b8b168df4c6bc7e494ae413a54c6da5b71c	quality of experience in uhd-1 phase 2 television: the contribution of uhd+hfr technology		A key factor to determine the quality of experience (QoE) of a video is its capability to convey the large spectrum of perceptual phenomena that our eyes can sense in real life. In order to meet this demand, the recent DVB UHD-1 Phase 2 specification employs new video features, such as higher spatial resolutions (4K/8K) and High Frame Rate (HFR). The first enables larger field of view and level of details, while the second offers sharper images of moving objects going well beyond the current frame rates. While the contribution of each of these technologies to QoE has been investigated individually, in this paper we are interested to study their interaction, and in quantifying the benefits to users from their combination. To this end, we conduct a subjective test on compressed UHD+HFR content on a recent display capable of reproducing 100 pictures per second at 2160p resolution, with the goal to assess the increase in QoE of UHD and HFR with respect to conventional video, both individually and in combination. The results indicate that for content with fast motion, at higher bitrates the combination of UHD and HFR significantly improves the QoE compared to that obtained when these features are used individually.	4k resolution;digital video broadcasting;real life	Vedad Hulusic;Giuseppe Valenzise;Jean-Charles Gicquel;Jérome Fournier;Frédéric Dufaux	2017	2017 IEEE 19th International Workshop on Multimedia Signal Processing (MMSP)	10.1109/MMSP.2017.8122238	hfr cell;computer science;high-definition video;computer vision;artificial intelligence;frame rate;quality of experience;digital video broadcasting;image resolution;field of view	Visualization	64.21342257400603	-62.89857313554481	165122
89738b052513d7194579e0283c4f727087400b3d	reliability of local ground truth data for image quality metric assessment		Image Quality Metrics (IQMs) automatically detect differences between images. For example, they can be used to find aliasing artifact in the computer generated images. An obvious application is to test if the costly anti-aliasing techniques must be applied so that the aliasing is not visible to humans. The performance of IQMs must be tested based on the ground truth data, which is a set of maps that indicate the location of artifacts in the image. These maps are manually created by people during so called marking experiments. In this work, we evaluate two different techniques of marking. In the side-by-side experiment, people mark differences between two images displayed side-by-side on the screen. In the flickering experiment, images are displayed at the same location but are exchanged over time. We assess the performance of each technique and use the generated reference maps to evaluate the performance of the selected IQMs. The results reveal the better accuracy of the flickering technique.	ground truth;image quality	Rafal Piórkowski;Radoslaw Mantiuk	2018		10.1007/978-3-030-03658-4_5	flicker;computer vision;image quality;computer-generated imagery;pattern recognition;ground truth;artificial intelligence;aliasing;computer science	Vision	62.528174921352	-58.25138978003751	165255
3ccc7cd3f3f8a2db6377bd9f2fa714e0afc96f45	a mixed noise filter acceleration algorithm based on mean value and variance similarity	gaussian noise;nonlinear filters;classification algorithm;image processing;impulse noise;nonlinear filter;image processing filtering theory;filtering algorithms;noise filtering algorithms classification algorithms maximum likelihood detection nonlinear filters noise reduction filtering theory;noise reduction;classification algorithms;maximum likelihood detection;mean value;digital image;digital images mixed noise filter acceleration algorithm mean value variance similarity local neighborhood filter speed;mixed noise;filtering theory;noise;variance digital image gaussian noise impulse noise mixed noise mean value;variance	A mixed noise filter acceleration algorithm base on mean value and variance similarity is proposed, Which is a improvement of existing better filter algorithm MNF in the present. The algorithm is to Classify of mean value and variance similarity is a local neighborhood of a pixel, the around pixels are divided into two types: similar or not similar. Similar pixels participate in filter algorithm, on the contrary do not. Thus, the algorithm effectively improves the filter speed. Simulation experimental results show that compared with MNF the algorithm greatly increased the computing speed, and the filter effect is not reduced.	computational problem;experiment;noise (electronics);noise reduction;peterson's algorithm;pixel;scalable vector graphics;simulation	Xiaojun Luo;Shixiu Wang;Bing Li;Junling Xu	2011	2011 Second International Conference on Digital Manufacturing & Automation	10.1109/ICDMA.2011.22	adaptive filter;kernel adaptive filter;machine learning;pattern recognition;mathematics;bilateral filter;statistics;salt-and-pepper noise	Robotics	56.3479520304348	-65.86294016118998	165466
25678d6b357c68d5c1f041061a534de178a693fd	generalization of lambert's reflectance model	computer graphic;rough surfaces;reflection models;reflection model;ray tracing;lambert s model;rough surface;brdf;moon reflectance	Lambert's model for body reflection is widely used in computer graphics. It is used extensively by rendering techniques such as radiosity and ray tracing. For several real-world objects, however, Lambert's model can prove to be a very inaccurate approximation to the body reflectance. While the brightness of a Lambertian surface is independent of viewing direction, that of a rough surface increases as the viewing direction approaches the light source direction. In this paper, a comprehensive model is developed that predicts body reflectance from rough surfaces. The surface is modeled as a collection of Lambertian facets. It is shown that such a surface is inherently non-Lambertian due to the foreshortening of the surface facets. Further, the model accounts for complex geometric and radiometric phenomena such as masking, shadowing, and interreflections between facets. Several experiments have been conducted on samples of rough diffuse surfaces, such as, plaster, sand, clay, and cloth. All these surface demonstrate significant deviation from Lambertian behavior. The reflectance measurements obtained are in strong agreement with the reflectance predicted by the model.	approximation;computer graphics;experiment;lambertian reflectance;mask (computing);radiosity (computer graphics);ray tracing (graphics);rough set;viewing cone	Michael Oren;Shree K. Nayar	1994		10.1145/192161.192213	ray tracing;oren–nayar reflectance model;photometric stereo;computer science;bidirectional reflectance distribution function;computer graphics (images)	Graphics	59.90140276969266	-52.17797613904779	165480
619b1e6a880ab63c7411a7c202f50e99ff716aab	masking noise in up-scaled video on large displays	noise reduction liquid crystal displays plasma displays spatial resolution large screen displays image converters signal resolution signal processing filtering tv;probability;video signal processing;television receivers;indexing terms;high pass filters;interference suppression;random noise;flat panel displays;noise reduction;noise reduction frequency liquid crystal displays plasma displays image converters modems colored noise signal processing filtering pollution;standard definition;spatial format conversion noise masking upscaled video large display large flat screen television set artifact reduction noise reduction;high pass filters random noise interference suppression image denoising television receivers video signal processing;image denoising;large displays;high pass filter noise masking up scaled video large displays flat screen television sets up converted standard definition material perceived noise display technology video processing;signal denoising video signal processing flat panel displays probability television receivers;signal denoising	Video noise is often perceived to be annoyingly visible on large flat-screen television sets, particularly if spatially up-converted standard definition material is displayed. We introduce a novel method to reduce the perceived noise using masking.	algorithm;error diffusion;experiment;image noise;liquid-crystal display;noise reduction;programmed data processor;snapshot (computer storage);standard-definition television;television set;video processing	Frank H. van Heesch;Michiel A. Klompenhouwer;Gerard de Haan	2005	2005 Digest of Technical Papers. International Conference on Consumer Electronics, 2005. ICCE.	10.1109/TCE.2005.1405735	standard-definition television;computer vision;electronic engineering;index term;computer science;sound masking;noise reduction;probability;high-pass filter;computer graphics (images)	EDA	60.45122073491825	-58.6852588975752	165515
89d45d69990cfe042bd7426394ccfd4001ba68e7	fractal interpolation for natural images	spline;fractals;splines mathematics interpolation brownian motion image processing;interpolation;image processing;application software;computer graphics;brownian motion;computational geometry;image interpolation;natural images;fractal interpolation;splines mathematics;fractal dimension;image generation;linear interpolation;fractional brownian motion fractal interpolation natural images linear interpolation spline interpolation;high frequency;fractional brownian motion;spline interpolation;fractals interpolation equations spline image processing image generation brownian motion computational geometry application software computer graphics	This paper proposes a fractal interpolation for natural images. Generally, linear interpolation and spline interpolation are used for image interpolation. However, an image interpolated b y the above conventional methods lose some high-frequency components of an original image. The loss of components lower fidelity of the interpolated images. Since the proposed method reduces the loss, an interpolated image generated b y the proposed method has higher fidelity than the one generated b y the conventional method. The reduction of the loss is realized b y using the fractional Brownian motion (FBM) in a process of the interpolation. The proposed method uses a characteristic that the fractal dimension is strongly correlated with a sense of roughness.	brownian motion;fractal compression;fractal dimension;linear interpolation;spline interpolation	Hiroyuki Honda;Miki Haseyama;Hideo Kitajima	1999		10.1109/ICIP.1999.817197	spline interpolation;demosaicing;spline;interpolation;computer vision;mathematical optimization;mathematical analysis;application software;bilinear interpolation;fractal;image processing;computational geometry;interpolation;computer science;stairstep interpolation;inverse quadratic interpolation;high frequency;brownian motion;bicubic interpolation;mathematics;geometry;fractional brownian motion;fractal dimension;linear interpolation;nearest-neighbor interpolation;computer graphics;multivariate interpolation;fractal compression;statistics;trilinear interpolation;image scaling	Graphics	58.54429185185192	-64.77166511560182	165563
70abd4b78e2a562c94c2b16e761f20df47ab77da	hi-fi printer characterization method using color correlation for gamut extension	gamut extension hi fi printer characterization method color correlation;printers;printers hi fi equipment image colour analysis image representation;image colour analysis;image representation;gamut;characterization;printers ink printing interpolation redundancy analytical models table lookup computer science character generation reflectivity;hi fi equipment;low light;correlation;correlation characterization gamut	This paper proposes a colorimetric characterization method using the color correlation between the colorants in a hi-fi printer. While several colorant combinations can be used to match a certain color stimulus in a hi-fi printing system with more than 3 colorants, conventional colorimetric characterization methods only use 3 or 4 colorants to render a color, thereby limiting the color representation. As a result, the gamut is limited as they give up the other combinations of colorants. Therefore, this paper proposes a method of colorimetric characterization that uses combinations of all the colorants. As such, certain colorant combinations are selected based on considering the correlation factor between the colorant amount distributions. The correlation factor also affects the interpolation error, as the colorants are not independent of each other. Consequently, the total gamut is increased in low lightness regions, and the colors are represented more accurately.	color;interpolation;printer (computing);printing	In-Su Jang;Chang-Hwan Son;Tae-Yong Park;Kyung-Woo Ko;Yeong-Ho Ha	2006	2006 International Conference on Image Processing	10.1109/ICIP.2006.312571	computer vision;mathematics;correlation;computer graphics (images)	Robotics	64.00621820086016	-59.74336475307493	165681
eed11afca3c042271228d33514ec8592a0915b02	cathode-ray-tube to reflection-print matching under mixed chromatic adaptation using rlab	turn off;cathode ray tube;mixed state;crts;image generation;color appearance;difference set;reflection;steady state		cathode ray tube;ray casting;rlab	Roy S. Berns;Heui-Keun Choh	1995	J. Electronic Imaging	10.1117/12.218927	cathode ray tube;computer vision;reflection;mathematics;optics;steady state;difference set	Vision	63.25681253841771	-55.199472019591546	165833
906b25f26a2eaf31a07aa1b5efeb04f7a2f808fe	accuracy of the spider model in decomposing layered surfaces	mathematical model computational modeling radio frequency image color analysis scattering optical imaging optical surface waves;opacity;reflectivity;optical surface waves;computer model;scattering;reflectivity computer vision image colour analysis opacity;computer vision;computational modeling;optical imaging;radio frequency;surface wave;image color analysis;image colour analysis;optical properties;mathematical model;multilayered surface spider model layered surface decomposition optical property surface reflectance light transmission lambert beer model layer opacity rgb space color distribution kubelka munk model computer vision algorithm	The surface of most natural objects is composed of two or more layers whose optical properties jointly determine the surface's overall reflectance. Light transmission through these layers can be approximated by using the Lambert-Beer (LB) model, which provides a good trade-off between the accuracy and simplicity to handle layer decomposition. Recently, a layer decomposition based on the LB-based model is proposed. Assuming surfaces with two layers, it estimates the reflectance of top and bottom layers, as well as the opacity of the top layer. The method introduces the “spider model”, which is named after the color distribution in the RGB space that resembles the shape of spiders. In this paper, we intend to verify the accuracy of the spider model and the optical model where it is based on (i.e., the LB-based model). We verify the LB-based model by comparing to the Kubelka-Munk (KM) model, which has previously been shown to be reliably accurate. The benefits of layer decomposition are easy to notice. First, many computer vision algorithms assume a single layer, and tend to fail when encountering multi-layered surfaces. Second, knowing the optical properties of each layer can provide further knowledge of the target objects.	approximation algorithm;computer vision;ising model;lambert's cosine law;lattice boltzmann methods;next-generation network;norm (social)	Tetsuro Morimoto;Robby T. Tan;Rei Kawakami;Katsushi Ikeuchi	2011	2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)	10.1109/ICCVW.2011.6130336	computer simulation;computer vision;opacity;surface wave;optical imaging;mathematical model;reflectivity;scattering;computational model;radio frequency	Vision	60.21869697424122	-52.33896315727989	165909
b20ad6fe315f801047f1b0805677fd2041ee8c8a	estimating cumulus cloud shape from a single image		Cumulus cloud is a typical kind of low-altitude cloud and bears detailed appearance. However, construct a cumulus cloud shape from image remains a challenging task. In previous works, cloud thickness is first calculated from intensities of the pixels and then cloud surface shape is obtained based on symmetry assumption. Given an image, an optimization problem is formulated based on the shape from shading(SFS) algorithm with unknown illumination. This paper addresses this problem and presents a simple and effective method which estimates the surface shape directly by leveraging the rich details of the cloud surface. In this paper, we propose two constraints: the boundary constraint and relative height constraint. Meanwhile, a multi-scale optimization technique is utilized and then back surface of the cloud is constructed. The experimental results show that our method can generate natural and realistic cumulus cloud shape with details like the image.	cumulus	Yiming Zhang;Zili Zhang;Jiayue Hou;Xiaohui Liang	2017		10.1007/978-981-10-7389-2_15	pixel;computer vision;photometric stereo;cloud computing;effective method;cloud height;optimization problem;artificial intelligence;mathematics	Vision	58.60721939131227	-52.202679760981994	165990
02237d4759358b3f08fbcdd52378b56e76f98977	time-constrained photography	photonics;uncertainty;optical noise;photography;image restoration;layout;upper bound;pixel;photography cameras optical noise optical sensors uncertainty spatial resolution layout monte carlo methods upper bound frequency;lenses;optical sensors;signal to noise ratio;frequency;monte carlo methods;cameras;spatial resolution	Capturing multiple photos at different focus settings is a powerful approach for reducing optical blur, but how many photos should we capture within a fixed time budget? We develop a framework to analyze optimal capture strategies balancing the tradeoff between defocus and sensor noise, incorporating uncertainty in resolving scene depth. We derive analytic formulas for restoration error and use Monte Carlo integration over depth to derive optimal capture strategies for different camera designs, under a wide range of photographic scenarios. We also derive a new upper bound on how well spatial frequencies can be preserved over the depth of field. Our results show that by capturing the optimal number of photos, a standard camera can achieve performance at the level of more complex computational cameras, in all but the most demanding of cases. We also show that computational cameras, although specifically designed to improve one-shot performance, generally benefit from capturing multiple photos as well.	circuit restoration;computation;gaussian blur;image noise;monte carlo integration;monte carlo method	Samuel W. Hasinoff;Kiriakos N. Kutulakos;Frédo Durand;William T. Freeman	2009	2009 IEEE 12th International Conference on Computer Vision	10.1109/ICCV.2009.5459269	layout;image restoration;computer vision;photonics;image resolution;uncertainty;computer science;photography;frequency;lens;upper and lower bounds;signal-to-noise ratio;pixel;statistics;monte carlo method;computer graphics (images)	Vision	59.753956421280556	-57.47584733629722	166114
878b8ccbc74661ed79f0e2025606d0fe731f0dd4	subjective and objective quality assessment of image: a survey	reduced reference rr;full reference fr;high dynamic range hdr images;image quality assessment iqa;no reference nr	With the increasing demand for image-based applications, the efficient and reliable evaluation of image quality has increased in importance. Measuring the image quality is of fundamental importance for numerous image processing applications, where the goal of image quality assessment (IQA) methods is to automatically evaluate the quality of images in agreement with human quality judgments. Numerous IQA methods have been proposed over the past years to fulfill this goal. In this paper, a survey of the quality assessment methods for conventional image signals, as well as the newly emerged ones, which includes the high dynamic range (HDR) images, is presented. A thorough explanation of the subjective and objective IQA, and their classification is provided. Six widely used subjective quality datasets, and performance measures are overviewed. Emphasis is given to the full-reference image quality assessment (FRIQA) methods, and 9 often-used quality measures (including mean squared error (MSE), structural similarity index (SSIM), multi-scale structural similarity index (MS-SSIM), visual information fidelity (VIF), most apparent distortion (MAD), feature similarity measure (FSIM), feature similarity measure for color images (FSIMC), dynamic range independent measure (DRIM), and tone-mapped images quality index (TMQI)) are thoroughly described. Moreover, the performance and computation time of these metrics on four subjective quality datasets are evaluated.	algorithm;computation;data compression;digital image;distortion;high dynamic range;image processing;image quality;mad;mean squared error;medical imaging;similarity measure;stereoscopy;structural similarity;time complexity;tone mapping	Pedram Mohammadi;Abbas Ebrahimi-Moghadam;Shahram Shirani	2014	CoRR		computer vision;multimedia;computer graphics (images)	Graphics	62.15800340373962	-63.97183474812284	166118
96f01ecad9fa969480df9c6fbcee734e28beb283	optimal exposure compression for high dynamic range content	qa76 electronic computers computer science computer software	High dynamic range (HDR) imaging has become one of the foremost imaging methods capable of capturing and displaying the full range of lighting perceived by the human visual system in the real world. A number of HDR compression methods for both images and video have been developed to handle HDR data, but none of them has yet been adopted as the method of choice. In particular, the backwards-compatible methods that always maintain a stream/image that allow part of the content to be viewed on conventional displays make use of tone mapping operators which were developed to view HDR images on traditional displays. There are a large number of tone mappers, none of which is considered the best as the images produced could be deemed subjective. This work presents an alternative to tone mapping-based HDR content compression by identifying a single exposure that can reproduce the most information from the original HDR image. This single exposure can be adapted to fit within the bit depth of any traditional encoder. Any additional information that may be lost is stored as a residual. Results demonstrate quality is maintained as well, and better, than other traditional methods. Furthermore, the presented method is backwards-compatible, straightforward to implement, fast and does not require choosing tone mappers or settings.	backward compatibility;data compression;encoder;foremost;high dynamic range	Kurt Debattista;Thomas Bashford-Rogers;Elmedin Selmanovic;Ratnajit Mukherjee;Alan Chalmers	2015	The Visual Computer	10.1007/s00371-015-1121-z	computer vision;computer science;multimedia;computer graphics (images)	Graphics	61.38032728296747	-61.57490427040917	166242
ef0880cb8f5fb4f55f45ae642091aa956eef434d	reduced-reference video quality assessment of compressed video sequences	data compression;gaussian processes;image matching;video coding;image representation;feature extraction measurement video sequences discrete cosine transforms visualization histograms quantization;video quality monitoring system reduced reference video quality assessment compressed video sequences rr vqa spatial information loss temporal statistical characteristics energy variation descriptor evd individual encoded frame quantization process human visual system hvs generalized gaussian density function ggd function natural statistics interframe histogram distribution city block distance cbd adjacent frames temporal interframe relationship subjective quality video database representative rr video quality metric full reference vqa signal to noise ratio structure similarity index subjective rating matching hvs perception video sequence representation ancillary data channel;encoding;video quality assessment vqa energy variation descriptor evd generalized gaussian density ggd human visual system hvs reduced reference rr;video coding data compression encoding gaussian processes image matching image representation image sequences;image sequences	In this paper, a novel reduced-reference (RR) video quality assessment (VQA) is proposed by exploiting the spatial information loss and the temporal statistical characteristics of the interframe histogram. From the spatial perspective, an energy variation descriptor (EVD) is proposed to measure the energy change of each individual encoded frame, which results from the quantization process. Besides depicting the energy change, EVD can further simulate the texture masking property of the human visual system (HVS). From the temporal perspective, the generalized Gaussian density (GGD) function is employed to capture the natural statistics of the interframe histogram distribution. The city-block distance (CBD) is used to calculate the histogram distance between the original video sequence and the encoded one. For simplicity, the difference image between adjacent frames is employed to characterize the temporal interframe relationship. By combining the spatial EVD together with the temporal CBD, an efficient RR VQA is developed. Evaluation on the subjective quality video database demonstrates that the proposed method outperforms the representative RR video quality metric and the full-reference VQAs, such as peak signal-to-noise ratio and structure similarity index in matching subjective ratings. This means that the proposed metric is more consistent with the HVS perception. Furthermore, as only a small number of RR features are extracted for representing the original video sequence (each frame requires only one parameter for describing EVD and three parameters for recording GGD), the RR features can be embedded into the video sequences or transmitted through the ancillary data channel, which can be used in the video quality monitoring system.	channel (communications);component-based software engineering;distortion;embedded system;histogram equalization;human visual system model;peak signal-to-noise ratio;rapid refresh;round-robin scheduling;simulation;taxicab geometry	Lin Ma;Songnan Li;King Ngi Ngan	2012	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2012.2202049	data compression;computer vision;speech recognition;video quality;pattern recognition;gaussian process;mathematics;encoding;statistics;multiview video coding	Vision	62.005443104422554	-65.73362890346648	166336
8314c2881c3b8d2a4a83130f9fc4c4ec8c21cedb	quantization of color images for display/printing on limited color output devices	printing;quantization;affichage;cuantificacion;image processing;visualizacion;input output equipment;computer graphics;procesamiento imagen;quantification;traitement image;equipement entree sortie;display;impression;equipo entrada salida;imagen color;impresion;grafico computadora;infographie;image couleur;color image	A large number of output devices in use today are either bilevel or can produce only a limited number of display levels (gray-scale or color). Most color graphics terminals conforming to Enhanced Graphics Adapter (EGA), Professional Graphics Adapter (PGA), or Video Graphics Array (VGA) standards can display from 16–256 colors, whereas real-world (externally acquired) images constitute typically 16M colors. In this paper, a new color quantization algorithm has been proposed which maps an original image into an output image with a limited number of colors, while still preserving the image quality. The algorithm itself is based on the concepts of vector quantization where a color vector is defined by red, green, and blue components and, based on a random sampling of the input image, a color mapping table is generated. The random sampling provides an estimate of the color distribution of the input image, which is then further combined by a clustering technique to derive the desired number of output colors. A mapping process results in a limited-color output image which is optionally preprocessed (in cases where the number of output colors is very small) by a pseudo-random dithering algorithm rendering a high-quality output. This postprocessing step is particularly useful in images with very few output colors, e.g., 16. Through examples, it is shown that input images with over 16M colors can be easily displayed in as few as 16 colors, with negligible degradation in quality.	output device;printing	Sudhir S. Dixit	1991	Computers & Graphics	10.1016/0097-8493(91)90057-O	color co-site sampling;alpha compositing;color gradient;color histogram;false color;rgb color model;computer vision;color model;color quantization;color depth;color image;quantization;binary image;channel;image processing;computer science;high color;multimedia;color balance;color space;web colors;8-bit color;computer graphics;computer graphics (images)	Vision	60.83574466301933	-61.11218823825943	166476
176369748cb1a1fb2ccc05603df2dd88772c3052	a novel approach for colorization of a grayscale image using soft computing techniques		of grayscale image is a process to convert a grayscale image into a color one. Few research works reported in literature on this but there is hardly any generalized method that successfully colorizes all types of grayscale image. This study proposes a novel grayscale image colorization method using a reference color image. It takes the grayscale image and the type of the query image as input. First, it selects reference image from color image database using histogram index of the query image and histogram index of luminance channel of color images of respective type. Once the reference image is selected, four features are extracted for each pixel of the luminance channel of the reference image. These extracted features as input and chrominance blueCb value as target value forms the training dataset for Cb channel. Similarly training dataset for chrominance redCr channel is also formed. These extracted features of the reference image and associated chrominance values are used to train two artificial neural networkANN-one for Cb and one for Cr channel. Then, for each pixel of the of query image, same four features are extracted and used as input to the trained ANN to predict the chrominance values of the query image. Thus predicted chrominance values along with the original luminance values of the query image are used to construct the colorized image. The experiment has been conducted on images collected from different standard image database i.e. FRAV2D, UCID.v2 and images captured using standard digital camera etc. These images are initially converted into grayscale images and then the colorization method was applied. For performance evaluation, PSNR between the original color image and newly colorized image is calculated. PSNR shows that the proposed method better colorizes than the recently reported methods in the literature. Beside this, Colorization Turing test was conducted asking human subject to choose the image closer to the original color image among the colorized images using proposed algorithm and recently reported methods. In 80% of cases colorized images using the proposed method got selected.	grayscale;soft computing	Abul Hasnat;Santanu Halder;Debotosh Bhattacharjee;Mita Nasipuri	2017	IJMDEM	10.4018/IJMDEM.2017100102	pattern recognition;grayscale;pixel;computer vision;artificial intelligence;computer science;color image;digital camera;chrominance;ycbcr;color quantization;histogram	Vision	58.72259775638172	-63.651216533026506	166562
5a9c10071fd56abd3274a6e50a5839837fc00a4f	a non-singleton interval type-2 fuzzy logic system for universal image noise removal using quantum-behaved particle swarm optimization	gaussian noise;fls nonsingleton interval type 2 fuzzy logic system universal image noise removal quantum behaved particle swarm optimization gaussian noise impulse noise mgin image restoration it2 fuzzy logic system;frequency selective surface;training;impulse noise;image restoration;awgn;fuzzy logic;particle swarm optimizer;particle swarm optimisation fuzzy logic gaussian noise image denoising image restoration impulse noise;particle swarm optimization;fuzzy logic system;image denoising;particle swarm optimisation;algorithm design and analysis;noise removal;training algorithm;frequency selective surfaces awgn particle swarm optimization training algorithm design and analysis fuzzy logic;frequency selective surfaces	Removing Mixed Gaussian and Impulse Noise (MGIN) is considered to be very important in the domain of image restoration, but it is a somewhat more challenging topic than removing pure Gaussian or impulse noise. Therefore, relatively fewer works have been published in this area. This paper pro poses a Non-Singleton Interval Type-2 (IT2) Fuzzy Logic System (FLS) for MGIN removal, explains how it can be designed based on a Quantum-behaved Particle Swarm Optimization algorithm, and shows that it provides both quantitatively and visually much better results compared to other often-used non-fuzzy techniques as well as its Type-1 and singleton IT2 counterparts.	algorithm;circuit restoration;digital image;free library of springfield township;fuzzy logic;gradient descent;image noise;image restoration;impulse noise (audio);nsb/appstudio;noise reduction;particle swarm optimization;quantum	Daoyuan Zhai;Minshen Hao;Jerry M. Mendel	2011	2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011)	10.1109/FUZZY.2011.6007505	fuzzy logic;gaussian noise;image restoration;additive white gaussian noise;algorithm design;mathematical optimization;tunable metamaterials;impulse noise;computer science;machine learning;mathematics;particle swarm optimization	Robotics	56.39034483419649	-65.86394340761139	166670
6074c43d6487e9cbcf27c43f41d56c352f6a04c8	the research and implementation of super-resolution reconstruction for multi-frame blurring images	interpolation;low resolution;motion estimation;ibp;image reconstruction;interpolation method;super resolution;high resolution imager	In this paper, we design a super-resolution restoration system for multi-frame blurring images. There are two modules in this system, namely that motion estimation and reconstruction of super-resolution. Firstly, the motion parameters are estimated by a frequency algorithm; then a high-resolution image is reconstructed from low-resolution images based on the motion parameters by four different super-resolution reconstruction methods. The experimental results show that the restoration effect of POCS is better than the two restoration effects obtained by IBP method and the one presented in reference [9]. The bi-cubic interpolation method can get the best restoration effect based on the same motion estimation method under the condition that all low-resolution images have same blurring reason and noise feature. It can be known that bi-cubic interpolation reconstruction method is suitable than the other three methods for multi-frame images with same blurring reason and noise.	algorithm;bicubic interpolation;circuit restoration;cubic hermite spline;cubic function;image resolution;motion estimation;super-resolution imaging	Rong Wang;Xiaogang Yang	2011		10.1145/2071639.2071654	iterative reconstruction;image restoration;computer vision;image resolution;interpolation;computer science;motion estimation;superresolution;computer graphics (images)	Vision	58.434781022696825	-59.37322230037448	166732
d6c04505e1bfd184c00440e55893ec39a389009a	quality assessment for stereoscopic image based on dct frequency information	perceptual quality assessment metric stereoscopic image dct frequency information discrete cosine transform coding visual sensitivity function frequency domain analysis human visual system hvs contrast sensitivity function csf left reference block block matching method 3d image video distorted stereoscopic images region of interest mechanism roi mechanism;image coding;psnr;measurement;stereo image processing measurement quality assessment discrete cosine transforms three dimensional displays image quality psnr;image matching;frequency domain analysis;region of interest roi quality assessment dct coding stereoscopic image video human visual system hvs;region of interest roi;quality assessment;human visual system hvs;three dimensional displays;discrete cosine transforms;image quality;stereo image processing;stereo image processing discrete cosine transforms frequency domain analysis image coding image matching;dct coding;stereoscopic image video	In this paper, we propose an efficient perceptual quality assessment for stereoscopic image based on Discrete Cosine Transform (DCT) coding. In this algorithm, visual sensitivity for human beings to perceive stereoscopic image in frequency domain has been considered. Excepting for this important property related to Human Visual System (HVS), another property is also taken into account: contrast sensitivity function (CSF). In our algorithm, for the left reference block, its most matching block in the left view is found by using the block matching method, and the right reference block is searched based on disparity information as a vital feature in 3D image/video. In addition, its matching block for the right reference one is also finished. The processing mode is completed for the original and distorted stereoscopic images. Then, these blocks are processed by DCT and calculated by modified PSNR in view of CSF and Region of Interest (ROI) mechanism (visual sensitivity). The calculated results have demonstrated its validity compared with the subjective test results. And the proposed perceptual quality assessment metric outperforms other currently popular metrics.	algorithm;binocular disparity;discrete cosine transform;human visual system model;peak signal-to-noise ratio;region of interest;stereoscopy	Chao Sun;Xingang Liu;Kai Kang;Laurence Tianruo Yang	2013	2013 12th IEEE International Conference on Trust, Security and Privacy in Computing and Communications	10.1109/TrustCom.2013.168	computer vision;mathematics;multimedia;computer graphics (images)	Vision	62.81278067896225	-63.743901720759126	166835
607c799ef66fb062aeb8e05c6d5a0a007f7c346e	aligning projection images from binary volumes	automatic;binary tomography;prior knowledge;alignment	In tomography, slight differences between the geometry of the scanner hardware and the geometric model used in the reconstruction lead to alignment artifacts. To exploit high-resolution detectors used in many applications of tomography, alignment of the projection data is essential. Markerless alignment algorithms are the preferred choice over alignment with markers, in case a fully automatic tomography pipeline is required. Moreover, marker based alignment is often not feasible or even possible. At the same time, markerless alignment methods often fail in scenarios where only a small number of projections are available. In this case, the angular separation between projection images is large and therefore the correlation between them is low. This is a property that most markerless algorithms rely on. The intermediate reconstruction problem of alignment by projection matching is highly underdetermined in the limited data case. Therefore, we propose a projection matching method that incorporates prior knowledge of the ground truth. We focus on reconstructing binary volumes. A discrete tomography algorithm is employed to generate intermediate reconstructions. This type of reconstruction algorithm does not rely heavily on correlated projection images. Our numerical results suggest that alignment using discrete tomography projection matching produces much better results in the limited angle case, when compared to a projection matching method that employs an algebraic reconstruction method.	align (company);angularjs;discrete tomography;finite difference;genetic algorithm;geometric modeling;gradient;ground truth;image resolution;levenberg–marquardt algorithm;linear algebra;matrix regularization;numerical analysis;reconstruction conjecture;sap business one;semiconductor detector;sensor	Folkert Bleichrodt;Jan De Beenhouwer;Jan Sijbers;Kees Joost Batenburg	2014	Fundam. Inform.	10.3233/FI-2014-1110	computer vision;mathematical optimization;oblique projection;mathematics;geometry;automatic transmission	Vision	54.20740465863556	-52.86922572116729	167203
388d61473da2d411fe2544b5162fce863b010737	high-quality scanning using time-of-flight depth superresolution	image sampling;image scanners;time of flight;image resolution;depth upsampling method;time of flight camera;x y resolution;image edge detection;three dimensional displays;image color analysis;image colour analysis;pixel;video frame rate;high quality 3d scanning;color image superresolution;image scanners image colour analysis image resolution image sampling;depth upsampling method high quality 3d scanning time of flight depth superresolution time of flight camera video frame rate color image superresolution x y resolution;cameras;noise;time of flight depth superresolution;signal resolution cameras image resolution colored noise layout color robustness statistics pulse measurements pulse modulation	Time-of-flight (TOF) cameras robustly provide depth data of real world scenes at video frame rates. Unfortunately, currently available camera models provide rather low X-Y resolution. Also, their depth measurements are starkly influenced by random and systematic errors which renders them inappropriate for high-quality 3D scanning. In this paper we show that ideas from traditional color image superresolution can be applied to TOF cameras in order to obtain 3D data of higher X-Y resolution and less noise. We will also show that our approach, which works using depth images only, bears many advantages over alternative depth upsampling methods that combine information from separate high-resolution color and low-resolution depth data.	3d scanner;algorithm;color image;image resolution;noise (electronics);rendering (computer graphics);super-resolution imaging;time-of-flight camera;upsampling	Sebastian Schuon;Christian Theobalt;James E. Davis;Sebastian Thrun	2008	2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2008.4563171	computer vision;time of flight;image resolution;computer science;noise;frame rate;pixel;computer graphics (images)	Vision	59.11997422373636	-57.70650078654594	167272
1aa792cb23d2b658da07e7d5c7b0d85bf1e5c3cd	a novel approach of harris corner detection of noisy images using adaptive wavelet thresholding technique		"""In this paper we propose a method of corner detection for obtaining features which is required to track and recognize objects within a noisy image. Corner detection of noisy images is a challenging task in image processing. Natural images often get corrupted by noise during acquisition and transmission. Though Corner detection of these noisy images does not provide desired results, hence de-noising is required. Adaptive wavelet thresholding approach is applied for the same. A corner is a point for which there are two dominant and different edge directions in the vicinity of the point. In simpler terms, a corner can be defined as the intersection of two edges, where an edge is a sharp change in image brightness. Generally termed as interest point detection, corner detection is a methodology used within computer vision systems to obtain certain kinds of features from a given image. The initial operator concept of """" points of interest """" in an image, which could be used to locate matching regions in different images, was developed by Hans P. Moravec in 1977. The Moravec operator is considered to be a corner detector because it defines interest points as points where there are large intensity variations in all directions. For a human, it is easier to identify a """" corner """" , but a mathematical detection is required in case of algorithms. Chris Harris and Mike Stephens in 1988 improved upon Moravec's corner detector by taking into account the differential of the corner score with respect to direction directly, instead of using shifted patches. Moravec only considered shifts in discrete 45 degree angles whereas Harris considered all directions. Harris detector has proved to be more accurate in distinguishing between edges and corners. He used a circular Gaussian window to reduce noise. Still in cases of noisy Images, it's difficult to find out the exact number of corners. One of the most conventional ways of image de-noising is using linear filters like Wiener filter. In the presence of additive noise the resultant noisy image, through linear filters, gets blurred and smoothed with poor feature localization and incomplete noise suppression. To overcome these limitations, nonlinear filters have been proposed like adaptive wavelet thresholding approach. Adaptive wavelet thresholding approach gives a very good result for the same. Wavelet Transformation has its own excellent space-frequency localization property and thresholding removes coefficients that are inconsiderably relative to some adaptive data-driven threshold."""	additive white gaussian noise;algorithm;coefficient;complex adaptive system;computer vision;corner detection;digital signal processing;hans moravec;harris affine region detector;image processing;interest point detection;network security;noise reduction;nonlinear system;point of interest;resultant;smoothing;thresholding (image processing);utility functions on indivisible goods;wavelet;wiener filter;window function;zero suppression	Nilanjan Dey;Pradipti Nandi;Nilanjana Barman	2012	CoRR		corner detection;computer vision;mathematical optimization;pattern recognition;mathematics;interest point detection;statistics	Vision	53.82356403711315	-64.87824370772265	167326
ab95ded5afcec4797721e8c459a9809e31f1f334	an adaptive algorithm for improving the fractal image compression (fic)	range block;image compression;fractal;encoding time;variance	In this paper an adaptive algorithm is proposed to reduce the long time that has been taken in the Fractal Image Compression (FIC) technique. This algorithm worked on reducing the number of matching operations between range and domain blocks by reducing both of the range and domain blocks needed in the matching process, for this purpose, two techniques have been proposed; the first one is called Range Exclusion (RE), in this technique variance factor is used to reduce the number of range blocks by excluding ranges of the homogenous or flat regions from the matching process; the second technique is called Reducing the Domain Image Size (RDIZ), it is responsible for the reduction of the domain by minimizing the Domain Image Size to 1/16 instead of 1/4 of the original image size used in the traditional FIC. This in turn will affect the encoding time, compression ratio and the reconstructed image quality. For getting best results, the two techniques are coupled in one algorithm; the new algorithm is called (RD-RE). The tested (256x256) gray images are partitioned into fixed (4x4) blocks and then compressed using visual C++ 6.0 code. The results show that RE technique is faster and gets more compression ratio than the traditional FIC and keeping a high reconstructed images quality while RDRE is faster and it gets higher compression ratio than RE but with slight loss in the reconstructed image quality.	adaptive algorithm;c++;data compression ratio;fractal compression;image compression;image quality;image resolution;ruby document format	Taha Mohammed Hasan;Xiangqian Wu	2011	Journal of Multimedia	10.4304/jmm.6.6.477-485	computer vision;data compression ratio;fractal;image compression;theoretical computer science;fractal transform;variance;algorithm;computer graphics (images)	EDA	57.96913516999095	-63.05071830441932	167733
001a86188d563c89c8ebee89d5ecbda598aecac8	an approach to improve the quality of infrared images of vein-patterns	hybrid cumulative histogram equalization;infrared rays;adaptive contrast enhancement;veins;noise detection;noise removal	This study develops an approach to improve the quality of infrared (IR) images of vein-patterns, which usually have noise, low contrast, low brightness and small objects of interest, thus requiring preprocessing to improve their quality. The main characteristics of the proposed approach are that no prior knowledge about the IR image is necessary and no parameters must be preset. Two main goals are sought: impulse noise reduction and adaptive contrast enhancement technologies. In our study, a fast median-based filter (FMBF) is developed as a noise reduction method. It is based on an IR imaging mechanism to detect the noisy pixels and on a modified median-based filter to remove the noisy pixels in IR images. FMBF has the advantage of a low computation load. In addition, FMBF can retain reasonably good edges and texture information when the size of the filter window increases. The most important advantage is that the peak signal-to-noise ratio (PSNR) caused by FMBF is higher than the PSNR caused by the median filter. A hybrid cumulative histogram equalization (HCHE) is proposed for adaptive contrast enhancement. HCHE can automatically generate a hybrid cumulative histogram (HCH) based on two different pieces of information about the image histogram. HCHE can improve the enhancement effect on hot objects rather than background. The experimental results are addressed and demonstrate that the proposed approach is feasible for use as an effective and adaptive process for enhancing the quality of IR vein-pattern images.	biometrics;bit plane;computation (action);fast fourier transform;histogram equalization;hypochondroplasia (disorder);image histogram;image noise;impulse noise (audio);median filter;noise reduction;peak signal-to-noise ratio;physical object;pixel;plant roots;population parameter;preprocessor;radix sort;sorting;two-hybrid system techniques;verification and validation;benefit;brightness	Chih-Lung Lin	2011		10.3390/s111211447	median filter;computer vision;electronic engineering;infrared;histogram matching;optics;adaptive histogram equalization;physics;quantum mechanics;salt-and-pepper noise	Vision	56.94994135131483	-64.85275713011777	167899
5385e0ca1afdd9da34b408e66ae7025f4939274b	a single algorithm combining exposure and focus fusion	tone mapping;focus fusion;image segmentation;laplace equation;image fusion;real time systems data acquisition image fusion image representation image segmentation;high dynamic range imaging;laplace equations;realtime system;medical image;image color analysis;image representation;heuristic algorithms;medical imaging;lenses;dynamic range;medical imaging image fusion exposure fusion high dynamic range imaging focus fusion;dynamic range laplace equations lenses image color analysis heuristic algorithms cameras real time systems;low dynamic range;high dynamic range;exposure fusion;data acquisition;cameras;heuristic algorithm;realtime system implementation lighting conditions spatial depth tone mapping true high dynamic range representation exposure fusion focus fusion varied exposure low dynamic range images image fusion image segmentation scene based data acquisition techniques input data computational resources;real time systems	In scenes of significantly varying lighting conditions, under and over exposed regions can suffer from a loss of information. Similarly, the presence of spatial depth within a scene can cause some image regions to be out of focus. Several methods of addressing these issues exist, including tone mapping for true high dynamic range representation and exposure fusion for combining varied-exposure low dynamic range images, as solutions to the former, and image fusion and segmentation etc. to address the latter. This paper proposes an overhauled method of exposure fusion that solves the exposure and focus problems simultaneously, achieving a well-exposed, all-in-focus result. Smart, scene-based data acquisition techniques for reducing both required input data and computational resources are discussed. A platform for a realtime system implementation is also presented.	algorithm;computational resource;data acquisition;high dynamic range;high-dynamic-range rendering;image fusion;information;real-time computing;tone mapping	Azhar A. Sufi;David C. Zhang;Gooitzen S. van der Wal	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6115680	medical imaging;heuristic;computer vision;dynamic range;tone mapping;simulation;computer science;lens;image segmentation;image fusion;data acquisition;dense plasma focus;laplace's equation;computer graphics (images)	Robotics	56.54997284770504	-57.11776538346975	168390
585598821780d08dd323494d16bd7350ad1a44c3	a post-processing algorithm using histogram-driven anisotropic diffusion	image coding;data compression;anisotropic diffusion;visual quality;diffusion coefficient;anisotropic magnetoresistance image coding video compression image restoration histograms adaptive filters filtering transform coding bit rate degradation;video coding;gradient methods;visual quality post processing algorithm histogram driven anisotropic diffusion coding artifact reduction jpeg compressed images mpeg 4 compressed videos image sequences video sequences gradient distribution;quality measures;gradient methods data compression image coding video coding;compressed video	We propose a new post-processing algorithm for reducing coding artifacts in compressed image and video sequences. The algorithm is based on an anisotropic diffusion process using a histogram driven diffusion coefficient. The new method can adaptively adjust the diffusion degree according to the gradient distribution for different images to improve the visual quality. Experiments on JPEG compressed images and MPEG-4 compressed videos show that the proposed method can achieve consistent improvement in terms of subjective and objective quality measures.	algorithm;anisotropic diffusion;coefficient;gradient;image gradient;image processing;jpeg;video post-processing	Susu Yao;Keng Pang Lim;Xiao Lin;Susanto Rahardja	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1465565	data compression;computer vision;computer science;theoretical computer science;fick's laws of diffusion;multimedia;context-adaptive binary arithmetic coding;anisotropic diffusion;multiview video coding	Vision	57.665942723708525	-63.32738465585821	168436
c7ed30c52172809aac4985c4421ea83621b65f63	a novel technique for reducing demosaicing artifacts	boats abstracts refining;optical filters cameras image capture image colour analysis image denoising image filtering image reconstruction image representation interpolation;off line post processing demosaicing artifact reduction image capturing image representation digital camera color filter array image pixel measurement demosaicing reconstruction image visual quality color interpolation	Demosaicing is the process of reconstructing the full dimension representation of an image captured by a digital camera with a color filter array. The color filter array allows for only one color measuring for each pixel and the missing two colors have to be estimated. In literature many demosaicing techniques have been proposed but the reconstructed images are affected by some visible and annoying artifacts. In this paper we propose a new effective algorithm to reduce these artifacts. This algorithm improves the performances of the demosaicing reconstruction, increasing the visual quality of the resulting images. It can be applied directly after the color interpolation, or as an off-line post-processing to improve the image provided by the digital camera.	algorithm;algorithmic efficiency;bayer filter;color filter array;computation;demosaicing;digital camera;interpolation;iteration;monochrome;online and offline;performance;pixel;video post-processing	Daniele Menon;Stefano Andriani;Giancarlo Calvagno	2006	2006 14th European Signal Processing Conference		image quality;demosaicing;color histogram;rgb color model;computer vision;color filter array;feature detection;image resolution;color image;image gradient;binary image;image processing;bayer filter;digital image processing;mathematics;multimedia;digital image;computer graphics (images)	Robotics	58.610001769156106	-59.797104041696805	168442
1c1b90bce24654bcf356890de550d5ca11969e45	automatic dynamic texture transformation based on a new motion coherence metric	histograms;color;video editing dynamic texture transformation motion coherence analysis motion template matching special effects;video signal processing dynamic programming image colour analysis image matching image motion analysis image sequences image texture;videos dynamics histograms motion segmentation coherence image color analysis color;motion segmentation;dynamics;image color analysis;patch matching automatic dynamic texture transformation new motion coherence metric dynamic texture appearances video color appearances video motion appearances physical models motion texture dynamic texture transformation algorithm video sequences 3d patch creation motion coherence analysis;coherence;videos	Changing dynamic texture appearances can create new looks in both the motion and color appearances of videos. Dynamic textures with sophisticated shape and motion appearance are difficult to represent by physical models and are difficult to predict, especially for transformation to new motion texture. We propose a dynamic texture transformation algorithm for the video sequences based on the motion coherence of patches. We successfully apply the technology in many special effect videos, using the interactive tool we developed. In this paper, we address the issues of 3-D patch creation, motion coherence analysis, and patch matching for the dynamic texture transformation. The main contribution includes two issues. First, we propose a new metric for evaluating motion coherence with solid tests to justify the usefulness (close to human eye perception). Second, the proposed algorithm for an automatic dynamic texture transformation only needs users to segment textures on the first frame using an optional threshold to identify the texture area. The rest of the process is completed automatically. The experimental results show that the motion coherence index is effectively used to find the coherent motion region for patch matching and transformation. The experimental results, test data, source code, and system demonstration videos are posted at http://video.minelab.tw/DTT/index.html.	algorithm;central processing unit;coherence (physics);connected component (graph theory);connected-component labeling;emoticon;integrated circuit layout design protection;interactivity;markov random field;mcgurk effect;pixel;polynomial basis;quadratic function;semiconductor industry;simulation;stationary process;stochastic process;test data;texture mapping;texture synthesis;tulip;waterfall model	Kanoksak Wattanachote;Timothy K. Shih	2016	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2015.2469552	image texture;computer vision;dynamics;coherence;quarter-pixel motion;motion estimation;histogram;mathematics;multimedia;motion field;statistics;computer graphics (images)	Vision	57.99859028021759	-54.79462251248997	168666
7df29401b6b288b018d18708d5ad99c3e574c31b	hierarchical guidance strategy and exemplar-based image inpainting		To solve the issue that it is difficult to maintain the consistency of linear structures when filling large regions by the exemplar-based technique, a hierarchical guidance strategy and exemplar-based image inpainting technique is proposed. The inpainting process is as follows: (i) the multi-layer resolution images are firstly acquired through decomposing of the pyramid on the target image; (ii) the top-layer inpainted image, the beginning of the inpainting from the top layer, is generated by the exemplar-based technique; (iii) there is a combined result between the next layer of the top image and the up-sampling output on the top-layer inpainted image, and the target regions are filled with information as guidance data; (iv) this process is repeated until the inpainting of all layers have been completed. Our results were compared to those obtained by existing techniques, and our proposed technique maintained the consistency of linear structures in a visually plausible way. Objectively, we choose SSIM (structural similarity index measurement) and PSNR (peak signal-to-noise ratio) as the measurement indices. Since the values of SSIM are well reflected when compared with other techniques, our technique clearly demonstrated that our approach is better able to maintain the consistency of linear structures. The core of our algorithm is to fill large regions whether they are synthesis images or real-scene photographs. It is easy to apply in practice, with the goal of having plausible inpainted image.	algorithm;inpainting;layer (electronics);peak signal-to-noise ratio;sampling (signal processing);structural similarity	Huaming Liu;Guanming Lu;Xuehui Bi;Weilan Wang	2018	Information	10.3390/info9040096	inpainting;data mining;computer science;combined result;pyramid;artificial intelligence;pattern recognition	ML	56.95226739698114	-60.12637098425691	168671
4ff1e336bce59cf4197aedc36b91136c6bb6f59c	blind quality assessment of tone-mapped images via analysis of information, naturalness, and structure	astronomical fields;tone mapping;tone mapped image database;digital image processing;statistical naturalness;image processing;medical fields;tone mapping operators;structural preservation;ldr;image quality assessment iqa;naturalness analysis;brightness;indexes;visualization;no reference nr;visual databases image processing;disease diagnosis;tone mapped image database blind quality assessment information analysis naturalness analysis structure analysis high dynamic range imaging techniques hdr display devices fault detection disease diagnosis astronomical fields medical fields digital image processing computer vision communities hdr imaging devices tone mapping operators tmo low dynamic range images ldr;structural preservation high dynamic range hdr tone mapping image quality assessment iqa no reference nr information entropy statistical naturalness;blind quality assessment;fault detection;dynamic range;hdr imaging devices;tmo;entropy;low dynamic range images;high dynamic range;information entropy;computer vision communities;entropy dynamic range visualization brightness indexes;information analysis;high dynamic range hdr;hdr display devices;structure analysis;high dynamic range imaging techniques;visual databases	High dynamic range (HDR) imaging techniques have been working constantly, actively, and validly in the fault detection and disease diagnosis in the astronomical and medical fields, and currently they have also gained much more attention from digital image processing and computer vision communities. While HDR imaging devices are starting to have friendly prices, HDR display devices are still out of reach of typical consumers. Due to the limited availability of HDR display devices, in most cases tone mapping operators (TMOs) are used to convert HDR images to standard low dynamic range (LDR) images for visualization. But existing TMOs cannot work effectively for all kinds of HDR images, with their performance largely depending on brightness, contrast, and structure properties of a scene. To accurately measure and compare the performance of distinct TMOs, in this paper develop an effective and efficient no-reference objective quality metric which can automatically assess LDR images created by different TMOs without access to the original HDR images. Our model is shown to be statistically superior to recent full- and no-reference quality measures on the existing tone-mapped image database and a new relevant database built in this work.	computer vision;digital image processing;fault detection and isolation;high dynamic range;high-dynamic-range rendering;ldraw;limited availability;tone mapping	Ke Gu;Shiqi Wang;Guangtao Zhai;Siwei Ma;Xiaokang Yang;Weisi Lin;Wenjun Zhang;Wen Gao	2016	IEEE Transactions on Multimedia	10.1109/TMM.2016.2518868	database index;computer vision;entropy;dynamic range;tone mapping;speech recognition;visualization;image processing;computer science;digital image processing;structural analysis;data analysis;brightness;fault detection and isolation;entropy;computer graphics (images)	Vision	62.33996143322846	-63.97490681458239	168812
754b459ea2d63e7402f6887b070687744f05a721	automatic detection of 3d lighting inconsistencies via a facial landmark based morphable model	practical problems;forgery;image forensics;lighting estimation;shape;estimation;automatic detection;three dimensional displays;solid modeling;lighting conditions;face;3d morphable model;lighting;facial landmark;3 d face modeling	Existing 3D lighting consistency based forensic methods have some practical problems. They usually require additional images and human labor to reconstruct the 3D face model for lighting estimation, and furthermore, they cannot deal with expressional faces effectively. These drawbacks make them unusable in many practical cases. In this paper, we propose a more practical 3D lighting based forensic method by incorporating a facial landmark based 3D morphable model to efficiently fit the face shape. We also introduce a residual error based algorithm to automatically exclude outliers in lighting estimation. Our proposed method is fully automatic and very efficient compared to previous ones. Also, it does not depend on additional images and has better performance for expressional faces. Experiments on a realistic face dataset with variational lighting conditions indicate the efficacy and superiority of our method.	algorithm;sampling (signal processing);usability;variational principle	Bo Peng;Wei Wang;Jing Dong;Tieniu Tan	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7533097	face;computer vision;estimation;simulation;shape;lighting;mathematics;solid modeling;image-based lighting;computer graphics (images)	Vision	57.50628460090605	-53.139038106301705	168835
abcd61623f374876aedf7ff732c50af6164aef8a	colour-memory-dependent colour constancy: 2d vs 3d real surfaces				Yazhu Ling;Anya C. Hurlbert	2006			mathematics	Vision	63.770824587064716	-55.72967735798706	169130
ae88da3b8fe5ecedfa6f4c3b23ca98f3986abcef	psychovisual evaluation of the effect of color spaces and color quantification in jpeg2000 image compression	data compression;image coding;image colour analysis;rate distortion theory;cielab color space;cieluv color space;jpeg2000 image compression;xyz color space;ycrcb color space;yiq color space;yuv color space;color quantification;image quality performance;psychovisual evaluation;rate distortion	JPEG2000 is an emerging standard for still image compression. It is not only intended to provide rate-distortion and subjective image quality performance superior to existing standards, but also to provide features and additional functionalities that current standards can not address sufficiently such as lossless and lossy compression, progressive transmission by pixel accuracy and by resolution, etc. Currently the JPEG2000 standard is set up for use with the sRGB three-component color space.the aim of this research is to determine thanks to psychovisual experiences whether or not the color space selected will significantly improve the image compression. The RGB, XYZ, CIELAB, CIELUV, YIQ, YCrCb and YUV color spaces were examined and compared. In addition, we started a psychovisual evaluation on the effect of color quantification on JPEG2000 image compression. the final results indicate that the YCrCb color space is very suitable for this standard as mentioned in the amendement2.	color space;distortion;image compression;image quality;jpeg 2000;lossless compression;lossy compression;pixel;xyz file format	Mohamed-Chaker Larabi;Christine Fernandez-Maloigne;Noël Richard	2002	2002 11th European Signal Processing Conference		color histogram;rgb color model;computer vision;icc profile;color quantization;hsl and hsv;color normalization;color depth;color image;chromaticity;rgb color space;mathematics;color balance;optics;color space;computer graphics (images)	Vision	61.90178372138712	-63.057448569816444	169174
3337470f8037924da3657f7552ef783c143e7632	filter flow	optical distortion;nonlinear filters;kernel;nonlinear optics;image motion analysis;optical filters;layout;linear programming;lighting;cameras	The filter flow problem is to compute a space-variant linear filter that transforms one image into another. This framework encompasses a broad range of transformations including stereo, optical flow, lighting changes, blur, and combinations of these effects. Parametric models such as affine motion, vignetting, and radial distortion can also be modeled within the same framework. All such transformations are modeled by selecting a number of constraints and objectives on the filter entries from a catalog which we enumerate. Most of the constraints are linear, leading to globally optimal solutions (via linear programming) for affine transformations, depth-from-defocus, and other problems. Adding a (non-convex) compactness objective enables solutions for optical flow with illumination changes, space-variant defocus, and higher-order smoothness.	convex function;distortion;enumerated type;flow network;gaussian blur;linear programming;maxima and minima;optical flow;radial (radio)	Steven M. Seitz;Simon Baker	2009	2009 IEEE 12th International Conference on Computer Vision	10.1109/ICCV.2009.5459155	layout;nonlinear optics;computer vision;mathematical optimization;kernel;computer science;linear programming;lighting;optical filter;mathematics	Vision	55.745868694508395	-52.40733377377174	169279
f04d5a54f48392e60f143f17f410a942064dabaf	multi-layer surface segmentation using energy minimization	minimisation;gaussian noise;multilayer surface segmentation;interpolation;image segmentation;occlusion;additive noise;bayesian methods;contracts;layout;psychology;surface properties;nonhomogeneous media;image edge detection;layer assignment;pixel;image partitioning;transparency;energy minimization;computer science;transparency multilayer surface segmentation interpolation energy minimization depth planes approach image segmentation layer assignment image partitioning depth value assignment disjoint surfaces occlusion;depth value assignment;disjoint surfaces;noise removal;image segmentation computer science psychology layout additive noise gaussian noise pixel bayesian methods image edge detection contracts;depth planes approach;interpolation image segmentation minimisation	We propose a multi-layer, “depth planes” approach l o image segmentation where pixels that may have arisen from a single smooth surface in the scene are represented in a common layer. T w o types of output are produced at each pixel: a layer number and a ueclor of depth values, one value for each layer. The layer assignment performs image partitioning based on surface properties. The depth value assignment for each layer either represents the input data with the noise removed or interpolates between data values t o fill-in non-visible parts of the scene. The disjoint surfaces due to occlusion or transparency are also grouped togeiher if they form a smooth surface.	energy minimization;glossary of computer graphics;image segmentation;interpolation;layer (electronics);pixel	Suthep Madarasmi;Daniel J. Kersten;Ting-Chuen Pong	1993		10.1109/CVPR.1993.341169	gaussian noise;layout;computer vision;minimisation;mathematical optimization;bayesian probability;interpolation;computer science;theoretical computer science;mathematics;image segmentation;transparency;energy minimization;pixel;statistics	Vision	53.998689761117156	-54.51104443853815	169504
45d36e1b0eaf4ccd4f6bf50b270ae3940a1c8aee	depth recovery from a single defocused image based on depth locally consistency	l0 gradient minimization;defocus blur;guided filter;depth recovery;blur estimation	Demand to depth estimation from a single image is emerging in more and more multimedia applications, such as human-computer interaction, 3D video generation and representation. In this paper, we manage to solve this challenging problem by using the defocused cues contained in the image. Our algorithm is based on depth local consistency assumption that the scene in input image can be modeled as multiple planar surfaces, and each over-segmented patch corresponds to a small p lanar surface in the 3D scene. As a result, a graph-based algorithm is firstly applied to obtain an over-segmentation result of the input image. Based on the sharp edge prior, the blur amounts on the boundaries of segmentations are calculated according to the edge width at the corresponding locations. Then, the blur amounts of the unknown regions are interpolated from the boundary regions using a plane fitting method. In the next, an image guided filtering method is applied to refine the obtained blur map. To eliminate the affect of the tiny textures, an L0 gradient minimization algorithm is applied to the input image to preserving the prominent boundaries, and the resulting image is served as the guided image. At last, based on a simple geometry prior of photograph, a binary graph cut algorithm is adopted to eliminate the ambiguity in the depth map over the focal plane. The performance of our algorithm is evaluated by various test images. The results demonstrate our algorithm produces high quality depth maps, outperforming state of the art.	algorithm;autostereogram;box blur;cut (graph theory);depth map;depth perception;display resolution;focal (programming language);gaussian blur;gradient;graph cuts in computer vision;human–computer interaction;image plane;interpolation;local consistency	Shuai Fang;Tong Qin;Yang Cao;Fengyun Cao	2013		10.1145/2499788.2499836	image restoration;computer vision;mathematical optimization;mathematics;optics;depth map	Vision	57.34424571835233	-56.545969255186876	169552
1236557f0fa15a440f07d4cc3a60d13a33a3e5d7	bayesian outdoor defect detection.		We introduce a Bayesian defect detector to facilitate the defect detection on the motion blurred images on rough texture surfaces. To enhance the accuracy of Bayesian detection on removing non-defect pixels, we develop a class of reflected non-local prior distributions, which is constructed by using the mode of a distribution to subtract its density. The reflected non-local priors forces the Bayesian detector to approach 0 at the non-defect locations. We conduct experiments studies to demonstrate the superior performance of the Bayesian detector in eliminating the non-defect points. We implement the Bayesian detector in the motion blurred drone images, in which the detector successfully identifies the hail damages on the rough surface and substantially enhances the accuracy of the entire defect detection pipeline.	bayesian approaches to brain function;bayesian network;experiment;pixel;software bug	Fei Jiang;Guosheng Yin	2018	CoRR		pixel;artificial intelligence;pattern recognition;prior probability;mode (statistics);computer science;detector;bayesian probability	Vision	56.282830816416045	-59.01222540053323	169676
fc16d9ed43b6958ad131935a22d4c78d6e4fd631	an evaluation of mtf determination methods for 35mm film scanners		Three different techniques were used to determine the Modulation Transfer Function (MTF) of a 35mm film scanner. The first involved scanning sine wave charts comprising a number of patches with different frequencies of known modulation. The second method involved the scanning and Fourier transform of a photographic grain noise pattern to simulate low modulation white noise. Finally, the ISO 12233 Slanted-edge Spatial Frequency Response (SFR) plug-in was used to determine the average MTF of the device. This creates a super-sampled edge profile from sequential scan-lines of the sampled image of an edge. Procedures for creating test targets, where appropriate, are described. Advantages and limitations encountered in applying each methodology are discussed, as well as the precision of each method for deriving the MTF. Conclusions are drawn concerning the comparability of MTFs determined by the three methods.	chart;film scanner;frequency response;full table scan;image scanner;imaging technology;modulation;plug-in (computing);scan line;simulation;transfer function;white noise	Sophie Triantaphillidou;Ralph E. Jacobson;Robin B. Fagard-Jenkin	1999				Graphics	65.84193009634315	-58.84706282394815	169679
29d8a76dc9907c37aacedc6b1f76ac0bc3e34186	new perceptual approach for ccd matrix calculation	optimal solution;ccd imaging;optimisation;filtro optico;optimizacion;information extraction;color;optical filters;movie camera;low pass filter;qualite image;imageneria ccd;linear filtering;lumiere diffuse;camara;matrices;optical filter;smoothing;color reproduction;image quality;pixel;scattered light;lenses;optical systems;couleur;optimization;calidad imagen;superresolution;superresolucion;luz difusa;filtre optique;imagerie ccd;camera	One of important functions of DCPs (Digital Camera Processors) on which the image quality is largely depends is color matrix calculation from the CCD's optical filter output. An algorithm which can go closer to the optimal solution for generation of color components data from CCD filter output is proposed in this paper. As far as the image quality is concerned, the best way is acquiring complete color data per pixel without any noise, and without any using an optical low pass filter. The most important role of the optical low pass filter is to scatter the ray penetrating the lens system as well as to cut below the frequency of the infrared light. Basically, the proposed algorithm is non-filtered color reproduction flow. First, the proposed algorithm recovers the error made during the path of optic system in order to get better interpolation result. Then, the different procedures are adopted for green component and red or blue component interpolation based on the edge information extracted from the green image. The last procedure is edge preserving smoothing, where image data with high quality can be obtained by preserving the edge segments while smoothing the overall image.	algorithm;anti-aliasing filter;charge-coupled device;digital camera;display resolution;edge-preserving smoothing;image quality;interpolation;low-pass filter;pixel	Young Ho Kim;Kun S. Kim;Byung D. Nam	1999		10.1117/12.348477	demosaicing;computer vision;geography;optics;computer graphics (images)	Vision	58.993989246876616	-60.234394148155474	169853
b02f7f26606c330e39351616955bea8dbbf80e5a	the role of perceptual contrast non-linearities in image transform quantization	rate distortion;human vision;image coding;non linear model;perceptual metric;perceptual quantization;non linear perception model	The conventional quantizer design based on average error minimization over a training set does not guarantee a good subjective behaviour on individual images even if perceptual metrics are used. In this work a novel criterion for transform coder design is analyzed in depth. Its aim is to bound the perceptual distortion in each individual quantization according to a non-linear model of early human vision. A common comparison framework is presented to describe the qualitative behaviour of the optimal quantizers under the proposed criterion and the conventional rate-distortion based criterion. Several underlying metrics, with and without perceptual non-linearities, are used with both criteria. Analytical results show that the proposed design criterion gives rise to a JPEG-like quantization if a simple linear metric is used. Experimental results show that signiicant improvements over the perceptually weighted rate-distortion approach are obtained if a more meaningful non-linear metric is used.	distortion;eisenstein's criterion;jpeg;linear model;nonlinear system;quantization (signal processing);rate–distortion theory;test set;transform coding	Jesús Malo;Francesc J. Ferri;Jesús V. Albert;J. Soret;J. M. Artigas	2000	Image Vision Comput.	10.1016/S0262-8856(99)00010-4	computer vision;mathematical optimization;machine learning;mathematics	Vision	63.394674417331935	-64.74607058231584	169955
bb7a6a36c0acb53dd93df2f08d9810a5e70c26f1	feature-based texture design using deformation techniques	texture synthesis;search algorithm;wang tile;small samples;design method;large deformation;structural similarity	In this paper we present a novel feature-based texture design scheme using deformation techniques. Firstly, we apply a compass operator to extract the feature map from the input small sample texture. Secondly, we use the feature-guided patch searching algorithm to find satisfactory candidate patches taking both color errors and feature errors into account. When the new feature map is created, the designed texture is obtained simultaneously. Thirdly, a completion-based texture design method is employed to design a variety of large deformed textures. A designer can repeat the above steps to design satisfactory textures. The proposed algorithm has the ability to design a variety of versatile textures from a single small sample texture by measuring the structural similarity. Our experimental results demonstrate that our proposed technique can be used for other texture synthesis applications, such as Wang Tiles based cyclic texture design.		Jianbing Shen;Xiaogang Jin;Hanli Zhao	2007		10.1007/978-3-540-73011-8_70	bidirectional texture function;image texture;computer vision;design methods;wang tile;computer science;structural similarity;texture compression;texture synthesis;texture filtering;projective texture mapping;search algorithm;computer graphics (images)	EDA	54.970647311063786	-61.88195257298228	170025
7c9c2a63d8e56eea79cd6d960aef85ef7bd73313	virtual view synthesis quality refinement	hole filling;interpolation;reliability;ftv;three dimensional displays filling image edge detection cameras reliability image color analysis interpolation;filling;dibr virtual view synthesis backward warping disocclusion background hole filling ftv 3dtv;background;unreliable region extraction virtual view synthesis quality refinement multiple reference view texture information multiple reference view depth information fast backward warping adaptive blending hole filling constructed background model mpeg test sequence high quality virtual view;disocclusion;image edge detection;image texture feature extraction image sequences;three dimensional displays;image color analysis;proceedings paper;3dtv;backward warping;dibr;cameras;virtual view synthesis	A 3D virtual view synthesis system is to generate a virtual view at an arbitrary viewpoint from the texture and depth information of multiple reference views. There are several technical challenges in producing a high-quality synthesized view such as warping, blending, ghost artifact reduction and hole filling. Four tools (techniques) have been proposed in this paper to solve these problems. They are unreliable region extraction, fast backward warping, adaptive blending, and hole filling by using the constructed background models. Other techniques such as disocclusion detection, bicubic interpolation, and background model construction are also employed in the proposed algorithm. All above techniques have been designed and tested on the MPEG test sequences. Experimental results show that high-quality virtual views are generated with fewer artifacts.	algorithm;alpha compositing;bicubic interpolation;image warping;moving picture experts group;refinement (computing);view synthesis	Tzu-Chin Lee;Chun-Liang Chien;Hsueh-Ming Hang	2016	2016 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON)	10.1109/3DTV.2016.7548958	computer vision;computer science;multimedia;computer graphics (images)	Visualization	57.469586498240986	-54.99064668528936	170259
29838257dbff5d9d7cc14461f3149a561a748f54	regularized cdwt optical flow applied to moving-target detection in ir imagery	gabor transform;pixel size object detection;multiresolution gabor transform;infrared imaging;image sequence;registration;military application;optical flow;target detection;object detection;moving frames	A modified version of the CDWT optical flow algorithm developed by Magarey and Kingsbury is applied to the problem of moving-target detection in noisy infrared image sequences, in the case where the sensor is also moving. Frame differencing is used to detect pixel-size targets moving in strongly cluttered backgrounds. To compensate for sensor motion, prior to differencing, the background is registered spatially using the estimated motion field between the frames. Results of applying the method to three image sequences show that the target SNR is higher when the estimated motion field for the whole scene is explicitly regularized. A comparison with another optical flow algorithm is also presented.	algorithm;autoregressive integrated moving average;frame language;kinetic data structure;maximum flow problem;motion field;optical flow;pixel;signal-to-noise ratio;white noise	Gabriela Castellano;James F. Boyce;Mark B. Sandler	2000	Machine Vision and Applications	10.1007/s001380050112	gabor transform;computer vision;computer science;optical flow	Vision	56.00842792637051	-58.39916180583086	170632
30fba54404924bbf867a4c1f4c7818b587dc8ca1	a ga-based approach for epipolar geometry estimation	computer vision;epipolar geometry;genetic algorithm;camera calibration	In this paper, a genetic algorithm (GA)-based approach to estimate the fundamental matrix is presented. The aim of the proposed GA-based algorithm is to reduce the e®ect of noise and outliers in the corresponding points which a®ect the accuracy of the estimated fundamental matrix. Although in the proposed approach the GA is allowed to select the signi ̄cant among all detected points, on the average half of the matched points have been determined to give optimum estimation of the fundamental matrix. Experiments with synthetic and real data show that the proposed approach is accurate especially in the presence of a high percentage of outliers. The proposed GA can always obtain good results in both high and low detailed images. Even for low detailed images which have a small number of matched points available to estimate the fundamental matrix, the proposed GA outperformed other methods.	3d reconstruction;closed-circuit television;computer graphics;epipolar geometry;experiment;fundamental matrix (computer vision);genetic algorithm;graphics processing unit;nl (complexity);numerical aperture;overhead (computing);software release life cycle;synthetic intelligence;tournament selection	Nehal Khaled;Elsayed E. Hemayed;Magda B. Fayek	2013	IJPRAI	10.1142/S0218001413550148	computer vision;mathematical optimization;camera resectioning;genetic algorithm;eight-point algorithm;computer science;mathematics;geometry;fundamental matrix;epipolar geometry	Vision	54.4865779707065	-54.64790357559782	170715
288ee300b4bb1f6723b8e825a8356fd816ed3b1d	memory efficient stereoscopy from light fields	stereo image processing cameras three dimensional displays rendering computer graphics labeling optimization;light field;hd content memory efficient stereoscopy light fields multiperspective imaging stereoscopic content generation target disparity map stereoscopic image pair synthesis light rays target disparity constraints variational convex optimization problem occlusions disocclusions scene depth grid bias image distortion resolution input processing;stereoscopy;variational techniques convex programming image resolution stereo image processing;multi perspective imaging;variational convex optimization;variational convex optimization light field stereoscopy multi perspective imaging disparity modification;disparity modification	We address the problem of stereoscopic content generation from light fields using multi-perspective imaging. Our proposed method takes as input a light field and a target disparity map, and synthesizes a stereoscopic image pair by selecting light rays that fulfill the given target disparity constraints. We formulate this as a variational convex optimization problem. Compared to previous work, our method makes use of multi-view input to composite the new view with occlusions and disocclusions properly handled, does not require any correspondence information such as scene depth, is free from undesirable artifacts such as grid bias or image distortion, and is more efficiently solvable. In particular, our method is about ten times more memory efficient than the previous art, and is capable of processing higher resolution input. This is essential to make the proposed method practically applicable to realistic scenarios where HD content is standard. We demonstrate the effectiveness of our method experimentally.	binocular disparity;calculus of variations;convex optimization;decision problem;distortion;experiment;light field;mathematical optimization;optimization problem;ray (optics);solver;speedup;stereoscopy;time complexity;variational principle	Changil Kim;Ulrich Muller;Henning Zimmer;Yael Pritch;Alexander Sorkine-Hornung;Markus H. Gross	2014	2014 2nd International Conference on 3D Vision	10.1109/3DV.2014.12	computer vision;mathematics;optics;computer graphics (images)	Vision	58.162331232751704	-54.179020022977454	171011
834516ca0ce2f1c1ccc5e5389b3162176e66c285	a parallel ray tracing architecture suitable for application-specific hardware and gpgpu implementations	paper;raytracing;rendering computer graphics computer graphic equipment coprocessors field programmable gate arrays parallel architectures ray tracing;nvidia geforce gtx 465;computer graphic equipment;virtual reality;fpga parallel ray tracing architecture application specific hardware gpgpu ray tracing rendering algorithm high fidelity image 3d scene shadow effect interactive real time rendering uniform spatial subdivision ray triangle intersection asip based implementation;fpga;coprocessors;cuda;application specific;computer architecture;gpgpu;parallel architectures;ray tracing graphics processing unit instruction sets computer architecture registers image color analysis hardware;registers;image color analysis;ray tracing;nvidia;algorithms;cuda ray tracing parallel architecture application specific asip gpgpu;computer science;parallel architecture;field programmable gate arrays;rendering computer graphics;graphics processing unit;asip;instruction sets;hardware;rendering	The Ray Tracing rendering algorithm can produce high-fidelity images of 3-D scenes, including shadow effects, as well as reflections and transparencies. This is currently done at a processing speed of at most 30 frames per second. Therefore, actual implementations of the algorithm are not yet suitable for interactive real-time rendering, which is required in games and virtual reality based applications. Fortunately, the algorithm allows for massive parallelization of its computations. In this paper, we present a parallel architecture for ray tracing based on a uniform spatial subdivision of the scene and exploiting an embedded computation of ray-triangle intersections. This approach allows for a significant acceleration of intersection computations, as well as, a reduction of the total number of the required intersections checks. Furthermore, it allows for these checks to be performed in parallel and in advance for each ray. In this paper we discuss and analyze an ASIP-based implementation using FPGAs and a GPGPU-based parallel implementation of the proposed architecture. The performance of both implementations are reported and compared.	algorithm;application-specific instruction set processor;application-specific integrated circuit;cuda;computation;embedded system;fermi (microarchitecture);field-programmable gate array;geforce 400 series;general-purpose computing on graphics processing units;overhead (computing);parallel computing;ray tracing (graphics);real-time clock;real-time locating system;reflection (computer graphics);space partitioning;subdivision surface;transparency (projection);virtual reality	Alexandre Solon Nery;Nadia Nedjah;Felipe Maia Galvão França;Lech Józwiak	2011	2011 14th Euromicro Conference on Digital System Design	10.1109/DSD.2011.71	embedded system;ray tracing;computer architecture;parallel computing;computer science;operating system;virtual reality;field-programmable gate array;computer graphics (images)	Graphics	67.69452254480318	-52.201861722533764	171101
d405d556aba0e55b77abc8b956f4f57ddc42dd7a	automatic background adjustment for chinese paintings using pigment lines		Most traditional Chinese paintings are painted on hand-made paper that easily suffers from severe spectral changes caused by prolonged light exposure, resulting in color distortion and low contrast. To recover the original appearance of a Chinese painting, especially its background color, an automatic background adjustment framework is proposed. This framework is based on the insightful observation that the fading model of a painting image is analogue to the common hazy image formation model when the painting image is transformed into the K-M (Kubelka-Munk) space. We demonstrate that this fading model is quite useful in extracting pigment lines from any painting image. These pigment lines represent clusters of distinct color pigments used in a Chinese painting, which is the key to density map estimation and background restoration. Experimental results prove that our approach is able to restore a variety of deteriorated Chinese paintings without any user intervention or training.	pigment	Jie Guo;Chunyou Li;Jingui Pan	2017		10.1007/978-3-319-77383-4_58	artificial intelligence;computer vision;computer science;image formation;fading;pattern recognition;pigment;distortion;painting	Vision	53.90759821471593	-57.12510904297454	171242
b4b053d9f925f9f0e143cec7cfe77406d364fc98	deep generative filter for motion deblurring		Removing blur caused by camera shake in images has always been a challenging problem in computer vision literature due to its ill-posed nature. Motion blur caused due to the relative motion between the camera and the object in 3D space induces a spatially varying blurring effect over the entire image. In this paper, we propose a novel deep filter based on Generative Adversarial Network (GAN) architecture integrated with global skip connection and dense architecture in order to tackle this problem. Our model, while bypassing the process of blur kernel estimation, significantly reduces the test time which is necessary for practical applications. The experiments on the benchmark datasets prove the effectiveness of the proposed method which outperforms the state-of-the-art blind deblurring algorithms both quantitatively and qualitatively.	algorithm;benchmark (computing);box blur;circuit restoration;computer vision;convolutional neural network;deblurring;end-to-end principle;experiment;glossary of computer graphics;raman scattering;run time (program lifecycle phase);well-posed problem	Sainandan Ramakrishnan;Shubham Pachori;Aalok Gangopadhyay;Shanmuganathan Raman	2017	2017 IEEE International Conference on Computer Vision Workshops (ICCVW)	10.1109/ICCVW.2017.353	kernel (linear algebra);motion blur;pattern recognition;architecture;generative grammar;artificial intelligence;computer vision;image restoration;deblurring;computer science;kernel density estimation	Vision	54.92164205383264	-53.46764178550146	171310
620d739b3f8217dc080341f2ffbd7b4077cf3c27	error diffusion on an adaptive raster	halftoning error diffusion	Whenever a continuous-tone image has to be displayed on a bi-level medium, some kind of halftoning has to be carried out. Nowadays processing of the continuous-tone image is often done by a computer, i.e. the sampled and quantized continuous-tone image is transformed into a binary image by a halftoning algorithm. In the following we will assume the continuous-tone image to be scaled between 0 and 1. In addition, we will restrict the discussion to algorithms for output devices where the addressability of the pulse location is approximately equal to the pulse extension. In this case the binary picture can be described by a pattern of independent black and white pixels. Depending on the properties and needs of the output device, pulse-width modulation (PWM) or pulse-density modulation (PDM) is used. A special kind of PDM is generated by the error diffusion algorithm (ED) [1]. Because of its easy implementation and its relatively high speed, this algorithm is in widespread use.	algorithm;approximation;binary image;black and burst;error diffusion;output device;pixel;pulse-density modulation;pulse-width modulation	Thomas Zeggel;Olof Bryngdahl	1993	Electronic Publishing		computer vision;computer science;theoretical computer science;computer graphics (images)	ML	64.06255131105894	-59.78993838659985	171338
25d13dea4a3d05b422e54a17382d50d8c2b3f0c3	distributed image reconstruction of micro-ct data: computation and communication issues	image reconstruction		computation;iterative reconstruction	Thomas M. Benson;Jens Gregor	2005			iterative reconstruction;computer vision;computation;communication issues;computer science;artificial intelligence;feature detection (computer vision)	Robotics	56.63405941333505	-54.94528256944444	171378
027fcd71ea6a22a5053eace740c0558a0c8e2631	accurate depth-color scene modeling for 3d contents generation with low cost depth cameras	telecomunicaciones;robotica e informatica industrial;iterative methods;image colour analysis;image denoising;filtering theory;cameras	In this paper, we present a depth-color scene modeling strategy for indoors 3D contents generation. It combines depth and visual information provided by a low-cost active depth camera to improve the accuracy of the acquired depth maps considering the different dynamic nature of the scene elements. Accurate depth and color models of the scene background are iteratively built, and used to detect moving elements in the scene. The acquired depth data is continuously processed with an innovative joint-bilateral filter that efficiently combines depth and visual information thanks to the analysis of an edge-uncertainty map and the detected foreground regions. The main advantages of the proposed approach are: removing depth maps spatial noise and temporal random fluctuations; refining depth data at object boundaries, generating iteratively a robust depth and color background model and an accurate moving object silhouette.	bilateral filter;depth map;mathematical model	Massimo Camplani;Tomás Mantecón;Luis Salgado	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6467216	computer vision;simulation;mathematics;iterative method;computer graphics (images)	Robotics	57.504017886487325	-54.99139859806775	171468
ec17f8c0aa2fdef27005555e5d548023147567c9	new approach for a spectral image capture device based on a micromirror system	microelectromechanical systems;diffraction grating;design criteria;diffraction gratings;linear array;micromirrors;image acquisition;image quality;spectral method;spectral imaging;data handling;multi spectral	"""ABSTRACT Traditional technologies for image capture devices are based on the three RGB filters for the generation ofthe color signals.As an alternative, in the last years spectral methods of image capture were introduced with the prospect of severaladvantages for image quality and color data handling. Most of these solutions are so-called multi-spectral technologies andtherefore still use filters.In this paper a spectral approach for image capturing is discussed, which is based on a diffraction grating. The grating isintegrated on a micro-mirror device in a linear array structure with very small dimensions. The main design criteria and theimportant parameters are discussed. Also the influences of MEMS technologies for the production of such devices areshown.Keywords: Spectral images, color scanner, micro mirror device, diffractive grating 1. INTRODUCTION In colloquial speech image capture devices (lCD) are known as """"color scanner"""" or """"image scanner"""" or on the other hand asdigital cameras. The common optical design of a scanner is a direct contact ofthe lCD and the original picture, while thecamera systems are able to take the image over a distance. An lCD enables us to make through-light or on-light scans oftwo-dimensional images and to convert the received information into a set of electronic signals that represent the imageinformation unambiguously. This image capture is a process that is complex and tangents to a multitude of academic fieldsand basic technologies. In the following, however, priority will be given to the matter of optical image dissection. Theevaluation and processing of the received image data, analog-digital conversion for instance, the colorimetric calculation aswell as the handling of data sets will not be considered any further.The optical elements of an lCD in its main part are a light source, the original image, reproduction and beam form lenses,and a detector. Normally, image dissection comprises two central functions:(I) The geometrical image dissection to address the smallest dissolved image elements (pixel).(II) The determination ofthe color values (or density) for each pixel.The geometrical image dissection, effecting an unambiguous determination of the image place, can thereby result"""		Arved C. Hübler;Fouad Guessous;Susanne Reuter	2001		10.1117/12.410803	image warping;computer vision;feature detection;diffraction grating;binary image;image processing;digital image processing;clipping;microelectromechanical systems;optics;physics	Vision	62.70973237640152	-57.14765371274166	171600
a23a02900e0a7e0fb49e594eb0a1930c0a7d52c0	the relationship between ambient illumination and psychological factors in viewing of display images	man;analisis factorial;metodo diferencial;4266n;psicofisica;0130c;arriere plan;differential method;brightness;background;analyse factorielle;brillance;factor analysis;methode differentielle;led lighting;psychophysique;analisis semantico;analyse semantique;imagen color;image couleur;semantic analysis;color image;homme;psychophysics	"""In this paper, we have clarified the relationship between ambient illumination and psychological factors in viewing of display images. Psychological factors were obtained by the factor analysis with the results of the semantic differential (SD) method. In the psychological experiments, subjects evaluated the impressions of displayed images with changing ambient illuminating conditions. The illumination conditions were controlled by a fluorescent ceiling light and a color LED illumination which was located behind the display. We experimented under two kinds of conditions. One was the experiment with changing brightness of the ambient illumination. The other was the experiment with changing the colors of the background illumination. In the results of the experiment, two factors """"realistic sensation, dynamism"""" and """"comfortable,"""" were extracted under different brightness of the ambient illumination of the display surroundings. It was shown that the """"comfortable"""" was improved by the brightness of display surroundings. On the other hand, when the illumination color of surroundings was changed, three factors """"comfortable,"""" """"realistic sensation, dynamism"""" and """"activity"""" were extracted. It was also shown that the value of """"comfortable"""" and """"realistic sensation, dynamism"""" increased when the display surroundings were illuminated by the average color of the image contents."""	ambient occlusion	Takuya Iwanami;Ayano Kikuchi;Takashi Kaneko;Keita Hirai;Natsumi Yano;Toshiya Nakaguchi;Norimichi Tsumura;Yasuhiro Yoshida;Yoichi Miyake	2009		10.1117/12.810118	computer vision;color image;optics;factor analysis;led lamp;psychophysics;brightness;physics	HCI	62.95046495074735	-60.82342736480211	171657
3b0e48223e289ad27be627407f04bff468fa8bbf	perceptual preference for noise and color saturation tradeoff in digital camera images			digital camera	Xuemei Zhang;Richard L. Baer	2006			computer vision;computer science;artificial intelligence;digital camera;perception;saturation (chemistry)	Vision	61.98975622482728	-61.0302362897174	172296
2860ad38fddd64683333738857086035b6a25669	virtual microscopy with extended depth of field	high resolution;depth of field;microscopy virtual manufacturing focusing information technology bit rate rendering computer graphics collaboration digital images milling machines image quality;image quality;virtual microscopy;field of view;extended depth of field;jpeg 2000;jpeg 2000 virtual microscopy extended depth of field;bits per pixel;on line learning	In this paper, we describe a virtual microscope system, based on JPEG 2000, which utilizes extended depth of field (EDF) imaging. Through a series of observer trials we show that EDF imaging improves both the local image quality of individual fields of view (FOV) and the accuracy with which the FOVs can be mosaiced (stitched) together. In addition, we estimate the required bit rate to adequately render a set of histology and cytology specimens at a quality suitable for on-line learning and collaboration. We show that, using JPEG 2000, we can efficiently represent high-quality, high-resolution colour images of microscopic specimens with less than 1 bit per pixel.	1-bit architecture;earliest deadline first scheduling;focal (programming language);focus stacking;heuristic;image quality;image resolution;image stitching;image viewer;jpeg 2000;jpip;maxima and minima;mobile data terminal;online and offline;online machine learning;pixel;proxy server;server (computing);z/vm	Andrew P. Bradley;Michael Wildermoth;Paul Mills	2005	Digital Image Computing: Techniques and Applications (DICTA'05)	10.1109/DICTA.2005.87	image quality;computer vision;image resolution;color depth;field of view;computer science;depth of field;jpeg 2000;multimedia;computer graphics (images)	Vision	62.57938127305114	-55.18969895686432	172696
50a0c112eae52ddde21cabb70d69543ca83cb729	a framework for holographic scene representation and image synthesis	image synthesis;expressive imagery;non realistic modeling	We present a framework for the holographic representation and display of graphics objects. As opposed to traditional graphics representations, our approach reconstructs the light wave reflected or emitted by the original object directly from the underlying digital hologram. Our novel holographic graphics pipeline consists of several stages including the digital recording of a full-parallax hologram, the reconstruction and propagation of its wavefront, and rendering of the final image onto conventional, framebuffer-based displays. The required view-dependent depth image is computed from the phase information inherently represented in the complex-valued wavefront. Our model also comprises a correct physical modeling of the camera taking into account optical elements, such as lens and aperture. It thus allows for a variety of effects including depth of field, diffraction, interference, and features built-in anti-aliasing. A central feature of our framework is its seamless integration into conventional rendering and display technology which enables us to elegantly combine traditional 3D object or scene representations with holograms. The presented work includes the theoretical foundations and allows for high quality rendering of objects consisting of large numbers of elementary waves while keeping the hologram at a reasonable size	aliasing;canonical account;digital recording;display device;display resolution;foundations;framebuffer;graphics pipeline;holography;interference (communication);parallax;physical object;rendering (computer graphics);seamless3d;software propagation;spatial anti-aliasing	Remo Ziegler;Peter Kaufmann;Markus H. Gross	2006	IEEE Transactions on Visualization and Computer Graphics	10.1145/1179849.1179984	computer vision;image-based modeling and rendering;rendering;computer science;computer graphics (images)	Visualization	62.88800233909792	-52.96049705848923	172994
152503305c9fd9a6a86d67b577e8419d61db857f	vector field fitting for real-time environment matting of transparent objects	vector field fitting;working environment noise data mining image segmentation filtering minimization methods poisson equations color reflection data engineering computer errors;image processing;real time;presses;indexing terms;data mining;transparent objects;real time systems image processing knowledge acquisition;vector fitting;environment matting;transparent objects environment matting light motion field vector field fitting;image color analysis;knowledge acquisition;pixel;light motion field;energy minimization;vector field;algorithm design and analysis;extracted environment vector field fitting algorithm real time environment matting transparent objects filtering methods energy minimization approach light motion field;noise;real time systems	The major drawback of real-time environment matting method is that the extracted environment matte data often contains significant amount of noise and errors. Although some filtering methods have been employed to remove the noise and obtain acceptable composition results, they are incapable of removing potential errors. In this paper, we first establish a light motion field to better describe the environmental matting effect of transparent objects and propose a new vector field fitting algorithm to simultaneously remove both noise and errors in the extracted matte data by using energy minimization approach. Experimental results show that our method is less sensitive to noise and error and can generate perceptually better composition results than the existing real-time environment matting approaches.	algorithm;energy minimization;matte display;motion field;real-time locating system	Qi Duan;Jianfei Cai;Jianmin Zheng	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414359	algorithm design;computer vision;vector field;simulation;index term;image processing;computer science;noise;energy minimization;pixel;computer graphics (images)	Robotics	56.57221685510699	-60.94564337706704	173332
7bedc47732737fc42a2db67553feece281a5cec9	image reconstruction from dense binary pixels		The pursuit of smaller pixel sizes at ever increasing resolution in digital image sensors is mainly driven by the stringent price and form-factor requirements of sensors and optics in the cellular phone market. Recently, Eric Fossum proposed a novel concept of an image sensor with dense sub-diffraction limit one-bit pixels (jots) [1], which can be considered a digital emulation of silver halide photographic film. This idea has been recently embodied as the EPFL Gigavision camera. We denote by x the radiant exposure at the camera aperture measured over a given time interval. This exposure is subsequently degraded by the optical point spread function denoted by the operator H, producing the exposure of the sensor λ = Hx. The number of photoelectrons ejk generated at pixel j in time frame k follows the Poisson distribution with the rate λj . A binary pixel compares the accumulated charge against a pre-determined threshold qj , outputting a one-bit measurement bjk. Thus the probability of a single binary pixel j to assume an ”on” value in frame k is P(bjk = 1) = P(ejk ≥ qj). Our goal is to estimate an intensity field vector x̂ best predicting x given the measurement matrix B. In [2], a maximum likelihood (ML) approach was proposed. Assuming independent measurements, the negative likelihood function can be expressed as	aperture (software);digital image;emulator;form factor (design);halide;image sensor;iterative reconstruction;mobile phone;photoelectric effect;pixel;radiant ai;requirement;eric	Or Litany;Tal Remez;Alexander M. Bronstein	2015	CoRR		computer vision;mathematical optimization;simulation;mathematics	Vision	61.26961330975418	-57.75852785437134	173476
ad315653b14f2196bafd4bc2f867dee865472767	hybrid image deblurring by fusing edge and power spectrum information		Recent blind deconvolution methods rely on either salient edges or the power spectrum of the input image for estimating the blur kernel, but not both. In this work we show that the two methods are inherently complimentary to each other. Edge-based methods work well for images containing large salient structures, but fail on small-scale textures. Power-spectrum-based methods, on the contrary, are efficient on textural regions but not on structural edges. This observation inspires us to propose a hybrid approach that combines edge-based and power-spectrum-based priors for more robust deblurring. Given an input image, our method first derives a structure prediction that coincides with the edge-based priors, and then extracts dominant edges from it to eliminate the errors in computing the power-spectrum-based priors. These two priors are then integrated in a combined cost function for blur kernel estimation. Experimental results show that the proposed approach is more robust and achieves higher quality results than previous methods on both real world and synthetic examples.	blind deconvolution;box blur;deblurring;european conference on computer vision;gaussian blur;kernel (operating system);loss function;mathematical optimization;spectral density;synthetic intelligence	Tao Yue;Sunghyun Cho;Jue Wang;Qionghai Dai	2014		10.1007/978-3-319-10584-0_6	computer vision;pattern recognition;computer graphics (images)	Vision	54.57136641257041	-57.532999253738005	173686
ef85f02654ac54a01185aac60c2386e6f89c33b0	a proposed standard procedure to define minimum scanning attribute levels for hard copy documents	resolution;text analysis document image processing image colour analysis image resolution;image resolution;text analysis;document;image colour analysis;document image processing;resolution document scanning dpi;standards image color analysis spatial resolution law optical character recognition software;dpi;scanning;legal archival quality standard standard procedure minimum scanning attribute level hard copy documents public agency electronic systems archival copy high resolution level legal requirement industry attributes dpi text documents gray scale colors black and white photos line work document	Public agencies have been scanning hard copy documents into electronic systems for a number of years. Legally, the document needs to be a true copy of the original but no standards setting body has defined what specific attributes define when a document is a true archival copy. Documents can be scanned at high resolution levels but there is a trade off between resolution and file size. The goal is to meet the legal requirement but at the smallest file size that is reasonable. The paper will show that commonly used industry attributes of 200 dots per inch, (DPI) for text documents, 300 DPI and 8 bit gray scale colors for black and white photos and 300 DPI and 24 bit color for full color and or line work documents met the legal archival quality standard. With this recommendation, agencies can adhere to a common set of practical scanning attributes.	24-bit;archive;color depth;dots per inch;grayscale;image resolution;vector graphics	Mitchell Cochran	2014	2014 47th Hawaii International Conference on System Sciences	10.1109/HICSS.2014.258	text mining;resolution;image resolution;computer science;database;advertising;world wide web;information retrieval	DB	64.71623759758471	-60.3381892731042	173911
69aeb476f29d06468b0bb8681137ad1adf983c98	optimal halftoning over hexagonal grids	printing;rendu image;interpolation;posicionamiento;halftones;restitucion imagen;availability;imagen medio tinte;disponibilidad;reduccion de colores;image demi teinte;interpolacion;half tone image;optimization method;metodo optimizacion;gray scale;printer;algorithme;algorithm;positioning;imprimante;dithering;image rendering;methode optimisation;impression;impresora;algorithms;tramage;methode reechantillonnage;computer hardware;resampling method;4230v;echelle gris;impresion;diffusion;disponibilite;escala gris;positionnement;algoritmo	Halftoning approaches to image rendering on binary devices have traditionally relied on rectangular grids for dot placement. This practice has been followed mainly due to restrictions on printer hardware technology. However, recent advances on printing devices coupled with the availability of efficient interpolation and resampling algorithms are making the implementation of halftone prints over alternate dot placement tessellations feasible. This is of particular interest since blue noise dithering principles indicate that the visual artifacts at several tone densities, which appear in rectangular-grid halftones, can be overcome through the use of hexagonal tessellations. While the spectral analysis of blue noise dithering provides the desired spectral characteristics one must attain, it does not provide the dithering structures needed to achieve these. In this paper, these optimal dithering mechanisms are developed through modifications of the Direct Binary Search (DBS) algorithm extensively used for rectangular grids. Special attention is given to the effects of the new geometry on the Human Visual System (HVS) models and on the efficient implementation of the hexagonal-grid DBS. This algorithm provides the best possible output at the expense of high computational complexity, and while the DBS algorithm is not practical in most applications, it provides a performance benchmark for other more practical algorithms. Finally, a tonedependent, hexagonal-grid, error-diffusion algorithm is developed, where the DBS algorithm is used to optimize the underlying filter weights. The characteristics of the HVS are thus implicitly used in the optimization. Extensive simulations show that hexagonal grids do indeed reduce disturbing artifacts, providing smoother halftone textures over the entire gray-scale region. Results also show that tone-dependent error-diffusion can provide comparable results to that of the DBS algorithms but at a significantly lower computational complexity.	artifact (software development);benchmark (computing);binary search algorithm;cluster analysis;coefficient;colors of noise;computational complexity theory;direct-broadcast satellite;dither;error diffusion;grayscale;hexagonal sampling;human visual system model;interpolation;mathematical optimization;nyquist–shannon sampling theorem;printer (computing);printing;rendering (computer graphics);sampling (signal processing);simulation;visual artifact	Jan Bacca Rodríguez;Alvaro J. González;Gonzalo R. Arce;Daniel Leo Lau	2006		10.1117/12.643527	simulation;telecommunications;interpolation;computer science;optics;computer graphics (images)	Graphics	54.231676352842946	-62.86262057640929	173917
d21ab41dc8a4b1478202e85172353a7cac244d05	high-dimensional optimization of color coded apertures for compressive spectral cameras		A spectral image can be regarded as a three-dimensional cube where each pixel is a vector of intensities representing a spectral signature. Compressive spectral imaging (CSI) is a sensing and reconstruction framework, based on the fundamentals of the compressive sensing theory, which focuses on capturing spectral images efficiently, exploiting their highly correlated information by coding its spectral characteristics commonly using a black-and-white, grayscale or recently a color coded aperture. The distribution of the entries of the coded apertures determines the quality of the estimated spectral images. State of the art methods have used random coded apertures, and some optimization procedures have focused on the optimal design of horizontal sections of the coded apertures; however, they do not fully exploit the spatio-spectral correlations within the spectral images. To that end, in this paper, it is proposed a high-dimensional optimization procedure to design color coded apertures for CSI systems, which exploits not only the spectral correlations but also the spatial correlations within an spectral image. Simulations analyzing the conditioning of the sensing matrices, as well as the reconstruction quality of the attained spectral images show the improvement entailed by the proposed method.	coded aperture;compressed sensing;computer simulation;grayscale;mathematical optimization;optimal design;pixel	Hoover F. Rueda;Henry Arguello;Gonzalo R. Arce	2017	2017 25th European Signal Processing Conference (EUSIPCO)	10.23919/EUSIPCO.2017.8081250	compressed sensing;pixel;grayscale;aperture;computer vision;spectral imaging;matrix (mathematics);spectral signature;artificial intelligence;mathematics;coded aperture	Vision	67.82468276209096	-61.48673558492173	174102
fc29abbbf93847556244650b6bf4aacbdb3103bc	bidirectional symmetry and median filter with dynamic smoothness weight on horn-schunck optical flow algorithm	smoothing methods awgn image sequences median filters motion estimation;motion estimation;awgn;bidirectional symmetry psnr peak signal to noise ratio awgn additive white gaussian noise pixel based over video sequence motion vector motion estimation horn schunck optical flow algorithm dynamic smoothness weight median filter;smoothing methods;optical filters computer vision image motion analysis optical imaging reliability psnr heuristic algorithms;median filters;image sequences	For motion estimation, optical flow is an algorithm that uses to determine motion vector (MV) in a level of pixel based over video sequences. Under noisy situations, they corrupt the achievement in reliability of MV on optical flow. To lift the reliability of MV, this paper presents bidirectional symmetry and median filter with dynamic smoothness weight on Horn-Schunck optical flow algorithm. From the experiment, it shows the effective result in our proposed algorithm when various values of smoothness weight are comprehensively considered. We also analyze on how importance of the appropriated value of smoothness weight (α) that should be selected for the best achievement. Several differences in characteristic of conventional sequences are used in our experiment. We also simulate theses sequences with several levels of Additive White Gaussian Noise (AWGN) for performance investigation under noisy environment where Peak Signal to Noise Ratio (PSNR) is a performance indicator.	additive model;additive white gaussian noise;experiment;hs algorithm;maximum flow problem;median filter;motion estimation;optical flow;peak signal-to-noise ratio;pixel;simulation	Darun Kesrarat;Vorapoj Patanavijit	2013	2013 International Symposium on Intelligent Signal Processing and Communication Systems	10.1109/ISPACS.2013.6704641	computer vision;mathematical optimization;electronic engineering;mathematics	Vision	59.99246042516819	-64.24587640401414	174103
0894b12924d3c7ffed34a43f1e2e177cde313b41	enhanced dynamic quadrant histogram equalization plateau limit for image contrast enhancement	databases;brightness perserving image enhancement clipped histogram equalization sub histogram equalization edqhepl plateau limit equalization dynamic histogram equalization;low contrast image preservation enhanced dynamic quadrant histogram equalization plateau limit image contrast enhancement consumer electronic device brightness median value input image histogram customary histogram equalization image mean brightness sipi database;histograms;histograms decision support systems brightness image enhancement dynamic range databases;image enhancement brightness;brightness;image enhancement;decision support systems;dynamic range	Consumer electronic devices require brightness preservation while image enhancement. Many of the histogram equalization techniques that have been introduced tend to strike out the mean image brightness while the process of enhancement is carried out. Hence, we suggest a method that enhances the image while keeping its brightness in a better form as compared to the former methods applied. Firstly, based on median value, the input image histogram is divided into sub-histogram and this process reprises until we have derived 8 sub-histograms. Further, the clipping of the histogram is implemented based on the average pixels in each sub-histogram. Then, each sub-histogram is assigned with a new dynamic range and finally customary histogram equalization is done independently on each sub-histogram. Proposed method is tested against conventional methods and results indicate that image mean brightness is preserved better than other methods while enhancing the image. The proposed method keep the brightness save of the original image while during the enhancement of the images. Many images has been taken from the SIPI database and applied the proposed method for the contrast enhancement of the low contrast images and results are improved. The major problem is to keep the brightness of the low contrast image preserve. The proposed technique has dealt with brightness preservation of the images.	dynamic range;histogram equalization;image editing;image histogram;pixel	Rizwan Khalid;Saad Rehman;Farhan Riaz;Ali Hassan	2015	2015 Fifth International Conference on Digital Information and Communication Technology and its Applications (DICTAP)	10.1109/DICTAP.2015.7113176	color histogram;computer vision;dynamic range;color normalization;decision support system;shadow and highlight enhancement;computer science;histogram matching;balanced histogram thresholding;histogram;adaptive histogram equalization;histogram equalization;brightness;statistics;image histogram;computer graphics (images)	Vision	57.618876060528	-63.93941341394599	174129
3263a74a147d84ce65ba1eb1f0dbdb51ffaa13f1	biologically inspired blind quality assessment of tone-mapped images		Currently, many tone mapping operators (TMOs) have been provided to compress high dynamic range images to low dynamic range (LDR) images for visualizing them on the common displays. Since quality degradation is inevitably induced by compression, how to evaluate the obtained LDR images is indeed a headache problem. Until now, only a few full reference (FR) image quality assessment metrics have been proposed. However, they highly depend on reference image and neglect human visual system characteristics, hindering the practical applications. In this paper, we propose an effective blind quality assessment method of tone-mapped image without access to reference image. Inspired by that the performance of existing TMOs largely depend on the brightness and chromatic and structural properties of a scene, we evaluate the perceptual quality from the perspective of color information processing in the brain. Specifically, motivated by the physiological and psychological evidence, we simulate the responses of single-opponent (SO) and double-opponent (DO) cells, which play an important role in the processing of the color information. To represent the textural information, we extract three features from gray-level co-occurrence matrix (GLCM) calculated from SO responses. Meanwhile, both GLCM and local binary pattern descriptor are employed to extract texture and structure in the responses of DO cells. All these extracted features and associated subjective ratings are learned to reveal the connection between feature space and human opinion score. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art blind quality assessment methods and is comparable with the popular FR methods on two recently published tone-mapped image databases.	binary pattern (image generation);co-occurrence matrix;database;document-term matrix;elegant degradation;experiment;feature vector;high dynamic range;high-dynamic-range imaging;high-dynamic-range rendering;image quality;information processing;ldraw;simulation;tone mapping	Guanghui Yue;Chunping Hou;Ke Gu;Shasha Mao;Wenjun Zhang	2018	IEEE Transactions on Industrial Electronics	10.1109/TIE.2017.2739708	computer vision;local binary patterns;control engineering;brightness;tone mapping;engineering;high dynamic range;image quality;feature extraction;artificial intelligence;feature vector;human visual system model	Vision	61.843749410686605	-64.87914628916614	174635
7adc8da3ddee6d662f8c782eee968d008ffbf169	modelfest: year one results and plans for future years	databases;modelizacion;electronic imaging;human vision;data compression;image communication;psicofisica;data collection;hombre;digital imaging;efecto contraste;modelisation;stimulus visuel;human vision and color perception;image compression;percepcion visual;visual stimulus;human visual system;frequence spatiale;human;estimacion parametro;psychophysique;perception visuelle;contrast effect;temporal processing;visual perception;cross validation;estimulo visual;compresion dato;digital image;parameter estimation;estimation parametre;frecuencia espacial;modeling;spatial frequency;visual masking;compression donnee;effet contraste;homme;psychophysics	A robust model of the human visual system (HVS) would have a major practical impact on the difficult technological problems of transmitting and storing digital images. Although most HVS models exhibit similarities, they may have significant differences in predicting performance. Different HVS models are rarely compared using the same set of psychophysical measurements, so their relative efficacy is unclear. The Modelfest organization was formed to solve this problem and accelerate the development of robust new models of human vision. Members of Modelfest have gathered psychophysical threshold data on the year one stimuli described at last year's SPIEmeeting1 . Modelfest is an exciting new approach to modeling involving the sharing of resources, learning from each other's modeling successes and providing a method to cross-validate proposed HVS models. The purpose of this presentation is to invite the Electronic Imaging community to participate in this effort and inform them of the developing database, which is available to all researchers interested in modeling human vision. In future years, the database will be extended to other domains such as visual masking, and temporal processing. This Modelfest progress report summarizes the stimulus definitions and data collection methods used, but focuses on the results of the phase one data collection effort. Each of the authors has provided at least one dataset from their respective laboratories. These data and data collected subsequent to the submission of this paper are posted on the www for further analysis and future modeling efforts.	digital image;human visual system model;transmitter;visual basic[.net];world wide web	Thom Carney;Christopher W. Tyler;Andrew B. Watson;Walter Makous;Brent Beutter;Chien-Chung Chen;Anthony Matthew Norcia;Stanley A. Klein	2000		10.1117/12.387150	computer vision;simulation;geography;communication	ML	63.38701071725763	-61.79466769955026	174857
cbc5de8866b30aca4a5d846cbbc4913c1178e317	an adaptive interpolation scheme based on iterative back-projection and hvs-based quality metric for image sequences	iterative back projection;image interpolation;resolution enhancement	In this paper, we propose an adaptive interpolation scheme based on iterative back-projection and human visual system based quality metric for image sequences. Initial estimates of each up-sampled image can be generated individually by using subpixel interpolation and subpixel motion estimation in the spatial and temporal domains respectively. Then, based on the initial estimates and edge information, up-sampled images are derived by using an iterative back-projection technique and a quality metric based on the human visual system. After fusing the up-sampled images into a final version, a low-pass filter is applied as a post-processing step to reduce the effect of blocking artifacts in each reconstructed up-sampled image. Our experimental results demonstrate that, in terms of PSNR (Peak Signal-to-Noise Ratio) and NQM (Noise Quality Metric), the proposed scheme outperforms four existing methods.	human visual system model;interpolation;iterative method	Guo-Shiang Lin;Min-Kuan Lai	2011	IJPRAI	10.1142/S0218001411008658	computer vision;mathematical optimization;discrete mathematics;computer science;stairstep interpolation;mathematics;nearest-neighbor interpolation;image scaling	Vision	58.45287277041981	-66.02368855137422	174983
745b64caf359c3686b1c63e9852ba724949b9f2f	an improved scheme for full fingerprint reconstruction	journal article;drntu engineering electrical and electronic engineering;image reconstruction fingerprint identification;image reconstruction;gray scale fingerprint image reconstruction full fingerprint image reconstruction fingerprint recognition systems minutiae based fingerprint templates minutiae points amplitude and frequency modulated fingerprint model am fm fingerprint model binary ridge pattern generation continuous phase refinement process phase image reconstruction spiral phase refined phase image;image reconstruction spirals gray scale fingerprint recognition frequency modulation security;reconstruction am fm model fingerprint minutiae;fingerprint identification	Different fingerprint recognition systems store minutiae-based fingerprint templates differently. Some store them inside a small token; some can be found in a server database. As the minutiae template is very compact, many take it for granted that the template does not contain sufficient information for reconstructing the original fingerprint. This paper proposes a scheme to reconstruct a full fingerprint image from the minutiae points based on the amplitude and frequency modulated (AM-FM) fingerprint model. The scheme starts with generating a binary ridge pattern which has a similar ridge flow to that of the original fingerprint. The continuous phase is intuitively reconstructed by removing the spirals in the phase image estimated from the ridge pattern. To reduce the artifacts due to the discontinuity in the continuous phase, a refinement process is introduced for the reconstructed phase image, which is the combination of the continuous phase and the spiral phase (corresponding to the minutiae). Finally, the refined phase image is used to produce a thinned version of the fingerprint, from which a real-look alike gray-scale fingerprint image is reconstructed. The experimental results show that our proposed scheme performs better than the-state-of-the-art technique.	algorithm;fm broadcasting;fingerprint recognition;grayscale;minutiae;modulation;refinement (computing);reflections of signals on conducting lines;server (computing)	Sheng Li;Alex ChiChung Kot	2012	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2012.2212012	iterative reconstruction;fingerprint;computer vision;speech recognition;computer science;pattern recognition	Vision	55.076778607656756	-61.173366500601986	175075
18e7892748a20faef2c951478084358c8fff5e00	direct depth recovery from motion blur caused by random camera rotations imitating fixational eye movements		It has been reported that small involuntary vibrations of a human eyeball for fixation called ”fixational eye movements” play a role of image analysis, for example contrast enhancement and edge detection. This mechanism can be interpreted as an instance of stochastic resonance, which is inspired by biology, more specifically by neuron dynamics. A depth recovery method has been proposed, which uses many successive image pairs generated by random camera rotations imitating fixational eye movements. This method, however, is not adequate for images having fine texture details because of an aliasing problem. To overcome this problem, we propose a new integral formed method for recovering depth, which uses motion blur caused by the same camera motions, i.e. many random small camera rotations. As an algorithm, we examine a method directly recovering depth without computing a blur function. To confirm the feasibility of our scheme, we perform simulations using artificial images.	algorithm;aliasing;approximation algorithm;box blur;computation;depth map;edge detection;experiment;gaussian blur;image analysis;mike lesser;neuron;numerical analysis;real-time clock;simulation;stochastic resonance	Norio Tagawa;Shoei Koizumi;Kan Okubo	2013			computer vision;computer graphics (images)	Vision	55.769269523145795	-58.190114773899744	175172
0d0246cb5ee53f3697095bd01c18d67ce6eefb40	combinatorial and geometric problems related to digital halftoning	binary image	Digital halftoning is a technique to convert a continuous-tone image into a binary image consisting of black and white dots. It is an important technique for printing machines and printers to output an image with few intensity levels or colors which looks similar to an input image. The purposes of this paper are to reveal that there are a number of problems related to combinatorial and computational geometry and to present some solutions or clues to those problems.		Tetsuo Asano;Naoki Katoh;Koji Obokata;Takeshi Tokuyama	2002		10.1007/3-540-36586-9_4	computer vision;binary image;theoretical computer science;digital image processing;mathematics;error diffusion;computer graphics (images)	Theory	62.53042184466618	-57.31998618694555	175410
e2a892591ba83a8382fba330501b44af5e693bce	availability of redundancy of projection data applied to reduce the noise sensitivity of reconstructed images		Abstract#R##N##R##N#Previously, in the processing of CT projection data, it was assumed that the measured values are independent at each point. Here it is shown that the actual data have a dependent property, which is called the redundancy of the projection data. It is shown that the redundancy is effective for the reduction of the noise sensitivity of the reconstruction formula. This paper evaluates the effect qualitatively and experimentally in comparison with the convolution method, which is the mainstream of the present reconstruction procedure. The reconstruction formula (formula II), which was derived in a previous paper to reduce the noise sensitivity utilizing the redundancy, is compared with the convolution method. The analytical comparison with the mean-square error as the evaluation measure, as well as the visual comparison of the actually reconstructed examples, are performed.		Itsuo Kumazawa;Taizo Iijima	1986	Systems and Computers in Japan	10.1002/scj.4690170309	theoretical computer science;machine learning;mathematics;algorithm	HPC	66.81073510280973	-58.94276933092099	175488
fbd047862ea869973ecf8fc35ae090ca00ff06d8	literature review of fingerprint quality assessment and its evaluation	fingerprint image quality assessment;fqa;biometrics;quality metric evaluation;quality assessment;fingerprint image quality evaluation approach;evaluation;enrolment selection;index terms fingerprint;noisy information	Fingerprint quality assessment (FQA) has been a challenging issue due to a variety of noisy information contained in the samples, such as physical defect and distortions caused by sensing devices. Existing studies have made efforts to find out more suitable techniques for assessing fingerprint quality but it is difficult to achieve a common solution because of, for example, different image settings. This paper gives a twofold study related to FQA, including a literature review of the prior work in assessing fingerprint image quality and the associated evaluation approaches. First, we categorized some representative studies proposed in last few decades to show how this problem has been solved so far. Second, the paper gives a brief introduction of the associated evaluation approaches, and then contributes an extended evaluation framework based on the enrollment selection, which offers repeatable and statistically convincing measures for evaluating quality metrics. Experimental results demonstrate the usability of the proposed evaluation framework via offline trials.	biometrics;categorization;distortion;enhanced entity–relationship model;fingerprint;globalization management system;image processing;image quality;online and offline;performance evaluation;software bug;software quality assurance;usability	Zhigang Yao;Jean-Marie Le Bars;Christophe Charrier;Christophe Rosenberger	2016	IET Biometrics	10.1049/iet-bmt.2015.0027	data science;evaluation;archaeology;data mining;biometrics	HCI	61.86634179324111	-64.09763902198374	176004
6ac2eea7d90fff64cc89ba4fce77c71651f55dab	no-reference stereoscopic 3d image quality assessment via combined model	no-reference 3d iqa;human visual system;statistic feature;ssim	Currently, stereoscopic 3D image has been widely applied in many fields. However, it may suffer from various quality degradations during the acquisition and transmission. Therefore, an effective 3D image quality assessment (IQA) method has great significance for 3D multimedia applications. Since 3D image pair has two images, it is easily distorted asymmetrically. In this paper, we have designed a no-reference quality assessment algorithm for asymmetrically distorted 3D images by utilizing combined model. First, in order to extract the distorted information in different frequency, the Gabor filter bank is employed to decompose the 3D image pair. Second, the “Cyclopean” and difference maps, representing for binocular characteristic and asymmetric information, are generated from the Gabor filter results. Then, the statistical characteristics of “Cyclopean” and difference maps are estimated by utilizing the generalized Gaussian distribution (GGD) fitting. Finally, a SVR regression is learned to map the feature vector to the recorded subjective difference mean opinion scores (DMOS). Besides, we also make an attempt to utilize structural similarity index (SSIM) to measure the asymmetric information of 3D image pair. The performance of our algorithm is evaluated on the popular 3D IQA databases. Extensive results show that the proposed algorithm outperforms state-of-the-art no-reference 3D IQA algorithms and is comparable to some full-reference 3D IQA algorithms.	algorithm;binocular vision;database;experiment;feature vector;filter bank;gabor filter;human visual system model;image quality;map;noise reduction;stereoscopic video game;stereoscopy;structural similarity	Lili Shen;Jinyi Lei;Chunping Hou	2017	Multimedia Tools and Applications	10.1007/s11042-017-4709-7	stereoscopy;computer vision;computer science;artificial intelligence;image quality;structural similarity;gabor filter;pattern recognition;feature vector;generalized normal distribution;human visual system model	Vision	62.319450734055266	-65.0100669387003	176017
889dbb4fc8f8cfc3fd7dd8d1fe4ae900bb73ee73	new adaptive postprocessing to remove blocking artifacts for dct coded image	quantization;adaptive thresholds postprocessing blocking artifact;adaptive thresholding;quantization constraint set;image coding;psnr;convex programming;block based discrete cosine transform;projection onto convex sets;adaptive codes;visual quality;blocking artifacts;quantisation signal;statistical properties;subjective visual qualities;deblocking filter;statistical analysis adaptive codes convex programming discrete cosine transforms image coding quantisation signal;smoothing methods;statistical analysis;filtering algorithms;discrete cosine transforms;subjective visual qualities adaptive postprocessing blocking artifacts dct coded image block based discrete cosine transform serial concatenation deblocking filter pocs algorithm local statistical property quantization constraint set projection onto convex sets algorithm;pixel;pocs algorithm;projection onto convex sets algorithm;postprocessing;dct coded image;adaptive thresholds;discrete cosine transforms iterative algorithms low pass filters quantization image coding adaptive filters smoothing methods frequency iterative methods video compression;local statistical property;based discrete cosine transform;algorithm design and analysis;filtering theory;blocking artifact;serial concatenation;adaptive postprocessing	To remove blocking artifacts, this paper proposes a novel adaptive postprocessing method in block-based discrete cosine transform (BDCT) coded images and presents a framework, which is a serial-concatenation of a simple deblocking filter and a constraint set based on the POCS algorithm. At first, we classify the simple deblocking filter by an adaptive threshold depending on local statistical properties, and in the meantime, update block types appropriately by a simple rule, which affects the performance of deblocking progresses decisively. Secondly, a quantization constraint set (QCS) based on the projection onto convex sets (POCS) algorithm is introduced. At last, comprehensive experiments indicate that the proposed algorithm outperforms a large number of deblocking methods in the literature, in terms of evaluation of PSNR and subjective visual qualities.	algorithm;blocking (computing);concatenation;deblocking filter;discrete cosine transform;experiment;norm (social);peak signal-to-noise ratio	Zhongjing Wang;Jongho Kim;Jechang Jeong	2008	2008 IEEE International Conference on Signal Image Technology and Internet Based Systems	10.1109/SITIS.2008.61	algorithm design;computer vision;mathematical optimization;convex optimization;quantization;peak signal-to-noise ratio;computer science;deblocking filter;theoretical computer science;mathematics;thresholding;pixel;statistics	Robotics	57.46087071763146	-63.5401313309038	176114
e21816f2bb273d6f4fbcbbff944d4afdb23cc379	depth extraction using adaptive blur channel selection for dual aperture camera	cmos image sensors;image edge detection;image color analysis;feature extraction;correlation coefficient;cameras;apertures	We introduce the Dual-aperture (DA) system which is a novel camera based on depth from defocus (DFD). However, conventional method for DA system has an object color dependency problem, which comes from color transition region between objects or textures in an object. This problem is very important because it results in errors in depth map. For this reason, we propose a 2-step process to obtain the improved depth map compared to the conventional method. First is new interpolation method to align the edge in different color channel using IR channel information. Second is the process to adaptively select suitable color channel for the depth extraction. In addition, experimental results indicate that we can obtain the improved depth map only with a small amount of additional computing time, or about 1% of the computing time for the conventional method.	align (company);channel (digital image);data flow diagram;depth map;gaussian blur;interpolation	Kyungho Kim;Yeongmin Lee;Hyun Sang Park;Chong-Min Kyung	2016	2016 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)	10.1109/APCCAS.2016.7803932	aperture;computer vision;color depth;feature extraction;computer science;machine learning;computer graphics (images)	Vision	58.56393032445672	-59.43258024650304	176412
b8f922b37d1994ded050723816285313ad3d09cb	shape and reflectance estimation from dielectric materials using statistical analysis and polarisation		Polarisation has proven to be an effective method in the analysis of light reflection in computer vision. It plays an important role in separating reflectance components using cross polarisation settings. Some polarisation based methods have also been presented in the literature for surface orientation estimation, where the incident light is unpolarised. This thesis aims at exploiting the polarising properties of surface reflection for computer vision applications. In particular, we focus on developing a framework for measuring the shape and reflectance information based on polarisation data. The polarised images are acquired using simple and low-cost devices, and the estimates are satisfactory compared with other surface characteristic methods such as shape from shading and stereo approaches. Statistical methods are also used to aid the computations, such as blind source separation (BSS), mutual information estimation, the iterated conditional modes method, etc. Chapter 2 surveys the related literature in the fields of polarisation, reflectance function analysis, shape recovery, and pattern recognition. We also discuss other shape recovery techniques, such as shape from shading, photometric stereo, and geometric stereo. We make comparisons between these approaches to show their advantages and drawbacks. In Chapter 3, we introduce the method of BSS, and show how to incorporate it into polarisation state estimation. The traditional method of using polarisation models for shape recovery and polarisation state estimation usually requires more than three images, normally 10 or more pictures in different polarisation angles. The proposed method solves these problems, and provides a robust way to measure polarisation state and refractive index information using only three images, and without the need to have polarisation angle information. We also demonstrate in the experimental section that the estimates using the proposed method offer significant improvements over existing polarisation based i approaches. In Chapter 4, we extend the work in the Chapter 3 in a number of ways. Firstly, we present detailed review of the Fresnel theory in polarisation vision, and propose a novel polarisation model which considers both diffuse and specular reflections. Secondly, we develop a new novel criterion functions to be used in the Chapter 3, which improves its robustness and accuracy. In Chapter 5, we also explore the shape recovery method using the polarised light. When the polarised light is transmitted through the polariser, it becomes fully polarised. The specular reflection part can be fully eliminated using the setting of “cross-polarisation”. In this case the methodology is changed accordingly. This leads us to develop a new framework which firstly uses BSS for reflectance component separation, and then applies a polarisation model built specifically for this case. The polarisation model is based on Fresnel equation and Malus’s law, which describes the reflection of polarised light when transmitting through the polariser in front of the viewer. It is also noted that the proposed framework reduces the noise in the zenith angle estimate, which we demonstrate in the experiments. In Chapter 6 we consider an alternative way to estimate shape and reflectance properties using polarised light. Using the separated diffuse and specular reflection components obtained in Chapter 5, we use parametric reflectance models, that relate the reflectance intensity with the zenith angle when the parameter values are given. The reflectance models are only applied for either diffuse or specular reflections, and the two zenith angle estimates from the two components should be identical as they describe the same object. We use mutual information estimation and Newton’s method to find their similarities and their parameter values measured by model fitting. The two estimates then are combined using a mixture model. We choose six reflectance models and compare their performance in this framework.	blind signal separation;computation;computer vision;curve fitting;diffuse reflection;effective method;experiment;iterated conditional modes;iteration;mixture model;mutual information;newton;newton's method;pattern recognition;photometric stereo;polarization (waves);polarizer;ray (optics);reflection (computer graphics);shading;source separation;specular highlight;transmitter	Lichi Zhang	2008			computer vision;geography;optics;engineering drawing	Vision	59.23816805720473	-52.20285276255614	176554
022eca0078ed5350dd979614dea87caf6921b184	using reduced motion vector set for real time motion registration in hdr imaging	vectors algorithm design and analysis motion estimation joints cameras computational complexity image registration;computational complexity motion vector set reduction hdr imaging global motion registration algorithm hierarchical motion estimation hdr input images handheld devices;computational complexity;image registration;image registration computational complexity	In this paper, we propose a new global motion registration algorithm based on hierarchical motion estimation of a reduced motion vector set, specifically adapted for the properties of the HDR input images, and designed for implementation on handheld devices, demanding low computational complexity and memory load. The procedure is fully automatic and does not require any end-user involvement. The algorithm shows great effectiveness, correctly registering over 95% of the HDR input images captured by free hand.	algorithm;algorithmic efficiency;computation;computational complexity theory;high-dynamic-range imaging;image quality;mobile device;motion estimation;rollover (key)	Tomislav Kartalov;Zoran A. Ivanovski;Ljupcho Panovski	2013	2013 IEEE 15th International Workshop on Multimedia Signal Processing (MMSP)	10.1109/MMSP.2013.6659312	computer vision;quarter-pixel motion;computer science;image registration;theoretical computer science;motion estimation;computational complexity theory;algorithm;computer graphics (images)	Vision	55.493434746235195	-55.5111943959128	176808
be410ee7c6552e027641d6adb749d045bea1b1aa	variational disparity estimation framework for plenoptic image			binocular disparity;variational principle	Trung-Hieu Tran;Sven Simon	2018	CoRR			Vision	56.62618464748915	-55.087787046017	176931
fc57da72d92a696ffaec6936b3be0d8f8eb3df05	camera identification from cropped and scaled images	test hypothese;filigranage numerique;protection information;digital watermarking;digital forensics;sistema 2 canales;detection numerique;cross correlation;sensors;generalized likelihood ratio test;zoom lenses;two channel system;digital watermark;correlation croisee;test hipotesis;zoom;deteccion numerica;accuracy;precision;proteccion informacion;fingerprint recognition;information protection;filigrana digital;likelihood ratio test;digital detection;test razon verosimilitud;systeme 2 canaux;test rapport vraisemblance;cameras;correlacion cruzada;normalized cross correlation;reverse engineering;hypothesis test	In this paper, we extend our camera identification technology based on sensor noise to a more general setting when the image under investigation has been simultaneously cropped and scaled. The sensor fingerprint detection is formulated using hypothesis testing as a two-channel problem and a detector is derived using the generalized likelihood ratio test. A brute force search is proposed to find the scaling factor which is then refined in a detailed search. The cropping parameters are determined from the maximum of the normalized cross-correlation between two signals. The accuracy and limitations of the proposed technique are tested on images that underwent a wide range of cropping and scaling, including images that were acquired by digital zoom. Additionally, we demonstrate that sensor noise can be used as a template to reverse-engineer in-camera geometrical processing as well as recover from later geometrical transformations, thus offering a possible application for re-synchronizing in digital watermark detection.	analysis of algorithms;authorization;brute-force search;cross-correlation;digital watermarking;digital zoom;fingerprint;image noise;image scaling;jpeg;reverse engineering	Miroslav Goljan;Jessica J. Fridrich	2008		10.1117/12.766732	computer vision;simulation;telecommunications;digital watermarking;likelihood-ratio test;digital forensics;cross-correlation;accuracy and precision	ML	53.9610771531488	-61.27993811937632	177051
9e1a04d4c6875ab9f30c4f90a0bf5a5f0f0441b4	adaptive order spline interpolation for edge-preserving colour filter array demosaicking	spline;interpolation;cubic spline interpolation adaptive order spline interpolation edge preserving colour filter array demosaicking single sensor digital camera full colour image capture image sensor;edge detection;optical filters;digital camera;image sensors;adaptive algorithms;splines mathematics;interpolation spline image color analysis image edge detection estimation equations adaptive algorithms;estimating equation;adaptive algorithm;estimation;image edge detection;image color analysis;image colour analysis;adaptive method;splines mathematics image colour analysis image sensors interpolation optical filters;spline interpolation;image sensor;cubic spline	When a single-sensor digital camera is used for full colour image capture, only one colour for each pixel is acquired through the colour filter array which is placed in front of the image sensor. CFA demosaicking is a process for the estimation of the other missing colour values in order to produce a full colour image. To preserve sharp colour edges and avoid colour artifacts, it is desirable not to interpolate across an edge. It has been shown that cubic spline interpolation has been applied successfully for CFA demosaicking. However, cubic spline interpolation utilises pixels as far as five pixels away from the point of estimation. If an image contains closer edges, cubic spline interpolation will inevitably include samples from the other side of the edge for interpolation, and a lower order spline interpolation should be applied so as to avoid pixel values from the other side of the edge. In this paper, we propose an adaptive method to apply various orders of spline interpolation depending on the location of an edge from the point of estimation. It has been shown that images with closer edges can now be demosaicked to produce more accurate outputs with less colour artifacts.	color filter array;color image;cubic hermite spline;cubic function;demosaicing;digital camera;image sensor;monochrome;pixel;spline interpolation	Sharmil Randhawa;J. S. Jimmy Li	2011	2011 International Conference on Digital Image Computing: Techniques and Applications	10.1109/DICTA.2011.118	demosaicing;spline;computer vision;mathematical optimization;computer science;stairstep interpolation;image sensor;bicubic interpolation;mathematics;geometry;nearest-neighbor interpolation;multivariate interpolation;statistics	Vision	59.149484709112976	-60.09749564149976	177095
bd891497d2a321676d7c9aa1fa5eb7a057d67195	saliency target detection in polarimetric sar images	filtering;speckle;synthetic aperture radar filtering object detection visualization speckle covariance matrices;visualization;covariance matrices;synthetic aperture radar correlation methods object detection radar imaging radar polarimetry;saliency target detection nonlocal filtering pattern recurrence visual attention polarimetric sar images local neighbors cross correlation normalized patches cfar;object detection;synthetic aperture radar	Inspired by nonlocal filtering, this paper proposes a saliency detector based on pattern recurrence. In visual attention and saliency detection framework, people are extracting patches that are not redundant, which would more likely to attracting attention. Hence, target detection or saliency detection in SAR image could also be done using the dissimilarity as an indicator of saliency, meaning that the interesting target or saliency target is different from background around it. Similarity of two pixels can be defined together with their local neighbors, then calculate the cross-correlation of two normalized patches. To analyze results better, a normalized version of cross-correlation is used. Experimental results on SAR image are shown to test the effectiveness of the proposed method. The experimental results compared with CFAR on SAR images also prove the effectiveness in saliency detection on SAR images.	constant false alarm rate;cross-correlation;nonlocal lagrangian;patch (computing);pixel;polarimetry	Haipeng Wang;Feng Xu	2016	2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2016.7729263	filter;speckle pattern;speckle noise;computer vision;continuous-wave radar;synthetic aperture radar;visualization;computer science;optics;radar imaging;inverse synthetic aperture radar;physics;remote sensing	Vision	53.8828971642986	-62.483559355468195	177123
1344d6748171e44bf827e5d2c375ae7374adab6d	when does computational imaging improve performance?	image coding;decoding;image priors;cameras signal to noise ratio multiplexing lighting performance gain;image restoration;multiplexing;motion deblurring;digital photography;computational imaging;image restoration computational imaging techniques image quality improvement optical coding stronger signal level measurement decoding noise amplification computational photography deconvolution denoising;defocus deblurring;extended depth of field;deconvolution;computational photography;image denoising;denoising;computer science;image restoration decoding deconvolution digital photography image coding image denoising;multiplexing computational imaging computational photography deconvolution defocus deblurring denoising extended depth of field image priors image restoration motion deblurring	A number of computational imaging techniques are introduced to improve image quality by increasing light throughput. These techniques use optical coding to measure a stronger signal level. However, the performance of these techniques is limited by the decoding step, which amplifies noise. Although it is well understood that optical coding can increase performance at low light levels, little is known about the quantitative performance advantage of computational imaging in general settings. In this paper, we derive the performance bounds for various computational imaging techniques. We then discuss the implications of these bounds for several real-world scenarios (e.g., illumination conditions, scene properties, and sensor noise characteristics). Our results show that computational imaging techniques do not provide a significant performance advantage when imaging with illumination that is brighter than typical daylight. These results can be readily used by practitioners to design the most suitable imaging systems given the application at hand.	computation;daylight;image noise;image quality;imaging techniques;noise-induced hearing loss;signal-to-noise ratio;throughput	Oliver Cossairt;Mohit Gupta;Shree K. Nayar	2013	IEEE Transactions on Image Processing	10.1109/TIP.2012.2216538	image restoration;computer vision;digital photography;computational photography;computer science;deconvolution;noise reduction;multiplexing;computer graphics (images)	Visualization	60.25770211315343	-57.03797324252118	177259
49f6888afb1991ac09f735c82891e400c42680ef	a new binarization algorithm for historical documents		Monochromatic documents claim for much less computer bandwidth for network transmission and storage space than their color or even grayscale equivalent. The binarization of historical documents is far more complex than recent ones as paper aging, color, texture, translucidity, stains, back-to-front interference, kind and color of ink used in handwriting, printing process, digitalization process, etc. are some of the factors that affect binarization. This article presents a new binarization algorithm for historical documents. The new global filter proposed is performed in four steps: filtering the image using a bilateral filter, splitting image into the RGB components, decision-making for each RGB channel based on an adaptive binarization method inspired by Otsu’s method with a choice of the threshold level, and classification of the binarized images to decide which of the RGB components best preserved the document information in the foreground. The quantitative and qualitative assessment made with 23 binarization algorithms in three sets of “real world” documents showed very good results.	algorithm;bilateral filter;binary image;grayscale;historical document;interference (communication);mega man network transmission;monochrome;otsu's method;printing	Marcos Martins de Almeida;Rafael Dueire Lins;Rodrigo Barros Bernardino;Darlisson Marinho de Jesus;Bruno Lima	2018	J. Imaging	10.3390/jimaging4020027	mathematics;grayscale;computer vision;filter (signal processing);artificial intelligence;monochromatic color;bilateral filter;algorithm;handwriting;rgb color model;communication channel;bandwidth (signal processing)	Vision	56.89870067783874	-63.49019045182155	177363
8c675bf5b542b98bf81dcf70bd869ab52ab8aae9	analysis of focus measure operators for shape-from-focus	defocus model;focus measure;autofocus;shape from focus	Shape-from-focus (SFF) has widely been studied in computer vision as a passive depth recovery and 3D reconstruction method. One of the main stages in SFF is the computation of the focus level for every pixel of an image by means of a focus measure operator. In this work, a methodology to compare the performance of different focus measure operators for shape-from-focus is presented and applied. The selected operators have been chosen from an extensive review of the state-of-the-art. The performance of the different operators has been assessed through experiments carried out under different conditions, such as image noise level, contrast, saturation and window size. Such performance is discussed in terms of the working principles of the analyzed operators.	3d reconstruction;computation;computer vision;experiment;image noise;noise (electronics);pixel	Said Pertuz;Domenec Puig;Miguel Angel García	2013	Pattern Recognition	10.1016/j.patcog.2012.11.011	computer vision;autofocus;computer science;mathematics	Vision	59.051856180644414	-59.258590433338966	177624
6e9cd163fa2892c205e28d0285b2706a9ff07a71	analysis of a physically realistic film grain model, and a gaussian film grain synthesis algorithm	texture synthesis;film grain;boolean model;stochastic geometry	Film grain is a highly valued characteristic of analog images, thus realistic digital film grain synthesis is an important objective for many modern photographers and film-makers. We carry out a theoretical analysis of a physically realistic film grain model, based on a Boolean model, and derive expressions for the expected value and covariance of the film grain texture. We approximate these quantities using a Monte Carlo simulation, and use them to propose a film grain synthesis algorithm based on Gaussian textures. With numerical and visual experiments, we demonstrate the correctness and subjective qualities of the proposed algorithm		Alasdair Newson;Noura Faraj;Julie Delon;Bruno Galerne	2017		10.1007/978-3-319-58771-4_16	stochastic geometry;monte carlo method;expected value;correctness;gaussian;covariance;algorithm;texture synthesis;film grain;computer science	EDA	63.81033203929614	-52.60616842113645	177675
01bcf123a66ebe166769fe07c95e26a899c28dac	artifact reduction using reliability reasoning for image generation of ftv	artefacts;reliability;forward warping;ftv;image interpolation;image generation;view synthesis;backward warping;depth image based rendering;depth error	1047-3203/$ see front matter 2009 Elsevier Inc. A doi:10.1016/j.jvcir.2009.09.009 * Corresponding author. Fax: +81 52 789 3628. E-mail address: yanglu@tanimoto.nuee.nagoya-u.a In this paper, we present a new view synthesis method in multiview camera configurations of Free viewpoint TV (FTV) where potential depth errors are considered. The emphasis is on the artifacts eliminating for photorealistic synthesis especially near object boundaries. In contrast to conventional techniques which ignore geometry errors, we first categorize the artifact cases and depth modes. Furthermore, this paper infers the complementarity principle of the artifacts from left and right references. This complementarity guarantees the effectiveness of our reliability-based synthesis. The reliability reasoning is crucial for artifacts reduction. The reliable and unreliable areas from different views can be correctly labeled. Then artifacts caused by unreliable pixels from one reference can be replaced by the reliable pixels from the other reference. As a final result, artifacts of novel view are demonstrated to be significantly reduced on different multiview sequences. 2009 Elsevier Inc. All rights reserved.	categorization;complementarity (physics);complementarity theory;fax;glossary of computer graphics;pixel;reasoning system;view synthesis	Lu Yang;Tomohiro Yendo;Mehrdad Panahpour Tehrani;Toshiaki Fujii;Masayuki Tanimoto	2010	J. Visual Communication and Image Representation	10.1016/j.jvcir.2009.09.009	computer vision;computer science;theoretical computer science;reliability;mathematics;statistics;image scaling;computer graphics (images)	AI	56.17197934489898	-56.844002478567475	177822
7fe63ad0a306ac2c541171a90509038edad159c1	am object-based approach to plenoptic videos	error correction codes;linear camera arrays;image segmentation;conference_paper;data compression;video sequences;video segmentation;plenoptic video sequences;layout;content scalability;video coding;rendering computer graphics image segmentation cameras layout sampling methods video sequences image sequences shape scalability resilience;shape information;shape;resilience;video cameras;data compression rendering computer graphics video coding realistic images image sequences image segmentation error correction codes video cameras;object based approach;image sequence;individual ibr object interactivity;portable capturing system;electronics;image sequence object based approach plenoptic video sequences video segmentation depth map shape information content scalability error resilience individual ibr object interactivity portable capturing system linear camera arrays jvc video cameras compression real world scenes image based rendering;realistic images;error resilience;scalability;jvc video cameras;compression;image based rendering;sampling methods;depth map;real world scenes;rendering computer graphics;cameras;image sequences	This paper proposes an object-based approach to plenoptic videos, where the plenoptic video sequences are segmented into image-based rendering (IBR) objects each with its image sequence, depth map and other relevant information such as shape information. This allows desirable functionalities such as scalability of contents, error resilience, and interactivity with individual IBR objects to be supported. A portable capturing system consisting of two linear camera arrays, each hosting 6 JVC video cameras, was developed to verify the proposed approach. Rendering and compression results of real-world scenes demonstrate the usefulness and good quality of the proposed approach.		Zhi-Feng Gan;Shing-Chow Chan;King To Ng;Harry Shum	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1465367	data compression;layout;sampling;computer vision;electronics;scalability;image-based modeling and rendering;shape;computer science;multimedia;image segmentation;compression;psychological resilience;statistics;depth map;computer graphics (images)	Vision	58.72050028727478	-54.051910020810624	177844
12b22c739aee955016e9a549c200bd3764005d68	an iterative approach for shadow removal in document images		Uneven illumination and shadows in document images cause a challenge for digitization applications and automated workflows. In this work, we propose a new method to recover unshadowed document images from images with shadows/un-even illumination. We pose this problem as one of estimating the shading and reflectance components of the given original image. Our method first estimates the shading and uses it to compute the reflectance. The output reflectance map is then used to improve the shading and the process is repeated in an iterative manner. The iterative procedure allows for a gradual compensation and allows our algorithm to even handle difficult hard shadows without introducing any artifacts. Experiments over two different datasets demonstrate the efficacy of our algorithm and the low computation complexity makes it suitable for most practical applications.	algorithm;computation;illumination (image);iteration;iterative method;shading	Vatsal Shah;Vineet Gandhi	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462476	interpolation;computation;shading;pattern recognition;artificial intelligence;digitization;reflectivity;computer science;shadow	Vision	57.55477605969921	-53.350546767220486	177989
ea12d466c04b883bec79c5fbe91ebbf0a00a83c8	real-time video re-coloring algorithm considering the temporal color consistency for the color-blind	color adjustment;temporal color consistency;fast temporal color adjustment technique;video signal processing;computational complexity real time video re coloring algorithm temporal color consistency color blind fast color modification method color differences perceptual illusion based color transform technique color contrast fast temporal color adjustment technique real time video playback video sequences;video processing;video sequences;color vision deficiency;real time video playback;perceptual illusion based color transform technique;brightness;video processing color vision deficiency color adjustment color blind re coloring protanope;vectors;streaming media;computational complexity;image color analysis;image colour analysis;re coloring;color differences;fast color modification method;transforms;protanope;real time video re coloring algorithm;image color analysis real time systems streaming media vectors computational efficiency brightness transforms;computational efficiency;video signal processing computational complexity image colour analysis image sequences real time systems transforms;color contrast;color blind;real time systems;image sequences	In this paper, a fast color modification method is proposed for the color-blind (CB) who experiences difficulty in discriminating some color differences. The proposed algorithm uses a new perceptual illusion based color transform technique that can enhance the color contrast for the CB while preventing significant change of the original image. In addition, a fast temporal color adjustment technique is introduced for the real-time video playback. Experimental results show that the proposed method can produce more comprehensible video sequences for the CB with reduced computational complexity.	algorithm;color management;computational complexity theory;graph coloring;real-time clock	Jae-Yun Jeong;Hyun-Ji Kim;Tae-Shick Wang;Seung-Won Jung;Sung-Jea Ko	2012	IEEE Transactions on Consumer Electronics	10.1109/TCE.2012.6227482	computer vision;computer science;electrical engineering;multimedia;video processing;computational complexity theory;brightness;algorithm;computer graphics (images)	Vision	58.72546069787224	-58.3467658820242	178095
07ddec3daa84baf09105bb71f3a8665dbec74695	the non-parametric sub-pixel local point spread function estimation is a well posed problem	subpixel convolution kernel estimation;aliasing;camera quality assessment;point spread function;modulated transfer function;image blur;inverse problems	Most medium to high quality digital cameras (dslrs) acquire images at a spatial rate which is several times below the ideal Nyquist rate. For this reason only aliased versions of the cameral point-spread function (psf) can be directly observed. Yet, it can be recovered, at a sub-pixel resolution, by a numerical method. Since the acquisition system is only locally stationary, this psf estimation must be local. This paper presents a theoretical study proving that the sub-pixel psf estimation problem is well-posed even with a single well chosen observation. Indeed, theoretical bounds show that a near-optimal accuracy can be achieved with a calibration pattern mimicking a Bernoulli(0.5) random noise. The physical realization of this psf estimation method is demonstrated in many comparative experiments. We use an algorithm to accurately estimate the pattern position and its illumination conditions. Once this accurate registration is obtained, the local psf can be directly computed by inverting a well conditioned linear system. The psf estimates reach stringent accuracy levels with a relative error of the order of 2% to 5%. To the best of our knowledge, such a regularization-free and model-free sub-pixel psf estimation scheme is the first of its kind.	algorithm;approximation error;digital camera;display resolution;experiment;linear system;noise (electronics);numerical method;nyquist rate;pixel;stationary process;sub-pixel resolution;well-posed problem	Mauricio Delbracio;Pablo Musé;Andrés Almansa;Jean-Michel Morel	2011	International Journal of Computer Vision	10.1007/s11263-011-0460-0	aliasing;computer vision;mathematical optimization;computer science;inverse problem;point spread function;mathematics;statistics	Vision	61.887731241593634	-54.242989430703915	178288
a3d2d934b4e21cc86682c424e3c3aa7111b61f11	display pre-filtering for multi-view video compression	3d displays;video streaming;multi view compression;data compression;video compression;monograph or book;multi dimensional;antialiasing;image quality;stereoscopic display;3d display	Multi-view 3D displays are preferable to other stereoscopic display technologies because they provide autostereoscopic viewing from any viewpoint without special glasses. However, they require a large number of pixels to achieve high image quality. Therefore, data compression is a major issue for this approach. In this paper, we present a framework for efficient compression of multi-view video streams for multi-view 3D displays. Our goal is to optimize image quality without increasing the required data bandwidth. We achieve this by taking into account a precise notion of the multi-dimensional display bandwidth. The display bandwidth implies that scene elements that appear at a given distance from the display become increasingly blurry as the distance grows. Our main contribution is to enhance conventional multi-view compression pipelines with an additional pre-filtering step that bandlimits the multi-view signal to the display bandwidth. This imposes a shallow depth of field on the input images, thereby removing high frequency content. We show that this pre-filtering step leads to increased image quality compared to state-of-the-art multi-view coding at equal bitrate. We present results of an extensive user study that corroborate the benefits of our approach. Our work suggests that display pre-filtering will be a fundamental component in signal processing for 3D displays, and that any multi-view compression scheme will benefit from our pre-filtering technique.	autostereoscopy;bandlimiting;data compression;display device;high frequency content measure;image quality;multiview video coding;pipeline (computing);pixel;signal processing;stereo display;stereoscopy;streaming media;usability testing	Matthias Zwicker;Sehoon Yea;Anthony Vetro;Clifton Forlines;Wojciech Matusik;Hanspeter Pfister	2007		10.1145/1291233.1291461	video compression picture types;data compression;computer vision;stereo display;image compression;volumetric display;computer science;multimedia;statistics;computer graphics (images)	Graphics	61.096613525981866	-55.49887842280103	178351
deb5d33b07b50389ee33ebc9b095acc0a3be5d96	image contrast enhancement using classified virtual exposure image fusion	contrast enhancement;image fusion;image classification;photography;video cameras image classification image enhancement image fusion photography;image enhancement;image fusion dynamic range discrete wavelet transforms weight measurement digital cameras brightness;image fusion classified virtual exposure image fusion contrast enhancement exposure fusion;video cameras;classified virtual exposure image fusion;low contrast imaging image contrast enhancement digital camera smart phone over exposure artifact under exposure artifact cveif classified virtual exposure image fusion f stop concept photography distinct luminance class backlight imaging dark scene imaging;exposure fusion;article	In our daily life, digital cameras and smart phones have been widely used to take pictures. However, digital cameras and smart phones have a limited dynamic range, which is much lower than that human eyes can perceive. Thus, the photographs taken in high dynamic range scenes often exhibit under-exposure or over-exposure artifacts in shadow or highlight regions. In this study, an image fusion based approach, called classified virtual exposure image fusion (CVEIF), is proposed for image enhancement. First, a function imitating the F-stop concept in photography is designed to generate several virtual images having different intensity. Then, a classified image fusion method, which blends pixels in distinct luminance classes using different fusion functions, is proposed to produce a fused image in which every image region is well exposed. Experimental results on four different kinds of generic images, including a normal image, a low-contrast images, a backlight image, and a dark scene image, have shown that the proposed CVEIF approach produced more pleasingly enhanced images than other methods.	algorithm;backlight;digital camera;entity framework;high dynamic range;image editing;image fusion;pixel;smartphone	Chang-Hsing Lee;Ling-Hwei Chen;Wei-Kang Wang	2012	IEEE Transactions on Consumer Electronics	10.1109/TCE.2012.6414993	image quality;image warping;image texture;image restoration;computer vision;contextual image classification;computational photography;feature detection;image resolution;color image;binary image;image processing;photography;digital image processing;digital imaging;clipping;edge enhancement;image fusion;image formation;digital image;computer graphics (images)	Vision	59.73309975271024	-61.13672265402212	178360
988e5bfa01eae20185144a3fee757e4e936d5920	an improved alternating-projection demosaicing algorithm	psnr performance;interpolation;image segmentation;psnr;least mean squares methods;minimum square error;minimum squared error;channel estimation;alternating projections;interpolated artifact;visualization;image edge detection;least mean squares methods image colour analysis image segmentation interpolation;image color analysis;image colour analysis;pixel;alternating projection demosaicing algorithm;interpolation psnr sensor arrays information security filters computational intelligence information science computer errors testing bandwidth;interpolated artifact alternating projection demosaicing algorithm psnr performance minimum square error	To improve PSNR performance of alternating-projection demosaicing algorithm, the estimated initialization of alternating-projection algorithm that based on pre-estimating the minimum square error is meliorated in this paper. The improved algorithm is compared with other typical demosaicing approaches by testing the twenty-four Kodak set (6) images. The PSNR of the improved algorithm is better than other typical methods, and the interpolated artifact is found to be less. The experimental results prove that the improved algorithm is with best PSNR and the least interpolated artifact. The performance of alternating-projection algorithm can be improved by mending the initialization.	algorithm;demosaicing;interpolation;peak signal-to-noise ratio	Wei-jiang Wang;Ting-zhi Shen;Xue-mei Yan	2008	2008 International Conference on Computational Intelligence and Security	10.1109/CIS.2008.116	computer vision;mathematical optimization;visualization;peak signal-to-noise ratio;interpolation;computer science;pattern recognition;mathematics;image segmentation;pixel;statistics	EDA	57.23737335316105	-65.4665904095646	178394
d78e2dd3a4ed2195fbc0ba7385c4930c86b1acb0	multiresolution fusion in remotely sensed images: use of gibbs prior and pso optimization	remote sensing image;spatial resolution histograms remote sensing pixel satellites cost function;histograms;satellite data;gibbs distribution;high resolution;filter bank;image resolution;cost function;time complexity;image fusion;model based approach;multiresolution fusion;markov random field;particle swarm optimizer;remote sensing image fusion image resolution markov processes particle swarm optimisation;computational complexity;particle swarm optimization;remote sensing;satellites;joint probability density function;pixel;pso optimization;particle swarm optimization multiresolution fusion remotely sensed images pso optimization high spatial resolution high spectral resolution decimation matrix entries markov random field joint probability density function gibbs distribution;markov processes;multi spectral;decimation matrix entries;multi resolution;remotely sensed images;digital number;data fitting;particle swarm optimisation;high spectral resolution;spectral resolution;high spatial resolution;spatial resolution	In this paper, we propose a model based approach for multi-resolution fusion of remotely sensed images. We obtain a high spatial resolution (HR) and high spectral resolution multi-spectral (MS) image using a high spatial resolution Panchromatic (Pan) image and a low spatial resolution (LR) MS image. This problem is ill-posed since we need to predict the missing high resolution pixels in each of the MS images and requires proper regularization in order to get better solution. Each of the low spatial resolution MS images is modeled as aliased and noisy versions of the corresponding fused HR image. The decimation matrix entries are estimated using the Pan data and the MS image. The prior for regularization is obtained by modeling the texture of the HR MS image as a Markov random field (MRF) that can be expressed as a joint probability density function (PDF) using the Gibbs distribution (GD). In our work we make inference about this joint PDF by using the available high spatial resolution Pan image. As proposed in [1] a set of filters is chosen from a filter bank to obtain the estimates of the marginal distributions of the GD as the histograms of the filtered outputs. Our final cost function consists of a data fitting term and a prior term which is then minimized to obtain the high spatial and spectral resolution MS image. The process is repeated for each of the MS images. The optimization is done using Particle swarm optimization (PSO) which can be implemented in parallel mode in order to reduce the time complexity. The main advantages of our approach are: 1) It requires no registration between Pan and MS images; 2) The spectral distortion is minimum as we are not using the actual Pan digital numbers; 3) The method can be applied to the fusion of Pan and MS images captured at different times and using different sensors. The drawback of the proposed method is its time complexity as one cannot use fast optimization approaches for minimization. However, we have attempted to reduce the computational complexity by using PSO. We demonstrate the effectiveness of our approach by conducting experiments on real satellite data captured by Quickbird satellite.	computational complexity theory;curve fitting;decimation (signal processing);distortion;experiment;filter bank;image fusion;image resolution;lr parser;loss function;marginal model;markov chain;markov random field;mathematical optimization;matrix regularization;particle swarm optimization;pixel;portable document format;sensor;time complexity;well-posed problem	Manjunath V. Joshi;Prakash P. Gajjar;Subrahmanyam Ravishankar;K. V. V. Murthy	2010	2010 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2010.5648889	computer vision;image resolution;machine learning;physics;statistics;remote sensing	Vision	68.32504845619195	-65.09033116349477	178627
0a1c05f981fba0685e34b6193fb989c34bf4c3f5	modeling the subjective quality of highly contrasted videos displayed on lcd with local backlight dimming	liquid crystal display lcd displayed image quality backlight dimming video quality contrast led backlight light leakage;image segmentation;led backlight;measurement;measurement quality assessment light emitting diodes videos algorithm design and analysis image segmentation computational modeling;light emitting diodes;sequence display subjective quality modeling highly contrasted video quality lcd local backlight dimming algorithm energy saving visual quality improvement television sets image rendition numerical signal display model objective quality metrics locally backlit displays quality assessment temporal variations power consumption contrast measure leakage modeling objective quality assessment;video quality;video signal processing image sequences liquid crystal displays;contrast;computational modeling;quality assessment;backlight dimming;liquid crystal display lcd;light leakage;displayed image quality;algorithm design and analysis;videos	Local backlight dimming is a technology aiming at both saving energy and improving visual quality on television sets. As the rendition of the image is specified locally, the numerical signal corresponding to the displayed image needs to be computed through a model of the display. This simulated signal can then be used as input to objective quality metrics. The focus of this paper is on determining which characteristics of locally backlit displays influence quality assessment. A subjective experiment assessing the quality of highly contrasted videos displayed with various local backlight-dimming algorithms is set up. Subjective results are then compared with both objective measures and objective quality metrics using different display models. The first analysis indicates that the most significant objective features are temporal variations, power consumption (probably representing leakage), and a contrast measure. The second analysis shows that modeling of leakage is necessary for objective quality assessment of sequences displayed with local backlight dimming.	algorithm;approximation;backlight;clomiphene;dark web;display resolution;evaluation;extravasation;fda quality metrics terminology;flicker (screen);grade;holomatix rendition;information leakage;lafora disease;least-squares analysis;limited company;liquid-crystal display;numerical analysis;partial least squares regression;photopsia;preparation;spectral leakage;standard deviation;television set;videocassette	Claire Mantel;Soren Bech;Jari Korhonen;Søren Forchhammer;Jesper Melgaard Pedersen	2015	IEEE Transactions on Image Processing	10.1109/TIP.2014.2383319	algorithm design;computer vision;contrast;computer science;video quality;machine learning;multimedia;image segmentation;computational model;measurement;computer graphics (images);light-emitting diode	Visualization	60.747678024993206	-62.454490182971774	178648
96adfa1d49c0cde4f307141bd45e0a90641ff161	anti-forensics of contrast enhancement in digital images	contrast enhancement;contrast enhanced;forgery detection;image database;image quality;undetectable contrast enhancement;digital image forensics;digital image;anti forensics	The blind detection of contrast enhancement in digital images has attracted much attention of the forensic analyzers. In this paper, we propose new variants of contrast enhancement operators which are undetectable by the existing contrast enhancement detectors based on the peak-gap artifacts of the pixel graylevel histogram. Local random dithering is introduced into the design of contrast enhancement mapping for removing such artifacts. Effectiveness of the proposed anti-forensic scheme is validated by experimental results on a large image database for various parameter settings. Both detectability and the resulting image quality are evaluated via comparison with the traditional contrast enhancement. The developed anti-forensic techniques could verify the reliability of existing contrast enhancement forensic tools against sophisticated attackers and serve as the targets for developing more reliable and secure forensic techniques.	anti-computer forensics;digital image;dither;image quality;pixel;sensor	Gang Cao;Yao Zhao;Rongrong Ni;Huawei Tian	2010		10.1145/1854229.1854237	computer vision;computer science;edge enhancement;multimedia;computer graphics (images)	Vision	55.65277469749345	-63.483899060024726	178714
8b5d1b01a63df695a8e5e000862c32a4b5555f54	multispectral/hyperspetral image compression using inter-band correlation and wavelet transform	haar wavelet;data compression;geographic information;spatial structure;hyper spectral;wavelet transform;image compression;loss function;remote sensing;multispectral images;multi hyper spectral;inter band correlation;spectral imaging;compression	In present era the multi/hype spectral images are being increasingly used in traditional and key application areas such as remote sensing and geosciences. These images contain geographical information and reflect the complexity of geographical features and spatial structures. As the means of observing and describing geographical phenomena, the rapid development of remote sensing has provided an enormous amount of geographical information which is highly correlated. The massive wealth of information is very useful in a variety of applications but the sheer bulk of this information has increased beyond what can be analyzed and used efficiently and effectively. This uneven increase in the technologies of gathering and analyzing information has created difficulties in its storage, transfer, and processing. The paper attempts to develop an application-specific data compression technique that exploits inter-band correlation and then applies HAAR wavelet to selected bands of the multispectral image 'orlea_s.lan'. We finally calculate the loss functions in statistics like MSE, MAE, RMSE and PSNR.	bittorrent protocol encryption;complexity;data compression;haar wavelet;image compression;loss function;multispectral image;peak signal-to-noise ratio;wavelet transform	B. K. Mishra;R. R. Sedamkar;Y. Chaudhari	2010		10.1145/1741906.1741922	computer vision;geography;multispectral pattern recognition;remote sensing;computer graphics (images)	ML	67.92420536282492	-62.561273705004446	178846
c4aa3d3df6acff478d5d0b9648818532571c1e91	shading attenuation in human skin color images	skin lesion;image processing and analysis;human skin;face detection;color image	This paper presents a new automatic method to significantly attenuate the color degradation due to shading in color images of the human skin. Shading is caused by illumination variation across the scene due to changes in local surface orientation, lighting conditions, and other factors. Our approach is to estimate the illumination variation by modeling it with a quadric function, and then relight the skin pixels with a simple operation. Therefore, the subsequent color skin image processing and analysis is simplified in several applications. We illustrate our approach in two typical color imaging problems involving human skin, namely: (a) pigmented skin lesion segmentation, and (b) face detection. Our preliminary experimental results show that our shading attenuation approach helps reducing the complexity of the color image analysis problem in these applications.	shading	Pablo Gautério Cavalcanti;Jacob Scharcanski;Carlos B. O. Lopes	2010		10.1007/978-3-642-17289-2_19	computer vision;face detection;color image;computer science;color balance;computer graphics (images)	Vision	57.472521984367454	-61.67576039521027	178941
3af5b315743dd8e673809a478da7fa1103002bb3	perceptual metrics quality: comparative study for 3d static meshes		A 3D mesh can be subjected to different types of operations, such as compression, watermarking etc. Such processes lead to geometric distortions compared to the original version. In this context, quantifying the resultant modifications to the original mesh and evaluating the perceptual quality of degraded meshes become a critical issue. The perceptual 3D meshes quality is central in several applications to preserve the visual appearance of these treatments. The used metrics results have to be well correlated to the visual perception of humans. Although there are objective metrics, they do not allow the prediction of the perceptual quality, and do not include the human visual system properties. In the current work, a comparative study between the perceptual quality assessment metrics for 3D meshes was conducted. The experimental study on subjective database published by LIRIS / EPFL was used to test and to validate the results of six metrics. The results established that the Mesh Structural Distortion Measure metric achieved superior results compared to the other metrics. KEywoRDS 3D Meshes, 3D Triangle Mesh, Human Visual System, Objective Metrics, Perceptual Quality, Quality Assessment, Static Metrics 3D, Statistical Modeling		Nessrine Elloumi;Habiba Loukil Hadj Kacem;Nilanjan Dey;Amira S. Ashour;Med Salim Bouhlel	2017	IJSSMET	10.4018/IJSSMET.2017010105	computer vision;simulation;computer science;engineering drawing	Visualization	61.799780153265445	-64.05212866071317	179018
299c00a527d106d906026f4fef8b322312fb491c	reconstructing the indirect light field for global illumination	ambient occlusion;reconstruction;light field;defocus;motion blur;indirect illumination	Stochastic techniques for rendering indirect illumination suffer from noise due to the variance in the integrand. In this paper, we describe a general reconstruction technique that exploits anisotropy in the light field and permits efficient reuse of input samples between pixels or world-space locations, multiplying the effective sampling rate by a large factor. Our technique introduces visibility-aware anisotropic reconstruction to indirect illumination, ambient occlusion and glossy reflections. It operates on point samples without knowledge of the scene, and can thus be seen as an advanced image filter. Our results show dramatic improvement in image quality while using very sparse input samplings.	ambient occlusion;composite image filter;global illumination;illumination (image);image quality;light field;pixel;reflection (computer graphics);sampling (signal processing);sparse matrix	Jaakko Lehtinen;Timo Aila;Samuli Laine;Frédo Durand	2012	ACM Trans. Graph.	10.1145/2185520.2185547	computer vision;computer science;light field;köhler illumination;optics;ambient occlusion;computer graphics (images)	Graphics	64.16577672057893	-53.5796888643189	179104
5631781cc89d6e9bfe4ad2a7d622d7c5241831f3	reconstruction-free action inference from compressive imagers	biological patents;biomedical journals;image coding;correlation cameras image coding training three dimensional displays image reconstruction optical sensors;text mining;europe pubmed central;training;citation search;pixel domain matched filters reconstruction free action inference compressive imagers persistent surveillance camera networks video data compressive cameras data deluge issues inference tasks action recognition compressive sensing theory cs theory compression ratios cs measurements compressive measurements spatio temporal smashed filters publicly available databases;citation networks;action recognition compressive sensing reconstruction free;research articles;three dimensional displays;abstracts;compressive sensing;image reconstruction;action recognition;open access;life sciences;clinical guidelines;optical sensors;correlation;full text;video surveillance image sensors inference mechanisms pose estimation surveillance;rest apis;cameras;orcids;europe pmc;reconstruction free;biomedical research;bioinformatics;literature search	Persistent surveillance from camera networks, such as at parking lots, UAVs, etc., often results in large amounts of video data, resulting in significant challenges for inference in terms of storage, communication and computation. Compressive cameras have emerged as a potential solution to deal with the data deluge issues in such applications. However, inference tasks such as action recognition require high quality features which implies reconstructing the original video data. Much work in compressive sensing (CS) theory is geared towards solving the reconstruction problem, where state-of-the-art methods are computationally intensive and provide low-quality results at high compression rates. Thus, reconstruction-free methods for inference are much desired. In this paper, we propose reconstruction-free methods for action recognition from compressive cameras at high compression ratios of 100 and above. Recognizing actions directly from CS measurements requires features which are mostly nonlinear and thus not easily applicable. This leads us to search for such properties that are preserved in compressive measurements. To this end, we propose the use of spatio-temporal smashed filters, which are compressive domain versions of pixel-domain matched filters. We conduct experiments on publicly available databases and show that one can obtain recognition rates that are comparable to the oracle method in uncompressed setup, even for high compression ratios.	biologic preservation;calcium-sensing receptor;compressed sensing;compression;computation;database;display resolution;experiment;inference;information explosion;matched filter;nonlinear system;pixel;preparation;reconstruction conjecture;unmanned aerial vehicle;version	Kuldeep Kulkarni;Pavan K. Turaga	2016	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2015.2469288	iterative reconstruction;computer vision;text mining;simulation;computer science;machine learning;data mining;mathematics;geometry;compressed sensing;correlation	Vision	60.2235006231029	-56.00336940545904	179243
4b1a1bf633b5ff563d77f2718d48936ac5a79d60	comparative analysis of pan-sharpening techniques on dubaisat-1 images	geophysical image processing;measurement spatial resolution sensor phenomena and characterization wavelet transforms;remote sensing;spectral quality pan sharpening techniques dubaisat 1 images generalized intensity hue saturation method principle component substitution method gram schmidt method component substitution category brovey transform method university of new brunswick method smoothing filter based intensity modulation method modulation based category high pass filtering method substitutive wavelet method additive wavelet luminance proportional method remote sensing ergas evaluation metric q4 evaluation metric sam evaluation metric spectral quality measurement laplacian based metric;remote sensing geophysical image processing;performance evaluation image fusion remote sensing image enhancement high resolution imaging pan sharpening	This paper evaluates the performance of a set of pan-sharpening methods on DubaiSat-1 images. DubaiSat-1 is a new satellite and the evaluation of pan-sharpening methods shall promote new applications of data. Methods are selected to represent different approaches of pan-sharpening. The methods are the generalized Intensity-Hue-Saturation method, the principle component substitution method, and Gram-Schmidt method from the component substitution category, Brovey transform method, University of New Brunswick method, and smoothing filter based intensity modulation method from the modulation-based category, and basic high-pass filtering method, substitutive wavelet method, and additive wavelet luminance proportional method from the filtering-based category. The pan-sharpened images are quantitatively evaluated for their spatial and spectral quality using a set of well-established measures in the field of remote sensing. The evaluation metrics are ERGAS, Q4, and SAM which measure the spectral quality and a Laplacian-based metric that measures the spatial quality. Results show that images pan-sharpened using additive wavelet luminance proportional method are the best in terms of spatial and spectral quality.	high performance fortran;horner's method;modulation;principal component analysis;schmidt decomposition;sensor;smoothing;substitution method;utility functions on indivisible goods;wavelet	Essa Basaeed;Harish Bhaskar;Mohammed E. Al-Mualla	2013	Proceedings of the 16th International Conference on Information Fusion		computer vision;mathematics;statistics;remote sensing	Robotics	65.88981397193324	-66.10995554840595	179730
3dd0616db7922e92a15159d37725522e164c0b7a	anchor points coding for depth map compression	image coding encoding context vectors image segmentation image edge detection image resolution;sparse matrices data compression decoding image coding image segmentation image sequences;lossless and lossy compression;anchor points;contour compression;sparse binary matrix anchor points coding depth map compression contour encoding contour segment sequence chain code symbol string anchor points selection contour segments generation contour crossing point analysis chain code contour sequences skewed symbol distribution decoder context tree coding;depth map lossless and lossy compression contour compression anchor points;depth map	The paper deals with encoding the contours of given regions in an image. All contours are represented as a sequence of contour segments, each such segment being defined by an anchor (starting) point and a string of contour edges, equivalent to a string of chain-code symbols. We propose efficient ways for anchor points selection and contour segments generation by analyzing contour crossing points and imposing rules that help in minimizing the number of anchor points and in obtaining chain-code contour sequences with skewed symbol distribution. When possible, part of the anchor points are efficiently encoded relative to the currently available contour segments at the decoder. The remaining anchor points are represented as ones in a sparse binary matrix. Context tree coding is used for all entities to be encoded. The results for depth map compression are similar (in lossless case) or better (in lossy case) than the existing results.	chain code;contour line;depth map;entity;lossless compression;lossy compression;sparse matrix	Ionut Schiopu;Ioan Tabus	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7026138	data compression;computer vision;pattern recognition;mathematics;depth map	Robotics	54.18058983132369	-58.99231147672548	179770
ef1e941c99000d0459defedf7b653ec5b2e9231e	an empirical study on estimating motions in video stabilization	kanade lucas tomasi;empirical study;video signal processing;video quality;motion estimation;motion estimation cameras optical sensors optical filters optical receivers frequency low pass filters testing motion measurement parameter estimation;periodic correction strategy motion estimation video stabilization video quality kanade lucas tomasi algorithm average pixel deviation apd error accumulation;video signal processing motion estimation	"""The objective of video stabilization is to remove undesirable motion effects so that only intentional motion effects are retained. The primary benefit of video stabilization is to improve video quality. We present an empirical study that addresses some important practical issues on estimating motions in the development of video stabilization applications. Specifically, we use synthetic test data to investigate the performance of the Kanade-Lucas-Tomasi algorithm. Our contributions include the following. First, We propose a measure, """"average pixel deviation"""" (APD), to directly assess the accuracy of estimated motion parameters in comparison to true motion parameters, which is capable of overcoming some shortcomings of measures used in previous performance studies. Second, a practical issue of error accumulation often arises during the estimation of motion between frames, which has not been addressed in the previous studies to the best of our knowledge. We propose a novel periodic correction strategy, which is capable of effectively reducing error accumulation."""	algorithm;auditory processing disorder;computation;motion estimation;pixel;reference frame (video);synthetic data;test data;tomasi–kanade factorization;tree accumulation;video	Qiming Luo;Taghi M. Khoshgoftaar	2007	2007 IEEE International Conference on Information Reuse and Integration	10.1109/IRI.2007.4296647	computer vision;simulation;quarter-pixel motion;computer science;video quality;machine learning;video tracking;motion estimation;block-matching algorithm;rate–distortion optimization;empirical research;motion compensation	Vision	63.98047790784187	-62.517191948169234	179921
efb985ca880c1d71d1f19718f3eefde342de6577	led projector with two liquid crystal on silicon light valves and a fly's eye integrator	proyeccion;trazado rayos;42 79 ag;management system;state of polarization;cristal liquide sur silicium;concepcion sistema;display equipment;liquid crystal devices;42 79 bb;wire grid polarizers;light emitting diodes;polarizador;valvula optica;projection displays;trace rayon;light emitting diode;lente ojo mosca;42 15 dp;dispositif cristaux liquides;polariseur;optical valve;optoelectronic device;light source;fly s eye integrator;technology and engineering;system design;lentille oeil mouche;polarizer;integrator;projection;displays;lighting fitting;integrador;ray tracing;fly eye lens;85 60 pb;source lumineuse;color sequential;equipement affichage;fuente luminosa;diodo electroluminescente;appareil eclairage;4272;42 15 eq;dispositif optoelectronique;diode electroluminescente;aparato alumbrado;valve optique;equipo visualizacion;85 60 pg;conception systeme;liquid crystal on silicon;dispositivo optoelectronico;42 79 kr;integrateur;wire grid polarizer	0141-9382/$ see front matter 2008 Elsevier B.V. A doi:10.1016/j.displa.2008.03.003 * Corresponding author. Tel.: +32 2 6293658; fax: + E-mail addresses: bvangiel@tona.vub.ac.be, bart Giel). We present a projection system with two liquid crystal on silicon light valves that are illuminated by light-emitting diode light sources (LEDs). The LED illumination system is designed with a fly’s eye integrator. Using this integrator we combine for each primary color two LEDs into one bright source to illuminate the light valve. In the color management system a wire-grid polarizer is used to combine two beams with opposite state of polarization. The contrast behavior of our two LCOS approach is investigated with a basic model and ray tracing. The consequences of using this polarizer are discussed. We present contrast, uniformity and light output measurements of a concept demonstration set-up. 2008 Elsevier B.V. All rights reserved.	bartpe;ccir system a;circuit complexity;color management;diode;fax;illumination (image);light valve;liquid crystal on silicon;polarization (waves);polarizer;ray tracing (graphics);video projector	Bart Van Giel;Youri Meuret;Lawrence Bogaert;Hüseyin Murat;Herbert De Smet;Hugo Thienpont	2008	Displays	10.1016/j.displa.2008.03.003	electronic engineering;light extraction in leds;engineering;optoelectronics;optics;physics;light-emitting diode	AI	64.00298125308532	-59.07225157602896	180042
fcec3b0db5fc326560f08e4b1fd909938e27dace	visual undersampling in raster sampled images	spectral noise picture processing visual undersampling edge gratings sinusoidal gratings raster sampled images nyquist limit observers perceptual tolerance illusory visual effects;picture processing;visualization image reconstruction gratings image edge detection noise visual systems observers;pattern recognition;visual perception;visual perception pattern recognition picture processing	When a periodic signal is sampled at a rate below the Nyquist limit, it is considered undersampled because it is impossible to recover the original signal from the samples without some additional signal information. The authors have studied the visual effects of raster sampling upon the discriminability of simple monochrome test targets (edges and sinusoidal gratings) and found out that observers have good perceptual tolerance to the sinusoidal test gratings the low sampling rates produce illusory visual effects that disturb the recognition of the waveform. The occurrence of these illusions in sampled images points out two different sources of sampling noise in images: genuine spectral noise and purely visual spatial noise. The findings suggest that scenes containing abundant edge information tolerate low sampling rates better than scenes in which low and medium spatial frequencies are dominant.	monochrome;nyquist frequency;sampling (signal processing);signal-to-noise ratio;undersampling;visual effects;waveform	Göte Nyman;Pentii Laurinen	1985	IEEE Transactions on Systems, Man, and Cybernetics	10.1109/TSMC.1985.6313444	computer vision;visual perception;computer science;mathematics	Visualization	60.49999919383948	-59.41970139633139	180358
b39a394dcfd262b0b5af84bceb09dfb014e4f5e2	effect of black background on color appearance of ncs samples.				Celeste M. Howard	1997			computer science;computer vision;artificial intelligence	Vision	63.73905680681369	-55.974137141723915	180453
0f5a365c04585d75835235c6e32a1e02b9e96a9b	two-step super-resolution technique using bounded total variation and bisquare m-estimator under local illumination changes	strontium lighting image resolution image reconstruction indexes robustness;high resolution;image resolution;bisquare m estimation;bounded total variation;strontium;image resolution image reconstruction image registration;local illumination changes;indexes;image reconstruction;image registration;indexation;super resolution;robustness;total variation;lighting;bounded total variation super resolution bisquare m estimation local illumination changes;high resolution frame reconstruction two step superresolution technique bisquare m estimator local illumination changes image registration sr image reconstruction bounded total variation based approach	In this paper, we present a super-resolution (SR) technique for images having arbitrarily-shaped local illumination changes. These variations tend to degrade the performance of the image registration, and hence impact the SR image reconstruction. Conventional SR techniques focus on enhancing the reconstruction step assuming aligned images. In this paper, we exploit our recent registration approach of images having illumination variations using a robust bisquare M-estimation. Then, we extend a bounded total variation-based approach for upsampling single frames to super-resolving multi-frames in order to reconstruct the unknown high-resolution (HR) frame. The proposed SR technique shows clear improvements over competing techniques in terms of objective metrics using simulated and real image pairs with illumination variations.	bounded variation;half rate;high-resolution scheme;image registration;image resolution;iterative reconstruction;list of common shading algorithms;super-resolution imaging;upsampling	Mohamed M. Fouad;Richard M. Dansereau;Anthony D. Whitehead	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6115689	computer vision;image resolution;computer science;computer graphics (images)	Vision	54.82511983725183	-56.35286186477439	180533
6ca3127c0986196d671cfd9a93bb5265078fa0d0	computer retina that models the primate retina	silicon;visual acuity;sensors;data processing;nonuniform sampling;retina;machine vision;information processing;dynamic range;computing systems;data reduction;visual system	"""At the retinal level, the strategies utilized by biological visual systems allow them to outperform machine vision systems, serving to motivate the design of electronic or """"smart"""" sensors based on similar principles. Design of such sensors in silicon first requires a model of retinal information processing which captures the essential features exhibited by biological retinas. In this paper, a simple retinal model is presented, which qualitatively accounts for the achromatic information processing in the primate cone system. The model exhibits many of the properties found in biological retinas such as data reduction through nonuniform sampling, adaptation to a large dynamic range of illumination levels, variation of visual acuity with illumination level, and enhancement of spatiotemporal contrast information. The model is validated by replicating experiments commonly performed by electrophysiologists on biological retinas and comparing the response of the computer retina to data from experiments in monkeys. In addition, the response of the model to synthetic images is shown. The experiments demonstrate that the model behaves in a manner qualitatively similar to biological retinas and thus may serve as a basis for the development of an """"artificial retina""""."""	dynamic range;experiment;illumination (image);information processing;machine vision;nonuniform sampling;sampling (signal processing);sensor;synthetic intelligence	Samir Shah;Martin D. Levine	1994		10.1117/12.179279	computer vision;nonuniform sampling;dynamic range;data reduction;visual system;data processing;machine vision;information processing;computer science;sensor;optics;silicon;computer graphics (images)	ML	63.01273137207919	-59.59522966292355	180580
5a36bb89717c6205207229562b2d175ce81ee34b	a median cut algorithm for efficient sampling of radiosity functions	higher order radiosity;substructuring;median;sampling;b splines	Abstract#R##N##R##N#This paper presents an efficient method for sampling the illumination functions in higher order radiosity algorithms. In such algorithms, the illumination function is not assumed to be constant across each patch, but it is approximated by a function which is at least C1 continuous. Our median cut sampling algorithm is inspired by the observation that many form factors are computed at higher precision than is necessary. While a high sampling rate is necessary in regions of high illumination, dark areas can be sampled at a much lower rate to compute the received radiosity within a given precision. We adaptively subdivide the emitter into regions of approximately equal influence on the result. Form factors are evaluated by the disk approximation and a ray tracing based test for occlusion detection. The implementation of a higher order radiosity system using B-splines as radiosity function is described. The median cut algorithm can also be used for radiosity algorithms based on the constant radiosity assumption.	algorithm;median cut;radiosity (computer graphics)	Martin Feda;Werner Purgathofer	1994	Comput. Graph. Forum	10.1111/1467-8659.1330433	b-spline;sampling;mathematical optimization;radiosity;theoretical computer science;mathematics;geometry;median;computer graphics (images)	Graphics	64.9275257088444	-53.18945419203944	180597
f2cd0814bd446d0fe30e69dab382cc5a262f15b6	moment-preserving curve detection	picture processing;equations detectors image segmentation feature extraction image processing image coding object recognition technical drawing robustness matched filters;letter;parabolic equation picture processing image processing curve detection moment preserving principle curve locations 4 5 unit circle	A method is described for deriving, from digitiied imager. objective measures that correlate strongly with simple perceptual judgements on the same images. Each measure is the normalized variance of an image obtained by convolving the original image with a specific local operator. This operator is designed to optimize the correlation between the particular percept and the objective measure, subject to certain consbaint~. I. TEXTURAL FEATURES The analysis and characterization of visual textures is an important requirement for both human and machine vision. Mathematical approaches have been developed for the purposes of feature extraction, pattern recognition, and scene segmentation, as summarized in the surveys of Haralick [l] and Van Goo1 et al. [2]. Machine discrimination of textures is an area of considerable interest, requiring measurement of specific features rather than a knowledge of the underlying structure and synthesis of the texture. It would be of value if measures on digital images of textures could be found that correlated with human performance in discriminating these textures. In a significant experiment, Tamura el a/ . [3] attempted to relate objective measures of digitized textures to psychophysical judgements of the same textures on the basis of defined perceptual criteria. They implemented different computational procedures for each of six scales, and obtained rank correlations in the range 0.65 to 0.90 between the psychological and objective measures. In this paper, we investigate further the problem of finding measures on the digitized image that correlate highly with human perception of texture. The results from our experiments suggest that the optimization of a local operator offers the prospect of a general technique for the determination of objective measures. There is an important difference between our approach and that of Tamura et a / . They aimed at measuring the correspondence between computational definitions of textural features and psychophysical assessments. We have decided to use a general operator and adjust its parameters so as to optimize the correlation between objective measures and psychophysical assessments. The psychological assessment of the textures used by Tamura et al. required a judgement based on a verbally defined textural feature. We investigated three perceptual scales similar to those used in [3]. Subjects were asked to separately allocate images of textures along each of three psychological scales. These were line-like versus bloblike nondirectional versus directional (where monodirectionality was ranked above bidirectionality), and random versus regular. 11. OBJECTIVE MEASURES OF TEXTURAL FEATURES In developing a mathematical approach to the problem of texture characterization, we will describe the texture in statistical rather than structural terms [l], [2]. The texture field is analysed by computing the statistics of the local properties after filtering with a convolution mask [4], [5]. In the past, these operators have inspired models for edge detection because of their similarities to the feature extractors believed to exist in the human visual system, as described, for example, by Marr 161. Manuscript received Feb. 21. 1987: revised Nov. 3, 19x7 The authors are with the Australian Department of Defence. Material, IEEE Log Number 8718873. Research Laboratories, P.O. Box 50. Ascot Vale. Victoria 3032. Aii,tralla 0018-9472/88/0100-0158$01.00 01988 IEEE Authorized licensed use limited to: IEEE Xplore. Downloaded on December 25, 2008 at 17:58 from IEEE Xplore. Restrictions apply.	bi-directional text;computation;convolution;digital image;edge detection;experiment;feature extraction;human reliability;image sensor;machine vision;mathematical optimization;pattern recognition;robert haralick;texture mapping;the australian;victoria (3d figure)	Ling-Hwei Chen;Wen-Hsiang Tsai	1988	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.87063	computer vision;mathematical optimization;feature detection;letter;image processing;mathematics;geometry	Vision	61.18235978030365	-64.68280939815028	180944
12c5ff918cfd7b343e671934bf0fbbe0e35dacef	analysis of ssim based quality assessment across color channels of images	color space;quality assessment;ssim;cie lab image;tid2013	Advances in imaging and computing hardware have led to an explosion in the use of color images in image processing, graphics and computer vision applications across various domains such as medical imaging, satellite imagery, document analysis and biometrics to name a few. However, these images are subjected to a wide variety of distortions during its acquisition, subsequent compression, transmission, processing and then reproduction, which degrade their visual quality. Hence objective quality assessment of color images has emerged as one of the essential operations in image processing. During the last two decades, efforts have been put to design such an image quality metric which can be calculated simply but can accurately reflect subjective quality of human perception. In this paper, the authors evaluated the quality assessment of color images using SSIM (structural similarity index) metric across various color spaces. They experimented to study the effect of color spaces in metric based and distance based quality assessment. The authors proposed a metric using CIE Lab color space and SSIM, which has better correlation to the subjective assessment in a benchmark dataset. Analysis of SSIM based Quality Assessment across Color Channels of Images	benchmark (computing);biometrics;channel (digital image);color space;computability in europe;computer hardware;computer vision;distortion;graphics;image processing;image quality;medical imaging;structural similarity	T. Chandrakanth;B. Sandhya	2015	IJSDA	10.4018/IJSDA.2015070102	computer vision;color quantization;geography;multimedia;computer graphics (images)	Graphics	62.07574935386149	-63.96011096237018	181072
135e9f5191c843c3031b322a7c4a7392183845f4	video retargeting combining warping and summarizing optimization	bidirectional similarity;video retargeting;video summarization;texture synthesis;video completion	We construct a unified interactive video retargeting system for video summarization, completion, and reshuffling. Our system combines the advantages of both video warping and summarizing processing. We first warp the video to present initial editing results, then refine the results using patch-based summarizing optimization, which mainly eliminates possible distortion produced in the warping step. We develop a Mean Value Coordinate (MVC) warping method due to its simplicity and efficiency used in the initialization. For refining processing, the summarization optimization is built on a 3D bidirectional similarity measure between the original and edited video, to preserve the coherence and completeness of the final editing result. We further improve the quality of summarization by applying a color histogram matching during the optimization, and accelerate the summarization optimization by using a constrained 3D Patch-Match algorithm. Experiment results show that the proposed video retargeting system effectively supports video summarization, completion, and reshuffling while avoiding issues like texture broken, video jittering, and detail losing.	asp.net mvc;algorithm;coherence (physics);color histogram;distortion;histogram matching;image scaling;mathematical optimization;retargeting;similarity measure;video synopsis	Yongwei Nie;Qing Zhang;Renfang Wang;Chunxia Xiao	2013	The Visual Computer	10.1007/s00371-013-0830-4	computer vision;computer science;automatic summarization;video tracking;multimedia;texture synthesis;multiview video coding;computer graphics (images)	Graphics	55.63950462753356	-58.716972661237556	181110
8172dfd4e67226b43de874cfb3a580108f60ac30	novel view synthesis using a translating camera	pan detection;camera translation;view synthesis;image based rendering;article;novel view synthesis	This paper addresses the problem of synthesizing novel views of a scene using images taken by an uncalibrated translating camera. We propose a method for synthesis of views corresponding to translational motion of the camera. Our scheme can handle occlusions and changes in visibility in the synthesized views. We give a characterisation of the viewpoints corresponding to which views can be synthesized. Experimental results have established the validity and effectiveness of the method. Our synthesis scheme can also be used to detect translational pan motion of the camera in a given video sequence. We have also presented experimental results to illustrate this feature of our scheme.	view synthesis;z-buffering	Geetika Sharma;Ankita Kumar;Shakti Kamal;Santanu Chaudhury;J. B. Srivastava	2005	Pattern Recognition Letters	10.1016/j.patrec.2004.08.011	computer vision;camera auto-calibration;image-based modeling and rendering;computer science;multimedia;computer graphics (images)	Vision	57.32320620881651	-53.79623757007051	181154
b3155174ab9d88e9337c1e6b1e2a6c367b7e0111	nonlinear resampling for both moire suppression and edge preservation	printing;reechantillonnage;moire patterns;halftones;image processing;frequency analysis;low frequency;laser engraving;degree of freedom;transformacion fourier discreta;discrete fourier transformation;procesamiento imagen;low pass filter;traitement image;moire suppression;filtro paso bajo;analyse frequentielle;transformation fourier discrete;criterio nyquist;filtre passe bas;critere nyquist;technology and engineering;linear interpolation;impression;interpolation lineaire;suppression moire;anti alaising;impresion;nyquist criterion;interpolacion lineal	Moire formation is often a major problem in the printing applications. These artifacts introduce new low frequency components which are very disturbing. Some printing techniques, e.g. gravure printing, are very sensitive to moire. The halftoning scheme used for gravure printing can basically be seen as a two-dimensional non-isotropic subsampling process. The moire problem is much more important in gravure printing than in conventional digital halftoning since the degree of freedom in constructing halftone dots is much more limited due to the physical constraints of the engraving mechanism. Conventional moire detection schemes use ad hoc decision rules to distinguish between high-frequency patterns, low-frequency patterns and edges. We present a new technique to locally estimate the risk of moire after resampling. The appearance of moire is estimated by frequency analysis: a 16 x 16 window is considered around every pixel in the original image and is transformed using the discrete Fourier transform (DFT). Since moire is a form of aliasing, the two-dimensional Nyquist-criterion of the screen lattice is used to devise a risk of moire' for each frequency component. The energy at different frequencies is weighted by these risks. The total weighted energy in proportion to the unweighted energy estimates the pixel's risk of moire. Regions in the image with high risks need to be sufficiently lowpass filtered, while others should remain as sharp as possible. The estimation of the risk of moire can be used to linearly blend the outputs of two different linear resampling filters. The obtained resampling scheme is not linear anymore. Preliminary simulation results of the gravure printing process on a 'zoneplate' image and on a representative test image show that the appearance of the moire artifacts in the subsampled image is avoided while edges are preserved as well as possible.	resampling (statistics);zero suppression	Dimitri Van De Ville;Koen N. Denecker;Wilfried Philips;Ignace Lemahieu	1999		10.1117/12.334586	computer vision;electronic engineering;mathematics;engineering drawing	Theory	59.6548746677613	-60.325637920629475	181267
353e9f5cabf9eb9324feeeaa451fa5bc934efc0c	"""efficient spatiotemporal grouping using the nystro""""m method"""	eigenvalues and eigenfunctions;video sequence;spectral graph theoretic methods;image segmentation;video signal processing image segmentation eigenvalues and eigenfunctions image sequences;numerical solution;video signal processing;spatiotemporal phenomena image sequences image segmentation eigenvalues and eigenfunctions computer applications video sequences extrapolation pixel educational institutions psychology;extrapolation;video sequences;psychology;efficient spatiotemporal grouping;eigenfunction problems;computer applications;coherent groups;pixel;image sequence;nystrom method;spatiotemporal phenomena;spectral partitioning;pairwise voxel similarities;coherent groups efficient spatiotemporal grouping nystrom method spectral graph theoretic methods image segmentation video sequence pairwise voxel similarities spectral partitioning numerical solution eigenfunction problems extrapolation;image sequences	Spectral graph theoretic methods have recently shown great promise for the problem of image segmentation, but due to the computational demands, applications of such methods to spatiotemporal data have been slow to appear. For even a short video sequence, the set of all pairwise voxel similarities is a huge quantity of data: one second of a 256 384 sequence captured at 30Hz entails on the order of 1013 pairwise similarities. The contribution of this paper is a method that substantially reduces the computational re quirements of grouping algorithms based on spectral partitioning, making it feasible to apply them to very large spatiotemporal grouping problems. Our approach is based on a technique for the numerical solution of eigenfunction prob lems known as the Nystr öm method. This method allows extrapolation of the complete grouping solution using only a small number of “typical” samples. In doing so, we successfully exploit the fact that there are far fewer coherent groups in an image sequence than pixels.	approximation algorithm;coherence (physics);color;computation;display resolution;extrapolation;finite element method;image segmentation;numerical partial differential equations;nyström method;optical flow;pixel;spatiotemporal database;theory;uc browser;video clip;voxel;xslt/muenchian grouping	Charless C. Fowlkes;Serge J. Belongie;Jitendra Malik	2001		10.1109/CVPR.2001.990481	computer vision;mathematical optimization;nyström method;computer science;theoretical computer science;mathematics;image segmentation;extrapolation;computer applications;pixel	Vision	67.60390771619461	-57.20526534105119	181274
a54a66b52c240b6c0cf299dc4fac32fe6d91b714	parallelisms in mpeg and its applications to 3-d visualization	desciframiento;parallelisme;modelo 3 dimensiones;visualizacion;decodage;decoding;modele 3 dimensions;real time;three dimensional shape;three dimensional model;satisfiability;forma tridimensional;visualization;parallelism;paralelismo;forme tridimensionnelle;moving picture expert group;visualisation;image realiste;temps reel;tiempo real;realistic images;mpeg;capability;capabilite	Techniques for visualization of 3-D objects must present to the user photo-realistic images in addition to offering various 3-D manipulations including rotation and translation as well as panning and zooming capabilities. The image-based visualization can satisfy these requirements at low-cost by capturing images of the 3-D object at all view angles. This technique naturally offers photo-realistic 3-D visualization, since the viewing software-while the user manipulates the 3-D object in real-time-simply displays the corresponding frame. However, due to the large number of captured images, typically 300 to 600 cuts, the data must be efficiently compressed; and at the same time the compressed data must allow real-time decoding to give the effect of 3-D manipulation to the viewer. As neighboring frames (in close proximity in terms of the view angle) will almost always be similar, we have opted to use MPEG to compress the captured data. The MPEG standard offers a multitude parallelism that can be exploited and in this paper, we discuss various parallelisms in MPEG applicable to our image-based 3-D visualization for achieving photo-realistic visualization and manipulation of 3-D objects.	moving picture experts group;parallel computing	Samuel Moon-Ho Song;Gunho Lee;Sunghyun Kim;Manhee Lee;Hyeokman Kim;Dong-Sik Jang	2002		10.1007/3-540-48051-X_28	computer vision;visualization;computer science;operating system;multimedia;computer graphics (images)	Visualization	61.01168930768401	-54.33382475182144	181317
67ada6be12491e6ac6bc4f1b5d22811582403d02	a shared-scene-graph image-warping architecture for vr: low latency versus image quality	nvidia geforce gtx 260;nvidia geforce 8800 gtx;paper;nvidia quadro fx 5600;virtual reality;rendering system;client server;image generation;low latency;object manipulation;image quality;nvidia;opengl;latency;computer science;image based rendering;system architecture;3d graphics and realism;image warping	Designing low end-to-end latency system architectures for virtual reality is still an open and challenging problem. We describe the design, implementation and evaluation of a client-server depth-image warping architecture that updates and displays the scene graph at the refresh rate of the display. Our approach works for scenes consisting of dynamic and interactive objects. The end-to-end latency is minimized as well as smooth object motion generated. However, this comes at the expense of image quality inherent to warping techniques. We evaluate the architecture and its design trade-offs by comparing latency and image quality to a conventional rendering system. Our experience with the system confirms that the approach facilitates common interaction tasks such as navigation and object manipulation.	client–server model;display resolution;end-to-end principle;graphics processing unit;image quality;image warping;multiprocessing;real-time clock;refresh rate;rendering (computer graphics);scene graph;server (computing);shared memory;stereoscopy;virtual reality	Ferdi A. Smit;Robert van Liere;Stephan Beck;Bernd Fröhlich	2010	Computers & Graphics	10.1016/j.cag.2009.10.006	image quality;image warping;computer vision;latency;simulation;image-based modeling and rendering;computer science;operating system;virtual reality;client–server model;computer graphics (images);low latency	Graphics	67.20785123418432	-52.569675726261764	181372
0b4d3dec9d63aedab76124b78c8cc39e14bb9601	visually favorable tone-mapping with high compression performance in bit-depth scalable video coding	tone mapping;perceptual quality;optimisation;image coding;image segmentation;image enhancement tone mapping operator perceptual quality image compression optimization statistical models;bit depth scalable compression;statistical models;statistical model;coding gain;optimization problem;video coding;image enhancement;enhancement layer;image coding pixel image segmentation conferences optimization estimation;statistical analysis;estimation;image compression;pixel;bit depth scalable compression high dynamic range hdr tone mapping;visual perception;optimization;high dynamic range;high dynamic range hdr;tone mapping operator;conferences;base layer;visual perception image enhancement optimisation statistical analysis video coding	In bit-depth scalable video coding, the tone-mapping scheme used to convert high-bit-depth to eight-bit videos is an essential yet very often ignored component. In this paper, we demonstrate that an appropriate choice of a tone-mapping operator can improve the coding efficiency of bit-depth scalable encoders. We present a new tone-mapping scheme that delivers superior compression efficiency while adhering to a predefined base layer perceptual quality. We develop numerical models that estimate the base layer bit-rate (Rb), the enhancement layer bitrate (Re), and the mismatch (QL) between the resulting low dynamic range (LDR) base-layer signal and the predefined base layer representation. Our proposed tone curve is given by the solution of an optimization problem which minimizes a weighted sum of Rb, Re, and QL. The problem formulation also considers the temporal effect of tone-mapping by adding a constraint to the optimization problem that suppresses flickering artifacts. We also propose a technique with which to tone-map a high-bit-depth video directly in a compression-friendly color space (e.g., one luma and two chroma channels) without converting to the RGB domain. Experimental results show that we can save up to 40% of the total bit-rate (or 3.5 dB PSNR improvement for the same bitrate), and, in general, about 20% bit-rate savings can be achieved.	8-bit;algorithmic efficiency;color space;computer simulation;data compression;dynamic range;encoder;high-dynamic-range rendering;ldraw;mathematical optimization;numerical analysis;optimization problem;peak signal-to-noise ratio;scalability;scalable video coding;tone mapping;weight function;xojo	Zicong Mai;Hassan Mansour;Panos Nasiopoulos;Rabab Kreidieh Ward	2010	IEEE Transactions on Multimedia	10.1109/ICIP.2010.5653283	statistical model;computer vision;computer science;mathematics;multimedia;statistics;computer graphics (images)	Vision	60.54570162276773	-62.06890656005218	181404
bf1152b9ed691920116c02ea0fb3b23c443ad933	block unshifting high-accuracy motion estimation: a new method adapted to super-resolution enhancement		Abstract Sub-pixel motion estimation plays a vital role in a multitude of video applications, including encoding, audiovisual archiving/heritage and super-resolution enhancement. Most existing block-based methods rely on the implicit assumption that blocks can be accurately predicted through appropriate shifts. In particular, shifted blocks in the target frame are estimated from the associated anchor frame blocks. The present paper introduces a different strategy, which discards this assumption and treats anchor and target frame blocks equally, as sub-pixel shifted versions of an unavailable implied block. The new method attempts to construct this implied block and, by calculating the “imaginary” motion vectors that relate it to the two existing blocks, it estimates the wanted motion vectors more accurately. This approach aims at extracting motion vectors that more accurately represent the actual movements of objects, minimizing the interpolation error that is associated with sub-pixel shifting, which manifests as blurring and a lowering of contrast. The new method focuses on accurate motion estimation, paying less attention to the associated computational load. Hence, the approach is both inspired from, and proposed for, super-resolution enhancement scenarios, where higher definition motion image sequences are estimated from their available lower definition counterparts. In order to implement the new strategy, an algorithm for reversing the bilinear sub-pixel shift of a block (unshifting) is implemented and validated. Comparisons between original blocks of images and blocks that have been shifted and unshifted back to their original coordinates showcase the accuracy of the unshifting process. The proposed motion estimation method is evaluated through a number of different experimental assessment procedures and metrics, comparing it to existing high-accuracy state-of-the-art motion estimation methods.	motion estimation;super-resolution imaging	Konstantinos Konstantoudakis;Lazaros Vrysis;Nikolaos Tsipas;Charalampos Dimoulas	2018	Sig. Proc.: Image Comm.	10.1016/j.image.2018.03.016	computer vision;interpolation;motion estimation;artificial intelligence;superresolution;computer science;bilinear interpolation;reversing	Vision	58.489270287898755	-56.66959977732618	181559
db3353041f0ef8fdf1ef3c7f16292f1aca7ee340	content-aware seamless stereoscopic 3d compositing	stereoscopic 3d compositing;computational photography	This paper addresses the challenges in creating good quality composite 3D contents for 3DTV applications and post-production visual-effects. We present a novel content-aware compositing technique that faithfully preserves the salient structures of cloned source and target content, and avoid major conflicting stereopsis cues to maintain a pleasant 3D illusion altogether. Our approach typically learns the appearance layouts of both source and target scene elements. The system extracts object's significance prior maps using classified labels and derive geometric transforms to compensate the 3D perspective mismatches between source and target images using a novel depth image-based rendering procedure. For seamless cloning, we apply a new depth-consistent interpolant technique which utilizes the classified likelihood confidences in weighting the salient or low-significant regions and re-estimating the plausible depth values of the cloned region in accordance with target 3D structure. Further, we adopt a novel content-preserving local warping scheme to reduce the apparent distortions in object shape, size and perspective. Finally, we propose a content-aware mean value cloning technique that seamlessly merges the warped cloned patches with the geometric-appearance context of new background and homogenize vague boundaries with the aid of an object salient map to remove the smudging effects. The overall process is formulated as an energy minimization problem and optimally regularized for large warps, vertical disparities, and stereo baseline changes. Plausible results are demonstrated to show the effectiveness of our approach.	3d television;alpha compositing;baseline (configuration management);compositing;distortion;energy minimization;experiment;glossary of computer graphics;image warping;interpolation;map;no-cloning theorem;object-based language;seamless3d;stereopsis;stereoscopy;vagueness;warp (information security)	Mansi Sharma;Santanu Chaudhury;Brejesh Lall	2014		10.1145/2683483.2683555	computer vision;simulation;geography;computer graphics (images)	Vision	58.05431793425847	-55.9506805638425	181593
0ae887ed5f59f20e3f1d9f54e6717ce741b8e286	on evaluation the quality of subjective s3d comfort assessment		Stereoscopic 3D (S3D) image technology has been extensively developed in the last decades. Visual discomfort such as eye strain, headache, fatigue, asthenopia, and other phenomena leading to a less pleasant viewing experience is still a potential issue in S3D applications. How to evaluate S3D image quality that related to visual discomfort is still a challenging problem. A larger number of studies have been done on S3D Image Quality Assessment (S3D IQA) where the subjective assessed S3D Image Databases play an important role. The subjective scores were collected for each S3D image in database with a number of viewers. Usually, Likert scale is adopted for observers to mark their subjective quality score, and then mean opinion score (MOS) is estimated. Due to the law of comparative judgment, the quality of subjective scores varies among observers and depends on the judgment method. This paper studied the quality of two subjective assessment methodologies — single stimulus (SS) and pairwise comparison (PC). Considering the S3D IQA as a S3D images' quality ranking problem, we applied single stimulus and pairwise comparison subjective testing on a set of S3D images with known geometric distortions. From SS subjective testing results, the S3D images' ranking can be derived by sorting MOSs directly. From PC subjective testing results, the ranking can be derived from DMOS scores. The distorted S3D images can be ranked via their geometric distortion parameters. The quality of subjective assessed results from SS and PC are then evaluated on the correlation between their ranking results to corresponding geometric distortions. With the collected MOSs for geometric distorted S3D image database, a deep-learning based S3D IQA model was used to study the relationship between the model performance and the quality of subjective assessment.	comparison sort;database;deep learning;distortion;image quality;map overlay and statistical system;simulation;sorting;stereoscopic video game;stereoscopy	Jun Zhou;Xiao Gu;Ya Zhang	2017	2017 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)	10.1109/BMSB.2017.7986241	visual discomfort;mean opinion score;law of comparative judgment;computer science;image quality;subjective video quality;computer vision;pairwise comparison;quality score;artificial intelligence;ranking	Vision	62.149532592386585	-63.627065975389925	181719
33a8f86f1f51a9ca92471a7695e4c98d3c54aad7	robust energy minimization for brdf-invariant shape from light fields		Highly effective optimization frameworks have been developed for traditional multiview stereo relying on lambertian photoconsistency. However, they do not account for complex material properties. On the other hand, recent works have explored PDE invariants for shape recovery with complex BRDFs, but they have not been incorporated into robust numerical optimization frameworks. We present a variational energy minimization framework for robust recovery of shape in multiview stereo with complex, unknown BRDFs. While our formulation is general, we demonstrate its efficacy on shape recovery using a single light field image, where the microlens array may be considered as a realization of a purely translational multiview stereo setup. Our formulation automatically balances contributions from texture gradients, traditional Lambertian photoconsistency, an appropriate BRDF-invariant PDE and a smoothness prior. Unlike prior works, our energy function inherently handles spatially-varying BRDFs and albedos. Extensive experiments with synthetic and real data show that our optimization framework consistently achieves errors lower than Lambertian baselines and further, is more robust than prior BRDF-invariant reconstruction methods.	bidirectional reflectance distribution function;energy minimization;experiment;gradient;ibm notes;invariant (computer science);lambertian reflectance;light field;mathematical optimization;powell's method;regular language description for xml;synthetic intelligence;uc browser;variational principle;visual computing	Zhengqin Li;Zexiang Xu;Ravi Ramamoorthi;Manmohan Krishna Chandraker	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.69	bidirectional reflectance distribution function;artificial intelligence;robustness (computer science);computer vision;iterative reconstruction;microlens;smoothness;light field;energy minimization;mathematical optimization;invariant (mathematics);mathematics	Vision	54.48762143930967	-53.05961216783484	181744
98abd85964e93ff34ec1f632243eea743299fdac	an efficient approach for 2d to 3d video conversion based on structure from motion	depth warping;2d to 3d conversion;depth map;structure from motion	With the popularity of 3D films, the conversion of existing 2D videos to 3D videos has attracted a wide interest in 3D content production. In this paper, we present an efficient approach for 2D to 3D video conversion based on structure from motion (SFM). The key contributions include a piece-wise SFM approach and a novel nonlinear depth warping considering the characteristics of stereoscopic 3D. The dense depth maps are generated and further refined with color segmentation. Experiments show that the proposed approach can yield more visually satisfactory results.	2d to 3d conversion;3d film;algorithm;depth map;desktop computer;experiment;image segmentation;map;nonlinear system;sparse matrix;stereoscopy;structure from motion	Wei Liu;Yihong Wu;Fusheng Guo;Zhanyi Hu	2013	The Visual Computer	10.1007/s00371-013-0904-3	computer vision;structure from motion;computer science;2d to 3d conversion;multimedia;depth map;computer graphics (images)	Vision	57.60112602214653	-54.461671356192994	181870
ecee4123e6b557f1cc69aadc6c67e1594061a041	probabilistic approaches for the digital restoration of television archives. (approches probabilistes appliquées à la restauration numérique d'archives télévisées)		Within the context of archives restoration, we investigate in this thesis the concealment of impulsive defects (blotches, video dropouts). Existing detection and correction methods reach their limits with the presence of motion estimation failures due to complex natural events. We aim at taking into account these events that we shall call pathological motion. For both detection and correction steps, we investigate probabilistic approaches and our algorithms are expressed by means of parametric or non-parametric Markov random fields. The proposed detection method relies on the framework of the Bayesian theory of estimation. We consider a larger temporal window than the usual three frames, in order to better distinguish defects from pathological motion and thus dramatically reduce the number of false alarms. We also propose a method to correct missing data areas which is inspired by works on texture synthesis. After generalizing these techniques to natural images, we integrate them in a spatio-temporal context which allows an implicit fallback on a spatial correction when motion is too complex. We first validate the proposed methods separately before combining them to create a complete prototype for the concealment of impulsive defects. te l-0 06 57 63 6, v er si on 1 7 Ja n 20 12	algorithm;archive;circuit restoration;linear algebra;markov chain;markov random field;missing data;motion estimation;prototype;texture synthesis;window function	Raphaël Bornard	2002				Vision	54.824240183975746	-58.150995596524865	181935
8f4f5bd2f4db425b9bfa66da92e2cc001ad01e8b	epi distortion correction for mr-dti by using texture memory on graphics hardware	graphics hardware		distortion;graphics hardware;texture memory	Yoshitaka Masutani;Takeharu Yoshikawa;Shigeki Aoki;Kuni Ohtomo;Anthony J. Sherbondy;Sandy Napel	2003			artificial intelligence;texture mapping unit;software rendering;s3 texture compression;computer vision;3d computer graphics;2d computer graphics;graphics pipeline;texture memory;real-time computer graphics;medicine	Graphics	65.84708530162133	-52.464195278122226	181992
7b2684519a49ec1e07530f0769625207d7bc8b48	biased anisotropic diffusion - a unified regularization and diffusion approach to edge detection	edge detection;boundary value problem;anisotropic diffusion	We present a global edge detection algorithm based on variational regularization. The algorithm can also be viewed as an anisotropic diffusion method. We thereby unify these two fro m the original outlook quite different methods. The algorithm to be presented moreover has the following attractive properties: 1) It only requires the solution of a single boundary value problem over the entire image domain--almost always a very simple (rectangular) region. 2) It converges to a solution of interest. 1 I n t r o d u c t i o n Edge detection can in short be described as the process of finding the discontinuities of the partial derivatives up to some order of an image function defined on an open bounded connected image domain B C_ [i 2. The image function can represent various kinds of data collected from the visible surfaces in the scene. We will be concerned with brightness data. In this case the image function is real-valued, and the discontinuities of interest appear in the zeroth order derivative, that is the image function itself. If the true image function one would obtain by pure projection of the brightness in the scene onto the image domain were known, the edge detection problem would be easy. However, because of imperfections in the image formation process, the original image function one is given, is distorted so that the discontinuities in the true image function are disguised into large gradients. Edge detection therefore essentially boils down to numerical differentiation--a problem well-known to be ill-posed (in the sense of Hadamard) due to its instability with respect to the initial data. Since measurement noise and other undesirable disturbances cannot be avoided, the edge detection problem thus has to be stabilized in order to have a meaningful solution. In more practical terms this means that the undesirable disturbances must be suppressed without the disappearance or dislocation of any of the edges. Over the last six years or so many attempts along these lines have appeared in the literature. One can distinguish between two seemingly quite different approaches, viz. regularization and anisotropic diffusion. Regularization can be achieved in different ways. In probabilistic regularization the problem is reformulated as Bayesian estimation. In variational regularization it is posed	acronis true image;algorithm;anisotropic diffusion;calculus of variations;edge detection;gradient;image formation;instability;matrix regularization;microsoft outlook for mac;numerical analysis;numerical differentiation;variational principle;viz: the computer game;well-posed problem	Niklas Nordström	1990		10.1007/BFb0014846	computer vision;mathematical optimization;edge detection;boundary value problem;computer science;anisotropic diffusion	Vision	56.4252889202188	-52.84246448038195	181998
966fa901e476ecf51c3f1848b3b5da4e0ea27112	enhancing digital images through cuckoo search algorithm in combination with morphological operation	morphological operation;digital images enhancement;cuckoo search algorithm	Corresponding Author: Ratna Babu, K. Department of CME SUVR and SR Govt. Polytechnic for Women, Ethamukkala, Prakasam Dt., India Email: kratnababuphd@gmail.com Abstract: This study presents an image enhancement approach to Cuckoo Search Algorithmin with Morphological Operation. At the present time, in many image processing applications digital images are developed. Machine vision, computer interfaces, manufacturing, compression for storage and more are some of the fields of image processing application. Before using it in any applications the image has to be managed, such processing is said to be image enhancement. We propose a method to combine with an enhancing digital images through cuckoo search algorithmin and morphological operation. Therefore, the appearance of noise produces distortion in an image and thus the image will be unattractive. This decreases the discernibility of many features inside the images. In this study, we are working to overcome this drawback by getting an improved contrast value after converting the color image into grayscale image. The fundamental characteristic of this CS algorithm is that the amplitudes of its components can objectively reflect the contribution of the gray levels to the representation of image information for the best contrast value of an image. After selecting the best contrast value of an image in CS algorithm, morphological operations have to be done. In morphological operations, the intensity parameters of the image are adjusted to improve its quality. Experimental results demonstrate that the proposed approach is converted into original color image without noise and adaptive process to enhance the quality of images.	benchmark (computing);color image;cuckoo search;digital image;distortion;email;grayscale;image editing;image processing;matlab;machine vision;mathematical morphology;search algorithm;simulation	K. Ratna Babu;K. V. N. Sunitha	2015	JCS	10.3844/jcssp.2015.7.17	image quality;image texture;image restoration;computer vision;feature detection;binary image;image processing;artificial intelligence;morphological gradient;machine learning;digital image processing;top-hat transform	Vision	58.69325335859121	-63.60682289687363	182189
45a9f51d7f583aefb9ca65fcdc4798de59a8c66d	figure/ground video segmentation via low-rank sparse learning	minimization;image segmentation;motion segmentation;dictionaries;coherence;target tracking;sparse matrices	Due to its importance, figure/ground segmentation in video has gained interest recently. The key factor of the segmentation is the construction of the spatio-temporal coherence. Previous works usually use the motion approximation as a measurement of the coherence, resulting in a low accuracy. In this paper, we present a novel method to measure the coherence, and an algorithm for target segmentation and tracking is proposed. Each image is abstracted by some compact and perceptually homogeneous elements, and by representing the elements as sparse linear combinations of dictionary templates, this algorithm capitalizes on the inherent low-rank structure of representations that are learned jointly. The coefficients of the constrained representation will act as the measurement of the spatio-temporal coherence. At last, a simple energy minimization solution with an online parameter-updating scheme is adopted in segmented stage, leading to a binary object's segmentation. Meanwhile, an adaptive dictionary is proposed to enhance the system's robust against occlusion. Our approach outperforms the state-of-the-art methods in object segmentation accuracy.	algorithm;approximation;coefficient;coherence (physics);dictionary;energy minimization;sparse matrix	Song Gu;Jianguo Wang;Lili Pan;Shilei Cheng;Zheng Ma;Mei Xie	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532480	computer vision;coherence;sparse matrix;computer science;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation	Vision	55.37037092406655	-56.58099386237042	182190
ea748b56add94b6fda15cfa8741c38d85b9f429a	saliency-based content-aware lifestyle image mosaics	image saliency;image resizing;image cropping;feature extraction;image mosaic;image rendering;face segmentation;image retrieval	We proposed a novel content-aware lifestyle image mosaic technique based on image saliency.A novel energy map for variable-size tile decomposition is proposed by combining NIF and GBVS.Saliency-weighted image retrieval is introduced to choose the tile images.Seam carving is used to change the tile image's aspect ratio to meet the tile. In this paper, a novel content-aware lifestyle image mosaic technique is proposed based on image saliency. The image saliency is used in the whole process of creating mosaics. Firstly, a novel energy map for variable-size tile decomposition is proposed by combining Neighborhood Inhomogeneity Factor and Graph-Based Visual Saliency. The target image is divided into small tiles with variable sizes based on the energy map. Secondly, saliency-weighted image retrieval is introduced to choose the tile images from a certain image database. Different aspect ratio between the tile and the corresponding tile image may lead to obvious distortion. Therefore, before filled into the tile, the chosen tile image is resized by seam carving to change its aspect ratio. At last, color correction is done on the mosaic result to improve the color similarity. Compared with other mosaic methods, the proposed technique creates much better mosaics in visual aspect.	experiment;image retrieval	Dongyan Guo;Jinhui Tang;Ying Cui;Jundi Ding;Chunxia Zhao	2015	J. Visual Communication and Image Representation	10.1016/j.jvcir.2014.11.011	image texture;image restoration;computer vision;feature detection;color image;image processing;feature extraction;image retrieval;computer science;cropping;multimedia;computer graphics (images)	Vision	56.879647923073286	-61.62304434811797	182397
18d303a6ffa76fcacea1f48c2a8de4c55c6c7fb0	space-time light field rendering	image morphing;control systems;lighting control;image motion analysis;motion control;image morphing image based rendering space time light field epipolar constraint;epipolar constraint;photorealistic images;light field;spatiotemporal phenomena data visualisation image morphing image registration image sequences realistic images rendering computer graphics;layout;edge preserving image morphing;space time;global synchronized image;computer vision;space time light field;data visualisation;space time light field rendering;visualization;rendering system;image generation;optical imaging;photorealistic images space time light field rendering unsynchronized input video sequences data visualization smooth slow motion two stage rendering algorithm global synchronized image spatial temporal image registration algorithm edge preserving image morphing;two stage rendering algorithm;control system synthesis;synchronization;layout robustness motion control lighting control control systems visualization control system synthesis rendering computer graphics image generation image registration;image registration;light field rendering;data visualization;algorithms artificial intelligence computer graphics computer simulation image enhancement image interpretation computer assisted imaging three dimensional information storage and retrieval lighting models theoretical numerical analysis computer assisted user computer interface;smooth slow motion;spatiotemporal phenomena;realistic images;robustness;temporal integration;image based rendering;rendering computer graphics;cameras;adaptive optics;spatial temporal image registration algorithm;dynamic scenes;image sequences;unsynchronized input video sequences	In this paper, we propose a novel framework called space-time light field rendering, which allows continuous exploration of a dynamic scene in both space and time. Compared to existing light field capture/rendering systems, it offers the capability of using unsynchronized video inputs and the added freedom of controlling the visualization in the temporal domain, such as smooth slow motion and temporal integration. In order to synthesize novel views from any viewpoint at any time instant, we develop a two-stage rendering algorithm. We first interpolate in the temporal domain to generate globally synchronized images using a robust spatial-temporal image registration algorithm followed by edge-preserving image morphing. We then interpolate these software-synchronized images in the spatial domain to synthesize the final view. In addition, we introduce a very accurate and robust algorithm to estimate subframe temporal offsets among input video sequences. Experimental results from unsynchronized videos with or without time stamps show that our approach is capable of maintaining photorealistic quality from a variety of real scenes.	algorithm;alpha compositing;basic stamp;feature model;frame (physical object);itln1 wt allele;image registration;imagery;interpolation imputation technique;light field;matching;morphing;multivariate interpolation;multiview video coding;optical flow;published comment;real-time clock;stereopsis;telemedicine;visual effects;registration - actclass	Huamin Wang;Mingxuan Sun;Ruigang Yang	2007	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2007.1019	motion control;layout;synchronization;computer vision;image-based modeling and rendering;visualization;rendering;computer science;image registration;light field;space time;optical imaging;multimedia;adaptive optics;data visualization;robustness;computer graphics (images)	Visualization	57.24766596193576	-53.8225592647814	182444
f07c7cb28656cfab19afaf5708326a2981a640ff	pattern recognition using inverse resonance filtration	eigenvalue problem;singular value decomposition;vector space;pattern recognition;parameter estimation;correlation matrix;characteristic polynomial	An approach to textures pattern recognition based on inverse resonance filtration (IRF) is considered. A set of principal resonance harmonics of textured image signal fluctuations eigen harmonic decomposition (EHD) is used for the IRF design. It was shown that EHD is invariant to textured image linear shift. The recognition of texture is made by transfer of its signal into unstructured signal which simple statistical parameters can be used for texture pattern recognition. Anomalous variations of this signal point on foreign objects. Two methods of 2D EHD parameters estimation are considered with the account of texture signal breaks presence. The first method is based on the linear symmetry model that is not sensitive to signal phase jumps. The condition of characteristic polynomial symmetry provides the model stationarity and periodicity. Second method is based on the eigenvalues problem of matrices pencil projection into principal vectors space of singular values decomposition (SVD) of 2D correlation matrix. Two methods of classification of retrieval from textured image foreign objects are offered.	characteristic polynomial;eigen (c++ library);estimation theory;information retrieval facility;pattern recognition;quasiperiodicity;resonance;singular value decomposition;stationary process	Olga Sofina;Yuriy Bunyak;Roman Kvetnyy	2010	CoRR		covariance matrix;mathematical optimization;mathematical analysis;vector space;mathematics;geometry;estimation theory;characteristic polynomial;singular value decomposition;statistics	Vision	67.8044354015419	-58.53408741388302	182587
ce4e100e1b6a863c85d812480c573c1d2f1ed001	local scale control for edge detection and blur estimation	global solution;non convex optimization;edge detection;spatial scale;natural images;operator norm;local computation;fourier method	Selecting the appropriate spatial scale for local edge analysis is a challenge for natural images, where blur scale and contrast may vary over a broad range. While previous methods for scale adaptation have required the global solution of a non-convex optimization problem [8], it is shown that knowledge of sensor properties and operator norms can be exploited to define a unique, locally-computable minimum reliable scale for local estimation. The resulting method for local scale control allows edges spanning a broad range of blur scales and contrasts to be reliably localized by a single system with no input parameters other than the second moment of the sensor noise. Local scale control further permits the reliable estimation of local blur scale in complex images where the conditions demanded by Fourier methods for blur estimation break down.	edge detection;gaussian blur	James H. Elder;Steven W. Zucker	1996		10.1007/3-540-61123-1_127	computer vision;mathematical optimization;edge detection;operator norm;spatial ecology;computer science;mathematics;geometry	Vision	53.926589923549265	-56.696638052913094	182769
45c27cc86e934270e8d10a6cff8cc505e06960ae	motion-aware video quality assessment		This work focuses on considering motion towards improving video quality assessment algorithms. The improvement refers to improving computational video quality assessment algorithms in order to be in closer agreement with the subjective evaluation of video quality. We propose a motion saliency model that exploits motion features on spatial level and also an approach for consideration of global motion in the temporal dimension, leading to further improvements in the accuracy of video quality assessment. We perform evaluation by integrating our approaches in existing objective quality models and also by comparing them to existing related state-of-the-art video quality assessment methods.	algorithm	Marina Georgia Arvanitidou;Thomas Sikora	2017	2017 51st Asilomar Conference on Signals, Systems, and Computers	10.1109/ACSSC.2017.8335727	visualization;mathematical optimization;image quality;salience (neuroscience);computer vision;computer science;distortion;video quality;artificial intelligence	Vision	63.50931198627713	-63.683717945979865	182829
58678846bcfb8cc4ad04ef4ec75aedc66494c26e	simulating ordered grb color triples using j to create images with gradients in both hue and value	data array;grb color triple;raster image;216-color web palette;contour map;grb palette;perfect order;suggested order;new palette	This paper proposes a way to connect the properties of color called hue and value. We order colors using J and then simulate a palette of GRB color triples. The new palette arranges the colors in an almost perfect order from dark to light. From the suggested order for all 16, 777, 216 colors, we select a 216-color web palette which orders its colors by both hue and value. Using the GRB palette we create raster images from data arrays. In these images the colors indicate the relative magnitudes of the data. Contour maps follow easily. The proposed model serves as an excellent way to develop arrays in J. The model also demonstrates the process by which we make conjectures, create prototypes and assess their weaknesses and strengths.	color;contour line;gradient;map;palette (computing);raster graphics;simulation;web colors	Linda Alvord;Tama Traberman	2003		10.1145/882067.882068	computer vision;mathematics;computer graphics (images)	Graphics	61.787302980145554	-60.28417834109716	182877
9d1ce482cc669b5cbcb973024ac0671d8044c619	learning-based underwater image enhancement with adaptive color mapping	image color analysis dictionaries signal processing algorithms scattering image enhancement histograms cameras;marine engineering computer vision image colour analysis image enhancement image restoration image segmentation;high level computer vision algorithm learning based underwater image enhancement adaptive color mapping blurring color cast underwater imaging automatic segmentation color correction algorithm sub sea image learned dictionary sparse representation distance object camera water quality	Blurring and color cast are two of the most challenging problems for underwater imaging. The poor quality hinders the automatic segmentation or analysis of images. In this paper, we describe an image enhancement method to reduce the blurring and color cast of the underwater medium. It is a two-folded approach; First, a color correction algorithm is applied to correct the color cast and produce a natural appearance of the sub-sea images. Second, a pair of learned dictionaries based on sparse representation are applied to sharpen the image and enhance the details. Our strategy is a single image approach that does not require additional knowledge of environment such as depth, distance object/camera or water quality. The experimental results show that the proposed method can efficiently enhance almost every underwater image; And offers a quality that is typically sufficient for the high level computer vision algorithms.	algorithm;autostereogram;color mapping;computer vision;dictionary;high-level programming language;image editing;image quality;sparse approximation;sparse matrix	Fahimeh Farhadifard;Zhiliang Zhou;Uwe von Lukas	2015	2015 9th International Symposium on Image and Signal Processing and Analysis (ISPA)	10.1109/ISPA.2015.7306031	image quality;demosaicing;color histogram;image texture;image restoration;false color;computer vision;feature detection;color quantization;hsl and hsv;color normalization;color image;image gradient;binary image;image processing;computer science;machine learning;image segmentation;automatic image annotation;computer graphics (images)	Vision	58.36335123666218	-61.77855510127548	182902
4366ac6d2726eb22dcbf7c6ad989b419cc35ca66	on independent color space transformations for the compression of cmyk images	image sampling;image coding;image processing;data compression;reproduccion color;color space;procesamiento imagen;transformacion;traitement image;rate distortion theory;image sampling image colour analysis image coding data compression rate distortion theory;image coding printing color image converters transform coding distortion measurement testing image generation page description languages monitoring;image colour analysis;color reproduction;rate distortion theory independent color space transformations cmyk image compression image independent color space transformations device independent color space transformation yycc color space independent compression chrominance subsampling ycbcrk image dependent klt based approach;espace chromatique;compresion dato;transformation;espacio cromatico;imagen color;reproduction couleur;image couleur;compression donnee;color image	Device and image-independent color space transformations for the compression of CMYK images were studied. A new transformation (to a YYCC color space) was developed and compared to known ones. Several tests were conducted leading to interesting conclusions. Among them, color transformations are not always advantageous over independent compression of CMYK color planes. Another interesting conclusion is that chrominance subsampling is rarely advantageous in this context. Also, it is shown that transformation to YYCC consistently outperforms the transformation to YCbCrK, while being competitive with the image-dependent KLT-based approach.	chroma subsampling;color management;color space;compression;cell transformation;kang-lai-te	Ricardo L. de Queiroz	1999	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.791970	data compression;transformation;computer vision;color image;rate–distortion theory;image processing;computer science;mathematics;color space;computer graphics (images)	Visualization	61.9585324938186	-62.523227140630645	182929
7df039049948d54fd1f4d75526743f315cf4865f	image and video upscaling from local self-examples	filter bank;real time;low resolution;natural images;nondyadic filter banks;image and video upscaling;resolution enhancement;natural image modeling;super resolution;parallel implementation;high resolution imager;high definition;wavelets;scale invariance	We propose a new high-quality and efficient single-image upscaling technique that extends existing example-based super-resolution frameworks. In our approach we do not rely on an external example database or use the whole input image as a source for example patches. Instead, we follow a local self-similarity assumption on natural images and extract patches from extremely localized regions in the input image. This allows us to reduce considerably the nearest-patch search time without compromising quality in most images. Tests, that we perform and report, show that the local self-similarity assumption holds better for small scaling factors where there are more example patches of greater relevance. We implement these small scalings using dedicated novel nondyadic filter banks, that we derive based on principles that model the upscaling process. Moreover, the new filters are nearly biorthogonal and hence produce high-resolution images that are highly consistent with the input image without solving implicit back-projection equations. The local and explicit nature of our algorithm makes it simple, efficient, and allows a trivial parallel implementation on a GPU. We demonstrate the new method ability to produce high-quality resolution enhancement, its application to video sequences with no algorithmic modification, and its efficiency to perform real-time enhancement of low-resolution video standard into recent high-definition formats.	algorithm;autostereogram;database;filter bank;graphics processing unit;hdmi;image resolution;image scaling;real-time clock;relevance;self-similarity;super-resolution imaging	Gilad Freedman;Raanan Fattal	2011	ACM Trans. Graph.	10.1145/1944846.1944852	wavelet;computer vision;simulation;image resolution;computer science;scale invariance;filter bank;mathematics;statistics;superresolution;computer graphics (images)	Graphics	55.64621207136526	-62.87904976612466	183007
63ff99a0070de1092cf0764a353659b63863c070	a fast auto exposure algorithm for industrial applications based on false-position method		In this paper we implemented an Auto-exposure algorithm based on the False-position method in order to correctly expose the leather samples. Though we are doing this for the leather industry, we can directly use this false position based, auto exposure algorithm for the natural scenes. The main reason for choosing the False-position method is that, it converge the root values quickly when compared to bisection method. The implementation of our auto exposure algorithm is performed by using the point grey research programmable camera.	algorithm;false position method	B. Ravi Kiran;G. V. N. A. Harsha Vardhan	2013		10.1007/978-3-319-02931-3_58	false position method;algorithm;bisection method;computer science	EDA	54.858238080145206	-58.08669187593467	183083
537534c0dc3d0b79b4ed57217249ec5de470bde0	digital archiving of tapestries of kyoto gion festival using a high-definition and multispectral image capturing system	image capture;image resolution;optical filters;wiener estimation digital archiving tapestries kyoto gion festival multispectral image capturing system archiving project high resolution multiband imaging camera illumination spectral reflectance 6 band image;digital archive;digital cameras;sensitivity;ultra high definition image;image color analysis;color reproduction;ultra high definition image digital archive multispectral imaging color reproduction;lenses;image retrieval cameras image capture;image color analysis lenses digital cameras image resolution sensitivity optical filters;cameras;multispectral imaging;image retrieval	We report the archiving project of Kyoto Gion Festival using high-resolution multiband imaging camera. We have been developing a two-shot six-band image capturing system for recording the color and physical properties of early modern tapestries. In an experiment, an image of tapestry whose image size was 2700 M-pixel was synthesized. The resolution of the images was 0.02 mm/pixel. Accurate color under an arbitrary illumination and spectral reflectance can be reproduced from the 6-band image using the Wiener estimation.	archive;image resolution;multispectral image;pixel;wiener–khinchin theorem	Masaru Tsuchida;Kunio Kashino;Junji Yamato;Aki Takayanagi;Wataru Wakita;Hiromi T. Tanaka	2013	2013 International Conference on Culture and Computing	10.1109/CultureComputing.2013.66	computer vision;geography;remote sensing;computer graphics (images)	Robotics	63.16061652360914	-57.4509452100329	183108
37ab9d65ddd11a69bddabdbdec699e91afe82c56	blind removal of image non-linearities	optical imaging;image processing;layout;frequency domain;higher order;computer science;explicit knowledge;calibration;second order;frequency domain analysis;lenses	This paper presents a technique for blindly removing image non-linearities in the absence of any calibration information or explicit knowledge of the imagingdevice. The basic approach exploits the fact that a non-linearity introduces specijc higher-order correlations in the frequency domain (beyond second-order). These correlations can be detected using tools from polyspectral analysis. The non-linearities can then be estimated and removed by simply minimizing these correlations.	nonlinear system	Hany Farid;Alin C. Popescu	2001		10.1109/ICCV.2001.10009	computer vision;simulation;speech recognition;higher-order logic;computer science;explicit knowledge;mathematics;frequency domain;second-order logic	Vision	58.26842986688485	-65.65047468785727	183139
33547d9287fb8c591e2348e85b852ba499fbb37e	robust local restoration of space-variant blur image	piecewise linear;restauration image;convolution;fotografia digital;0130c;photographie numerique;image restoration;0768;algorithme;red green blue;digital photography;optical modulator;algorithms;refractive index;radiometric corrections;cameras	In this paper, we propose a space variant image restoration method where the each different local regions of a given image are de-blurred by each different estimated de-convolution filter locally. The depth of each local blocks are estimated roughly on the optical module representing different indices of refraction for different wavelengths of light. Following the depth, each different region of an image is restored based on the sharpest channel among 3 channels (Red, Green, Blue). Then, in order to prevent discontinuities between the differently restored image regions, we use the piecewise linear interpolation on overlapping regions. Also, practically, this method is applied to 3Mega camera module in order to confirm the effect of proposed algorithm.© (2008) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	circuit restoration;gaussian blur	JaeGuyn Lim;Joo Young Kang;Hyun Wook Ok	2008		10.1117/12.766456	image restoration;rgb color model;computer vision;digital photography;optical modulator;piecewise linear function;convolution;refractive index;optics;physics;computer graphics (images)	Vision	58.74737880487427	-60.48878890068027	183312
78f50f370fa4652a8b45e5296eab07fe99258659	adaptive fuzzy contrast factor enhancement technique for low contrast and nonuniform illumination images	school of engineering sciences	This paper presents a new enhancement technique using the fuzzy set theory for low contrast and nonuniform illumination images. A new parameter called the contrast factor which will provide information on the difference among the gray-level values in the local neighborhood is proposed. The contrast factor is measured by both local and global information to ensure that the fine details of the degraded image are enhanced. This parameter is used to divide the degraded image into bright and dark regions. The enhancement process is applied on gray-scale images wherein the modified Gaussian membership function is employed. The process is performed separately according to the image’s respective regions. The performance of the proposed method is comparable with other state-of-the-art techniques in terms of processing time. The proposed method exhibits the best performance and defeats other methods in terms of preserving brightness and details without amplifying existing noises.	algorithm;fuzzy set;grayscale;image quality;pixel;real-time clock;real-time computing;set theory;zero suppression	Khairunnisa Hasikin;Nor Ashidi Mat Isa	2014	Signal, Image and Video Processing	10.1007/s11760-012-0398-x	computer vision;computer science;artificial intelligence;mathematics	Vision	56.96430114379	-63.59606390964226	183360
21cb1e7f74527861b1847ca067963cc2b60db139	n-candidate methods for location invariant dithering of color images	location invariant;n candidate methods;image quality;dithering;parallelization;color image quantization;color image	We introduce a new class of dithering methods called N-candidate methods. The main idea is that the output color is randomly chosen among several candidate colors so that the estimated color average would be preserved. The dithering process is pixelwise without any interaction with the neighboring pixels. The N-candidate methods are thus location invariant, which has two benefits: (1) the algorithm can be fully parallelized; and (2) the image can be partially processed without effecting the pixels outside the processed part. The proposed approach allows more efficient dithering than error diffusion but at the cost of a slightly lower image quality. q 2000 Elsevier Science B.V. All rights reserved.	8-bit color;algorithm;color;dither;error diffusion;experiment;image quality;parallel computing;pixel;printing;quantization (signal processing);randomness	Kjell Lemström;Pasi Fränti	2000	Image Vision Comput.	10.1016/S0262-8856(99)00039-6	image quality;color histogram;computer vision;color quantization;color image;floyd–steinberg dithering;binary image;computer science;theoretical computer science;mathematics;8-bit color;dither;computer graphics (images)	Vision	55.39504202721763	-62.612144788787845	183413
2be0f9c5ca6672e275a2d2062e522078f0d4556e	automated bird plumage coloration quantification in digital images		Quantitative measurements of bird plumage color and patch size provide valuable insights into the impact of environmental conditions on the habitat and breeding of birds. This paper presents a novel perceptual-based framework for the automated extraction and quantification of bird plumage coloration from digital images with slowly varying background colors. The image is first coarsely segmented into a few classes using the dominant colors of the image in a perceptually uniform color space. The required foreground class is then identified by eliminating the dominant background color based on the color histogram of the image. The determined foreground is segmented further using a Bayesian classifier and an edge-enhanced model-based classification for eliminating regions of human skin and is refined by using a perceptual-based Saturation-Brightness quantization to only preserve the perceptually relevant colors. Results are presented to illustrate the performance of the proposed method.		Tejas S. Borkar;Lina J. Karam	2014		10.1007/978-3-319-14364-4_21	artificial intelligence;computer vision;digital image;pattern recognition;naive bayes classifier;computer science;plumage;color histogram;quantization (signal processing);color space	HCI	57.576248438538215	-62.66728319157153	183433
26590025ff1424ad564f511b33530f86a084bc6a	spatio-temporal segmentation and object tracking: an applicaton to second generation video coding	video coding;object tracking		data compression;second generation multiplex plus	Fabrice Moscheni	1997	Signal Processing	10.1016/S0165-1684(97)89609-0	computer vision;computer science;video tracking;mathematics;multiview video coding	Vision	56.488623430669065	-55.05263044417325	183456
1a6d5cd7bb2770cecd54cdb803226be91aa86f39	a decoupled method for image inpainting with patch-based low rank regulariztion			inpainting	Fang Xing Li;Xiao-Guang Lv	2017	Applied Mathematics and Computation	10.1016/j.amc.2017.06.027		ML	56.60956192185303	-54.885279646822084	183501
4bb14d3beb7f257a31fbc34304693ee7cd730cb1	an improved technique for full spectral rendering	kvazi monte carlo;sledovani paprsku;cele spektrum;vykreslovani;clanek;article	In this paper we present an improved approach to full spectral rendering. The technique is optimized for quasi-Monte Carlo ray tracing, however the underlying physical theory can be applied to any global illumination scheme. We start with explanation of the necessity of full spectral rendering in any correct global illumination system. Then we present, step by step, a rendering scheme using full spectrum simulation. First, we give details on a random point sampling as a method of representing spectra, then we introduce improved spectral sampling technique, designed to reduce variance of image of wavelength dependent phenomena, and finally we show how to integrate the novel sampling technique with selected ray tracing algorithms.	algorithm design;algorithmic efficiency;approximation algorithm;bidirectional search;computation;computational complexity theory;emoticon;gibbs sampling;global illumination;importance sampling;monte carlo integration;monte carlo method;nearest-neighbor interpolation;overhead (computing);path tracing;photon mapping;quasi-monte carlo method;ray tracing (graphics);rendering (computer graphics);sampling (signal processing);simulation;spectral rendering;specular highlight;weight function	Michal Radziszewski;Krzysztof Boryczko;Witold Alda	2009	Journal of WSCG		computer graphics (images)	Graphics	64.31620489916631	-53.28738226321436	183664
88b21594cd504f24873d4ddd1ef1a118d840470e	rolling shutter effect compensation with global waves analysis	cmos integrated circuits;image resolution;sensors;image processing cmos image sensors;interference;rolling shutter effect compensation wobble effect geometric image distortion general compensation algorithm global wave analysis image processing technique cmos camera;algorithm design and analysis cameras noise sensors cmos integrated circuits image resolution interference;global wave analysis cmos camera rolling shutter effect video correction;cmos camera;video correction;rolling shutter effect;algorithm design and analysis;global wave analysis;cameras;noise	CMOS camera expose different portions of the frame at different points in time, resulting in the so-called rolling shutter effect that bright and dark region of striation moving in the movie vertically. In this paper, we propose an image processing technique using global wave analysis and develop a general compensation algorithm to handle this problem. Researches in this area aggregated in geometric image distortion or wobble effect. Our algorithm can perform this rolling shutter effect removal under varying environments. Experimental results demonstrate improvement of quality on rolling shutter effected video.	algorithm;cmos;distortion;image processing;movie projector	Xiaofu Huang;Bin Sheng;Ruimin Shen;Ping Li	2015	2015 IEEE International Conference on Multimedia Big Data	10.1109/BigMM.2015.81	computer vision;electronic engineering;engineering;cmos sensor;computer graphics (images)	Robotics	60.083857776426996	-57.67208618392405	183788
8194376c6a515792cfe9526c2798784c35632951	weighted-adaptive motion-compensated frame rate up-conversion	motion analysis;block boundaries;filtering;image matching motion compensation interpolation;degradation;interpolation;frame rate up conversion algorithm;motion compensation;video signal processing;overlapping processing;motion analysis weighted adaptive frame rate up conversion video sequences frame rates motion compensated frame rate up conversion frame rate up conversion algorithm weighted adaptive motion compensated interpolation blocking artifacts reduction weighted sum of multiple interpolation filtering block boundaries overlapping block motion compensation overlapping processing;image matching;overlapping processing motion compensated frame rate up conversion weighted adaptive motion compensated interpolation wamci block artifacts interpolation scheme weighted sum block artichokes block boundaries;motion estimation;video sequences;overlapped block motion compensation;testing;yield estimation;indexing terms;interpolation filtering motion analysis motion estimation displays adaptive filters yield estimation testing image quality degradation;motion compensated;adaptive filters;adaptive signal processing;frame rates;weighted sums;motion estimation interpolation motion analysis motion compensation displays filtering algorithms nonlinear filters image converters pixel digital tv;overlapping block motion compensation;displays;image quality;weighted adaptive frame rate up conversion;weighted adaptive motion compensated interpolation;weighted sum of multiple interpolation filtering;filtering theory;blocking artifact;blocking artifacts reduction;adaptive signal processing interpolation video signal processing filtering theory motion compensation;motion compensated frame rate up conversion	In this paper, we propose a frame rate upconversion algorithm using the weighted-adaptive motioncompensated interpolation (WAMCI) that reduces the block artifacts due to the failure of motion estimation and blockbased processing. The proposed method is based on the interpolation scheme by weighted sum of multiple motioncompensated interpolation (MCI) images. Also, in the proposed method, the block artifacts on the block boundaries are reduced by applying a technique similar to the overlapped block motion compensation (OBMC). To reduce the blurring of overlapping processing, the proposed method uses the motion analysis to determine the type of motion and applies the OBMC adaptively. Experimental results indicate good performance of the proposed scheme with significantly reduced block artifacts. Index Terms — Frame rate up-conversion, motion estimation, motion-compensated interpolation, weightedadaptive motion-compensated interpolation.	algorithm;computational complexity theory;frame rate control;interpolation;motion compensation;motion estimation;ringing artifacts;weight function	Sung-Hee Lee;Ohjae Kwon;Rae-Hong Park	2003	IEEE Trans. Consumer Electronics	10.1109/TCE.2003.1233759	adaptive filter;computer vision;mathematical optimization;computer science;control theory;mathematics	Robotics	57.32426810664373	-63.405058951589105	183831
6ff03dd99695809500f91f6b34d671be3cf9a956	evaluating of selected systems for colorimetric calibration of lcd monitors		In many applications there is a need for exact colour reproduction. The common solution of this problem is the usage of an open Colour Management System and device profiles. The profiles are determined in a colorimetric calibration process, so the quality of this process influences colour reproduction quality. The aim of this work was to evaluate the colour reproduction quality of monitors depending on used types of monitor and calibration system. Obtained results show that colour reproduction strongly depends on those devices and good results can be achieved even on a monitor not designed for colour critical works. However, in case where exact colour reproduction is crucial the use of a professional-grade graphical monitor is necessary.	computer monitor	Artur Bal;Andrzej Kordecki;Henryk Palus;Mariusz Frackiewicz	2015		10.1007/978-3-319-23437-3_28	calibration;computer vision;liquid-crystal display;colorimetry;artificial intelligence;computer science	Logic	62.86619196615267	-58.8973534186176	183872
4e45470a2b4ea441c8c5bcc2844625d1f8b840c4	estimation of frame sequence noise with removal of jpeg artifacts	image processing;image restoration;noise	This paper proposes a method to remove JPEG noise artifacts from frame sequences. Using extensive experimental results we show how an online system with periodic noise estimation functionality can estimate the real frame noise even if the images are in JPEG format. We present the mathematical basis of the methodology and show in real content that we can have reliable measurements. We also present the results obtained on a real network camera and show that our method can provide a much better estimation of the noise standard deviation compared to common practice but comparable interchannel and spatial intra-channel correlation estimates. We also provide some guidelines for capturing datasets necessary to apply computer vision tasks. Our approach exploits the well known stochastic linearization phenomenon which we prove that is present in our case.	compression artifact;computer vision;ip camera;jpeg;whole earth 'lectronic link;on-line system	Vasileios I. Anagnostopoulos;Emmanuel Sardis;Theodora A. Varvarigou	2013	Int. J. Image Graphics	10.1142/S0219467813500034	image restoration;computer vision;simulation;image processing;computer science;noise measurement;noise;computer graphics (images)	Vision	58.89998672230175	-57.75013959922992	184091
32d0e62119aa0de68213c0a78953646a10f0e9b2	image resizing in an arbitrary transform domain	image;image scaling;matrix multiplication;computational complexity;image resolution;orthogonal transformation	This paper develops a methodology for resizing image resolutions in an arbitrary orthogonal block transform domain. To accomplish this, we represent the procedures resizing images in an orthogonal transform domain in the form of matrix multiplications from which the matrix scaling the image resolutions is produce. The experiments showed that the proposed method produces reliable performance without increasing the computational complexity, compared to conventional methods when applied to various transforms.	image scaling	Wonha Kim;Hyung Suk Oh	2009	IEICE Transactions		image quality;computer vision;image resolution;image processing;matrix multiplication;computer science;theoretical computer science;image;mathematics;computational complexity theory;dimensioning;multiplication;image scaling;computer graphics (images);orthogonal transformation	Vision	65.64250844206508	-56.0019026761708	184152
1d076490b9630844d31c49b9dacb789bc4ea6a8f	texture compression of light maps using smooth profile functions	datavetenskap datalogi	Light maps have long been a popular technique for visually rich real-time rendering in games. They typically contain smooth color gradients which current low bit rate texture compression techniques, such as DXT1 and ETC2, do not handle well. The application writer must therefore choose between doubling the bit rate by choosing a codec such as BC7, or accept the compression artifacts, neither of which is desirable. The situation is aggravated by the recent popularity of radiosity normal maps, where three light maps plus a normal map are used for each surface. We present a new texture compression algorithm targeting smoothly varying textures, such as the light maps used in radiosity normal mapping. On high-resolution light map data from real games, the proposed method shows quality improvements of 0.7 dB in PSNR over ETC2, and 2.8 dB over DXT1, for the same bit rate. As a side effect, our codec can also compress many standard images (not light maps) with better quality than DXT1/ETC2.	academy;algorithm;codec;color gradient;compression artifact;ericsson texture compression;image quality;image resolution;lightmap;map;normal mapping;peak signal-to-noise ratio;period-doubling bifurcation;radiosity (computer graphics);real-time locating system;s3 texture compression;smoothing	Jim Rasmusson;Jacob Ström;Per Wennersten;Michael C. Doggett;Tomas Akenine-Möller	2010			computer vision;mathematics;multimedia;texture compression;computer graphics (images)	Graphics	65.32851138424205	-54.24498439807822	184189
d19aec331111432f6659f0cb9f2a36f591139122	reclustering techniques improve early vision feature maps	cluster algorithm;cost function;cost reduction;natural images;segmentation;reclustering;clustering;stereo;optical flow;statistical distribution;feature maps improvement	We propose a recursive post-processing algorithm to improve feature-maps, like disparity- or motion-maps, computed by early vision modules. The statistical distribution of the features is computed from the original feature-map, and from this the most likely candidate for a correct feature is determined for every pixel. This process is performed automatically by a clustering algorithm which determines the feature candidates as the cluster centres in the distribution. After determining the feature candidates, a cost function is computed for every pixel, and a candidate will only replace the original feature if the cost is reduced. In this way, a new feature-map is generated which, in the next iteration, serves as the basis for the computation of the updated feature distribution. Iterations are stopped if the total cost reduction is less than a pre-defined threshold. In general, our technique is albe to reduce two of the most common problems that affect feature-maps, the sparseness, i.e. the presence of areas where the algorithm is not able to give meaningful measurements, and the blur. To show the efficacy of our approach, we apply the reclustering algorithm to several examples of increasing complexity, showing results for synthetic and natural images.	algorithm;cluster analysis;computation;gaussian blur;iteration;loss function;map;neural coding;pixel;recursion;synthetic intelligence;video post-processing	Alex Cozzi;Florentin Wörgötter	1998	Pattern Analysis and Applications	10.1007/BF01238025	probability distribution;computer vision;computer science;machine learning;pattern recognition;optical flow;mathematics;cluster analysis;stereophonic sound;segmentation;feature	Vision	53.7717598314719	-59.05187106996421	184302
db540f6ec0fab5b2da230d8f3e99abe10b0ee65a	image contrast enhancement based on the intensities of edge pixels	contrast enhanced	Histogram modification can improve the contrast of an image. Histogram equalization has been the most popular histogram modification technique. However, the technique has the tendency to magnify local noise for images with large homogeneous regions. In this paper we suggest a new histogram modification technique which utilizes the intensity distribution of the edge pixels of an image. We first identify the edge pixels of an image. Then the intensity histogram of the edge pixels is constructed. An intensity transformation function is derived from the edge-pixel histogram and then applied to the entire image. In general, this transformation will increase the intensity difference between neighboring homogeneous regions. We also have suggested three tools to measure the performance of contrast-enhancing methods. The three measurements are image contrast value, image information loss value, and local intensity variance value. Our goal for enhancing is to significantly increase an image's contrast value while keeping both the information loss value and the local intensity variance value low. In the experiments, we have compared the performance of the suggested method with that of the ordinary histogram equalization technique and the local area histogram equalization (LAHE) technique using both synthetic and real images. The results were then evaluated by the three tools. The suggested method performed very well both analytically and visually.	pixel	Jia-Guu Leu	1992	CVGIP: Graphical Model and Image Processing	10.1016/1049-9652(92)90069-A	color histogram;computer vision;color normalization;computer science;histogram matching;balanced histogram thresholding;mathematics;adaptive histogram equalization;histogram equalization;image histogram;computer graphics (images)	ML	56.81734293996277	-62.88554695933777	184373
0d38e3ab76cd71afad3555452ac3e4c606ba8da7	fast image super-resolution via local adaptive gradient field sharpening transform		This paper proposes a single-image super-resolution scheme by introducing a gradient field sharpening transform that converts the blurry gradient field of upsampled low-resolution (LR) image to a much sharper gradient field of original high-resolution (HR) image. Different from the existing methods that need to figure out the whole gradient profile structure and locate the edge points, we derive a new approach that sharpens the gradient field adaptively only based on the pixels in a small neighborhood. To maintain image contrast, image gradient is adaptively scaled to keep the integral of gradient field stable. Finally, the HR image is reconstructed by fusing the LR image with the sharpened HR gradient field. Experimental results demonstrate that the proposed algorithm can generate more accurate gradient field and produce super-resolved images with better objective and visual qualities. Another advantage is that the proposed gradient sharpening transform is very fast and suitable for low-complexity applications.	algorithm;artificial neural network;autostereogram;complexity;deep learning;emoticon;gradient descent;high-resolution scheme;image gradient;image resolution;iterative reconstruction;lr parser;pixel;super-resolution imaging;upsampling	Qiang Song;Ruiqin Xiong;Dong Liu;Zhiwei Xiong;Feng Wu;Wen Gao	2018	IEEE Transactions on Image Processing	10.1109/TIP.2017.2789323	computer vision;vector field;iterative reconstruction;interpolation;pixel;artificial intelligence;image gradient;image resolution;superresolution;mathematics;pattern recognition;sharpening	Vision	58.80017021009343	-65.67673209017714	184960
52e4cc1ab3c6edd24b3e82c4b68ffc82a8ba974f	real-time active range finder using light intensity modulation	laser beam;intensite lumineuse;lasers;ccd camera;mirrors;filtro optico;espejo giratorio;image processing;median filter;reflectivity;miroir tournant;standard deviation;real time;procesamiento imagen;modulacion;haz laser;traitement image;filtre passe bande;telemetro;telemetre;band pass filter;optical filter;light intensity;temps reel;bandpass filters;filtro paso banda;lenses;intensidad luminosa;camara ccd;tiempo real;camera ccd;faisceau laser;bandpass filter;luminous intensity;video;rotating mirror;range finder;cameras;filtre optique;modulation	We propose a new real-time active range finder system, which operates at a video frame rate. Our system consists of a laser line pattern marker, a rotating mirror, a camera with an optical narrow bandpass filter, and a pipe-lined image signal processor. The vertical laser line pattern is horizontally scanned by the video-synchronized rotating mirror. The pattern is modulated with alternating a monotone decreasing intensity function and a monotone increasing function. The bandpass filter selectively transmits the laser light to decrease the disturbance of background light. Depth images are obtained using the intensity ratio of successive video field images, the coordinates of each pixel, and the baseline length. The intensity ratio specifies a line pattern in a plane. Meanwhile, the coordinates of each pixel specify a line that goes through the pixel position on the CCD and the center of the lens. Depth is calculated as an intersection point on the specified plane and line. The image signal processor can perform the above calculation using LUTs within a video frame. In this paper, we evaluate the measurable range, precision, and color properties (i.e., reflectance properties) of our system. Experimental results show that a two meters measurable range is obtained with a 50 mW laser power, the standard deviations of the depth images with 5 x 5 median filtering are about 1% of the measured depth, and the object color has little effect on the measured depth except when the reflectance of the object is very small.	baseline (configuration management);charge-coupled device;field (video);frame language;median filter;modulation;pixel;point process;real-time clock;signal processing;monotone	Takeo Azuma;Kenya Uomori;Atsushi Morimura	1999		10.1117/12.341067	computer vision;electronic engineering;geography;optics	Vision	61.73119895302395	-58.266534992825655	184966
da83c49a5c17a22735ffce438ac16ef85531457b	tone-replacement error diffusion for multitoning	printing;image reconstruction dot distribution tone replacement error diffusion multitoning halftone method image quality image processing visual perception;quantization signal;ordered dithering multitoning banding effect anisotropy error diffusion;satellite broadcasting;image reconstruction;image quality;image quality quantization signal satellite broadcasting printing ink electrical engineering visual perception;ink;visual perception;electrical engineering	Error diffusion is an efficient halftone method for mainly being applied on printers. The promising high image quality and processing efficiency endorse it as a popular and competitive candidate in halftoning and multitoning applications. The multitoning is an extension of halftoning, adopting more than two-tone levels for the improvement of the similarity between an original image and the converted image. Yet, the banding effect, indicating the areas with discontinuous tone level, disturbs the visual perception, and thus seriously degrades image quality. To solve the banding effect, the tone-replacement strategy is proposed in this paper. As documented in the experimental results, excellent tone-similarity as that of the original image and promising reconstructed dot-distribution can be provided simultaneously. Comparing with the former banding-free methods, the apparent improvements/features suggest that the proposed method can be a very competitive candidate for multitoning applications.	anisotropy;colors of noise;colour banding;document completion status - documented;error diffusion;experiment;frequency analysis;image quality;mathematical optimization;numerous;selective calling	Jing-Ming Guo;Jia-Yu Chang;Yun-Fu Liu;Guo-Hong Lai;Jiann-Der Lee	2015	IEEE Transactions on Image Processing	10.1109/TIP.2015.2460451	iterative reconstruction;image quality;computer vision;speech recognition;visual perception;computer science;computer graphics (images)	Vision	59.00951814884504	-63.555816333856725	185014
b1061811b601defb3ec79a5f87680888d6fd8145	an evolutionary blind image deconvolution algorithm through the pseudo-wigner distribution	quality metric;multi objective optimisation;blind deconvolution;information extraction;articulo;image fusion;image enhancement;quality assessment;wigner distribution;blind image deconvolution;pseudo wigner distribution;evolutionary algorithms;perceptual metric;space frequency;evolutionary algorithm	This paper describes a new blind deconvolution method implemented by means of an evolutionary algorithm (EA). The EA is designed following a multi-objective optimisation problem approach. The last generation of the EA is assessed by different quality metrics for determining the solution that provides the best performance. It is shown that different restored images can be obtained from a given testing image. The selection of the best result is accomplished though the use of quality metrics. However, the existence of many quality metrics entails a difficult problem for determining the best output. Here, we present a new robust quality metric, based on the use of the local space-frequency information extracted from the Wigner distribution. We empirically compared its performance with other well-known perceptual metrics. In addition to that, a fusion procedure between all candidate restored output images from the EA is also proposed as an alternative to the selection process. The fusion method is also based on the use of this new measure recently developed by the authors with excellent experimental results. 2005 Elsevier Inc. All rights reserved.	blind deconvolution;evolutionary algorithm;mathematical optimization;multi-objective optimization;software quality assurance;wigner distribution function	Salvador Gabarda;Gabriel Cristóbal	2006	J. Visual Communication and Image Representation	10.1016/j.jvcir.2005.07.005	computer vision;mathematical optimization;computer science;wigner distribution function;machine learning;evolutionary algorithm;pattern recognition;mathematics;blind deconvolution;image fusion	Vision	61.71727052599699	-65.78224077757208	185373
56ee4f52fefb9e0c0e84f2f9dc262326030073d9	expressive chromatic accumulation buffering for defocus blur	defocus blur;optical effect;spectral rendering;chromatic aberration;lens system	This article presents a novel parametric model to include expressive chromatic aberrations in defocus blur rendering and its effective implementation using the accumulation buffering. Our model modifies the thin-lens model to adopt the axial and lateral chromatic aberrations, which allows us to easily extend them with nonlinear and artistic appearances beyond physical limits. For the dispersion to be continuous, we employ a novel unified 3D sampling scheme, involving both the lens and spectrum. We further propose a spectral equalizer to emphasize particular dispersion ranges. As a consequence, our approach enables more intuitive and explicit control of chromatic aberrations, unlike the previous physically-based rendering methods.	box blur;compositing;equalization (communications);experience;gaussian blur;lateral thinking;nonlinear system;parametric model;sampling (signal processing);stellar classification;tree accumulation	Yuna Jeong;Sangmin Lee;Soonhyeon Kwon;Sungkil Lee	2016	The Visual Computer	10.1007/s00371-016-1244-x	computer vision;chromatic aberration;mathematics;computer graphics (images)	Graphics	63.07532715143526	-53.65368318641598	185756
fd4bffaaf90976fcfeeb71ba77849958b2ecf2f3	an optimized radial basis function model for color characterization of a mobile device display	color displays;piecewise linear approximation;neural networks;00 01;characterization;colorimetry;approximation methods;99 00;radial basis functions	This paper presents an optimized color characterization model based on radial basis functions (RBF). The performance of the proposed model was tested on a number of different mobile devices and compared with the performance of other state of the art color characterization models. We compared the accuracy of models using the CIELAB color difference. Four different models were discussed in detail: Piecewise Linear Model Assuming Variation in Chromaticity, Polynomial regression, Artificial Neural Network, and proposed Radial Basis Function model. For training and evaluation of the models we measured a large number of color samples on various mobile device displays. Results have shown that our optimized RBF model has superior accuracy over other models with median color difference of 0.39. In addition, it has particularly good accuracy for colors on the boundary of device’s gamut with maximum color difference of 0.87, where other models shown unacceptably high (>10) color difference.	artificial neural network;color;function model;linear model;mobile device;polynomial;radial (radio);radial basis function	Ante Poljicak;Jurica Dolic;Jesenka Pibernik	2016	Displays	10.1016/j.displa.2015.12.005	computer vision;radial basis function;electronic engineering;colorimetry;computer science;radial basis function network;artificial neural network;computer graphics (images)	Vision	65.4957804096033	-60.0629547451996	185784
5d03def7da3ecc59b3db865f123e67a6e28fa3b8	an adaptive pansharpening method by using weighted least squares filter	image fusion;least squares approximations geophysical image processing image fusion;distortion remote sensing hybrid fiber coaxial cables satellites image fusion spatial resolution;distortion;hybrid fiber coaxial cables;remote sensing;satellites;spectral signatures adaptive pansharpening method weighted least squares filter multisensor image fusion multispectral image panchromatic image intensity hue saturation based methods pansharpened ms images local artifacts spectral distortions low frequency components;weighted least squares wls filter image fusion intensity hue saturation ihs transform pansharpening;spatial resolution	Multisensor image fusion or pansharpening aims to sharpen a multispectral (MS) image by integrating the detail map derived from a panchromatic (Pan) image. The intensity-hue-saturation (IHS)-based methods are well adopted in pansharpening applications. However, the pansharpened MS images by IHS-based methods usually suffer from serious spectral distortions and local artifacts due to the mismatch between the estimated detail map and its ground truth. To overcome these defects, we propose a weighted least squares (WLS)-filter-based method in this letter. Different from existing IHS-based methods, the proposed method eliminates the influence of the low-frequency components of the Pan and MS images with the WLS filter. Moreover, the derived detail map is further refined based on the spectral signatures for different bands of the MS image. We test the proposed method on various satellites data; the experimental results demonstrate that the proposed method performs well in both spectral and spatial qualities.	distortion;ground truth;image fusion;least mean squares filter;least squares;log-spectral distance;multispectral image;type signature	Xiaomin Yang;Wei Wu;Zheng Liu;Xiaomin Yang;Kai Liu;Wei wei Lu	2016	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2015.2492569	computer vision;image resolution;distortion;computer science;optics;image fusion;physics;satellite;remote sensing	Vision	67.43109343214887	-66.1204878071785	186115
260e1f3618f07e43e2562e989124a843fbeea85d	image quality analysis of a novel histogram equalization method for image contrast enhancement	visual inspection;image quality;histogram equalization;digital image	The use of image contrast enhancement has become increasingly essential due to the need to better show the visual information contained within the image for all vision-based systems. This has lead to motivation for the design of a powerful and accurate automatic contrast enhancement for a digital image. Histogram equalization is the most commonly used contrast enhancement method. However, the conventional histogram equalization methods usually result in excessive contrast enhancement, which causes the unnatural look and visual artifacts of the processed image. In this paper, we propose a novel histogram equalization method using the automatic histogram separation along with the piecewise transformed function. The contrast enhancement results of the proposed method were not only analyzed through qualitative visual inspection and for quantitative accuracy, but are also compared to the results of other state-of-the-art methods.	histogram equalization;image quality	Fan-Chieh Cheng;Shanq-Jang Ruan	2010	IEICE Transactions		image quality;computer vision;color normalization;shadow and highlight enhancement;computer science;histogram matching;balanced histogram thresholding;multimedia;adaptive histogram equalization;histogram equalization;digital image;image histogram;computer graphics (images);visual inspection	Vision	58.63851835893636	-61.51478028411493	186158
b80d2d567bf23ceff19d711f9676ee35a5dec480	natural scene statistics for noise estimation		We investigate the scale-invariant properties of divisively normalized bandpass responses of natural images in the DCT-filtered domain. We found that the variance of the normalized DCT filtered responses of a pristine natural image is scale invariant. This scale invariance property does not hold in the presence of noise and thus it can be used to devise an efficient blind image noise estimator. The proposed noise estimation approach outperforms other statistics-based methods especially for higher noise levels and competes well with patch-based and filter-based approaches. Moreover, the new variance estimation approach is also effective in the case of non-Gaussian noise. The research code of the proposed algorithm can be found at https://github.com/guptapraful/Noise Estimation.	algorithm;discrete cosine transform;filtered-popping recursive transition network;image noise;kernel density estimation;noise shaping;scene statistics	Praful Gupta;Christos George Bampis;Yize Jin;Alan C. Bovik	2018	2018 IEEE Southwest Symposium on Image Analysis and Interpretation (SSIAI)	10.1109/SSIAI.2018.8470313	image noise;computer vision;estimator;pattern recognition;artificial intelligence;additive white gaussian noise;computer science;discrete cosine transform;normalization (statistics);scale invariance;band-pass filter;scene statistics	Vision	54.21843518479786	-65.18666380636512	186273
0e7b582861fa33602801cec66d0908030f47249a	reflection reprojection using temporal coherence		A powerful approach for rendering high-quality images at low cost is to exploit temporal coherence by projecting already computed images into a novel view. However, conventional temporal coherence projection methods assume pixel values remain almost unchanged from frame to frame, which does not extend well to reflection rendering. We present a novel projection method to reuse reflections from adjacent frames. A novel reflection reprojection method is introduced to establish the mapping of reflections between individual frames. By reusing the information from the reference frame, our method can reduce the overall workloads of reflection computation, which makes rendering efficiently.	coherence (physics);computation;diffuse reflection;map projection;pixel;reference frame (video);reflection (computer graphics);virtual reality headset	Naiwen Xie;Lili Wang;Philip Dutré	2017	The Visual Computer	10.1007/s00371-017-1358-9	pixel;artificial intelligence;3d rendering;rendering (computer graphics);computer vision;projection method;reference frame;computation;computer science;specular reflection;coherence (physics)	Graphics	59.84848453686713	-53.63099742736278	186472
ea5a28097126c3537346e9dd815d8b372f97a80a	spatio-temporal attribute morphology filters for noise reduction in image sequences	image sequence video compression noise reduction sliding window spatio temporal attribute morphology filter peak signal to noise ratio psnr;video compression;visual quality;temporal filtering;video coding;low pass filters video coding image sequences spatiotemporal phenomena;noise reduction;peak signal to noise ratio;image sequence;spatiotemporal phenomena;low pass filters;morphology noise reduction image sequences psnr image coding filtering low pass filters video compression degradation noise generators;sliding window;image sequences	In video compression image noise degrades the overall compression performance. Reducing the noise content can produce a more compressible sequence, ideally with minimal effect on the visual quality. This paper presents a new approach to noise reduction using sliding window spatio-temporal attribute morphology filters with both area and power attributes. A combined spatial and spatio-temporal algorithm is proposed that produces a better overall peak signal to noise ratio (PSNR) performance than either spatial or spatio-temporal filtering individually. The compression benefits of such an approach are also demonstrated.	galaxy morphological classification;noise reduction	Nata Young;Adrian N. Evans	2003		10.1109/ICIP.2003.1246966	data compression;gradient noise;gaussian noise;sliding window protocol;median filter;image noise;computer vision;dark-frame subtraction;peak signal-to-noise ratio;low-pass filter;computer science;noise;pattern recognition;noise reduction;mathematics;matched filter;salt-and-pepper noise	Vision	57.81686069909924	-63.47146888361507	186477
c84bf26d73ab6ed258db8b33e87e0cd291c89c43	rendering antialiased shadows with depth maps	shadow algorithm;soft shadow;performance analysis;depth map	We present a solution to the aliasing problem for shadow algorithms that use depth maps. The solution is based on a new filtering technique called percentage closer filtering. In addition to antialiasing, the improved algorithm provides soft shadow boundaries that resemble penumbrae. We describe the new algorithm in detail, demonstrate the effects of its parameters, and analyze its performance.	algorithm;aliasing;depth map;spatial anti-aliasing	William T. Reeves;David Salesin;Robert L. Cook	1987		10.1145/37401.37435	computer vision;shadow mapping;shadow volume;depth map;computer graphics (images)	Graphics	65.2235148991137	-52.74308823179241	186611
fffb53e84dd47049f5d2b5baa9dbfeeffc1d4a02	implementation and comparative quantitative assessment of different multispectral image pansharpening approches		In remote sensing, images acquired by various earth observation satellites tend to have either a high spatial and low spectral resolution or vice versa. Pansharpening is a technique which aims to improve spatial resolution of multispectral image. The challenges involve in the pansharpening are not only to improve the spatial resolution but also to preserve spectral quality of the multispectral image. In this paper, various pansharpening algorithms are discussed and classified based on approaches they have adopted. Using MATLAB image processing toolbox, several state-of-art pan-sharpening algorithms are implemented. Quality of pansharpened images are assessed visually and quantitatively. Correlation coefficient (CC), Root mean square error (RMSE), Relative average spectral error (RASE) and Universal quality index (Q) indices are used to measure spectral quality while to spatial-CC (SCC) quantitative parameter is used for spatial quality measurement. Finally, the paper is concluded with useful remarks.	algorithm;coefficient;distortion;fast fourier transform;image processing;matlab;mean squared error;multispectral image;principal component analysis;resultant	Shailesh Panchal;Rajesh V Thakker	2015	CoRR	10.5121/sipij.2015.6503	computer vision	Vision	65.84612532765203	-66.06358219901979	186808
6ed8599e7edd651fbaf5ee2e36f79953190781e4	thermal light longitudinal correlated imaging with random orthogonal matching pursuit algorithm		A thermal light correlated longitudinal imaging experiment is proposed. The quasi-thermal light beam is split into two beams, a test beam and a reference beam, respectively. The light in the test beam is scattered by two amplitude objects with a specific longitudinal distance between them, while the light of the reference beam travels uninterrupted. At the end of the test and reference beams, two charge-coupled detectors (CCDs) are used to measure the intensity of the optical field. Through intensity correlation measurement the images of the two detected objects can be achieved simultaneously, only if the distance between the objects is less than the longitudinal coherent length. The theoretical analysis shows that the longitudinal coherent length is determined by both the transverse size of the incoherent thermal light source and the length of the optical path. The quality of the correlated images of the two objects is improved greatly by making use of the orthogonal matching pursuit (OMP) and the propos...		Lu Gao;Ke Xiao;Hanquan Song;Xiaoman Qi	2018	IJPRAI	10.1142/S0218001418540307	pattern recognition;artificial intelligence;mathematics;beam (structure);amplitude;reference beam;light beam;optical field;detector;optical path;optics;ghost imaging	Vision	68.27126639503808	-60.63652825746387	187289
337ce27b34684674bae2dfa6ef44255aa4cee35d	a multiresolution wavelet scheme for irregularly subdivided 3d triangular mesh	object recognition;lossless compression multiresolution wavelet scheme irregularly subdivided 3d triangular mesh regular 1 4 face split irregularly subdivided triangular meshes wavelet transforms real medical meshes multiresolution schemes;computational geometry;lossless compression;triangular mesh;wavelet transforms;computational geometry wavelet transforms object recognition mesh generation;wavelet transform;subdivision scheme;mesh generation;multiresolution analysis wavelet analysis surface waves filters wavelet transforms acceleration data compression proposals surface fitting surface reconstruction	We propose a new subdivision scheme derived from the Lounsbery’s regular 1:4 face split, allowing multiresolution analysis of irregularly subdivided triangular meshes by the wavelet transforms. Some experimental results on real medical meshes prove the efficiency of this approach in multiresolution schemes. In addition we show the effectiveness of the proposed algorithm for lossless compression.	algorithm;lossless compression;multiresolution analysis;polygon mesh;scale (map);subdivision surface;triangulated irregular network;wavelet transform	Sébastien Valette;Yun-Sang Kim;Ho-Youl Jung;Isabelle E. Magnin;Rémy Prost	1999		10.1109/ICIP.1999.821589	multiresolution analysis;wavelet;mathematical optimization;discrete mathematics;second-generation wavelet transform;computational geometry;mathematics;geometry;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;wavelet transform	Visualization	67.32685239321529	-55.73950390180501	187299
0efb94f123ba9d7608f94122f9122bce2a4273f1	ccd color camera characterization for image measurements	charge coupled image sensors;dark current;image processing;charge coupled devices charge coupled image sensors digital cameras additive noise dark current image color analysis gain measurement image processing application software optical noise;generic model;additive noise;video cameras ccd image sensors image processing noise measurement;video cameras charge coupled device ccd charge coupled image sensors digital imaging sensors gain measurement image processing noise measurement;universiteitsbibliotheek;noise measurement;ccd image sensors;multiplicative noise;video cameras;normal operator;charged couple device;charge coupled device ccd;digital image;gain measurement;gamma correction;digital imaging sensors;image sensor;noise measurement charge coupled image sensors digital imaging sensors image processing video cameras gamma correction dark current additive noise multiplicative noise	In this article, we will analyze a range of different types of cameras for its use in measurements. We verify a general model of a charged coupled device camera using experiments. This model includes gain and offset, additive and multiplicative noise, and gamma correction. It is shown that for several cameras, except a typical consumer webcam, the general model holds. The associated model parameters are estimated. It is shown that for most cameras the model can be simplified under normal operating conditions by neglecting the dark current. We further show that the amount of additive noise is exceeded by the amount of multiplicative noise at intensity values larger than 10%-30% of the intensity range	additive white gaussian noise;charge-coupled device;dark current (physics);experiment;gamma correction;multiplicative noise;utility functions on indivisible goods;webcam	Paul J. Withagen;Frans C. A. Groen;Klamer Schutte	2007	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2006.887667	dark current;computer vision;electronic engineering;image processing;noise measurement;gamma correction;multiplicative noise;image sensor;optics;charge-coupled device;normal operator;digital image;physics	Vision	61.46438820838764	-58.362571696892466	187508
7e22c3b3f9bd7b0f3b5c414fbdb383c9754e907b	a fuzzy scheme for image noise reduction		The improvement of acquisition devices increases the need for processing of multicomponent images. In this context, the noise reduction is a preliminary preprocessing step affecting the results of the other image operations. This paper proposes a framework explaining usual noise reduction methods by the means of two fuzzy logic techniques: first a pixel fuzzification and second a defuzzification for estimating the filtered values. A new density-based filter is built for removing both impulse noise and Gaussian noise. The filter we propose is robust against outliers and it improves the classical bilateral approach for noise reduction of multicomponent images.	bilateral filter;defuzzification;fuzzy logic;fuzzy set;high- and low-level;high-level programming language;image processing;impulse noise (audio);noise reduction;pixel;preprocessor;the filter;theory of computation	Philippe Vautrot;Michel Herbin;Laurent Hussenet	2011			median filter	ML	55.784645765791154	-65.09572011324838	187569
e0860e33deceba37eec7cbe6326137c6c214d5e4	image de-hazing from the perspective of noise filtering		Digital images captured in outdoor environment are easily polluted by haze, which will degrade the conveyed information. To overcome this problem, a large number of researches have been conducted for image haze removal, among which the approach based on the dark channel prior assumption is in recent years considered as the state-of-the-art. This method is primarily dependent on consolidation of observations. However, in the proposed method, a theoretical perspective is adopted for image, which considers the degraded image as a product contaminated by noise. Two maps are constructed to label the noise severity and atmospheric light. The parameters involved are optimized via Particle Swarm Optimization with a penalty function in terms of hue change. Experimental results were compared with seven available approaches, along with an analysis on algorithm complexity. These analyses verify the effectiveness, efficiency, wide adaptability and theoretical soundness of the proposed approach. © 2016 Elsevier Ltd. All rights reserved.	algorithm;map;mathematical optimization;particle swarm optimization;penalty method;pixel;real-time clock;resultant;semiconductor consolidation;weight function	Shilong Liu;Md. Arifur Rahman;San Chi Liu;Chin Yeow Wong;Stephen Ching-Feng Lin;Hongkun Wu;Ngai Ming Kwok	2017	Computers & Electrical Engineering	10.1016/j.compeleceng.2016.11.021	adaptability;penalty method;digital image;haze;computer vision;filter (signal processing);real-time computing;computer science;simulation;soundness;artificial intelligence;particle swarm optimization;communication channel	AI	58.788780984657855	-62.74985014748003	187858
24e26b69ee4e5d2fd79fbbd67979bdd0b8a17b9c	recovering face shape and reflectance properties from single images	real world imagery face shape recovery facial surface reflectance properties single image surface reflectance function curve fitting statistical face shape constraint model based integrability shape estimation synthetic imagery;shape estimation;facial surface reflectance properties;statistical analysis curve fitting face recognition shape recognition;shape constraints;real world imagery;synthetic imagery;surface reflectance function;statistical face shape constraint;skin;reflectivity surface fitting skin cameras shape measurement photometry light scattering noise shaping facial features pixel;shape recognition;face shape recovery;noise measurement;surface treatment;model based integrability;face recognition;shape;statistical analysis;pixel;property a;single image;face;curve fitting;light sources	In this paper we show how to estimate facial surface reflectance properties (a slice of the BRDF and the albedo) in conjunction with facial shape from a single image. We show how an estimate of the surface reflectance function can be made by fitting a curve to the scattered and noisy reflectance samples provided by the estimated shape. We present a novel statistical face shape constraint which we term dasiamodel-based integrabilitypsila which we enforce on the field of surface normals. We iteratively interleave the two processes of estimating reflectance properties based on the current shape estimate and updating the shape estimate based on the current estimate of the reflectance function. We show that the method is capable of recovering accurate shape and reflectance information from single images using both synthetic and real world imagery.	algorithm;autostereogram;bidirectional reflectance distribution function;biometrics;cobham's thesis;curve fitting;ground truth;interleaved memory;normal (geometry);photometric stereo;rendering (computer graphics);shading;statistical classification;synthetic intelligence	William A. P. Smith;Edwin R. Hancock	2008	2008 8th IEEE International Conference on Automatic Face & Gesture Recognition	10.1109/AFGR.2008.4813385	computer vision;geography;optics;remote sensing	Vision	55.40718948063603	-52.15281109174645	188227
10563dfa54f423aa576b19e2467b5da893b23e44	real-time depth-of-field rendering using single-layer composition	depth of field;gpu;single layer composition;post processing	In this paper, we propose a single-layer post-processing method for real-time depth-of-field rendering that uses single-layer composition. In the proposed method, blurring is achieved by gathering background pixels and scattering foreground pixels. Major artifacts in post-filtering techniques such as intensity leakage and blurring discontinuity are reduced by using two different blurring functions and the controllable parameter in the gathering process. The method can be entirely implemented in GPU parallelization to achieve the real-time performance required for virtual reality. The results of comparisons of our method with recent post-processing methods in terms of rendering quality and rendering performance indicate that our method generates realistic natural images and is also the fastest in terms of frames per second. Copyright © 2014 John Wiley & Sons, Ltd.	academy;fastest;graphics processing unit;interactivity;john d. wiley;mathematical optimization;parallel computing;pattern recognition;pixel;real-time clock;real-time transcription;reflections of signals on conducting lines;simulation;spectral leakage;video post-processing;virtual reality	Xiaoxin Fang;Bin Sheng;Wen Wu;Zengzhi Fan;Lizhuang Ma	2014	Journal of Visualization and Computer Animation	10.1002/cav.1591	computer vision;simulation;3d rendering;rendering;computer science;depth of field;multimedia;real-time rendering;video post-processing;computer graphics (images)	Visualization	65.9788695837948	-53.84850323416145	188501
3e1c7a9cfab8224940e2badaefd2041657141ddc	super-interpolation with edge-orientation-based mapping kernels for low complex 2× upscaling	silicon;biological patents;interpolation;biomedical journals;image resolution;text mining;europe pubmed central;training;citation search;citation networks;research articles;image edge detection;abstracts;image reconstruction;open access;dictionaries;life sciences;clinical guidelines;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	With the advent of ultrahigh-definition (UHD) video services, super-resolution (SR) techniques are often required to generate high-resolution (HR) images from low-resolution (LR) images, such as HD images. To generate such HR images and a video of UHD resolutions in limited computing devices with hardware and software, low complex but excellent SR methods are particularly required. In this paper, we present a novel and fast SR method, called super-interpolation (SI), by unifying an interpolation step and a quality-enhancement step. The proposed SI method utilizes edge-orientation (EO)-based pre-learned kernels, which inherits the simplicity of interpolation and the quality enhancement of SR. It performs SR directly from the initial resolution of an input image to the target resolution of an up-scaled output image without requiring any intermediate interpolated image. The proposed SI method involves offline training and online up-scaling phases. In the offline training phase, training LR image patches are clustered based on their edge orientations into different EO classes for which class-dependent linear mapping functions are learned between training LR and HR image patches. In up-scaling phase, an HR output image patch for each LR input image patch is generated by applying an appropriate linear mapping function selected based on the EO of LR input image patch. Our proposed SI method is intensively compared with the ten state-of-the-art SR methods for common image sets and many HD/UHD images. The experimental results show that the SI method yields the smallest running time and requires relatively small hardware resources. It outperforms the six state-of-the-art methods in average (peak signal-to-noise ratio) PSNR/(structural similarity) SSIM, and exhibits competitive or somewhat lower PSNR/SSIM performance compared with the others.		Jae-Seok Choi;Munchurl Kim	2016	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/TIP.2015.2507402	iterative reconstruction;computer vision;text mining;image resolution;interpolation;computer science;theoretical computer science;machine learning;data mining;mathematics;multimedia;silicon;algorithm;statistics	Vision	59.26734617060057	-65.84690306371873	188522
29cd3648588b2a836d3002e5bdbf3eec2d67f0ed	cloud model-based method for range-constrained thresholding		Display Omitted A cloud model-based framework for range-constrained thresholding with uncertainty.Improving four traditional methods under the new framework.Representing the image using cloud model.Implementing image transformation to focus on mid-region of the image.Cloud model-based framework is efficient and effective. Thresholding is a popular image segmentation method that converts a grayscale image into a binary image. In this paper, we propose a cloud model-based framework for range-constrained thresholding with uncertainty, and improve four traditional methods. The method involves four major steps, including representing the image using cloud model, estimating the automatic threshold for gray level ranges of object and background, implementing image transformation to focus on mid-region of the image, and determining the binary threshold within the constrained gray level range. Cloud model can effectively represent various visual properties of the image, such as intensity-based class uncertainty, intra-class homogeneity, and between-class contrast. The approach is validated both quantitatively and qualitatively. Compared with the traditional state-of-art algorithms on a variety of synthetic and real images, with or without noisy, as well as laser cladding images, the experimental results suggest that the presented method is efficient and effective.	thresholding (image processing)	Tao Wu;Jin Xiao;Kun Qin;Yixiang Chen	2015	Computers & Electrical Engineering	10.1016/j.compeleceng.2014.03.016	computer vision;binary image;balanced histogram thresholding;data mining;thresholding;computer graphics (images)	AI	58.28239923909381	-62.469483758687126	188599
c104ac573ff5ef50386d84eb046640fd3f05da12	impulse noise replacement with adaptive neighborhood median filtering		This paper presents an impulse noise replacement scheme based on an adaptive neighborhood median filtering (ANMF). Note that the window size used in the filtering process affects the contrast and smoothness in restored images. That is, larger windows applied in the filtering process result in a stronger smoothing effect and less contrast in restored images. On the other hand, a smaller window leads to a better contrast and less smoothness in restored images. Thus, three adaptive window expansion criteria are employed in the proposed ANMF schemes such that smaller windows are used in the replacement of noisy pixels. To justify the proposed ANFM schemes, three images are given where the salt and pepper noise with various densities are under study. The results indicate that the proposed ANFM schemes have better visual quality of restored images than those by [12], even though less peak signal-to-noise ratio is for the proposed ANFM in some cases of higher noise densities.	impulse noise (audio);median filter;microsoft windows;national supercomputer centre in sweden;peak signal-to-noise ratio;pixel;romp;salt (cryptography);salt-and-pepper noise;simulation;smoothing	Cheng-Hsiung Hsieh;Po-Chin Huang;Qiangfu Zhao	2018	2018 International Conference on Machine Learning and Cybernetics (ICMLC)	10.1109/ICMLC.2018.8527058	image restoration;salt-and-pepper noise;artificial intelligence;smoothing;noise measurement;filter (signal processing);median filter;impulse noise;pattern recognition;computer science;adaptive filter	Vision	57.19193407050391	-65.8131627172577	188791
54eb11e1ceeb31c0d9538c966c5f35d78a9a0f1b	spatio-temporal segmentation with mumford-shah functional	mumford shah functional;spatio temporal segmentation regions;image segmentation;video domain hierarchy;video signal processing;video signal processing image segmentation transforms;video sequences;2d shapes;spatiotemporal grouping complexity;motion segmentation;shape;sift;2d time extension;streaming media;scale invariant feature transform;2d regions;transforms;merging;optimization;spatio temporal segmentation regions 2d regions spatiotemporal grouping complexity video segmentation methods volumetric approach 2d time extension multiscale energy video domain hierarchy scale invariant feature transform sift 2d shapes image segmentation mumford shah functional;multiscale energy;image segmentation motion segmentation streaming media optimization merging shape video sequences;video segmentation methods;volumetric approach	Image segmentation is intended to group perceptually similar pixels into 2D regions, and the corresponding border is gained at the same time. Video segmentation generalizes this concept to the grouping of pixels into spatio-temporal regions that exhibit coherence in both appearance and motion, but this generalization pose the complexity of spatio-temporal grouping, and in order to overcome this complexity, the existing video segmentation methods have extended the image segmentation methods to 3D domain. In these volumetric approaches, it is not known a priori, which regions to track, what frames contain those regions, or the time-direction for tracking (forward or backward). In this paper we present an efficient and scalable method for spatio-temporal segmentation obtained by minimizing a 2D+time extension of the simplified Mumford-Shah functional. The 2D+time extension permits to write the Mumford-Shah functional as an a multiscale energy, which is minimized on a hierarchy of video domain. The construction of this hierarchy based on the 2D-shapes of video images, and Scale-invariant feature transform (SIFT).	image segmentation;mumford–shah functional;pixel;scalability;scale-invariant feature transform	Mohamed El Aallaoui;Abdelwahad Gourch	2013	2013 ACS International Conference on Computer Systems and Applications (AICCSA)	10.1109/AICCSA.2013.6616501	computer vision;range segmentation;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;computer graphics (images)	Vision	55.34680927073429	-56.154949448313985	188855
99a77687036a33b1d1e3f04180abf2d84c189bcb	enhancement of human visual perception-based image quality analyzer for assessment of contrast enhancement methods		Absolute Mean Brightness Error (AMBE) and entropy are two popular Image Quality Analyzer (IQA) metrics used for assessment of Histogram Equalization (HE)-based contrast enhancement methods. However, recent study shows that they have poor correlation with Human Visual Perception (HVP); Pearson Correlation Coefficient (PCC)<0.4. This paper, proposed a new IQA which takes into account important properties of HVP with respect to luminance, texture and scale. evaluation results show that the proposed IQA has significantly improved performance (PCC>0.9). It outperforms all IQAs in study, including two prominent IQAs designed for assessment of image fidelity in image coding-multi-scale structural similarity and information fidelity criterion.	ansi escape code;coefficient;compiler;histogram equalization;image quality;industry foundation classes;multi-band excitation;real-time clock;real-time computing;recommender system;structural similarity	Soong-Der Chen	2016	Int. Arab J. Inf. Technol.			HCI	61.834794943282226	-64.47863108671645	189223
0adcbef76ef8b30e917e166dfbfc7c7cc62c722e	novel image polarization method for measurement of lens decentration	lenses plastics optical imaging optical polarization adaptive optics residual stresses image color analysis;tilt;linear polarizers;light polarisation image processing lenses;residual stress;image processing;lens decentration;image polarization method;decenter;residual stresses;plastics;optical imaging;optical polarization;image color analysis;image of polarization;lenses;polarized beam image polarization method lens decentration tilt measurement linear polarizers;light polarisation;polarized beam;tilt decenter image of polarization image processing;tilt measurement;adaptive optics	Tilt and decenter errors in lenses are usually caused by lens manufacturing processes: either injection or glass molding. This paper presents an image polarization method for measuring tilting and decentering of lenses. We placed a sample lens between two linear polarizers and passed either a focused or a divergent polarized beam through a half-wave or a full-wave plate. If any error were present in the shape of the lens, patterns and colors shown in the image of polarization would be altered. Finally, the degree of error in the shape of the lens can be computed using image processing software.	color;image processing;photographic plate;polarization (waves);polarizer;waveplate	Kuo-Cheng Huang;Chun-Li Chang;Wen-Hong Wu	2011	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2011.2108070	computer vision;lens speed;simple lens;image processing;residual stress;lens;lens flare;optics;soft focus;cylindrical lens;physics	Visualization	60.267329242990805	-53.31670146165516	189301
dd6c8d118f9542570a3374dd62a1be5541709e1b	liquid crystal image analysis by image descriptors	liquid crystal image descriptors;image color analysis visualization liquid crystals feature extraction histograms layout image sequences	Liquid crystals are substances with high impact technological, new substances have been discovered and the properties of these materials need to be examined. When viewed under a microscope using a polarized light source, different liquid crystal phases will appear to have distinct textures and colors. The use of digital image processing and computer vision is being initialized in the analysis of these materials. The goal of this work is to propose methods, based on visual descriptors, which are able to identify phase transitions and classify phases in liquid crystals from a sequence of images.	color;computer vision;digital image processing;image analysis;polarization (waves);visual descriptor	Guilherme Enoc Egas de Carvalho;Franklin César Flores;Fernando Carlos Messias Freire;Anderson Reginaldo Sampaio	2014	2014 International Conference on Computer Vision Theory and Applications (VISAPP)	10.5220/0004695805310537	image texture;computer vision;color image;binary image;computer graphics (images)	Vision	61.9294816729919	-56.692444753034096	189304
e321fb183c4d632e434f3974a651b6afc4cb4e2d	geometric encoding, filtering, and visualization of genomic sequences		This article describes a three-channel encoding of nucleotide sequences, and proper formulas for filtering and downsampling such encoded sequences for multi-scale signal analysis. With proper interpolation, the encoded sequences can be visualized as curves in three-dimensional space. The filtering uses Gaussian-like smoothing kernels, chosen so that all levels of the multi-scale pyramid (except the original curve) are practically free from aliasing artifacts and have the same degree of smoothing. With these precautions, the overall shape of the space curve is robust under small changes in the DNA sequence, such as single-point mutations, insertions, deletions, and shifts.	aliasing;chroma subsampling;decimation (signal processing);filter (signal processing);interpolation;signal processing;smoothing;spatial anti-aliasing	Helena Cristina da Gama Leitão;Rafael Felipe V. Saracchini;Jorge Stolfi	2015		10.5220/0005297102190224	encoding (memory);data mining;filter (signal processing);visualization;computer science;bioinformatics	Comp.	66.55717053198715	-56.05864860190812	189306
ab9ee43e9a016500b38a3945e3bab283ba1bdfb6	motion smoothing strategies for 2d video stabilization		Video stabilization aims at removing the undesirable effects of camera motion by estimating its shake and applying a smoothing compensation. This paper proposes a unified mathematical analysis and classification of existing smoothing strategies. We assume that the apparent velocity induced by the camera is estimated as a set of global parametric models, typically those of a homography. We classify the existing smoothing strategies into compositional and additive methods and discuss their technical issues, particularly the definition of the boundary conditions. Our discussion of the various alternatives leads to clear-cut conclusions. It rules out the global compositional methods in favor of local linear methods and finds the adequate boundary conditions. We also show that the best smoothing strategy yields a scale-space analysis of the camera ego-motion parameters. Analyzing this scale-space on examples, we show how it is highly characteristic of the camera path, permitting us to compute ego-motion freque...	smoothing	Javier Sánchez;Jean-Michel Morel	2018	SIAM J. Imaging Sciences	10.1137/17M1127156	mathematics;shake;mathematical optimization;parametric model;boundary value problem;motion compensation;image stabilization;homography;smoothing	Theory	55.66233958774538	-53.764751530583105	189603
262f2258567e3543a4625b079e52832872b886e5	beyond the neutral interface reflection assumption in illuminant color estimation	incident illumination;fresnel effect;specularity based illuminant color estimation;neutral interface reflection;minerals;systematic error;color;material dependent rectification;illuminant color estimation;image colour analysis computer vision;color shift;image color analysis estimation petroleum epidermis color minerals;computer vision;epidermis;illumination estimation neutral interface reflection fresnel reflectance cook torrance reflection model;petroleum;estimation;image color analysis;color shift neutral interface reflection assumption illuminant color estimation computer vision incident illumination fresnel reflectance fresnel effect human skin specularity based illuminant color estimation material dependent rectification;image colour analysis;fresnel reflectance;reflection model;cook torrance reflection model;human skin;illumination estimation;neutral interface reflection assumption	The neutral interface reflection (NIR) assumption is a widely accepted theory in computer vision. According to the NIR the color of specularities of dielectric materials is the color of the incident illumination and the influence of the Fresnel reflectance is neglected. We show, that there is a material- and geometry-dependent shift between the color of the specularity and the color of the incident light due to the Fresnel effect which for human skin can be up to approximately 5.8%. As the NIR concept is often the core idea of specularity-based illuminant-color estimation techniques, the ignored Fresnel effect introduces a systematic error in the estimation result. We thus propose a material-dependent rectification method for correcting this color shift. Our experiments on human skin regions show an average improvement of the illuminant color estimation of about 30%.	computer vision;dependent ml;experiment;ray (optics);rectifier;specularity	Eva Eibenberger;Elli Angelopoulou	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5653890	computer vision;estimation;fresnel equations;systematic error;mathematics;petroleum;epidermis;statistics;computer graphics (images)	Vision	59.994700885354305	-52.717934473550514	189631
11bb945a70510ac5046c5719fc5cb32bf56e8e8f	diffusion based photon mapping	i 3 7 computer graphics three dimensional graphics and realism;diffusion filtering;density estimation;global illumination;caustics;ray tracing;photon mapping	Density estimation employed in multi-pass global illumination algorithms give cause to a trade-off problem between bias and noise. The problem is seen most evident as blurring of strong illumination features. In particular this blurring erodes fine structures and sharp lines prominent in caustics. To address this problem we introduce a novel photon mapping algorithm based on nonlinear anisotropic diffusion. Our algorithm adapts according to the structure of the photon map such that smoothing occurs along edges and structures and not across. In this way we preserve the important illumination features, while eliminating noise. We call our method diffusion based photon mapping.	algorithm;anisotropic diffusion;global illumination;image noise;nonlinear system;photon mapping;smoothing	Lars Schjøth;Ole Fogh Olsen;Jon Sporring	2006	Comput. Graph. Forum	10.1111/j.1467-8659.2008.01196.x	ray tracing;computer vision;density estimation;computer science;photon mapping;beam tracing;global illumination;caustic;statistics;computer graphics (images)	Graphics	64.0668003037678	-53.69679267839558	189782
027505d1b6ab6d3af7be4caf4a07e21618e6c289	fuzzy edge detector using entropy optimization	detectors;phase detection;contrast intensification operator entropy optimization gray level images global contrast intensification fuzzy edge detection gaussian membership function intensification parameter fuzzifier crossover point image enhancement gradient descent function fuzzy image processing;optimisation;fuzzy edge detection;image processing;fuzzifier;gaussian processes;edge detection;edge detector;optimal method;testing;gray level images;fuzzy set theory;fuzzy sets;computer vision;fuzzy logic;image enhancement;gradient descent function;fuzzy image processing;image edge detection;gradient descent;detectors entropy image edge detection fuzzy sets image processing phase detection optimization methods testing image enhancement computer vision;fuzzy set theory edge detection image enhancement gaussian processes entropy optimisation fuzzy logic;membership function;entropy optimization;generalized gaussian;gaussian membership function;contrast intensification operator;entropy;article;global contrast intensification;intensification parameter;optimization methods;crossover point	This paper proposes a fuzzy-based approach to edge detection in gray-level images. The proposed fuzzy edge detector involves two phases - global contrast intensification and local fuzzy edge detection. In the first phase, a modified Gaussian membership function is chosen to represent each pixel in the fuzzy plane. A global contrast intensification operator, containing three parameters, viz., intensification parameter t, fuzzifier f/sub h/ and the crossover point x/sub c/, is used to enhance the image. The entropy function is optimized to obtain the parameters f/sub h/, and x/sub c/ using the gradient descent function before applying the local edge operator in the second phase. The local edge operator is a generalized Gaussian function containing two exponential parameters, /spl alpha/ and /spl beta/. These parameters are obtained by the similar entropy optimization method. By using the proposed technique, a marked visible improvement in the important edges is observed on various test images over common edge detectors.	algorithm;canny edge detector;distortion;edge detection;facial recognition system;fingerprint;gradient descent;grayscale;mathematical optimization;pixel;sensor;thresholding (image processing);time complexity;viz: the computer game	Madasu Hanmandlu;John See;Shantaram Vasikarla	2004	International Conference on Information Technology: Coding and Computing, 2004. Proceedings. ITCC 2004.	10.1109/ITCC.2004.1286542	mathematical optimization;machine learning;pattern recognition;mathematics	Robotics	55.99118265354266	-64.42160739696656	189862
a5e812f2c66aee925df257e1c8b21e78cacca9f7	image deblurring	digital image processing;additional correlated image;align image pair;guide image;noisy image pair;multiple image;degraded image;high-quality image;image deblurring;single image deblurring	Multimedia is ubiquitous and the application of digital imaging is prolific, yet environmental conditions and hard ware limitations may adversely affect image quality. Advanced techniques such as image enhancement, deblurring, denoise, and super resolution have been developed to improve image quality post-digitization. Image enhancement is primarily concerned with problems caused by overexposure, underexposure, poor photographic technique, and optical noise. A. Gorai and A. Ghosh proposed a method based on a heuristic algorithm to enhance images by adjusting brightness and contrast. Lighting problems are resolved effectively through this method, yet there are limitations when applied to blurred images, i.e., images with defocus blur, motion blur, handshake blur, or fog blur. For those kinds of blurred images, we need a specific technique of image deblurring to apply. As a result, this research is primarily concerned with (a) image enhancement: improving upon the objective function and transformation function proposed by A. Gorai and A. Ghosh and (b) image deblurring: a proposed method to estimate blur kernel and its application towards image deblurring. This paper proposes a blind deblurring method needing to predict a blur kernel in our own way. The color distribution of edge is more distinct in a clear image than in a blurred image. A filter is proposed to make edges in a blurred image clearer for use as a reference image. The blur kernel is estimated from this reference image. The blurred image is then deconvolved with the estimated blur kernel to introduce a latent image.	algorithm;deblurring;deconvolution;digital imaging;emoticon;gaussian blur;heuristic (computer science);image editing;image quality;kernel (operating system);latent image;loss function;noise reduction;optimization problem;super-resolution imaging;warez	Fu-Wen Yang;Hwei-Jen Lin;Hua Chuang	2017	2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)	10.1109/UIC-ATC.2017.8397472	distributed computing;exposure;motion blur;computer science;image restoration;deconvolution;brightness;image quality;computer vision;digital imaging;deblurring;artificial intelligence	Vision	57.28198271392324	-58.5223573242846	189915
631f6ffe6e8cb68ecda0fdbf7e5e9bd98507f05a	sensitivity and resolution improvement in rgbw color filter array sensor	color filter array;demosaicing;red-green-blue-white (rgbw);resolution improvement;sensitivity improvement;texture decomposition	Recently, several red-green-blue-white (RGBW) color filter arrays (CFAs), which include highly sensitive W pixels, have been proposed. However, RGBW CFA patterns suffer from spatial resolution degradation owing to the sensor composition having more color components than the Bayer CFA pattern. RGBW CFA demosaicing methods reconstruct resolution using the correlation between white (W) pixels and pixels of other colors, which does not improve the red-green-blue (RGB) channel sensitivity to the W channel level. In this paper, we thus propose a demosaiced image post-processing method to improve the RGBW CFA sensitivity and resolution. The proposed method decomposes texture components containing image noise and resolution information. The RGB channel sensitivity and resolution are improved through updating the W channel texture component with those of RGB channels. For this process, a cross multilateral filter (CMF) is proposed. It decomposes the smoothness component from the texture component using color difference information and distinguishes color components through that information. Moreover, it decomposes texture components, luminance noise, color noise, and color aliasing artifacts from the demosaiced images. Finally, by updating the texture of the RGB channels with the W channel texture components, the proposed algorithm improves the sensitivity and resolution. Results show that the proposed method is effective, while maintaining W pixel resolution characteristics and improving sensitivity from the signal-to-noise ratio value by approximately 4.5 dB.	algorithm;aliasing;bayer filter;brilliant blue fcf;cell fusing agent virus;color filter array;demosaicing;elegant degradation;image noise;image resolution;list of content management frameworks;morphologic artifacts;pixel;signal-to-noise ratio;vena cava filters;video post-processing;xqd card;cyclophosphamide/fluorouracil/methotrexate protocol	Seung Hoon Jee;Ki-Sun Song;Moon Gi Kang	2018		10.3390/s18051647	engineering;electronic engineering;pixel;rgb color model;color filter array;computer vision;demosaicing;color difference;color gel;colors of noise;image resolution;artificial intelligence	Robotics	59.13202962316267	-60.389080724926075	190019
df8b5a53470e0c4ec566fa07ff3e61b7a4a3cdd3	image and video retargetting by darting	video summarization;convex programming;texture synthesis;dynamic program;fast algorithm;optical flow;motion artifact;aspect ratio	This paper considers the problem of altering an image by imperceptibly adding or removing pixels, for example, to fit a differently shaped frame with minimal loss of interesting content. We show how to construct a family of convex programs that suitably rearrange pixels while minimizing image artifacts and distortions. We call this ”darting” on analogy to a tailor’s dartssmall edits are discreetly distributed throughout the fabric of the image. We develop a reduction to integer dynamic programming on edit trellises, yielding fast algorithms. Oneand two-pass variants of the method have 0(1) per-pixel complexity. Of the many edits that darting supports, five are demonstrated here: image retargeting to smaller aspect ratios: adding or moving or removing scene objects while preserving image dimensions: image expansion with gaps filled by a rudimentary form of texture synthesis; temporal video summarization by ”packing” motion in time; and an extension to spatial video retargetting that avoids motion artifacts by preserving optical flow. Int. Conf. Image Analysis and Recognition This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of Mitsubishi Electric Research Laboratories, Inc.; an acknowledgment of the authors and individual contributions to the work; and all applicable portions of the copyright notice. Copying, reproduction, or republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Research Laboratories, Inc. All rights reserved. Copyright c ©Mitsubishi Electric Research Laboratories, Inc., 2009 201 Broadway, Cambridge, Massachusetts 02139	acknowledgment index;algorithm;broadway (microprocessor);convex optimization;distortion;dynamic programming;image analysis;optical flow;pixel;retargeting;seam carving;set packing;texture synthesis;time complexity;visual artifact	Matthew Brand	2009		10.1007/978-3-642-02611-9_4	computer vision;aspect ratio;convex optimization;computer science;optical flow;mathematics;texture synthesis;computer graphics (images)	Vision	59.70404600813955	-55.21143552505443	190054
a6450b4edc8ed13a8301a0fecbd812514f0954f0	stereoscopic view synthesis based on region-wise rendering and sparse representation	region wise rendering;sparse background dictionary;depth image based rendering;stereoscopic image generation	Depth image-based rendering (DIBR), which is used to render virtual views with a color image and the corresponding depth map, is one of the key techniques in the 2D to 3D conversion process. One of the main problems in DIBR is how to reduce holes that occur on the generated virtual view images. In this paper, we make two main contributions to deal with the problem. Firstly, a region-wise rendering framework, which divides the original image regions into three special classes and renders each with optimal adaptive process respectively, is introduced. Then, a novel sparse representation-based inpainting method, which can yield visually satisfactory results with less computational complexity for high quality 2D to 3D conversion, is proposed. Numerical experimental results demonstrate the good performance of the proposed methods. & 2016 Elsevier B.V. All rights reserved.	2d to 3d conversion;color image;computation;computational complexity theory;depth map;display resolution;distortion;glossary of computer graphics;inpainting;numerical method;rendering (computer graphics);sparse approximation;sparse matrix;stereoscopy;view synthesis	Wei Liu;Liyan Ma;Bo Qiu;Mingyue Cui	2016	Sig. Proc.: Image Comm.	10.1016/j.image.2016.05.013	computer vision;tiled rendering;image-based modeling and rendering;3d rendering;rendering;computer science;multimedia;real-time rendering;image-based lighting;computer graphics (images)	Vision	58.6160100439661	-54.80515031694723	190085
c4e2354ec4aedb71c4bb6edfcd3f8a0de391adaf	hardware aware fast depth map generation algorithm		This paper proposes a jumped matching depth estimation algorithm to calculate the disparity value in the 3D virtual view synthesis. The matching criterion adopts pixel value similarity, distance similarity and color information similarity as the weighted cost aggregation. In order to reduce the depth discontinuing and then providing the comfort of 3D watching, this paper proposes the disparity prediction method to obtain the disparity value in an object. To reduce the depth discontinuous on scene changed, we proposed a depth stability algorithm to smooth the depth transition on sequential depth maps. The experimental result shows that the method proposed in this paper reduces almost 90% computation time than that of the document [7] and the depth map accuracy of the method is very close to that of other methods in SSIM estimation. Finally, the designed algorithm is implemented in FPGA.	algorithm;binocular disparity;computation;depth map;field-programmable gate array;pixel;structural similarity;time complexity;view synthesis	Pei-Jun Lee;Hung-Peng Lee;En-Pei Wu	2017	2017 IEEE 7th International Conference on Consumer Electronics - Berlin (ICCE-Berlin)	10.1109/ICCE-Berlin.2017.8210591	view synthesis;pixel;field-programmable gate array;computation;algorithm design;structural similarity;algorithm;computer science;depth map	Robotics	57.60791910213221	-55.454632821569895	190580
80dae838c7a24f7487e064ccec972a590d8b50fa	generating pseudo-3d painting based on visual saliency and composition rules	general;image displays;i 4 0 image processing and computer vision		2.5d	Zhibin Zheng;Yan Zhang;Zhengxing Sun	2013		10.2312/conf/EG2013/short/049-052	computer vision;feature detection;image processing;computer science;multimedia;human visual system model;computer graphics (images)	AI	62.086566399207	-56.45546422402642	190588
9c0b6cc4c00b7acbe162556227257ec15f3e1b12	quality prediction of asymmetrically distorted stereoscopic 3d images	databases;stereo image processing distortion;contrast sensitivity function image quality assessment stereoscopic image 3d image asymmetric distortion ssim divisive normalization;transform coding;prediction bias elimination asymmetrically distorted stereoscopic 3d image quality prediction eye dominance effect visual quality decision content divisive normalization based pooling scheme binocular rivalry inspired multiscale model;distortion;visualization;three dimensional displays;image quality;stereo image processing;three dimensional displays distortion databases image quality stereo image processing visualization transform coding	Objective quality assessment of distorted stereoscopic images is a challenging problem, especially when the distortions in the left and right views are asymmetric. Existing studies suggest that simply averaging the quality of the left and right views well predicts the quality of symmetrically distorted stereoscopic images, but generates substantial prediction bias when applied to asymmetrically distorted stereoscopic images. In this paper, we first build a database that contains both single-view and symmetrically and asymmetrically distorted stereoscopic images. We then carry out a subjective test, where we find that the quality prediction bias of the asymmetrically distorted images could lean toward opposite directions (overestimate or underestimate), depending on the distortion types and levels. Our subjective test also suggests that eye dominance effect does not have strong impact on the visual quality decisions of stereoscopic images. Furthermore, we develop an information content and divisive normalization-based pooling scheme that improves upon structural similarity in estimating the quality of single-view images. Finally, we propose a binocular rivalry-inspired multi-scale model to predict the quality of stereoscopic images from that of the single-view images. Our results show that the proposed model, without explicitly identifying image distortion types, successfully eliminates the prediction bias, leading to significantly improved quality prediction of the stereoscopic images.	binocular status code:find:pt:eyes:nom;binocular vision;distortion;dominance, ocular;estimated;evaluation;image quality;inspiration function;numerous;self-information;stereoscopic video game;stereoscopy;structural similarity;recurrent childhood visual pathway glioma	Jiheng Wang;Abdul Rehman;Kai Zeng;Shiqi Wang;Zhou Wang	2015	IEEE Transactions on Image Processing	10.1109/TIP.2015.2446942	image quality;computer vision;transform coding;visualization;distortion;computer science;multimedia;computer graphics (images)	Vision	63.194926943363164	-63.78029296034692	190595
a9558b32e89c21231d5398d0599745685cc79dd8	visual-weighted motion compensation frame interpolation with motion vector refinement	mci;interpolation;frame rate up conversion algorithm;psnr;motion compensation;video signal processing;prediction algorithms;motion estimation;inaccurate motion vector correction;maximum likelihood estimation;visual weighted motion compensation interpolation;video signal processing maximum likelihood estimation motion compensation;visual weighted motion compensation frame interpolation;vectors psnr interpolation visualization motion estimation prediction algorithms motion compensation;visualization;ssim;ebme;vectors;visual quality improvement visual weighted motion compensation frame interpolation frame rate up conversion algorithm joint motion vector refinement visual weighted motion compensation interpolation mci hierarchical motion vector refinement inaccurate motion vector correction global controlling maximum likelihood method similarity index ssim visual compensation ebme;visual quality improvement;visual compensation;joint motion vector refinement;global controlling;hierarchical motion vector refinement;maximum likelihood method;similarity index	In this paper, we propose a novel frame rate up-conversion algorithm based on joint motion vector refinement and visual-weighted motion compensation interpolation (MCI). It utilizes a hierarchical motion vector refinement to correct inaccurate motion vectors (MVs), which is composed of the global level and the local level. In the global level, distinct inaccurate MVs are detected by global controlling and then corrected by neighborhood information. Afterwards, the local level performs the local controlling to pick out local outliers and re-estimate them with the maximum likelihood method. Finally, plausible weights for each block in the interpolated frame, computed by the similarity index(SSIM), are applied for visual compensation. The experimental results demonstrate that compared with the conventional algorithm EBME, the proposed algorithm achieved the average PSNR by up to 2.7dB while the visual quality improvement is also remarkable.	algorithm;image quality;minimal recursion semantics;motion compensation;motion interpolation;peak signal-to-noise ratio;refinement (computing)	Wei Bai;Jiaying Liu;Jie Ren;Zongming Guo	2012	2012 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2012.6272075	computer vision;mathematical optimization;quarter-pixel motion;computer science;pattern recognition;motion estimation;mathematics;maximum likelihood;statistics	Vision	54.8900888913723	-58.73799665874533	190669
0f34187d2c6e06c0bdd16307bf4f72721834c43b	video painting via motion layer manipulation	i 3 3 computer graphics picture image generation display algorithms	Temporal coherence is an important problem in Non-Photorealistic Rendering for videos. In this paper, we present a novel approach to enhance temporal coherence in video painting. Instead of painting on video frame, our approach first partitions the video into multiple motion layers, and then places the brush strokes on the layers to generate the painted imagery. The extracted motion layers consist of one background layer and several object layers in each frame. Then, background layers from all the frames are aligned into a panoramic image, on which brush strokes are placed to paint the background in one-shot. The strokes used to paint object layers are propagated frame by frame using smooth transformations defined by thin plate splines. Once the background and object layers are painted, they are projected back to each frame and blent to form the final painting results. Thanks to painting a single image, our approach can completely eliminate the flickering in background, and temporal coherence on object layers is also significantly enhanced due to the smooth transformation over frames. Additionally, by controlling the painting strokes on different layers, our approach is easy to generate painted video with multi-style. Experimental results show that our approach is both robust and efficient to generate plausible video painting.	autostereogram;coherence (physics);collision detection;flicker (screen);image processing;layers (digital image editing);moe;non-photorealistic rendering;parse tree;parsing;population;robustness (computer science);software propagation;thin plate spline;video renderer	Hua Huang;Lei Zhang;TianNan Fu	2010	Comput. Graph. Forum	10.1111/j.1467-8659.2010.01792.x	computer vision;computer science;multimedia;computer graphics (images)	Vision	58.729970439864616	-54.241933310983505	190740
ff95b60b661b487a008a6110c6021062f8c02b12	the suitability of young students for video subjective testing		In this paper we describe a set of video subjective tests with 498 student viewers in the age range 9–17. These tests are based on earlier tests that we carried out with adult viewers. The aim was not to validate our earlier results, but to use them as a reference against which we could compare the scores of the students, to allow us to address the questions of how sensitive students in this age range are to video quality issues, and how suitable they would be as viewers in subjective tests. We used the same test content as in our earlier tests, including content at high and ultra-high resolution, and with standard and high dynamic range.	high dynamic range;image resolution;video	Mike Nilsson;Brahim Allan;Steve Appleby	2018	2018 Tenth International Conference on Quality of Multimedia Experience (QoMEX)	10.1109/QoMEX.2018.8463399	multimedia;high dynamic range;dynamic range;video quality;computer science	Robotics	63.84508768344868	-62.70439211499902	190828
0d31df81e28a8dad6c3960ec6e0386774c107d4d	a psychophysical evaluation of inverse tone mapping techniques	tone mapping;high dynamic range imaging;qa76 electronic computers computer science computer software;inverse tone mapping;i 4 0 image processing and computer vision general image displays;i 3 3 computer graphics picture image generation and display algorithms;tk electrical engineering electronics nuclear engineering	In recent years inverse tone mapping techniques have been proposed for enhancing low-dynamic range (LDR) content for a high-dynamic range (HDR) experience on HDR displays, and for image based lighting. In this paper, we present a psychophysical study to evaluate the performance of inverse (reverse) tone mapping algorithms. Some of these techniques are computationally expensive because they need to resolve quantization problems that can occur when expanding an LDR image. Even if they can be implemented efficiently on hardware, the computational cost can still be high. An alternative is to utilize less complex operators; although these may suffer in terms of accuracy. Our study investigates, firstly, if a high level of complexity is needed for inverse tone mapping and, secondly, if a correlation exists between image content and quality. Two main applications have been considered: visualization on an HDR monitor and image-based lighting.	algorithm;algorithmic efficiency;analysis of algorithms;computation;dynamic range;high-dynamic-range imaging;high-dynamic-range rendering;high-level programming language;image-based lighting;ldraw;quantization (signal processing);tone mapping	Francesco Banterle;Patrick Ledda;Kurt Debattista;Marina Bloj;Alessandro Artusi;Alan Chalmers	2009	Comput. Graph. Forum	10.1111/j.1467-8659.2008.01176.x	computer vision;tone mapping;simulation;computer science;computer graphics (images)	HCI	60.22755153959015	-61.505728207033464	191018
e4c25f9ac71cb7184fea38f4dd248bfdc60408e9	a wavelet representation of reflectance functions	transformation ondelette;rendering wavelet representation reflectance functions light reflection models computer graphics realism illumination models data sets reflectance distribution function interpolation filtering nonstandard wavelet decomposition image quality;representation graphique;interpolation;reflexion luz;algorithm performance;image processing;data compression;reflectivity;etude experimentale;representacion grafica;efficient algorithm;luminance;computational geometry;procesamiento imagen;local illumination;wavelet decomposition;traitement image;computer graphic;algorithme;wavelet transforms;algorithm;distribution function;bidirectional reflectance;reflectivity wavelet transforms realistic images rendering computer graphics computational geometry light reflection interpolation;reflexion lumiere;resultado algoritmo;image quality;estructura datos;local shading;performance algorithme;realistic images;reflectance models;reflectivity analytical models distribution functions optical reflection computer graphics computational modeling lighting frequency interpolation filtering;light reflection;structure donnee;expert knowledge;compresion dato;transformacion ondita;distribution functions;compression;rendering computer graphics;data structure;estudio experimental;high frequency;wavelets;graphics;wavelet transformation;compression donnee;analytical model;algoritmo;luminancia	Analytical models of light reflection are in common use in computer graphics. However, models based on measured reflectance data promise increased realism by making it possible to simulate many more types of surfaces to a greater level of accuracy than with analytical models. They also require less expert knowledge about the illumination models and their parameters. There are a number of hurdles to using measured reflectance functions, however. The data sets are very large. A reflectance distribution function sampled at five degrees angular resolution, arguably sparse enough to miss highlights and other high frequency effects, can easily require over a million samples, which in turn amount to over four megabytes of data. These data then also require some form of interpolation and filtering to be used effectively. In this paper, we examine issues of representation of measured reflectance distribution functions. In particular, we examine a wavelet basis representation of reflectance functions, and the algorithms required for efficient point-wise reconstruction of the BRDF. We show that the nonstandard wavelet decomposition leads to considerably more efficient algorithms than the standard wavelet decomposition. We also show that thresholding allows considerable improvement in running times, without unduly sacrificing image quality. Index Terms —Reflectance models, bidirectional reflectance, distribution functions, local shading, local illumination, wavelets,		Paul Lalonde;Alain Fournier	1997	IEEE Trans. Vis. Comput. Graph.	10.1109/2945.646236	computer vision;data structure;image processing;computational geometry;computer science;distribution function;mathematics;statistics;computer graphics (images)	Graphics	64.88521831274001	-53.976553868863505	191209
8816364677c64ae1ac3b450efe49535bc3d3201c	per-separation clustered-dot color halftone watermarks: separation estimation based on spatial frequency content	digital watermarking;image coding;halftones;estimation method;spatial frequency;object detection;continuous phase modulated	A framework for clustered-dot color halftone watermarking is considered, wherein watermark patterns are embedded in individual colorant halftones prior to printing and embedded watermarks are detected from scans of the printed images after obtaining estimates of the individual halftone separations. The principal challenge in this methodology arises in the watermark detection phase. Typical three-channel RGB scanner systems do not directly provide good estimates of the four CMYK colorant halftones that are commonly used in color printing systems. To address this challenge, we propose an estimation method that, when used with suitably selected halftone periodicities, jointly exploits the differences in the spatial periodicities and the color (spectra) of the halftone separations to obtain good estimates of the individual halftones from conventional RGB scans. We demonstrate the efficacy of this methodology experimentally using continuous phase modulation for the embedding of independent visual watermark patterns in the individual halftone separations. Watermarks detected from the estimates of halftone separations obtained using the proposed estimation method have a much higher contrast than those detected directly. We also evaluate the accuracy of the estimated halftones through simulations and demonstrate that the proposed estimation method offers high accuracy. © 2010 SPIE and IS&T. [DOI: 10.1117/1.3497615]	embedded system;experiment;modulation;newton's method;printing;simulation	Basak Oztan;Gaurav Sharma	2010	J. Electronic Imaging	10.1117/1.3497615	computer vision;speech recognition;digital watermarking;spatial frequency;optics;computer graphics (images)	ML	53.97612726481688	-61.293045703330414	191246
413f908e1b4e1fba1265755e822e7ed6b3b4ec7b	dance motion analysis and editing using hilbert-huang transform		"""Human motions (especially, dance motions) are very noisy and it is difficult to analyze the motions. To resolve this problem, we propose a new method to decompose and edit the motions using the Hilbert-Huang transform (HHT). The HHT decomposes a chromatic signal into """"monochromatic"""" signals that are the so-called Intrinsic Mode Functions (IMFs) using an Empirical Mode Decomposition (EMD)[Huang 2014]. The HHT has the advantage to analyze non-stationary and nonlinear signals like human joint motions over the FFT or Wavelet transform. In the present research, we propose a new framework to analyze a famous Japanese threesome pop singer group """"Perfume"""". Then using the NA-MEMD, we decompose dance motions into motion (choreographic) primitives or IMFs, which can be scaled, combined, subtracted, exchanged, and modified self-consistently."""	fast fourier transform;hilbert–huang transform;leabhar na ngenealach;monochrome;nonlinear system;stationary process;wavelet transform	Ran Dong;DongSheng Cai;Nobuyoshi Asai	2017		10.1145/3084363.3085023	fast fourier transform;computer vision;motion analysis;artificial intelligence;hilbert–huang transform;computer graphics (images);wavelet transform;monochromatic color;chromatic scale;dance;computer science;nonlinear system	Vision	67.33485552502331	-58.04492884532232	191346
138a4c741f0e70a2295ae5fbfc22483d8bbc2440	analysis on multiresolution mosaic images	transformation ondelette;analisis imagen;evaluation performance;mosaicism;wavelet transforms computer vision computer graphics image segmentation image resolution;image segmentation;performance evaluation;image processing;image resolution;overlapping sequence;mosaicisme;computer graphics;evaluacion prestacion;procesamiento imagen;metodo subespacio;analyse multiresolution;traitement image;computer graphic;methode sous espace;computer vision;wavelet transforms;evaluation subjective;subspace method;algorithms artificial intelligence computer graphics image enhancement image interpretation computer assisted information storage and retrieval numerical analysis computer assisted pattern recognition automated reproducibility of results sensitivity and specificity signal processing computer assisted subtraction technique user computer interface;image analysis;mosaicismo;energy minimization;transformacion ondita;subjective evaluation;multiresolution analysis;analyse image;blended image multiresolution mosaic images computer vision image mosaicing image processing computer graphics wavelet subspaces energy minimization model;image analysis image resolution information science application software computer graphics computer vision image processing head pixel energy resolution;wavelet transformation;analisis multiresolucion;image mosaicing;sequence chevauchante;evaluacion subjetiva;secuencia en sobreposicion	Image mosaicing is the act of combining two or more images and is used in many applications in computer vision, image processing, and computer graphics. It aims to combine images such that no obstructive boundaries exist around overlapped regions and to create a mosaic image that exhibits as little distortion as possible from the original images. In the proposed technique, the to-be-combined images are first projected into wavelet subspaces. The images projected into the same wavelet space are then blended. Our blending function is derived from an energy minimization model which balances the smoothness around the overlapped region and the fidelity of the blended image to the original images. Experiment results and subjective comparison with other methods are given.	algorithm;alpha compositing;basis function;computation (action);computer graphics;computer vision;digital image processing;distortion;energy minimization;exhibits as topic;experiment;high- and low-level;humans;image quality;matlab;mathematical optimization;mosaic - computer software;ncsa mosaic;pixel;population parameter;projections and predictions;wavelet	Ming-Shing Su;Wen-Liang Hwang;Kuo-Young Cheng	2004	IEEE Transactions on Image Processing	10.1109/TIP.2004.828416	multiresolution analysis;computer vision;image analysis;image resolution;image processing;computer science;theoretical computer science;image segmentation;computer graphics;energy minimization;image formation;wavelet transform;computer graphics (images)	Vision	60.57151745240969	-62.962373380865486	191394
021cb24d73339749c05863db95bfb64a7c9a6c72	application of a visual model to the design of an ultra-high definition upscaler	high resolution;0130c;4266s;observers;algorithme;haute resolution;feedback;observateur;evaluation subjective;visual modeling;algorithms;visual feedback;boucle reaction;subjective evaluation;discriminacion;vision;high definition;high resolution methods;discrimination;evaluacion subjetiva	A Visual Model (VM) is used to aid in the design of an Ultra-high Definition (UHD) upscaling algorithm that renders High Definition legacy content on a UHD display. The costly development of such algorithms is due, in part, to the time spent subjectively evaluating the adjustment of algorithm structural variations and parameters. The VM provides an image map that gives feedback to the design engineer about visual differences between algorithm variations, or about whether a costly algorithm improvement will be visible at expected viewing distances. Such visual feedback reduces the need for subjective evaluation. This paper presents the results of experimentally verifying the VM against subjective tests of visibility improvement versus viewing distance for three upscaling algorithms. Observers evaluated image differences for upscaled versions of high-resolution stills and HD (Blu-ray) images, viewing a reference and test image, and controlled a linear blending weight to determine the image discrimination threshold. The required thresholds vs. viewing distance varied as expected, with larger amounts of the test image required at further distances. We verify the VM by comparison of predicted discrimination thresholds versus the subjective data. After verification, VM visible difference maps are presented to illustrate the practical use of the VM during design.	algorithm;alpha compositing;blu-ray;experiment;image map;image resolution;rendering (computer graphics);standard test image;verification and validation;visual modeling	Jon M. Speigle;Dean S. Messing;Scott J. Daly	2009		10.1117/12.811784	vision;computer vision;discrimination;simulation;image resolution;computer science;artificial intelligence;feedback;optics	HCI	62.53356506387069	-62.486896006591394	191511
817fe8e160203f75192706046923e79a69f62d46	single-shot photometric stereo by spectral multiplexing	time varying;non photorealistic rendering npr;multiple channels;photometric stereo;optical flow;font design;texture transfer;off the shelf;high speed;dynamic scenes	Spectral multiplexing allows multiple channels of information to be captured simultaneously, using readily available color cameras. Information may be multiplexed across the color channels of a camera by use of colored lights (e.g. [Woodham 1980; Hernandez and Vogiatzis 2010]) or colored filters (e.g. [Bando et al. 2008]). We propose a novel method for single-shot photometric stereo by spectral multiplexing. The output of our method is a simultaneous per-pixel estimate of the surface normal and full-color reflectance. Our method is well suited to materials with varying color and texture, requires no time-varying illumination, and no high-speed cameras. Being a single-shot method, it may be applied to dynamic scenes without any need for optical flow. Our key contributions are a generalization of three-color photometric stereo to multiple (more than three) color channels, and the design of a practical six-color-channel system using off-the-shelf parts only.	channel (digital image);color;multiplexing;normal (geometry);optical flow;photometric stereo;pixel;spectral efficiency;traffic enforcement camera	Graham Fyffe	2010	2011 IEEE International Conference on Computational Photography (ICCP)	10.1145/1899950.1899970	computer vision;photometric stereo;computer science;optical flow;computer graphics (images)	Vision	60.87298978630081	-54.48203394458621	191541
d3e1160d6ca7615b9ae5ac94cf05fe007955b2a5	an evolving localised learning model for on-line image colour quantisation	image coding;image processing;evolving localised learning model;information science;computer graphics;learning model;prototypes;on line image colour quantisation;digital colour image;network topology;vector quantization;image color analysis;image colour analysis;displays;prototypes image color analysis displays clustering algorithms network topology information science image processing vector quantization image coding computer graphics;clustering algorithms;digital colour image evolving localised learning model on line image colour quantisation image processing;image coding image colour analysis	Although widely studied for many years, colour quantisation remains a practical problem in image processing. Unlike previous works where the image can only be quantised after the whole set of image data is acquired, we propose to use an evolving localised learning model for on-line colour quantisation. This approach is compared with some conventional algorithms.	algorithm;color quantization;image processing;online and offline;quantization (image processing)	Jeremiah D. Deng;Nikola K. Kasabov	2001		10.1109/ICIP.2001.959193	computer vision;image processing;information science;computer science;machine learning;prototype;multimedia;cluster analysis;computer graphics;vector quantization;network topology	Vision	54.59873913811789	-64.13136330654393	191576
189fa895001744cc44cb33784eaa38ec3e4a6728	restoring the spatial resolution of refocus images on 4d light field	baja resolucion;4230;resolution spatiale;sensors;0130c;depth of field;light field;low resolution;basse resolution;imagerie;profondeur champ;imagery;pixel;profundidad campo;imagineria;cameras;spatial information;spatial resolution	This paper presents the method for generating a refocus image with restored spatial resolution on a plenoptic camera, which functions controlling the depth of field after capturing one image unlike a traditional camera. It is generally known that the camera captures 4D light field (angular and spatial information of light) within a limited 2D sensor and results in reducing 2D spatial resolution due to inevitable 2D angular data. That’s the reason why a refocus image is composed of a low spatial resolution compared with 2D sensor. However, it has recently been known that angular data contain sub-pixel spatial information such that the spatial resolution of 4D light field can be increased. We exploit the fact for improving the spatial resolution of a refocus image. We have experimentally scrutinized that the spatial information is different according to the depth of objects from a camera. So, from the selection of refocused regions (corresponding depth), we use corresponding pre-estimated sub-pixel spatial information for reconstructing spatial resolution of the regions. Meanwhile other regions maintain out-of-focus. Our experimental results show the effect of this proposed method compared to existing method.	angularjs;expanded memory;experiment;image resolution;light field;pixel	JaeGuyn Lim;Byung Kwan Park;Joo Young Kang;Seong-Deok Lee	2010		10.1117/12.838830	computer vision;image resolution;optics;physics;remote sensing	Vision	67.68858038879826	-61.98919837553574	191635
5c74bbe11f64c50e1dd55222c0448539f8c5b039	gamut mapping: an overview of the problem		Gamut mapping is a very actual problem in todayÕs color reproduction. For a target device, a gamut mapping algorithm establishes a correspondence between the out of gamut color and the color within the gamut, trying to preserve same color appearance with the original color when that color is rendered on the target device. This problem becomes more and more important with the increasing number of color reproduction devices and cross media image representation. An immediate application of gamut mapping is for creation of color tables of ICC profiles that are used in color management workflow to communicate colors from one device to another. A large number of gamut mapping algorithms is encountered in the literature. This paper does not try to give an overview of these algorithms, despite the fact that few of them are mentioned or discussed. The intention of this paper is more to discuss the factors that influence the gamut mapping and gamut mapping results. Gamut representation and factors to influence the gamut shape size are discussed in the first section. In the second section the influence of the color space is discussed and a linearization procedure for CIELAB is proposed with direct application to gamut mapping problem. In the third section, few gamut mapping algorithms are discussed, in their results are compared with respect to linear mLAB or CIELAB spaces. The immediate application of this analysis is for creation of ICC profiles used to characterize several printing devices.	algorithm;color management;color space;printing	Gabriel Marcu	1999			computer vision;gamut;artificial intelligence;computer science	HCI	62.09449626728599	-53.71627044366174	191824
5c5f3a96be872c27923d847b4c0053d0b0593d1e	the effect of image content on color difference perceptibility	imaging science	"""Considerable work has been conducted regarding the perceptibility of color differences for simple images such as uniform color patches. From this work comes such tools as the MacAdam ellipses. Less is known regarding color difference perceptibilitywhen complex images are involved. However, the proliferation ofdesktop publishing equipment and the increasingly technically elegant accompanying software makes questions around this topic bothmore facile andmore relevant to study. The information that could be gleanedmay be useful for the design of color printer algorithms or other imaging systems. The objective of this research was to examine the impact of image content, if any, on the perceptibility of color differences. Psychophysical experimentation was conducted to determine if image content is a significant factor in color difference perceptibility. Portraits and nature scenes containing """"memory colors"""" or colors for which observers have some preconceptions, images ofman-made objects void ofmemory colors, andmosaic images containing no recognizable image content were examined in the experimenta tion. Four different mosaic images, each composed ofpatches of a given size, were also included to investigate the effect of the size of image elements making up the image content on color difference perceptibility. The experimental results suggest that the size of elements contained in the image affected the perceptibility of color differences while the presence ofmemory colors or recognizable image content had no conclusive effect."""	algorithm;color;printer (computing)	Susan Farnand	1996			computer vision;computer science	HCI	64.1516370050791	-58.51940018871568	191977
37682985120dd10e40808d8e62ee2de5ed500d64	compressive light field photography using overcomplete dictionaries and optimized projections	compressive sensing;computational photography	Light field photography has gained a significant research interest in the last two decades; today, commercial light field cameras are widely available. Nevertheless, most existing acquisition approaches either multiplex a low-resolution light field into a single 2D sensor image or require multiple photographs to be taken for acquiring a high-resolution light field. We propose a compressive light field camera architecture that allows for higher-resolution light fields to be recovered than previously possible from a single image. The proposed architecture comprises three key components: light field atoms as a sparse representation of natural light fields, an optical design that allows for capturing optimized 2D light field projections, and robust sparse reconstruction methods to recover a 4D light field from a single coded 2D projection. In addition, we demonstrate a variety of other applications for light field atoms and sparse coding, including 4D light field compression and denoising.	autostereogram;dictionary;image resolution;light field;multiplexing;neural coding;noise reduction;sparse approximation;sparse matrix	Kshitij Marwah;Gordon Wetzstein;Yosuke Bando;Ramesh Raskar	2013	ACM Trans. Graph.	10.1145/2461912.2461914	computer vision;computational photography;computer science;optics;compressed sensing;computer graphics (images)	Graphics	60.37350704850039	-55.98949229260391	192135
74b92bfb411a72ea183b1233e9e5adbf384bd36f	stereo day-for-night: retargeting disparity for scotopic vision	t technology general;scotopic vision;night vision;stereoscopic 3d	Several approaches attempt to reproduce the appearance of a scotopic low-light night scene on a photopic display (“day-for-night”) by introducing color desaturation, loss of acuity, and the Purkinje shift toward blue colors. We argue that faithful stereo reproduction of night scenes on photopic stereo displays requires manipulation of not only color but also binocular disparity. To this end, we performed a psychophysics experiment to devise a model of disparity at scotopic luminance levels. Using this model, we can match binocular disparity of a scotopic stereo content displayed on a photopic monitor to the disparity that would be perceived if the scene was actually scotopic. The model allows for real-time computation of common stereo content as found in interactive applications such as simulators or computer games.	binocular disparity;binocular vision;color;computation;pc game;real-time locating system;retargeting;simulation	Petr Kellnhofer;Tobias Ritschel;Peter Vangorp;Karol Myszkowski;Hans-Peter Seidel	2014	TAP	10.1145/2644813	stereoscopy;computer vision;optics;scotopic vision;photopic vision;mesopic vision;computer graphics (images)	Vision	62.33727941314298	-52.71159547463493	192175
1a27c936d01932ea9620143c0e252a5f58763f7d	scale-invariant structure saliency selection for fast image fusion		In this paper, we present a fast yet effective method for pixel-level scale-invariant image fusion in spatial domain based on the scale-space theory. Specifically, we propose a scale-invariant structure saliency selection scheme based on the difference-of-Gaussian (DoG) pyramid of images to build the weights or activity map. Due to the scale-invariant structure saliency selection, our method can keep both details of small size objects and the integrity information of large size objects in images. In addition, our method is very efficient since there are no complex operation involved and easy to be implemented and therefore can be used for fast high resolution images fusion. Experimental results demonstrate the proposed method yields competitive or even better results comparing to state-of-the-art image fusion methods both in terms of visual quality and objective evaluation metrics. Furthermore, the proposed method is very fast and can be used to fuse the high resolution images in real-time. Code is available at https://github.com/yiqingmy/Fusion.	difference of gaussians;display resolution;effective method;image fusion;image resolution;map;pixel;real-time clock;real-time computing;scale space;time complexity;zero suppression	Yixiong Liang;Yuan Mao;Jiazhi Xia;Yao Xiang;Jianfeng Liu	2018	CoRR			Vision	58.82679405624694	-65.17625010122397	192221
7da3c1824cbee48036cec13bf7840a7e34cf4f77	color for moving-map cockpit displays	vision system;computer simulations;aeronautique;image coding;image processing;systeme vision;cockpit displays;vision color;procesamiento imagen;real time simulation;traitement image;color perception;vision couleur;codage image;nomenclature;roads;visual search;sistema cognitiva;situated display;situation awareness;aeronautica;systeme cognitif;cognitive system;aeronautics;target detection;water;reaction time;imagerie visuelle;color vision;imagineria visual;visual imagery	Three color schemes (monochrome, dichrome, and polychrome) based on basic principles for color perception and cognition were optimized and applied to an electronic map in a horizontal-situation display. Principles for color discrimination, symbol coding, and color naming were applied for the super-imposed symbols (targets, waypoints etc) and for the map symbology (land, water, roads). The color codes were tested in a visual search and detection experiment in a real-time simulation in an air-to-air mission with test pilots as subjects. The simulation task was as close as possible to a real-life situation. The pilots had to track a maneuvering target within specified limits. Reaction times for target detection were recorded. After the simulation, the test pilots gave a subjective estimation ofthe different color schemes. They also estimated them according to situation awareness using a rating technique on cognitive compatibility (CC-SART). All the results, both the objective and the subjective show that color schemes are advantageous in comparison to the monochrome code. The reactions times were significantly lower for the chromatic color codes. The estimated situation awareness was higher for the chromatic schemes and the subjects gave higher preferences to the chromatic codes.	code;cognition;cognitive architecture;color vision;map;monochrome;real life;real-time clock;simulation	Gunilla Derefeldt;Jan Westlund;Oerjan Skinnars;Jens Alfredson;Peter Andersson;Johan Holmberg;Lars Eriksson;Ulf Berggrund;Rolf Santesson	1998		10.1117/12.320128	computer vision;simulation;telecommunications;image processing;color vision;optics;physics;computer graphics (images)	ML	62.853085327123026	-61.70024403852746	192251
43bfc623971212987e4a76d97ba90e8acb63a872	a fuzzy approach to real-time digital color reproduction of clothing with 3d camera	fuzzy approach boundary fidelity quality reproduction clothing boundary decision foreground extraction depth image intel realsense sdk frb color reproduction system real time high quality fuzzy rule based color reproduction system object boundary decision temporal color consistency handling color space transform computational complexity object color reproduction anomalous trichromats dichromats discrimination improvement medical imaging projection based appearances editing object color appearance 3d camera real time digital color reproduction;image color analysis clothing histograms real time systems three dimensional displays face skin;3d camera digital color reproduction fuzzy logic hsv color space intel realsense tm sdk;image colour analysis cameras clothing fuzzy set theory	Digital color reproduction[1][2] is a technique that can change the color appearance of objects shown in an image or video. The technique is utilized in various aspects and scenarios including projection-based appearances editing[3], medical imaging[4] and discrimination improvement for dichromats or anomalous trichromats[5][6]. However, traditional object color reproduction has high computational complexity due to color space transform, temporal color consistency handling and object boundary decision, which makes real-time processing for videos impossible without additional computing power such as GPGPU acceleration[7]. In this paper, a real-time high quality fuzzy rule-based(FRB) color reproduction system for clothing with Intel® RealSenseTM SDK and 3D Camera is proposed. By virtue of the SDK and 3D camera, depth image can be used to accelerate the user foreground extraction and clothing boundary decision, which not only provides a high quality reproduction result with better boundary fidelity, but also speeds up the computation to achieve real-time processing capability.	color space;computation;computational complexity theory;display resolution;fuzzy logic;fuzzy rule;general-purpose computing on graphics processing units;real-time clock;real-time locating system;real-time transcription;software development kit;video	Finn Wong;Donghai Dai	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7351543	demosaicing;color histogram;false color;rgb color model;computer vision;icc profile;color model;hsl and hsv;color normalization;color depth;color image;multimedia;color balance;color space;computer graphics (images)	Robotics	58.92381129891408	-61.01915834397267	192279
492459131901ef9a333a6971f9a003b99ef713ca	structural similarity image quality reliability: determining parameters and window size	vision system;vision ordenador;evaluation image;control de calidad;image processing;systeme vision;signal distortion;articulo;erreur quadratique moyenne;procesamiento imagen;metric;distorsion signal;qualite image;traitement image;similitude;computer vision;mean square error;peak signal to noise ratio;image quality;multimedia communication;similarity;mean opinion score;image quality assessment;controle qualite;evaluacion imagen;metrico;vision ordinateur;calidad imagen;image evaluation;rapport signal bruit;relacion senal ruido;similitud;error medio cuadratico;human vision system;signal to noise ratio;communication multimedia;quality control;structural similarity;sistema vision;metrique;distorsion senal	The need to obtain objective values of the quality of distorted images with respect to the original is fundamental in multimedia and image processing applications. It is generally required that this value correlates well with the human vision system (HVS). In spite of the properties and the general use of the mean square error (MSE) measurement, this has a poor correlation with HSV, which has led to the development of methods such as structural similarity (SSIM). This metric improves the correlation with respect to the classic MSE and PSNR (peak signal to noise ratio). However, its behavior depends on the values assigned to constants and on the windows size selected. These values are usually assigned arbitrarily and there have been no studies on how they affect the SSIM. In this work, we have analyzed empirically the most appropriate values for the different constants used in the SSIM equations. We have also analyzed the importance of window size in the calculation of MSSIM, and propose a method for determining the window size based on image complexity. Using the values selected and the window size defined, the correlation between SSIM and DMOS (differential mean opinion score) is significantly improved by around 17% with respect to the values commonly used. & 2010 Elsevier B.V. All rights reserved.	best, worst and average case;database;expectation–maximization algorithm;human visual system model;image processing;image quality;ms-dos api;mean squared error;microsoft windows;multiscale modeling;peak signal-to-noise ratio;s5 (modal logic);structural similarity;window function	Javier Silvestre-Blanes	2011	Signal Processing	10.1016/j.sigpro.2010.10.003	image quality;mean opinion score;computer vision;quality control;similarity;metric;peak signal-to-noise ratio;image processing;computer science;artificial intelligence;similitude;structural similarity;mathematics;mean squared error;signal-to-noise ratio	AI	62.254830592109755	-63.52023700645395	192335
a7c477c576689bd8e6b50f8df0b31e3e27033896	efficient compression of rhythmic motion using spatial segmentation and temporal blending	image motion analysis;image segmentation;data compression;visual quality motion data compression primitives matching spatial temporal blending animation;rendering computer graphics data compression image motion analysis image segmentation;motion segmentation abstracts indexes torso bones;rendering computer graphics;automatic weight propagation strategy rhythmic motion compression spatial segmentation temporal blending compression ratio rendered quality motion capture data visual quality	We investigate the effectiveness of combining spatial segmentation and temporal blending to improve compression ratio and rendered quality on motion capture data of rhythmic nature. Experimental results demonstrate the feasibility of our method. We also observe that visual quality can be further improved by adopting an automatic weight propagation strategy.	alpha compositing;motion capture;software propagation	Amirhossein Firouzmanesh;Mitch Lindgren;Teri Drummond;L. Irene Cheng;Anup Basu	2013	2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)	10.1109/ICMEW.2013.6618261	data compression;computer vision;quarter-pixel motion;computer science;multimedia;image segmentation;scale-space segmentation;statistics;computer graphics (images)	Robotics	58.038876034779484	-56.017634128752015	192470
6b278a14a482d20ef89c96b8bf3838b827c01640	a new convolution kernel for atmospheric point spread function applied to computer vision	participating media;convolution kernel;image matching;image matching computer vision feature extraction filtering theory gaussian distribution;feature extraction convolution kernel atmospheric point spread function computer vision filtering theory generalized gaussian distribution light ray scattering atmospheric degraded image matching geometric invariance;convolution kernel computer vision optical scattering light scattering degradation optical filters gaussian distribution atmospheric modeling rain;atmospheric point spread function;computer vision;point spread function;multiple scattering;feature extraction;visual cues;weather condition;generalized gaussian distribution;light ray scattering;atmospheric degraded image matching;gaussian distribution;filtering theory;invariant feature;geometric invariance	In this paper we introduce a new filter to approximate multiple scattering of light rays within a participating media. This filter is derived from the generalized Gaussian distribution GGD. It characterizes the Atmospheric Point Spread Function (APSF) and thus makes it possible to introduce three new approaches. First, it allows us to accurately simulate various weather conditions that induce multiple scattering including fog, haze, rain, etc. Second, it allows us to propose a new method for a cooperative and simultaneous estimation of visual cues, i.e., the identification of weather degradations and the estimation of optical thickness between two images of the same scene acquired under unknown weather conditions. Third, by combining this filter with two new sets of invariant features we recently developed, we obtain invariant features that can be used for the matching of atmospheric degraded images. The first set leads to atmospheric invariant features while the second one simultaneously provides atmospheric and geometric invariance.	approximation algorithm;computer vision;convolution;experiment;image formation;ray (optics);simulation;thickness (graph theory)	Samy Metari;François Deschênes	2007	2007 IEEE 11th International Conference on Computer Vision	10.1109/ICCV.2007.4408899	normal distribution;computer vision;mathematical optimization;sensory cue;feature extraction;computer science;point spread function;mathematics;generalized normal distribution;kernel	Vision	59.422870393812595	-53.69986715222716	192488
3167d7af4f778c6b2d7c3e0c1412e8791c2dc4be	ultimate grain filter	residual morphological operator;grain filter;tree of shapes;ultimate grain filter;connected operator	This work introduces a residual operator called ultimate grain filter which is a powerful image operator based on numerical residues. With a multi-scale approach, the ultimate grain filter analyzes an image under a series of grain filters of increasing grain sizes. Thus, contrasted objects can be detected if a relevant residue is generated when they are filtered out by one of these grain filters. We also present an efficient algorithm for ultimate grain filter computation by using a structure called tree of shapes. In fact, since the result of a given grain filter can be obtained by pruning the corresponding tree and reconstructing it, we show that the result of the ultimate grain filter (which is based on numerical residues from a family of grain filters) can be obtained by the computation of the difference (remaining nodes) of the corresponding pruned trees. Finally, we propose the use of ultimate grain filter to extract contrasted objects using a priori knowledge of an application.	algorithm;computation;numerical analysis;tree (data structure)	Wonder Alexandre Luz Alves;Ronaldo Fumio Hashimoto	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025597	mathematical optimization;discrete mathematics	Robotics	66.67488065863391	-56.29520655192787	192491
e3e1076392735b5990f51d1a78c743711d0d8110	shape dynamical models for activity recognition and coded aperture imaging for light-field capture	dissertation	Title of Dissertation: Shape Dynamical Models for Activity Recognition and Coded Aperture Imaging for Light-Field Capture Ashok Veeraraghavan, Doctor of Philosophy, 2008 Dissertation directed by: Professor Rama Chellappa Department of Electrical and Computer Engineering Classical applications of Pattern recognition in image processing and computer vision have typically dealt with modeling, learning and recognizing static patterns in images and videos. There are, of course, in nature, a whole class of patterns that dynamically evolve over time. Human activities, behaviors of insects and animals, facial expression changes, lip reading, genetic expression profiles are some examples of patterns that are dynamic. Models and algorithms to study these patterns must take into account the dynamics of these patterns while exploiting the classical pattern recognition techniques. The first part of this dissertation is an attempt to model and recognize such dynamically evolving patterns. We will look at specific instances of such dynamic patterns like human activities, and behaviors of insects and develop algorithms to learn models of such patterns and classify such patterns. The models and algorithms proposed are validated by extensive experiments on gait-based person identification, activity recognition and simultaneous tracking and behavior analysis of insects. The problem of comparing dynamically deforming shape sequences arises repeatedly in problems like activity recognition and lip reading. We describe and evaluate parametric and non-parametric models for shape sequences. In particular, we emphasize the need to model activity execution rate variations and propose a non-parametric model that is insensitive to such variations. These models and the resulting algorithms are shown to be extremely effective for a wide range of applications from gait-based person identification to human action recognition. We further show that the shape dynamical models are not only effective for the problem of recognition, but also can be used as effective priors for the problem of simultaneous tracking and behavior analysis. We validate the proposed algorithm for performing simultaneous behavior analysis and tracking on videos of bees dancing in a hive. In the last part of this dissertaion, we investigate computational imaging, an emerging field where the process of image formation involves the use of a computer. The current trend in computational imaging is to capture as much information about the scene as possible during capture time so that appropriate images with varying focus, aperture, blur and colorimetric settings may be rendered as required. In this regard, capturing the 4D light-field as opposed to a 2D image allows us to freely vary viewpoint and focus at the time of rendering an image. In this dissertation, we describe a theoretical framework for reversibly modulating 4D light fields using an attenuating mask in the optical path of a lens based camera. Based on this framework, we present a novel design to reconstruct the 4D light field from a 2D camera image without any additional refractive elements as required by previous light field cameras. The patterned mask attenuates light rays inside the camera instead of bending them, and the attenuation recoverably encodes the rays on the 2D sensor. Our mask-equipped camera focuses just as a traditional camera to capture conventional 2D photos at full sensor resolution, but the raw pixel values also hold a modulated 4D light field. The light field can be recovered by rearranging the tiles of the 2D Fourier transform of sensor values into 4D planes, and computing the inverse Fourier transform. In addition, one can also recover the full resolution image information for the in-focus parts of the scene. Shape Dynamical Models for Activity Analysis and Coded Aperture Imaging for Light-Field Capture by Ashok Veeraraghavan Dissertation submitted to the Faculty of the Graduate School of the University of Maryland, College Park in partial fulfillment of the requirements for the degree of Doctor of Philosophy 2008	activity recognition;algorithm;apache hive;coded aperture;computer engineering;computer vision;dynamical system;experiment;gaussian blur;gene expression programming;image formation;image processing;light field;modulation;parametric model;pattern recognition;pixel;ray (optics);requirement;sensor	Ashok Veeraraghavan	2008			computer vision;geometry;optics	Vision	60.10468877854095	-52.849577606579935	192801
7a534cf0f1262fb88a26b4dc857ec4589bb338fc	colorization for gray scale facial image by locality-constrained linear coding	map;mrf;colorization;llc	Colorization for gray scale facial image is an important technique in various practical applications. However, the methods that have been proposed are essentially semi-automatic. In this paper, we present a new probabilistic framework based on Maximum A Posteriori (MAP) estimation to automatically transform the given gray scale facial image to corresponding color one. Firstly, the input image is divided into several patches and non-parametric Markov random field (MRF) is employed to formulate the global energy. Secondly, Locality-constrained Linear Coding (LLC) is employed to learn the color distribution for each patch. At the same time, the simulated annealing algorithm is employed to iteratively update the patches chosen by LLC to optimize the MRF by decreasing global energy cost. The experimental results demonstrate that the proposed framework is effective to colorize the gray scale facial images to corresponding color ones.	linear code;locality of reference	Yang Liang;Mingli Song;Jiajun Bu;Chun Chen	2014	Signal Processing Systems	10.1007/s11265-013-0809-4	computer vision;materials recovery facility;computer science;map;programming language;computer graphics (images)	ML	56.59803430143718	-61.466771705319815	192932
c0ccd2766f493d19f4f0f4ccfe8dacec5358b842	image restoration of noisy images using tchebichef moments	gaussian noise;zernike polynomials speckle image restoration optical noise chebyshev approximation gaussian noise legendre polynomials;speckle;degradation;zernike moments;circuit noise;legendre moments;optical noise;speckle noise;additive noise;noisy images;image restoration;noise generators;salt and pepper noise;tchebichef moments;polynomials;noise level;image restoration polynomials speckle degradation circuit noise gaussian noise additive noise image reconstruction noise generators noise level;image reconstruction;speckle noise image restoration noisy images tchebichef moments gaussian noise salt and pepper noise legendre moments zernike moments;legendre polynomials;chebyshev approximation;pepper;zernike polynomials	It is shown that by utilizing the method of Tchebichef (also know as Chebyshev) moments, an image degraded by Gaussian, salt-and-pepper and speckle noise can be restored. The performance of Tchebichef moments is compared with that of Zernike and Legendre moments.	circuit restoration;image restoration	Pew-Thian Yap;Raveendran Paramesran	2002		10.1109/APCCAS.2002.1115328	iterative reconstruction;speckle pattern;gaussian noise;speckle noise;image restoration;computer vision;electronic engineering;degradation;legendre polynomials;zernike polynomials;mathematics;salt-and-pepper noise;polynomial;approximation theory	Vision	57.40874802449812	-65.61502804014955	192933
4f644c8165e8e6f064d2ea24a8962c0f4eabbf9d	generalized video deblurring for dynamic scenes	motion segmentation silicon;optical flow estimation generalized video deblurring dynamic scenes captured scenes blurry videos camera shake moving objects depth variation pixel wise kernel bidirectional opticalflows latent frames energy junction blurry frames;video signal processing image restoration image sensors image sequences	Several state-of-the-art video deblurring methods are based on a strong assumption that the captured scenes are static. These methods fail to deblur blurry videos in dynamic scenes. We propose a video deblurring method to deal with general blurs inherent in dynamic scenes, contrary to other methods. To handle locally varying and general blurs caused by various sources, such as camera shake, moving objects, and depth variation in a scene, we approximate pixel-wise kernel with bidirectional optical flows. Therefore, we propose a single energy model that simultaneously estimates optical flows and latent frames to solve our deblurring problem. We also provide a framework and efficient solvers to optimize the energy model. By minimizing the proposed energy function, we achieve significant improvements in removing blurs and estimating accurate optical flows in blurry frames. Extensive experimental results demonstrate the superiority of the proposed method in real and challenging videos that state-of-the-art methods fail in either deblurring or optical flow estimation.	approximation algorithm;deblurring;jumbo frame;kernel (operating system);mathematical optimization;optical flow;pixel;shake;software framework	Tae Hyun Kim;Kyoung Mu Lee	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2015.7299181	computer vision;mathematics;multimedia;computer graphics (images)	Vision	54.96722393047456	-53.539807517426595	193590
d5ae6b83de179325abc2d54ac3d08752f2ef905f	psychophysical study of lcd motion-blur perception	artefacto;4279k;liquid crystal display;methode mesure;0130c;luminance;analisis objetivos;liquid crystal displays;illuminance;4266s;observers;artefact;lcds;observateur;motion blur;imagen borrosa;blurred image;displays;measuring methods;image floue;vision;analyse objective;objective analysis;affichage cristaux liquides	Motion-blur is still an important issue on liquid crystal displays (LCD). In the last years, efforts have been done in the characterization and the measurement of this artifact. These methods permit to picture the blurred profile of a moving edge, according to the scrolling speed and to the gray-to-gray transition considered. However, other aspects should be taken in account in order to understand the way LCD motion-blur is perceived. In the last years, a couple of works have adressed the problem of LCD motion-blur perception, but only few speeds and transitions have been tested. In this paper, we have explored motion-blur perception over 20 gray-to-gray transitions and several scrolling speeds. Moreover, we have used three different displays, to explore the influence of the luminance range as well as the blur shape on the motion-blur perception. A blur matching experiment has been set up to obtain the relation between objective measurements and perception. In this experiment, observers must adjust a stationary test blur (simulated from measurements) until it matches their perception of the blur occuring on a moving edge. Result shows that the adjusted perceived blur is always lower than the objective measured blur. This effect is greater for low contrast edges than for high contrast edges. This could be related to the motion sharpening phenomenon.	box blur;display motion blur;gaussian blur;grayscale;rc circuit;scrolling;simulation;stationary process;vesa bios extensions	Sylvain Tourancheau;Patrick Le Callet;Kjell Brunnström;Börje Andrén	2009		10.1117/12.811757	computer vision;liquid-crystal display;optics;physics;computer graphics (images)	HCI	62.5699779547302	-61.693120348702344	193739
dd3d8115b5e1263eb34311a3a8a53afb796f56f6	an intelligent image agent based on soft-computing techniques for color image processing	image filtering;agent based;soft computing;impulse noise;color image processing;mean absolute error;learning methods;peak signal to noise ratio;fuzzy inference;genetic algorithm;subjective evaluation	An intelligent image agent based on soft-computing techniques for color image processing is proposed in this paper. The intelligent image agent consists of a parallel fuzzy composition mechanism, a fuzzy mean related matrix process and a fuzzy adjustment process to remove impulse noise from highly corrupted images. The fuzzy mechanism embedded in the filter aims at removing impulse noise without destroying fine details and textures. A learning method based on the genetic algorithm is adopted to adjust the parameters of the filter from a set of training data. By the experimental results, the intelligent image agent achieves better performance than the state-of-the-art filters based on the criteria of Peak-Signal-to-Noise-Ratio (PSNR) and Mean-Absolute-Error (MAE). On the subjective evaluation of those filtered images, the intelligent image agent also results in a higher quality of global restoration.	agent-based model;color image;image processing;soft computing	Shu-Mei Guo;Chang-Shing Lee;Chin-Yuan Hsu	2005	Expert Syst. Appl.	10.1016/j.eswa.2004.12.010	median filter;computer vision;genetic algorithm;peak signal-to-noise ratio;image processing;impulse noise;computer science;artificial intelligence;machine learning;digital image processing;soft computing;mean absolute error	Vision	56.130699710086446	-65.22047146333055	194002
1e13ccd779e2d4a7798b9463f97159fc1b032f67	polarization vision: a new sensory approach to image understanding	polarisation optique;comprension imagen;object recognition;vision ordenador;human vision;image processing;state of polarization;sensors;image understanding;procesamiento imagen;movie camera;traitement image;computer vision;captador medida;camara;measurement sensor;capteur mesure;optical polarization;polarization cameras;polarization vision;comprehension image;visual performance;vision ordinateur;image comprehension;polarizacion optica;camera	Overviewed are recent results of a new general approach to image understanding and computer vision utilizing the sensing of polarization of light. Whereas human vision is oblivious to components of light polarization, polarization parameters of light are shown to provide an important visual extension to intensity and color significantly expanding the application potential for image understanding. A physical state of polarization can be visualized directly in human terms as a particular hue and saturation, and this paper utilizes such a scheme presenting images of ordinary scenes as never seen before by humans in the domain of polarization vision. Metaphorically, humans are 'color blind' with respect to the perception of polarization, and even though this does not appear to inhibit human visual performance, we show how polarization vision is a sensory augmentation that can significantly enhance both automated image understanding and even possibly improve human visual performance itself under certain conditions. Sensors, calledpolarization cameras, have been developed that automatically sense components of partial linear polarization and computationally process these components to produce polarization images. Prototypes of different polarization camera sensors have been presented in earlier literature. A recent advancement in the design of polarization cameras has made it possible to interface low-cost modular components with almost any existing imaging device converting it into an automatic polarization camera. This compatibility with small portable imaging devices is making polarization imaging for the first time accessible to a number of application areas outside the laboratory, both outdoors and underwater, revealing polarization vision as a vast new visually augmented domain with unique capabilities. This paper presents various results from three on-going field applications: natural object recognition, inspection of ship hulls for damage, and marine biology.	computer vision;polarization (waves)	Lawrence B. Wolff	1997	Image Vision Comput.	10.1016/S0262-8856(96)01123-7	computer vision;image processing;computer science;sensor;cognitive neuroscience of visual object recognition	Vision	64.0491307196999	-57.341664654945994	194234
0ebe3ecbf7ac13f36908261909afeb0ebe8a1731	adaptive spatial and temporal prefiltering for video compression	video compression;motion vector;compressed video	When compressing video sequences, noise is fatal to compression performance. Imaging is inherently a noisy process since the number of photons hitting a detector is a statistical process. When scene illumination is high, the statistical nature of the number of photons hitting each detector is of no importance, since other effects such as quantization dominate. For other applications, military for example, where the imaging has to be done with whatever illumination is naturally available, the noise can be both annoying for the observer and make transmission of the imagery consume excessive amounts of bandwidth. We show results of using content based spatial prefiltering combined with motion vector certainty controlled temporal prefiltering to reduce the noise and thus improve both visual impression and transmission properties.	data compression	Astrid Lundmark;Leif Haglund	2003		10.1007/3-540-45103-X_125	data compression;computer vision;computer science;multimedia;statistics;computer graphics (images)	Vision	59.84185821978251	-57.56009388211648	194249
ca5b136575aa88afe446271024e11a6903c9a1a1	color segmentation based depth image filtering		We present a novel enhancement method that addresses the problem of corrupted edge information in depth maps. Corrupted depth information manifests itself in zigzag edges instead of straight ones. We extract the depth information from an associated color stream and use this information to enhance the original depth map. Besides the visual results, a quantitative analysis is conducted to prove the capabilities of our approach.	algorithm;depth map;image segmentation;time consistency	Michael Schmeing;Xiaoyi Jiang	2012		10.1007/978-3-642-40303-3_8	computer vision;machine learning;mathematics;information retrieval	AI	57.91692155773216	-61.03293420813308	194303
4d1c6796366ab01d2a2a47604d1b0a4195b32615	spectral color reproduction based on six-color inkjet output system				Lawrence A. Taplin;Roy S. Berns	2001			artificial intelligence;computer vision;spectral color;computer science	EDA	63.72091351514874	-56.15998586659604	194437
dd55ed6a4a8323563adecedf97c24f4bb442b3e9	optimization of single filter network on visual corrosion defect	psnr;wavelet de noising software filter image neural network;wavelet transforms corrosion image denoising image enhancement inspection mean square error methods neural nets object recognition optimisation production engineering computing;inspection;wavelet transforms;visualization;corrosion inspection psnr visualization noise reduction wavelet transforms optimization;noise reduction;peak signal to noise ratio optimization single filter network visual corrosion defect visual corrosion inspection operation software image filter object recognition technique wavelet denoising image enhancement neural network mean square error;optimization;corrosion	Due to the challenging environmental conditions and characteristics, the complexity of the corrosion inspection operation increases. By using software image filter to enhance image data, the object recognition technique will be able to analyze the image data accurately. A selected software filter, wavelet de-noising has been identified to enhance image data for visual corrosion inspection application. Therefore, in order to obtain a better image enhancement, neural network is used for validation. The experiment result shows neural network wavelet de-noising filter used to enhance image in order to obtain better image for visual corrosion inspection is achievable, and gives desirable result in terms of Mean Square Error and Peak Signal to Noise Ratio. This project is focusing on corrosion inspection using image.	artificial neural network;composite image filter;image editing;mean squared error;outline of object recognition;program optimization;signal-to-noise ratio;software bug;wavelet	Syahril Anuar Idris;Fairul Azni Jafar;Noraidah Blar	2014	2014 11th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)	10.1109/URAI.2014.7057489	computer vision;corrosion;visualization;inspection;peak signal-to-noise ratio;computer science;machine learning;noise reduction;wavelet transform	Robotics	59.46305316335223	-64.31358688177755	194491
5ea630814de9671572e37572164de94ed5fb21c2	eye tracking observers during color image evaluation tasks	achromatique;color science;eye;achromatic;movimiento ocular;qualite image;acromatico;chromaticity;psychometrie;visual search;image quality;eye movement;psychometrics;paired comparison;chromaticite;image analysis;calidad imagen;cromaticidad;eye tracking;psicometria;scanning;imagen color;mouvement oculaire;image couleur;color image	Eye movement behavior was investigated for image-quality and chromatic adaptation tasks. examined the differences between paired comparison, rank order, and graphical rating tasks, and t examined the strategies adopted when subjects were asked to select or adjust achromatic regio indicate that subjects spent about 4 seconds looking at images in the rank order task, 1.8 seconds pe comparison task, and 3.5 seconds per image in the graphical rating task. Fixation density maps correlated highly in four of the five images. Eye movements gravitated toward faces and s introspective report was not always consistent with fixation density 5% of the time), they did so early. Foveations were directed to semantic features to adapt to the averag were directed to near-neutral regions, showing behavi	color image;eye tracking;graphical user interface;map	Jason S. Babcock;Jeff B. Pelz;Mark D. Fairchild	2003		10.1117/12.477770	computer vision;geography;optics;computer graphics (images)	HCI	62.538565300869976	-62.305355079940554	194494
23436f1b56363bb3c47dfe8e86ff69f1a2d8675a	fusion of hyperspectral and panchromatic images using multiresolution analysis and nonlinear pca band reduction	geophysical image processing;nonlinear pca band reduction;high resolution;image resolution;neural nets;image fusion;nonlinear principal component analysis;spatial quality;spatial resolution hyperspectral imaging principal component analysis indexes image fusion;indexes;image enhancement;pansharpening technique;dimensionality reduction;principal component analysis;indexation;panchromatic image;principal component analysis geophysical image processing image enhancement image fusion image resolution neural nets;autoassociative neural network;hyperspectral imaging;nlpca;multiresolution analysis;dimensional reduction;hyperspectral image;pansharpening;autoassociative neural network image fusion hyperspectral image panchromatic image multiresolution analysis nonlinear pca band reduction image enhancement spatial quality pansharpening technique dimensionality reduction nonlinear principal component analysis;image fusion neural network nlpca hyperspectral image pansharpening;neural network;spatial resolution	This paper presents a novel method for the enhancement of spatial quality of Hyperspectral (HS) images while making use of a high resolution panchromatic (PAN) image. Due to the high number of bands the application of a pansharpening technique to HS images may result in an increase of the computational load and complexity. Thus a dimensionality reduction preprocess, compressing the original number of measurements into a lower dimensional space, becomes mandatory. To solve this problem we propose a pansharpening technique combining both dimensionality reduction and fusion, exploited by non-linear Principal Component Analysis (NLPCA) and Indusion respectively, to enhance the spatial resolution of a hyperspectral image.	dimensionality reduction;image resolution;multiresolution analysis;nonlinear system;preprocessor;principal component analysis	Giorgio Licciardi;Muhammad Murtaza Khan;Jocelyn Chanussot;Annick Montanvert;Laurent Condat;Christian Jutten	2011	2011 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2011.6049466	computer vision;image resolution;computer science;machine learning;pattern recognition;artificial neural network;remote sensing	Robotics	67.80757274769987	-65.04674970972403	194614
ca1f0ff54a4e5408233b80119cafeb6cc648e72b	transformation-grounded image generation network for novel 3d view synthesis		We present a transformation-grounded image generation network for novel 3D view synthesis from a single image. Our approach first explicitly infers the parts of the geometry visible both in the input and novel views and then casts the remaining synthesis problem as image completion. Specifically, we both predict a flow to move the pixels from the input to the novel view along with a novel visibility map that helps deal with occulsion/disocculsion. Next, conditioned on those intermediate results, we hallucinate (infer) parts of the object invisible in the input image. In addition to the new network structure, training with a combination of adversarial and perceptual loss results in a reduction in common artifacts of novel view synthesis such as distortions and holes, while successfully generating high frequency details and preserving visual aspects of the input image. We evaluate our approach on a wide range of synthetic and real examples. Both qualitative and quantitative results show our method achieves significantly better results compared to existing methods.	3d reconstruction;autostereogram;cg artist;command & conquer: tiberian sun;distortion;encoder;generative modelling language;glossary of computer graphics;graphics processing unit;ground truth;ibm notes;motion estimation;pixel;robustness (computer science);synthetic intelligence;type conversion;view synthesis;wheels	Eunbyung Park;Jimei Yang;Ersin Yumer;Duygu Ceylan;Alexander C. Berg	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.82	view synthesis;artificial intelligence;computer vision;visibility;pixel;pattern recognition;machine learning;computer science;distortion;hallucinate;solid modeling	Vision	55.434994852560955	-53.92313124439791	194669
c0ac1c20ad10d4fab0ae589bd290dcaffddad986	a new algorithm for objective video quality assessment on eye tracking data	measurement;gaze tracking;visualization;quality assessment;multimedia communication;video recording;algorithm design and analysis	In this paper, we present an innovative algorithm based on a voting process approach, to analyse the data provided by an eye tracker during tasks of user evaluation of video quality. The algorithm relies on the hypothesis that a lower quality video is more “challenging” for the Human Visual System (HVS) than a high quality one, and therefore visual impairments influence the user viewing strategy. The goal is to generate a map of saliency of the human gaze on video signals, in order to create a No Reference objective video quality assessment metric. We consider the impairment of video compression (H.264/AVC algorithm) to generate different versions of video quality. We propose a protocol that assigns different playlists to different user groups, in order to avoid any effect of memorization of the visual stimuli on strategy. We applied our algorithm to data generated on a heterogeneous set of video clips, and the final result is the computation of statistical measures which provide a rank of the videos according to the perceived quality. Experimental results show that there is a strong correlation between the metric we propose and the quality of impaired video, and this fact confirms the initial hypothesis.	algorithm;computation;data compression;display resolution;experiment;eye tracking;h.264/mpeg-4 avc;human body weight;human visual system model;lossy compression;mobile device;user experience;video clip;xslt/muenchian grouping	Maria Grazia Albanesi;Riccardo Amadeo	2014	2014 International Conference on Computer Vision Theory and Applications (VISAPP)	10.5220/0004672104620469	subjective video quality;algorithm design;computer vision;simulation;visualization;computer science;video quality;video tracking;block-matching algorithm;multimedia;motion compensation;measurement	Vision	63.662677196064095	-63.63124359158623	194672
0ed1d2ae33f7c747de7383a10203a4c3e74d8b40	super resolution results in panoptes, an adaptive multi-aperture folded architecture	panoptes;optical system;image resolution;cost function;adaptive multi aperture folded architecture;data collection;multi aperture miniature folded imaging;low resolution;form factor;image resolution image reconstruction computer architecture optical imaging robustness cost function;computer architecture;optical imaging;adaptive signal processing;image reconstruction;srum;super resolution;robustness;post processing step;image resolution adaptive signal processing;super resolution with unsharpenning mask;digital super resolution;post processing step panoptes adaptive multi aperture folded architecture digital super resolution multi aperture miniature folded imaging optical system srum super resolution with unsharpenning mask cost function	We present experimental results of digital super resolution (DSR) techniques on low resolution data collected using PANOPTES, a multi-aperture miniature folded imaging architecture. The flat form factor of PANOPTES architecture results in an optical system that is heavily blurred with space variant PSF which makes super resolution challenging. We also introduce a new DSR method called SRUM (Super-Resolution with Unsharpenning Mask) which can efficiently highlight edges by embedding an unsharpenning mask to the cost function. This has much better effect than just applying the mask after all iterations as a post-processing step.	ansi escape code;image resolution;iteration;loss function;super-resolution imaging;video post-processing	Esmaeil Faramarzi;Vikrant R. Bhakta;Dinesh Rajan;Marc P. Christensen	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5652102	computer vision;image resolution;computer science;computer graphics (images)	Robotics	59.06416316842344	-58.091005261980094	194709
fa32c381004d05a1327051ad57a0eaf988d13120	evolved gate arrays for image restoration	evolutionary computation;reconfigurable architectures;evolvable hardware;image restoration;chip;image corruption gate arrays image restoration fitness function on chip solution on chip evolvable hardware phenotypic evolution image recovery;image restoration degradation nonlinear filters signal restoration psnr signal processing optical noise filtering hardware working environment noise;field programmable gate arrays;reconfigurable architectures image restoration evolutionary computation field programmable gate arrays;fitness function	Evolved gate arrays with a proposed fitness function, are considered for image restoration. In this paper we proposed an on-chip solution for image restoration using an on-chip evolvable hardware method. The corrupted image is considered to be the environment of evolvable hardware structures, which are restoring the original through phenotypic evolution. We compare our solution with some classical techniques for image restoration.	circuit restoration;evolvable hardware;fitness function;image restoration	Adrian Burian;Jarmo Takala	2004	Proceedings of the 2004 Congress on Evolutionary Computation (IEEE Cat. No.04TH8753)	10.1109/CEC.2004.1330996	chip;image restoration;computer vision;computer science;artificial intelligence;theoretical computer science;machine learning;fitness function;field-programmable gate array;evolutionary computation	Vision	55.601254417939344	-64.97818013595362	194988
4dd4f0a54d71a067cf1c84c6a97c97348a50c00c	blind image quality assessment for gaussian blur images using exact zernike moments and gradient magnitude		Features that exhibit human perception on the effect of blurring on digital images are useful in constructing a blur image quality metric. In this paper, we show some of the exact Zernike moments (EZMs) that closely model the human quality scores for images of varying degrees of blurriness can be used to measure these distortions. A theoretical framework is developed to identify these EZMs. Together with the selected EZMs, the gradient magnitude (GM), which measures the contrast information, is used as a weight in the formulation of the proposed blur metric. The design of the proposed metric consists of two stages. In the first stage, the EZM differences and the GM dissimilarities between the edge points of the test image and the same re-blurred image are extracted. Next, the mean of the weighted EZM features are then pooled to produce a quality score using support vector machine regressor (SVR). We compare the performance of the proposed blur metric with other state-of-the-art full-reference (FR) and no-reference (NR) blur metrics on three benchmark databases. The results using Pearson's correlation coefficient (CC) and Spearman's rankedorder correlation coefficient (SROCC) for the LIVE image database are 0.9659 and 0.9625 respectively. Similarly, high correlations with the subjective scores are achieved for the other two databases as well. & 2016 The Franklin Institute. Published by Elsevier Ltd. All rights reserved. rg/10.1016/j.jfranklin.2016.08.012 2016 The Franklin Institute. Published by Elsevier Ltd. All rights reserved. ding author. dresses: limchernloon@gmail.com (C.-L. Lim), ravee@um.edu.my (R. Paramesran), yahoo.com (W.A. Jassim), richieyyp@yahoo.com (Y.-P. Yu), knngan@ee.cuhk.edu.hk (K.N. Ngan). C.-L. Lim et al. / Journal of the Franklin Institute 353 (2016) 4715–4733 4716	benchmark (computing);box blur;coefficient;database;digital image;distortion;encode;exact cover;expanded memory;franklin electronic publishers;gaussian blur;gradient;image quality;noise reduction;standard test image;support vector machine	Chern-Loon Lim;Raveendran Paramesran;Wissam A. Jassim;Yong-Poh Yu;King Ngi Ngan	2016	J. Franklin Institute	10.1016/j.jfranklin.2016.08.012	computer vision;pattern recognition;mathematics;statistics	Vision	61.27476024979255	-65.45635192586116	195008
5bdae41d1daec604c57c6b067c1160faae052fc2	height and gradient from shading	photoclinometry;shape from shading;perforation;exact solution;depth and slope;turn off;digital elevation model;boundary condition;digital systems;smoothness;calculus of variation;digital elevation models;variational method;synthetic data;shape fromsshading;height function;variational methods;singular point;elevation gradient	The method described here for recovering the shape of a surface from a shaded image can deal with complex, wrinkled surfaces. Integrability can be enforced easily because both surface height and gradient are represented. (A gradient field is integrable if it is the gradient of some surface height function.) The robustness of the method stems in part from linearization of the reflectance map about the current estimate of the surface orientation at each picture cell. (The reflectance map gives the dependence of scene radiance on surface orientation.) The new scheme can find an exact solution of a given shape-from-shading problem even though a regularizing term is included. The reason is that the penalty term is needed only to stabilize the iterative scheme when it is far from the correct solution; it can be turned off as the solution is approached. This is a reflection of the fact that shape-from-shading problems are not ill posed when boundary conditions are available, or when the image contains singular points. This article includes a review of previous work on shape from shading and photoclinometry. Novel features of the new scheme are introduced one at a time to make it easier to see what each contributes. Included is a discussion of implementation details that are important if exact algebraic solutions of synthetic shape-from-shading problems are to be obtained. The hope is that better performance on synthetic data will lead to better performance on real data.	gradient;iteration;iterative method;linear algebra;photometric stereo;shading;synthetic data;synthetic intelligence;well-posed problem	Berthold K. P. Horn	1990	International Journal of Computer Vision	10.1007/BF00056771	mathematical optimization;topology;digital elevation model;mathematics;geometry	Vision	55.67154435972336	-52.203417838544546	195123
eaa6ab0151bdb987b54ec67a7e73114bad96a8e9	color recovery from biased illumination: color constancy	image processing;information science;reflectivity;biased illumination;color constancy;color;image processing colour lighting reflectivity spectral analysis;colour recovery;ambient light colour recovery biased illumination color constancy finite dimensional model homomorphic model statistical model surface reflectance spectral distribution;distributed computing;homomorphic model;layout;power distribution;statistical model;surface reflectance;eyes;ambient light;lighting solid modeling reflectivity color humans power distribution eyes distributed computing information science layout;solid modeling;humans;lighting;spectral analysis;finite dimensional model;spectral distribution;colour	An algorithm for achieving color constancy is presented. The algorithm consists of four models: the finite-dimensional model, the homomorphic model, the statistical model, and the recovery model. From them, it is possible to estimate the surface reflectance, even when the spectral distribution of the ambient light is unknown. Thus color constancy can be achieved under biased illumination. To prove the correctness of the authors' algorithm, some experiments are conducted under different illuminative conditions. >	global illumination	Po-Wie Hwang;Yung-Sheng Chen;Fang-Hsuan Cheng;Wen-Hsing Hsu	1993		10.1109/CVPR.1993.341053	spectral power distribution;layout;statistical model;computer vision;color normalization;image processing;information science;computer science;lighting;reflectivity;solid modeling;color constancy;available light;statistics	Vision	55.680221304202576	-52.88026733197338	195141
e37c8b2e547eb4f0a8af0aba2fed282f2fabcb10	"""response to the comments on """"fundamental limits of reconstruction-based superresolution algorithms under local translation'"""	strontium upper bound image resolution;image resolution;reconstruction based superresolution algorithms;perturbation theory;perturbation theory superresolution reconstruction based algorithms;perturbation techniques;strontium;upper bound;perturbation theorem;local translation;perturbation theorem fundamental limits reconstruction based superresolution algorithms local translation;algorithm theory;machine intelligence;boundary value problems;superresolution;perturbation techniques boundary value problems algorithm theory;fundamental limits;reconstruction based algorithms	Wang and Feng (IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 28, no. 5, p 846, May 2006) pointed out that the deduction in (Z. Lin and H. Y. Shum, IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 26, no. 1, pp. 83-97, Jan. 2004) overlooked the validity of the perturbation theorem used in (Z. Lin and H. Y. Shum, IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 26, no. 1, pp. 83-97, Jan. 2004). In this paper, we show that, when the perturbation theorem is invalid, the probability of successful superresolution is very low. Therefore, we only have to derive the limits under the condition that validates the perturbation theorem, as done in (Z. Lin and H. Y. Shum, IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 26, no. 1, pp. 83-97, Jan. 2004).	algorithm;super-resolution imaging	Zhouchen Lin;Harry Shum	2006	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/TPAMI.2006.105	mathematical optimization;image resolution;strontium;boundary value problem;computer science;calculus;perturbation theory;mathematics;geometry;upper and lower bounds;superresolution	Vision	54.11592296711954	-52.1294118339208	195455
45fbf3883766542cd8814e945617d0f64e54dc74	improved seam carving for image retargeting with sift feature preservation	energy resolution;dynamic programming;marine animals;image coding;image resolution;backtracking basis seam carving image retargeting sift feature preservation image resizing method scale invariant feature transform;shape;transforms image processing;sift image resizing seam carving;image resolution marine animals shape energy resolution dynamic programming image coding asia;asia	This paper presents an effective and simple image resizing method. Our method is an improved version of seam carving that changes the backtracking basis of seam carving. We use a Scale Invariant Feature Transform (SIFT) feature in our method. SIFT key points are mainly located on high-contrast regions of an image. By using saliency considering SIFT as our backtracking basis, a resized image can preserve the SIFT features of the original image. Therefore, the resized image can be more visually acceptable than the image resized by traditional seam carving.	backtracking;image scaling;retargeting;scale-invariant feature transform;seam carving	Ke Li;Qianxu Zeng;Bahetiyaer Bare;Bo Yan;Hamid Gharavi	2015	2015 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP)	10.1109/ChinaSIP.2015.7230489	seam carving;computer vision;feature detection;pattern recognition;mathematics;computer graphics (images)	Vision	56.67318704700089	-61.65911970607103	195467
1e832f838238883f437b49199338d7b5eddff5e8	noisy mpeg motion vector reduction for motion analysis	motion analysis;motion compensation;zero comparison;motion estimation;transform coding;data mining;noise measurement;motion compensated;manganese;video coding;object detection mpeg motion vector reduction motion compensation motion estimation interframe compression motion information;motion information;motion vector;noise reduction;interframe compression;moving object detection;video coding motion compensation motion estimation;motion analysis noise reduction motion compensation videos transform coding mpeg standards decoding motion estimation object detection tracking;mpeg motion vector reduction;mpeg standards;correlation;moving object detection motion vector mpeg standards noise reduction global motion zero comparison;reduction method;global motion;global motion estimation;cameras;object detection	In MPEG standards, motion compensation is used for inter-frame compression. Motion compensation generates motion vectors in order to predict the current frame regions from previously decoded frames. These motion vectors represent motion information between regions in different frames and are useful in motion analysis. However, the motion vectors of homogeneous, low-textured, and line regions tend to be unstable and noisy; therefore, it becomes difficult to conduct motion analysis. In this paper, we propose a noisy motion vector reduction method for motion analysis using MPEG motion vectors by introducing a global motion estimation method and a zero-comparison method. The proposed method outputs local motion vectors that are free of noise and high-quality global motion vectors that can be used for object detection, tracking, etc. Further, this method works even for videos captured by a moving camera. We demonstrate the effectiveness of the proposed method through several experiments using actual videos.	control theory;experiment;motion compensation;motion estimation;moving picture experts group;object detection;preprocessor;video content analysis	Takanori Yokoyama;Shuhei Ota;Toshinori Watanabe	2009	2009 Sixth IEEE International Conference on Advanced Video and Signal Based Surveillance	10.1109/AVSS.2009.42	computer vision;match moving;structure from motion;transform coding;speech recognition;quarter-pixel motion;computer science;noise measurement;motion interpolation;manganese;motion estimation;noise reduction;block-matching algorithm;motion field;motion compensation;correlation;computer graphics (images)	Vision	55.70704212994213	-55.088724309493294	195477
441f1bd08100f5da94d40f13fe482796c0aa92ed	sequential sampling for dynamic environment maps	dynamic environment;expressive imagery;non realistic modeling;sequential sampling	Sampling of complex direct illumination in the form of high dynamic range (HDR) environment maps has recieved a lot of attention in computer graphics, with major applications in realistic relighting. The best known techniques for direct illumination sample from the product of the incident illumination and the surface reflectance [ Burke et al. 2005 ]. Recent work on sampling from HDR video sequences only take the dynamic importance of the illumination into account while proposing samples for the video sequence. This can be sub-optimal in the presence of high frequencies in both the illumination as well as the surface BRDFs. In this work, we aim to efficiently sample from the product distribution of the illumination and the BRDF in a video sequence with dynamic illumination using asequential Monte Carlo (SMC) sampling strategy. The basic idea is to generate samples according to the product distribution in the first frame of the sequence, and thereafter to filter these samples (particles) in the subsequent frames according to the dynamic product distribution. This sequential sampling mechanism is more efficient than independently sampling from the product distribution at each time step (Figure 1), especially for scenes with high frequencies in both the dynamic illumination and the BRDF. We employ bidirectional importance sampling for generating samples in the first frame of the sequence.	bidirectional reflectance distribution function;computer graphics;high dynamic range;illumination (image);importance sampling;map;monte carlo method;sampling (signal processing);william l. burke	Abhijeet Ghosh;Arnaud Doucet;Wolfgang Heidrich	2006		10.1145/1179849.1180045	computer vision;simulation;sequential analysis;statistics	Graphics	63.619321328572106	-53.32818333759618	195699
9aca754bbce3894e32d05807da06d44dd22d7029	removing rolling shutter distortion using optical flow and ransac	optical distortion;cmos integrated circuits;computer vision;smart phones cmos image sensors image sequences motion estimation optical distortion optical elements sensor arrays;arrays;optical imaging;cameras cmos integrated circuits arrays optical distortion computer vision videos optical imaging;cameras;videos;smartphone rolling shutter distortion optical flow camera motion estimation algorithm ransac global motion vector sensor array cmos camera modules	CMOS camera modules are used for smartphone. But CMOS camera modules have a problem of rolling shutter effect. In this paper, we propose an efficient method of removing the rolling shutter effect. We first compute the global motion vector by using the optical flow and the camera motion estimation algorithm. Then we estimate the velocity for each row of sensor array. Finally the distorted image is realigned. Experimental results show that the proposed method is more robust and faster than the previous method.	algorithm;camera module;distortion;motion estimation;movie projector;optical flow;random sample consensus;smartphone;velocity (software development)	Woo Jin Jeong;Young Shik Moon	2015	2015 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2015.7066435	computer vision;electronic engineering;computer science;optical imaging;image sensor;cmos sensor;mathematics;cmos;computer graphics (images)	Robotics	60.07674725497966	-57.515784820979	195814
7a69a93fd1129ab1f8f533c93e9ea5bd851c2010	speeding-up differential motion detection algorithms using a change-driven data flow processing strategy	image processing;deata flow processing;real time implementation;data flow;motion detection	A constraint of real-time implementation of differential motion detection algorithms is the large amount of data to be processed. Full image processing is usually the classical approach for these algorithms: spatial and temporal derivatives are calculated for all pixels in the image despite the fact that the majority of image pixels may not have changed from one frame to the next. By contrast, the data flow model works in a totally different way as instructions are only fired when the data needed for these instructions are available. Here we present a method to speed-up low level motion detection algorithms. This method is based on pixel change instead of full image processing and good speed-up is achieved.	algorithm;dataflow architecture	Jose Antonio Boluda;Fernando Pardo	2007		10.1007/978-3-540-74272-2_10	data flow diagram;computer vision;feature detection;image processing;computer science;theoretical computer science;digital image processing;motion estimation;motion field;computer graphics (images)	ML	58.85088356169735	-56.899280069967006	195883
b556fc7d1749b6d571e7a7df70d8096c58038ebc	spatial modelling of multi-layered lidar images using reversible jump mcmc	3d imaging;photon counting;spatial modelling;markov random field;field of view;reversible jump markov chain monte carlo;birth death process;reversible jump mcmc	3D imaging LiDAR systems have the potential to acquire multi -layered 3D image data; that is rather than store a single depth value at e ach pixel, it is possible to store the range to more than one surface within the pi xel v ew direction. Multiple returns are possible at a single pixel when imaging through transparent surfaces, for example when acquiring depth images of cars or buildings that have windows, in which case it is possible to record both external a d internal structure. Multiple returns are also possible when the pixel field of view encompasses more than one opaque surface. However, to build such multi-l ayered 3D images, we need to think of new ways of processing the LiDAR data. In this paper, we present a unified theory of pixel processing for such data. This is based on a reversible jump Markov chain Monte Carlo (R JMCMC) methodology extended to include spatial constraints by a Mark ov Random Field with a Potts prior model. We consider two distinct proposal d istributions, based on spatial mode jumping and spatial birth/death processes r espectively. We also include a delayed-rejection step in the RJMCMC algorithm to improve the estimates of the range and reflectance of each surface element. Ou r methodology is demonstrated on both photon count and burst illumination Li DAR data.	3d reconstruction;algorithm;bil herd;burst error;clutter;glossary of computer graphics;markov random field;microsoft windows;monte carlo method;pixel;potts model;rejection sampling;reversible-jump markov chain monte carlo;turbulence	Sergio Hernandez-Marin;Andrew M. Wallace;Gavin J. Gibson	2007		10.5244/C.21.7	stereoscopy;computer vision;simulation;field of view;mathematics;birth–death process	Vision	63.85527806276992	-52.64231507688475	195951
fd3b725cc7762508a0014e4324f3049148080a4c	image fusion in the context of aerospace applications	image fusion	This issue includes a special section on image fusion in the context of aerospace applications. I would like to take this opportunity to acknowledge the assistance of Dr. Sheela V. Belur who contributed as a Guest Editor in putting together this special section. Since the review process whittled down the number of papers (submitted in response to the call for papers for the special issue) acceptable for publication to just four, we are also including two additional papers that fall under the regular issue banner. Scanning the contributions appearing in this issue, the first four papers represent the special section on aerospace applications that concentrates on image data fusion related topics. The first of these is a survey paper on image fusion techniques employed in the context of remote sensing applications. It spans three types of remote sensing applications: synthetic aperture radar (SAR) Interferometers, multi-sensor/multi-temporal (Landsat, Thematic Mapper, and SAR) images, and multi-frequency/multi-polarization/multi-resolution SAR images. The second paper deals specifically with the problem fusion of Landsat TM and SPOT Panchromatic images and addresses it using discrete wavelet transforms. The third paper offers two fusion techniques in the context of super-resolution image generation from a sequence of lowresolution image data. The last paper in the special section discusses the role of invariant algebra in the context of multi-spectral information fusion. It presents a probability density function technique that employs invariant algebra to derive algebraic expressions that remain constant under an object’s joint geometrical and surface material changes. In closing, it is desirable to point out that many of the fusion techniques covered here by these four studies are not necessarily restricted to aerospace applications only. The first of the two regular section papers addresses the problem of robotic map building. Towards this end, a new concept referred to as pseudo information measure is proposed which aids the Bayesian fusion of independent information sources. The second paper describes the development of a fusion system based on the uncertainty approach utilizing an extension of the Choquet fuzzy integral called the generalized Choquet fuzzy integral, GCFI. The methodology is illustrated with an application to the land mine detection problem. Continuing our in-a-lighter-vein-series on ‘FUN in FUsioN’, we present as a final piece a crossword puzzle wherein most, if not all, the words are at least loosely connected to information fusion and related technical topics. We continue to hope for and welcome feedback from the readership irrespective of whether they are bouquets or brickbats on this as well as other aspects of the Journal. We would also be receptive to suggestions from the readership regarding suitable topics for special issues of the Journal, a couple of which are currently in the works.	aperture (software);closing (morphology);crossword;discrete wavelet transform;glossary of computer graphics;image fusion;linear algebra;polarization (waves);remote sensing application;robot;super-resolution imaging;synthetic data;uncertainty principle;wario land: shake it!	Belur V. Dasarathy	2002	Information Fusion	10.1016/S1566-2535(02)00045-3	computer science;image fusion	Robotics	65.02791828997057	-56.04971234334546	195988
49c3985b7aba3c0498cad62f2b832dd5de699915	low-cost high-resolution visualization system for scientific images and simulations.	high resolution;visual system			Jorge V. Sánchez Compte;Xavier Ochoa Chehab	2005			computer vision	HPC	63.16954147805229	-56.75735569881069	196217
959d9f2680aae68b85fc4b466c623d6223a8b88c	fpga implementation of a real-time super-resolution system with a cnn based on a residue number system		A super-resolution technology is used for filling the gap between high-resolution displays and lower-resolution images. One of various algorithms to interpolate the lost information is to use a convolutional neural network (CNN). This paper shows an FPGA implementation and a performance evaluation of our CNN-based super-resolution system, which can process moving images in real time. We apply horizontal and/or vertical flips to input images instead of pre-enlargement. This method prevents information loss and enables the network to make the best use of its input size. In addition, we adopted the residue number system (RNS) to reduce resource utilization. The proposed system can perform super-resolution from 960×540 to 1920×1080 at 60fps with a latency of less than 1ms. In spite of resource restriction of the FPGA, the system generates clear super-resolution images with smooth edges. The evaluation results also revealed the superior quality in terms of the peak signal-to-noise ratio (PSNR), compared to other systems using pre-enlargement.	algorithm;artificial neural network;convolutional neural network;field-programmable gate array;graphics display resolution;image resolution;information;interpolation;peak signal-to-noise ratio;performance evaluation;real-time clock;residue number system;super-resolution imaging	Taito Manabe;Yuichiro Shibata;Kiyoshi Oguri	2017	2017 International Conference on Field Programmable Technology (ICFPT)	10.1109/FPT.2017.8280165	convolutional neural network;parallel computing;latency (engineering);real-time computing;interpolation;field-programmable gate array;artificial neural network;computer science;spite;image resolution;residue number system	Robotics	66.23552576991975	-53.996258429848965	196219
1d260be61af10ad5436734253037f719d64a4619	when product designers use perceptually based color tools	vision system;systeme temps reel;image processing;systeme vision;vision color;procesamiento imagen;traitement image;vision couleur;sistema cognitiva;real time system;systeme cognitif;sistema tiempo real;cognitive system;imagerie visuelle;color vision;imagineria visual;visual imagery	Palette synthesis and analysis tools have been built based upon a model of color experience. This model adjusts formal compositional elements such as hue, value, chroma, and their contrasts, as well as size and proportion. Clothing and household product designers were given these tools to give guidance to their selection of seasonal palettes for use in production of the private-label merchandise of a large retail chain. The designers chose base palettes. Accents to these palettes were generated with and without the aid of the color tools. These palettes are compared by using perceptual metrics and interviews. The results are presented.© (1998) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.		Walter Bender	1998		10.1117/12.320129	mental image;computer vision;image processing;color vision	HCI	62.74706749644741	-62.05204014906847	196225
09f0a6f92896781a23c1450ee06149ae15834f05	full-reference image quality assessment measure based on color distortion		The purpose of this paper is to introduce a new method for image quali- ty assessment (IQA). The method adopted here is assumed to be Full-reference measure. Color images that are corrupted with different kinds of distortions are assessed by applying a color distorted algorithm on each color component sepa- rately. This approach use especially YIQ color space in computation. Gradient operator was successfully introduced to compute gradient image from the lumin- ance channel of images. In this paper, we propose an alternative technique to eva- luate image quality. The main difference between the new proposed method and the gradient magnitude similarity deviation (GMSD) method is the usage of color component for the detection of distortion. Experimental comparisons demonstrate the effectiveness of the proposed method.	distortion;image quality	Zianou Ahmed Seghir;Fella Hachouf	2015		10.1007/978-3-319-19578-0_6	computer vision;mathematics;optics;engineering drawing	Vision	61.20892271709197	-64.0197310408507	196237
0aea35acf11304613cabfc2b80b8c28ba1513637	dct-based edge detector for snapshot images	image sampling;biomedical monitoring;detectors;image coding;objective psnr criterion snapshot images dct based edge detector discrete cosine transform decimation image partitioning nonoverlapping blocks block size decimation factor dct ac coefficient variances directional edge map directional ac coefficients vertical direction horizontal direction diagonal direction;image segmentation;image resolution;directional edge map;image resolution edge detection image coding discrete cosine transforms image segmentation;signal sampling;edge detection;decimation factor;decimation;discrete cosine transform;image edge detection detectors discrete cosine transforms ac generators frequency image resolution image sampling biomedical monitoring low pass filters signal sampling;ac generators;dct ac coefficient variances;dct based edge detector;objective psnr criterion;image edge detection;discrete cosine transforms;vertical direction;image partitioning;nonoverlapping blocks;low pass filters;frequency;snapshot images;horizontal direction;diagonal direction;block size;directional ac coefficients	In this paper, we propose a discrete cosine transform (DCT)-based edge detector for snapshot images that are obtained by performing decimation (by a factor of two or more) on a much larger original image. We first partition the original image into non-overlapping blocks, where the block size equals the decimation factor, and then perform DCT on each block. The variances of the DCT's AC coefficients are used to decide the edge map of snapshot images. A directional edge map of the snapshot images can also be generated if the variance of the directional AC coefficients in the vertical, horizontal, or diagonal direction is used. Simulation results show that the proposed DCT-based edge detector performs better (based on an objective PSNR criterion) than the conventional technique - applying an edge detector on the down-sampled image.	discrete cosine transform;edge detection;snapshot (computer storage)	Mon Wei Wu;C. J. Kuo	2002		10.1109/ISCAS.2002.1010560	computer vision;mathematical optimization;detector;electronic engineering;decimation;edge detection;image resolution;low-pass filter;computer science;frequency;block size;discrete cosine transform;mathematics;vertical direction;image segmentation	Vision	57.79346026310989	-64.62549552948583	196250
54142d7aa7c265f260b997b8dec0377d483d0cf5	fuzzy-based parameterized gaussian edge detector using global and local properties	gaussian edge detector;optimisation;image representation edge detection image enhancement gaussian processes optimisation;gaussian processes;edge detection;detectors image edge detection filters pixel image processing information technology gray scale gaussian processes phase detection entropy;image enhancement;contrast intensification;image representation;image quality;membership function;entropy optimization;gaussian membership function;image representation gaussian edge detector image quality grayscale images gaussian membership function contrast intensification function fuzzy image entropy optimization gradient descent technique image enhancement;entropy optimization image enhancement gaussian edge detector gaussian membership function contrast intensification	Many edge detection schemes suffer from the lack of image quality at the global level. Global properties are more vital in grayscale images due to loss of hue and texture. This paper proposes a novel fuzzy-based Gaussian edge detector that uses both global and local image properties for grayscale images. In the global contrast intensification phase, each pixel in an image is represented in the fuzzy domain using a modified Gaussian membership function. A nonlinear contrast intensification function containing three parameters is used to further enhance the image. In the local phase, we present a novel fuzzy parameterized Gaussian-type edge detector mask containing two fuzzifier parameters, which are chosen based on experimental selection rules. Optionally, the fuzzy image entropy function can be used to optimize all the parameters through simple gradient descent technique. In experiments conducted on various classic images, this algorithm showed notable visual improvement on both strong and weak edges in comparison with common edge detectors.	algorithm;distortion;edge detection;experiment;fingerprint recognition;gradient descent;grayscale;image quality;mathematical optimization;nonlinear system;performance evaluation;pixel;selection rule;sensor	John See;Madasu Hanmandlu;Shantaram Vasikarla	2005	International Conference on Information Technology: Coding and Computing (ITCC'05) - Volume II	10.1109/ITCC.2005.158	image texture;computer vision;feature detection;scale space;edge detection;image gradient;morphological gradient;machine learning;difference of gaussians;gaussian blur;pattern recognition;mathematics;canny edge detector;gaussian function	Vision	56.07467523231568	-64.46594931070875	196437
a99982279c88daa69b0de81a6c21b8e9524f3c91	quality assessment of watermarked 3d polygonal models	forma libre;filigranage numerique;protection information;digital watermarking;evaluation performance;performance evaluation;image processing;modelo 3 dimensiones;imagen fija;quality measurement;modele 3 dimensions;digital watermark;evaluacion prestacion;free form;procesamiento imagen;three dimensional model;qualite image;visual quality;traitement image;predictor;algorithme;algorithm;predicteur;forme libre;quality assessment;evaluation subjective;fixed image;proteccion informacion;information protection;image quality;filigrana digital;controle qualite;free form deformation;transparency;image fixe;calidad imagen;quality control;subjective evaluation;control calidad;evaluacion subjetiva;algoritmo	In this paper, we present the design and results of subjective tests for evaluating the perceptibility of digital watermarks in 3D polygonal models. Based on the results we investigate different types of metrics with respect to their usefulness as predictors for the perceived visual quality of models that have been modified using a specific watermarking algorithm. We describe two experiments with models that have been watermarked using controlled free form deformations. The first experiment was conducted in supervised mode with still images of rendered models as stimuli and used the Two Alternative Forced Choice (2AFC) method. The second experiment was based on animated sequences and run in 2AFC mode with additional ratings of the perceived differences, but without assistance by the experimenter. We present a transparency analysis of the results and investigate the ability of image-based and geometry-based metrics to predict the perceived quality of the watermarked models. Our results show that the effectiveness of prediction depends on the type of model and in particular on the feature positions selected by the watermarking algorithm. The results of previous experiments with simplified polygonal models are confirmed, in that geometric measures are good predictors of quality ratings. We found that the symmetric Haussdorf distance is a promising candidate to evaluate the visual impact of the watermarking algorithm used in our experiments.	watermark (data file)	Wolfgang Funk;Jennifer Prasiswa	2006		10.1117/12.643510	computer vision;simulation;image processing;digital watermarking;electrical engineering	HCI	62.42345700074837	-62.76609933424309	196480
e21424c546c638e1386d51cc420840c104d816cc	exploring ink spreading	integration;color image processing;approximation theory;color reproduction;ink;prediction model;calibration;color printing	This study aims at exploring ink spreading, which causes significant colour deviations in ink-jet printing. We present a method for investigating this phenomenon by considering only a limited number of cases. Using a combinatorial approach based on Pólya’s counting theory, we determine a small set of ink drop configurations which allows to deduce the ink spreading in all other cases. This improves the estimation of the area covered by each ink combination which is crucial in colour prediction models. Such models simplify the calibration of ink-jet printers.	printing;pólya enumeration theorem	Patrick Emmel;Roger D. Hersch	2000			computer vision;calibration;computer science;machine learning;mathematics;predictive modelling;approximation theory;computer graphics (images)	Vision	64.87297681933718	-59.16767236252684	196674
98dcbf9b05df95ac8f5200b8810925b574331a6c	local detail enhanced hyperspectral image visualization	image resolution;color;laplacian pyramid hyperspectral image visualization multi resolution multidimensional scaling;laplace equations;image color analysis;optimisation geophysical image processing hyperspectral imaging image colour analysis image resolution;data visualization;spatial distribution enhanced hyperspectral image visualization pairwise distance preservation sequential steps high dimensional spectral space 3d color space distance preservation method laplacian pyramid optimization problem pairwise pixel distances multiresolution multidimensional scaling algorithm spatial distinction color image hsi visualization method global information local information spectral distribution;hyperspectral imaging;image color analysis laplace equations data visualization hyperspectral imaging color image resolution	A new method for hyperspectral image (HSI) visualization is proposed in this paper, which emphasizes on pairwise distance preservation and detail enhancement. It includes two sequential steps. The first reduces the high dimensional spectral space of HSI to 3-D color space by distance preservation method, and then enhancesthe detailed information by Laplacian pyramid. Distance preservation is an optimization problem that minimizes the difference between the pairwise pixel distances in the original spectral space and the corresponding color space. In general, solving this optimization problem is always very time and storage consuming. A multi-resolution multidimensional scaling algorithm is proposed in this paper to mitigate this hardness. Obviously the loss of some local details is not avoided in multidimensional scaling. In order to enhance the spatial distinction of different scene objects, Laplacian pyramid is used to draw the locally details from the original HSI, and embed it into the color image. The proposed HSI visualization method takes the global and local information of spectral and spatial distribution in HSI into account for visualization, which makes the color display of HSI carry as much original information as possible.	algorithm;color image;color space;horizontal situation indicator;image scaling;mathematical optimization;multidimensional scaling;optimization problem;pixel;pyramid (geometry)	Jialu Fang;Yuntao Qian	2015	2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2015.7325960	computer vision;image resolution;color image;computer science;hyperspectral imaging;machine learning;mathematics;data visualization;physics;remote sensing;computer graphics (images)	Visualization	57.905445418126554	-61.90341775834224	196737
aeab314b52045dc56c92120471eae9484e896b89	low complexity optical flow using neighbor-guided semi-global matching	optical distortion;fsgm;nonlinear optics;image motion analysis;complexity theory;sgm;computer vision image motion analysis optical imaging complexity theory optical distortion nonlinear optics optimization;low complexity;computer vision;optical imaging;optimization;optical flow;low complexity optical flow sgm fsgm	This paper presents Neighbor-Guided SemiGlobal Matching (NG-fSGM), a new method for optical flow. It is based on SGM, a popular dynamic programming algorithm for stereo vision, where the disparity of each pixel is calculated by aggregating local matching costs over the entire image to resolve local ambiguity in texture-less and occluded regions. Unlike conventional SGM, NG-fSGM operates on a subset of the search space that has been aggressively pruned based on neighboring pixels' information. Our proposed method achieves a fast approximation of SGM with significantly simpler cost aggregation and flow computation. Compared to a prior SGM extension for optical flow, the proposed NG-fSGM provides about 9x reduction in the number of computations and 5x reduction in the memory requirement with only 0.17% accuracy degradation when evaluated with Middlebury benchmark test cases.	algorithm;approximation;benchmark (computing);binocular disparity;computation;dynamic programming;elegant degradation;optical flow;pixel;second generation multiplex;semiconductor industry;stereopsis;test case	Jiang Xiang;Ziyun Li;David Blaauw;Hun-Seok Kim;Chaitali Chakrabarti	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7533208	nonlinear optics;computer vision;mathematical optimization;computer science;theoretical computer science;machine learning;optical imaging;optical flow;mathematics	Vision	55.12240731853841	-55.72609131847673	197303
76c4611c042ef813beca4007a7296877b4ae3cda	color restoration for infrared cutoff filter removed rgbn multispectral filter array image sensor		Imaging systems based on multispectral filter arrays(MSFA) can simultaneously acquire wide spectral information. A MSFA image sensor with R, G, B, and near-infrared(NIR) filters can obtain the mixed spectral information of visible bands and that of the NIR bands. Since the color filter materials used in MSFA sensors were almost transparent in the NIR range, the observed colors of multispectral images were degraded by the additional NIR spectral band information. To overcome this color degradation, a new signal processing approach is needed to separate the spectral information of visible bands from the mixed spectral information. In this paper, a color restoration method for imaging systems based on MSFA sensors is proposed. The proposed method restores the received image by removing NIR band spectral information from the mixed wide spectral information. To remove additional spectral information of the NIR band, spectral estimation and spectral decomposition were performed based on the spectral characteristics of the MSFA sensor. The experimental results show that the proposed method restored color information by removing unwanted NIR contributions to the RGB color channels.	algorithm;channel (digital image);circuit restoration;color;elegant degradation;image sensor;multispectral image;signal processing;spectral density estimation	Chul Hee Park;Hyun Mook Oh;Moon Gi Kang	2015		10.5220/0005263600300037	color filter array;bayer filter	Robotics	59.25948976392532	-60.343125794678485	197316
401c80a43ebb3cbc7980852da9e10af88725a833	efficient temporal alignment of video sequences using unbiased bidirectional dynamic time warping	video signal processing;image sequence;video;dynamic time warping	We propose an efficient technique for temporally aligning video sequences of similar activities. The proposed technique is able to synchronize view-variance videos from different scenes performing similar 3-D activities. Unlike existing techniques that just consider unidirectional alignment, the proposed technique considers symmetric temporal alignment and computes the optimal alignment by eliminating any view-based bias. The advantages of our technique are validated by experiments conducted on synthetic and real video data. The experimental results show that the proposed technique out-performs existing techniques in several test video sequences. © 2010 SPIE and IS&T. [DOI: 10.1117/1.3488415]	dynamic time warping;experiment;sequence alignment;synthetic intelligence;temporal logic	Cheng Lu;Mrinal Kanti Mandal	2010	J. Electronic Imaging	10.1117/1.3488415	computer vision;video;computer science;dynamic time warping;video tracking;multimedia;computer graphics (images)	AI	56.427424719557	-55.26328751156803	197385
d829cd342676e9a044082988621597ab5b673b2a	spatiotemporal deinterlacing using a maximum a posteriori estimator based on multiple-field registration	sensors;point spread functions;motion estimation;algorithms;video;signal to noise ratio	This paper proposes an accurate deinterlacing algorithm using a maximum a posteriori (MAP) estimator. First, we produce accurate motion vector fields between the current field and adjacent fields by employing an advanced motion compensation scheme that is suitable for an interlaced format. Next, the progressive frame corresponding to the current field is found via the MAP estimator based on the derived motion vector fields. Here, in order to obtain a stable solution, well-known bilateral total variation–based regularization is applied. Then, at a specific mode decision step, it is decided whether the result from the aforementioned temporal deinterlacing is acceptable or not. Finally, if the temporal deinterlacing is determined to be inappropriate by the mode decision, a typical spatial deinterlacing is applied instead of the MAP estimator-based temporal deinterlacing. Experimental results show that the proposed algorithm provides at maximum 2 dB higher PSNR than a cutting-edge deinterlacing algorithm, while providing better visual quality than the latter. © The Authors. Published by SPIE under a Creative Commons Attribution 3.0 Unported License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI. [DOI: 10.1117/1.JEI.22.4.043038]	algorithm;bilateral filter;deinterlacing;interlaced video;matrix regularization;motion compensation;peak signal-to-noise ratio	Ho-Taek Lee;Dong-Bok Lee;Byungju Lee;Byung Cheol Song	2013	J. Electronic Imaging	10.1117/1.JEI.22.4.043038	computer vision;mathematical optimization;video;computer science;sensor;motion estimation;mathematics;signal-to-noise ratio	AI	56.331195926140076	-59.390130081437576	197704
eae51cd7c92894411165fda0bd844f1cdca6a157	video-based document image scanning using a mobile device	image resolution;image enhancement;feature extraction;mobile communication;mobile handsets;lighting;cameras	Document scanning is a daily office work. However, traditional devices like flatbed scanners are not easy to carry for mobile work. In this paper, we present a new document image scanning method using a mobile device. With a clip of continuous video capturing, our approach first extracts several key-frames. A mobile GPU based image stitching method is adopted to generate a high resolution document image. Semi-automatic document image dewarping is then applied to rectify the perspective distortion and document page warping. For shadow and uneven environment lighting problem, we use Retinex theory based image enhancement method to remove those artifacts. Experimental results show that our results are outperforming existing commercial mobile applications.	distortion;graphics processing unit;image editing;image resolution;image scanner;image stitching;mobile app;mobile device	Bo Jiang;Sijiang Liu;Si-Yu Xia;Xiao Yu;Mengmeng Ding;Xuedong Hou;Yu Gao	2015	2015 International Conference on Image and Vision Computing New Zealand (IVCNZ)	10.1109/IVCNZ.2015.7761529	image warping;computer vision;image resolution;mobile telephony;feature extraction;computer science;machine learning;lighting;multimedia;image-based lighting;computer graphics (images)	Vision	59.43685883642335	-60.50042359635388	197847
93107cabac909500b67d16096a0e083b4754caa7	joint motion estimation/segmentation for object-based video coding	motion segmentation image coding video coding motion estimation encoding image segmentation bit rate;image coding;image segmentation;motion estimation;bit rate;video coding;motion segmentation;encoding	A video coding scheme is presented in which the coding is performed on individual moving objects. A Markov Random Field model is employed in finding the motion and boundaries of the objects. By guiding the object segmentation process with the spatial color information, meaningful objects representative of the real video scene are extracted. Furthermore, this enables a systematic treatment in handling the covered/uncovered regions, as well as the appearance/disappearance of moving objects. The rate for transmitting object motion and boundary is greatly reduced by use of temporal updating. The interior coding is performed by object-based subband decomposition. Simulations indicate promising results for low bitrate applications.	cholesky decomposition;data compression;markov chain;markov random field;motion estimation;object-based language;transmitter	Soo-Chul Han;Lilla Böröczky;John W. Woods	1996	1996 8th European Signal Processing Conference (EUSIPCO 1996)	10.5281/zenodo.36075	sub-band coding;computer vision;quarter-pixel motion;computer science;segmentation-based object categorization;coding tree unit;motion estimation;block-matching algorithm;multimedia;image segmentation;scale-space segmentation;motion compensation;multiview video coding;computer graphics (images)	Vision	55.479515669923074	-56.17161140913986	197967
9eedd2b9126ae49dd99ad2ff804913d3bb2eac6f	joint denoising and demosaicking of raw video sequences		The demosaicking provokes the spatial and color correlation of noise, which is afterwards enhanced by the imaging pipeline. The correct removal previous or simultaneously with the demosaicking process is not usually considered in the literature. We present a novel joint demosaicking and denoising algorithm for image sequences. The proposed algorithm uses a spatio-temporal patch method modifying all pixels, including those of the Bayer CFA. However, only original values are considered for averaging. The experimentation, including real examples, illustrates how a joint denoising and demosaicking algorithm avoids the creation of artifacts and colored spots in the final image.		Antoni Buades;Joan Duran	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451853	interpolation;optical imaging;computer vision;grayscale;artificial intelligence;noise reduction;demosaicing;adaptive optics;pixel;computer science;pattern recognition	Vision	56.710931078728365	-58.985549937355024	198066
dea101718fea0918757b08094a0ac300e44b7efc	image matching in bayer raw domain to de-noise low-light still images, optimized for real-time implementation	sensors;data storage;algorithms;denoising;computer hardware	ABSTRACT Temporal accumulation of images is a well -known approach to improve signal to noise ratios of still images taken in a low light conditions. However , the complexity of known algorithms often lead s to high hardware resource usage, increased memory bandwidth and computational complexity, making their practical use impossible. In our research we attempt to solve this problem with an implementation of a practical spatial -temporal de -noising algorithm , based on image accumulation . Image matching and spatial -temporal filtering was performed in Bayer RAW data space, which allowed us to benefit from predictable sensor noise characteristics , thus allowing using a range of algorithmic optimizations . The proposed algorithm accurately compensates for global and local motion and efficiently removes different kinds of no ise in noisy images taken in low light conditions. I n our algorithm we were able to perform global and local motion compensation in Bay er RAW data space, while preserving the resolution and effectively improving signal to noise ratios of moving objects as well as non -stationary background . The proposed algorithm is suitable for implementation in co PPHUFLDOJUDGH)3*$¶VDQGFDSDEOHRI proc ess ing 16MP images at capturing rate (10 frames per second) . The main challenge for matching between still images is the compromise between the quality of the motion prediction and the complexity of the algorithm and required memory bandwidth. Still images taken in a burst sequence must be aligned to compensate for background motion and foreground objects movements in a scene. H igh resolution still images coupled with significant time between successive frames can produce large displacements bet ween images, which creates additional difficulty for image matching algorithms. In photo applications it is very important that the noise is efficiently removed in both static , and non -static background as well as in a moving objects , maintaining the resol ution of the image. In our proposed algorithm we solved the issue of matching current image with accumulated image data in Bayer RAW data space in order to efficiently perform spatio -temporal noise reduction and reduce the computational requirements . In th is paper we provide subjective experimental results to demonstrate the ability of the proposed method to match noisy still images in order to perform efficient de -noising and avoid motion artefacts in resulting still images. Keywords: spatial -temporal de -noising , image processing , sensor noise, Bayer RAW, non -local means , FPGA, real -time , motion compensation .	image registration;real-time clock	Ilya V. Romanenko;Eran A. Edirisinghe;Daniel Larkin	2014		10.1117/12.2042489	computer vision;simulation;computer science;sensor;noise reduction;computer data storage;computer graphics (images)	Vision	59.0190358425287	-56.77993005417755	198216
959c9bed28b32c32945f21f84b1553c77659bdda	efficient visibility encoding for dynamic illumination in direct volume rendering	environment maps;volumetric illumination;annan data och informationsvetenskap;precomputed radiance transfer;image coding;transfer functions;approximation method;low frequency;volume rendering;spherical harmonic;real time;scattering;integration;computer graphic;approximation theory;isotropic phase functions visibility encoding dynamic illumination direct volume rendering real time dynamic shading general lighting directional lights point lights environment maps spherical harmonic basis functions low frequency approximation level of detail selection transfer function setting piecewise integration;level of detail;transfer function;transfer functions approximation theory encoding image coding integration rendering computer graphics;approximation methods;lighting;lighting rendering computer graphics light sources harmonic analysis scattering real time systems approximation methods;rendering computer graphics;direct volume rendering;encoding;high frequency;volume rendering volumetric illumination precomputed radiance transfer;light sources;other computer and information science;real time systems;harmonic analysis	We present an algorithm that enables real-time dynamic shading in direct volume rendering using general lighting, including directional lights, point lights, and environment maps. Real-time performance is achieved by encoding local and global volumetric visibility using spherical harmonic (SH) basis functions stored in an efficient multiresolution grid over the extent of the volume. Our method enables high-frequency shadows in the spatial domain, but is limited to a low-frequency approximation of visibility and illumination in the angular domain. In a first pass, level of detail (LOD) selection in the grid is based on the current transfer function setting. This enables rapid online computation and SH projection of the local spherical distribution of visibility information. Using a piecewise integration of the SH coefficients over the local regions, the global visibility within the volume is then computed. By representing the light sources using their SH projections, the integral over lighting, visibility, and isotropic phase functions can be efficiently computed during rendering. The utility of our method is demonstrated in several examples showing the generality and interactive performance of the approach.	algorithm;angularjs;approximation;basis function;coefficient;computation;coupling (computer programming);digital video recorder;document completion status - documented;graphics visualization;illumination (image);image quality;isosurface;level of detail;map;multiresolution analysis;orthographic projection;polygon mesh;population parameter;projections and predictions;real-time clock;rendering (computer graphics);shading;spherical power:invlen:pt:eye.left:qn;supernumerary maxillary right third molar;transfer function;volume rendering;wavelet;light absorption	Joel Kronander;Daniel Jönsson;Joakim Löw;Patric Ljung;Anders Ynnerman;Jonas Unger	2012	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2011.35	computer vision;computer science;harmonic analysis;mathematics;transfer function;spherical harmonic lighting;computer graphics (images)	Visualization	65.67759188146186	-52.443219629344334	198430
c05c72893c3f26a055f7d8f7399960ce14d76ee2	examplar-based face colorization using image morphing		Colorization of gray-scale images relies on prior color information. Examplar-based methods use a color image as source of such information. Then the colors of the source image are transferred to the grayscale image. In the literature, this transfer is mainly guided by texture descriptors. Face images usually contain few texture so that the common approaches frequently fail. In this paper we propose a new method based on image morphing. This technique is able to compute a correspondence map between images with similar shapes. It is based on the geometric structure of the images rather than textures which is more reliable for faces. Our numerical experiments show that our morphing based approach clearly outperforms state-of-the-art methods.	color image;experiment;grayscale;morphing;numerical analysis;texture filtering;texture mapping	Johannes Persch;Fabien Pierre;Gabriele Steidl	2017	CoRR			Vision	55.61098346174601	-61.077452653930855	198461
2323b136a91c4d8b388383e9823e8e0fee502b28	partial plane sweep volume for deep learning based view synthesis		We propose a partial plane sweep volume that can be a more suitable input format for deep-learning-based view synthesis approaches. Our approach makes it possible to synthesize higher quality images with a smaller number of learning iterations, while keeping the number of depth planes.	deep learning;iteration;sweep line algorithm;view synthesis	Kouta Takeuchi;Kazuki Okami;Daisuke Ochi;Hideaki Kimata	2017		10.1145/3102163.3102220	computer graphics (images);view synthesis;computer vision;artificial intelligence;computer science;deep learning;sweep line algorithm	EDA	66.09361060304124	-52.8069996106291	198473
4223921b8d203c923eded78e8c8c4b482f5b6ae1	chromatic aberration and depth extraction	optical transfer function;computer vision;optical focusing;lenses aberrations computer vision optical focusing optical transfer function optical information processing image colour analysis;aberrations;optical information processing;image colour analysis;lenses;depth estimation;lenses retina cameras humans visual system focusing layout data mining computer vision optical surface waves;rgb color channels chromatic aberration depth extraction visual information autofocus depth estimation color video camera occlusion edges step edges defocus measures	In this paper, we introduce chromatic aberration as a source of visual information that can be useful for autofocus and depth estimation. A color video camera equiped with a lens with chromatic aberration has been used to take images of both step and occlussion edges at several distances from the camera. The defocus measures obtained in the three d$ferent RGB color channels of each image are different. We suggest the way this information can be exploited in order to design an autofocus sensor, and also how depth information can be derived.	channel (digital image);list of monochrome and rgb palettes	Josep Garcia;Juan María Sánchez;Xavier Orriols;Xavier Binefa	2000		10.1109/ICPR.2000.905499	computer vision;science of photography;optical transfer function;lens;mathematics;computer graphics (images)	HCI	62.471429606575654	-56.66098099657558	198474
037888f91c48b556e038e983c644d70c9d52ab0a	interactive shadow removal and ground truth for variable scene categories		We present an interactive, robust and high quality method for fast shadow removal. To perform detection we use an on-the-fly learning approach guided by two rough user inputs for the pixels of the shadow and the lit area. From this we derive a fusion image that magnifies shadow boundary intensity change due to illumination variation. After detection, we perform shadow removal by registering the penumbra to a normalised frame which allows us to efficiently estimate non-uniform shadow illumination changes, resulting in accurate and robust removal. We also present the first reliable, validated and multi-scene category ground truth for shadow removal algorithms which overcomes limitations in existing data sets – such as inconsistencies between shadow and shadow-free images and limited variations of shadows. Using our data, we perform the most thorough comparison of state of the art shadow removal methods to date. Our algorithm outperforms the state of the art, and we supply our P-code and evaluation data and scripts to encourage future open comparisons.	algorithm;display resolution;ground truth;illumination (image);microsoft p-code;p-code machine;pixel	Han Gong;Darren Cosker	2014			computer vision;shadow and highlight enhancement;computer graphics (images)	Vision	57.60845312925374	-53.30296170388409	198525
586bf469e370063f5d5ef7c93796a1a3e6839399	adaptive high dynamic range for time-of-flight cameras	time of flight tof high dynamic range hdr noise reduction photonic mixer device pmd suppression of background illumination sbi;cameras measurement by laser beam dynamic range lighting image resolution estimation	The limited dynamic range of conventional digital cameras is a well-known problem. A common solution is to apply high-dynamic-range (HDR) techniques over several images acquired using different exposure times. Time-of-flight (ToF) cameras using a photonic mixer device (PMD) are not an exception, since the dynamic range of its dual pixels is also limited. Furthermore, in this case, the saturation of the pixel channels leads to wrong depth measurements. An appropriate solution is the suppression of background illumination (SBI) system designed by the PMD. This system actually extends the dynamic range of the camera by hardware, but it also introduces noise when activated. In this paper, we present an adaptive HDR (AHDR) solution to the problem for the ToF case that overcomes the limited dynamic range of the system, allowing sensing along a theoretically infinite dynamic range with the only limitations of the power of the illumination system and the decay of the SNR with higher distances or lower illumination intensities. Our method is able to detect and segment relevant scene regions responsible for unexpected saturation, i.e., close foreground objects, from the rest of the scene and adjust the exposure times of the acquired images considering them. The results show a reduction in detail losses and a higher SNR in the AHDR raw images, with respect to single acquisitions. This results in a dramatic depth error reduction and effective axial resolution improvement in critical areas, while keeping a high frame rate. In addition, the SBI-related noise is eliminated.	adaptive algorithm;binary image;coherence (physics);depth-first search;digital camera;expect;experiment;high dynamic range;high-dynamic-range imaging;high-dynamic-range rendering;image noise;lateral computing;lateral thinking;pmd;pixel;raw image format;signal-to-noise ratio;stellar classification;synchronous backplane interconnect;time-of-flight camera;xfig;zero suppression	Miguel Heredia Conde;Klaus Hartmann;Otmar Loffeld	2015	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2014.2377993	computer vision;electronic engineering;optics;physics	Vision	60.5408145808702	-56.91468631020449	198528
1da42d9868247a99bc63a4c588e3a971430d839c	demosaicking of noisy bayer-sampled color images with least-squares luma-chroma demultiplexing and noise level estimation	least squares approximations;noise level noise measurement image color analysis noise reduction colored noise estimation;parameter estimation adaptive signal processing computational complexity filtering theory image colour analysis least squares approximations;noise reduction bayer sampling color filter array demosaicking noise estimation noise model;adaptive signal processing;computational complexity;image colour analysis;web site noisy bayer sampled color image demosaicking noise level estimation least squares luma chroma demultiplexing demosaicking method bayer color filter array image white balanced gamma corrected cfa image noise parameter estimation noise adaptive demosaicking scheme noise estimation computational complexity;parameter estimation;filtering theory	This paper adapts the least-squares luma-chroma demultiplexing (LSLCD) demosaicking method to noisy Bayer color filter array (CFA) images. A model is presented for the noise in white-balanced gamma-corrected CFA images. A method to estimate the noise level in each of the red, green, and blue color channels is then developed. Based on the estimated noise parameters, one of a finite set of configurations adapted to a particular level of noise is selected to demosaic the noisy data. The noise-adaptive demosaicking scheme is called LSLCD with noise estimation (LSLCD-NE). Experimental results demonstrate state-of-the-art performance over a wide range of noise levels, with low computational complexity. Many results with several algorithms, noise levels, and images are presented on our companion web site along with software to allow reproduction of our results.	algorithm;bayer filter;channel (digital image);color filter array;computational complexity theory;current-feedback operational amplifier;demosaicing;least squares;least-squares analysis;multiplexing;noise (electronics);noise reduction;quantum decoherence;sampling - surgical action;signal-to-noise ratio;web site	Gwanggil Jeon;Eric Dubois	2013	IEEE Transactions on Image Processing	10.1109/TIP.2012.2214041	gradient noise;adaptive filter;gaussian noise;median filter;image noise;computer vision;speech recognition;dark-frame subtraction;value noise;computer science;noise measurement;mathematics;estimation theory;computational complexity theory;algorithm;statistics;salt-and-pepper noise	Vision	55.41830885512464	-66.03588906915368	198545
a328ba5a781f1c7bbcda4ee677270caf302265c4	an inverse tone mapping operator based on reinhard's global operator	tone mapping;hdr image;inverse tone mapping	A number of inverse tone mapping operators (TMOs) for dynamic range expansion have been proposed due to the need to visualize low dynamic range (LDR) images on high dynamic range (HDR) devices. This paper proposes a novel inverse TMO, which enables to generate HDR images from LDR ones, not only without using any specific parameters but also at low computing costs. Furthermore, the inverse TMO has a new characteristic when an LDR image is mapped from an HDR one by Reinhard's global operator. In the case, the HDR image reconstructed by the proposed method without parameters can be remapped into the same image as that remapped from an HDR one reconstructed with parameters. Experimental results show that the proposed inverse tone mapping (TM) operation can be carried out, while keeping better structural similarity and lower computing cost than conventional methods.	high dynamic range;high-dynamic-range rendering;ldraw;structural similarity;tone mapping	Yuma Kinoshita;Sayaka Shiota;Hitoshi Kiya	2016	2016 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS)	10.1109/ISPACS.2016.7824712	computer vision;mathematical optimization;mathematics;computer graphics (images)	Robotics	60.01730837095882	-61.66337080467762	198638
bc2dd5fcd06ad02938ce71085174f4b3e8ab896b	partial differential equation inpainting method based on image characteristics		Inpainting is an image processing method to automatically restore the lost information according to the existing image information. Inpainting has great application on restoration of the lost information for photographs, text removal of image, and recovery for the loss coding of image, etc. Image restoration based on partial differential equation (PDE) is an important repair technology. To overcome the shortcomings of the existing PDEs in repair process, such as false edge, incomplete interpolation information, a new PDE for image restoration based on image characteristics is proposed. The new PDE applies different diffusion mode for image pixels with the different characteristics, which can effectively protect the edges, angular points, and other important characteristics of the image during the repair process. The experimental results in both gray images and color images show that our method can obviously improve the image visual effect after inpainting compared with different traditional diffusion models.	angularjs;circuit restoration;image processing;image restoration;inpainting;interpolation;loss function;pixel;tracing (software);velocity (software development);visual effects	Fang Zhang;Ying Chen;Zhitao Xiao;Lei Geng;Jun Wu;Tiejun Feng;Ping Liu;Yufei Tan;Jinjiang Wang	2015		10.1007/978-3-319-21969-1_2	computer vision;pattern recognition;interpolation;pixel;image processing;image restoration;inpainting;artificial intelligence;partial differential equation;computer science	Vision	57.20247588305836	-61.02697486994378	198773
058fe211f020a041a1425ba91fb497b3094ef2b9	digital watermarking: an approach based on hilbert transform		Most of the well known algorithms for watermarking of digital images involve transformation of the image data to Fourier or singular vector space. In this paper, we introduce watermarking in Hilbert transform domain for digital media. Hilbert transform provides an analytic representation of a signal in terms of a phase and amplitude function. In this work, we apply one-dimensional Hilbert transform on each of the vectors that define an image and embed the watermark in its phase. Based on this idea, we propose an algorithm for embedding and extracting watermark in a host image and analytically obtain a parameter related to this procedure. Using extensive simulations, we show that the algorithm performs well even if the host image is corrupted by various attacks.	additive white gaussian noise;algorithm;analytic signal;digital image;digital media;digital watermarking;dilation (morphology);embedded system;gamma correction;gaussian blur;hilbert transform;image scaling;jpeg 2000;median filter;peak signal-to-noise ratio;personal computer;real-time clock;real-time computing;simulation;sparse matrix;utility functions on indivisible goods;whole earth 'lectronic link	Rashmi Agarwal;R. Krishnan;M. S. Santhanam;K. Srinivas;K. Venugopalan	2016	2016 International Conference on Computing, Communication and Automation (ICCCA)		computer vision;discrete mathematics;digital watermarking;computer science;theoretical computer science;hilbert–huang transform;mathematics;top-hat transform;digital image	EDA	67.26534671074694	-58.02975039550425	198799
26badd656b21fa6cd6fcd18faa22361b7e245fc4	single image dehazing based on dark channel prior and energy minimization		Hazy images have limited visibility and low contrast. The degradation is expressed by transmission map, which is one of the most important estimates of single image dehazing. Transmission map estimation is an underconstraint problem, and lots of priors have been proposed. Among them, the dark channel prior is widely recognized. However, traditional methods have not fully exploited its power due to improper assumptions or operations, which cause unwanted artifacts. The postrefinement algorithms employed to remove these artifacts in turn undermine the merits of the prior. In this letter, a novel method for estimating transmission map by energy minimization is proposed to solve this problem. The energy function combines the dark channel prior with piecewise smoothness. The method is compared to the state-of-the-art methods and shows outstanding performance.	algorithm;artifact (software development);autostereogram;elegant degradation;energy minimization;mathematical optimization	Mingzhu Zhu;Bingwei He;Qiang Zheng	2018	IEEE Signal Processing Letters	10.1109/LSP.2017.2780886	piecewise;visibility;mathematical optimization;mathematics;smoothness;algorithm;prior probability;energy minimization;minification;communication channel	Vision	57.00950727833066	-59.59050575517181	198991
f4e05aa138cf7fd87347fc55c5d565bfdeae6ea9	characteristics of color digital image correlation for deformation measurement in geomechanical structures	speckle;strain measurement;color;gray scale;image color analysis;correlation;strain	"""Digital image correlation (DIC) is a photogrammetric method that allows reconstruction of displacement and strain fields from the surface images. The displacements and strains are reconstructed by correlating sections (subsets) of the reference and deformed images generally taken before and after deformation respectively. Traditionally, DIC is applied to grayscale images taken with monochromatic cameras, but with the advent of digital color cameras there is a potential to use Color DIC using color images. It is expected that color images have more information and therefore same subset size would produce better results for Color DIC as compared to traditional grayscale DIC. This study examines various sizes of speckles in speckle patterns and different types of deformation and their effect on the performance of Color DIC and grayscale DIC. The speckle patterns and deformations were simulated and images were numerically generated. The errors are calculated by obtaining the difference between the measured and actual introduced simulated displacement or strain values. The results obtained suggest that generally Color DIC provides better results as compared to grayscale DIC. The quantity of improvement in performance of Color DIC dependent on the size of the speckles exits in the image and type of deformation. Usually, improvement is high for small subset sizes which decreases with the increase of the subset size. It is also found that for complex deformation scenarios, the performance of grayscale DIC is better than Color DIC if selected subset is larger than a specific subset size usually known as """"optimal subset size""""."""	color;digital differential analyzer;digital image;displacement mapping;monochrome;numerical analysis;photogrammetry;simulation	David Hang;Ghulam Mubashar Hassan;Cara MacNish;Arcady V. Dyskin	2016	2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)	10.1109/DICTA.2016.7797018	speckle pattern;computer vision;mathematics;geometry;strain;correlation;grayscale	Vision	61.51426140328959	-57.359649644473045	199296
e6f3ce745c494d090cfc2a7977e0cb033bc8c763	reverse engineering the human vision system: a possible explanation for the role of microsaccades	eye;image reconstruction;reverse engineering;splines (mathematics);human vision system;image reconstruction;microsaccade;reverse engineering;spline grid	We present a method of image reconstruction, which is invariant to the chosen group of transformations of the spline grid used to reconstruct the image. Integration over a group of transformations may be what the human eye does during microsaccades, which may be an explanation of why the images we see are not aliased although the sensors with which we record them are irregularly placed in the retina.	aliasing (computing);iterative reconstruction;reverse engineering;sensor;spline (mathematics)	Alexander Kadyrov;Maria Petrou	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1333706	iterative reconstruction;spline;computer vision;microsaccade;machine vision;image processing;interpolation;computer science;image sensor;reverse engineering;approximation theory;computer graphics (images)	Vision	62.30810319959072	-55.02575757416601	199367
2fa7d91edf42c9bc52b3e29cb66029ec276d0b87	coiflet based methods for range image processing	filtering;segmentation;range image;coiflet;sub sampling;wavelets;3d reconstruction	"""In industry applications, the range images are generally huge points arrays and are additively noised. They usually represent surfaces of 3D objects and are used for reverse engineering process in CAD/CAM domains. To compute the geometrical model of each surface present in the range image, we denoise and sub-sample the raw range data. Denoising allows us to avoid the adverse effects of the noise on the obtained result. Sub-sampling the raw range data leads to a low image processing overheads like those of segmentation process. Based on interpolation properties of particular wavelets named coiflets, we propose a method for smoothing noisy range images. The smoothed image keeps invariant the """"topological characteristics"""" of the represented surfaces. Thereafter, we propose a method for sub-sampling dense range images which leads to the reduction of the amount of raw data by a factor of four. This method eliminates the """"redundant"""" information, thus the obtained result describes the essential details (as the shape of the physical surface) of the initial range image. The smoothing and sub-sampling methods are designed to be easily integrated in any reconstruction algorithm to improve its result and reduce its overhead in spite of its high complexity."""	coiflet;image processing;range imaging	Mourad Djebali;Mahmoud Melkemi;Kamal E. Melkemi;Nickolas S. Sapidis	2007	Int. J. Image Graphics	10.1142/S0219467807002672	3d reconstruction;filter;wavelet;computer vision;mathematical optimization;computer science;data mining;mathematics;coiflet;segmentation	Robotics	57.837884320523195	-52.57922645094298	199457
b194adbf7732da7cba4279dd285453f7723b69a6	convolutional sparse coding for capturing high speed video content		Video capture is limited by the trade-off between spatial and temporal resolution: when capturing videos of high temporal resolution, the spatial resolution decreases due to bandwidth limitations in the capture system. Achieving both high spatial and temporal resolution is only possible with highly specialized and very expensive hardware, and even then the same basic trade-off remains. The recent introduction of compressive sensing and sparse reconstruction techniques allows for the capture of single-shot high-speed video, by coding the temporal information in a single frame, and then reconstructing the full video sequence from this single coded image and a trained dictionary of image patches. In this paper, we first analyze this approach, and find insights that help improve the quality of the reconstructed videos. We then introduce a novel technique, based on convolutional sparse coding (CSC), and show how it outperforms the state-of-the-art, patch-based approach in terms of flexibility and efficiency, due to the convolutional nature of its filter banks. The key idea for CSC high-speed video acquisition is extending the basic formulation by imposing an additional constraint in the temporal dimension, which enforces sparsity of the first-order derivatives over time.	algorithm;ana (programming language);blink;compressed sensing;computation;computer graphics;convolutional neural network;data compression;dictionary;digital video;filter bank;first-order predicate;first-order reduction;fixed-point iteration;imaging technology;neural coding;pixel;sampling (signal processing);sparse matrix;traffic enforcement camera;visual computing	Ana Serrano;Elena Garces;Diego Gutierrez;Belén Masiá	2017	Comput. Graph. Forum	10.1111/cgf.13086	computer vision;computer science;artificial intelligence;compressed sensing;video capture;computational photography;temporal resolution;bandwidth (signal processing);neural coding;image resolution;multiview video coding	Vision	59.8474297369823	-56.32859107268757	199499
528d236521e54a546714ff671e7185f3edca23f8	robust and efficient adaptive direct lighting estimation	pmc;global illumination;ray tracing;monte carlo	Hemispherical integrals are important for the estimation of direct lighting which has a major impact on the results of global illumination. This work proposes the population Monte Carlo hemispherical integral (PMC-HI) sampler to improve the efficiency of direct lighting estimation. The sampler is unbiased and derived from the population Monte Carlo framework which works on a population of samples and learns to be a better sampling function over iterations. Information found in one iteration can be used to guide subsequent iterations by distributing more samples to important sampling techniques to focus more efforts on the sampling sub-domains which have larger contributions to the hemispherical integrals. In addition, a cone sampling strategy is also proposed to enhance the success rate when complex occlusions exist. The images rendered with PMC-HI are compared against those rendered with multiple importance sampling (Veach and Guibas In: SIGGRAPH ’95, pp 419–428, 1995), adaptive light sample distributions (Donikian et al. IEEE Trans Vis Comput Graph 12(3):353–364, 2006), and multidimensional hemispherical adaptive sampling (Hachisuka et al. ACM Trans Graph 27(3):33:1–33:10, 2008). Our PMC-HI sampler can improve rendering efficiency.	adaptive sampling;algorithm;computation;computer graphics;dirac comb;global illumination;importance sampling;iteration;kalman filter;monte carlo method;nyquist–shannon sampling theorem;rendering (computer graphics);resampling (statistics);siggraph;sampling (signal processing)	Yu-Chi Lai;Hsuan-Ting Chou;Kuo-Wei Chen;Shaohua Fan	2013	The Visual Computer	10.1007/s00371-013-0908-z	ray tracing;simulation;computer science;slice sampling;mathematics;monte carlo integration;global illumination;statistics;monte carlo method;computer graphics (images)	Graphics	64.0360335545985	-53.41820352763051	199644
b661c8b60598569204f2060d39a7108ee4c2838f	image contrast enhancement based on intensity-pair distribution	contrast enhanced;histograms helium pixel image analysis image generation noise generators gold electronic mail working environment noise image recognition;image enhancement;mapping function image contrast enhancement algorithms intensity pair distribution image noise suppression image processing image content information expansion force set;proceedings paper;image denoising;image enhancement image denoising	Current contrast enhancement algorithms sometimes come with undesired drawbacks, like the loss of tiny details, enhancement of image noise, occasional over-enhancement, and unnatural look of the processed images. In this paper, we propose a new approach for contrast enhancement based on the use of a so-called intensity-pair distribution. This distribution possesses both local information and global information of the image content. By analyzing the content of intensity-pair distribution, a set of expansion forces are generated for contrast enhancement while another set of anti-expansion forces are generated to suppress image noise. To avoid over-enhancement and preserve the natural look of the processed images, a magnitude mapping function is also proposed. Experimental results show that the proposed algorithm does provide a flexible and reliable way for contrast enhancement.	algorithm;image noise	Tzu-Cheng Jen;B. Hsieh;Sheng-Jyh Wang	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1529900	image quality;image warping;image texture;image restoration;image noise;computer vision;feature detection;binary image;image processing;digital image processing;mathematics;edge enhancement;anisotropic diffusion;non-local means;top-hat transform;computer graphics (images)	Robotics	56.26003481610114	-64.54175200288907	199815
