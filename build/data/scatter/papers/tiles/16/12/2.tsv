id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
484467db5fe974e6f29c04b1eb01d36e0b29ef52	adaptive noise smoothing filter for images with signal-dependent noise	statistique;degradation;restauration image;image processing;nonstationary image model;information filtering;image restoration;single dependent noise adaptive noise smoothing image restoration nonstationary image model;image bruitee;traitement image;multiplicative noise;adaptive noise smoothing;smoothing methods;adaptive filters;computational modeling;adaptation model;smoothing;signal processing;noisy image;statistics;single dependent noise;a priori information;signal restoration;smoothing methods adaptive filters image restoration statistics degradation signal restoration context modeling signal processing information filtering information filters;wiener filter;filtre adaptatif;information filters;context modeling;image modeling;lissage;adaptive filter;noise;covariance matrix;estadistica	In this paper, we consider the restoration of images with signal-dependent noise. The filter is noise smoothing and adapts to local changes in image statistics based on a nonstationary mean, nonstationary variance (NMNV) image model. For images degraded by a class of uncorrelated, signal-dependent noise without blur, the adaptive noise smoothing filter becomes a point processor and is similar to Lee's local statistics algorithm [16]. The filter is able to adapt itself to the nonstationary local image statistics in the presence of different types of signal-dependent noise. For multiplicative noise, the adaptive noise smoothing filter is a systematic derivation of Lee's algorithm with some extensions that allow different estimators for the local image variance. The advantage of the derivation is its easy extension to deal with various types of signal-dependent noise. Film-grain and Poisson signal-dependent restoration problems are also considered as examples. All the nonstationary image statistical parameters needed for the filter can be estimated from the noisy image and no a priori information about the original image is required.	algorithm;box blur;circuit restoration;computation;derivation procedure;digital image processing;image restoration;mean squared error;multiplicative noise;recursion;recursive filter;sample variance;scene statistics;signal-to-noise ratio;smoothing (statistical technique);thin-film transistor	Darwin T. Kuan;Alexander A. Sawchuk;Timothy C. Strand;Pierre Chavel	1985	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.1985.4767641	gradient noise;adaptive filter;gaussian noise;median filter;image restoration;image noise;computer vision;noise shaping;kernel adaptive filter;value noise;image processing;computer science;noise measurement;signal processing;pattern recognition;mathematics;statistics;salt-and-pepper noise;smoothing	Vision	54.14234378387745	-67.30441172593194	91449
4d9e681051a990d8a0c7f83e0547c1b67dc72d40	adaptive total variation deringing method for image interpolation	minimization;projection subgradient method;adaptive total variation deringing;interpolation;local algorithm;image resolution;gradient method;adaptive postprocessing algorithm;subgradient method;low resolution;image interpolation;interpolation tv algorithm design and analysis compression algorithms transform coding filtering algorithms image edge detection image processing fractals mathematics;image restoration;transform coding;indexing terms;local conditional gradient method;image enhancement;interpolation gradient methods image resolution;local conditional gradient method adaptive total variation deringing image interpolation adaptive postprocessing algorithm ringing artifact reduction low resolution image projection subgradient method;ringing artifact reduction;gradient methods;image enhancement deringing total variation image interpolation;total variation;tv;deringing;algorithm design and analysis;low resolution image	This paper presents a new adaptive post-processing algorithm for ringing artifact reduction after image interpolation (up sampling). The algorithm is based on the concept of total variation (TV) for ringing control. It uses known TV of the blocks of the low-resolution image. Conditional gradient, subgradient and projection subgradient methods for this algorithm are considered and analyzed. A test set of 181300 overlapping 11times11 blocks of real images was used for local algorithm optimization and analysis. Local conditional gradient method shows the best objective and subjective results.	frank–wolfe algorithm;gradient descent;gradient method;interpolation;local algorithm;mathematical optimization;ringing (signal);ringing artifacts;sampling (signal processing);subgradient method;test set;video post-processing	Andrey S. Krylov;Andrey V. Nasonov	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4712328	computer vision;mathematical optimization;discrete mathematics;image resolution;computer science;mathematics	Robotics	56.84314900230575	-66.81017860322233	91604
0075b275e1e42d66361e3d506dc3e470062a9548	the closest-to-mean filter: an edge preserving smoother for gaussian environments	image sampling;gaussian noise;image processing;median filter;edge detection;digital filters computational complexity gaussian noise smoothing methods image sampling edge detection interference suppression;filters smoothing methods image processing gaussian noise signal processing computational complexity filtering theory sufficient conditions world wide web noise robustness;root closest to mean filter edge preserving smoother gaussian environments median based filters input sample ctm filtering framework computational complexity performance near gaussian environments s filters generalized selection type filters edge preservation impulse suppression image processing gaussian noise;interference suppression;smoothing methods;computational complexity;digital filters	Median based lters have gained wide-spread use because of their ability to preserve edges and suppress impulses. In this paper, we introduce the Closest-to-Mean(CTM) lter, which outputs the input sample closest to the sample mean. The CTM ltering framework o ers lower computational complexity and better performance in near Gaussian environments than median lters. The formulation of the CTM is derived from the theory of Slters, which form a class of generalized selection-type lters with the features of edge preservation and impulse suppression. Slters can play a signi cant role in image processing, where edge and detail preservation are of paramount importance. We compare the performance of CTM, median, and mean lters in the smoothing of edges and impulses immersed in Gaussian noise. A su cient condition for a signal to be a root of the CTM lter is included. Data, gures and source code utilized in this paper are available at http://www.ee.udel.edu/signals/robust	computational complexity theory;generalized selection;image processing;smoothing;zero suppression	Daniel Leo Lau;Juan Guillermo González	1997		10.1109/ICASSP.1997.595319	nonlinear filter;gaussian noise;median filter;computer vision;mathematical optimization;digital filter;edge detection;image processing;computer science;machine learning;gaussian blur;mathematics;prototype filter;gaussian filter;computational complexity theory;non-local means	Vision	55.239356400217545	-66.5067631718303	92589
69a5b0e83d3005f58850123a85765dbf2014304a	fast, robust total variation-based reconstruction of noisy, blurred images	modified total variation regularization functional;linear systems;preconditionnement;iterative method;kernel;satellite image reconstruction application;regularisation;convergence;image processing;metodo diferencia finita;optical noise;efficient algorithm;convergence of numerical methods;nonlinear equations image reconstruction image enhancement iterative methods convergence of numerical methods optical noise conjugate gradient methods functional equations;virgule fixe;noisy blurred images;regularisation tikhonov;fast robust total variation based reconstruction;preconditioning;finite difference;indexing terms;conjugate gradient method;image bruitee;linear system;fixed point iteration;coma fija;finite difference method;desconvolucion;regularization;fixed point;metodo iterativo;satellite broadcasting;imagen sonora;iterative methods;methode difference finie;reconstruction image;image enhancement;imagen borrosa;reconstruccion imagen;metodo gradiente conjugado;methode iterative;blurred image;image reconstruction;noisy image;discretized problem;deconvolution;functional equations;robustness;precondicionamiento;nonlinear equations;nonlinearity;total variation;robustness image reconstruction image processing laboratories tv kernel wiener filter linear systems satellite broadcasting finite difference methods;regularizacion;wiener filter;image floue;tv;tikhonov regularization;methode gradient conjugue;conjugate gradient methods;satellite image reconstruction application fast robust total variation based reconstruction noisy blurred images tikhonov regularization modified total variation regularization functional image processing discretized problem fixed point iteration nonlinearity preconditioned conjugate gradient iteration linear systems convergence;preconditioned conjugate gradient iteration;finite difference methods;variation totale	Tikhonov regularization with a modified total variation regularization functional is used to recover an image from noisy, blurred data. This approach is appropriate for image processing in that it does not place a priori smoothness conditions on the solution image. An efficient algorithm is presented for the discretized problem that combines a fixed point iteration to handle nonlinearity with a new, effective preconditioned conjugate gradient iteration for large linear systems. Reconstructions, convergence results, and a direct comparison with a fast linear solver are presented for a satellite image reconstruction application.	algorithm;conjugate gradient method;convergence (action);discretization;fixed point (mathematics);fixed-point number;fixed-point iteration;image processing;immunostimulating conjugate (antigen);iterative reconstruction;linear system;matrix regularization;nonlinear system;numerical linear algebra;solver;total variation denoising	Curtis R. Vogel;Mary E. Oman	1998	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.679423	mathematical optimization;mathematical analysis;nonlinear system;finite difference method;calculus;mathematics;iterative method;linear system	Vision	56.05201058799181	-72.83180454346446	92700
94bc5a7fadbcff90f881f1d655a8f46bbd61e533	robust multi-frame super-resolution based on spatially weighted half-quadratic estimation and adaptive btv regularization		Multi-frame image super-resolution focuses on reconstructing a high-resolution image from a set of low-resolution images with high similarity. Combining image prior knowledge with fidelity model, the Bayesian-based methods have been considered as an effective technique in super-resolution. The minimization function derived from maximum a posteriori probability (MAP) is composed of a fidelity term and a regularization term. In this paper, based on the MAP estimation, we propose a novel initialization method for super-resolution imaging. For the fidelity term in our proposed method, the half-quadratic estimation is used to choose error norm adaptively instead of using fixed  $L_{1}$  and  $L_{2}$  norms. Besides, a spatial weight matrix is used as a confidence map to scale the estimation result. For the regularization term, we propose a novel regularization method based on adaptive bilateral total variation (ABTV). Both the fidelity term and the ABTV regularization guarantee the robustness of our framework. The fidelity term is mainly responsible for dealing with misregistration, blur, and other kinds of large errors, while the ABTV regularization aims at edge preservation and noise removal. The proposed scheme is tested on both synthetic data and real data. The experimental results illustrate the superiority of our proposed method in terms of edge preservation and noise removal over the state-of-the-art algorithms.	algorithm;bilateral filter;biologic preservation;brake to vacate;fda quality metrics terminology;gaussian blur;image noise;image resolution;majority function;manifold regularization;matrix regularization;numerous;regularization by spectral filtering;super-resolution imaging;synthetic data	Xiaohong Liu;Lei Chen;Wenyi Wang;Jiying Zhao	2018	IEEE Transactions on Image Processing	10.1109/TIP.2018.2848113	iterative reconstruction;robustness (computer science);maximum a posteriori estimation;artificial intelligence;initialization;matrix (mathematics);pattern recognition;synthetic data;regularization (mathematics);image resolution;mathematics	Vision	58.117026974087075	-70.51308229896452	93308
ac9c1ac6c8d14bf5618d1858a35b1e6fb5959965	variational bayesian sparse kernel-based blind image deconvolution with student's-t priors	loi student;identificacion ciega;metodo estadistico;student t distribution bayesian approach blind image deconvolution bid inverse problem kernel model sparse prior;image processing;blind deconvolution;bayesian approach;methode noyau;funcion densidad probabilidad;bayes methods;probability density function;methode bayes;fonction etalement point;student t distribution;variational techniques;procesamiento imagen;bayesian methods;variational bayesian;statistical method;heavy tail;desconvolucion ciega;problema inverso;traitement image;kernel model;algorithme;fonction densite probabilite;algorithm;ley student;reconstruction image;inference algorithms variational bayesian sparse kernel blind image deconvolution students t priors point spread function image reconstruction probability density function;blind image deconvolution bid;inverse problem;students t priors;variational techniques bayes methods image reconstruction;image edge detection;reconstruccion imagen;variational inference;methode statistique;point spread function;image reconstruction;robustesse;metodo nucleo;blind image deconvolution;identification aveugle;deconvolution aveugle;bayesian methods deconvolution tv robustness inference algorithms inverse problems computer science education educational programs shape image reconstruction;deconvolution;funcion distribucion punto;kernel method;robustness;inference algorithms;numerical experiment;numerical models;sparse prior;student distribution;variational bayesian sparse kernel;probleme inverse;blind identification;bayesian model;robustez;algoritmo	In this paper, we present a new Bayesian model for the blind image deconvolution (BID) problem. The main novelty of this model is the use of a sparse kernel-based model for the point spread function (PSF) that allows estimation of both PSF shape and support. In the herein proposed approach, a robust model of the BID errors and an image prior that preserves edges of the reconstructed image are also used. Sparseness, robustness, and preservation of edges are achieved by using priors that are based on the Student's-t probability density function (PDF). This pdf, in addition to having heavy tails, is closely related to the Gaussian and, thus, yields tractable inference algorithms. The approximate variational inference methodology is used to solve the corresponding Bayesian model. Numerical experiments are presented that compare this BID methodology to previous ones using both simulated and real data.	appendix;approximation algorithm;assumed;bid protein;binary integer decimal;biologic preservation;body dysmorphic disorders;calculus of variations;cobham's thesis;computation (action);deconvolution;experiment;gamma correction;gaussian (software);inference;kernel;lindane;morphologic artifacts;neural coding;normal statistical distribution;numerical method;partition function (mathematics);portable document format;ringing (signal);ringing artifacts;sparse matrix;tail;tails;variational principle;visually impaired persons;weight	Dimitris Tzikas;Aristidis Likas;Nikolas P. Galatsanos	2009	IEEE Transactions on Image Processing	10.1109/TIP.2008.2011757	econometrics;mathematical optimization;image processing;bayesian probability;computer science;student's t-distribution;mathematics;statistics	ML	54.62461721050564	-73.78969118693517	93546
87d8d3e4b938e3f23136b53f6329469d7a4ce264	iterative image restoration combining total variation minimization and a second-order functional	second order;partial differential equation;iterative image restoration;total variation minimization;image restoration;pdes;total variation;weight function;numerical experiment;noise removal;characteristic features;convex combination	A noise removal technique using partial differential equations (PDEs) is proposed here. It combines the Total Variational (TV) filter with a fourth-order PDE filter. The combined technique is able to preserve edges and at the same time avoid the staircase effect in smooth regions. A weighting function is used in an iterative way to combine the solutions of the TV-filter and the fourth-order filter. Numerical experiments confirm that the new method is able to use less restrictive time step than the fourth-order filter. Numerical examples using images with objects consisting of edge, flat and intermediate regions illustrate advantages of the proposed model.	algorithm;circuit restoration;courant–friedrichs–lewy condition;experiment;gaussian blur;grayscale;image restoration;infinite impulse response;iteration;iterative method;numerical analysis;numerical method;numerical partial differential equations;radio over fiber;tv tuner card;variational principle;weight function	Ola Marius Lysaker;Xue-Cheng Tai	2005	International Journal of Computer Vision	10.1007/s11263-005-3219-7	image restoration;computer vision;mathematical optimization;mathematical analysis;weight function;convex combination;mathematics;geometry;total variation;second-order logic;partial differential equation;statistics	Vision	55.53215897694941	-70.28945287373128	93996
1ca871efb57d02f5c146e2e0997383c51ad67018	digital phase-shifting holography based on sparse approximation of phase and amplitude	sparse representations phase shifting digital holography coherent imaging phase and magnitude reconstruction phase shifting interferometry poissonian noise;image reconstruction noise measurement three dimensional displays holography lenses optical interferometry;3d objects digital phase shifting holography sparse approximation lens focussing complex object reconstruction variational inverse imaging sparse phase and amplitude reconstruction algorithm holographic reconstruction;variational techniques compressed sensing holography image reconstruction lenses optical focusing	Phase-shifting holography with a lens focussing the complex object on the sensor plane is considered. In this setup a scaled copy of the complex object is reconstructed at the sensor plane. In order to reconstruct the complex object from noisy data, we apply variational inverse imaging, where the amplitude and the absolute phase of the object are assumed to admit sparse representations in suitable sparsifying transforms. The developed technique is based on the sparse phase and amplitude reconstruction (SPAR) algorithm developed in [4]-[6]. Numerical experiments demonstrate effectiveness of SPAR for holographic reconstruction of 3D objects.	algorithm;calculus of variations;experiment;holography;numerical method;signal-to-noise ratio;sparse approximation;sparse matrix	Vladimir Katkovnik;Jos&#x00E9; M. Bioucas-Dias;N. V. Petrov	2014	2014 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON)	10.1109/3DTV.2014.6874743	computer vision;electronic engineering;digital holographic microscopy;optics;physics	Vision	63.036762246886916	-70.84135027854454	94021
1b4dd116187ec0a213c7c354ac22f3f32038649b	joint demosaicking and denoising by total variation minimization	minimisation;variational techniques;image color analysis color noise reduction image reconstruction noise measurement colored noise;spatio spectral sampling demosaicking denoising bayer color filter array frequency selection;variational techniques image colour analysis image denoising image reconstruction minimisation;image colour analysis;image reconstruction;image denoising;sensor image demosaicking image denoising color image digital camera image reconstruction luminance channel digital camera noisy raw data output consistency constraint total variation minimization	Joint demosaicking and denoising consists in reconstructing a color image from the noisy raw data output by the sensor of a digital camera. We adopt a variational formulation in which the reconstructed image has minimal total variation under the constraint of consistency with the available measurements. This way, the recovered color image has smooth chrominance but the sharp edges are maintained and the noise is transferred to the luminance channel. This channel is denoised subsequently.	calculus of variations;color image;demosaicing;digital camera;noise reduction	Laurent Condat;Saleh Mosaddegh	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6467476	iterative reconstruction;demosaicing;image texture;image restoration;image noise;computer vision;minimisation;mathematical optimization;feature detection;image resolution;color image;image gradient;binary image;image processing;pattern recognition;mathematics;non-local means;statistics	Robotics	58.331368327532346	-71.28878318820699	94061
c187aaedc91e13c5cf50b55bfac51097af70c793	efficient low dose x-ray ct reconstruction through sparsity-based map modeling		Ultra low radiation dose in X-ray Computed Tomography (CT) is an important clinical objective in order to minimize the risk of carcinogenesis. Compressed Sensing (C S) enables significant reductions in radiation dose to be achie v d by producing diagnostic images from a limited number of CT projections. However, the excessive computation time that conven tional CS-based CT reconstruction typically requires has limitedclinical implementation. In this paper, we first demonstrate that a thorough analysis of CT reconstruction through a Maximum a Posteriori objective function results in a weighted compressive sensing problem. This analysis enables us to formulate a low dose fan beam and helical cone beam CT reconstruction. Subsequen tly, we provide an efficient solution to the formulated CS problem based on a Fast Composite Splitting Algorithm-Latent Expec ted Maximization (FCSA-LEM) algorithm. In the proposed method we use pseudo polar Fourier transform as the measurement matrix in order to decrease the computational complexity; and rebinning of the projections to parallel rays in order to extend its application to fan beam and helical cone beam scans. The weight involved in the proposed weighted CS model, denoted by Error Adaptation Weight (EAW), is calculated based on the statistical characteristics of CT reconstruction and is a f unction of Poisson measurement noise and rebinning interpolation e rror. Simulation results show that low computational complexity of the proposed method made the fast recovery of the CT images possible and using EAW reduces the reconstruction error by o ne order of magnitude. Recovery of a high quality 512× 512 image was achieved in less than 20 sec on a desktop computer without numerical optimizations.	ct scan;compressed sensing;computation;computational complexity theory;desktop computer;display resolution;expectation–maximization algorithm;interpolation;loss function;numerical analysis;optimization problem;simulation;sparse matrix;time complexity;tomography	SayedMasoud Hashemi;Soosan Beheshti;Patrick Robert Gill;Narinder S. Paul;R. S. C. Cobbold	2014	CoRR		computer vision;mathematical optimization;mathematics	Vision	55.58138172903081	-76.3672187961962	94665
30e4a070b816e6151171e9f5a22cc4a0180bc4f7	dual stack filters and the modified difference of estimates approach to edge detection	fonction booleenne;noise edge detection filtering theory boolean functions parameter estimation mathematical operators;image processing;boolean functions;edge detection;optimal filtering;natural images dual stack filters intensity edges detection edge detection optimal stack filtering modified difference of estimates noisy images symmetry condition symmetric difference of estimates edge operator boolean function threshold decomposition architecture threshold boolean filter training time;duality;procesamiento imagen;boolean function;natural images;indexing terms;image edge detection filtering theory boolean functions detectors noise robustness training data nonlinear filters;traitement image;experimental result;mathematical operators;deteccion contorno;algorithme;algorithm;detection contour;dualite;funcion booliana;robustesse;stack filter;analyse performance;performance analysis;resultado experimental;robustness;dualidad;parameter estimation;resultat experimental;filtering theory;filtrado optimo;filtrage optimal;noise;difference of estimates;robustez;algoritmo;analisis eficacia	The theory of optimal stack filtering has been used in the difference of estimates (DoE) approach to the detection of intensity edges in noisy images. The DoE approach is modified by imposing a symmetry condition on the data used to train the two stack filters. Under this condition, the stack filters obtained are duals of each other. Only one filter must therefore be trained; the other is simply its dual. This new technique is called the symmetric difference of estimates (SDoE) approach. The dual stack filters obtained under the SDoE approach are shown to be comparable. This allows the difference of these two filters to be represented by a single equivalent edge operator. This latter result suggests that an edge operator can be found by directly training a (possibly nonpositive) Boolean function to be used on each level of the threshold decomposition architecture. This approach, which is called the threshold Boolean filter (TBF) approach, requires less training time but produces operators that are less robust than those produced by the SDoE approach. This is demonstrated and interpreted via comparisons of results for natural images.	boolean;call stack;dual;edge detection;estimated	Jisang Yoo;Edward J. Coyle;Charles A. Bouman	1997	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.650117	mathematical optimization;combinatorics;discrete mathematics;image processing;computer science;mathematics;boolean function;algorithm;statistics	Vision	54.45513791916923	-66.27784768594728	95150
38238d94484a0db8965ab89f67dc6077c53828df	a study on estimation of high resolution component for image enhancement using wavelet	high resolution;sound field;multi resolution analysis;estimation method;low resolution;early and late reflections;image enhancement;classification rules;acoustical features;computer simulation	An estimation method of high resolution component, based on multi-scale wavelet representation, is discussed. We have newly proposed an edge classification rule and edge model. In our method, each edge in an image is analyzed by multi-resolution analysis and is classified according to the pattern of neighboring right and left edges together. A high deflnition image and its band-limited image are used for learning the edge transition from low resolution to high resolution. Finally, high resolution component is estimated by switching the transition rule that is adapted to an input image. We have confirmed the validity of our estimation method by computer simulation.	bandlimiting;computer simulation;image editing;image resolution;multiresolution analysis;newton's method;selection rule;wavelet	Yasumasa Itoh;Yoshinori Izumi;Yutaka Tanaka	1999		10.1145/319878.319885	computer simulation;computer vision;speech recognition;image resolution;computer science;sub-pixel resolution	ML	57.04134092000037	-67.54871514157179	97131
a61f7e3be7c38d47ca35499b09f117c53751d052	echocardiography noise reduction using sparse representation		Noise reduction in echocardiography images is proposed.Filtering framework is based on temporal information and sparse representation.Proposed method consists of smoothing intensity variation time curves assessed in each pixel.A smooth version of signal can be reconstructed by using a proper sparse recovery which is followed by an adaptive thresholding method to locate the most important atoms.After a comprehensive comparison of sparse recovery algorithms, three were selected for our method: Bayesian Compressive Sensing (BCS), Bregman Iterative algorithm, and Orthogonal Matching Pursuit (OMP).The proposed method preserves the edges and rapidly moving structures. The clarity and accuracy of echocardiography images are greatly reduced by speckle noise. Noise suppression, however, is difficult to achieve without also obscuring both rapidly moving structures and object edges. This research seeks to address these challenges by introducing a novel filtering framework based on temporal information and sparse representation. The proposed method involves smoothing intensity variation time curves (IVTCs) assessed in each pixel. Using an over-complete dictionary that contains prototype signal-atoms, IVTCs can be reconstructed as linear combinations of a few of these atoms. After a comprehensive comparison of sparse recovery algorithms, three were selected for our method: Bayesian Compressive Sensing (BCS), the Bregman iterative algorithm, and Orthogonal Matching Pursuit (OMP). The performance of the proposed method was then evaluated and compared with other speckle reduction filters. The experimental results demonstrate that the proposed algorithm can be used to achieve better-preserved edges and reduce blurring. Display Omitted	noise reduction;sparse approximation;sparse matrix	Parisa Gifani;Hamid Behnam;Farzan Haddadi;Zahra Alizadeh Sani;Peyman Gifani	2016	Computers & Electrical Engineering	10.1016/j.compeleceng.2015.12.008	computer vision;mathematical optimization;machine learning;sparse approximation;mathematics;statistics	HPC	58.059038240756095	-67.48126054793296	97196
3e6926b2888ff1f4b19fcf7fd3eb729d9b312010	adapted scan based lossless image compression		This paper deals with the application of lossless compression algorithms to two-dimensional curves scanned images. An image is scanned along a space filling curve (SFC) so as to exploit inherent coherence in the image. The used SFC is determined by a gradient based method allowing the detection of global pixel’s change direction for each image block. The resulting one-dimensional representation of the image has improved auto-correlation compared with universal scans such as the Peano-Hilbert space filling curve. Combined with conventional coding algorithms the proposed algorithm shows significant compression efficiency improvement. The new algorithm used for SFC determination is presented and used as an input to conventional coding schemes.	algorithm;autocorrelation;ct scan;coherence (physics);computation;data compression;gradient;hilbert curve;hilbert space;image compression;image processing;image scanner;iterative reconstruction;lossless compression;lossy compression;mathematical optimization;pixel;space-filling curve	Tarek Ouni;Arij Lassoued;Mohamed Abid	2011		10.1007/978-3-642-27183-0_10	data compression;computer vision;mathematical optimization;image compression;theoretical computer science;mathematics	Vision	55.08846373960313	-68.07967303756364	97482
d4eee09f8e3be7106e78c84a1312128eb74ba102	a new fusion method for remote sensing images based on salient region extraction		The goal of the remote sensing image fusion is to inject the detail information extracted from panchromatic (PAN) images to multispectral (MS) images with minimized spectral distortion. However, different regions in the image may practically have different demands on the spatial and spectral resolution. In this paper, a new fusion method for remote sensing images based on salient region extraction is proposed. By introducing the hybrid visual saliency analysis, information in the PAN and MS image are automatically partitioned into two categories: salient and non-salient regions. Then, a sub-region fusion strategy is applied to fuse the non-salient and salient regions respectively. For non-salient regions, such as farmland and mountains, the wavelet transform is used in the process of spatial infusion to suppress spectral distortion. As for salient regions like residential areas, the windowed IHS transform is carried out for its merits of effective integration of spatial and spectrum information. Experimental results demonstrate that our proposal achieves a better balance between spatial injection and spectral maintenance in different regions.	distortion;image fusion;multispectral image;wavelet transform;window function	Libao Zhang;Jue Zhang	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296624	wavelet transform;salient;multispectral image;computer vision;computer science;panchromatic film;principal component analysis;pattern recognition;artificial intelligence;remote sensing;feature extraction;distortion;image fusion	Robotics	59.43661136986809	-67.28398757672431	97485
ea6904fb05802fac33c9d04aad2bc7e13bd4e65d	restoration of simulated enmap data through sparse spectral unmixing	photogrammetrie und bildanalyse;dead pixels;inpainting;sparse reconstruction;spectral unmixing;denoising;enmap	This paper proposes the use of spectral unmixing and sparse reconstruction methods to restore a simulated dataset for the Environmental Mapping and Analysis Program (EnMAP), the forthcoming German spaceborne hyperspectral mission. The described method independently decomposes each image element into a set of representative spectra, which come directly from the image and have previously undergone a low-pass filtering in noisy bands. The residual vector from the unmixing process is considered as mostly composed of noise and ignored in the reconstruction process. The first assessment of the results is encouraging, as the original bands taken into account are reconstructed with a high signal-to-noise ratio and low overall distortions. Furthermore, the same method could be applied for the inpainting of dead pixels, which could affect EnMAP data, especially at the end of the satellite’s life cycle.	circuit restoration;defective pixel;distortion;html element;image noise;inpainting;low-pass filter;signal-to-noise ratio;sparse matrix	Daniele Cerra;Jakub Bieniarz;Rupert Müller;Tobias Storch;Peter Reinartz	2015	Remote Sensing	10.3390/rs71013190	computer vision;noise reduction;remote sensing;inpainting	Vision	67.37459504964609	-66.43686881057702	97555
15ab6043e8267b5a7c35916c5b17dd39c2e81fe3	image based metrology for quantitative analysis of local structural similarity of nanostructures	image processing;measurement;indexing terms;applications image based metrology image processing for nanoscale orientation estimation;nanostructured materials;local structure;electron microscopy;electron microscope;image processing for nanoscale;correlation function;quantitative analysis;image based metrology;nanostructured materials electron microscopy image processing measurement;orientation estimation;metrology image analysis nanostructures image processing nanobioscience electron microscopy mechanical engineering chemical processes robustness self assembly;applications;image metrology system image based metrology quantitative analysis local structural similarity nanostructures orientation correlation function image processing system correlation function estimation electron microscope images chemically patterned nanoscale structures	The orientation correlation function is a measure of the spatial range over which nanoscale structures maintain their structural (orientational) similarity. In this paper we describe an image processing system that is used to estimate this correlation function from electron microscope images of the chemically patterned nanoscale structures. We describe the estimation of a robust orientation field from the image and the subsequent estimation of the correlation function from the orientation field. We present results that have been obtained using our image metrology system. Sensitivity of the estimated values with respect to the image processing parameters is also presented.	electron;image processing;structural similarity	P. Ravindran;Nicola J. Ferrier;S. M. Park;Paul F Nealey	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4380021	computer vision;image processing;computer science;electron microscope	Robotics	57.30992977508954	-77.95961254209202	98222
6d7995d7b0442dd2a1b426db2d10cd425e5580f7	a k-wishart markov random field model for clustering of polarimetric sar imagery	cluster algorithm;spatial context;pattern clustering;image segmentation;vdp matematikk og naturvitenskap 400 fysikk 430 elektromagnetisme akustikk optikk 434;synthetic aperture radar expectation maximisation algorithm markov processes pattern clustering radar imaging radar polarimetry statistical distributions;vdp teknologi 500 elektrotekniske fag 540 elektronikk 541;vdp mathematics and natural science 400 physics 430 electromagnetism acoustics optics 434;statistical modeling polarimetric synthetic aperture radar polsar expectation maximization em k wishart markov random field mrf;contextual information;statistical model;image clustering;quad pol l band polarimetric sar imagery image clustering statistical distribution synthetic aperture radar k wishart distribution markov random field expectation maximization algorithm spatial context model;markov random field;energy function;data model;context model;potts model;accuracy;statistical distributions;conference object;peer reviewed;expectation maximization;radar polarimetry;clustering method;radar imaging;markov random field mrf;markov process;clustering algorithms;data models markov processes clustering algorithms context modeling accuracy image segmentation algorithm design and analysis;wishart distribution;markov processes;statistical modeling;parameter estimation;polarimetric synthetic aperture radar polsar;em algorithm;context modeling;algorithm design;statistical distribution;algorithm design and analysis;expectation maximization em;vdp technology 500 electrotechnical disciplines 540 electronics 541;data models;k wishart;konferansebidrag;synthetic aperture radar;expectation maximisation algorithm	A clustering method that combines an advanced statistical distribution with spatial contextual information is proposed for multilook polarimetric synthetic aperture radar (PolSAR) data. It is based on a Markov random field (MRF) model that integrates a K-Wishart distribution for the PolSAR data statistics conditioned to each image cluster and a Potts model for the spatial context. Specifically, the proposed algorithm is constructed based upon the expectation maximization (EM) algorithm. A new formulation of EM is developed to jointly address parameter estimation in the K-Wishart distribution and the spatial context model, and also minimization of the energy function. Experiments are presented with simulated and real quad-pol L-band data.	cluster analysis;estimation theory;expectation–maximization algorithm;experiment;l band;markov chain;markov random field;mathematical optimization;pixel;polarimetry;portable document format;potts model;synthetic data;test card	Vahid Akbari;Anthony Paul Doulgeris;Stian Normann Anfinsen;Torbjørn Eltoft;Sebastiano B. Serpico	2011	2011 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2011.6049317	probability distribution;statistical model;algorithm design;expectation–maximization algorithm;machine learning;pattern recognition;context model;markov process;statistics	Vision	61.34159760479236	-72.18632245751871	98828
c416a6b68398825e5f0d6b720d0e77e82db3ead3	octonion sparse representation for color and multispectral image processing		A recent trend in color image processing combines the quaternion algebra with dictionary learning methods. This paper aims to present a generalization of the quaternion dictionary learning method by using the octonion algebra. The octonion algebra combined with dictionary learning methods is well suited for representation of multispectral images with up to 7 color channels. Opposed to the classical dictionary learning techniques that treat multispectral images by concatenating spectral bands into a large monochrome image, we treat all the spectral bands simultaneously. Our approach leads to better preservation of color fidelity in true and false color images of the reconstructed multispectral image. To show the potential of the octonion based model, experiments are conducted for image reconstruction and denoising of color images as well as of extensively used Landsat 7 images.		Srdan Lazendic;Hendrik De Bie;Aleksandra Pizurica	2018		10.23919/EUSIPCO.2018.8553272	quaternion;iterative reconstruction;color image;multispectral image;false color;octonion algebra;sparse approximation;artificial intelligence;channel (digital image);pattern recognition;computer science	Vision	67.53744432342306	-66.55782933329617	98941
317841b0bec680487b9c0122cbee2a91419c13ca	an approach to positron emission tomography based on penalized cross-entropy minimization	cross entropy;iterative method;evaluation performance;optimisation;condition kuhn tucker;methode entropie minimum;performance evaluation;tomoscintigraphie positon;condicion kuhn tucker;optimizacion;maximum likelihood;evaluacion prestacion;merit function;simulation;minimum entropy methods;maximum vraisemblance;simulacion;fixed point iteration;positron emission tomography;metodo iterativo;optimization problem;reconstruction image;reconstruccion imagen;methode iterative;image reconstruction;algorithme em;optimization;algoritmo em;em algorithm;kuhn tucker condition;maxima verosimilitud	"""We describe an approach to positron emission tomography, which aims to minimize the cross-entropy between the detected photon coincidence counts and the projection due to image vector, subject to the almost conservation in total tube counts and nonnegativity constraints. We derive formally a """"xed point iterative algorithm to solve the corresponding optimization problem. The algorithm is applied to both simulated data based on a Shepp}Logan phantom and real data, and compared with some standard algorithms, the results demonstrating its e!ectiveness. 2001 Elsevier Science B.V. All rights reserved."""	algorithm;cross entropy;experiment;image quality;imaging phantom;iterative method;mathematical optimization;numerical analysis;optimization problem;shepp–logan phantom;signal-to-noise ratio;simulation;tomography	Song Zhang;Yuan Mei Wang	2001	Signal Processing	10.1016/S0165-1684(00)00245-0	iterative reconstruction;fixed-point iteration;optimization problem;econometrics;mathematical optimization;expectation–maximization algorithm;calculus;mathematics;iterative method;maximum likelihood;cross entropy;statistics	AI	54.554025248892046	-73.23130610432818	99034
f81b8d8f167f8c8e5fabfe6dd71fcf68d45df127	robust breast tumor detection via shrinkage covariance matrix estimation		Microwave imaging (MWI) is a promising imaging modality for breast tumor detection. One challenge faced by the ultra-wideband (UWB) radar-based breast cancer detection is the estimation of clutter-plus-noise covariance matrix. To render a more accurate covariance matrix estimate when the number of samples is not large, this paper presents a new covariance matrix estimate using the shrinkage method. The parameters of the proposed shrinkage-based covariance matrix are cast as a modified semi-definite programming (SDP) problem based on the minimum mean-squared error (MMSE) criterion. Moreover, to reduce the computational overhead, we also incorporate the compressive sensing (CS) technique with the above scheme for UWB breast tumor detection. The performance of the Capon beamformer based on the new reconstructed covariance matrix is tested under multistatic scenario by a 2-D numerical breast analysis model. Simulations show that the proposed approach possesses a better target identification capability and improves the signal-to-clutter-noise ratio (SCNR) than the existing counterparts.	beamforming;clutter;compressed sensing;computer simulation;image quality;many-worlds interpretation;mean squared error;microwave;numerical analysis;overhead (computing);semiconductor industry;semidefinite programming;signal-to-noise ratio;ultra-wideband	Li-Der Fang;Wen-Hsien Fang;Dau-Chyrh Chang;Yie-Tarng Chen	2017	2017 IEEE International Conference on Signal and Image Processing Applications (ICSIPA)	10.1109/ICSIPA.2017.8120584	artificial intelligence;pattern recognition;compressed sensing;computer science;overhead (computing);radar;covariance matrix;shrinkage;microwave imaging	Robotics	57.41613730665649	-74.33157566762092	99491
afb78e35965978ce5ce42c6527b094583cd8cc0e	image colorization based on the mixed l0/l1 norm minimization	least squares approximations;iterative methods;matrix completion image colorization image restoration sparse optimization;image colour analysis;iterative reweighted least squares algorithm image colorization mixed l0 l1 norm minimization color image recovery grayscale image;least squares approximations image colour analysis iterative methods;image color analysis minimization gray scale vectors color tv matrix converters	This paper proposes a new image colorization algorithm based on the mixed L0/L1 norm minimization. Introducing some assumptions, a problem of recovering a color image from a grayscale image with the small number of known color pixels is formulated as a mixed L0/L1 norm minimization, which is solved approximately by an iterative reweighted least squares (IRLS) algorithm. Numerical examples show that the proposed algorithm colorizes a grayscale image well using a small number of color pixels.	algorithm;color image;grayscale;iterative method;iteratively reweighted least squares;numerical method;pixel	Kazunori Uruma;Katsumi Konishi;Tomohiro Takahashi;Toshihiro Furukawa	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6467309	color histogram;computer vision;mathematical optimization;binary image;mathematics;iterative method;computer graphics (images)	Robotics	57.39962406249721	-71.39136537079439	99821
af8ebd434e34e4f03bd53b62bc27283ecb1523ff	image fusion based on simultaneous empirical wavelet transform		In this paper, a new multi-scale image fusion algorithm for multi-sensor images is proposed based on Empirical Wavelet Transform (EWT). Different from traditional wavelet transform, the wavelets of EWT are not fixed, but the ones generated according to the processed signals themselves, which ensures that these wavelets are optimal for processed signals. In order to make EWT can be used in image fusion, Simultaneous Empirical Wavelet Transform (SEWT) for 1D and 2D signals are proposed, by which different signals can be projected into the same wavelet set generated according to all the signals. The fusion algorithm constructed on the 2D SEWT contains three steps: source images are decomposed into a coarse layer and a detail layer first; then, the algorithm fuses detail layers using maximum absolute values, and fuses coarse layers using the maximum global contrast selection; finally, coefficients in all the fused layers are combined to obtain the final fused image using 2D inverse SEWT. Experiments on various images are conducted to examine the performance of the proposed algorithm. The experimental results have shown that the fused images obtained by the proposed algorithm achieve satisfying visual perception; meanwhile, the algorithm is superior to other traditional algorithms in terms of objective measures.	algorithm;coefficient;color image;digital camera;image fusion;image resolution;iterative reconstruction;multimodal interaction;noise reduction;real-time computing;wavelet transform	Xiaoli Zhang;Xiongfei Li;Yuncong Feng	2016	Multimedia Tools and Applications	10.1007/s11042-016-3453-8	wavelet;computer vision;mathematical optimization;continuous wavelet transform;machine learning;cascade algorithm;stationary wavelet transform;discrete wavelet transform;fast wavelet transform;wavelet transform	Robotics	59.16396452718768	-66.87002672881462	99913
d2b93dfbe50f3c642c64b8ea581cb6e449c71f82	nonlocal operators with applications to image processing	70h20;nonlocal operators;image processing;65d25;regularization;68u10;68r10;hamilton jacobi equations;total variation;spectral graph theory;35a15;variational methods;35s05	We propose the use of nonlocal operators to define new types of flows and functionals for image processing and elsewhere. A main advantage over classical PDE-based algorithms is the ability to handle better textures and repetitive structures. This topic can be viewed as an extension of spectral graph theory and the diffusion geometry framework to functional analysis and PDE-like evolutions. Some possible application and numerical examples are given, as is a general framework for approximating Hamilton-Jacobi equations on arbitrary grids in high demensions, e.g., for control theory.	aharonov–bohm effect;algorithm;control theory;gradient;image processing;inpainting;jacobi method;matrix regularization;nonlocal lagrangian;numerical analysis;pixel;processor affinity;radio over fiber;spectral graph theory;vergence	Guy Gilboa;Stanley Osher	2008	Multiscale Modeling & Simulation	10.1137/070698592	regularization;mathematical optimization;mathematical analysis;discrete mathematics;image processing;mathematics;total variation;spectral graph theory	ML	54.52317389852584	-70.59530588758032	101130
884163225679e0ca06d148091103dea892e13917	an em-based spatial-spectral restoration approach for hyperspectral images	expectation maximization algorithms;signal to noise ratio abstracts nickel image restoration image quality;remote sensing expectation maximisation algorithm geophysical image processing hyperspectral imaging image resolution image restoration;会议论文;observation model;observation models em based spatial spectral restoration approach hyperspectral image restoration approach spectral resolutions spatial resolutions multiband image restoration scheme expectation maximization algorithm;restoration expectation maximizition em algorithm hyperspectral image;hyper spectral images	In this paper, a restoration approach for hyperspectral images with two observations of specific spatial and spectral resolutions is presented. This proposed approach is based on a multi-band image restoration scheme employing Expectation-Maximization (EM) algorithm, in which both observation models are used for restoration. Simulation experimental results illustrate that compared to existing restoration approaches, in which only one observation model is utilized while the other observation is treated as auxiliary, the proposed restoration approach is capable of producing improved restored results.	circuit restoration;expectation–maximization algorithm;image restoration;norm (social);simulation;utility functions on indivisible goods	Yifan Zhang;Mingyi He	2012	2012 4th Workshop on Hyperspectral Image and Signal Processing (WHISPERS)	10.1109/WHISPERS.2012.6874272	computer vision;geography;pattern recognition;remote sensing	Vision	67.911945210854	-66.46632867276351	101452
cd2118e6dad7fee45c8c8a758a1e867cf2df2c1b	on joint distribution modeling in distributed video coding systems	domain model;spatially varying model parameters;laplace model;video coding adaptive codes source coding;adaptive codes;joints;frame spatial redundancy;video coding;adaptation model;joint source information distribution modeling;image edge detection;stationary joint distribution models;discrete cosine transforms;pixel;distributed models;distributed video coding;joint distribution modeling;double gamma model;spatially adaptive model;side information;frame spatial redundancy joint distribution modeling distributed video coding system joint source information distribution modeling stationary joint distribution models double gamma model laplace model spatially adaptive model spatially varying model parameters pixel domain model;distributed video coding system;pixel domain model;noise;adaptation model discrete cosine transforms pixel noise image edge detection joints;source coding	Performance of a distributed video coding system depends, to a large extent, on the accuracy of joint source and side information distribution modeling. In this work we first examine a family of stationary joint distribution models. As one of our findings, we propose to use the double-Gamma model as an alternative to the widely adopted Laplace model, due to its superior performance. In addition, we suggest a new spatially adaptive model, which enables to follow the spatially varying joint statistics of the source and side information. We present two methods, class-based and neighborhood-based, for estimation of the spatially varying model parameters. We then show how the obtained pixel domain model can be used in the transform domain to facilitate utilization of frame spatial redundancy. Integration of the proposed models into a distributed video coding system resulted in improved performance.	3d computer graphics;computer simulation;data compression;domain model;model selection;motion field;pixel;selection algorithm;stationary process	Yevgeny Priziment;David Malah	2010	2010 IEEE International Workshop on Multimedia Signal Processing	10.1109/MMSP.2010.5662037	computer vision;simulation;computer science;noise;domain model;pixel;source code	Vision	60.455669035219955	-69.55800211188846	101840
887339a587fa279e1f2df3517fd968f53576615b	bridging scale-space to multiscale frame analyses	nonlinear filters;wavelet frame;multiscale analysis;image restoration;smoothing methods filtering wavelet analysis image analysis image texture analysis information analysis nonlinear filters bridges partial differential equations kernel;image texture;wavelet transforms;interference suppression;filtering theory image texture wavelet transforms interference suppression nonlinear filters image restoration;scale space;denoising techniques scale space analysis multiscale frame analysis nonlinear image diffusion texture information correlation structure wavelet frame based technique image filtering nonlinear filter;filtering theory	We address a well known problem of nonlinear image diffusion techniques, namely the loss of texture information. We do so by first determining that it is due to unaccounted correlation structure in the image which we subsequently mitigate by proposing a wavelet frame-based technique. This, by the same token establishes a theoretical bridge between the scale space methodology and the multiscale analysis approach. We provide examples to illustrate the effectiveness of the proposed approach.	bridging (networking);nonlinear system;scale space;wavelet	Yufang Bao;Hamid Krim	2001		10.1109/ICASSP.2001.941329	image texture;image restoration;computer vision;mathematical optimization;scale space;computer science;theoretical computer science;mathematics;wavelet packet decomposition;texture filtering;wavelet transform	Vision	56.45706902692904	-67.71280909824365	102391
50ba619aaea4c0c4c316ac7734eec73d4ca98368	an efficient tof-sims image analysis with spatial correlation and alternating non-negativity-constrained least squares		MOTIVATION Advances in analytical instrumentation towards acquiring high-resolution images of mass spectrometry constantly demand efficient approaches for data analysis. This is particularly true of time-of-flight secondary ion mass spectrometry imaging where recent advances enable acquisition of high-resolution data in multiple dimensions. In many applications, the distribution of different species from a sampled surface is spatially continuous in nature and a model that incorporates the spatial correlation across the surface would be preferable to estimations at discrete spatial locations. A key challenge here is the capability to analyse the high-resolution multidimensional data to extract relevant information reliably and efficiently.   RESULTS We propose a framework based on alternating non-negativity-constrained least squares which accounts for the spatial correlation across the sample surface. The proposed method also decouples the computational complexity of the estimation procedure from the image resolution, which significantly reduces the processing time. We evaluate the performance of the algorithm with biochemical image datasets generated from mixture of metabolites.	computational complexity theory;dimensions;image analysis;image resolution;instrument - device;ions;linear least squares (mathematics);metabolite;negativity (quantum mechanics);sampling - surgical action;secondary ion mass spectrometry;algorithm	Parham Aram;Lingli Shen;John A. Pugh;Seetharaman Vaidyanathan;Visakan Kadirkamanathan	2015	Bioinformatics	10.1093/bioinformatics/btu734	generalized least squares;total least squares;iteratively reweighted least squares;mathematical optimization;non-linear iterative partial least squares;pattern recognition;non-linear least squares;statistics	Vision	54.99701380750998	-77.53937168722362	102578
06261632db8efedbdd75cc8b42d3c65a8f5ee22b	a nonconvex regularized approach for phase retrieval	optimisation approximation theory image reconstruction image retrieval;nonconvex optimization;proximal methods;signal processing algorithms convergence vectors optimization image reconstruction minimization measurement;phase retrieval problem;nonsmooth optimization;proximal methods phase retrieval problem nonconvex optimization nonsmooth optimization;image reconstruction problem nonconvex regularized approach phase retrieval imaging systems large size data sets nonconvex formulation estimation performance computational cost nonfourier measurements smooth nonconvex approximation standard data fidelity convex separable regularization functions optimization process block coordinate proximal algorithm large scale problems	With the development of new imaging systems delivering large-size data sets, phase retrieval has become recently the focus of much attention. The problem is especially challenging due to its intrinsically nonconvex formulation. In addition, the applicability of many existing solutions may be limited either by their estimation performance or by their computational cost, especially in the case of non-Fourier measurements. In this paper, we propose a novel phase retrieval approach, which is based on a smooth nonconvex approximation of the standard data fidelity term. In addition, the proposed method allows us to employ a wide range of convex separable regularization functions. The optimization process is performed by a block coordinate proximal algorithm which is amenable to solving large-scale problems. An application of this algorithm to an image reconstruction problem shows that it may be very competitive with respect to state-of-the-art methods.	algorithm;algorithmic efficiency;approximation;computation;iterative reconstruction;mathematical optimization;phase retrieval;reconstruction conjecture	Audrey Repetti;Emilie Chouzenoux;Jean-Christophe Pesquet	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025351	mathematical optimization;combinatorics;mathematical analysis;mathematics	Vision	56.68027137302793	-73.31547532105957	102618
bc2f031a8225fa2ae9cdd2a792a0a913a8986614	tchebichef moments based nonlocal-means method for despeckling optical coherence tomography images	speckle;optical coherence tomography;anisotropic diffusion;human vision and color perception;image analysis;image denoising;denoising;signal to noise ratio;wavelets	Speckle reduction in optical coherence tomography (OCT) images plays an important role in further image analysis. Although numerous despeckling methods, such as the Kuan’s filter, the Frost’s filter, wavelet based methods, anisotropic diffusion methods, have been proposed for despeckling OCT images, these methods generally tend to provide insufficient speckle suppression or limited detail preservation especially at high speckle corruption because of the insufficient utilization of image information. Different from these denoising methods, the nonlocal means (NLM) method explores nonlocal image self-similarities for image denoising, thereby providing a new method for speckle reduction in OCT images. However, the NLM method determines image self-similarities based on the intensities of noisy pixels, which will degrade its performance in restoring OCT images. To address this problem, the Tchebichef moments based nonlocal means (TNLM) method is proposed for speckle suppression. Distinctively, he TNLM method determines the nonlocal self-similarities of the OCT images by computing the Euclidean distance between Tchebichef moments of two image patches centered at two pixels of interest in the prefiltered image. Due to the superior feature representation capability of Tchebichef moments, the proposed method can utilize more image structural information for the accurate computation of image self-similarities. The experiments on the clinical OCT images indicate that the TNLM method outperforms numerous despeckling methods in that it can suppress speckle noise more effectively while preserving image details better in terms of human vision, and it can provide higher signal-to-noise ratio (SNR), contrast-to-noise ratio (CNR), equivalent number of looks (ENL) and cross correlation (XCOR). © (2015) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.	noise reduction;nonlocal lagrangian;tomography	Wanying Jiang;Wenqi Xiang;Mingyue Ding;Xuming Zhang	2015		10.1117/12.2082449	speckle pattern;wavelet;speckle noise;computer vision;image analysis;noise reduction;optics;signal-to-noise ratio;anisotropic diffusion;computer graphics (images)	HPC	58.12176272145957	-67.04119586695205	102813
0929452065116164c8c21b5ffc9202700e69e1f2	a multi directional perfect reconstruction filter bank designed with 2-d eigenfilter approach: application to ultrasound speckle reduction	filter banks;perfect reconstruction;ultrasound speckle reduction	B-Mode ultrasound images are degraded by inherent noise called Speckle, which creates a considerable impact on image quality. This noise reduces the accuracy of image analysis and interpretation. Therefore, reduction of speckle noise is an essential task which improves the accuracy of the clinical diagnostics. In this paper, a Multi-directional perfect-reconstruction (PR) filter bank is proposed based on 2-D eigenfilter approach. The proposed method used for the design of two-dimensional (2-D) two-channel linear-phase FIR perfect-reconstruction filter bank. In this method, the fan shaped, diamond shaped and checkerboard shaped filters are designed. The quadratic measure of the error function between the passband and stopband of the filter has been used an objective function. First, the low-pass analysis filter is designed and then the PR condition has been expressed as a set of linear constraints on the corresponding synthesis low-pass filter. Subsequently, the corresponding synthesis filter is designed using the eigenfilter design method with linear constraints. The newly designed 2-D filters are used in translation invariant pyramidal directional filter bank (TIPDFB) for reduction of speckle noise in ultrasound images. The proposed 2-D filters give better symmetry, regularity and frequency selectivity of the filters in comparison to existing design methods. The proposed method is validated on synthetic and real ultrasound data which ensures improvement in the quality of ultrasound images and efficiently suppresses the speckle noise compared to existing methods.	algorithm;bank (environment);bittorrent protocol encryption;diamond;experiment;filter bank;finite impulse response;image analysis;image quality;interpretation (logic);linear phase;loss function;low-pass filter;noise reduction;noise-induced hearing loss;optimization problem;peak signal-to-noise ratio;reconstruction filter;selectivity (electronic);structural similarity;synthetic intelligence;vena cava filters	Madhukar Nagare;Bhushan D. Patil;Raghunath S. Holambe	2016	Journal of Medical Systems	10.1007/s10916-016-0675-2	adaptive filter;computer vision;root-raised-cosine filter;filter bank;control theory;electronic filter topology;filter design;prototype filter;composite image filter;m-derived filter	Robotics	57.298018410931675	-66.24502344202556	103068
7351f7ff66019a4840a7c2b00abddf769c5c5722	integro-differential equations based on (bv, l1) image decomposition	inverse scale;hierarchical image decompositions;integro differential equation;variational problem;65c20;68u10;energy decomposition;total variation;denoising;92c50;deblurring;26b30	A novel approach for multiscale image processing based on integro-differential equations (IDEs) was proposed in [E. Tadmor and P. Athavale, Inverse Probl. Imaging, 3 (2009), pp. 693–710]. These IDEs, which stem naturally from multiscale (BV,L) hierarchical decompositions, yield inverse scale representations of images in the sense that the BV -dual norms of their residuals are inversely proportional to the scaling parameters. Motivated by the fact that (BV,L) decomposition is more suitable for extracting local scale-space features than (BV,L), we introduce here the IDEs which arise from multiscale (BV,L) hierarchical decompositions. We study several variants of this (BV,L)-based IDE model, depending on modifications to the curvature term.	image processing;image scaling;scale space	Prashant Athavale;Eitan Tadmor	2011	SIAM J. Imaging Sciences	10.1137/100795504	integro-differential equation;computer vision;mathematical optimization;mathematical analysis;calculus;noise reduction;mathematics;total variation	Vision	54.99878727306468	-70.18604894885827	103396
3c72f17906d6662f59fb53344b18afe82d608c55	off-the-grid recovery of piecewise constant images from few fourier samples	annihilating filter;65r32;94a08;prony s method;finite rate of innovation;trigonometric curves;off the grid;mri;94a12;94a20;92c55;superresolution;parametric image models;42b05;fourier extrapolation	We introduce a method to recover a continuous domain representation of a piecewise constant two-dimensional image from few low-pass Fourier samples. Assuming the edge set of the image is localized to the zero set of a trigonometric polynomial, we show the Fourier coefficients of the partial derivatives of the image satisfy a linear annihilation relation. We present necessary and sufficient conditions for unique recovery of the image from finite low-pass Fourier samples using the annihilation relation. We also propose a practical two-stage recovery algorithm which is robust to model-mismatch and noise. In the first stage we estimate a continuous domain representation of the edge set of the image. In the second stage we perform an extrapolation in Fourier domain by a least squares two-dimensional linear prediction, which recovers the exact Fourier coefficients of the underlying image. We demonstrate our algorithm on the super-resolution recovery of MRI phantoms and real MRI data from low-pass Fourier samples, which shows benefits over standard approaches for single-image super-resolution MRI.	algorithm;arabic numeral 0;autostereogram;coefficient;extrapolation;least squares;low-pass filter;phantoms, imaging;stage level 1;stage level 2;super-resolution imaging;trigonometric polynomial;benefit	Greg Ongie;Mathews Jacob	2016	SIAM journal on imaging sciences	10.1137/15M1042280	fourier transform;computer vision;mathematical optimization;discrete-time fourier transform;mathematical analysis;discrete mathematics;radiology;magnetic resonance imaging;discrete fourier transform;fourier inversion theorem;discrete fourier series;mathematics;fourier analysis;phase correlation;trigonometric polynomial;fourier series;fourier transform on finite groups;superresolution	Vision	54.63689703079499	-79.76596590264742	103628
bcfee52c2912fa1852bc564cecb144ad986e6262	reducing integrability error of color tensor gradients for image fusion	poisson equation;integrability error image enhancement di zenzo color tensor data fusion partial differential equations contrast enhancement retinex algorithms log space log differences logarithmic color space linear image space multispectral images hsv color space color image desaturation multispectral contrast greyscale representation poisson equation multichannel image visualization gradient field gradient based operators image fusion color tensor gradients;image fusion;tensors image colour analysis image enhancement image fusion image representation poisson equation;image enhancement;image colour analysis;image representation;image fusion gray scale image color analysis image enhancement;tensors	To overcome the difficulties in applying gradient-based operators to color images, Di Zenzo introduced the color tensor, an operator that provides a gradient field for multichannel images. An elegant application for this operator was developed in the domain of multichannel image visualization: Socolinsky and Wolff proposed to reintegrate Di Zenzo's gradient by solving a Poisson equation, yielding a greyscale representation of the multispectral contrast of the input image. Di Zenzo's gradients are, however, generally not integrable and some approximation must be introduced. Thus, the resulting image can suffer from artifacts such as the smearing of edges. In this paper, we focus on the integrability of Di Zenzo's gradients. We show that the integrability of the obtained field can be improved dramatically through a simple desaturation of the color image (as in the HSV color space). This result can be readily extended to multispectral images by defining an analogue to saturation. We present several results explaining what happens to color tensors as the saturation changes. Significantly we show that small changes of the saturation in the linear image space can result in large improvements in the integrability of tensor gradients calculated in logarithmic color space. This result is important for two reasons. 1) Log-differences are more perceptually meaningful. 2) In log-space we can operate with retinex algorithms, which are well known techniques for contrast enhancement. We propose that they can be used to “put back” any contrast that might be lost in the desaturation step and, more importantly, they can enhance contrast at the same time as reintegrating the gradient field because of their relation to partial differential equations. Finally, we evaluate our method psychophysically. Compared with other commonly used image fusion methods, experiments show that our data fusion using the Di Zenzo color tensor after desaturating the image and where a simple contrast boost is applied is strongly preferred.	algorithm;analog;approximation;color image;color space;experiment;grayscale;image fusion;image gradient;log-space reduction;morphologic artifacts;multispectral image;ploidies	Roberto Montagna;Graham D. Finlayson	2013	IEEE Transactions on Image Processing	10.1109/TIP.2013.2270108	color histogram;computer vision;feature detection;topology;tensor;image gradient;binary image;poisson's equation;mathematics;geometry;color balance;image fusion	Vision	55.14870011520083	-69.33077922524821	104077
02bd826227b242053f77b7f625c06dec462f44fb	a measure of near-orthogonality of pr biorthogonal filter banks	perceptual quality estimation near orthogonality pr biorthogonal filter banks nonorthogonality perfect reconstruction energy preservation transform domains spatial domains riesz constants modelization image compression systems mean square error;perceptual quality;relevance model;quantisation signal data compression image coding band pass filters transform coding filtering theory;image coding;filter bank;data compression;band pass filters;transform coding;quantisation signal;image compression;mean square error;channel bank filters filter bank quantization wavelet transforms low pass filters bit rate image reconstruction filtering mathematical model mean square error methods;perfect reconstruction;filtering theory	We study the non-orthogonality of perfect-reconstruction (PR) biorthogonal filter banks by measuring the energy preservation between the spatial and transform domains. The mathematical formulation of that issue leads to the computation of the Riesz constants, and a more relevant modelization leads to a measure of nearorthogonality which is well suited for image compression systems based on filter banks. This provides a criterion for the validity of the energy preservation approximation: we can compare the latter approximation with the one that is made when estimating the percep tual quality of an image by the mean square error.	approximation;computation;filter bank;image compression;mathematical model;mean squared error	F. Moreau de Saint-Martin;Albert Cohen;Pierre Siohan	1995		10.1109/ICASSP.1995.480564	data compression;computer vision;transform coding;speech recognition;image compression;computer science;filter bank;mathematics;band-pass filter;mean squared error;composite image filter;statistics;m-derived filter	Robotics	56.02655568447935	-67.90788725682626	104101
f91f088d56caba1839c9eb99a030777756a092e3	restoration of motion blurred images based on rich edge region extraction using a gray-level co-occurrence matrix		To improve the efficiency of blur kernel estimation based on prior knowledge, a method of deblurring an image based on rich edge region extraction using a gray-level co-occurrence matrix is proposed in this paper. First, the relationship between the image edge information and the related coefficients of a gray-level co-occurrence matrix is analyzed, based on which an index representing the amount of image edge information is proposed. Next, high-frequency layer information is extracted from the blurred image to be processed with a bilinear interpolation method in the luminance channel. Subsequently, the high-frequency layer image is divided into nine regions, based on a sliding window, and the rich edge region index of each region is calculated; then, the region with the richest edge information is extracted. Finally, the extracted rich edge region, instead of the entire motion blurred image, is used to estimate the blur kernel with L0-regularized intensity and gradient prior, and the blurred image is blindly restored. An image quality evaluation function and the operation time are used to evaluate the performance of the proposed method. Experimental results show that the proposed method can improve the recovery efficiency while ensuring the recovery quality as well.	bilinear filtering;circuit restoration;co-occurrence matrix;coefficient;deblurring;document-term matrix;evaluation function;gaussian blur;gradient;image quality;interpolation;operation time	Minghua Zhao;Xin Zhang;Zhenghao Shi;Peng Li;Bing Li	2018	IEEE Access	10.1109/ACCESS.2018.2815608	image restoration;kernel (linear algebra);computer vision;distributed computing;image quality;bilinear interpolation;kernel density estimation;computer science;matrix (mathematics);deblurring;artificial intelligence;co-occurrence matrix	Vision	58.56174782969003	-66.25913345562358	104257
0a76cfc45800c210414419b39ab9dc06eb1c2f0e	removing impulse bursts from images by training-based soft morphological filtering	teledetection;filtering;filtrage;image processing;filtrado;procesamiento imagen;intelligence artificielle;traitement image;reduccion ruido;multiplicative noise;analisis morfologico;ruido multiplicativo;noise reduction;remote sensing;radar imaging;teledeteccion;morphological analysis;reduction bruit;analyse morphologique;artificial intelligence;numerical simulations;imagerie radar;inteligencia artificial;denoising;bruit multiplicatif;impulse;noise removal;impulsion;radar;numerical simulation	The characteristics of impulse bursts in radar images are analyzed and a model for this noise is proposed. The model also takes into consideration the multiplicative noise present in radar images. As a case study, soft morphological filters utilizing a training-based optimization scheme are used for the noise removal. Different approaches for the training are discussed. It is shown that the methods used can provide an effective removal of impulse bursts. At the same time the multiplicative noise in images is also suppressed together with good edge and detail preservation. Numerical simulation results as well as examples of real radar images are presented.	finite impulse response;mathematical morphology;mathematical optimization;multiplicative noise;radar;simulation	Pertti T. Koivisto;Jaakko Astola;Vladimir V. Lukin;Vladimir P. Melnik;Oleg V. Tsymbal	2001		10.1117/12.438272	computer vision;telecommunications;image processing;noise reduction	Robotics	54.605103349604356	-66.51328219495764	105500
5e78f42afa29f47e70d86f0ef7ee1dd79b227958	fuzzy logic-based automatic contrast enhancement of satellite images of ocean	satellite images;contrast enhancement;contrast enhanced;gray level grouping;visual quality;fuzzy logic;histogram;fuzzy;satellite image;entropy;histogram equalization	In this paper, we evaluate the conventional contrast enhancement techniques [histogram equalization (HE), adaptive HE] and the recent gray-level grouping method and the fuzzy logic method in order to find out which of these is well suited for automatic contrast enhancement for satellite images of the ocean, obtained from a variety of sensors. All the techniques evaluated were based on the principle of transforming the skewed histogram of the original image into a uniform histogram. The performance of the different contrast enhancement algorithms are evaluated based on the visual quality and the Tenengrad criterion. The inter comparison of different techniques was carried out on a standard lowcontrast image and also three different satellite images with different characteristics. Based on our study, we advocate that a modified fuzzy logic method elucidated in this paper is well suited for contrast enhancement of low-contrast satellite images of the ocean. M. S. Nair (B) Rajagiri School of Computer Science, Rajagiri College of Social Sciences, Kalamassery, Kochi 683104, Kerala, India e-mail: madhu_s_nair2001@yahoo.com R. Lakshmanan KMEA Engineering College, Aluva, Kerala, India e-mail: rekhavibin@gmail.com M. Wilscy Department of Computer Science, University of Kerala, Kariavattom, Trivandrum 695581, Kerala, India e-mail: wilsyphilipose@hotmail.com R. Tatavarti Academic Research, VIT University, Vellore 632014, Tamil Nadu, India e-mail: rtatavarti@gmail.com	algorithm;computer science;email;fuzzy logic;histogram equalization;image histogram;sensor	Madhu S. Nair;Rekha Lakshmanan;M. Wilscy;Rao Tatavarti	2011	Signal, Image and Video Processing	10.1007/s11760-009-0143-2	fuzzy logic;computer vision;computer science;histogram matching;artificial intelligence;adaptive histogram equalization;histogram equalization;statistics	Vision	60.748844376634196	-66.30175465444266	105940
0172841092816f1ce4a6c94da92465903fcf29f2	image compression using nonlinear pyramid vector quantization	nonlinear pyramid vector quantization;median filter;progressive image transmission;multistage median filter;nonlinear pyramid data structure;natural images;first order;image compression;image quality;vector quantizer;data structure	In this paper, we present a new image compression scheme that exploits the VQ technique in a hierarchical nonlinear pyramid structure. We use multistage median filters (MMF) to build the image pyramids. Image pyramids generated by MMF show a better details preservation than the ones generated by Burt's kernel. It is shown that MMF effectively decorrelates the difference pyramids, resulting in smaller first order entropy. Our simulations on natural images show that NPVQ yields a higher SNR as well as better image quality, in comparison with LPVQ. The NPVQ scheme is also appropriate for progressive image transmission.		Xudong Song;Yrjö Neuvo	1994	Multidim. Syst. Sign. Process.	10.1007/BF00986975	image quality;median filter;computer vision;pyramid;speech recognition;data structure;image compression;computer science;machine learning;first-order logic;mathematics	Vision	56.72253659898491	-67.45322562299894	106055
cc95f2b90d72ced4aa8968f6830b6a478c9faa33	nonlinear multiscale regularisation in mr elastography: towards fine feature mapping	elastography;complex dualtree wavelet;denoising;magnetic resonance elastography;wave inversion	Fine-featured elastograms may provide additional information of radiological interest in the context of in vivo elastography. Here a new image processing pipeline called ESP (Elastography Software Pipeline) is developed to create Magnetic Resonance Elastography (MRE) maps of viscoelastic parameters (complex modulus magnitude |G*| and loss angle ϕ) that preserve fine-scale information through nonlinear, multi-scale extensions of typical MRE post-processing techniques.   METHODS A new MRE image processing pipeline was developed that incorporates wavelet-domain denoising, image-driven noise estimation, and feature detection. ESP was first validated using simulated data, including viscoelastic Finite Element Method (FEM) simulations, at multiple noise levels. ESP images were compared with MDEV pipeline images, both in the FEM models and in three ten-subject cohorts of brain, thigh, and liver acquisitions. ESP and MDEV mean values were compared to 2D local frequency estimation (LFE) mean values for the same cohorts as a benchmark. Finally, the proportion of spectral energy at fine frequencies was quantified using the Reduced Energy Ratio (RER) for both ESP and MDEV.   RESULTS Blind estimates of added noise (σ) were within 5.3% ± 2.6% of prescribed, and the same technique estimated σ in the in vivo cohorts at 1.7 ± 0.8%. A 5 × 5 × 5 truncated Gabor filter bank effectively detects local spatial frequencies at wavelengths λ ≤ 10px. For FEM inversions, mean |G*| of hard target, soft target, and background remained within 8% of prescribed up to σ=20%, and mean ϕ results were within 10%, excepting hard target ϕ, which required redrawing around a ring artefact to achieve similar accuracy. Inspection of FEM |G*| images showed some spatial distortion around hard target boundaries and inspection of ϕ images showed ring artefacts around the same target. For the in vivo cohorts, ESP results showed mean correlation of R=0.83 with MDEV and liver stiffness estimates within 7% of 2D-LFE results. Finally, ESP showed statistically significant increase in fine feature spectral energy as measured with RER for both |G*| (p<1×10-9) and ϕ (p<1×10-3).   CONCLUSION Information at finer frequencies can be recovered in ESP elastograms in typical experimental conditions, however scatter- and boundary-related artefacts may cause the fine features to have inaccurate values. In in vivo cohorts, ESP delivers an increase in fine feature spectral energy, and better performance with longer wavelengths, than MDEV while showing similar stability and robustness.		Eric Barnhill;Lyam Hollis;Ingolf Sack;Jürgen Braun;Peter R. Hoskins;Pankaj Pankaj;Colin Brown;Edwin J. R. van Beek;Neil Roberts	2017	Medical image analysis	10.1016/j.media.2016.05.012	computer vision;simulation;computer science;noise reduction	Visualization	57.934209376260824	-69.00369010158765	106644
6b698ac6b5608cb633e11c4e253337f6795bbd8e	stepwise ratio gm(1,1) model for image denoising	dynamics stepwise ratio;gm 11 model;image processing;signal detection;grey models;mean square error;peak signal to noise ratio;noise	To reduce image noise, we propose a novel image filter based on stepwise ratio grey model (SGM). The basic theory and the method of stepwise ratio grey prediction model are introduced first. The new filter makes use of neighborhoods around each noisy pixel to predict its intensity value and reflects the dynamics of stepwise ratio. The experimental results show that the proposed method, compared with the median filter and GM(1,1) model, improves the effect of the removal of impulse noise, such as salt & pepper noise. The improved algorithm can effectively eliminate image noise, preserve the image's details and edges, increase SNR(signal-to-noise ratio) as well as PSNR (peak signal-to-noise ratio), reduce MSE (mean square error) and MAE (mean absolute error), and significantly improve the image's visual effect. Therefore the proposed method is practicable.	algorithm;approximation error;composite image filter;image noise;impulse noise (audio);mean squared error;median filter;noise reduction;peak signal-to-noise ratio;pixel;salt (cryptography);salt-and-pepper noise;second generation multiplex;stepwise regression;visual effects	Sujin Yang;Xin Liu	2011	Proceedings of 2011 IEEE International Conference on Grey Systems and Intelligent Services	10.1108/20439371211197659	machine learning;pattern recognition;mathematics;non-local means;statistics	Robotics	57.40890454554888	-66.39702223938014	106712
0f1e5680c83daaf04581879748b16dd70341b24f	application of compressive sensing to portable ultrasound elastography		Feasibility of applying compressive sensing (CS) to ultrasound radio-frequency (RF) data to produce elastography is investigated. The research also compares the performance of various CS frameworks associated with three common model bases (Fourier transform, discrete cosine transform (DCT), and wave atom (WA)) and two reconstruction algorithms (ℓ1 minimization and block sparse Bayesian learning (BSBL)) using the quality of B-mode images and elastograms from the RF data subsampled and reconstructed by each framework. Results suggest that CS reconstruction adopting BSBL algorithm with DCT model basis can yield the best results for all the measures tested, and the maximum data reduction rate for producing readily discernable elastograms is around 60%.	algorithm;base;calcium-sensing receptor;compressed sensing;discrete cosine transform;elastography;high-frequency ventilation;radio frequency;sparse matrix	Bonghun Shin;Soo Jeon;Jeongwon Ryu;Hyock Ju Kwon	2017	2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2017.8037486	fourier transform;compressed sensing;computer vision;artificial intelligence;electronic engineering;data reduction;computer science;minification;discrete cosine transform;ultrasound elastography;elastography;bayesian inference	Visualization	54.87349128691792	-77.43053270332331	106762
f5c5f6eb21ba69605447c8f870c6ae2cbe4433e2	an efficient algorithm for ℓ 0 minimization in wavelet frame based image restoration	wavelet frames;image restoration;l 0 minimization;lminimization;augmented lagrangian	Wavelet frame based models for image restoration have been extensively studied for the past decade [1, 2, 3, 4, 5, 6]. The success of wavelet frames in image restoration is mainly due to their capability of sparsely approximating piecewise smooth functions like images. Most of the wavelet frame based models designed in the past are based on the penalization of the l1 norm of wavelet frame coefficients, which, under certain conditions, is the right choice, as supported by theories of compressed sensing [7, 8, 9]. However, the assumptions of compressed sensing may not be satisfied in practice (e.g. for image deblurring and CT image reconstruction). Recently in [10], the authors propose to penalize the l0 “norm” of the wavelet frame coefficients instead, and they have demonstrated significant improvements of their method over some commonly used l1 minimization models in terms of quality of the recovered images. In this paper, we propose a new algorithm, called the mean doubly augmented Lagrangian (MDAL) method, for l0 minimizations based on the classical doubly augmented Lagrangian (DAL) method [11]. Our numerical experiments show that the proposed MDAL method is not only more efficient than the method proposed by [10], but can also generate recovered images with even higher quality. This study reassures the feasibility of using the l0 “norm” for image restoration problems.	approximation algorithm;augmented lagrangian method;bregman divergence;ct scan;circuit restoration;coefficient;compressed sensing;converge;convex optimization;deblurring;experiment;image restoration;iterative reconstruction;mathematical optimization;numerical analysis;numerical method;penalty method;simulation;taxicab geometry;theory;wavelet	Bin Dong;Yong Zhang	2013	J. Sci. Comput.	10.1007/s10915-012-9597-4	image restoration;mathematical analysis;augmented lagrangian method;computer science;artificial intelligence;calculus;mathematics;algorithm;statistics;algebra	Vision	56.35828565280873	-71.99562202537508	107342
65a1ad79426d092582eab52f147d3a182dfdf2e3	a fast convergence method with simultaneous iterative reconstruction technique for computerized tomography	expectation maximization;dynamic system;iterative reconstruction;numerical method;fixed point	Considering a system of simultaneous iterative recon- struction technique (SIRT) for X-ray computerized tomography (CT) as a discrete dynamical system, the reconstruction process can be reduced to a procedure of finding a fixed point of the dynamical system. We examine a numerical method for solving fixed points of dynamical systems derived from the algebraic reconstruction tech- nique (ART) and the expectation maximization (EM) formulation, giv- ing rise to the very first convergence for CT reconstruction. Because the proposed method is based on the SIRT, it has an advantage for reducing metal artifact against the filtered backprojection procedure.	ct scan;iterative method;iterative reconstruction;tomography	Tetsuya Yoshinaga	1999	Int. J. Imaging Systems and Technology	10.1002/(SICI)1098-1098(1999)10:6%3C432::AID-IMA4%3E3.0.CO;2-I	iterative reconstruction;computer vision;mathematical optimization;radiology;expectation–maximization algorithm;numerical analysis;ordered subset expectation maximization;dynamical system;calculus;mathematics;fixed point	Vision	55.83966802340854	-73.82465950442553	107363
df074d945e8325281ef524482d92e0a65213267e	a multiresolution em algorithm for unsupervised image classification	multiresolution em algorithm;image resolution;iterative algorithms;markov random fields;image classification;maximum likelihood estimation;markov random field;computer vision;iterative methods;markov model;hidden markov models;stochastic processes;classification algorithms;expectation maximization algorithm;image resolution image classification hidden markov models markov random fields iterative algorithms spatial resolution maximum likelihood estimation stochastic processes classification algorithms computer vision;expectation maximization algorithm multiresolution em algorithm unsupervised image classification causal markov model quadtree parameter estimation maximum likelihood estimation iterative methods;causal markov model;parameter estimation;quadtree;em algorithm;quadtrees;unsupervised image classification;spatial resolution	We take beneet from a causal Markov model deened on a quadtree to derive a multiresolution EM algorithm for unsupervised image classiication. This algorithm is an eecient alternative to expensive or approximate EM algorithms associated with Markov Random Fields. We show on synthetic and real images that our algorithm also provides good or even better results than those obtained by spatial MRF models.	approximation algorithm;causal filter;computer vision;expectation–maximization algorithm;markov chain;markov model;markov random field;quadtree;synthetic intelligence	Jean-Marc Laferté;Fabrice Heitz;Patrick Pérez	1996		10.1109/ICPR.1996.547196	statistical classification;stochastic process;image resolution;expectation–maximization algorithm;computer science;machine learning;pattern recognition;mathematics;hidden markov model;statistics	ML	61.110140001693594	-72.29775666100392	107461
b1102a52f99a09ff52f98a7d1befd8c6ccaba08a	sinogram blurring matrix estimation from point sources measurements with rank-one approximation for fully 3-d pet		An accurate system matrix is essential in positron emission tomography (PET) for reconstructing high quality images. To reduce storage size and image reconstruction time, we factor the system matrix into a product of a geometry projection matrix and a sinogram blurring matrix. The geometric projection matrix is computed analytically and the sinogram blurring matrix is estimated from point source measurements. Previously, we have estimated a 2-D blurring matrix for a preclinical PET scanner. The 2-D blurring matrix only considers blurring effects within a transaxial sinogram and does not compensate for inter-sinogram blurring effects. For PET scanners with a long axial field of view, inter-sinogram blurring can be a major problem influencing the image quality in the axial direction. Hence, the estimation of a 4-D blurring matrix is desirable to further improve the image quality. The 4-D blurring matrix estimation is an ill-conditioned problem due to the large number of unknowns. Here, we propose a rank-one approximation for each blurring kernel image formed by a row vector of the sinogram blurring matrix to improve the stability of the 4-D blurring matrix estimation. The proposed method is applied to the simulated data as well as the real data obtained from an Inveon microPET scanner. The results show that the newly estimated 4-D blurring matrix can improve the image quality over those obtained with a 2-D blurring matrix and requires less point source scans to achieve similar image quality compared with an unconstrained 4-D blurring matrix estimation.	approximation;blurred vision;condition number;display resolution;electron tomography;image quality;iterative reconstruction;kernel (operating system);polyethylene terephthalate;positron-emission tomography;positrons;scanner device component;scanning;sinogram display;x-ray computed tomography	Kuang Gong;Jian Zhou;Michel S. Tohme;Martin S. Judenhofer;Yongfeng Yang;Jinyi Qi	2017	IEEE Transactions on Medical Imaging	10.1109/TMI.2017.2711479	computer vision;econometrics;mathematical optimization	Vision	55.82635031857362	-74.95773965682406	107514
327a35d1475c4add47bb0ce16736cdc4b3ff60cc	depth map upsampling via compressive sensing	image sampling;compressed sensing;upsampling depth map compressive sensing;image colour analysis;compressive sensing;sparse signal recovery depth map upsampling compressive sensing high resolution color images;upsampling;image sampling compressed sensing image colour analysis;depth map;image resolution color tv noise measurement compressed sensing optimization sensors	We propose a new method to enhance the lateral resolution of depth maps with registered high-resolution color images. Inspired by the theory of Compressive Sensing (CS), we formulate the up sampling task as a sparse signal recovery problem. With a reference color image, the low-resolution depth map is converted into suitable sampling data (measurements). The signal recovery problem, defined in a constrained optimization framework, can be efficiently solved with variable splitting and alternating minimization. Experimental results demonstrate the effectiveness of our CS-based method: it competes favorably with other state-of-the-art methods with large up sampling factors and noisy depth inputs.	color image;compressed sensing;constrained optimization;constraint (mathematics);depth map;detection theory;image resolution;lateral thinking;mathematical optimization;optimization problem;sampling (signal processing);sparse matrix;upsampling;variable splitting	Longquan Dai;Haoxing Wang;Xing Mei;Xiaopeng Zhang	2013	2013 2nd IAPR Asian Conference on Pattern Recognition	10.1109/ACPR.2013.11	computer vision;remote sensing;computer graphics (images)	Vision	57.37807851906268	-70.65204363478817	107570
8bb2958c70bf4a9db1c7153a25d56c3425a58908	sar imaging of moving targets by subaperture based low-rank and sparse decomposition		We propose a subaperture based method for synthetic aperture radar (SAR) imaging of moving targets. It exploits low-rank and sparse decomposition for extraction of moving targets from the complex SAR scene. First SAR raw data are divided into subapertures in the azimuth direction. Subsequently, low-rank and sparse decomposition is applied using the multiple subapertures data to accomplish the separation of moving targets from the stationary SAR background. A full resolution moving target image is reconstructed by combining the spectral information of the sparse subaperture images. Such an image has a high signal to clutter ratio and is well suited for motion estimation and focusing algorithms. This proposed framework extends the applicability of sparsity-driven moving target focusing methods to very low signal to clutter ratio environments. We demonstrate the performance of our approach through experiments with synthetic and real SAR data.	algorithm;clutter;experiment;motion estimation;sparse matrix;stationary process;synthetic data	Mubashar Yasin;Müjdat Çetin;Ahmed Shaharyar Khwaja	2017	2017 25th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2017.7960474	computer science;computer vision;iterative reconstruction;raw data;pattern recognition;artificial intelligence;inverse synthetic aperture radar;clutter;synthetic aperture radar;image resolution;motion estimation;sparse approximation	Vision	68.18228724443885	-66.72077346790213	107631
15225bdc2553c21c53805394ccea9c3216072a20	fusion of multispectral and hyperspectral images based on sparse representation	dictionaries optimization hyperspectral imaging image resolution bayes methods;alternating multiplier direction method hyperspectral image fusion multispectral image fusion sparse image representation hyperspectral image resolution inverse problem regularization term sparse decomposition dictionary image coding iterative optimization;traitement du signal et de l image;intelligence artificielle;vision par ordinateur et reconnaissance de formes;traitement des images;synthese d image et realite virtuelle;iterative methods decomposition dictionaries geophysical image processing hyperspectral imaging image coding image fusion image representation image resolution;alternating direction method of multipliers admm image fusion hyperspectral image multispectral image sparse representation	This paper presents an algorithm based on sparse representation for fusing hyperspectral and multispectral images. The observed images are assumed to be obtained by spectral or spatial degradations of the high resolution hyperspectral image to be recovered. Based on this forward model, the fusion process is formulated as an inverse problem whose solution is determined by optimizing an appropriate criterion. To incorporate additional spatial information within the objective criterion, a regularization term is carefully designed, relying on a sparse decomposition of the scene on a set of dictionaries. The dictionaries and the corresponding supports of active coding coefficients are learned from the observed images. Then, conditionally on these dictionaries and supports, the fusion problem is solved by iteratively optimizing with respect to the target image (using the alternating direction method of multipliers) and the coding coefficients. Simulation results demonstrate the efficiency of the proposed fusion method when compared with the state-of-the-art.	algorithm;augmented lagrangian method;coefficient;dictionary;image resolution;multispectral image;simulation;sparse approximation;sparse matrix	Qi Wei;Jos&#x00E9; M. Bioucas-Dias;Nicolas Dobigeon;Jean-Yves Tourneret	2014	2014 22nd European Signal Processing Conference (EUSIPCO)		computer vision;feature detection;image processing;machine learning;pattern recognition;mathematics;image fusion	Vision	67.73761742610051	-66.4042334965303	108032
29ced6b42e0e4e8e8566fd64b8f7ac1459b0cdfc	high resolution spectral analysis of images using the pseudo-wigner distribution	loi discrete;discrete distribution;ley discreta;interferencia;senal bidimensional;high resolution;image processing;image resolution;analytic representation;defasaje;aliasing;analytic signal;image resolution spectral analysis signal resolution signal analysis frequency distributed computing two dimensional displays spatial resolution interference image analysis;multicomponent analysis;low pass;procesamiento imagen;analisis multielemento;phase shift;dephasage;spectrum;interference;indexing terms;traitement image;antialiasing spectral analysis wigner distribution interference suppression image resolution;interference high resolution spectral analysis images pseudo wigner distribution discrete wigner distribution two dimensional analytic signals 2d analytic signals phase shift aliasing dwd low pass prefiltering spatial frequency support local spectral analysis small window sizes resolution cross terms;representation analytique;interference suppression;haute resolution;wigner distribution;antialiasing;analyse spectrale;alta resolucion;pseudo wigner distribution;analisis espectral;signal bidimensionnel;two dimensional signal;representacion analitica;spectral analysis;analyse multielement;spatial frequency;distribution pseudo wigner;repliegue espectro;repliement spectre	Several methods for the computation of the discrete Wigner Distribution (DWD) through the use of two-dimensional (2-D) analytic signals have been proposed, depending of the direction of the phase shift. Most of the methods cope the problem of aliasing of the DWD by lowpass prefiltering the spectrum but reducing spatial frequency support. In this correspondence, a new method for 2-D DWD free of aliasing and simultaneously increasing the spatial frequency support is proposed through the use of a new analytic signal. In this way, local spectral analysis for small window sizes can be accomplished improving the resolution but reducing cross terms and other interferences inherent in the DWD computation. For the three analytic images used in the comparison, the method proposed here also performs better than the other two for such tasks.	aliasing;analytic signal;computation;low-pass filter;spectral density estimation;wigner distribution function;wigner quasiprobability distribution	Javier Hormigo;Gabriel Cristóbal	1998	IEEE Trans. Signal Processing	10.1109/78.678519	computer vision;image resolution;image processing;computer science;calculus;mathematics;statistics	Visualization	64.47348876214973	-68.10932707239434	108227
1a3d4bd9dff54d8be8c9fc6320d177034fbbbe24	despeckling of ultrasound images of bone fracture using multiple filtering algorithms	psnr;anisotropic diffusion;median;ultrasound imaging;wiener;average;wavelet	Ultrasound images are popularly known to contain speckle noise that degrades the quality of the images for good and fast interpretation in many areas of medicine, especially for bone fracture detection. This necessitates the need for robust despeckling techniques for clinical practice. Therefore, a study was carried out to reduce speckle using filtering algorithms such as Wiener, Average, Median, Anisotropic Diffusion and Wavelets. This paper discusses the level of improvement obtained through these filtering algorithms using the peak signal-to-noise ratio (PSNR) as a measurement tool. The results of our work presented in this paper suggest that the combination of Daubechies–Wiener which we call as a hybrid technique with Anisotropic Diffusion, gave the best performance, which is a new contribution in this field. This despeckling algorithm can be further developed and evaluated at a larger scale.	algorithm;noise reduction	Irraivan Elamvazuthi;Muhammad Luqman Bin Muhd Zain;K. M. Begam	2013	Mathematical and Computer Modelling	10.1016/j.mcm.2011.07.021	wavelet;computer vision;speech recognition;peak signal-to-noise ratio;mathematics;anisotropic diffusion;median;statistics	Vision	56.773520044130656	-79.59023977303896	108250
fc83a1fd9e54db91b0bd01ce84d4b1c7ce472b52	a hybrid scheme of image compression employing wavelets and 2d-pca		In this paper, we have presented a method of compressing 2D grey-scale images employing wavelets and two-dimensional principal component analysis (2D-PCA). Principal component analysis (PCA) is an already established technique for image compression which primarily aims at exploiting inter pixel redundancies present in the image, while wavelet is a tool widely used in multi-resolution image processing. In the proposed method the image is subjected to a multi-resolution decomposition using wavelet. Subsequently, 2D-PCA is applied on the set of detail images at each level of resolution. The compressed form of the image is constituted by representative pairs of principal components and projection vectors from each level of resolution along with the approximate image at the coarsest resolution. The proposed method requires relatively few number of principal components (of varied dimension) to produce improved compression ratio with acceptable peak signal to noise ratio (PSNR). The method has been implemented a...	image compression;wavelet	Manoj K. Mishra;Rohit Ghosh;Susanta Mukhopadhyay	2017	IJCVR	10.1504/IJCVR.2017.10005882	pixel;computer vision;image compression;wavelet;compression ratio;artificial intelligence;image processing;principal component analysis;computer science;peak signal-to-noise ratio;pattern recognition;vector projection	Vision	60.00899057472119	-66.8481673757312	108400
236686ae11200c2f165a9f7e88c69944e33ece25	image recovery via nonlocal operators	mathematics;theoretical mathematical and computational physics;computational mathematics and numerical analysis;appl mathematics computational methods of engineering;spatial interaction;tomographic reconstruction;inverse problem;variational model;deconvolution;algorithms;nonlocal methods;weight function;variational models;tomography	This paper considers two nonlocal regularizations for image recovery, which exploit the spatial interactions in images. We get superior results using preprocessed data as input for the weighted functionals. Applications discussed include image deconvolution and tomographic reconstruction. The numerical results show our method outperforms some previous ones.		Yifei Lou;Xiaoqun Zhang;Stanley Osher;Andrea L. Bertozzi	2010	J. Sci. Comput.	10.1007/s10915-009-9320-2	mathematical optimization;mathematical analysis;weight function;inverse problem;deconvolution;mathematics;tomography;tomographic reconstruction	Vision	55.86323916390224	-71.65196568438371	109278
91eb155a4e8f7eac69cc19851351bcba78c3a1e7	fusing hyperspectral and multispectral images via coupled sparse tensor factorization		Fusing a low spatial resolution hyperspectral image (LR-HSI) with a high spatial resolution multispectral image (HR-MSI) to obtain a high spatial resolution hyperspectral image (HR-HSI) has attracted increasing interest in recent years. In this paper, we propose a coupled sparse tensor factorization (CSTF)-based approach for fusing such images. In the proposed CSTF method, we consider an HR-HSI as a 3D tensor and redefine the fusion problem as the estimation of a core tensor and dictionaries of the three modes. The high spatial-spectral correlations in the HR-HSI are modeled by incorporating a regularizer, which promotes sparse core tensors. The estimation of the dictionaries and the core tensor are formulated as a coupled tensor factorization of the LR-HSI and of the HR-MSI. Experiments on two remotely sensed HSIs demonstrate the superiority of the proposed CSTF algorithm over the current state-of-the-art HSI-MSI fusion approaches.	algorithm;circuit restoration;code;dictionary [publication type];horizontal situation indicator;image restoration;lr parser;multiplication;multispectral image;n-methylsuccinimide;nonlocal lagrangian;semiconductor industry;sparse matrix;heart rate;negative regulation of phospholipase c-activating g-protein coupled receptor signaling pathway	Shutao Li;Renwei Dian;Leyuan Fang;Jos&#x00E9; M. Bioucas-Dias	2018	IEEE Transactions on Image Processing	10.1109/TIP.2018.2836307	computer vision;tensor;multispectral image;artificial intelligence;hyperspectral imaging;pattern recognition;mathematics;factorization	Vision	66.87834758076183	-67.37275665923163	109629
daf3e1135fc8db96bdb2fb3faedb588df65f1957	a collaborative enhancement-compression approach for historical document images based on pde-analysis	enhancement;jpeg xr;historical document image;jpeg 2000;compression;diffusion	Historical document images suffer from several kinds of degradation affecting their readability and increase their sizes when digitizing. Indeed, preserving the old document heritage requires an important storage infrastructures, while this space could be significantly reduced if one manages to safeguard these documents without their considerable amount of artifacts which increasing their entropy. We present in this work a new and more advantageous solution for the document image compression technology dedicated to National Archives and Libraries. The aim of this work is to propose a novel historical document compression scheme judiciously combined with a novel automatic enhancement scheme, both based on PDE-Analysis applied in an innovative way, to face this challenging issue without affecting the look and feel of the historical document image. Throughout experiment, the proposal is fairly evaluated on overall DIBCO datasets and some historical documents collected from the web by means of objective measures and perceptual judgment against the most used enhancement algorithms, the most robust compression standards such as JPEG-XR and the most used sequential compression–enhancement combinations techniques to proof the robustness of the proposed collaborative approach against classic sequential approaches.	historical document	Mohamed Riad Yagoubi;Amina Serir;Azeddine Beghdadi	2017	Digital Signal Processing	10.1016/j.dsp.2017.04.009	robustness (computer science);look and feel;historical document;data mining;image compression;information retrieval;jpeg 2000;computer science	Graphics	61.80124315457488	-67.76421887202098	109806
bbdcb4129e3f6fded8fa85a9f3e0c4848ae61628	noise reduction and edge detection via kernel anisotropic diffusion	traitement signal;evaluation performance;detection forme;proceso difusion;performance evaluation;methode noyau;edge detection;evaluacion prestacion;kernel methods;processus diffusion;diffusion anisotrope;anisotropic diffusion;shape detection;reduccion ruido;deteccion contorno;detection contour;deteccion forma;difusion anisotropica;signal processing;noise reduction;robustesse;metodo nucleo;reduction bruit;kernel method;robustness;rapport signal bruit;diffusion process;relacion senal ruido;anisotropic scattering;signal to noise ratio;estimacion adaptativa;procesamiento senal;adaptive estimation;estimation adaptative;robustez	A novel kernel anisotropic diffusion (KAD) method is proposed for robust noise reduction and edge detection. The KAD incorporates a kernelized gradient operator in the diffusion, leading to more effective edge detection and providing a better control to the diffusion process. Adaptive diffusion threshold estimation and automatic diffusion termination criterion are also introduced to enhance the robustness of the KAD. The KAD outperforms several previous anisotropic diffusion-based methods for low SNR images.	anisotropic diffusion;edge detection;kernel (operating system);noise reduction	Jin-Hua Yu;Yuanyuan Wang;Yuzhong Shen	2008	Pattern Recognition Letters	10.1016/j.patrec.2008.03.002	computer vision;kernel method;speech recognition;computer science;signal processing;mathematics;anisotropic diffusion	Vision	54.00593547489776	-67.19045882702649	109960
f7254be68795a274b22701ae6746e6016764b4c8	trilateral filter on graph spectral domain	trilateral filter;graph signal processing;smoothing methods frequency domain analysis graph theory image denoising image representation;denoising trilateral filter bilateral filter graph signal processing spectral graph theory;spectral analysis image edge detection smoothing methods noise reduction kernel eigenvalues and eigenfunctions;spectral graph theory;bilateral filter;denoising;frequency domain representation graph spectral domain trilateral filter graph signal processing single pass nonlocal filter edge preserving smoothing methods tf coefficients image data	This paper presents the trilateral filter (TF) in the perspective of graph signal processing. The TF is a single-pass nonlocal filter for edge-preserving smoothing. To smooth an image, it does not require many iterations compared to conventional smoothing methods, e.g., the bilateral filter. Additionally, one parameter is only required for filtering. Since the TF coefficients depend on original image data, it is not possible to provide a frequency domain representation using regular signal processing. To overcome this problem, we firstly show the TF as a vertex domain transform on a graph and then define it on graph spectral domain. In the experimental results, the proposed method presents better denoising performances than conventional methods.	bilateral filter;coefficient;edge-preserving smoothing;filter (signal processing);iteration;noise reduction;nonlocal lagrangian;performance;signal processing	Masaki Onuki;Yuichi Tanaka	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025410	filter;edge-preserving smoothing;computer vision;mathematical optimization;discrete mathematics;kernel adaptive filter;computer science;pattern recognition;noise reduction;mathematics;filter design;bilateral filter;spectral graph theory	Robotics	55.734406025197664	-68.45638634902576	110060
f50d5287362fdba28eed25048a21a2ec8b6b9d3b	3d medical image enhancement based on wavelet transforms		This paper studies 2D and 3D wavelet domain medical image resolution enhancement method. The proposed approach is based on the interpolation of the low resolution input image and the derived high frequency sub-band images obtained using Discrete Wavelet Transform (DWT). Experimental results on both 2D and 3D images show how our method enhances the image’s details and preserves edges.	block size (cryptography);coefficient;discrete wavelet transform;emoticon;image editing;image resolution;interpolation;peak signal-to-noise ratio;standard widget toolkit;stationary process	Amir Yavariabdi;Chafik Samir;Adrien Bartoli	2011			wavelet;computer vision;wavelet transform;second-generation wavelet transform;artificial intelligence;stationary wavelet transform;mathematics;image resolution;harmonic wavelet transform;discrete wavelet transform;wavelet packet decomposition	Vision	56.484703823811266	-68.02349678934398	110144
d0bb8bfb772ebc5aa4fd2734b821302065ba5337	double well potential as diffusive function for pde-based scalar image restoration method	image restoration;double well potential	Anisotropic regularization PDE’s (Partial Differential Equation) raised a strong interest in the field of image processing. The benefit of PDE-based regularization methods lies in the ability to smooth data in a nonlinear way, allowing the preservation of important image features (contours, corners or other discontinuities). In this article, we propose a PDE-based method restoration approach integrating a double-well potential as diffusive function. It is shown that this particular potential leads to a particular regularization PDE which makes it possible integration of prior knowledge about the gradients intensity level to restore. As a proof a feasibility, results of restoration are presented both on ad hoc and natural images to show potentialities of the proposed method.	circuit restoration;double-well potential;gradient;hoc (programming language);image processing;image restoration;manifold regularization;matrix regularization;nonlinear system;scalar processor	Aymeric Histace;Michel Ménard	2009			image restoration;mathematical optimization;computer science	Vision	54.68502833167468	-70.7899225198231	110411
b96ecdbfc0678ebe653be2a08957754f06c96f06	total variation regularization via continuation to recover compressed hyperspectral images	smoothing methods convergence of numerical methods convex programming decoding gradient methods hyperspectral imaging image coding image reconstruction;image quality improvement total variation regularization hyperspectral image compression recovery low complexity scheme subgradient method variation based regularization problem image reconstruction algorithm residual constraint convergence rate improvement numerical experiment real hyperspectral data synthetic hyperspectral data;image coding;hyperspectral imaging image reconstruction tv optimization image coding materials;materials;inverse problems compressed sensing hyperspectral imaging;image reconstruction;optimization;tv;hyperspectral imaging	In this paper, we investigate a low-complexity scheme for decoding compressed hyperspectral image data. We have exploited the simplicity of the subgradient method by modifying a total variation-based regularization problem to include a residual constraint, employing convex optimality conditions to provide equivalency between the original and reformed problem statements. A scheme that utilizes spectral smoothness by calculating informed starting points to improve the rate of convergence is introduced. We conduct numerical experiments, using both synthetic and real hyperspectral data, to demonstrate the effectiveness of the reconstruction algorithm and the validity of our method for exploiting spectral smoothness. Evidence from these experiments suggests that the proposed methods have the potential to improve the quality and run times of the future compressed hyperspectral image reconstructions.	algorithm;bands;computation;continuation;convergence (action);experiment;feasible region;horizontal situation indicator;image processing;mathematical optimization;numerical analysis;population parameter;rate of convergence;solutions;stellar classification;subgradient method;synthetic intelligence;total variation denoising;undersampling;well-posed problem;while;methyl-isp	Duncan T. Eason;Mark Andrews	2015	IEEE Transactions on Image Processing	10.1109/TIP.2014.2376273	iterative reconstruction;computer vision;mathematical optimization;hyperspectral imaging;machine learning;mathematics	Vision	57.22513031161322	-72.22795617604136	110609
164574b4e7c08842fe975dd4f28ea6f5965248b4	nonlocal linear minimum mean square error methods for denoising mri	discrete cosine transform;linear minimum mean square error;magnetic resonance image;rician distribution;principal component analysis;denoising	The presence of noise results in quality deterioration of magnetic resonance (MR) images and thus limits the visual inspection and influence the quantitative measurements from the data. In this work, an efficient two stage linear minimum mean square error (LMMSE) method is proposed for the enhancement of magnitude MR images in which data in the presence of noise follows a Rician distribution. The conventional Rician LMMSE estimator determines a closed-form analytical solution to the aforementioned inverse problem. Even-though computationally efficient, this approach fails to take advantage of data redundancy in the 3D MR data and hence leads to a suboptimal filtering performance. Motivated by this observation, we put forward the concept of nonlocal implementation with LMMSE estimation method. To select appropriate samples for the nonlocal version of the LMMSE estimation, the similarity weights are computed using Euclidean distance between either the gray level values in the spatial domain or the coefficients in the transformed domain. Assuming that the signal dependent component of the noise ician distribution is optimally suppressed by this filtering and the rest is a white and uncorrelated noise with the image, we adopt a second stage LMMSE filtering in the principal component analysis (PCA) domain to further enhance the image and the noise variance is adaptively adjusted. Experiments on both simulated and real data show that the proposed filters have excellent filtering performance over other state-of-the-art methods. © 2015 Elsevier Ltd. All rights reserved.	aharonov–bohm effect;algorithmic efficiency;coefficient;data redundancy;euclidean distance;experiment;grayscale;kernel density estimation;mean squared error;noise reduction;nonlocal lagrangian;principal component analysis;resonance;simulation;visual inspection;white noise	P. V. Sudeep;Praveen Palanisamy;Chandrasekharan Kesavadas;Jeny Rajan	2015	Biomed. Signal Proc. and Control	10.1016/j.bspc.2015.04.015	econometrics;mathematical optimization;computer science;magnetic resonance imaging;machine learning;discrete cosine transform;noise reduction;mathematics;statistics;principal component analysis	Vision	58.32191816140971	-68.20021968855082	110633
1e534ef0a6d895c63aeeb37d94c119206d258a66	multiframe demosaicing and super-resolution of color images	robust regularization;resolution;regularisation;restauration image;image segmentation;motion estimation image segmentation image resolution optical filters maximum likelihood estimation;image processing;robust estimator;accentuation image;image resolution;color enhancement;cost function;optical filters;demosaicing;algorithms color colorimetry image enhancement image interpretation computer assisted information storage and retrieval signal processing computer assisted subtraction technique;low resolution;procesamiento imagen;digital camera;motion estimation;image restoration;indexing terms;cost function multiframe demosaicing color image superresolution image restoration color digital cameras low spatial resolution color filtering maximum a posteriori estimation multiterm cost function motion estimation bilateral regularization luminance component tikhonov regularization chrominance components;maximum likelihood estimation;traitement image;robust estimation;regularization;digital cameras;restauracion imagen;image enhancement;maximum a pos teriori;hybrid method;mathematical models;image resolution color spatial resolution cost function motion estimation image restoration digital cameras robustness maximum a posteriori estimation motion measurement;super resolution;regularizacion;high resolution imager;reprints;superresolution;tikhonov regularization;color photography;imagen color;superresolucion;image couleur;estimates;super resolution color enhancement demosaicing image restoration robust estimation robust regularization;color image;spatial resolution	In the last two decades, two related categories of problems have been studied independently in image restoration literature: super-resolution and demosaicing. A closer look at these problems reveals the relation between them, and, as conventional color digital cameras suffer from both low-spatial resolution and color-filtering, it is reasonable to address them in a unified context. In this paper, we propose a fast and robust hybrid method of super-resolution and demosaicing, based on a maximum a posteriori estimation technique by minimizing a multiterm cost function. The L/sub 1/ norm is used for measuring the difference between the projected estimate of the high-resolution image and each low-resolution image, removing outliers in the data and errors due to possibly inaccurate motion estimation. Bilateral regularization is used for spatially regularizing the luminance component, resulting in sharp edges and forcing interpolation along the edges and not across them. Simultaneously, Tikhonov regularization is used to smooth the chrominance components. Finally, an additional regularization term is used to force similar edge location and orientation in different color channels. We show that the minimization of the total cost function is relatively easy and fast. Experimental results on synthetic and real data sets confirm the effectiveness of our method.	bilateral filter;categories;cell hybridization;channel (digital image);circuit restoration;demosaicing;digital camera;image resolution;image restoration;interpolation;loss function;manifold regularization;motion estimation;projections and predictions;super-resolution imaging;synthetic intelligence	Sina Farsiu;Michael Elad;Peyman Milanfar	2006	IEEE Transactions on Image Processing	10.1109/TIP.2005.860336	demosaicing;computer vision;image resolution;image processing;computer science;mathematics;computer graphics (images)	Vision	55.60427024871475	-72.88958573793522	110712
7fc4e3fd51ee735d6d68a7dfcec219ef7ebc7c98	a novel thresholding algorithm for image deblurring beyond nesterov’s rule		Image deblurring problem is a tough work for improving the quality of images, in this paper; we develop an efficient and fast thresholding algorithm to handle such problem. We observe that the improved fast iterative thresholding algorithm (IFISTA) can be further accelerated by using a sequence of over relaxation parameters which do not satisfy the Nesterov’s rule. Our proposed algorithm preserves the simplicity of the IFISTA and fast iterative shrinkage thresholding algorithm (FISTA). In addition, we theoretically study the convergence of our proposed algorithm and obtain some improved convergence rate. Furthermore, we investigate the local variation of iterations which is still unknown in FISTA and IFISTA algorithms so far. Extensive experiments have been conducted and show that our proposed algorithm is more efficient and robust. Specifically, we compare our proposed algorithm with FISTA and IFISTA algorithms on a series of scenarios, including the different level noise signals as well as different weighting matrices. All results demonstrate that our proposed algorithm is able to achieve better recovery performance, while being faster and more efficient than others.	algorithm;deblurring;experiment;fast fourier transform;iteration;linear programming relaxation;rate of convergence;thresholding (image processing)	Zhi Wang;Jianjun Wang;Wendong Wang;Chao Gao;Muna AlSaadi	2018	IEEE Access	10.1109/ACCESS.2018.2873628	image restoration;robustness (computer science);rate of convergence;computer science;matrix (mathematics);deblurring;thresholding;algorithm;convergence (routing);weighting	ML	56.51660143696326	-71.38418994634456	110899
f46ca29f69ac6e485aed10b837903b65a10db40f	k-space deep learning for accelerated mri		The annihilating filter-based low-rank Hanel matrix approach (ALOHA) is one of the state-of-the-art compressed sensing approaches that directly interpolates the missing k-space data using low-rank Hankel matrix completion. Inspired by the recent mathematical discovery that links deep neural networks to Hankel matrix decomposition using data-driven framelet basis, here we propose a fully data-driven deep learning algorithm for k-space interpolation. Our network can be also easily applied to non-Cartesian k-space trajectories by simply adding an additional re-gridding layer. Extensive numerical experiments show that the proposed deep learning method significantly outperforms the existing image-domain deep learning approaches.	algorithm;artificial neural network;cartesian closed category;compressed sensing;deep learning;experiment;interpolation;numerical analysis	Yoseob Han;Jong Chul Ye	2018	CoRR		artificial neural network;machine learning;compressed sensing;interpolation;deep learning;computer science;k-space;hankel matrix;matrix (mathematics);artificial intelligence	ML	57.726062665433176	-74.01242473580182	110980
bf14219b43ad7eed2dbacdb978b036dc08c46988	continuous multiclass labeling approaches and algorithms	saddle point problem;combinatorial problems;65d18;global convergence;segmentation;65k10;first order;90c27;68u10;90c25;continuous cut;pattern recognition;total variation;nonsmooth optimization;convex relaxation;49m20;variational methods;splitting methods	We study convex relaxations of the image labeling problem on a continuous domain with regularizers based on metric interaction potentials. The generic framework ensures existence of minimizers and covers a wide range of relaxations of the originally combinatorial problem. We focus on two specific relaxations that differ in flexibility and simplicity – one can be used to tightly relax any metric interaction potential, while the other one only covers Euclidean metrics but requires less computational effort. For solving the nonsmooth discretized problem, we propose a globally convergent Douglas-Rachford scheme, and show that a sequence of dual iterates can be recovered in order to provide a posteriori optimality bounds. In a quantitative comparison to two other first-order methods, the approach shows competitive performance on synthetical and real-world images. By combining the method with an improved binarization technique for nonstandard potentials, we were able to routinely recover discrete solutions within 1%–5% of the global optimum for the combinatorial image labeling problem. 1 Problem Formulation The multi-class image labeling problem consists in finding, for each pixel x in the image domain Ω ⊆ R a label l(x) ∈ {1, . . . , l} which assigns one of l class labels to x so that the labeling function l adheres to some local data fidelity as well as nonlocal spatial coherency constraints. This problem class occurs in many applications, such as segmentation, multiview reconstruction, stitching, and inpainting [PCF06]. We consider the variational formulation inf l:Ω→{1,...,l} f(u), f(u) := ∫ Ω s(x, l(x))dx } {{ } data term + J(l). } {{ } regularizer (1) The data term assigns to each possible label l(x) a local cost s(x, l(x)), while the regularizer J enforces the desired spatial coherency. We will in particular be interested in regularizers that penalize the weighted length of boundaries between regions	algorithm;calculus of variations;computation;discretization;first-order predicate;global optimization;image segmentation;image stitching;inpainting;multiclass classification;multiview video coding;pixel;quantum nonlocality	Jan Lellmann;Christoph Schnörr	2011	SIAM J. Imaging Sciences	10.1137/100805844	mathematical optimization;combinatorics;mathematical analysis;discrete mathematics;computer science;first-order logic;mathematics;total variation;segmentation	ML	54.239162266456695	-72.4305575378132	111412
d4d5149f840b44c73aa7b10e669eb60717d0f435	an image super-resolution scheme based on compressive sensing with pca sparse representation	primary component analysis pca;super resolution reconstruction;compressive sensing cs	Image super-resolution (SR) reconstruction has been an important research fields due to its wide applications. Although many SR methods have been proposed, there are still some problems remain to be solved, and the quality of the reconstructed high-resolution (HR) image needs to be improved. To solve these problems, in this paper we propose an image super-resolution scheme based on compressive sensing theory with PCA sparse representation. We focus on the measurement matrix design of the CS process and the implementation of the sparse representation function for the PCA transformation. The measurement matrix design is based on the relation between the low-resolution (LR) image and the reconstructed high-resolution (HR) image. While the implementation of the PCA sparse representation function is based on the PCA transformation process. According to whether the covariance matrix of the HR image is known or not, two kinds of SR models are given. Finally the experiments comparing the proposed scheme with the traditional interpolation methods and CS scheme with DCT sparse representation are conducted. The experiment results both on the smooth image and the image with complex textures show that the proposed scheme in this paper is effective.	compressed sensing;sparse approximation;super-resolution imaging	Aixin Zhang;Chao Guan;Haomiao Jiang;Jianhua Li	2012		10.1007/978-3-642-40099-5_40	computer vision;sparse pca;machine learning;pattern recognition;mathematics	Vision	59.75321821388931	-68.6423015877258	111586
f7d7a6edbe749a504eff029e28f1b6ede492d49e	an edge fusion scheme for image denoising based on anisotropic diffusion models	enhanced denoising model;anisotropic diffusion;edge fusion scheme;image denoising	In this paper, we propose an enhanced anisotropic diffusion model. The improved model can classify finely image information as smooth regions, edges, corners and isolated noises by characteristic parameters and gradient variance parameter. And for different image information the eigenvalues of diffusion tensor are designed to conduct adaptive diffusion. Moreover, an edge fusion scheme is posed to preserve edges after denoising by combing different denoising and edge detection methods. Firstly, different denoising methods are applied for noisy image to obtain denoised images, and the best method among them is selected as main method. Then edge images of denoised images are obtained by edge detection methods. Finally, by fusing edge images together more integrated edges can be achieved to replace edges of denoised image obtained by main method. The experimental results show the proposed model can denoise meanwhile preserve edges and corners, and the edge fusion scheme is accurate and effective.	anisotropic diffusion;edge detection;edge enhancement;gradient;noise reduction	Hongjin Ma;Yufeng Nie	2016	J. Visual Communication and Image Representation	10.1016/j.jvcir.2016.06.027	computer vision;mathematical optimization;computer science;pattern recognition;mathematics;anisotropic diffusion	Vision	56.779478790412384	-67.30440011708768	111766
7b9de5903cf325df9e32ee6cf13b1d3e4cfe1a41	unsupervised markovian segmentation of sonar images	iterative method;high resolution performance unsupervised markovian segmentation estimation segmentation procedure unsupervised sonar image segmentation iterative conditional estimation iterative method distribution mixture parameter estimation label field markov random field mrf maximum likelihood estimation noise model parameters least square method convergence rate multigrid strategy automatic information extraction ice algorithm;ice algorithm;high resolution performance;least squares approximations;image segmentation;sonar applications;image resolution;mrf;distribution mixture;estimation segmentation procedure;convergence of numerical methods;image segmentation parameter estimation ice maximum likelihood estimation sonar applications sonar detection layout iterative methods markov random fields maximum likelihood detection;markov random fields;noise model parameters;least square method;convergence rate;layout;maximum likelihood estimation;sonar imaging;markov random field;sonar detection;unsupervised sonar image segmentation;iterative methods;maximum likelihood estimate;unsupervised markovian segmentation;feature extraction;image segmentation sonar imaging maximum likelihood estimation markov processes iterative methods convergence of numerical methods feature extraction least squares approximations noise image resolution random processes;random processes;iterative conditional estimation;maximum likelihood detection;automatic information extraction;markov processes;parameter estimation;iteration method;multigrid strategy;ice;label field;noise	This work deals with unsupervised sonar image segmentation. We present a new estimation segmentation procedure using the recent iterative method of estimation called Iterative Conditional Estimation (ICE). This method takes into account the variety of the laws in the distribution mixture of a sonar image and the estimation of the parameters of the label eld (modeled by a Markov Random Field (MRF)). For the estimation step we use a maximum likelihood estimation for the noise model parameters and the least square method proposed by Derin et al. to estimate the MRF prior model. Then, in order to obtain a good segmentation and to speed up the convergence rate, we use a multigrid strategy with the previously estimated parameters. This technique has been sucessfully applied to real sonar images and is compatible with an automatic treatment of massive amounts of data.	image segmentation;iterative method;markov chain;markov random field;multigrid method;rate of convergence;sonar (symantec);unsupervised learning	Max Mignotte;Christophe Collet;Patrick Pérez;Patrick Bouthemy	1997		10.1109/ICASSP.1997.595366	stochastic process;computer science;machine learning;pattern recognition;mathematics;iterative method;image segmentation;scale-space segmentation;statistics	Vision	60.903552833579795	-72.3592210769517	111812
8f695c9dc2e669671d010fe56679f8a312e35dcc	study on medical image processing algorithm based on contourlet transform and correlation theory	mutli scale analysis;anamorphic image;image processing contourlet transform correlation theory medical image;high resolution;image coding;filter bank;image processing;computed tomography;correlation theory;time frequency;biomedical image processing biomedical imaging medical diagnostic imaging transportation photography performance analysis algorithm design and analysis anisotropic magnetoresistance image reconstruction image coding;wavelet transform medical image processing contourlet transform correlation theory anamorphic image tight bracing mutli scale analysis;wavelet transforms correlation theory medical image processing;wavelet transforms;wavelet transform;tight bracing;medical image;medical image processing;noise reduction;transforms;contourlet transform;multi scale analysis;medical diagnostic imaging	In recent years, processed medical image becomes more and more important to diagnosis. Anamorphic image may result in wrong judgment on state of an illness, even leads to fateful danger. Meanwhile, more efficient data transportation and storage are required with the development of high-resolution photography. Therefore, thousands of researchers work on the field of medical image processing. In this paper, contourlet transform, which may provide with tight bracing and mutli-scale analysis, is introduced. And a new medical image processing algorithm based on contourlet transform and correlation theory is presented. This algorithm possesses some excellent performances such as multi-scale analysis, time-frequency-localization and multi-directions. Especially, this algorithm has excellent performance when describing anisotropic 2-D data. So high-quality medical image can be reconstructed even though a relative few coefficients are employed.To verify the algorithm, some medical images were processed. The experimental results show that this algorithm has better performance in compression and denoising than that of wavelet transform.	algorithm;coefficient;contourlet;image processing;image resolution;medical imaging;noise reduction;performance;wavelet transform	Yan Kang	2009	2009 WRI World Congress on Computer Science and Information Engineering	10.1109/CSIE.2009.1010	computer vision;speech recognition;image processing;computer science;mathematics;top-hat transform;wavelet transform;computer graphics (images)	ML	56.83308140315014	-79.64408327180125	111918
c91ec9a6c4f791ce63f6c747dab090d26b882894	nonlinear multiresolution gradient adaptive filter for medical images	edge enhancement;medical imaging modalities;image processing;magnetism;ultrasound;low frequency noise;angiography;band pass;medical image;magnetic resonance;noise reduction;real time implementation;ultrasonography;denoising;computer hardware;multi resolution;fluoroscopy;adaptive filter;scale dependence;x rays	"""We present a novel method for intra-frame image processing, which is applicable to a wide variety of medical imaging modalities, like X-ray angiography, X-ray fluoroscopy, magnetic resonance, or ultrasound. The method allows to reduce noise significantly - by about 4.5 dB and more - while preserving sharp image details. Moreover, selective amplification of image details is possible. The algorithm is based on a multi-resolution approach. Noise reduction is achieved by non-linear adaptive filtering of the individual band pass layers of the multi-resolution pyramid. The adaptivity is controlled by image gradients calculated from the next coarser layer of the multi-resolution pyramid representation, thus exploiting cross-scale dependencies. At sites with strong gradients, filtering is performed only perpendicular to the gradient, i.e. along edges or lines. The multi-resolution approach processes each detail on its appropriate scale so that also for low frequency noise small filter kernels are applied, thus limiting computational costs and allowing a real-time implementation on standard hardware. In addition, gradient norms are used to distinguish smoothly between """"structure"""" and """"noise only"""" areas, and to perform additional noise reduction and edge enhancement by selectively attenuating or amplifying the corresponding band pass coefficients.© (2003) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only."""		Dietmar Kunz;Kai Eck;Holger Fillbrandt;Til Aach	2003		10.1117/12.481323	gradient noise;median filter;computer vision;electronic engineering;simulation;computer science	Vision	56.34449588811251	-68.4421672962636	112430
661ab9c8cb2809392d8ea3d83416e27ad69d3f0e	inference on gibbs optic-flow priors : application to atmospheric turbulence characterization	geophysical image processing;model selection;image motion analysis;self similar process;bayes methods;bayesian inference;inverse motion estimation problem;atmospheric turbulence;bayesian methods;motion estimation;polynomials;multiscale modeling;gibbs random fields;image generation;meteorological second generation image sequences;gibbs optic flow priors;multiscale spatial priors;image sequence;motion estimation atmospheric modeling bayesian methods image sequences meteorology polynomials coherence image generation inverse problems image motion analysis;bayesian evidence;coherence;optical flow;image sequences atmospheric turbulence bayes methods geophysical image processing;atmospheric modeling;self similar process gibbs optic flow priors atmospheric turbulence bayesian inference motion estimation multiscale spatial priors inverse motion estimation problem meteorological second generation image sequences gibbs random fields;meteorology;optic flow;atmospheric turbulence bayesian evidence gibbs random fields optic flow self similar process;inverse problems;image sequences;random field	In this paper, Bayesian inference is used to select the most evident Gibbs prior model for motion estimation given some image sequence. The proposed method supplements the maximum a posteriori motion estimation scheme proposed in He¿as et al. (2008). Indeed, in this recent work, the authors have introduced a family of multiscale spatial priors in order to cure the ill-posed inverse motion estimation problem. We propose here a second level of inference where the most likely prior model is optimally chosen given the data by maximization of Bayesian evidence. Model selection and motion estimation are assessed on Meteorological Second Generation (MSG) image sequences. Selecting from images the most evident multiscale model enables the recovery of physical quantities which are of major interest for atmospheric turbulence characterization.	expectation–maximization algorithm;model selection;motion estimation;optical flow;turbulence;well-posed problem	Patrick Héas;Étienne Mémin	2009	2009 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2009.5417901	computer vision;econometrics;optical flow;mathematics;statistics	Vision	66.93900043362787	-68.85086774314208	112809
ff974543eba4c37905680a0dfeb8befb88da28e8	interpolation filter design based on all-phase dst and its application to image demosaicking		Based on a deep understanding of all-phase digital filter (APDF) design and all-phase biorthogonal transform (APBT), this paper will further study the windowed all-phase digital filter (WAPDF) and windowed all-phase biorthogonal transform (WAPBT), discuss the principle of the WAPBT, and provide a unified construction method of the all-phase transform (APT). Based on a type of orthogonal transform, i.e., discrete sine transform (DST), an interpolation filter called an all-phase DST (APDST) filter is constructed and used for image demosaicking; it is compared with bilinear interpolation and all-phase inverse discrete cosine transform (APIDCT) interpolation filters, to test its performance in image interpolation and provide analysis and discussion. The experimental results show that APIDCT and APDST filters with a size of 7 × 7 are similar in interpolation performance, but better than the bilinear interpolation method. In addition to its use in image interpolation demosaicking, the low-pass filter designed in this paper can also be widely used in image interpolation, image denoising, image resizing, and other fields of image processing.	algorithm;bilinear filtering;computational complexity theory;demosaicing;digital filter;discrete cosine transform;discrete sine transform;experiment;filter design;image processing;image scaling;interpolation;low-pass filter;microsoft windows;noise reduction;window function	Xiao Zhou;Chengyou Wang;Zhi Zhang;Qiming Fu	2018	Information	10.3390/info9090206	machine learning;interpolation;demosaicing;computer science;discrete cosine transform;computer vision;filter design;digital filter;image scaling;bilinear interpolation;discrete sine transform;artificial intelligence	Vision	56.451458315599396	-67.86481563788267	113119
71174922a6a766dab008686fdd55c7b6c50d374e	show-through cancellation in scanned images using blind source separation techniques	icm image separation scanned images bss total variation;linear mixed model;image processing;cost function;mean square error methods blind source separation convolution image processing;convolution;blind source separation;optimal method;indexing terms;scanned images;mean square error;bss;image separation;mean square error methods;blind source separation cost function printing cleaning optimization methods source separation image quality decorrelation parameter estimation minimization methods;total variation;convolutive mixture;icm;location dependent regularization tradeoff show through cancellation scanned image blind source separation techniques scanning duplex printed document nonlinear convolutive mixture decorrelation mean squared error term	Show-Through is a common occurrence when scanning duplex printed documents. The back-side printing shows through the paper, contaminating the front side image. Previous work modeled the problem as a non-linear convolutive mixture of images and offered solutions based on decorrelation. In this work we propose a cleaning process based on a Blind Source Separation approach. We define a cost function incorporating the non-linear mixing model in a mean-squared error term, along with a regularization term based on Total-Variation. We propose a location dependent regularization tradeoff, preserving image edges while removing show-through edges. The images and mixing parameters are estimated using an alternating minimization process, with each stage using only convex optimization methods. The resulting images exhibit significantly lower show-through, both visibly and in objective measures.	blind signal separation;convex optimization;decorrelation;duplex (telecommunications);loss function;mathematical optimization;matrix regularization;mean squared error;nonlinear system;plasma cleaning;printing;source separation	Boaz Ophir;David Malah	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379289	mixed model;mathematical optimization;index term;image processing;computer science;.bss;machine learning;pattern recognition;mathematics;mean squared error;blind signal separation;convolution;total variation	Vision	58.762749890064676	-72.2757171581082	113376
8c50c60d1e2f1656f6cc3f80deebf3dbe484f0ae	an adaptive fixed-point proximity algorithm for solving total variation denoising models		We study an adaptive fixed-point proximity algorithm to solve the total variation denoising model. The objective function is a sum of two convex functions and one of them is composed by an affine transformation, which is usually a regularization term. By decoupling and splitting, the problem is changed into two subproblems. Due to the nonsmooth and nondifferentiability of the subproblem, we solve its proximity minimization problem instead of the original one. To overcome the “staircase” effect during the process of denoising, an adaptive criterion on proximity parameters is put forward. At last we apply the improved algorithm to solve the isotropic total variation denoising model. The numerical results are given to illustrate the efficiency of the algorithm.	algorithm;noise reduction;total variation denoising	Jin-He Wang;Fan-Yun Meng;Li-Ping Pang;Xing-Hua Hao	2017	Inf. Sci.	10.1016/j.ins.2017.03.023	mathematical optimization;combinatorics;calculus;mathematics;total variation denoising	Logic	55.64151998946061	-71.3396925925304	113501
41ef413937c9b5aa4af0d155de366a45273c36fa	image denoising based on sparse representation and gradient histogram	gradient histogram prior;image priors;image texture;false textures;latent image;non local self similarity prior;image denoising;natural texture appearance;sparse representation;texture enhancement;sparsity prior;noise removal;sparse coding coefficients	Various image priors, such as sparsity prior, non-local self-similarity prior and gradient histogram prior, have been widely used for noise removal, while preserving the image texture. However, the gradient histogram prior used for texture enhancement sometimes generates false textures in the smooth areas. In order to address these problems, the authors propose a robust algorithm combining gradient histogram with sparse representation to obtain good estimates of the sparse coding coefficients of the latent image and realising image denoising while preserving the texture. The proposed model is solved by having a balance between over-enhancement and over-smoothing of the texture in order to preserve the natural texture appearance. Experimental results demonstrate the efficiency and effectiveness of the proposed method.	gradient;noise reduction;sparse approximation;sparse matrix	Mingli Zhang;Christian Desrosiers	2017	IET Image Processing	10.1049/iet-ipr.2016.0098	image texture;computer vision;latent image;computer science;histogram matching;machine learning;pattern recognition;sparse approximation;mathematics;texture compression;image histogram	Vision	56.80487345935872	-69.1232880138995	113869
45b8229a20cb8652fd98c0bc09c5f34415ba0ec2	convergence study on the symmetric version of admm with larger step sizes	convergence analysis;split bregman;94a08;alternating direction method of multipliers;convex programming;large step size;image reconstruction;90c25	The alternating direction method of multipliers (ADMM), also well known as a special split Bregman algorithm in imaging, is being popularly used in many areas including the image processing field. One useful modification is the symmetric version of the original ADMM, which updates the Lagrange multiplier twice at each iteration and thus the variables are treated in a symmetric manner. The symmetric version of ADMM, however, is not necessarily convergent. It was recently found that the convergence of symmetric ADMM can be sufficiently ensured if both the step sizes for updating the Lagrange multiplier are shrunk conservatively. Despite the theoretical significance in ensuring convergence, however, smaller step sizes should be strongly avoided in practice. On the contrary, we actually have the desire of seeking larger step sizes whenever possible in order to accelerate the numerical performance. Another technique leading to numerical acceleration of ADMM is enlarging its step size by a constant originally proposed by Fortin and Glowinski. These two numerically favorable techniques are commonly but usually separately used in ADMM literature, and intuitively they seem to be incompatible in combination with the symmetric ADMM due to the conflict between the theoretical role in ensuring the convergence with smaller step sizes and the empirical necessity in accelerating numerical performance with larger step sizes. It is thus open whether the ADMM scheme in combination with these two techniques simultaneously is convergent. We answer this question affirmatively in this paper and rigorously show the convergence of the symmetric version of ADMM with step sizes that can be enlarged by Fortin and Glowinski’s constant. We thus move forward to the counterintuitive understanding that shrinking both the step sizes is not necessary for the symmetric ADMM. We conduct the convergence analysis by specifying a step size domain that can ensure the convergence of symmetric ADMM; some known results in the ADMM literature turn out to be special cases of our discussion. Since the step sizes can be enlarged by constants that are problem-independent and the strategy is applicable to the general iterative scheme when the generic setting of the model is considered, our theoretical study provides an easily implementable strategy to accelerate the ADMM numerically which can be immediately applied to a variety of applications including some standard image processing tasks.	algorithm;augmented lagrangian method;bregman divergence;image processing;iteration;iterative method;lagrange multiplier;numerical analysis	Beixin Julie He;Feng Ma;Xiaoming Yuan	2016	SIAM J. Imaging Sciences	10.1137/15M1044448	iterative reconstruction;mathematical optimization;mathematical analysis;convex optimization;radiology;calculus;mathematics;geometry	ML	56.57870517830679	-72.43782766280974	114436
c43d0e62a03fcd774841d539801eda72fb8701f3	noise robust single image super-resolution using a multiscale image pyramid		Single image super-resolution (SR) generates a high-resolution (HR) image by estimating the mapping function between image patches of different resolutions. However, this kind of SR method cannot be directly applied to noisy images, since noise will be reinforced in the process of super-resolution. To this end, this paper presents a simultaneous super-resolution and denoising method by exploiting the noise decreasing property contained in the multiscale image pyramid. Experimental results confirm that our method is able to outperform other state-of-the-art super-resolution methods when super-resolving noisy images across differing noise levels.		Jing Hu;Xi Wu;Jiliu Zhou	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1016/j.sigpro.2018.02.020	pyramid (image processing);mathematical optimization;interpolation;image quality;structural similarity;noise reduction;input/output;superresolution;computer vision;correlation;mathematics;artificial intelligence	Vision	58.80742333287082	-66.1763679442179	114856
030fb33f24d81693fbccc0d18b08aa86e78bd8e8	the projected gsure for automatic parameter tuning in iterative shrinkage methods	inverse problems.;separable surrogate function;stein unbiased risk estimator;iterated shrinkage;inverse problems;inverse problem	Linear inverse problems are very common in signal and image processing. Many algorithms that aim at solving such problems include unknown parameters that need tuning. In this work we focus on optimally selecting such parameters in iterative shrinkage methods for image deblurring and image zooming. Our work uses the projected Generalized Stein Unbiased Risk Estimator (GSURE) for determining the threshold value λ and the iterations number K in these algorithms. The proposed parameter selection is shown to handle any degradation operator, including ill-posed and even rectangular ones. This is achieved by using GSURE on the projected expected error. We further propose an efficient greedy parameter setting scheme, that tunes the parameter while iterating without impairing the resulting deblurring performance. Finally, we provide extensive comparisons to conventional methods for parameter selection, showing the superiority of the use of the projected GSURE.	deblurring;elegant degradation;greedy algorithm;image processing;iteration;performance tuning;well-posed problem	Raja Giryes;Michael Elad;Yonina C. Eldar	2010	CoRR		econometrics;mathematical optimization;image processing;inverse problem;mathematics;statistics	Vision	57.02865993269276	-71.63000310723487	114970
3dcc2149e40685bea0ba0fe5f32e5e611e270f2d	nonlinear spectral image fusion		In this paper we demonstrate that the framework of nonlinear spectral decompositions based on total variation (TV) regularization is very well suited for image fusion as well as more general image manipulation tasks. The well-localized and edge-preserving spectral TV decomposition allows to select frequencies of a certain image to transfer particular features, such as wrinkles in a face, from one image to another. We illustrate the effectiveness of the proposed approach in several numerical experiments, including a comparison to the competing techniques of Poisson image editing, linear osmosis, wavelet fusion and Laplacian pyramid fusion. We conclude that the proposed spectral TV image decomposition framework is a valuable tool for semiand fullyautomatic image editing and fusion.	experiment;image fusion;nonlinear system;numerical analysis;poisson image editing;wavelet;whole earth 'lectronic link	Martin Benning;Michael Möller;Raz Z. Nossek;Martin Burger;Daniel Cremers;Guy Gilboa;Carola-Bibiane Schoenlieb	2017		10.1007/978-3-319-58771-4_4	wavelet;pattern recognition;computer science;artificial intelligence;total variation denoising;nonlinear system;computer vision;image editing;feature detection (computer vision);regularization (mathematics);image fusion;poisson image editing	Vision	55.27848447021402	-70.08370099933444	115039
3337b4a92bdf20f7d71e47a5dffcb26a58c41c9f	effect of dimensionality reduction on sparsity based hyperspectral unmixing		Interpretation of hyperspectral data is challenging due to the lack of spatial resolution, which causes mixing of endmember information in each pixel. Hyperspectral unmixing aims at extracting the information related to the fractional abundance of each endmember present in every pixel. The unmixing problem can be carried out by considering that the spectral signature of each endmember is a linear combination of the pure spectral signatures known in prior. In this work, sparse unmixing techniques such as, Orthogonal Matching Pursuit and Alternating Directional Multiplier Methods are applied along with dimensionality reduction of the hyperspectral image. Dimensionality reduction is obtained using the Inter-Band Block Correlation followed by singular value and QR decomposition (SVD-QR). Furthermore, we analyze the effect of dimensionality reduction on two different unmixing algorithms. Our experimentation is carried out on two real hyperspectral datasets namely ‘samson’ and ‘jasper ridge’ and the results comprises of a comparison between hyperspectral unmixing before and after dimensionality reduction using the standard metrics such as root mean square error, classwise-accuracy and visual perception. This provides a new outlook for the unmixing process as abundance estimation can be done with only the most informative bands of the image instead of using the entire data by using the dimensionality reduction technique.		M. Swarna;V. Sowmya;K. P. Soman	2016		10.1007/978-3-319-60618-7_42	artificial intelligence;endmember;pattern recognition;linear combination;computer science;qr decomposition;dimensionality reduction;hyperspectral imaging;image resolution;mean squared error;spectral signature	AI	67.38902324999589	-66.50569993638837	115061
059e06ebab437b61a9b2f34f75629a5bc6d39e3e	tensor completion for estimating missing values in visual data	minimization;tensile stress;iterative decoding;iterative algorithms;approximation algorithms;computer graphics;state estimation;computer vision;optimization problem;multiple constraints;matrix completion;heuristic algorithms;image reconstruction;pixel;robustness;global optimization;optimization;approximation methods;missing values;tensile stress state estimation robustness image reconstruction heuristic algorithms computer vision buildings iterative algorithms computer graphics iterative decoding;theoretical foundation;buildings	In this paper, we propose an algorithm to estimate missing values in tensors of visual data. The values can be missing due to problems in the acquisition process or because the user manually identified unwanted outliers. Our algorithm works even with a small amount of samples and it can propagate structure to fill larger missing regions. Our methodology is built on recent studies about matrix completion using the matrix trace norm. The contribution of our paper is to extend the matrix case to the tensor case by proposing the first definition of the trace norm for tensors and then by building a working algorithm. First, we propose a definition for the tensor trace norm that generalizes the established definition of the matrix trace norm. Second, similarly to matrix completion, the tensor completion is formulated as a convex optimization problem. Unfortunately, the straightforward problem extension is significantly harder to solve than the matrix case because of the dependency among multiple constraints. To tackle this problem, we developed three algorithms: simple low rank tensor completion (SiLRTC), fast low rank tensor completion (FaLRTC), and high accuracy low rank tensor completion (HaLRTC). The SiLRTC algorithm is simple to implement and employs a relaxation technique to separate the dependant relationships and uses the block coordinate descent (BCD) method to achieve a globally optimal solution; the FaLRTC algorithm utilizes a smoothing scheme to transform the original nonsmooth problem into a smooth one and can be used to solve a general tensor trace norm minimization problem; the HaLRTC algorithm applies the alternating direction method of multipliers (ADMMs) to our problem. Our experiments show potential applications of our algorithms and the quantitative evaluation indicates that our methods are more accurate and robust than heuristic approaches. The efficiency comparison indicates that FaLTRC and HaLRTC are more efficient than SiLRTC and between FaLRTC and HaLRTC the former is more efficient to obtain a low accuracy solution and the latter is preferred if a high-accuracy solution is desired.	algorithm;augmented lagrangian method;binary-coded decimal;convex optimization;coordinate descent;estimated;experiment;heuristic;large;laser therapy, low-level;linear programming relaxation;low-rank approximation;mathematical optimization;maxima and minima;missing data;optimization problem;relaxation techniques;smoothing (statistical technique);the matrix;emotional dependency	Ji Liu;Przemyslaw Musialski;Peter Wonka;Jieping Ye	2009	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/ICCV.2009.5459463	iterative reconstruction;optimization problem;mathematical optimization;computer science;theoretical computer science;machine learning;mathematics;geometry;stress;computer graphics;pixel;statistics;robustness;global optimization	Vision	58.119933847350836	-72.73595901296852	115541
28968b72d016af1502bba8b7131cb531cbacec4d	improved total variation minimization method for compressive sensing by intra-prediction	intra prediction;deblocking filter;compressive sensing;total variation tv	Total variation (TV) minimization algorithms are often used to recover sparse signals or images in the compressive sensing (CS). But the use of TV solvers often suffers from undesirable staircase effect. To reduce this effect, this paper presents an improved TV minimization method for block-based CS by intra-prediction. The new method conducts intra-prediction block by block in the CS reconstruction process and generates a residual for the image block being decoded in the CS measurement domain. The gradient of the residual is sparser than that of the image itself, which can lead to better reconstruction quality in CS by TV regularization. The staircase effect can also be eliminated due to effective reconstruction of the residual. Furthermore, to suppress blocking artifacts caused by intra-prediction, an efficient adaptive in-loop deblocking filter was designed for post-processing during the CS reconstruction process. Experiments show competitive performances of the proposed hybrid method in comparison with state-of-the-art TV models for CS with respect to peak signal-to-noise ratio and the subjective visual quality. & 2012 Elsevier B.V. All rights reserved.	algorithm;blocking (computing);compressed sensing;deblocking filter;experiment;gradient;iterative reconstruction;peak signal-to-noise ratio;performance;simulation;sparse matrix;total variation diminishing;video post-processing	Jie Xu;Jianwei Ma;Dongming Zhang;Yongdong Zhang;Shouxun Lin	2012	Signal Processing	10.1016/j.sigpro.2012.04.001	computer vision;mathematical optimization;electronic engineering;telecommunications;computer science;deblocking filter;mathematics;compressed sensing	Vision	57.563069148495956	-69.84350138512072	115743
483b832af5f82c2f801060cfc48aac54c07e9456	fully bayesian source separation of astrophysical images modelled by mixture of gaussians	cosmic microwave background radiation;cosmic microwave background cmb radiation;source separation bayes methods cosmic background radiation markov processes monte carlo methods;gibbs sampling;markov chain monte carlo techniques;planck satellite mission bayesian source separation cosmic microwave background cmb radiation gibbs sampling markov chain monte carlo;bayes methods;microwave radiation maps;planck satellite mission;prior information;bayesian methods;satellite mission planck;cosmic microwave background;cosmic microwave background radiation astrophysical images bayesian source separation gaussian mixture model markov chain monte carlo techniques microwave radiation maps satellite mission planck;gaussian mixture model;markov chain monte carlo;number of factors;cosmic background radiation;bayesian methods source separation gaussian processes monte carlo methods microwave theory and techniques satellites extraterrestrial measurements frequency predictive models parameter estimation;mixture of gaussians;astrophysical images;markov processes;parameter estimation;bayesian source separation;microwave theory and techniques;source separation;algorithm design and analysis;monte carlo methods;data models	We address the problem of source separation in the presence of prior information. We develop a fully Bayesian source separation technique that assumes a very flexible model for the sources, namely the Gaussian mixture model with an unknown number of factors, and utilize Markov chain Monte Carlo techniques for model parameter estimation. The development of this methodology is motivated by the need to bring an efficient solution to the separation of components in the microwave radiation maps to be obtained by the satellite mission Planck which has the objective of uncovering cosmic microwave background radiation. The proposed algorithm successfully incorporates a rich variety of prior information available to us in this problem in contrast to most of the previous work which assumes completely blind separation of the sources. We report results on realistic simulations of expected Planck maps and on WMAP 5th year results. The technique suggested is easily applicable to other source separation applications by modifying some of the priors.	algorithm;blind signal separation;cosmic;estimation theory;map;markov chain monte carlo;microwave;mixture model;monte carlo method;simulation;source separation	Simon P. Wilson;Ercan Engin Kuruoglu;Emanuele Salerno	2008	IEEE Journal of Selected Topics in Signal Processing	10.1109/JSTSP.2008.2005320	computer science;mixture model;mathematics;cosmic microwave background;statistics	ML	61.513858609942304	-72.58781513918768	115813
db8d567ee42ef5a3143a549bd3a57c35fe76fc7b	a multi-resolution approach to complexity reduction in tomographic reconstruction		Most of the algorithms for tomographic reconstruction face the same problem: high computational complexity. In order to tackle this problem, this paper proposes a general multi-resolution approach that enables a flexible choice of reconstruction focus and thus saves computational power in reconstructions. The approach is demonstrated in this paper based on a reconstruction algorithm using a (improper) Markov random field prior with sparsifying NUV terms (nor-mal with unknown variance), where the unknown variances are learned by approximate EM (expectation maximization). The experimental and practical results show that both for simulated and real-world objects the proposed framework yields satisfying results with much lower computational cost.	algorithmic efficiency;approximation algorithm;computation;computational complexity theory;expectation–maximization algorithm;markov chain;markov random field;reduction (complexity);simulation;tomographic reconstruction	Boxiao Ma;Nour Zalmai;Hans-Andrea Loeliger	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461528	message passing;reconstruction algorithm;markov random field;mathematical optimization;reduction (complexity);expectation–maximization algorithm;computational complexity theory;boundary value problem;tomographic reconstruction;computer science	Vision	56.714026506081446	-74.22181915651443	116201
ea3972179994fc83861a4d076a5877c827ef9194	smoothing approach using forward-backward kalman filter with markov switching parameters for speech enhancement	forward backward;traitement signal;processus non stationnaire;filtro kalman;colored noise;prior probability;markov switching;modelo markov;etude theorique;hidden markov model;statistical independence;raster;ruido estacionario;filtre kalman;probabilite a priori;additive noise;ruido aditivo;kalman filter;bruit additif;posterior probability;probabilistic approach;speech enhancement;markov model;smoothing;weighted sums;enfoque probabilista;probabilidad a priori;approche probabiliste;signal processing;stationary noise;trame;estudio teorico;alisamiento;non stationary process;modele markov;theoretical study;signal to noise ratio;procesamiento senal;proceso no estacionario;lissage;bruit stationnaire;trama	In this paper, a smoothing approach for enhancing speech signals degraded by statistically independent additive nonstationary noise is developed. The autoregressive hidden Markov model (ARHMM) is used for modeling the statistical characteristics of both the clean speech signal and the nonstationary noise process. In this case, the speech enhancement comprises a weighted sum of the conditional mean estimators for the composite states of the models for the speech and noise, where the weights are equal to the posterior probabilities of the composite states, given the noisy speech. The conditional mean estimators use a smoothing approach based on two Kalman filters with Markovian switching coefficients, where one of the filters propagates in the forward-time direction and the other propagates in the backward-time direction with one frame. The proposed method is tested on speech signals degraded by Gaussian colored noise or nonstationary noise at various input signal-to-noise ratios. An approximate improvement of 4.7–5.2 dB in SNR is achieved at input SNR 10 and 15 dB. Also, in comparison with conventional method (Ephraim, IEEE Trans. Signal Process. SP-41 (April 1992) 725–735), our proposed method shows improvement of about 0.3 dB in SNR.	kalman filter;markov chain;smoothing;speech enhancement	Ki Yong Lee;Souhwan Jung;JaeYeol Rheem	2000	Signal Processing	10.1016/S0165-1684(00)00140-7	kalman filter;independence;speech recognition;prior probability;colors of noise;raster graphics;computer science;signal processing;pattern recognition;mathematics;markov model;posterior probability;signal-to-noise ratio;hidden markov model;statistics;smoothing	ML	54.2623831886453	-66.85669228867907	116227
bb9079f70b766fe6ab59f723e43badd02a9134ef	a wavelet-based image denoising technique using spatial priors	bayesian framework;closed form expression;additive white gaussian noise;markov random field model;iterated conditional mode;iterative updating technique;bayes methods;probability density function;markov random fields;bayesian methods;spatial interaction;random variables;image restoration;awgn;prior knowledge;image denoising wavelet coefficients bayesian methods random variables markov random fields probability density function laplace equations higher order statistics parameter estimation performance analysis;markov random field;higher order statistics;wavelet transforms;iterative methods;interference suppression;additive white gaussian noise wavelet based method image denoising bayesian framework spatial priors spatial clustering wavelet coefficients local spatial interactions markov random field model iterative updating technique iterated conditional modes binary masks estimation shrinkage factor closed form expression;laplace equations;binary masks estimation;technology and engineering;awgn wavelet transforms image restoration interference suppression iterative methods markov processes bayes methods;wavelet based method;performance analysis;local spatial interactions;image denoising;markov processes;iterated conditional modes;parameter estimation;spatial clustering;shrinkage factor;wavelet coefficients;spatial priors	We propose a new wavelet-based method for image denoising that applies the Bayesian framework, using prior knowledge about the spatial clustering of the wavelet coefficients. Local spatial interactions of the wavelet coefficients are modeled by adopting a Markov Random Field model. An iterative updating technique known as iterated conditional modes (ICM) is applied to estimate the binary masks containing the positions of those wavelet coefficients that represent the useful signal in each subband. For each wavelet coefficient a shrinkage factor is determined, depending on its magnitude and on the local spatial neighbourhood in the estimated mask. We derive analytically a closed form expression for this shrinkage factor.	cluster analysis;coefficient;interaction;iterated conditional modes;iteration;iterative method;markov chain;markov random field;noise reduction;wavelet transform	Aleksandra Pizurica;Wilfried Philips;Ignace Lemahieu;Marc Acheroy	2000		10.1109/ICIP.2000.899360	wavelet;additive white gaussian noise;second-generation wavelet transform;machine learning;pattern recognition;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;statistics;wavelet transform	ML	61.29178018017555	-71.51083328107147	116689
d7ae5a5911f1def00e86a0038170134b0c63a125	quality management services and image registration: case of images technology in health services using a management outliers approach		Service quality is an elusive construct to improve and further effort is necessary to obtain a good quality of service. New technologies play a very important role in the improvement of service quality. This paper is interested on the improvement of service quality in health services within public hospitals using image registration. This research presents a management outliers approach using the polynomial transformation in order to obtain image registration. A good superposition of original image and reference image allows achieving the image registration, which allows getting an improvement of service quality. The evaluation of the results of this approach illustrates the good superposition of original image and reference image by reducing the Mean Squared Error(MSE) quality metric and increasing the Peak Signal-to-Noise Ratio (PSNR) quality metric.	antivirus software;apache axis;approximation;image registration;imaging technology;interpolation;mean squared error;medical imaging;non-uniform rational b-spline;peak signal-to-noise ratio;polynomial;polynomial-time reduction;quality of service;quantum superposition	Houda Hakim Guermazi;Ahlem Riahi	2017	2017 IEEE/ACS 14th International Conference on Computer Systems and Applications (AICCSA)	10.1109/AICCSA.2017.207	real-time computing;data mining;computer science;outlier;quality management;quality of service;image registration;service quality;mean squared error	Vision	57.17285311127803	-79.8368344968457	117405
6d9f22c9a8749c3fda03e68eec9703cf66dbd47b	bayesian tree-structured image modeling using wavelet-domain hidden markov models	transformation ondelette;bayes estimation;selfsimilarity;modelizacion;hidden markov tree;statistical image processing;metodo estadistico;structural model;bayesian tree structured image modeling;probability;wavelet based estimators bayesian tree structured image modeling wavelet domain hidden markov models wavelet domain hmm statistical signal processing statistical image processing hidden markov tree model joint probability density wavelet coefficients computationally expensive iterative training expectation maximization algorithm simplified hmt model self similarity real world images hmt parameters meta parameters bayesian universal hmt image estimation image denoising image structure fast shift invariant hmt estimation algorithm mean square error;wavelet domain hmm;image processing;image resolution;probability density;bayesian approach;fast shift invariant hmt estimation algorithm;wavelet based estimators;hidden markov model;bayes methods;metodo arborescente;statistical signal processing;modele markov variable cachee;markov random fields;procesamiento imagen;self similarity;bayesian methods;simplified hmt model;cycle spinning;statistical method;image processing bayes methods trees mathematics hidden markov models wavelet transforms probability parameter estimation;real world images;trees mathematics;indexing terms;estimation algorithm;traitement image;image structure;statistical image modeling;modelisation;wavelet transforms;bayesian universal hmt;estimacion bayes;hidden markov models;meta parameters;image edge detection;methode statistique;mean square error;signal processing;tree structure;autosimilitud;hidden markov models bayesian methods wavelet transforms image edge detection image processing wavelet coefficients markov random fields image resolution signal processing probability;expectation maximization algorithm;hidden markov tree model;estimacion parametro;signal and image processing;computationally expensive iterative training;tree structured method	Wavelet-domain hidden Markov models have proven to be useful tools for statistical signal and image processing. The hidden Markov tree (HMT) model captures the key features of the joint probability density of the wavelet coefficients of real-world data. One potential drawback to the HMT framework is the need for computationally expensive iterative training to fit an HMT model to a given data set (e.g., using the expectation-maximization algorithm). We greatly simplify the HMT model by exploiting the inherent self-similarity of real-world images. The simplified model specifies the HMT parameters with just nine meta-parameters (independent of the size of the image and the number of wavelet scales). We also introduce a Bayesian universal HMT (uHMT) that fixes these nine parameters. The uHMT requires no training of any kind, while extremely simple, we show using a series of image estimation/denoising experiments that these new models retain nearly all of the key image structure modeled by the full HMT. Finally, we propose a fast shift-invariant HMT estimation algorithm that outperforms other wavelet-based estimators in the current literature, both visually and in mean square error.	analysis of algorithms;coefficient;expectation–maximization algorithm;experiment;hidden markov model;image processing;iterative method;markov chain;mean squared error;methenamine;noise reduction;self-similarity;universal precautions;wavelet	Justin K. Romberg;Hyeokho Choi;Richard G. Baraniuk	2001	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.931100	image processing;bayesian probability;computer science;machine learning;pattern recognition;mathematics;hidden markov model;statistics	ML	61.23220700373595	-70.51063250477158	117418
f29d06b06d8c056e5ed87e98adae711e9d2e80be	an objective function for 3-d motion estimation from optical flow with lower error variance than maximum likelihood estimator	surface structures;3 d motion estimation;nonlinear optics;image motion analysis;optical noise;error variance;unbiased estimator;motion estimation;matrix algebra;maximum likelihood estimation;generalized rayleigh quotient;objective function;error analysis;maximum likelihood estimate;weighting function;maximum likelihood estimator;numerical evaluations;projection matrix;nonlinear equations;optical flow;humans;motion estimation image motion analysis nonlinear optics maximum likelihood estimation nonlinear equations laboratories humans computational efficiency optical noise surface structures;weight function;computational efficiency;rayleigh quotient;computational cost reduction;functions;projection matrix 3 d motion estimation optical flow maximum likelihood estimator error variance objective function generalized rayleigh quotient unbiased estimator computational cost reduction weighting function numerical evaluations;image sequences;functions motion estimation image sequences maximum likelihood estimation error analysis matrix algebra	This paper describes a new objective function for estimating 3-D motion from an optical flow. We construct a generalized Rayleigh-quotient form objective function, which provides an unbiased estimator and whose use drastically reduces the computational cost. This function includes an arbitrary weighting function. We show by numerical evaluations that with a suitable weighting function it provides an estimator whose variance is smaller than that of the maximum likelihood estimator. >	loss function;motion estimation;optical flow	Norio Tagawa;Takashi Toriu;Toshio Endoh	1994		10.1109/ICIP.1994.413570	efficient estimator;minimax estimator;econometrics;mathematical optimization;minimum-variance unbiased estimator;weight function;bayes estimator;stein's unbiased risk estimate;nonlinear system;efficiency;mathematics;mean squared error;maximum likelihood;variance function;quasi-maximum likelihood;bias of an estimator;estimation theory;consistent estimator;invariant estimator;statistics	Vision	55.26369548986777	-73.99598214397359	117794
59c82bdc10adf397a91c96c1110da0da975819ab	two-dimensional compact variational mode decomposition	image decomposition;image segmentation;spatio-spectral decomposition;microscopy;crystal grains;artifact detection;threshold dynamics;variational methods;sparse time–frequency analysis;68u10	Decomposing multidimensional signals, such as images, into spatially compact, potentially overlapping modes of essentially wavelike nature makes these components accessible for further downstream analysis. This decomposition enables space–frequency analysis, demodulation, estimation of local orientation, edge and corner detection, texture analysis, denoising, inpainting, or curvature estimation. Our model decomposes the input signal into modes with narrow Fourier bandwidth; to cope with sharp region boundaries, incompatible with narrow bandwidth, we introduce binary support functions that act as masks on the narrow-band mode for image recomposition. $$L^1$$ L 1 and TV terms promote sparsity and spatial compactness. Constraining the support functions to partitions of the signal domain, we effectively get an image segmentation model based on spectral homogeneity. By coupling several submodes together with a single support function, we are able to decompose an image into several crystal grains. Our efficient algorithm is based on variable splitting and alternate direction optimization; we employ Merriman–Bence–Osher-like threshold dynamics to handle efficiently the motion by mean curvature of the support function boundaries under the sparsity promoting terms. The versatility and effectiveness of our proposed model is demonstrated on a broad variety of example images from different modalities. These demonstrations include the decomposition of images into overlapping modes with smooth or sharp boundaries, segmentation of images of crystal grains, and inpainting of damaged image regions through artifact detection.	algorithm;corner detection;downstream (software development);frequency analysis;image segmentation;inpainting;mathematical optimization;noise reduction;sparse matrix;stellar classification;variable splitting;variational principle	Dominique Zosso;Konstantin Dragomiretskiy;Andrea L. Bertozzi;Paul S. Weiss	2017	Journal of Mathematical Imaging and Vision	10.1007/s10851-017-0710-z	computer vision;mathematical optimization;mathematics	Vision	57.63592005363761	-72.4727583388037	118346
811b53b51cae960b2c41a362f5e441f28181fccb	model-based iterative restoration for binary document image compression with dictionary learning		The inherent noise in the observed (e.g., scanned) binary document image degrades the image quality and harms the compression ratio through breaking the pattern repentance and adding entropy to the document images. In this paper, we design a cost function in Bayesian framework with dictionary learning. Minimizing our cost function produces a restored image which has better quality than that of the observed noisy image, and a dictionary for representing and encoding the image. After the restoration, we use this dictionary (from the same cost function) to encode the restored image following the symbol-dictionary framework by JBIG2 standard with the lossless mode. Experimental results with a variety of document images demonstrate that our method improves the image quality compared with the observed image, and simultaneously improves the compression ratio. For the test images with synthetic noise, our method reduces the number of flipped pixels by 48.2% and improves the compression ratio by 36.36% as compared with the best encoding methods. For the test images with real noise, our method visually improves the image quality, and outperforms the cutting-edge method by 28.27% in terms of the compression ratio.	circuit restoration;dictionary;distortion;encode;image compression;image quality;iterative method;jbig2;loss function;lossless compression;machine learning;mathematical optimization;pixel;printing;synthetic data	Yandong Guo;Cheng Lu;Jan P. Allebach;Charles A. Bouman	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.72	computer vision;image restoration;artificial intelligence;image compression;computer science;image quality;binary image;pattern recognition;noise measurement;feature detection (computer vision);jbig2;lossless compression	Vision	61.59723582854116	-66.67221829247683	118371
60e66b78adef71a9783bbc3a6123b0e29ed523ef	relating spatial and spectral models of oriented bandpass natural images	natural scene statistics;bandpass natural images;statistics band pass filters correlation methods natural scenes spatial filters spectral analysis;biological system modeling;bandpass spatial filters image spatial models image spectral models oriented bandpass natural images natural scene statistics perceived digital pictures image processing applications video processing applications bivariate image nss closed form correlation model;1 ƒ noise models;brain models;visualization;computational modeling;hidden markov models;correlation hidden markov models biological system modeling brain models computational modeling visualization;correlation;1 ƒ noise models natural scene statistics bivariate correlation models bandpass natural images;bivariate correlation models	Univariate models of the Natural Scene Statistics (NSS) of perceived digital pictures have been deployed in a wide variety of image and video processing applications. However, much less effort has been made towards understanding, modeling, and using bivariate image NSS. Towards filling this gap, Su et al. developed a closed form correlation model of oriented bandpass natural images applicable to adjacent pixels. We later extended this model to account for pixels separated by larger spatial distance. Here, we expand our previous work further to model the bivariate responses of bandpass spatial filters covering a wider range of bandwidths. Furthermore, we study the relationship between the parameters of the closed form correlation model and image spectral models.	bandwidth (signal processing);bivariate data;high-dynamic-range imaging;higher-order function;image;pixel;scene statistics;video processing	Zeina Sinno;Alan C. Bovik	2016	2016 IEEE Southwest Symposium on Image Analysis and Interpretation (SSIAI)	10.1109/SSIAI.2016.7459182	computer vision;speech recognition;visualization;computer science;mathematics;computational model;correlation;hidden markov model;statistics	Vision	64.66345580300614	-66.54119991335683	119096
1ca669f9e36c6eeb232a4cf3cd30afa799e179ad	image deconvolution by local order preservation of pixels values	kernel;image restoration;positron emission tomography;deconvolution;europe;spatial resolution	Positron emission tomography is more and more used in radiation oncology, since it conveys useful functional information about cancerous lesions. Its rather low spatial resolution, however, prevents accurate tumor delineation and heterogeneity assessment. Post-reconstruction deconvolution with the measured point-spread function can address this issue, provided it does not introduce undesired artifacts. These usually result from inappropriate regularization, which is either absent or making too strong assumptions about the structure of the signal. This paper proposes a deconvolution method that is based on inverse problem theory and involves a new regularization term that preserves local pixel value order relationships. Such regularization entails relatively mild constraints that are directly inferred from the observed data. This paper investigates the theoretical properties of the proposed regularization and describes its numerical implementation with a primal-dual algorithm. Preliminary experiments with synthetic images are presented to compare quantitatively and qualitatively the proposed method to other regularization schemes, like TV and TGV.	bias–variance tradeoff;circuit restoration;convolution;deconvolution;diffusing update algorithm;experiment;matrix regularization;numerical analysis;pixel;ringing artifacts;shot noise;signal-to-noise ratio;synthetic intelligence;tomography	Stéphanie Guérit;Laurent Jacques;John A. Lee	2016	2016 24th European Signal Processing Conference (EUSIPCO)	10.1109/EUSIPCO.2016.7760307	computer vision;mathematical optimization;mathematics;blind deconvolution;statistics	ML	56.060663434785894	-73.24680459719293	119469
864dae6c0c93d6a7c2929960fc23ca677c0a3a7b	multiplicative noise removal with spatially varying regularization parameters	spatially varying regularization parameters;multiplicative noise;65j22;65f22;total variation;textures	The Aubert-Aujol (AA) model is a variational method for multiplicative noise removal. In this paper, we study some basic properties of the regularization parameter in the AA model. We develop a method for automatically choosing the regularization parameter in the multiplicative noise removal process. In particular, we employ spatially varying regularization parameters in the AA model in order to restore more texture details of the denoised image. Experimental results are presented to demonstrate that the spatially varying regularization parameters method can obtain better denoised images than the other tested multiplicative noise removal methods.	matrix regularization;multiplicative noise	Fang Li;Michael K. Ng;Chaomin Shen	2010	SIAM J. Imaging Sciences	10.1137/090748421	computer vision;econometrics;mathematical optimization;mathematical analysis;multiplicative noise;mathematics;total variation;statistics	Theory	55.2931372411839	-70.22577846640549	119555
9adb5f65e1aa98ad451fe33075f2396ebbefd18f	huber–markov model for complex sar image restoration	speckle reduction;complex valued sar data;speckle;single look complex;image restoration synthetic aperture radar bayesian methods cost function speckle noise reduction feature extraction tv adaptive filters;numerical solution;half quadratic;cost function;helium;prior model;photogrammetrie und bildanalyse;regularization method;bayesian inference;huber markov random field;hmrf;bayesian methods;slc sar image despeckling;image restoration;complex valued sar data huber markov model complex sar image restoration slc sar image despeckling single look complex sar image synthetic aperture radar nonquadratic regularisation objective function image model prior model huber markov random field hmrf half quadratic regularisation methods;markov random field;image model;objective function;remote sensing by radar;adaptive filters;markov model;huber markov model;geophysical signal processing;synthetic aperture radar sar;feature extraction;half quadratic regularisation methods;noise reduction;single look complex sar image;synthetic aperture radar geophysical signal processing image denoising image restoration markov processes radar signal processing remote sensing by radar speckle;nonquadratic regularisation;sar image;synthetic aperture radar sar bayesian inference regularization methods speckle reduction;image denoising;markov processes;tv;regularization methods;complex sar image restoration;image modeling;radar signal processing;synthetic aperture radar	This letter presents the despeckling of single-look complex (SLC) synthetic aperture radar (SAR) images using nonquadratic regularization. The objective function consists of an image model, a gradient, and a prior model. The Huber-Markov random field (HMRF) models the prior. A numerical solution is achieved through extensions of half-quadratic regularization methods using complex-valued SAR data. The proposed method using the HMRF prior together with nonquadratic regularization shows the superior results on SLC synthetic and actual SAR images.	aperture (software);gradient;image restoration;loss function;manifold regularization;markov chain;markov model;markov random field;matrix regularization;multi-level cell;noise reduction;numerical partial differential equations;optimization problem;synthetic data;synthetic intelligence	Matteo Soccorsi;Dusan Gleich;Mihai Datcu	2010	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2009.2024011	adaptive filter;speckle pattern;image restoration;computer vision;synthetic aperture radar;feature extraction;bayesian probability;computer science;pattern recognition;noise reduction;markov process;markov model;helium;bayesian inference;statistics;remote sensing	Vision	60.60491160364275	-72.07965604250373	119776
9c39c5372a4a2bbf926f49c2c908bf997e2073b6	compressed sensing mri using total variation regularization with k-space decomposition		Compressed sensing theory facilitates the fast magnetic resonance imaging by reducing the required number of measurements for reconstruction. Conventional compressed sensing magnetic resonance imaging(CSMRI) method utilize the partial k-space measurements as a whole without considering their intrinsic property. Some recent researches have shown the advantage of dealing the high and low frequency image content separately. Based on this, we propose a novel CSMRI algorithm based on total variation regularization with k-space decomposition. First we decompose k-space into high frequency band and low frequency band, then we reconstruct the corresponding high and low MR images which will be used for integration later. All the steps can be unified into a objective function. We will show that the proposed objective function can be split into several subproblems to solve iteratively using ADMM technique. The experimental results show that the proposed method outperforms the conventional CSMRI method. Besides, the proposed method can be extended to other image processing applications as well.	algorithm;compressed sensing;frequency band;horner's method;image processing;loss function;optimization problem;resonance;total variation denoising	Liyan Sun;Yue Huang;Congbo Cai;Xinghao Ding	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296845	compressed sensing;computer vision;image processing;iterative reconstruction;total variation denoising;artificial intelligence;frequency band;k-space;linear programming;computer science	Robotics	56.942759226651795	-71.38570531571239	119799
98fa02909d66f5d4c88ecc36f6e7d374846b1cd7	reconstructing wafer surfaces with model based shape from shading	shape from shading	Abstract: Model based Shape From Shading (SFS) is a promising paradigm introduced by J. Atick for solving such inverse problems when we happen to have some prior information on the depth profiles to be recovered. In the present work we adopt this approach to address the problem of recovering wafer profiles from images taken by a Scanning Electron Microscope (SEM). This problem arises naturally in the microelectronics inspection industry. A low dimensional model based on our prior knowledge of the types of depth profiles of wafer surfaces has been developed and based on it the SFS problem becomes an optimal parameter estimation. Wavelet techniques were then employed to calculate a good initial guess to be used in Levenberg-Marguardt (LM) minimization process that yields the desired profile parametrization. The proposed algorithm has been tested under both Lambertian and SEM imaging models.	ct scan;clustered file system;electron;estimation theory;lambertian reflectance;levenberg–marquardt algorithm;photometric stereo;programming paradigm;shading;wafer (electronics);wavelet	Alexander Nisenboim;Alfred M. Bruckstein	2007			computer vision;photometric stereo;computer science	Vision	56.191231243508646	-75.17626730755792	119841
62453e35daebe87af3912e782e4583246c6660bf	convex-relaxed active contour model based on localised kernel mapping		Intensity inhomogeneity is one of the major obstacles for intensity-based segmentation in many applications. The recently proposed kernel mapping (KM) method has exhibited excellent performance on segmenting various types of noisy images while it is not effective to handle intensity inhomogeneity. To overcome this drawback, this study presents a localised KM (LKM) method based on the fact that intensity inhomogeneity can be ignored in a local neighbourhood. The authors’ method first reconstructs the KM formulation of image segmentation in a neighbourhood of each pixel, and then such formulations for all pixels can be integrated together to derive the LKM energy functional. Minimisation of the energy functional is implemented by solving an equivalent convex-relaxed problem whose optimisation can be quickly achieved via the split Bregman method. Experimental results on two-phase segmentation and multiphase segmentation demonstrate competitive performance of the LKM method in the presence of intensity inhomogeneity and severe noise.	active contour model;kernel (operating system)	Wenchao Cui;Guoqiang Gong;Ke Lu;Shuifa Sun;Fangmin Dong	2017	IET Image Processing	10.1049/iet-ipr.2017.0132	pixel;artificial intelligence;market segmentation;computer vision;kernel (linear algebra);mathematics;bregman method;energy functional;image segmentation;neighbourhood (mathematics);active contour model	Vision	54.20348146619194	-70.42171362594073	120178
c29b0ebdd370d75039c771d51cd32028abcd17b3	a statistical appraoch to image reconstruction from projections problem using recurrent neural network	x ray computed tomography;optimization problem;computer experiment;image reconstruction;recurrent neural network;reconstruction algorithm;neural network	This paper is concerned with the image reconstruction from projections problem. The presented paper describes a reconstruction approach based on recurrent neural network. The structure of this network is designed taking into account the probabilistic nature of distortion obesrved in x-ray computed tomography. The reconstruction process is performed using in this way constructed neural network solving the optimization problem. Computer experiments show that the appropriately designed recurrent neural network is able to reconstruct an image with better quality in comparison to the standart analytical reconstruction algorithm.	artificial neural network;iterative reconstruction;reconstruction from projections;recurrent neural network	Robert Cierniak	2010		10.1007/978-3-642-15822-3_17	iterative reconstruction;optimization problem;computer vision;mathematical optimization;computer experiment;computer science;recurrent neural network;machine learning;artificial neural network	ML	56.7414503471946	-74.86439963654043	120356
16997714feed0cbaab9c6a12ac786a6fa7b676ad	discontinuous seismic horizon tracking based on a poisson equation with incremental dirichlet boundary conditions	dirichlet boundary condition;poisson equation;image processing;seismology;boundary conditions;iterative methods;iterative resolution discontinuous seismic horizon tracking poisson equation incremental dirichlet boundary conditions discontinuity location nonlinear partial derivative equation;geophysics computing;boundary condition;signal processing;signal processing geophysics computing iterative methods poisson equation seismology;fault detection;fault detection seismic horizon tracking poisson equation incremental boundary conditions;coherence equations poisson equations boundary conditions conferences image processing mathematical model;seismic horizon tracking;mathematical model;coherence;conferences;poisson equations;incremental boundary conditions	We propose a new method to track a seismic horizon with a discontinuity due to a fault throw assumed to be quasi-vertical. Our approach requires the knowledge of the two points delimiting the horizon as well as the discontinuity location and jump. We deal with a non linear partial derivative equation relied on the estimated local dip. Its iterative resolution is based on a Poisson equation with incremental Dirichlet boundary conditions. By exploiting a coherence criterion, we finally present an efficient method even when the discontinuity location and jump are unknown.	delimiter;iterative method;point location;reflections of signals on conducting lines	Guillaume Zinck;Marc Donias;Sebastien Guillon;Olivier Lavialle	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6116436	computer vision;mathematical optimization;mathematical analysis;image processing;boundary value problem;computer science;signal processing;mathematics;geometry	Robotics	65.9654189646142	-69.83507122913008	120631
74156cf17fbe9c659045da05b8e0cc3933d9b317	analysis of non-local euclidean medians and its improvement	noise noise reduction robustness extraterrestrial measurements noise measurement convergence iterative methods;non local euclidean medians nlem;signal denoising iterative methods;nonlocal denoising methods nonlocal euclidean median analysis nlem inlem iterative algorithm;journal;iterative methods;期刊论文;non local means nlm;image denoising;non local means nlm improved non local euclidean medians inlem image denoising non local euclidean medians nlem;improved non local euclidean medians inlem;signal denoising	Non-Local Euclidean Medians (NLEM) has recently been proposed and shows more effective than Non-Local Means (NLM) in removing heavy noise. In this letter, we find the inconsistency between the two dissimilarity measures in NLEM can affect its robustness, thus develop an improved version (INLEM) to compensate such an inconsistency. Further, we provide a concise convergence proof for the iterative algorithm used in both NLEM and INLEM. Finally, our experiments on synthetic and natural images show that INLEM achieves encouraging results.	algorithm;algorithmic efficiency;experiment;impulse noise (audio);iterative method;median filter;netware loadable module;noise (electronics);noise reduction;non-deterministic turing machine;non-local means;speedup;synthetic intelligence	Zhonggui Sun;Songcan Chen	2013	IEEE Signal Processing Letters	10.1109/LSP.2013.2245322	mathematical optimization;machine learning;mathematics;iterative method;statistics	Vision	56.997811231841716	-70.75563892058051	120636
462aabf73ada9f8cabd926f11c78a9c7d23321a5	towards a partial differential equation remote sensing image method based on adaptive degradation diffusion parameter	total variation;partial differential equations;remote sensing image denoising;upwind scheme	For the anisotropy diffusion feature, Partial Differential Equation (PDE) methods keep edge detail characters well in case of denoising, thus being widely applied in remote sense image denoising, smoothing, filtering and reconstruction. A PDE remote sensing image denoising method based on Adaptive Degradation Diffusion Parameter (ADDP) was proposed in the paper to deal with fuzzy detail problem caused by increasing iteration number. The PDE denoising method with ADDP enlarged diffusion size in the plat region of remote sensing image without affecting the remote sensing image edge, thus avoiding loss of remote sensing image detail and intersections caused by Gaussian convolution smoothing in the PDE filtering model based on curvature-based movement (CM) and image denoising model based on total variation (TV). In the region where gradation value changes little, the method executed isotropic diffusion to remove isolated noise. The upwind scheme was applied for model numerical realization. Experimental in remote sensing image denoising results proved its feasibility and effectiveness.	angularjs;anisotropic diffusion;basis pursuit denoising;convolution;elegant degradation;filter (signal processing);image editing;iteration;noise reduction;numerical analysis;smoothing;upwind scheme	Xian-Yong Meng;Lei Che;Zhi-Hui Liu;Ning Che;Xiao-Nan Ji	2015	Multimedia Tools and Applications	10.1007/s11042-015-2881-1	computer vision;mathematical optimization;simulation;non-local means	Vision	55.914182677675385	-68.91492952473978	120713
c27c4556dbcdf128642093c3adbb34143a8c95a4	brightening and denoising lowlight images	gaussian noise;noise measurement;visualization;noise level;image color analysis;noise reduction;sparse matrices	This paper aims to improve the quality of images in low light conditions. After nonlinear intensity stretching, the enlarged mixed noise is reduced using a newly proposed low rank denoising algorithm. By similar patch stacking, the established noisy matrices are assumed to be composed of low-rank noise free image, sparse random impulse noise and zero-mean Gaussian noise. Minimizing the rank of the matrices, the mixed noise can be reduced efficiently. Comparative experiments on synthetic and real data are used to verify the algorithm's performance.	algorithm;dhrystone;experiment;focus stacking;impulse noise (audio);low-rank approximation;mean squared error;noise reduction;nonlinear system;sparse matrix	Xin-Wei Yang;Xiang-Bo Lin	2015	2015 10th International Conference on Information, Communications and Signal Processing (ICICS)	10.1109/ICICS.2015.7459919	gradient noise;gaussian noise;median filter;image noise;computer vision;speech recognition;visualization;dark-frame subtraction;sparse matrix;value noise;computer science;noise measurement;noise reduction;mathematics;statistics;salt-and-pepper noise	Robotics	57.69899574462404	-70.33787713676297	121146
ba4e0d79881f149285af1bc02b014556d398815d	multi-resolution based spatially adaptive multi-order total variation for image restoration		We propose a novel total variation based image restoration method that combines first and second order total variation functionals in a structurally adaptive manner. Specifically, as opposed to recent practical combined order methods, which choose the relative weight between the functionals of different derivative orders as a global scalar parameter, we construct a functional that uses a spatially varying relative weight between functionals of different orders so as to adapt to the local image structure. Further, as opposed to known combined order methods which leave the relative weight as a user parameter, the proposed method automatically computes the relative weight using a novel multi-resolution based formulation. The proposed spatially adaptive combined order TV (SAMTV) performs better than the state of art total generalized variation (TGV) method in experiments.	circuit restoration;experiment;image restoration	Sanjay Viswanath;Simon de Beco;Maxime Dahan;Muthuvel Arigovindan	2018	2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)	10.1109/ISBI.2018.8363624	image restoration;scalar (physics);artificial intelligence;pattern recognition;computer science	Vision	54.66806597052929	-69.59986963309773	121753
70e6d89e455f700d9f68343b947d6c7a711d7346	the initial estimate in constrained iterative restoration	degradation;iterative algorithms;projection onto convex sets;satisfiability;iterative methods;statistical distributions;signal restoration;iterative methods signal restoration iterative algorithms degradation equations statistical distributions	A relationship is shown between the usual formulation of constrained iterative restoration and restoration by successive projections onto convex sets. When used with a reasonable convergence criterion, this relationship characterizes the restoration as the point closest to the initial estimate which satisfies the convergence criterion. Experimental work is presented to demonstrate the effects of the choice of the initial estimate.	circuit restoration;iteration	H. Joel Trussell;M. Reha Civanlar	1983		10.1109/ICASSP.1983.1172254	probability distribution;mathematical optimization;combinatorics;degradation;mathematics;iterative method;statistics;satisfiability	Vision	54.8424087323994	-73.50495449155243	121867
cba8212e6ebce7ef7dc3c0080da2f97d140373a6	two curvelets variational models depend on decomposition spaces	curvelets;decomposition spaces;journal;shrinkage;variational model;besov spaces	Wavelet has become an appealing image processing technique, due to the fact that the sparseness of wavelet expansion is equivalent to smoothness measure in Besov spaces so that the regularization of image can be performed by manipulating its wavelet coefficients. Unfortunately, wavelets have good performance especially at representing point singularities, but they fail to efficiently represent object edges. As one of computational harmonic analysis tools, curvelets have an essentially optimal representation of objects which is C2 away from a C2 edge. In this paper, we first apply constraint of curvelet-type decomposition spaces as a regularizing term to variational model for image denoising. Based on the equivalent relationship between semi-norm of curvelet-type decomposition spaces and the weighted curvelet coefficients, solution to the proposed model approximately equals to different curvelet shrinkages. As a second contribution, we also propose another image restoration model from image decomposition point of view. Furthermore, an equivalent theorem of two proposed models is given. Finally, the experiment results show the superiority of proposed models over traditional wavelet-based ones.	spaces;variational principle	Guojun Liu;Xiangchu Feng;Weiwei Wang;Xuande Zhang	2013	IJWMIP	10.1142/S0219691313500069	computer vision;mathematical optimization;mathematical analysis;topology;computer science;curvelet;mathematics;shrinkage	NLP	55.37651565332419	-70.10631706838338	122707
cb764bcbea8875a2d135a167f716a005227886a5	a cascadic alternating krylov subspace image restoration method		This paper describes a cascadic image restoration method which at each level applies a two-way alternating denoising and deblurring procedure. Denoising is carried out with a wavelet transform, which also provides an estimate of the noise-level. The latter is used to determine a suitable regularization parameter for the Krylov subspace iterative deblurring method. The cascadic multilevel method proceeds from coarse to fine image resolution, using suitable restriction and prolongation operators. The choice of the latter is critical for the performance of the multilevel method. We introduce a special deblurring prolongation procedure based on TV regularization. Computed examples demonstrate the effectiveness of the method proposed for determining image restorations of high quality.	bidiagonalization;circuit restoration;deblurring;display resolution;image resolution;image restoration;iterative method;krylov subspace;noise reduction;numerical analysis;thresholding (image processing);wavelet transform;whole earth 'lectronic link	Serena Morigi;Lothar Reichel;Fiorella Sgallari	2013		10.1007/978-3-642-38267-3_9	computer vision;mathematical optimization;discrete mathematics;mathematics	Vision	56.34688331812118	-71.23646926912537	122729
7ea684fd3ddea96deda74116f0370cd76eec8ac6	the implementation of sure guided piecewise linear image denoising	stein s unbiased risk estimator;expectation maximization;denoising	SURE (Stein’s Unbiased Risk Estimator) guided Piecewise Linear Estimation (S-PLE) is a recently introduced patch-based state-of-the-art denoising algorithm. In this article, we focus on its implementation and show its performance by comparing it with several other acclaimed algorithms. Source Code ANSI C source code for both S-PLE and PLE is accessible on the article web page. A live demo for S-PLE can be found at the IPOL web page of this article1.	(rur-ple) - rur - python learning environment;ansi c;algorithm;noise reduction;patch (computing);web page	Yi-Qing Wang	2013	IPOL Journal	10.5201/ipol.2013.52	econometrics;mathematical optimization;minimum-variance unbiased estimator;stein's unbiased risk estimate;mathematics;statistics	ML	60.82223658505416	-73.93274417968803	123352
dabdf9dcdb14054f8baeb12d312723d1b2b52e71	gaussian modeling for channel errors diagnosis in image transmission	institutional repositories;pdf gaussian modeling channel errors diagnosis image transmission subband compressed image reconstruction channel transmission errors detection scheme subband decomposition gaussian random processes estimation step subband coefficients conditional statistics simulation visual results real world images image edges gaussian assumption ber gaussian probability density function;image coding;fedora;data compression;transmission error;gaussian processes;visual communication;signal detection;image restoration;vital;image transmission;random process;error statistics image coding data compression visual communication signal detection random processes gaussian processes image restoration;random processes;error diagnosis;error statistics;image communication image reconstruction statistics filters computational modeling image edge detection image restoration pixel redundancy error analysis;vtls;ils	In this paper we propose an original study of the reconstruction of subband compressed images impaired by channel transmission errors. The method proceeds in two steps : first a detection scheme is applied to determine which coefficients of the subband decomposition have been affected by transmission, and then an estimation step tries to evaluate the erroneous coefficients. In our model, subband coefficients are considered to be drawn from jointly gaussian random processes. Based on this assumption, conditional statistics can be computed which enable to test the likelihood of a given set of received coefficients with respect to the rest of the image. The detection and estimation processes are derived from these statistics. The method is validated through simulation and visual results are provided. The drawbacks of the method are outlined and explained through the discrepancies between the gaussian assumption and real world images, namely around image edges.	coefficient;qr decomposition;simulation;stochastic process	Fabrice Labeau;Luc Vandendorpe;Benoit M. Macq	1999		10.1109/ICASSP.1999.760627	data compression;image restoration;stochastic process;computer vision;computer science;pattern recognition;gaussian process;mathematics;statistics;visual communication;detection theory	ML	61.01529764695453	-70.61585326391533	123360
c2d2371efcf8763e9fcec3e80936c03b87c738cd	combining superresolution and fusion methods for sharpening misrsat-1 data	geophysical image processing;image resolution;image resolution geophysical image processing geophysical techniques high pass filters image fusion image registration;image fusion;high pass filters;image registration;superresolution sr fusion misrsat 1;strontium interpolation spatial resolution kernel image reconstruction noise;geophysical techniques	This paper presents an efficient technique for sharpening of Misrsat-1 data using superresolution (SR) methods and fusion methods. Due to the difference in spectral characteristics between bands 1 and 3 and the panchromatic (PAN) band of Misrsat-1, we implement SR on high details of these bands and use the resulting image to sharpen the bands of the multispectral (MS) image. Several SR methods are tested and compared in this paper for this purpose. The first class of methods uses spatial-domain SR, in which SR is performed on the high-pass details extracted from bands 1 and 3 and the PAN band. The superresolved high-pass details are used after that to enhance the spatial resolution of the MS data using the high-pass filter fusion method. The second class of methods depends on the interpolation of coefficients in the high-frequency subbands of a multiscale representation of bands 1 and 3 and the PAN band and an additive fusion method to add the high-frequency subband coefficients to different bands of the MS image. A comparison study between different SR methods belonging to the aforementioned classes such as nonuniform interpolation (NUI), projection onto convex sets (POCS), iterative back projection (IBP), structure-adaptive normalized convolution (SANC), and adaptive steering kernel regression (ASKR) is presented. The simulation results show that iterative SR methods such as IBP and POCS produce more noise than interpolation methods such as NUI, SANC, and ASKR. The results also reveal that combining the ASKR with a multiscale decomposition enhances the signal-to-noise ratio.	british undergraduate degree classification;coefficient;convolution;first-class function;interpolation;iterative method;multispectral image;natural user interface;signal-to-noise ratio;simulation;super-resolution imaging;utility functions on indivisible goods	Mohamed R. Metwalli;Ayman H. Nasr;Osama S. Faragallah;S. El-Rabaie;Fathi E. Abd El-Samie	2013	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2012.2209203	computer vision;image resolution;image registration;image fusion;high-pass filter;computer graphics (images)	Vision	66.0946892705456	-66.53151445646641	123490
71cfb34b1b7ffb5a377848729f1e2418dcbc4cd8	edge detection algorithm based on ica-domain shrinkage in noisy images	poisson noise;edge detection independent component analysis shrinkage function poisson noise;shrinkage function;edge detection;han xianhua dai shuiyan li jian xia guorong 边缘检测法 干扰图像 收索率 ica域 edge detection algorithm based on ica domain shrinkage in noisy images;natural images;independent component analysis;receptive field	We propose a robust edge detection method based on ICA-domain shrinkage (independent component analysis). It is known that most basis functions extracted from natural images by ICA are sparse and similar to localized and oriented receptive fields, and in the proposed edge detection method, a target image is first transformed by ICA basis functions and then the edges are detected or reconstructed with sparse components. Furthermore, by applying a shrinkage algorithm to filter out the components of noise in ICA-domain, we can readily obtain the sparse components of the original image, resulting in a kind of robust edge detection even for a noisy image with a very low SN ratio. The efficiency of the proposed method is demonstrated by experiments with some natural images.	algorithm;basis function;edge detection;experiment;independent computing architecture;independent component analysis;sparse matrix	Xian-Hua Han;Shuiyan Dai;Guorong Xia	2008	Science in China Series F: Information Sciences	10.1007/s11432-008-0114-1	independent component analysis;computer vision;speech recognition;edge detection;computer science;shot noise;pattern recognition;mathematics;canny edge detector;receptive field;statistics	Vision	55.97559181037041	-69.08662958183432	123712
ab4db78f678782824bbe7d33ba2f193d4426784a	non-linear filtering of images on the basis of generalized method of least absolute values		In article consider the possibility of usage of generalized method of least absolute values for non-linear filtering of images and signals. Generalized Method of Least Absolute Values is more efficient than median methods of image processing in case of impulse interference, as well as when suppressing noise interference on high-contrast images. Workload in case of data smoothing based on Generalized Method of Least Absolute Values is comparable with the volume of calculations of median filter. Examples of realization of a method are resulted.	image processing;impulse noise (audio);interference (communication);median filter;nonlinear system;smoothing	Alexander A. Tyrsin;Vladimir A. Surin	2014			filter (signal processing);image processing;least absolute deviations;median filter;statistics;smoothing;impulse (physics);nonlinear system;absolute value;mathematical optimization;mathematics	Graphics	55.84211417581769	-67.86118712450006	123750
b078f6d963167769f622e4753c46b950ee2d7538	integrability-regularized phase unwrapping via sparse error correction	optimization noise measurement surface reconstruction computational modeling manganese phase measurement synthetic aperture radar;synthetic surfaces integrability regularized phase unwrapping sparse error correction classical two dimensional phase unwrapping problem gradient domain measurement model sparse gradient errors corrected gradient field generalized lasso problem alternating direction method of multipliers algorithm interferometric synthetic aperture radar noise model;image processing gradient methods;interferometric synthetic aperture radar phase unwrapping sparse error correction integrability	We propose a new formulation of the classical two-dimensional phase unwrapping problem. Using a sparse-error, gradient-domain measurement model, we simultaneously seek the absolute phase and sparse gradient errors that minimize a novel energy functional that strongly encourages the integrability of the corrected gradient field. Our approach can be cast as a generalized lasso problem, and we compute the solution using the alternating direction method of multipliers (ADMM) algorithm. Adopting a commonly-used inter-ferometric synthetic aperture radar noise model, we evaluate our technique for several synthetic surfaces.	algorithm;augmented lagrangian method;error detection and correction;gradient;instantaneous phase;lasso;sparse matrix;synthetic intelligence	Garrett A. Warnell;Vishal M. Patel;Rama Chellappa	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7351736	computer vision;mathematical optimization;mathematics	Robotics	57.11077698544855	-72.58429482292075	123866
cf6f82027a92fa4913166bf36edccb1dc19cf4f1	compressed sensing reconstruction of 3d ultrasound data using dictionary learning and line-wise subsampling	undersampling patterns compressed sensing reconstruction 3d ultrasound data cs reconstruction method 3d ultrasound imaging overcomplete dictionary learning sparser representations us images spatially uniform random acquisition line wise random acquisition 3d acquisition process k svd algorithm training dataset 3d nonlog envelope us volumes 3d us log envelope volume minimal information loss;compressed sensing;overcomplete dictionaries;k svd;image reconstruction dictionaries three dimensional displays ultrasonic imaging compressed sensing biomedical imaging;3d ultrasound;sparse representation compressed sensing 3d ultrasound overcomplete dictionaries k svd;ultrasonic imaging biomedical ultrasonics compressed sensing image reconstruction image representation image sampling learning artificial intelligence medical image processing singular value decomposition;sparse representation	In this paper we present a compressed sensing (CS) method adapted to 3D ultrasound imaging (US). In contrast to previous work, we propose a new approach based on the use of learned overcomplete dictionaries that allow for much sparser representations of the signals since they are optimized for a particular class of images such as US images. In this study, the dictionary was learned using the K-SVD algorithm and CS reconstruction was performed on the non-log envelope data by removing 20% to 80% of the original data. Using numerically simulated images, we evaluate the influence of the training parameters and of the sampling strategy. The latter is done by comparing the two most common sampling patterns, i.e., point-wise and line-wise random patterns. The results show in particular that line-wise sampling yields an accuracy comparable to the conventional point-wise sampling. This indicates that CS acquisition of 3D data is feasible in a relatively simple setting, and thus offers the perspective of increasing the frame rate by skipping the acquisition of RF lines. Next, we evaluated this approach on US volumes of several ex vivo and in vivo organs. We first show that the learned dictionary approach yields better performances than conventional fixed transforms such as Fourier or discrete cosine. Finally, we investigate the generality of the learned dictionary approach and show that it is possible to build a general dictionary allowing to reliably reconstruct different volumes of different ex vivo or in vivo organs.	algorithm;chroma subsampling;compressed sensing;data dictionary;k-svd;machine learning;medical ultrasound;numerical analysis;performance;radio frequency;sampling (signal processing);singular value decomposition;video-in video-out	Oana Lorintiu;Hervé Liebgott;Martino Alessandrini;Olivier Bernard;Denis Friboulet	2014	IEEE Transactions on Medical Imaging	10.1109/ICIP.2014.7025263	computer vision;speech recognition;k-svd;computer science;machine learning;pattern recognition;sparse approximation;compressed sensing	Vision	54.57445634073419	-78.4663616277478	123937
3e8d702eb1c0885303499f3906d358f5bcd59321	adaptive varying-bandwidth modified nearest-neighborhood interpolation for denoising and edge detection	metodo polinomial;metodo adaptativo;largeur bande;driving force;analisis estadistico;intervalo confianza;fonction poids;nonparametric estimation;local polynomial approximation;edge detection;estimation non parametrique;methode adaptative;deteccion contorno;non parametric estimation;detection contour;confidence interval;aproximacion polinomial;statistical analysis;bandwidth selection;intersection of confidence intervals;polynomial method;intervalle confiance;adaptive method;anchura banda;analyse statistique;approximation polynomiale;funcion peso;bandwidth;denoising;estimacion no parametrica;weight function;methode polynomiale;polynomial approximation	New weight functions called lters or masks, have been designed through nonparametric Local Polynomial Approximation methods (LPA) for both, de-Noising and edge detection tasks. These new masks, are combined with the nearest neighborhood interpolators. The produced modi ed nearest neighborhood interpolation lter structures are incorporated to a new and e ective statistical strategy of bandwidth (window size) selection known as the Intersection of Con dence Intervals (ICI), rendering good performance and accuracy when dealing contaminated data. Nonparametric estimation methods (among them LPA) can be considered as being the driving force for the development of bandwidth selection methods (among them ICI):	algorithm;approximation;bandwidth (signal processing);edge detection;experiment;ici (programming language);interpolation;naruto shippuden: clash of ninja revolution 3;noise reduction;order of approximation;performance;polynomial;simulation;smoothing	Karen O. Egiazarian;Vladimir Katkovnik;Edisson Albán	2002		10.1117/12.467986	econometrics;mathematical optimization;mathematics;statistics	Vision	53.9254839667982	-66.75531290827284	124035
0073c802583918f67180effa7c77312fba604c9f	orthogonal matrix retrieval in cryo-electron microscopy	biological patents;x ray crystallography biological techniques electron microscopy mathematical programming singular value decomposition;biomedical journals;text mining;europe pubmed central;citation search;microscopy;citation networks;noise measurement;three dimensional displays image reconstruction covariance matrices microscopy crystallography noise measurement matrix decomposition;research articles;matrix decomposition;three dimensional displays;covariance matrices;abstracts;image reconstruction;open access;convex relaxation cryo electron microscopy 3d reconstruction single particle analysis ab initio modelling autocorrelation spherical harmonics polar decomposition semidefinite programming;life sciences;semidefinite programming orthogonal matrix retrieval cryo electron microscopy single particle reconstruction 3d molecule structure 2d projection images ad 1980 autocorrelation function 3d molecule expansion coefficient spherical harmonics phase retrieval problem x ray crystallography orthogonal extension orthogonal replacement singular value decomposition;clinical guidelines;full text;crystallography;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	In single particle reconstruction (SPR) from cryo-electron microscopy (EM), the 3D structure of a molecule needs to be determined from its 2D projection images taken at unknown viewing directions. Zvi Kam showed already in 1980 that the autocorrelation function of the 3D molecule over the rotation group SO(3) can be estimated from 2D projection images whose viewing directions are uniformly distributed over the sphere. The autocorrelation function determines the expansion coefficients of the 3D molecule in spherical harmonics up to an orthogonal matrix of size (2l+1)×(2l +1) for each l = 0, 1, 2, · · ·. In this paper we show how techniques for solving the phase retrieval problem in X-ray crystallography can be modified for the cryo-EM setup for retrieving the missing orthogonal matrices. Specifically, we present two new approaches that we term Orthogonal Extension and Orthogonal Replacement, in which the main algorithmic components are the singular value decomposition and semidefinite programming. We demonstrate the utility of these approaches through numerical experiments on simulated data.	autocorrelation;coefficient;cryoelectron microscopy;electron microscopy;experiment;like button;numerical analysis;phase retrieval;preparation;projection-slice theorem;semidefinite programming;single particle analysis;singular value decomposition;tomography, emission-computed, single-photon;tree rearrangement	Tejal Bhamre;Teng Zhang;Amit Singer	2015	2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2015.7164051	iterative reconstruction;computer vision;mathematical optimization;text mining;radiology;medicine;computer science;noise measurement;microscopy;theoretical computer science;machine learning;orthogonal procrustes problem;mathematics;matrix decomposition;statistics	Vision	55.407380319128585	-76.87447351784454	124647
6d507839987ab291774f7bf155c43754f44c9068	some modifications of gradient weighted filters	second order;π filters;image smoothing;local adaptation;rational filters;article;gradient weighted filters;pi filters	Gradient weighted filters are locally adaptive weighted mean filters. In this paper, a general formulation of gradient weighted filters with some characteristic parameters was derived first from existing gradient weighted filters. Then we propose some modifications by varying these parameters. We modify gradient inverse weighted filters, characterize filters into first and second order filters, and propose 5 filters. Moreover, an imposed criterion for second order filters to preserve fine details was introduced to promote the existing gradient weighted filters. Finally, a criterion to combine first and second order filters was proposed to remove noise with mixed types. Throughout this paper, rational analysis and experimental results demonstrate the efficiency of the proposed methods. C © 1999 Academic Press	gradient	Rey-Sern Lin;Yung-Cheh Hsueh	1999	J. Visual Communication and Image Representation	10.1006/jvci.1999.0427	network synthesis filters;mathematical optimization;combinatorics;control theory;mathematics;prototype filter;second-order logic	ML	55.240514719508916	-67.81381799032557	124722
be5dd8195cf1b2ba42aac21c5e875c44f20c2e2e	self-adaptive spatial image denoising model based on scale correlation and sure-let in the nonsubsampled contourlet transform domain	scale correlation;sure let;nonsubsampled contourlet transform;image denoising;spatial images	A novel self-adaptive image denoising model based on scale correlation and Stein’s unbiased risk estimate-linear expansion of thresholds (SURE-LET) in the nonsubsampled contourlet transform domain is proposed in this paper. First we implement the multidimensional and translation invariant decomposition for spatial images by the nonsubsampled contourlet transform, and establish the image cross-scale description structure. Then combining the scale correlation, we make improvements for the existing SURE-LET denoising idea and establish the self-adaptive denoising mechanism. The scale correlation calculation is needed for the coefficients at different scales and sub-bands to determine whether the coefficients are retained or processed with the adaptive SURE-LET threshold shrinkage. And meanwhile a new local context self-adaptive threshold strategy is proposed in the process of scale correlation calculation. Experimental results both on spatial images and standard images demonstrate that the proposed algorithm performs significantly better in terms of both the visual subjective evaluation and the quantitative objective evaluation. The method can achieve better noise suppression, and effectively retain image edge details.	algorithm;coefficient;contourlet;data structure;image analysis;image noise;lazy evaluation;noise reduction;zero suppression	MeiYu Liang;Junping Du;Honggang Liu	2013	Science China Information Sciences	10.1007/s11432-013-4943-1	computer vision;contourlet;pattern recognition;mathematics;statistics	Vision	58.380508103471605	-67.50289690033922	124778
af2d954b9f75faa78ada243a2cfe377a59d5be53	image bit-depth enhancement via maximum a posteriori estimation of ac signal	biological patents;biomedical journals;text mining;europe pubmed central;citation search;graph signal processing bit depth enhancement;citation networks;convex programming image bit depth enhancement maximum a posteriori estimation ac signal least significant bits minimum mean squared error mmse graph signal smoothness;research articles;abstracts;open access;life sciences;clinical guidelines;smoothing methods convex programming graph theory image enhancement least mean squares methods maximum likelihood estimation quantisation signal;quantization signal image reconstruction image edge detection discrete cosine transforms distortion spatial resolution imaging;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	When images at low bit-depth are rendered at high bit-depth displays, missing least significant bits needs to be estimated. We study the image bit-depth enhancement problem: estimating an original image from its quantized version from a minimum mean squared error (MMSE) perspective. We first argue that a graph-signal smoothness prior-one defined on a graph embedding the image structure-is an appropriate prior for the bit-depth enhancement problem. We next show that directly solving for the MMSE solution is, in general, too computationally expensive to be practical. We then propose an efficient approximation strategy. In particular, we first estimate the ac component of the desired signal in a maximum a posteriori formulation, efficiently computed via convex programming. We then compute the dc component with an MMSE criterion in a closed form given the computed ac component. Experiments show that our proposed two-step approach has improved performance over the conventional bit-depth enhancement schemes in both objective and subjective comparisons.	addendum;algorithm;analysis of algorithms;appendix;approximation;computation;concave function;const (computer programming);convex function;convex optimization;estimated;experiment;graph - visual representation;graph embedding;lafora disease;least significant bit;map;mathematical optimization;maxima and minima;mean squared error;mini-mental state examination;optimization problem;polynomial;quadratic function;quadratic programming;quantization (signal processing);sample variance;shiatsu	Pengfei Wan;Gene Cheung;Dinei A. F. Florêncio;Cha Zhang;Oscar C. Au	2016	IEEE Transactions on Image Processing	10.1109/TIP.2016.2553523	text mining;medical research;computer science;theoretical computer science;machine learning;pattern recognition;statistics	Vision	54.532413244625204	-74.92993517859968	124977
cd7a357dcb932120aaca340d8c8f3536a6ee31d5	a total variation regularization strategy in dynamic mri	euler lagrange equation;unconstrained minimization;dynamical processes;total variation regularization;magnetic resonance image;regularization;quasi newton method;total variation;numerical experiment;dynamic mri	Recently, the application of Magnetic Resonance Imaging to the study of dynamic processes has increased the search of ever-faster dynamic imaging methods. The Keyhole method is a popular technique for fast dynamic imaging that unfortunately produces images affected by artifacts. In this work, we propose a Total Variation regularization strategy for post-processing the dynamic Keyhole images. The strategy consists in the solution of an unconstrained minimization problem whose Euler–Lagrange equation is solved by a quasi-Newton method. A trace of the algorithm is given. Results of numerical experiments on simulated and real data are presented to illustrate the effectiveness of the proposed method. Our experimental results indicate that the method reduces the degrading artifacts and improves the quality of the final images.	algorithm;aliasing;artifact (software development);dynamic imaging;euler;euler–lagrange equation;experiment;heuristic;iteration;linear system;newton's method;noise reduction;numerical analysis;preconditioner;quasi-newton method;resonance;rollover (key);total variation denoising;truncation;video post-processing	Germana Landi;E. Loli Piccolomini	2005	Optimization Methods and Software	10.1080/10556780500140300	regularization;mathematical optimization;quasi-newton method;magnetic resonance imaging;calculus;dynamic contrast-enhanced mri;mathematics;geometry;total variation denoising;total variation	Robotics	56.08321462699201	-71.7831771783367	125140
cc6296383ff434f941e84c1a0a479ddabe29f6d7	generalized row-action methods for tomographic imaging	90c52;65r32;regularization;incremental methods;65k05;proximal methods;tomographic imaging;inverse problems	Row-action methods play an important role in tomographic image reconstruction. Many such methods can be viewed as incremental gradient methods for minimizing a sum of a large number of convex functions, and despite their relatively poor global rate of convergence, these methods often exhibit fast initial convergence which is desirable in applications where a low-accuracy solution is acceptable. In this paper, we propose relaxed variants of a class of incremental proximal gradient methods, and these variants generalize many existing row-action methods for tomographic imaging. Moreover, they allow us to derive new incremental algorithms for tomographic imaging that incorporate different types of prior information via regularization. We demonstrate the efficacy of the approach with some numerical examples.	algorithm;convex function;dynamic problem (algorithms);iterative reconstruction;numerical analysis;numerical method;proximal gradient method;proximal gradient methods for learning;rate of convergence;stochastic gradient descent;tomography	Martin S. Andersen;Per Christian Hansen	2013	Numerical Algorithms	10.1007/s11075-013-9778-8	regularization;mathematical optimization;mathematical analysis;inverse problem;mathematics;geometry	ML	56.49714515468513	-73.10962567702339	125471
47095f6650d6a26e48ee27ac55d6e77ec947b76f	robust dequantized compressive sensing	compressive sensing;signal reconstruction;optimization;quantization	We consider the reconstruction problem in compressed sensi ng in which the observations are recorded in a finite number of bits. They may thus contain quan tiz tion errors (from being rounded to the nearest representable value) and saturation errors (from being outside the range of representable values). Our formulation has an objective of weightedl2-l1 type, along with constraints that account explicitly for quantization and satu r tion errors, and is solved with an augmented Lagrangian method. We prove a consistency result for the recovered solution, stronger than those that have appeared to date in the literature, show ing in particular that asymptotic consistency can be obtained without oversampling. We present e xtensive computational comparisons with formulations proposed previously, and variants there of.	augmented lagrangian method;compressed sensing;computation;oversampling;reconstruction conjecture;sensible soccer	Ji Liu;Stephen J. Wright	2012	CoRR		signal reconstruction;computer vision;mathematical optimization;combinatorics;mathematical analysis;quantization;mathematics;geometry;compressed sensing;statistics	ML	66.39520520422748	-71.40860567215421	125684
853025046581986e3d2d6564553512846c6a80fd	image fusion method of sar and infrared image based on curvelet transform with adaptive weighting	sar image;infrared image;image fusion;adaptive weighting;curvelet transform	This paper analyses the characteristics of infrared detection imaging, with the specific application of multi-scale analysis theory for SAR and infrared image fusion. After learning and discussing the research results in image fusion area at home and abroad, it proposes an adaptive weighted image fusion method which combines the idea of fuzzy theory based on Curvelet transform, i.e., defines the membership function with fuzzy logic variables, makes different weights to transform coefficients of different levels, and designs a kind of adaptive weighted image fusion strategy. Experimental results validate the reliability and credibility of this method in term of visual quality and objective evaluation, and it can effectively improve the fusion quality.	coefficient;curvelet;fuzzy logic;image fusion	Xiuxia Ji;Gong Zhang	2015	Multimedia Tools and Applications	10.1007/s11042-015-2879-8	computer vision;curvelet;pattern recognition;data mining;image fusion	Vision	59.78580331066749	-66.71687251137222	125717
05c90a37b5a4394e03d3ff467c3e263d2482af26	escaping path approach with extended neighborhood for speckle noise reduction		This paper presents a new approach to multiplicative noise removal in ultrasound images. The proposed algorithm utilizes concept of digital paths created on the image grid presented in [14] adapted to the needs of multiplicative noise reduction. Digital Paths are used to determine the filter weights taking into account the structures present in the image. Method of creating path is crucial for the efficiency and speed of the filter. The new approach uses special type of digital paths based on so called Escaping Path Model created with extended neighborhood system. The experiments confirmed that the proposed algorithm achieves a comparable results with the existing state of the art denoising schemes in suppressing multiplicative noise in ultrasound images.	noise reduction	Marek Szczepański;Krystian Radlak;Adam Popowicz	2015		10.1007/978-3-319-19390-8_21	grid;computer vision;noise reduction;multiplicative noise;path analysis (statistics);speckle noise;artificial intelligence;computer science	EDA	56.964136004219654	-66.60266071405653	125917
54dc2d6f40085a66d835a703a2ef5fb59ad06366	multiresolution fusion of multispectral and panchromatic images through the curvelet transform	discrete wavelet transforms;domain model;remote sensing image;high resolution;discrete wavelet transform;filter bank;image resolution;image fusion;low resolution;data mining;physics;radiometry;laplacian pyramid;laplace equations;feature extraction;very high resolution;image resolution image fusion multiresolution analysis spatial resolution discrete wavelet transforms wavelet domain radiometry data mining laplace equations physics;wavelet domain;multiresolution analysis;high frequency;spectral resolution;spatial resolution	This paper presents a novel image fusion method, suitable for pan-sharpening of multispectral (MS) bands, based on multiresolution analysis (MRA). The low-resolution MS bands are sharpened by injecting highpass directional details extracted from the high-resolution panchromatic (Pan) image by means of the curvelet transform, which is a nonseparable MRA, whose basis function are directional edges with progressively increasing resolution. The advantage with respect to conventional separable MRA, either decimated or not, is twofold: directional detail coefficients matching image edges may be preliminarily softthresholded to achieve denoising better than in the separable wavelet domain; modeling of the relationships between highresolution detail coefficients of MS bands and of the Pan image is more fitting, being carried out in a directional wavelet domain. Experiments carried out on a very-high resolution MS + Pan QuickBird image show that the proposed curvelet method quantitatively outperforms state-of-the art image fusion methods, in terms of geometric, radiometric, and spectral fidelity.	basis function;coefficient;curvelet;decimation (signal processing);image fusion;image resolution;multiresolution analysis;multispectral image;noise reduction;wavelet	Andrea Garzelli;Filippo Nencini;Luciano Alparone;Stefano Baronti	2005	Proceedings. 2005 IEEE International Geoscience and Remote Sensing Symposium, 2005. IGARSS '05.	10.1109/IGARSS.2005.1525659	computer vision;image resolution;computer science;pattern recognition;physics;remote sensing	Robotics	66.4271968219318	-66.34911217525028	125966
cb55dfd6a285d3950978ee6e247349cd2d2374ca	locally adaptive speckle noise reduction using maximum a posteriori estimation based on maxwell distribution	wavelet transforms biomedical ultrasonics image denoising maximum likelihood estimation speckle;map;speckle;bayesian estimation;peppers image;ultrasonic imaging;edge preservation factor speckle noise reduction maximum a posteriori estimation maxwell distribution bayesian estimation image denoising ultrasound imaging peppers image;speckle noise;bayesian methods;maximum likelihood estimation;speckle noise reduction maximum a posteriori estimation maxwell boltzmann distribution bayesian methods psnr wavelet domain wavelet coefficients laplace equations ultrasonic imaging;maximum a posteriori estimation;maxwell distribution;noise measurement;wavelet transforms;ultrasound imaging;maxwell distribution speckle noise wavelet transform denoising map ultrasound image;ultrasound image;wavelet transform;noise reduction;image denoising;denoising;speckle noise reduction;edge preservation factor;biomedical ultrasonics;noise	This paper introduces a speckle noise reduction algorithm using Bayesian estimation in the wavelet domain. The wavelet coefficients of the log-transformed signal are modeled by Laplacian distribution, while those of the log-transformed speckle are modeled by Maxwell distribution. The Bayesian maximum a posteriori (MAP) estimation is basically based on the presumption that speckle is spatially correlated within a small window. In this paper, the window size is automatically regulated depending on the statistics, such as mean and variance. Simulations are performed using synthetically real speckled ultrasound (US)) image and peppers image. The results show that the proposed method can conduct better than some of the existing methods in terms of the Peak Signal to Noise Ratio (PSNR) and the edge preservation factor.	algorithm;coefficient;computer simulation;maxwell (microarchitecture);noise reduction;peak signal-to-noise ratio;wavelet	Sung Gug Kim;Yoo Shin Kim;Il Kyu Eom	2009	2009 IEEE Workshop on Signal Processing Systems	10.1109/SIPS.2009.5336242	computer vision;mathematical optimization;computer science;noise reduction;mathematics;statistics;wavelet transform	Vision	56.95537457204205	-67.74129087075522	126302
124efd0a7aae299b7d90fcb5dbaee6f1c3d78107	nonparametric estimation and simulation of two-dimensional gaussian image textures	simulation ordinateur;gaussian field;campo gaussiano;metodo monte carlo;algorithm performance;image processing;analisis textura;nonparametric estimation;spectrum analysis;analyse spectre;etude experimentale;analisis espectro;gaussian random field;procesamiento imagen;methode monte carlo;spectrum;traitement image;image texture;champ gaussien;algorithme;statistical properties;algorithm;texture analysis;resultado algoritmo;smoothing;monte carlo method;performance algorithme;alisamiento;simulacion computadora;estimation statistique;estimacion estadistica;statistical estimation;analyse texture;computer simulation;estudio experimental;lissage;algoritmo	The work to be described is motivated by the need to simulate a variety of real{world image textures, all of which can be well approximated by stationary Gaussian random elds (SGRFs). Speci cally, given an observed SGRF T , we wish to simulate SGRFs which look like and possess similar statistical properties to T . The main contribution of this paper is the development of an automatic and nonparametric spectrum estimation procedure which is able to produce an estimated spectrum of T in such a way that SGRFs simulated from this estimated spectrum have these desirable characteristics. Two special features of the procedure are: i) it relies on a di erent risk function to that commonly used in nonparametric spectrum estimation; and ii) it chooses its smoothing parameters by the technique of unbiased risk estimation. Results from a simulation study and a practical example demonstrate the good performance of the procedure. The practical example also illustrates how the proposed procedure can be combined with Monte Carlo testing to tackle target testing problems. Finally, the procedure is applied to the synthesis of some Brodatz textures, with some success.	approximation algorithm;bitmap textures;loss function;monte carlo method;simulation;smoothing;spectral density estimation;stationary process;textures: a photographic album for artists and designers	Thomas C. M. Lee;Mark Berman	1997	CVGIP: Graphical Model and Image Processing	10.1006/gmip.1997.0439	computer simulation;gaussian random field;image texture;spectrum;computer vision;econometrics;spectrum analyzer;image processing;computer science;mathematics;statistics;smoothing;monte carlo method	ML	61.80572607406658	-69.65363420178376	126776
d32aee17e626486be5ed99a76cce9373057ba2c3	an accelerated two-step iteration hybrid-norm algorithm for image restoration			algorithm;circuit restoration;image restoration;iteration	Yong Wang;Wenjuan Xu;Xiaoyu Yang;Qianqian Qiao;Zheng Jia;Quanxue Gao	2015		10.1007/978-3-662-48570-5_33	image restoration;mathematical optimization;compressed sensing;mathematics	Vision	56.640874914383225	-71.70386529318756	126859
cf886442ae9f33904e35307923e6525d7352d976	image decomposition using bregman-gtv and meyer's g-norm	minimisation;generalized total variation;computational modeling numerical models mathematical model image decomposition tv image edge detection;g norm;regularization;image texture;generalized total variational regularization image decomposition bregman gtv meyer g norm stair casing avoidance small scale texture information preservation classical total variation regularization minimization energy functional model preconditioned split bregman method generalized function space;image decomposition;minimisation image texture;g norm image decomposition regularization generalized total variation	In order to avoid stair casing and preserve small scale texture information for the classical total variation regularization, a new minimization energy functional model for image decomposition is proposed. We firstly introduce a generalized total variational regularization. The oscillatory component containing texture and/or noise is modeled in generalized function space. And then, the proposed model is numerically implemented by using a preconditioned Split Bregman method. Experiments show that the proposed model can avoid efficiently the staircasing effect, at the same time, both well remain edge and texture.	algorithm;bregman divergence;bregman method;calculus of variations;computation;doppler effect;experiment;fastest;function model;manifold regularization;matrix regularization;numerical analysis;total variation denoising;variational principle	Chengwu Lu;Daoqing Zhou	2013	2013 5th International Conference on Intelligent Networking and Collaborative Systems	10.1109/INCoS.2013.142	image texture;regularization;minimisation;mathematical optimization;computer science;total variation denoising;statistics	Robotics	54.96403353375169	-70.81485079491792	127020
6851886f401f45d42d8509e7549c8a4f454a96fc	an iterative algorithm for spectral estimation with spatial smoothing	admm multispectral imaging spectral reconstruction;spectral analysis hyperspectral imaging image resolution iterative methods;spectral regularizers iterative algorithm spatial smoothing multispectral imaging systems raw data processing radiance spectra spectral estimation algorithm alternating direction method of multipliers admm surface spectral reflectance;estimation lighting optimization convergence iterative methods light emitting diodes smoothing methods	Many multispectral imaging systems are computational in nature and require processing of raw data in order to obtain radiance spectra. In this paper, we derive a fast and scalable spectral estimation algorithm based on the Alternating Direction Method of Multipliers (ADMM). Using this approach we solve for the unknown surface spectral reflectance simultaneously for all pixels in the image. This global formulation allows us to incorporate spatial as well as spectral regularizers, such as total variation penalty or non-negativity. We show that the estimates derived with our solver are more accurate and more robust in the presence of noise.	algorithm;augmented lagrangian method;iterative method;multispectral image;negativity (quantum mechanics);pixel;scalability;smoothing;solver;spectral density estimation;total variation diminishing	Henryk Blasinski;Joyce E. Farrell;Brian A. Wandell	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7350937	full spectral imaging;computer vision;mathematical optimization	Robotics	57.156186917119925	-72.51876459722544	127479
cbb33f3e047bf8ea98fbfd666f2319bb2770b73f	joint dimensionality reduction, classification and segmentation of hyperspectral images	image sampling;image segmentation hyperspectral imaging bayesian methods pixel image analysis matrix decomposition image sampling computational modeling principal component analysis independent component analysis;image segmentation;blind source separation;bayes methods;image classification;markov processes approximation theory bayes methods blind source separation image classification image sampling image segmentation;approximation theory;bayesian computation;hidden variables;bayesian estimator;blind source sep aration;data reduction;markov processes;mfa bayesian estimation approach blind sources separation bss hidden classification variable potts markov field image segmentation hyperspectral image gibbs sampling mean field approximation method;mean field approximation;dimensional reduction;hyperspectral image;hierarchical model	Dimensionality reduction, spectral classification and segmentation are the three main problems in hyperspectral image analysis. In this paper we propose a Bayesian estimation approach which gives a solution for these three problems jointly. The data reduction problem is modeled as a blind sources separation (BSS) where the sources are the images which must be mutually independent and piecewise homogeneous. To insure these properties, we propose hierarchical model for the sources with a common hidden classification variable winch-is modelled as a Potts-Markov field. The joint Bayesian estimation of this hidden variable as well as the sources and the mixing matrix of the BSS problem gives a solution for all the three problems of dimensionality reduction, spectra classification and segmentation of hyperspectral images. For the Bayesian computation, we propose to use either Gibbs sampling (GS) or mean field approximation (MFA) methods. A few simulation results illustrate the performances of the proposed method and some comparison with other classical methods of PCA and ICA used for BSS.	approximation;computation;dimensionality reduction;gibbs sampling;hidden variable theory;hierarchical database model;image analysis;independent computing architecture;markov chain;markov random field;mean field particle methods;performance;potts model;principal component analysis;roland gs;sampling (signal processing);simulation;stellar classification	Nadia Bali;Ali Mohammad-Djafari;Adel Mohammadpour	2006	2006 International Conference on Image Processing	10.1109/ICIP.2006.312649	computer vision;contextual image classification;data reduction;computer science;mean field theory;machine learning;pattern recognition;mathematics;blind signal separation;image segmentation;markov process;hidden variable theory;hierarchical database model;statistics;approximation theory	ML	61.34455504906001	-71.83377051616449	128321
3a42a0d0d4668d4df29437cb39c9366be3e0096c	combined compressed sensing and parallel mri compared for uniform and random cartesian undersampling of k-space	image sampling;kernel;compressed sensing;magnetic resonance image;parallel imaging;springs;sampling patterns compressed sensing magnetic resonance imaging image reconstruction parallel imaging;image compression;coils;image reconstruction;magnetic resonance imaging;imaging;sampling patterns;coils springs kernel image reconstruction noise imaging compressed sensing;mri compressed sensing parallel imaging magnetic resonance images images sampling cs spring random undersampling undersampling factor image reconstruction;noise;biomedical mri;image sampling biomedical mri image reconstruction	Both compressed sensing (CS) and parallel imaging effectively reconstruct magnetic resonance images from undersampled data. Combining both methods enables imaging with greater undersampling than accomplished previously. This paper investigates the choice of a suitable sampling pattern to accommodate both CS and parallel imaging. A combined method named SpRING is described and extended to handle random undersampling, and both GRAPPA and SpRING are evaluated for uniform and random undersampling using both simulated and real data. For the simulated data, when the undersampling factor is large, SpRING performs better with random undersampling. However, random undersampling is not as beneficial to SpRING for real data with approximate sparsity.	approximation algorithm;compressed sensing;random number generation;resonance;sampling (signal processing);sparse matrix;spring;undersampling	Daniel S. Weller;Jonathan R. Polimeni;Leo Grady;Lawrence L. Wald;Elfar Adalsteinsson;Vivek K. Goyal	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5946463	iterative reconstruction;computer vision;kernel;oversampling and undersampling in data analysis;image compression;computer science;noise;magnetic resonance imaging;compressed sensing	Robotics	54.37474591452053	-78.57819164264131	128413
b92035cff386c70f046c0e4a713d7793b5ea6cd2	stitching defect detection and classification using wavelet transform and bp neural network	image segmentation;binary image;stitching defect;spectral measure;wavelet transform;automatic detection;quadrant mean filter;back propagation;neural network;defect classification	In the textile and clothing industry, much research has been conducted on fabric defect automatic detection and classification. However, little research has been done to evaluate specifically the stitching defects of a garment. In this study, a stitching detection and classification technique is presented, which combines the improved thresholding method based on the wavelet transform with the back propagation (BP) neural network. The smooth subimage at a certain resolution level using the pyramid wavelet transform was obtained. The study uses the direct thresholding method, which is based on wavelet transform smooth subimages from the use of a quadrant mean filtering method, to attenuate the texture background and preserve the anomalies. The images are then segmented by thresholding processing and noise filtering. Nine characteristic variables based on the spectral measure of the binary images were collected and input into a BP neural network to classify the sample images. The classification results demonstrate that the proposed method can identify five classes of stitching defects effectively. Comparisons of the proposed new direct thresholding method with the direct thresholding method based on the wavelet transform detailed subimages and the automatic band selection for wavelet reconstruction method were made and the experimental results show that the proposed method outperforms the other two approaches. 2008 Published by Elsevier Ltd.	artificial neural network;backpropagation;binary image;image stitching;software bug;software propagation;thresholding (image processing);wavelet transform	Wai Keung Wong;C. W. M. Yuen;D. D. Fan;L. K. Chan;E. H. K. Fung	2009	Expert Syst. Appl.	10.1016/j.eswa.2008.02.066	wavelet;computer vision;second-generation wavelet transform;binary image;computer science;backpropagation;machine learning;pattern recognition;wavelet packet decomposition;stationary wavelet transform;thresholding;image segmentation;discrete wavelet transform;artificial neural network;wavelet transform	Vision	57.21800602533812	-66.8095493400778	128493
951b763139faadcf4e3db2ff48afcd192dea7644	a magnetic resonance image reconstruction method using support of first-second order variation	split bregman;first second order variation;support information;compressive sensing	This article addresses the problem of reconstructing a magnetic resonance image from highly undersampled data, which frequently arises in accelerated magnetic resonance imaging. We propose to impose sparsity of first and second order difference sparse coefficients within the complement of the known support. Second order variation is involved to overcome blocky effects and support information is used to reduce the sampling rate further. The resulting optimization problem consists of a data fidelity term and first-second order variation terms penalizing entries within the complement of the known support. The efficient split Bregman algorithm is used to solve the problem. Reconstruction results from magnetic resonance imaging data corresponding to different sampling rates are shown to illustrate the performance of the proposed method. Then, we also assess the tolerance of the new method to noise briefly. © 2015 Wiley Periodicals, Inc. Int J Imaging Syst Technol, 25, 277-284, 2015	iterative reconstruction;resonance	Xiangzhen Gao;Huiqian Du;Ru Jia;Wenbo Mei	2015	Int. J. Imaging Systems and Technology	10.1002/ima.22149	computer vision;mathematical optimization;computer science;artificial intelligence;mathematics;compressed sensing;statistics	Robotics	56.75379244101422	-71.4297374474965	128690
65fb1e0c5c8be8244429faafa7b0d018eeb0055d	poisson cloning using bilateral image coarsening	poisson cloning;boundary condition;bic space;gradient projection;bleeding artifact	"""Poisson cloning can clone a source patch to target image seamlessly given a constant boundary condition, but it usually results in """"bleeding artifact"""" near the boundary, which is difficult to prevent only by boundary optimization. To address the problem, we switch Poisson cloning to bilateral image coarsening (BIC) space that binds similar pixels together in smooth regions and leaves pixels across edges independent to reduce the bleeding artifact. We present three ways to project Poisson cloning into the BIC space. Direct projection projects a result image into BIC space by solving a minimization problem. Gradient projection projects boundary and gradient of a source patch instead of the cloned image, which is more robust and is more consist with the operation of Poisson cloning. Modified gradient projection computes a correction function with deleted gradient of boundary in BIC space, and it improves the computation performance while it can obtain the similar result as gradient projection when the regions near the boundary are smooth. Experimental results demonstrate that our method reduces the bleeding artifact significantly and the sacrifice of performance is quite small."""	bayesian information criterion;bilateral filter;computation;gradient;mathematical optimization;pixel;quantum cloning	Hao Wu;Dan Xu	2010		10.1145/1937728.1937740	mathematical optimization;combinatorics;discrete mathematics;mathematics	ML	54.71608732290909	-71.09442703423925	129054
dd4d432b32cfec89c1ffb4532ed704899bf17baf	dwt-based additive image watermarking using the student-t prior	discrete wavelet transforms;discrete wavelet transform;probability density function;discrete wavelet transforms watermarking;statistical distributions;statistical distributions discrete wavelet transforms image watermarking;image watermarking;unintentional attacks dwt based additive image watermarking problem discrete wavelet transform student t prior blind watermark detectors marginal subband wavelet distributions student t probability density function intentional attacks;high performance	In this work, a class of new blind watermark detectors is proposed for the DWT (Discrete Wavelet Transform)-based additive image watermarking problem. More specific, we model the marginal subband wavelet distributions with the Student-t probability density function (pdf) deriving a new watermark detector. The proposed detector shows high performance with regard to the watermark detection and increased robust properties against intentional or unintentional attacks. Experimental results on real images demonstrate these properties comparing the proposed detector with other state of the art methods in the transform domain.	coefficient;digital watermarking;discrete wavelet transform;expectation–maximization algorithm;iterative method;jpeg;marginal model;markov chain monte carlo;noise shaping;portable document format;profiling (computer programming);receiver operating characteristic;sensor;utility functions on indivisible goods	Antonis Mairgiotis;Yongyi Yang;Lisimachos P. Kondi	2011	2011 IEEE International Workshop on Information Forensics and Security	10.1109/WIFS.2011.6123147	wavelet;computer vision;speech recognition;harmonic wavelet transform;second-generation wavelet transform;continuous wavelet transform;pattern recognition;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;wavelet transform	Vision	61.03524832474993	-69.30088788031094	129135
e4886b89fbe2e21981b4588ff275ee7801eb9bd2	error analysis for image inpainting	total variation minimization;image restoration;error analysis;inpainting;inpainting domain;image inpainting;error bound	Image inpainting refers to restoring a damaged image with missing information. In recent years, there have been many developments on computational approaches to image inpainting problem [2, 4, 6, 9, 11–13, 27, 28]. While there are many effective algorithms available, there is still a lack of theoretical understanding on under what conditions these algorithms work well. In this paper, we take a step in this direction. We investigate an error bound for inpainting methods, by considering different image spaces such as smooth images, piecewise constant images and a particular kind of piecewise continuous images. Numerical results are presented to validate the theoretical error bounds.	algorithm;error analysis (mathematics);experiment;inpainting;maxima and minima	Tony F. Chan;Sung Ha Kang	2006	Journal of Mathematical Imaging and Vision	10.1007/s10851-006-6865-7	image restoration;computer vision;mathematical optimization;computer science;pattern recognition;mathematics;inpainting	Vision	56.32193351087114	-72.12812120853675	129434
59afa2bc3ac6c57dc962c51cbb8413dcbe61547d	sparse representation of a blur kernel for blind image restoration		Blind image restoration is a non-convex problem which involves restoration of images from an unknown blur kernel. The factors affecting the performance of this restoration are how much prior information about an image and a blur kernel are provided and what algorithm is used to perform the restoration task. Prior information on images is often employed to restore the sharpness of the edges of an image. By contrast, no consensus is still present regarding what prior information to use in restoring from a blur kernel due to complex image blurring processes. In this paper, we propose modelling of a blur kernel as a sparse linear combinations of basic 2-D patterns. Our approach has a competitive edge over the existing blur kernel modelling methods because our method has the flexibility to customize the dictionary design, which makes it well-adaptive to a variety of applications. As a demonstration, we construct a dictionary formed by basic patterns derived from the Kronecker product of Gaussian sequences. We also compare our results with those derived by other state-of-the-art methods, in terms of peak signal to noise ratio (PSNR).	algorithm;box blur;circuit restoration;convex optimization;dictionary;gaussian blur;image restoration;kernel (operating system);peak signal-to-noise ratio;proximal gradient method;proximal gradient methods for learning;sparse approximation;sparse matrix;variable splitting	Chia-Chen Lee;Wen-Liang Hwang	2015	CoRR		image restoration;computer vision;machine learning;gaussian blur;pattern recognition;mathematics	Vision	57.30889304753425	-71.09057871001498	129781
0cecc05f54a8821f757a722fff593dad9ac8ff32	model-based region-of-interest estimation for adaptive resource allocation in multi-aperture imaging systems	additive noise region of interest estimation adaptive resource allocation multi aperture imaging systems flat profile multiplexed optical imaging system intelligent resource allocation aerial images local entropy 2d normalized power spectral density saliency map estimator;computational imaging systems;saliency map estimator;image resolution;saliency map;local entropy;saliency;resource allocation;resource management;additive noise;high resolution imaging;layout;information content;aerial image;power spectral density;imaging system;resource management optical imaging high resolution imaging layout sensor arrays entropy image reconstruction signal processing algorithms optical signal processing lenses;optical imaging;image reconstruction;image resolution image reconstruction;region of interest;optical signal processing;field of view;lenses;image reconstruction computational imaging systems information theory saliency;aerial images;2d normalized power spectral density;flat profile multiplexed optical imaging system;entropy;multi aperture imaging systems;region of interest estimation;intelligent resource allocation;signal processing algorithms;adaptive resource allocation;sensor arrays;information theory	Using intelligent resource allocation based on the information content in the imaging system's field-of-view for the successful design of a flat-profile multiplexed optical imaging system requires the use of adaptive techniques. This paper describes a model-based technique for determining regions of interest in aerial images using the 2D normalized power spectral density within Gilles' saliency map estimator. The proposed technique exploits the 1/falpha spatial spectral shape of such natural imagery in a computationally-simple approach that is robust to additive noise. Application of the method to candidate aerial images shows its ability to identify consistent regions of interest for such data.	additive white gaussian noise;aerial photography;multiplexing;region of interest;self-information;spectral density;utility functions on indivisible goods	Indranil Sinharoy;Scott C. Douglas	2007	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2007.366306	iterative reconstruction;layout;computer vision;entropy;mathematical optimization;image resolution;self-information;field of view;information theory;resource allocation;resource management;salience;optical imaging;lens;spectral density;statistics;region of interest	Robotics	65.14045959709004	-67.86341612949995	129849
82209ac4e33fbe9aa605ec508970b9cea11ab47d	best linear unbiased estimators for properties of digitized straight lines	quantization;best linear unbiased estimate;linearity;probability;measurement accuracy;performance evaluation;chain code string;digitized straight lines;chain code;probability density function;length measurement;data mining;blue estimators;universiteitsbibliotheek;physics;estimation;low earth orbit satellites;quantization blue estimators chain code string digitization error digitized straight lines length measurement measurement accuracy;length measurement character recognition low earth orbit satellites quantization performance evaluation physics pathology biomedical informatics linearity;digitization error;biomedical informatics;character recognition;pathology	This paper considers the problem of measuring properties of digitized straight lines from the viewpoint of measurement methodology. The measurement and estimation process is described in detail, revealing the importance of a step called ``characterization'' which was not recognized explicitly before. Using this new concept, BLUE (Best Linear Unbiased) estimators are found. These are calculated for various properties of digitized straight lines, and are briefly compared to previous work.	published comment;system of measurement	Leo Dorst;Arnold W. M. Smeulders	1986	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.1986.4767781	health informatics;computer vision;estimation;probability density function;speech recognition;quantization;length measurement;computer science;theoretical computer science;probability;mathematics;accuracy and precision;linearity;chain code;statistics	Vision	59.78911465778808	-79.88793633146098	129869
72466bf166b9077da9ef3ff8de52a67c1e4b2f4e	a total variation based reconstruction algorithm for 3d ultrasound		This paper presents an algorithm for reconstructing a three dimensional image from a set of noisy two dimensional images, corrupted with Rayleigh distributed multiplicative noise, which is the observational model for Ultrasound imaging. The proposed method performs a variable splitting to introduce an auxiliary variable to serve as the argument of the 3D total variation term. Applying the Augmented Lagrangian framework and using an iterative alternating minimization method leads to simpler problems involving TV minimization with a least squares term. The resulting Gauss Seidel scheme is an instance of the Alternating Direction Method of Multipliers (ADMM) method, for which convergence is guaranteed. Experimental results show that the proposed method is faster and achieves a lower mean square error than existing methods.	3d reconstruction;algorithm;augmented lagrangian method;computation;experiment;gauss–seidel method;iterative method;least squares;mean squared error;multiplicative noise;online locator service;rayleigh–ritz method;synthetic data;time complexity;variable splitting;velocity (software development)	Manya V. Afonso;João Miguel Raposo Sanches	2013		10.1007/978-3-642-38628-2_17	mathematical optimization;combinatorics;machine learning;mathematics;algorithm;statistics	Vision	56.86185525393371	-74.04757952576293	129906
8343233fd9fa18539e5b03127ba06c5274ca1f5d	application of kernel density estimation for color image filtering	color image;impulse noise;median filter;kernel density estimate;probability density function	This paper presents a new filtering scheme for the removal of impulsive noise in multichannel images. It is based on estimating the probability density function for image pixels in a filtering window by means of the kernel density estimation method. The filtering algorithm itself is based on the comparison of pixels with their neighborhood in a sliding filter window. The quality of noise suppression and detail preservation of the new filter is measured quantitatively in terms of the standard image quality criteria. The filtering results obtained with the new filter show its excellent ability to reduce noise while simultaneously preserving fine image details.	color image;kernel density estimation	Bogdan Smolka;Rastislav Lukac;Konstantinos N. Plataniotis;Anastasios N. Venetsanopoulos	2003			image quality;filter;median filter;kernel density estimation;computer vision;kernel method;probability density function;speech recognition;density estimation;color image;telecommunications;image processing;impulse noise;computer science;noise reduction;sound quality;mathematics;non-local means;variable kernel density estimation;statistics;salt-and-pepper noise	Vision	57.167767664503174	-67.09851198041484	129940
58e99bdbfc0a86ca7e04770389c292fa0ead7206	wavelet-based deconvolution for ill-conditioned systems	wavelet domain noise suppression;mse performance;balance;temporary;regularized inverse filter;fourier series;image processing;convolution;inversion;lti wiener filter;fourier domain system inversion;performance;frequency estimation;signals;mathematical filters;image restoration;theses;inverse system;distortion measurement;noise suppression;visual quality;ill conditioned systems;wavelet transforms;mean square error metric;digital filters deconvolution wavelet transforms fourier analysis image processing mean square error methods inverse problems;quality;mean square error;signal processing;noise reduction;digital filters;fourier domain system;wavelet based deconvolution methods;deconvolution;mean square error methods;fourier analysis;wavelet based deconvolution;algorithms;singularities;optimization;performance wavelet based deconvolution ill conditioned systems fourier domain system inversion wavelet domain noise suppression wavelet based deconvolution methods regularized inverse filter inverse system mean square error metric signals images edges singularities;wiener filter;wavelet domain regularization;edges;wavelet domain;images;deconvolution noise reduction wavelet domain signal processing convolution wiener filter frequency estimation distortion measurement image processing image restoration;noise;inverse problems	In this paper, we propose a new approach to wavelet-based deconvolution. Roughly speaking, the algorithm comprises Fourierdomain system inversion followed by wavelet-domain noise suppression. Our approach subsumes a number of other wavelet-based deconvolution methods. In contrast to other wavelet-based approaches, however, we employ a regularized inverse filter, which allows the algorithm to operate even when the inverse system is illconditioned or non-invertible. Using a mean-square-error metric, we strike an optimal balance between Fourier-domain and waveletdomain regularization. The result is a fast deconvolution algorithm ideally suited to signals and images with edges and other singularities. In simulations with real data, the algorithm outperforms the LTI Wiener filter and other wavelet-based deconvolution algorithms in terms of both visual quality and MSE performance.	algorithm;condition number;deconvolution;ibm system i;inverse filter;mean squared error;simulation;wavelet;wiener filter	Ramesh Neelamani;Hyeokho Choi;Richard G. Baraniuk	1999		10.1109/ICASSP.1999.757532	gravitational singularity;inversion;inverse system;image restoration;edge;computer vision;mathematical optimization;mathematical analysis;digital filter;performance;image processing;inverse problem;noise;deconvolution;signal processing;noise reduction;cascade algorithm;mathematics;mean squared error;blind deconvolution;wavelet packet decomposition;stationary wavelet transform;fourier analysis;convolution;wiener filter;balance;fourier series;statistics;wavelet transform;wiener deconvolution	ML	55.739005912299696	-67.34239083326845	130883
4023345566df6c106b3a91f5693fd76334fc3111	an optimization of spatio-spectral filter bank design for eeg classification	spatio spectral filter;classification;common spatial filter;mutual information;eeg;optimization	How to select the appropriate frequency band to classify EEG signal by motor imagery is discussed in this paper. Our proposal is an improvement of the conventional Bayesian Spatio-Spectral Filter Optimization (BSSFO). Defect of BSSFO is on the way to generate the renewal particle of the filter bank, such a random number generation. To avoid a local optimum, an evolutional update method of particles is introduced. It is shown that performance of the EEG classification ability is improved.	electroencephalography;filter (signal processing);filter bank;frequency band;local optimum;mathematical optimization;random number generation;software bug	Masanao Obayashi;Takuya Geshi;Takashi Kuremoto;Shingo Mabu	2016	JRNAL	10.2991/jrnal.2016.2.4.3	computer vision;machine learning;pattern recognition	ML	61.72199546863118	-76.28212436759458	131331
19fcd73353a351ea8351d9baa9e3e899b638c48e	denoising 3-d magnitude magnetic resonance images based on weighted nuclear norm minimization	weighted nuclear norm minimization;mri denoising;low rank matrix approximation;non local similarity	A new denoising algorithm based on low-rank matrix approximation (LRMA) with regularization of weighted nuclear norm minimization (WNNM) is proposed to remove Rician noise of magnetic resonance (MR) images. This technique simply groups similar non-local cubic blocks from noisy 3D MR data into a patch matrix with each block lexicographically vectorizing to be as a column, calculates the singular value decomposition (SVD) on this matrix, then the closed-form solution of LRMA is achieved by hard-thresholding different singular values with a different threshold. The denoised blocks are obtained from this estimate of the low-rank matrix, and the final estimate of the whole noise-free MR data is built up by aggregating all the denoised exemplar blocks that are overlapped each other. To further improve the denoising performance of the WNNM algorithm, we first realize the above denoising procedure in a two-iteration regularization framework, and then a simple non local means (NLM) filter based on single-pixel patch is utilized to reduce the intensity jumping at the homogeneous area. The proposed denoising algorithm was compared with related state-of-the-art methods and produced very competitive results over synthetic and real 3D MR data.	noise reduction;resonance	Yi Xia;Qingwei Gao;Nan Cheng;Yixiang Lu;Dexiang Zhang;Qiang Ye	2017	Biomed. Signal Proc. and Control	10.1016/j.bspc.2017.01.016	mathematical optimization;combinatorics;discrete mathematics;mathematics	ML	57.09173742458012	-71.2601973458239	131925
44ddca6ab8e140ec34c2277cdd109252f8edb7a2	sar image despeckling via bivariate shrinkage based on directionlet transform	bivariate shrinkage;directionlet transform;despeckling algorithm	Synthetic aperture radar (SAR) images are inherently affected by multiplicative speckle noise, which is due to the coherent nature of the scattering phenomenon. A novel and efficient SAR image despeckling algorithm based on Directionlet transform using bivariate shrinkage is proposed to remove speckle noise while preserving the structural features and textural information of the scene. First, an anisotropic directionlet transform is taken on the logarithmically transformed SAR images. The distribution of speckle noise is modeled as an additive Gaussian distribution with zero-mean. Then, a bivariate shrinkage with local variance estimation is applied to the decomposed directionlet coefficients of the logarithmically transformed image to estimate the best value for the noise-free signal. Finally, the performance of the proposed algorithm is compared with those of existing despeckling methods applied on both synthetic speckled images and actual SAR images. Experimental results show that compared with conventional wavelet and contourlet despeckling algorithm, the proposed algorithm can keep the better balance between suppresses speckle effectively and preserves image details, and the important feature of original image like textures and contour details is well maintained.	algorithm;anisotropic diffusion;bivariate data;coefficient;coherence (physics);contourlet;noise reduction;subject reduction;synthetic data;synthetic intelligence;thresholding (image processing);utility functions on indivisible goods;wavelet transform	Feng Xue;Dexiang Zhang;Honghai Wang	2014	JCP	10.4304/jcp.9.11.2587-2594	computer vision;pattern recognition;mathematics;statistics	Vision	57.29590975064772	-67.84137254516685	131932
31c6413e229930c0df71d558f382449ac6d4105a	spatial resolution analysis of iterative image reconstruction with separate regularization of real and imaginary parts	fmri reconstruction;image resolution;imaginary parts;fmri reconstruction spatial resolution analysis iterative image reconstruction image regularization real parts imaginary parts roughness penalty local impulse response fast algorithm;local impulse response;real parts;image regularization;spatial resolution image analysis image reconstruction cost function image resolution iterative algorithms vectors iterative methods reconstruction algorithms physics;spatial resolution analysis;iterative methods;medical image processing biomedical mri image reconstruction image resolution iterative methods;image reconstruction;medical image processing;fast algorithm;biomedical image processing;impulse response;iterative image reconstruction;roughness penalty;article;biomedical mri;spatial resolution	A common method of improving the conditioning in iterative image reconstruction is to include regularization in the reconstruction algorithm. One such regularization is the roughness penalty, which when used in the algorithm encourages smoother images. For complex valued images, the roughness penalty typically penalizes equally the real and imaginary parts. The desired resolution of the reconstructed image can then be evaluated using the local impulse response. A fast algorithm to calculate it was developed for the typical roughness penalty, used for matching the regularization parameter expediently to the desired resolution. For some cases its advantageous to penalize independently the real and imaginary parts. This paper proposes a fast algorithm to calculate the local impulse response for that penalty and applies it to an fMRI reconstruction problem	algorithm;imaginary time;iterative method;iterative reconstruction;matrix regularization;reconstruction conjecture	Valur T. Olafsson;Jeffrey A. Fessler;Douglas C. Noll	2006	3rd IEEE International Symposium on Biomedical Imaging: Nano to Macro, 2006.	10.1109/ISBI.2006.1624838	computer vision;mathematical optimization;radiology;image resolution;computer science;machine learning;mathematics	Vision	56.245766265907314	-73.67514116061135	132001
dab2968941fde4c547b9580941499bd7f603d8a5	adaptive weighted dα filter	adaptive filters;image edge detection;anisotropic magnetoresistance;information filters;noise	In this paper we propose a new adaptive weighted dα filter. The filter is adaptive regarding noise amplitude distribution, orientation of structures and anisotropy measures. The filter coefficient are chosen according to structure orientation and anisotropy measures. α value is chosen according to the result of local noise distribution and anisotropy coefficient estimations. Some experimental results on synthetic and natural images are presented. Results are compared with those of adaptive filters such as the adaptive trimmed mean.	adaptive filter;coefficient;synthetic intelligence;the filter	Imad Issa;Philippe Bolon	1996	1996 8th European Signal Processing Conference (EUSIPCO 1996)		adaptive filter;mathematical optimization;electronic engineering;kernel adaptive filter;control theory;mathematics;prototype filter	Vision	56.57220348678629	-66.44525130334883	132394
4b903bb83014ceb6886fd92b712d6b419ab80bee	digital signal restoration using fuzzy sets	traitement signal;degradation;convergence;fuzzy set;image coding;signal restoration fuzzy sets image restoration degradation wiener filter convergence pollution measurement size measurement image coding tomography;optimization technique;pollution measurement;restoration;size measurement;image;ensemble flou;image restoration;procesamiento de senales;set theory;fuzzy sets;codificacion;imagen;signal processing;tomographie;coding;restauration;conjunto flojo;a priori information;signal restoration;wiener filter;quality measures;tomografia;tomography;information theory;codage;restauracion	A new signal restoration method with considerable flexibility in incorporating a priori information is developed. The method defines a fuzzy set for each piece of information to restrict the set of acceptable solutions. Using fuzzy sets makes it possible to model partially defined information as well as exact knowledge. The intersection of all the fuzzy sets is the feasibility set. The original signal is a member of this set with a high membership value, and any high membership valued element of this set is a nonrejectable solution. Such solutions can be computed by using optimization techniques. Ideally, the feasibility set contains only the original signal. The chance of recovering the original signal decreases as the feasibility set gets larger. Thus, the size of the feasibility set gives a quality measure for the solution. The method generated successful results in many restorations for which the conventional techniques have failed, and may be applied in image coding and tomography.	circuit restoration;fuzzy set	M. Reha Civanlar;Henry Joel Trussell	1986	IEEE Trans. Acoustics, Speech, and Signal Processing	10.1109/TASSP.1986.1164875	computer vision;mathematical optimization;result set;membership function;defuzzification;computer science;artificial intelligence;fuzzy number;mathematics;tomography;fuzzy set;fuzzy set operations;set function;statistics	Vision	58.937314861094826	-69.70660462870687	132522
06aac9619741fbdfd279c9309170e858c5b16f38	blind image deconvolution using the sylvester matrix		Blind image deconvolution refers to the process of determining both an exact image and the blurring function from its inexact image. This thesis presents a solution of the blind image deconvolution problem using polynomial computations. The proposed solution does not require prior knowledge of the blurring function or noise level. Blind image deconvolution is needed in many applications, such as astronomy, remote sensing and medical X-ray, where noise is present in the exact image and blurring function. It is shown that the Sylvester resultant matrix enables the blurring function to be calculated using approximate greatest common divisor computations, rather than greatest common divisor computations. A developed method for the computation of an approximate greatest common divisor of two inexact univariate polynomials is employed here, to identify arbitrary forms of the blurring function. The deblurred image is then calculated by deconvolving the computed blurring function from the degraded image, using polynomial division. Moreover, high performance computing is considered to speed up the calculation performed in the spatial domain. The effectiveness of the proposed solution is demonstrated by experimental results for the deblurred image and the blurring function, and the results are compared with the state-of-the-art image deblurring algorithm.	approximation algorithm;computation;deblurring;deconvolution;noise (electronics);polynomial greatest common divisor;polynomial long division;resultant;supercomputer;sylvester matrix;x-ray (amazon kindle)	Nora Alkhaldi	2014			image restoration;mathematical optimization;mathematics;geometry	Vision	57.02444084090113	-74.57739353026068	133563
1b2cf57fc95853c364bb72bc4eaf9621e1b0bff1	pixel based reconstruction (pbr) techniques	complex objects;steepest descent approach parallel computers algebraic reconstruction algorithms computed tomography heuristics iterative methods memory requirement accelerating techniques pixel based reconstruction region of interest;computed tomography;medical image processing computerised tomography iterative methods;iterative methods;medical image processing;region of interest;computerised tomography;bioreactors image reconstruction computed tomography pixel concurrent computing reconstruction algorithms iterative methods acceleration frequency convolution;reconstruction algorithm;iteration method;acceleration techniques;steepest descent	"""W e have reinvestigated Algebraic Reconstruction algorithms for Computed Tomography, with some new heuristics and some new methodologies, which indicate that they merit serious consideration for practical applications. Methods of implementing these iterative methods more e f f ien t ly , by reducing the computations, reducing memory requirement and using accelerating techniques are discussed. W e have shown by simulations, using the Pixel Based Reconstruction ( PBR ) method proposed by Fager et. al, that excellent reconstructions of a complex object can be obtained with very few projections. For comparision, we have given reconstructions by CBP, which indicate that CBP performs rather poorly under the same conditions. PBR methods are simultaneous in nature. i.e., at each iteration, any pixel value does not depend on any other pixel value in the reconstructed image. Because of this, in principle, each pixel can be assigned to its own computer and advantage can be taken of parallel structured computers. W e have investigated two methods of improving these algorithms : ( I ) Region of Interest, which involves discarding all the zero-valued ray-sums from projections and zero-valued pixels from the reconstruction region, (2) Steepest Descent ( SD ) approach, giving very good reconstructions in about 6 iterations. Introduction Early in the short history of Computed Tomography, the so called """" algebraic methods """" were used. Now, however, commercial applications almost exclusively use Fourier methods, despite the fact that algebraic methods h2d been shown to be capable of providing good reconstructions from very few projections. The reasons for this were practical. Algebraic methods. many of which are iterative, took more computations. For reasons of both speed and computational cost, algebraic methods were ruled out. Now, computing power has become less expensive, practical memory size has increased many-fold, and, in particular. parallel processing has advanced significantly in power, ease of use. and affordability. For this reason, we have reinvestigated algebraic methods and have found that they merit serious consideration in practical devices. There are certain intrinsic advantages of algebraic methods. A principal advantage is that high spatial frequency components represent fine details, but they also represent noise and, in Fourier methods, these high frequencies are amplified by convolution with a chosen function, which tends to amplify the fine details in the reconstructed image. However, such convolution also amplifies the random noise present in the projections. It is never then possible to separate them completely. The baby can not be recovered without bringing along the bath water and the bath water can't be poured out without pouring out the baby. Algebraic methods reference local areas, so statistical fluctuations in the ray-sums of different rays will tend to cancel, while sharp edges in the real attenuation coefficient will be preserved. This results in good reconstructions with fewer projections. The value of this for CT scanners is more rapid scanning, less exposure to the patient, and less heating for the X-ray tubes. Among the algebraic methods which had been used before, we are interested more methods involves a separate equation for each pixel, i.e.. with each pixel calculation depending only on parameters from the previous iteration. The most prominent attempt of this type was the SIRT method of Peter Gilbert[3], but when the inventor of this method stated that it diverged, there was much less attention paid to this entire area, We have reinvestigated the question with a heuristic which seems more logical to us than that was originally used. In this monograph, we report on 170 0-8186-2742-5/92 $3.00"""	algorithm;algorithmic efficiency;ct scan;coefficient;computation;computer;convolution;dvd region code;gradient descent;heuristic (computer science);iteration;iterative method;linear algebra;noise (electronics);pbr theorem;parallel computing;pixel;region of interest;simulation;tomography;usability	Kumar V. Peddanarappagari;Roger S. Fager	1992		10.1109/CBMS.1992.244950	iterative reconstruction;computer vision;mathematical optimization;radiology;computer science;theoretical computer science;algebraic reconstruction technique;iterative method;computed tomography	Vision	56.18922548387383	-75.62085655225643	133617
3d800e0259af9fe10936f0c6911a8b33c879c147	image restoration and edge extraction based on 2-d stochastic models	stochastic resonance;edge detection;finite impulse response filter;additive noise;edge extraction;image restoration;noise robustness;image restoration stochastic processes finite impulse response filter signal to noise ratio stochastic resonance signal restoration image edge detection noise robustness additive noise covariance matrix;stochastic processes;image edge detection;fir filter;signal restoration;stochastic model;signal to noise ratio;white noise;covariance matrix	In this paper, we consider application of 2—D stochastic models discussed in [1] to develop non— causal FIR filters for restoration of images degraded by additive white noise. The semicausal model of [1] is used to design masks for edge extraction from the noisy images. The results presented here indicate that good restorations and robust edge detection are possible using relatively simple algorithms.	algorithm;causal filter;circuit restoration;edge detection;finite impulse response;image restoration;stochastic process;utility functions on indivisible goods;white noise	Anil K. Jain;Surendra Ranganath	1982		10.1109/ICASSP.1982.1171477	stochastic process;speech recognition;computer science;finite impulse response;mathematics;statistics	ML	53.885128044501556	-66.7853100657871	133743
e01ad2b04cf387860648ee5aff50a50d6fe5f8cb	image denoising using wavelet thresholding	wavelet thresholding;discrete wavelet transform.;image denoising;adaptive thresholding;wiener filter;discrete wavelet transform;standard deviation	This paper proposes an adaptive threshold estimation method for image denoising in the wavelet domain based on the generalized Guassian distribution(GGD) modeling of subband coefficients. The proposed method called NormalShrink is computationally more efficient and adaptive because the parameters required for estimating the threshold depend on subband data .The threshold is computed by βσ 2 / σy Where σ and σy are the standard deviationof the noise and the subband data of noisy image respectively . β is the scale parameter ,which depends upon the subband size and number of decompositions . Experimental results on several test image are compared with various denoising techniques like wiener Filtering [2], BayesShrink [3] and SureShrink [4]. To benchmark against the best possible performance of a threshold estimate , the comparison also include Oracleshrink .Experimental results show that the proposed threshold removes noise significantly and remains within 4% of OracleShrink and outperforms SureShrink, BayesShrink and Wiener filtering most of the time.	benchmark (computing);coefficient;noise reduction;standard test image;thresholding (image processing);wavelet;wiener filter	Lakhwinder Kaur;Savita Gupta;Ruchi Chauhan	2002			stationary wavelet transform;wiener filter;wavelet;pattern recognition;computer science;artificial intelligence;non-local means;second-generation wavelet transform;balanced histogram thresholding;wavelet packet decomposition;cascade algorithm	ML	57.65642996618401	-67.9293395377283	133899
b514b1eb69e2bde379876e130e593a84987e89a7	decision-based hybrid image watermarking in wavelet domain using hvs and neural networks	image texture;copyright protection;wavelet transform;image watermarking;spatial information;artificial neural network;neural network;generalization capability	This paper presents a Decision-based Hybrid Image Watermarking (DHIW) technique, based on the Human Visible System (HVS) and an Artificial Neural Network (ANN), for image copyright protection in wavelet domain. In [1], an image watermarking technique, called the IWNN technique, utilizes an ANN to extract watermarks without original images. However, the IWNN technique performs poorly for highly complicated image textures because the generalization capability of neural networks is powerfully effective in dealing with smooth image textures. Therefore, the PAIW method is proposed to enhance the IWNN technique, which uses the spatial information associated with wavelet-transformed images. The DHIW technique takes advantages of these two techniques by using a decision preprocessor. Experimental results prove that the DHIW technique remarkably outperforms other existing schemes.	artificial neural network;digital watermarking;human visual system model;wavelet	Hung-Hsu Tsai	2007		10.1007/978-3-540-72395-0_111	image texture;computer vision;computer science;artificial intelligence;machine learning;spatial analysis;artificial neural network;wavelet transform	Robotics	60.63897623353206	-66.59160517546526	133993
35427c95a37354b669a3a3b545f6dda76f9f2e85	variational models for fusion and denoising of multifocus images	image fusion;power transform variational models multifocus image fusion multifocus image denoising noisy multifocus images noise reduction gradients modulus wavelet coefficients source images;variational techniques;energy function;wavelet transforms;noise reduction;energy functional;power transformer;variational model;variational model energy functional multifocus image fusion and denoising;total variation;image denoising;weight function;variational models;multifocus image fusion and denoising;noise reduction focusing wavelet domain wavelet coefficients image fusion pixel layout tv wavelet transforms availability;wavelet transforms image denoising variational techniques	In this letter, variational models in pixel domain and wavelet domain are presented for fusion and denoising of noisy multifocus images. In pixel domain, the problem is formulized as minimizing a weighted energy functional, where the total variation (TV) is used as regularity constraint for noise reduction. A new family of weight functions for fusion is proposed that are based on the local average modulus of gradients and the power transform. In wavelet domain, the problem is formulized as shrinkage of the weighted wavelet coefficients of source images, where weight functions are based on the local average modulus of intra- and inter-scale wavelet coefficients and the power transform. The experiments are made to verify the effectiveness of the proposed methods.	algorithm;calculus of variations;coefficient;experiment;gradient;modulus of continuity;noise reduction;pixel;variational principle;wavelet;weight function	Weiwei Wang;Penglang Shui;Xiangchu Feng	2008	IEEE Signal Processing Letters	10.1109/LSP.2007.911148	computer vision;mathematical optimization;weight function;second-generation wavelet transform;pattern recognition;noise reduction;mathematics;wavelet packet decomposition;stationary wavelet transform;image fusion;fast wavelet transform;energy functional;total variation;transformer;statistics;wavelet transform	Vision	57.228971924083034	-68.59414542360079	134469
ecdc68eee555204aae91d553078aaa6c35addbcb	statistical regularization in linearized microwave imaging through mrf-based map estimation: hyperparameter estimation and image computation	minimisation;metodo regularizacion;bayes estimation;imagerie hyperfrequence;evaluation performance;bayesian estimation;quadratic function;funcion cuadratica;fonction quadratique;performance evaluation;image processing;image formation;fonction energie;modelo markov;cost function;gaussian processes;implementation;evaluacion prestacion;regularization method;simulacion numerica;procesamiento imagen;methode regularisation;estimation a posteriori;microwave imaging;indexing terms;maximum likelihood estimation;funcion coste;traitement image;markov random field;energy function;a posteriori estimation;iterative methods;ejecucion;minimizacion costo;estimacion bayes;markov model;campo aleatorio;minimisation cout;cost minimization;expectation maximization;estimacion a posteriori;image reconstruction;microwave imaging tomography markov random fields microwave theory and techniques radar scattering radar detection buried object detection approximation methods optimization methods gaussian processes;simulation numerique;tomographie;microwave tomography;markov random fields mrf;map estimation;algorithme em;funcion energia;bayesian estimator;fonction cout;image reconstruction maximum likelihood estimation markov processes microwave imaging gaussian processes iterative methods minimisation tomography;algoritmo em;markov processes;cost function statistical regularization linearized microwave imaging mrf based map estimation hyperparameter estimation image computation markov random fields maximum a posteriori estimation gaussian mrf energy function hyperparameters expectation maximization algorithm em algorithm complex nonhomogeneous images;tikhonov regularization;modele markov;tomografia;em algorithm;tomography;champ aleatoire;estimation bayes;numerical simulation;random field	The application of a Markov random fields (MRF) based maximum a posteriori (MAP) estimation method for microwave imaging is presented in this paper. The adopted MRF family is the so-called Gaussian-MRF (GMRF), whose energy function is quadratic. In order to implement the MAP estimation, first, the MRF hyperparameters are estimated by means of the expectation-maximization (EM) algorithm, extended in this case to complex and nonhomogeneous images. Then, it is implemented by minimizing a cost function whose gradient is fully analytically evaluated. Thanks to the quadratic nature of the energy function of the MRF, well posedness and efficiency of the proposed method can be simultaneously guaranteed. Numerical results, also performed on real data, show the good performance of the method, also when compared with conventional techniques like Tikhonov regularization.		Vito Pascazio;Giancarlo Ferraiuolo	2003	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/TIP.2003.811507	econometrics;mathematical optimization;expectation–maximization algorithm;computer science;mathematics;tomography;statistics	Vision	54.92510736584921	-73.03278157380808	134493
f6e756ea2ebd9faf0a0e5771896f6604abaeb650	spatial regularization of pixel-based classification maps by a two-step mrf method	spectral dissimilarity spatial regularization mrf;mrf classification models pixel based classification map spatial regularization markov random field based spatial regularizing methodology spatial smoothness image grid oversmoothing image boundary areas classic isotropous smoothness prior two step mrf regularization algorithm spectral cost class cost spatial modules local spectral dissimilarity object boundary preservation anisotropic spatial energy function class co occurrence dependency remote sensing data sets;image edge detection;linear programming image edge detection hyperspectral imaging context classification algorithms;classification algorithms;linear programming;hyperspectral imaging;context;remote sensing costing geophysical image processing image classification markov processes	Markov random field (MRF)based spatial regularizing methodology can improve the maps by imposing a spatial smoothness prior on the image grid, but also leads to oversmoothing at image boundary areas. This problem is caused by the reason that classic isotropous smoothness prior cannot take local discontinuities into account. In this context, this paper proposes a novel two-step MRF regularization algorithm, which addresses the problem by combining both spectral and class cost in spatial modules. The developed MRF method first establishes a spatial energy function integrating local spectral dissimilarity to smooth the initial classification map while preserving object boundaries. Second, a new anisotropic spatial energy function integrating the class co-occurrence dependency is constructed to regularize pixels around object boundaries. The effectiveness of the proposed MRF method is validated by a series of remote sensing data sets. The obtained results indicate that the method can significantly improve the classification accuracy with regards to traditional MRF classification models.	algorithm;estimation theory;map;markov chain;markov random field;mathematical optimization;matrix regularization;multispectral image;pixel;regularization by spectral filtering;smoothing	Leiguang Wang;Qinling Dai;Xin Huang	2016	2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2016.7729621	computer vision;computer science;linear programming;hyperspectral imaging;machine learning;pattern recognition;mathematics;remote sensing	Vision	67.43818615031712	-67.08236537717193	135061
996300ce8ee80bd2c0ffb6052920f27bb251162c	neutron imaging with coded sources: design pitfalls and the implementation of a simultaneous iterative reconstruction technique	sensors;point spread functions;reconstruction algorithms;scattering;transparency;imaging systems	The limitations in neutron flux and resolution (L/D) of current neutron imaging systems can be addressed with a Coded Source Imaging system with magnification (xCSI). More precisely, the multiple sources in an xCSI system can exceed the flux of a single pinhole system for several orders of magnitude, while maintaining a higher L/D with the small sources. Moreover, designing for an xCSI system reduces noise from neutron scattering, because the object is placed away from the detector to achieve magnification. However, xCSI systems are adversely affected by correlated noise such as non-uniform illumination of the neutron source, incorrect sampling of the coded radiograph, misalignment of the coded masks, mask transparency, and the imperfection of the system Point Spread Function (PSF). We argue that a model-based reconstruction algorithm can overcome these problems and describe the implementation of a Simultaneous Iterative Reconstruction Technique algorithm for coded sources. Design pitfalls that preclude a satisfactory reconstruction are documented.	algorithm;coded aperture;convolution;image quality;iterative method;iterative reconstruction;radiography;sampling (signal processing);sensor	Hector J. Santos-Villalobos;Philip R. Bingham;Jens Gregor	2013		10.1117/12.2008687	simulation;sensor;optics;transparency;scattering;physics;quantum mechanics	Networks	57.063594410643915	-75.60827998030067	135070
37b8ed4f9b332eb7566ee263fdfd009f6148a00f	noise estimation of polarization-encoded images by peano-hilbert fractal path	estimation theory;fractals;image coding;image denoising;image filtering;noise;polarimeters;polarisation;bootstrap re-sampling;peano-hilbert fractal path;stokes channels;stokes parameters imaging;additive noise;intensity measurement;multidimensional structure;multispectral filtering methods;noise estimation;physical information;polarimetric measurement;polarization-encoded images filtering	In the framework of Stokes parameters imaging, polarization-encoded images have four channels. The potential of such multidimensional structure comes from the set of physical information they carry about the local nature of the target. However, the noise that affects the intensity measurement may induce the non physical meaning of Stokes parameters making awkward their analysis and interpretation. This induces the need for a proper tool that allows the processing of polarization-encoded images while respecting their physical content. In this paper a new method to filter the additive noise of polarimetric measurement is addressed. This method is based on two multispectral filtering methods combined with a transformation of Stokes channels following a fractal path. The proposed algorithm is a tradeoff between the filtering of polarization-encoded images and the preserving of their physical content. The statistical performances of the method are tested on simulated and real images using Bootstrap re-sampling.	additive white gaussian noise;algorithm;automatic vectorization;filter (signal processing);fractal;interpretation (logic);multiplicative noise;multispectral image;navier–stokes equations;performance;physical information;polarimetry;polarization (waves);sampling (signal processing);smoothing;stokes parameters;utility functions on indivisible goods	Samia Ainouz;Jihad Zallat;Fabrice Mériaudeau	2008	2008 16th European Signal Processing Conference		gaussian noise;median filter;image noise;computer vision;value noise;noise measurement;mathematics;optics;statistics	Vision	66.96002711505876	-67.96492595032856	135097
dc3d387ae48ed6fbf991d9386f9a1e43eb0ad36b	halftone image watermarking via optimization	watermarking;halftone image;error diffusion;optimization;halftone image watermarking	Although halftone image watermarking technologies have been rapidly developing in the 21st century, the existing techniques lack a theoretical basis. In this paper, we tackle halftone image watermarking problems from a theoretical perspective. First, we propose a general optimization framework for Halftone Visual Watermarking (HVW), which is a certain category of halftone image watermarking techniques. Then two specific HVW problems, Single-sided Embedding Error Diffusion (SEED) and Double-sided Embedding Error Diffusion (DEED) are presented and solved by applying the proposed framework. With SEED and DEED obtained, both the theoretical solutions and experimental results indicate that our previous heuristic methods, Data Hiding by Conjugate Error Diffusion (DHCED) and Data Hiding by Dual Conjugate Error Diffusion (DHDCED), are special cases of SEED and DEED, respectively. We also demonstrate that DEED can achieve outstanding performance compared to DHDCED and other previous methods by selecting different parameters. With this paper, we essentially build a bridge between the theory and practical implementations of HVW problems.	digital watermarking;mathematical optimization	Yuanfang Guo;Oscar C. Au;Jiantao Zhou;Ketan Tang;Xiaopeng Fan	2016	Sig. Proc.: Image Comm.	10.1016/j.image.2015.12.002	computer vision;digital watermarking;computer science;theoretical computer science;error diffusion;computer graphics (images)	Vision	61.65201844625265	-67.34184111478324	135147
846367004ba6be7923e6456e5e933039cbbc379d	(φ,φ*) image decomposition models and minimization algorithms	oscillations;euler lagrange equation;additive noise;image restoration;functional minimization;bv duality;nonlinear evolution equation;total variation;image decomposition	We propose in this paper minimization algorithms for image restoration using dual functionals and dual norms. In order to extract a clean image u from a degraded version f=Ku+n (where f is the observation, K is a blurring operator and n represents additive noise), we impose a standard regularization penalty Φ(u)=∫ φ(|Du|)dx<∞ on u, where φ is positive, increasing and has at most linear growth at infinity. However, on the residual f−Ku we impose a dual penalty Φ*(f−Ku)<∞, instead of the more standard $\|f-\mathit{Ku}\|^{2}_{L^{2}}$ fidelity term. In particular, when φ is convex, homogeneous of degree one, and with linear growth (for instance the total variation of u), we recover the (BV,BV *) decomposition of the data f, as suggested by Y. Meyer (Oscillating Patterns in Image Processing and Nonlinear Evolution Equations, University Lecture Series, vol. 22, Am. Math. Soc., Providence, 2001). Practical minimization methods are presented, together with theoretical, experimental results and comparisons to illustrate the validity of the proposed models. Moreover, we also show that by a slight modification of the associated Euler-Lagrange equations, we obtain well-behaved approximations and improved results.	additive white gaussian noise;algorithm;approximation;calculus of variations;circuit restoration;computation;computational problem;consistency model;deblurring;euler;euler–lagrange equation;image analysis;image processing;image restoration;linear function;noise reduction;utility functions on indivisible goods	Triet M. Le;Linh H. Lieu;Luminita A. Vese	2008	Journal of Mathematical Imaging and Vision	10.1007/s10851-008-0130-1	image restoration;computer vision;mathematical optimization;mathematical analysis;topology;calculus;mathematics;geometry;oscillation;total variation;statistics	Vision	55.951673259514244	-71.06255872842549	135646
0694831add9cd8ddfb9bb7fbcc247f5d376a9652	alignment blur in coherently averaged images	alignement;filtering;filtrage;finite impulse response filter signal processing image restoration signal restoration costs covariance matrix eigenvalues and eigenfunctions sampling methods signal processing algorithms filtering;restauration image;image processing;video signal processing;filtrado;procesamiento imagen;filtering theory video signal processing image restoration image sequences noise iterative methods;image restoration;traitement image;reduccion ruido;human subjects;iterative methods;restauracion imagen;noisy video sequences coherently averaged images alignment blur image restoration methods iterative realignment post filtering blurr reduction;imagen borrosa;blurred image;noise reduction;alineamiento;reduction bruit;image floue;alignment;filtering theory;noise;image sequences	Blurring of coherently averaged images due to imperfect alignment is studied, and two restoration methods are proposed and evaluated. It is shown that iterative realignment is more powerful than post-filtering in reducing blur. The value of averaging and restoration is illustrated on human subjects in noisy video sequences.	gaussian blur	Donald M. Monro;D. M. Simpson	1996	IEEE Trans. Signal Processing	10.1109/78.506630	filter;image restoration;computer vision;speech recognition;image processing;computer science;noise;noise reduction;mathematics;iterative method	Vision	54.53569024104382	-66.97880279629746	136354
864449ac7d23d64861969dd7d2d3aa2c867e2f5f	image fusion by means of a trous discrete wavelet decomposition	discrete wavelet transforms;high resolution;image resolution;image fusion;low resolution;wavelet decomposition;image resolution sensor fusion discrete wavelet transforms;image fusion discrete wavelet transforms image resolution multispectral imaging partitioning algorithms remote sensing signal processing algorithms merging spatial resolution satellites;multispectral images;evolutionary strategy;sensor fusion;spatial information;merging algorithm image fusion high resolution panchromatic image low resolution multispectral image multiresolution wavelet decomposition evolutionary strategy wavelet planes	A new algorithm is developed to merge a high-resolution panchromatic image and a low-resolution multispectral image based on the combination of multiresolution wavelet decomposition, evolutionary strategy and the IHS transform. The high-resolution panchromatic image is firstly decomposed to the wavelet planes, then the regions are partitioned by evolutionary strategy in terms of difference of edge information from wavelet planes and the merging algorithm is done by adding edge influence factor in different region. The proposed method is compared with the IHS and the MWT methods. The results of the comparison show the proposed merger performing the best in combining and preserving spectral-spatial information for the test images.	image fusion;wavelet	Yan Wu;Ming Li;Guisheng Liao	2004		10.1109/ICARCV.2004.1469079	wavelet;computer vision;image resolution;second-generation wavelet transform;computer science;pattern recognition;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;image fusion;discrete wavelet transform;lifting scheme	Vision	59.1230256876508	-67.23992068866892	136514
19ef7715e96e18bae04614db249a81cd0524a98a	efficient ml estimation of the shape parameter for generalized gaussian mrfs	experimental results ml estimation shape parameter generalized gaussian mrf markov random fields performance bayesian image reconstruction bayesian image restoration ml estimate temperature closed form solution optimal estimation partition function tractable scheme off line numerical computation em algorithm expectation step fast simulation technique extrapolation convergence;fast simulation technique;partition function;tractable scheme;closed form solution;convergence;expectation step;maximum likelihood estimation shape image reconstruction image restoration computational modeling markov random fields bayesian methods temperature closed form solution partitioning algorithms;gaussian processes;bayes methods;convergence of numerical methods;performance;simulation image reconstruction image restoration gaussian processes markov processes random processes bayes methods maximum likelihood estimation convergence of numerical methods extrapolation digital simulation;optimal estimation;simulation;markov random fields;bayesian methods;extrapolation;image restoration;generalized gaussian mrf;maximum likelihood estimation;bayesian image reconstruction;markov random field;computational modeling;shape;ml estimate;simulation technique;image reconstruction;numerical computation;random processes;generalized gaussian;shape parameter;a priori information;markov processes;bayesian image restoration;temperature;em algorithm;experimental results;off line numerical computation;digital simulation;ml estimation;partitioning algorithms	A certain class of Markov Random Fields (MRF) known as generalized Gaussian MRFs (GGMRF) have been shown to yield good performance in modeling the a priori information in Bayesian image reconstruction and restoration problems. Though the ML estimate of temperature T of a GGMRF has a closed form solution, the optimal estimation of the shape parameter p is a di cult problem due to the intractable nature of the partition function. In this paper, we present a tractable scheme for ML estimation of p by an o -line numerical computation of the log of the partition function. In image reconstruction or restoration problems, the image itself is not known. To address this problem, we use the EM algorithm to compute the estimates directly from the data. For e cient computation of the expectation step, we propose a fast simulation technique and a method to extrapolate the estimates when the simulations are terminated prematurely prior to convergence. Experimental results show that the proposed methods result in substantial savings in computation and superior quality images.	circuit restoration;cobham's thesis;computation;expectation–maximization algorithm;extrapolation;gaussian blur;iterative reconstruction;markov chain;markov random field;numerical analysis;partition function (mathematics);simulation	Suhail S. Saquib;Charles A. Bouman;Ken D. Sauer	1996		10.1109/ICASSP.1996.545864	iterative reconstruction;optimal estimation;image restoration;stochastic process;closed-form expression;mathematical optimization;convergence;temperature;expectation–maximization algorithm;performance;bayesian probability;shape;machine learning;gaussian process;mathematics;maximum likelihood;markov process;extrapolation;partition function;shape parameter;computational model;statistics	ML	60.416230625084346	-72.6327851463211	136738
e459fdea5b3fc0333e400400cfa0fd1055921022	a projected gradient algorithm for image restoration under hessian matrix-norm regularization	mixed norm regularization linear inverse problems image restoration hessian matrix norms;minimisation;gaussian noise;convex programming;hessian matrix norm regularization nonquadratic hessian based regularizer total variation tv functional staircase effect tv based reconstruction projected gradient based algorithm minimization convex constraint image deblurring additive gaussian noise image restoration;image restoration;image reconstruction;gradient methods;tin abstracts tv;minimisation convex programming gaussian noise gradient methods hessian matrices image reconstruction image restoration;hessian matrices	We have recently introduced a class of non-quadratic Hessian-based regularizers as a higher-order extension of the total variation (TV) functional. These regularizers retain some of the most favorable properties of TV while they can effectively deal with the staircase effect that is commonly met in TV-based reconstructions. In this work we propose a novel gradient-based algorithm for the efficient minimization of these functionals under convex constraints. Furthermore, we validate the overall proposed regularization framework for the problem of image deblurring under additive Gaussian noise.	algorithm;circuit restoration;deblurring;gradient;hessian;image restoration;matrix regularization;utility functions on indivisible goods	Stamatios Lefkimmiatis;Michael Unser	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6467538	iterative reconstruction;gaussian noise;image restoration;computer vision;minimisation;mathematical optimization;mathematical analysis;convex optimization;computer science;mathematics;geometry;statistics	Vision	56.201663522281486	-72.69114302246139	136773
a8585349d4d70dca825ab413144d181bc421b9cc	a variational method for expanding the bit-depth of low contrast image	low contrast;bit depth expansion;spatial regularization;variational methods	Traditionally, bit-depth expansion is an image processing technique to display a low bit-depth image on a high bit-depth monitor. In this paper, we study a variational method for expanding the bit-depth of low contrast images. Our idea is to develop a variational approach containing an energy functional to determine a local mapping function fr,x for bit-depth expansion via a smoothing technique, such that each pixel can be adjusted locally to a high bit-depth value. In order to enhance low contrast images, we make use of the histogram equalization technique for such local mapping function. Both bit-depth expansion and equalization terms can be combined together into the resulting objective function. In order to minimize the differences among the local mapping function at the nearby pixel locations, the spatial regularization of the mapping is incorporated in the objective function. Experimental results are reported to show that the performance of the proposed method is competitive with the other compared methods for several testing low contrast images.	calculus of variations;variational method (quantum mechanics)	Motong Qiao;Wei Wang;Michael K. Ng	2013		10.1007/978-3-642-40395-8_5	computer vision;mathematical optimization;mathematics;geometry;adaptive histogram equalization	Vision	56.94156055065496	-70.65885404738044	136917
d73c848112e771cf3a16720d728cf02d311e3b90	image enhancement using diffusion equations	numerical solution;edge detection;image enhancement equations filters noise reduction image denoising smoothing methods pixel image analysis performance analysis algorithm design and analysis;image enhancement;smoothing methods;image edge detection;partial differential equations;noise reduction;pixel;diffusion equation;diffusion equations;image denoising;edge preservation;partial differential equations edge detection image denoising image enhancement;edge preservation image enhancement diffusion equations image denoising;noise	In this paper, a new algorithm is proposed for reducing noise from images using diffusion equations. The proposed algorithm is based on the fact that the intensity of adjacent pixels in a real image may vary gradually. Thus, in numerical solution of diffusion equations, for preserving edges, neighbors longer than one pixel must be considered. In the presented method, in addition to vertical and horizontal edges, slant edges are also considered to better preserve edges. This algorithm has been examined using a variety of standard images, and its performance has been compared with the existing algorithms. Experimental results show that, in comparison with other existing methods, the proposed algorithm has a better performance in denoising and preserving image edges.	algorithm;image editing;noise reduction;numerical partial differential equations;pixel	Hamid Hassanpour;Ehsan Nadernejad;H. Miar	2007	2007 9th International Symposium on Signal Processing and Its Applications	10.1109/ISSPA.2007.4555608	computer vision;diffusion equation;mathematical optimization;edge detection;computer science;noise;noise reduction;mathematics;geometry;partial differential equation;pixel	Vision	55.699334425028326	-69.00407882039512	137004
4b7487677ffefaeeda8a4f8474893e079a9cc65f	design of new surface detection operators in the case of an ansiotropic sampling of 3d volume data	medical image;signal processing;volume data	Medical imaging systems do not provide direct volume information. These data are generally a result of stacking-up 2D slices, with a resolution in the plane of the slice higher than the one perpendicular to the slice. The volume data is anisotropic. The objective of this paper is to provide an original way to design surface detection operators taking into account the anisotropic aspect. Two approaches are studied and illustrated by examples: signal processing and elaboration of continuous anisotropic model.	gibbs sampling	Chafiaâ Hamitouche-Djabou;Christian Roux;Jean-Louis Coatrieux	1995		10.1007/978-3-540-49197-2_69	computer vision;computer science;theoretical computer science;signal processing;digital image processing;data mining	Vision	55.09826589737704	-78.73805640316499	137227
3f85420c2a8417dae6a0a32bb1e924da04f86a74	restoration of atmospheric turbulence-distorted images via rpca and quasiconformal maps.		We address the problem of restoring a high-quality image from an observed image sequence strongly distorted by atmospheric turbulence. A novel algorithm is proposed in this paper to reduce geometric distortion as well as space-and-time-varying blur due to strong turbulence. By considering a suitable energy functional, our algorithm first obtains a sharp reference image and a subsampled image sequence containing sharp and mildly distorted image frames with respect to the reference image. The subsampled image sequence is then stabilized by applying the Robust Principal Component Analysis (RPCA) on the deformation fields between image frames and warping the image frames by a quasiconformal map associated with the low-rank part of the deformation matrix. After image frames are registered to the reference image, the low-rank part of them are deblurred via a blind deconvolution, and the deblurred frames are then fused with the enhanced sparse part. Experiments have been carried out on both synthetic and real turbulence-distorted video. Results demonstrate that our method is effective in alleviating distortions and blur, restoring image details and enhancing visual quality.	algorithm;blind deconvolution;circuit restoration;distortion;gaussian blur;robust principal component analysis;sparse matrix;synthetic intelligence;turbulence	Chun Pong Lau;Yu Hin Lai;Lok Ming Lui	2017	CoRR		computer vision;mathematical analysis;deformation (mechanics);distortion;matrix (mathematics);blind deconvolution;mathematics;image warping;robust principal component analysis;energy functional;artificial intelligence;turbulence	Vision	56.74070599733952	-70.44073151826507	137348
637cf5540c0fb1492d94292bf965b2c404e42fb4	restoring highly corrupted images by impulse noise using radial basis functions interpolation		Preserving details in restoring images highly corrupted by impulse noise remains a challenging problem. We proposed an algorithm based on radial basis functions (RBF) interpolation which estimates the intensities of corrupted pixels by their neighbors. In this algorithm, first intensity values of noisy pixels in the corrupted image are estimated using RBFs. Next, the image is smoothed. The proposed algorithm can effectively remove the highly dense impulse noise. Experimental results show the superiority of the proposed algorithm in comparison to the recent similar methods both in noise suppression and detail preservation. Extensive simulations show better results in measure of peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM), especially when the image is corrupted by very highly dense impulse noise.	algorithm;impulse noise (audio);interpolation;peak signal-to-noise ratio;pixel;radial (radio);radial basis function;simulation;smoothing;structural similarity;zero suppression	Fariborz Taherkhani;Mansour Jamzad	2018	IET Image Processing	10.1049/iet-ipr.2016.0521	computer vision;mathematical optimization;mathematics;statistics	Vision	58.2951754378326	-66.70982627652089	137689
2bd1407c52bc4a7c0861e1b8bd818e81814f55ee	consequences of pixel precision for compression rate and measurement		An increase in image pixel precision makes performance drop in many compression methods. Many entropy reducing transforms are homogeneous and this homogeneity is not influenced if this transform is followed by a uniform quantization step. The entropy in the resulting data after these steps will therefore be additive as a function of the pixel precision. The differences in compressibility dependent on increasing pixel precision or entropy values of original image data would be expressed more explicitly by using proposed new methods of measuring compression gain.	12-bit;decorrelation;experiment;pixel;quantization (signal processing);utility functions on indivisible goods	Ingvil Hovig;Simen Gaure	1997	Sig. Proc.: Image Comm.	10.1016/S0923-5965(96)00015-X	computer vision;mathematics;statistics	Vision	62.87781753084452	-67.18898303351483	138406
d51e5c1335a58c5bdab52e17435adbbdaa140773	spatially-adaptive regularized super-resolution image reconstruction using a gradient-based saliency measure	adaptive image processing gradient based saliency measure image reconstruction image super resolution degree of regulrisation spatially invariant regularization gradient based assessment criterion;image resolution;low resolution;image resolution gradient methods image reconstruction;strontium;image reconstruction;gradient methods;signal resolution;image reconstruction spatial resolution signal resolution strontium educational institutions;super resolution;spatial resolution	This paper addresses the super-resolution image reconstruction problem with the aim to produce a higher-resolution image based on its low-resolution counterparts. The proposed approach adaptively adjusts the degree of regularization using the saliency measure of the local content of the image. This is in contrast to that a spatially-invariant regularization is used for the whole image in conventional approaches. Furthermore, a gradient-based assessment criterion is proposed to measure the saliency of the image. Experiments are conducted to demonstrate the superior performance of the proposed approach.	adaptive filter;experiment;gradient;iterative reconstruction;reconstruction conjecture;super-resolution imaging	Zhenyu Liu;Jing Tian;Li Chen;Yongtao Wang	2011	The First Asian Conference on Pattern Recognition	10.1109/ACPR.2011.6166567	image texture;image restoration;computer vision;mathematical optimization;feature detection;image gradient;pattern recognition;mathematics;sub-pixel resolution	Vision	58.62790738767601	-66.29222878584338	139228
30195de44e18b6a2b2bea173b9e09f1aad11de56	an ℓ1 minimization algorithm for non-smooth regularization in image processing	fixed point;conjugate gradient;inpainting;image separation;denoising;deblurring	In this work, we consider a homotopic principle for solving large-scale 1 minimization problems arising in non-smooth regularization for image processing. Our approach consists in solving a sequence of relaxed unconstrained minimization problems depending on a positive regularization parameter that converges to zero. The optimality conditions of each sub-problem are characterized through a fixed point equation, where a preconditioned conjugate gradient algorithm is applied to solve a sequence of resulting linear systems. A convergence analysis is presented showing the viability of our approach from a theoretical point of view. Moreover, several numerical experiments are conducted showing that the proposed algorithm performs comparable as some state of the art solvers. In particular, we present numerical evidence that our algorithm is very competitive when recovering high dynamic range signals. In addition, we present a set of image processing applications including image denoising, image deblurring, image separation, image inpainting, and total variation inversion, exhibiting good numerical properties.	algorithm;conjugate gradient method;deblurring;experiment;fixed point (mathematics);high dynamic range;image processing;inpainting;linear system;matrix regularization;noise reduction;numerical analysis	Carlos Ramírez;Miguel Argáez	2015	Signal, Image and Video Processing	10.1007/s11760-013-0454-1	computer vision;mathematical optimization;mathematical analysis;computer science;noise reduction;mathematics;geometry;fixed point;conjugate gradient method;inpainting	ML	56.23979691696679	-72.0383840579328	139968
34539044699c1a3a68df2711015e7dc66f32f1c0	group sparsity residual constraint for image denoising with external nonlocal self-similarity prior		Nonlocal image representation has been successfully used in many image-related inverse problems including denoising, deblurring and deblocking. However, most existing methods only consider the nonlocal self-similarity (NSS) prior of degraded observation image, and few methods use the NSS prior from natural images. In this paper we propose a novel method for image denoising via group sparsity residual constraint with external NSS prior (GSRC-ENSS). Different from the previous NSS prior-based denoising methods, two kinds of NSS prior (e.g., NSS priors of noisy image and natural images) are used for image denoising. In particular, to enhance the performance of image denoising, the group sparsity residual is proposed, and thus the problem of image denoising is translated into reducing the group sparsity residual. Because the groups contain a large amount of NSS information of natural images, to reduce the group sparsity residual, we obtain a good estimation of the group sparse coefficients of the original image by the external NSS prior based on Gaussian Mixture Model (GMM) learning, and the group sparse coefficients of noisy image are used to approximate the estimation. To combine these two NSS priors better, an effective iterative shrinkage algorithm is developed to solve the proposed GSRC-ENSS model. Experimental results demonstrate that the proposed GSRC-ENSS not I* Corresponding Author. E-mail: wajj131420@hotmail.com. Preprint submitted to Neurocomputing November 9, 2017	aharonov–bohm effect;approximation algorithm;coefficient;deblocking filter;deblurring;google map maker;iterative method;mixture model;neurocomputing;noise reduction;nonlocal lagrangian;self-similarity;sparse matrix	Zhiyuan Zha;Xinggan Zhang;Qiong Wang;Yechao Bai;Yang Chen;Lan Tang;Xin Liu	2018	Neurocomputing	10.1016/j.neucom.2017.11.004	machine learning;deblocking filter;residual;mixture model;non-local means;prior probability;artificial intelligence;ringing artifacts;mathematical optimization;mathematics;pattern recognition;deblurring;inverse problem	Vision	57.118947250947265	-70.37764513486557	140079
2e666a0ffb7b5b2b9a416607001e48d394ce1aea	a data-consistent linear prediction method for image reconstruction from finite fourier samples	data consistency;image reconstruction	Linear prediction (LP) methods have been widely used for high-resolution spectral estimation from finite Fourier samples. Their application to image reconstruction, on the other hand, has been markedly less successful. In this article, we present an improved LP method for high-resolution image reconstruction. The distinguishing feature of the proposed method is its use of a generalized series model to enforce the data consistency constraint to compensate for reconstruction error resulting from LP modeling. Several reconstruction examples from magnetic resonance imaging data are included to demonstrate the performance of the method. © 1996 John Wiley u0026 Sons, Inc.	iterative reconstruction	Christopher P. Hess;Zhi-Pei Liang	1996	Int. J. Imaging Systems and Technology	10.1002/(SICI)1098-1098(199622)7:2%3C136::AID-IMA10%3E3.0.CO;2-%23	iterative reconstruction;econometrics;mathematical optimization;computer science;data consistency;statistics	Vision	58.72126392298893	-69.12649276432143	140164
d9e1e905283c7fd35cf28f15e9e14292e1134932	regularized selection: a new paradigm for inverse based regularized image reconstruction techniques		In this paper, we present a new regularization paradigm for inverse based regularized image reconstruction techniques. These methods usually attempt to minimize a cost function expressed as the sum of a data-fitting term and a regularization term. The trade-off between both terms is determined by a weighting parameter that has to be set by the user since this trade-off is data dependent. In the approach we present here, we first concentrate on finding a set of eligible candidates for the data fitting term minimization and then select the most appropriate candidate according to the regularization criterion. The main advantage of this method is that it does not require any weighting parameter, and guarantees that no over-regularization can occur. We illustrate this method with a super-resolution reconstruction technique to show its efficiency compared to other competitive methods. Comparisons are carried out with simulated and real data.	curve fitting;iterative reconstruction;loss function;matrix regularization;programming paradigm;super smash bros.;super-resolution imaging	Florentin Kucharczak;Cyril Mory;Olivier Strauss;Frédéric Comby;Denis Mariano-Goulart	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296559	kernel (linear algebra);interpolation;iterative reconstruction;computer science;artificial intelligence;pattern recognition;curve fitting;image resolution;regularization (mathematics);inverse problem;weighting	Vision	56.81079623039046	-73.29397418073724	140589
71bebdf9985eb67ba572af9cb01c59edbcea9221	denoising by spatial correlation thresholding	transformation ondelette;correlation methods image denoising wavelet transforms;traitement signal;evaluation performance;performance evaluation;image processing;threshold detection;ondelette;evaluacion prestacion;signal analysis;image analysis denoising spatial correlation thresholding wavelet transform noise reduction edge structures wavelet scales spatial correlation function wavelet coefficients image denoising;procesamiento imagen;threshold;analisis de senal;threshold scheme;correlation methods;indexing terms;traitement image;reduccion ruido;wavelet transforms;detection seuil;deteccion umbral;correlation spatiale;wavelet transform;spatial correlation;correlacion espacial;signal processing;noise reduction;fonction correlation;correlation function;reduction bruit;funcion correlacion;image denoising;transformacion ondita;noise reduction wavelet transforms wavelet coefficients discrete wavelet transforms minimax techniques noise robustness image edge detection additive noise wavelet domain filtering;procesamiento senal;wavelets;analyse signal;wavelet transformation	This paper presents a spatial-correlation thresholding scheme for noise reduction by wavelet transform. Observing that edge structures are of high magnitude across wavelet scales but noise decays rapidly, we multiply two adjacent wavelet scales to form a spatial-correlation function to enhance significant structures and dilute noise. Dissimilar to the traditional thresholding schemes that apply threshold to the wavelet coefficients, the proposed scheme applies threshold directly to the scale correlation. A robust threshold is presented and experiments show that the proposed scheme outperforms the traditional thresholding methods.	coefficient;experiment;noise reduction;thresholding (image processing);wavelet transform	Lei Zhang;Paul Bao	2003	IEEE Trans. Circuits Syst. Video Techn.	10.1109/TCSVT.2003.813426	computer vision;second-generation wavelet transform;computer science;signal processing;balanced histogram thresholding;mathematics;wavelet packet decomposition;stationary wavelet transform;lifting scheme;statistics;wavelet transform	Vision	54.23952587140388	-66.1961056744754	140721
ffe442923da857699b85056209f47c03a0de7896	anisotropic total variation method for text image super-resolution	handwriting recognition;image resolution;white spaces;anisotropic total variation super resolution text image super resolution total variation;higher order;text image super resolution;anisotropic magnetoresistance image resolution shape writing cameras handwriting recognition brushes white spaces image reconstruction face detection;anisotropic total variation;shape;image reconstruction;anisotropic magnetoresistance;writing;super resolution;total variation;face detection;cameras;brushes	This paper presents a text image super resolution algorithm based on total variation (TV). Text images typically consist of slim strokes on background. Thus, there are three different local characteristics as homogeneous, directed and complex on text image. Homogeneous region corresponds to background and directed means the region with dominant stroke direction and remaining is complex region. We proposed higher order smoothing on homogeneous region and anisotropic regularization on directed region which encodes the preference of edge direction by smoothing along preferred direction only. Required regularization terms are combined in proposed anisotropic TV functional and controlled by relating parameters. We calculated relating parameters byutilizing structure tensor field. Also to reduce the computational cost, we previously estimated nonchanging pixels and exclude them from calculation for speed up. Experiments shown that, proposed method performs better with low computational cost than general purpose TV on text image.	ascii art;algorithm;algorithmic efficiency;anisotropic filtering;computation;computational complexity theory;matrix regularization;pixel;smoothing;structure tensor;super-resolution imaging	Battulga Bayarsaikhan;Younghee Kwon;Jin Hyung Kim	2008	2008 The Eighth IAPR International Workshop on Document Analysis Systems	10.1109/DAS.2008.62	iterative reconstruction;magnetoresistance;computer vision;face detection;higher-order logic;image resolution;shape;computer science;white spaces;handwriting recognition;writing;total variation;superresolution;computer graphics (images)	Vision	53.79539812699326	-69.03331741496835	140757
cd670bdbfab9afe85fe48659ac7f7e704aaa2c4f	application of evolutionary programming to adaptive regularization in image restoration	optimisation evolutionary computation image restoration;texturation;extraction information;medida informacion;filtrage inverse;textured regions;correlacion;metodo adaptativo;filtering;optimisation;degradation;regularisation;evolutionary computation;local correlational properties;restauration image;programmation;image processing;optimizacion;cost function;information extraction;methode mesure;helium;potential regularization strategies;mesure information;evolutionary programming;procesamiento imagen;metodo medida;image restoration;methode adaptative;genetic programming;indexing terms;best subjective quality;traitement image;computer applications;regularization;most relevant function evolutionary programming adaptive regularization inverse filtering operation smooth regions blurring textured regions best subjective quality potential regularization strategies local correlational properties;programacion;restauracion imagen;smooth regions;blurring;adaptive filters;filtrado inverso;estimation erreur;genetic programming image restoration filtering image processing cost function degradation adaptive filters character generation computer applications;information measure;error estimation;reverse filtering;character generation;most relevant function;adaptive method;estimacion error;inverse filtering operation;texturacion;algorithme evolutionniste;adaptive regularization;algoritmo evolucionista;optimization;regularizacion;evolutionary algorithm;correlation;measurement method;programming;large classes;evolutionary computing;programmation evolutive;extraction informacion	Image restoration is a difficult problem due to the ill-conditioned nature of the associated inverse filtering operation, which requires regularization techniques. The choice of the correspondingregularization parameteris thus an important issue since an incorrect choice would either lead to noisy appearances in the smooth regions or excessive blurring of the textured regions. In addition, this choice has to be made adaptively across different image regions to ensure the best subjective quality for the restored image. In this paper, we employ evolutionary programming (EP) to solve this adaptive regularization problem by generating a population of potential regularization strategies, and allowing them to compete under a new error measure which characterizes a large class of images in terms of their local correlational properties. The nonavailability of explicit gradient information for this measure motivates the adoption of EP techniques for its optimization, which allows efficient search at multiple error surface points. The adoption of EP also allows the broadening of the range of possible cost functions for image processing so that we can choose the most relevant function rather than the most tractable one for a particular image processing application.	circuit restoration;cobham's thesis;condition number;evolutionary programming;expectation propagation;gradient descent;image processing;image restoration;inverse filter;mathematical optimization;matrix regularization	Hau-San Wong;Ling Guan	2000	IEEE Trans. Evolutionary Computation	10.1109/4235.887232	filter;evolutionary programming;adaptive filter;regularization perspectives on support vector machines;genetic programming;image restoration;regularization;computer vision;programming;mathematical optimization;degradation;index term;computer science;artificial intelligence;machine learning;mathematics;computer applications;helium;correlation;evolutionary computation	Vision	62.21676890239205	-68.34817617652187	140783
6148e34129e3272a899bddc2399a2e5db300e2a1	3-d nonlocal means filter with noise estimation for hyperspectral imagery denoising	hyperspectral imagery;geophysical image processing;gaussian noise;image denoising filtering theory gaussian noise geophysical image processing;会议论文;signal dependent noise;signal dependent noise hyperspectral imagery noise reduction 3 d nonlocal means;noise reduction;noise noise reduction silicon estimation hyperspectral imaging correlation;image denoising;3 d nonlocal means;filtering theory;detail preservation 3d nonlocal means filter noise estimation hyperspectral imagery denoising noise reduction hsi three dimensional nonlocal means filter global integrated property local integrated property nonlocal algorithm image patch search local pixels local filter global filter spectral spatial correlation signal independent noise signal dependent noise variance stabilizing transformation additive gaussian noise model noise removal	Noise reduction is one of important processing tasks for hyperspectral imagery (HSI). In this paper, a three-dimensional (3-D) nonlocal means filter is proposed for noise reduction of HSI. Recently, non-local means method attracts many attentions due to its global and local integrated property. Nonlocal algorithm searches the similar image patches in the whole scene to build the mean filter, so that it overcomes the disadvantage of local filter that only local pixels within a small neighbor is used, and the disadvantage of global filter that local structure is ignored. In order to explore the spectral-spatial correlation of HSI, nonlocal means method is extended from 2-D to 3-D. Furthermore, as HSI contains both of signal-independent and signal-dependent noises, variance-stabilizing transformation based on noise estimation is used to make noise reduction under the additive Gaussian noise model. Experiments with the real hyperspectral data set indicate that the proposed strategy can work well in both of detail preservation and noise removal.	3d computer graphics;aharonov–bohm effect;algorithm;bias–variance tradeoff;horizontal situation indicator;noise reduction;noise shaping;non-local means;nonlocal lagrangian;pixel;utility functions on indivisible goods	Yuntao Qian;Yanhao Shen;Minchao Ye;Qi Wang	2012	2012 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2012.6351287	gradient noise;gaussian noise;median filter;image noise;computer vision;dark-frame subtraction;value noise;computer science;noise measurement;hyperspectral imaging;machine learning;noise;pattern recognition;noise reduction;mathematics;non-local means;salt-and-pepper noise	Robotics	57.876438763166156	-66.28547203984438	140951
7f0af2a0c9e0f413410820d4766db2ded47c8282	edge-avoiding wavelets and their applications	image filtering;translation invariant;second generation wavelet;constraint propagation;laplace equation;multi resolution analysis;edge preserving filtering;lifting scheme;data dependent interpolation;data dependence;dynamic range;multi resolution;system of equations;wavelets	We propose a new family of second-generation wavelets constructed using a robust data-prediction lifting scheme. The support of these new wavelets is constructed based on the edge content of the image and avoids having pixels from both sides of an edge. Multi-resolution analysis, based on these new  edge-avoiding wavelets , shows a better decorrelation of the data compared to common linear translation-invariant multi-resolution analyses. The reduced inter-scale correlation allows us to avoid halo artifacts in band-independent multi-scale processing without taking any special precautions. We thus achieve nonlinear data-dependent multi-scale edge-preserving image filtering and processing at computation times which are  linear  in the number of image pixels. The new wavelets encode, in their shape, the smoothness information of the image at every scale. We use this to derive a new edge-aware interpolation scheme that achieves results, previously computed by solving an inhomogeneous Laplace equation, through an  explicit  computation. We thus avoid the difficulties in solving large and poorly-conditioned systems of equations.   We demonstrate the effectiveness of the new wavelet basis for various computational photography applications such as multi-scale dynamic-range compression, edge-preserving smoothing and detail enhancement, and image colorization.	wavelet	Raanan Fattal	2009	ACM Trans. Graph.	10.1145/1531326.1531328	system of linear equations;wavelet;computer vision;mathematical optimization;dynamic range;discrete mathematics;computer science;mathematics;lifting scheme;gabor wavelet;local consistency;laplace's equation	Graphics	55.468579862477775	-68.98226237454718	141106
8df30f35dd3f878fc909d985bd5bf000e62abc78	regularized split gradient method for nonnegative matrix factorization	nmf;matrix decomposition gradient methods;minimization;sgm;convolution;gradient method;regularization sgm nmf;regularization;positivity constraint regularized split gradient method nonnegative matrix factorization multiplicative algorithms sgm based algorithm frobenius norm hyperspectral data unmixing;matrix decomposition;nonnegative matrix factorization;hyperspectral data;mathematical model;gradient methods;hyperspectral imaging matrix decomposition gradient methods minimization equations convolution mathematical model;hyperspectral imaging;data consistency;hyperspectral image	This article deals with a regularized version of the split gradient method (SGM), leading to multiplicative algorithms. The proposed algorithm is available for the optimization of any divergence depending on two data fields under positivity constraint. The SGM-based algorithm is derived to solve the nonnegative matrix factorization (NMF) problem. An example with a Frobenius norm on both the data consistency and the penalty term is developped and applied to hyperspectral data unmixing.	algorithm;gradient method;mathematical optimization;matrix multiplication;non-negative matrix factorization;second generation multiplex	Henri Lantéri;Céline Theys;Cédric Richard;David Mary	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5946608	mathematical optimization;mathematical analysis;discrete mathematics;computer science;machine learning;mathematics;non-negative matrix factorization	Robotics	56.44363859143732	-72.71970852594183	142176
bb83ad80a85def72365875f496073b1a5151e364	super-resolution reconstruction of compressed video using transform-domain statistics	algorithms computer simulation data compression image enhancement image interpretation computer assisted information storage and retrieval models statistical pattern recognition automated reproducibility of results sensitivity and specificity signal processing computer assisted subtraction technique video recording;map;high resolution;image processing;image resolution;data compression;stochastic processes data compression video coding image reconstruction discrete cosine transforms image resolution;stochastic method;transformation cosinus discrete;additive noise;low resolution;ruido aditivo;procesamiento imagen;projection onto convex sets video compression transform domain statistics dct domain reconstruction map super resolution image reconstruction;bruit additif;estimation a posteriori;indexing terms;traitement image;a posteriori estimation;video coding;reconstruction image;compression image;senal video;signal video;image compression;stochastic processes;estimacion a posteriori;reconstruccion imagen;error cuantificuacion;discrete cosine transforms;image reconstruction;video signal;methode stochastique;super resolution;video compression statistics signal resolution image coding image reconstruction spatial resolution image resolution quantization hdtv additive noise;superresolution;dct domain reconstruction;multiframe image reconstruction;erreur quantification;pocs;superresolucion;compressed video;quantization error;compresion imagen;metodo estocastico	Considerable attention has been directed to the problem of producing high-resolution video and still images from multiple low-resolution images. This multiframe reconstruction, also known as super-resolution reconstruction, is beginning to be applied to compressed video. Super-resolution techniques that have been designed for raw (i.e., uncompressed) video may not be effective when applied to compressed video because they do not incorporate the compression process into their models. The compression process introduces quantization error, which is the dominant source of error in some cases. In this paper, we propose a stochastic framework where quantization information as well as other statistical information about additive noise and image prior can be utilized effectively.	additive white gaussian noise;algorithm;analysis of algorithms;assumed;coder device component;computational complexity theory;dct protein, human;data compression;discrete cosine transform;entity;estimated;frame (physical object);image noise;image resolution;information theory;matthews correlation coefficient;motion compensation;motion estimation;normal statistical distribution;parallel computing;quantization (signal processing);self-information;solutions;statistical model;stochastic process;super-resolution imaging;transform coding;uncompressed video;utility functions on indivisible goods;video coding format;standards characteristics	Bahadir K. Gunturk;Yücel Altunbasak;Russell M. Mersereau	2004	IEEE Transactions on Image Processing	10.1109/TIP.2003.819221	video compression picture types;computer vision;speech recognition;image resolution;image processing;computer science;video tracking;motion compensation;video denoising;computer graphics (images)	Vision	53.990231399320805	-67.7541864838877	142871
9aa3431213eade61b928b15431a5c743fce1153c	using of em algorithm to image reconstruction problem with tomography noises		This paper describes an analytical iterative approach to the problem of image reconstruction from parallel projections using Expectation Minimization algorithm. The experiments with noisy measurements have shown that EM algorithm can deblur the reconstructed image. The achieved results confirm that designed reconstruction procedure is able to reconstruct an image with better quality than image obtained using the traditional back-projection algorithm.	expectation–maximization algorithm;iterative reconstruction;reconstruction conjecture;tomography	Piotr Dobosz;Robert Cierniak	2013		10.1007/978-3-319-01622-1_5	computer vision;mathematical optimization	ML	56.61990229640897	-74.39894711408992	142884
3a2c6e5fee85988c60c06919ecb328e68ab0968e	joint deblurring and demosaicing of poissonian bayer-data based on local adaptivity	wiener filters adaptive filters bayes methods deconvolution image filtering image segmentation polynomial approximation smoothing methods stochastic processes;image color analysis kernel noise interpolation convolution noise measurement joints;deconvolution joint deblurring and demosaicing poissonian bayer data regularized inverse with adaptive filtering wiener inverse with adaptive filtering local polynomial approximation cross color lpa intersection of confidence interval smoothing filter derivative kernels design ici rule data adaptive selection cross color directional filter design	We present a novel technique for joint deblurring and demosaicing of noisy Poissonian Bayer data (e.g., data acquired by a digital CMOS or CCD imaging sensor). The technique incorporates the regularized inverse and the Wiener inverse with adaptive filtering based on the concept of cross-color local polynomial approximation (LPA) and intersection of confidence intervals (ICI). The directional filters designed by LPA utilize simultaneously the green, red, and blue color components. This is achieved by a linear combination of complementary-supported smoothing and derivative kernels designed for the Bayer data grid. The ICI rule is used for data-adaptive selection of the length of the designed cross-color directional filter. Simulation experiments demonstrate the efficiency of the proposed technique with respect to the conventional approach where deconvolution and demosaicing are computed independently.	adaptive filter;approximation;cmos;charge-coupled device;deblurring;deconvolution;demosaicing;digital camera;experiment;ici (programming language);image sensor;numerical analysis;polynomial;simulation;smoothing;wiener filter	Dmytro Paliy;Alessandro Foi;Radu Ciprian Bilcu;Vladimir Katkovnik;Karen O. Egiazarian	2008	2008 16th European Signal Processing Conference		demosaicing;computer vision;mathematical optimization;mathematics;statistics	Robotics	59.08131948949678	-71.01280389883378	142951
cd61d184b71b632a477ceb63a050d047fb98906d	on a family of nonlinear cell-average multiresolution schemes for image processing: an experimental study	cell average multiresolution;p;denoising;compression;nonlinear subdivision schemes;p power means	This paper is devoted to a new family of nonlinear cell-average multiresolution schemes and its applications to image processing. The algorithms are based on nonlinear reconstruction operators with several desirable features: the reconstructions are thirdorder accurate in smooth regions, the data used is always centered with optimal support and they are adapted to the presence of discontinuities. The goal is to obtain similar properties as linear multiresolution schemes but avoiding the classical Gibbs phenomenon of this type of reconstructions. Applications to image compression and denoising will be presented. c ⃝ 2015 International Association for Mathematics and Computers in Simulation (IMACS). Published by Elsevier B.V. All rights reserved.	algorithm;coefficient;experiment;gaussian blur;image compression;image processing;interpolation;multiresolution analysis;noise reduction;nonlinear system;numerical analysis;peak signal-to-noise ratio;simulation	Sergio Amat;Jacques Liandrat;Juan Ruiz;Juan Carlos Trillo	2015	Mathematics and Computers in Simulation	10.1016/j.matcom.2015.01.002	p;computer vision;mathematical optimization;discrete mathematics;noise reduction;mathematics;compression	EDA	56.21324425700523	-68.66671802577184	142980
5aaf4fb664ca86c2f050dd23a530ca6fc271544b	sparse-view statistical iterative head ct image reconstruction via joint regularization	ct;total generalized variation;regularization;sparse view;statistical iterative reconstruction	"""It is a significant challenge to accurately reconstruct medical computed tomography CT images with important details and features. Reconstructed images always suffer from noise and artifact pollution because the acquired projection data may be insufficient or undersampled. In reality, some """"isolated noise points"""" similar to impulse noise always exist in low-dose CT projection measurements. Statistical iterative reconstruction SIR methods have shown greater potential to significantly reduce quantum noise but still maintain the image quality of reconstructions than the conventional filtered back-projection FBP reconstruction algorithm. Although the typical total variation-based SIR algorithms can obtain reconstructed images of relatively good quality, noticeable patchy artifacts are still unavoidable. To address such problems as impulse-noise pollution and patchy-artifact pollution, this work, for the first time, proposes a joint regularization constrained SIR algorithm for sparse-view CT image reconstruction, named """"SIR-JR"""" for simplicity. The new joint regularization consists of two components: total generalized variation, which could process images with many directional features and yield high-order smoothness, and the neighborhood median prior, which is a powerful filtering tool for impulse noise. Subsequently, a new alternating iterative algorithm is utilized to solve the objective function. Experiments on different head phantoms show that the obtained reconstruction images are of superior quality and that the presented method is feasible and effective."""	ct scan;iterative method;iterative reconstruction;matrix regularization;sparse	Hong Shangguan;Yi Liu;Xueying Cui;Yunjiao Bai;Quan Zhang;Zhiguo Gui	2016	Int. J. Imaging Systems and Technology	10.1002/ima.22151	iterative reconstruction;regularization;computer vision;mathematical optimization;haplogroup ct;computer science;machine learning;mathematics	Vision	57.01580048464712	-71.71711104734354	143128
4f2efdfd10ee4ad23f0743bd265c4c950b3e8324	pixel recovery via el minimization in the wavelet domain	linear program;wavelet transform	This paper uses probability models on expansive wavelet transform coefcients with interpolation constraints to esti- mate missing blocks in images. We use simple probability models on wavelet coefcients to formulate the estimation process as a linear programming problem and solve it to re- cover the missing pixels. Our formulation is general and can be augmented with more sophisticated probability models to obtain even better estimates on a variety of image regions. The presented approach has many parallels to recently intro- duced dictionary based signal representations with which it shares certain optimality properties. We provide simulation examples over edge regions using both critically-sampled and expansive (over-complete) wavelet transforms.	pixel;wavelet	Ivan W. Selesnick;Richard M. Van Slyke;Onur G. Guleryuz	2004				Vision	56.62994321145165	-73.10586102573573	143761
6ad4daeea6ba21aa597d3a8d7f6851346b32cf2a	image estimation in film-grain noise	bayesian framework;nonlinear filters particle filters degradation recursive estimation bayesian methods monte carlo methods yield estimation silver image sensors;gaussian noise;belief networks;photographic emulsions;sensor nonlinearity image estimation film grain noise particle filter image recovery multiplicative nongaussian noise autoregressive process recursive bayesian framework;optimal estimation;auto regressive;recursive filters;sensor nonlinearity auto regressive process film grain noise image estimation multiplicative noise non gaussian noise particle filter recursive bayesian framework;multiplicative noise;autoregressive processes;particle filter;image denoising;belief networks image denoising photographic emulsions autoregressive processes recursive filters	A method based on the particle filter for recovering images degraded by film-grain noise is proposed. Due to the nonlinear relationship between the silver density and exposure, film-grain noise manifests itself as multiplicative non-Gaussian noise in the exposure domain. Since the posterior density is non-Gaussian, the proposed method works by representing it by a set of samples with associated weights. These samples are propagated in a recursive framework to obtain an optimal estimate of the original image. The effectiveness of the method is demonstrated with examples.	nonlinear system;particle filter;portable document format;recursion;recursive filter	S. Ibrahim Sadhar;A. N. Rajagopalan	2005	IEEE Signal Processing Letters	10.1109/LSP.2004.840850	gradient noise;optimal estimation;gaussian noise;median filter;econometrics;mathematical optimization;recursive bayesian estimation;particle filter;value noise;computer science;multiplicative noise;mathematics;autoregressive model;statistics	Vision	59.60706617384682	-70.45109110462812	143807
bc4a3128f35974aafcf24913c9b0e9b11a9b50e9	a novel method for 4d cone-beam computer-tomography reconstruction	image quality;computing systems;tomography	Image quality of Four Dimensional Cone-Beam Computer-Tomography (4DCBCT) is severely impaired by highly insufficient amount of projection data available for each phase. Therefore, making good use of limited projection data is crucial to solve this problem. Noticing that usually only a portion of the images is affected by motion, we separate the moving part (different between phases) of the images from the static part (identical among all phases) with the help of prior image reconstructed using all projection data. Then we update the moving part and the static part of images alternatively through solving minimization problems based on a global (use full projection data) and several local (use projection data for respective phase) linear systems. In the other word, we rebuild a large over-determined linear system for static part from the original under-determined systems and we reduce the number of unknowns in the original system for each phase as well. As a result, image quality for both static part and moving part are greatly improved and reliable 4D CBCT images are then reconstructed.	4d film;cone beam computed tomography;cone tracing;image quality;linear system	Hao Zhang;Justin C. Park;Yunmei Chen;Guanghui Lan;Bo Lu	2015		10.1117/12.2082128	image quality;computer vision;mathematical optimization;oblique projection;computer science;theoretical computer science;tomography	Graphics	56.36672967444898	-75.28022999516838	143867
b55e2bf0cb85463db812f26f2d028101e50854ce	sar image despeckling in the undecimated contourlet domain: a comparison of lmmse and map approaches	undecimated separable wavelet transform;maximum a posteriori criterion;filter bank;image resolution;probability density function;synthetic aperture radar signal;mmse filter;airports;image restoration;signal resolution filter bank wavelet domain wavelet transforms image restoration signal processing mean square error methods matrix decomposition telecommunications image resolution;linear minimum mean square error;wavelet transforms geophysical techniques image denoising synthetic aperture radar;map criterion;map approaches;multiplicative noise;linear minimum mean square error criterion;wavelet transforms;wavelet transform;nsct;matrix decomposition;map filter;signal processing;sar image;nonsubsampled contourlet transform;transforms;mean square error methods;signal resolution;sar image despeckling algorithms;image denoising;map filter synthetic aperture radar despeckling nonsubsampled contourlet transform mmse filter;wavelet domain;despeckling;sar images denoising map approaches undecimated separable wavelet transform nonsubsampled contourlet transform nsct multiplicative noise maximum a posteriori criterion map criterion linear minimum mean square error criterion lmmse criterion sar image despeckling algorithms synthetic aperture radar signal;lmmse criterion;geophysical techniques;telecommunications;sar images denoising;synthetic aperture radar	In this paper, we propose an extension of two despeckling algorithms, proposed to denoise SAR images and based on the undecimated separable wavelet transform, to work with the nonsubsampled contourlet transform (NSCT). The NSCT is a powerful and versatile nonseparable transform that allows a multiresolution and directional representation to be achieved. The SAR signal is modeled as affected by a multiplicative noise. The noise-free NSCT coefficients are estimated from the observed ones according to either the maximum-a-posteriori (MAP) or the linear minimum mean square error (LMMSE) criterion. The results show that the proposed de-speckling algorithms highly benefit from the fact of working into a multiresolution and multidirectional domain.	approximation algorithm;coefficient;contourlet;mean squared error;multiplicative noise;noise reduction;wavelet transform	Fabrizio Argenti;Tiziano Bianchi;Giovanni Martucci di Scarfizzi;Luciano Alparone	2008	IGARSS 2008 - 2008 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2008.4778834	computer vision;contourlet;signal processing;pattern recognition;mathematics;wavelet transform	Robotics	61.50655632131978	-70.43207299983358	143874
ba995cadc79315794cf8b8b2dbcfbb444c0008a5	fractional order total variation regularization for image super-resolution	partial differential equation;image super resolution;total variation;fractional order differentiation	In this paper, we present a fractional order total variation (TV) regularization functional for image super-resolution, the role of which is to better handle the texture details of image. This regularization functional is then incorporated into a variational formulation with an image fidelity term and the usual TV regularization that can efficiently preserve the discontinuities and image structures. The resulting evolution equation is the gradient descent flow that minimizes the overall functional. The proposed model has been applied to eight real images with promising results; unlike the existing TV-based image super-resolution models, the proposed model does not suffer from block artifacts, staircase edges and false edge near the edges.	super-resolution imaging;total variation denoising	Zemin Ren;Chuanjiang He;Qifeng Zhang	2013	Signal Processing	10.1016/j.sigpro.2013.02.015	mathematical optimization;calculus;mathematics;geometry;total variation denoising;total variation;partial differential equation	ML	54.90570699888265	-70.51615131158928	144162
71e3d1d0fea787a587de2abc4c321c32b267e6a8	despeckling trilateral filter	bilateral filtering;speckle;coefficient of variation;trilateral filter;ultrasound;ultrasonic imaging;speckle pixel ultrasonic imaging signal to noise ratio anisotropic magnetoresistance smoothing methods filtering;anisotropic diffusion;ultrasonic imaging medical image processing radar imaging smoothing methods speckle;computer graphic;smoothing methods;anisotropic diffusion despeckling trilateral filter speckle images image smoothing ultrasound image synthetic aperture radar image spatial closeness coefficient of variation component computational efficient iterative speckle;synthetic aperture radar trilateral filter coefficient of variation speckle reducing anisotropic diffusion ultrasound;medical image processing;radar imaging;speckle reducing anisotropic diffusion;computational efficiency;synthetic aperture radar	The bilateral filter smoothes noisy signals while preserving the semantic signal features. Its main advantage is being non-iterative. It is effective for a variety of applications in computer vision and computer graphics. However, little is known about the usefulness of bilateral filtering for speckle images. We propose a non-iterative, despeckling trilateral filter (DSTF) for smoothing ultrasound or synthetic aperture radar imagery. This filter combines the spatial closeness, intensity similarity and the coefficient of variation component. It generates outputs with speckle regions smoothed and structural features well preserved. The performance of the method is illustrated using synthetic, ultrasound and radar images. We show that the DSTF improves the bilateral filter with better speckle suppression, and is more computational efficient than the heavily iterative speckle reducing anisotropic diffusion.	anisotropic diffusion;bilateral filter;centrality;coefficient;computer graphics;computer vision;iteration;iterative method;noise reduction;radar;smoothing;synthetic data;synthetic intelligence;zero suppression	Yongjian Yu;Gang Dong;Jue Wang	2011	2011 IEEE 10th IVMSP Workshop: Perception and Visual Signal Analysis	10.1109/IVMSPW.2011.5970352	edge-preserving smoothing;speckle noise;computer vision;electronic engineering;geography;optics	Vision	55.64725848186205	-67.90066084016064	144247
ed233ea94d97cbaed43e6816625f06a15673fdca	locally adaptive multiscale bayesian method for image denoising based on bivariate normal inverse gaussian distributions	normal inverse gaussian distribution;bayesian method;complex wavelet transform;local adaptation;bivariate normal inverse gaussian distribution;image denoising;bivariate bayesian estimator	Recently, the use of wavelet transform has led to significant advances in image denoising applications. Among wavelet based denoising approaches, Bayesian techniques give more accurate estimates. Considering interscale dependencies, these estimates become closer to the original image. In this context, the choice of an appropriate model for wavelet coefficients is an important issue. The performance can also be improved by estimating model parameters in a local neighborhood. In this paper, we introduce a spatially adaptive MMSE-based Bayesian estimator using bivariate normal inverse Gaussian (NIG) distribution. The NIG distribution can model a wide range of processes, from heavy-tailed to less heavy-tailed processes. Exploiting this new statistical model in the dual-tree complex wavelet domain, we achieved state-of-the-art performance among related recent denoising approaches, both visually and in terms of peak signal-to-noise ratio (PSNR).	bivariate data;coefficient;noise reduction;peak signal-to-noise ratio;statistical model;wavelet transform	Mohamad Forouzanfar;Hamid Abrishami Moghaddam;Sona Ghadimi	2007	2007 International Conference on Wavelet Analysis and Pattern Recognition	10.1142/S0219691308002562	econometrics;mathematical optimization;bayesian probability;normal-inverse gaussian distribution;mathematics;wavelet packet decomposition;stationary wavelet transform;non-local means;statistics;wavelet transform	Vision	60.57354863378841	-69.95226276609236	144270
168a82f1ca8db283aa813ecfb553dc2948a3e488	sar image despeckling algorithms using stochastic distances and nonlocal means		This paper presents two approaches for filter design based on stochastic distances for intensity speckle reduction. A window is defined around each pixel, overlapping samples are compared and only those which pass a goodness-of-fit test are used to compute the filtered value. The tests stem from stochastic divergences within the Information Theory framework. The technique is applied to intensity Synthetic Aperture Radar (SAR) data with homogeneous regions using the Gamma model. The first approach uses a Nagao-Matsuyama-type procedure for setting the overlapping samples, and the second uses the nonlocal method. The proposals are compared with the Improved Sigma filter and with anisotropic diffusion for speckled data (SRAD) using a protocol based on Monte Carlo simulation. Among the criteria used to quantify the quality of filters, we employ the equivalent number of looks, and line and edge preservation. Moreover, we also assessed the filters by the Universal Image Quality Index and by the Pearson correlation between edges. Applications to real images are also discussed. The proposed methods show good results.	aharonov–bohm effect;anisotropic diffusion;case preservation;delta-sigma modulation;filter design;image quality;information theory;monte carlo method;noise reduction;pixel;simulation	Leonardo Torres;Alejandro César Frery	2013	CoRR		computer vision;mathematical optimization;mathematics;statistics	Vision	62.2498161640725	-69.6435278573215	145388
66f6c719d6375639fc493680e3f47e7aa317cc42	a full-plane block kalman filter for image restoration	kalman filtering;filtering;block kalman filter;additive white gaussian noise;filtrage kalman;restauration image;filtro kalman;image processing;filtre kalman;kalman filters;additive noise;two dimensional filtering;procesamiento imagen;kalman filter;two dimensional displays;image restoration;testing;state estimation;image bruitee;traitement image;filtrage bidimensionnel;image restoration filtering kalman filters state estimation covariance matrix two dimensional displays image generation additive noise testing additive white noise;filtering and prediction theory;two dimensional digital filters;imagen sonora;filtrado bidimensional;restauracion imagen;full plane blur;image generation;additive white gaussian noise causality block kalman filter image restoration two dimensional method full plane image model additive noise full plane blur multiple concurrent block estimators true state dynamics kalman gain matrix;full plane image model;image reconstruction;noisy image;additive white noise;kalman gain matrix;two dimensional digital filters filtering and prediction theory image reconstruction kalman filters;true state dynamics;two dimensional method;image modeling;filtrado kalman;covariance matrix;causality;multiple concurrent block estimators	A two-dimensional method which uses a full-plane image model to generate a more accurate filtered estimate of an image that has been corrupted by additive noise and full-plane blur is presented. Causality is maintained within the filtering process by using multiple concurrent block estimators. In addition, true state dynamics are preserved, resulting in an accurate Kalman gain matrix. Simulation results on a test image corrupted by additive white Gaussian noise are presented for various image models and compared to those of the previous block Kalman filtering methods.	additive white gaussian noise;biologic preservation;causality;circuit restoration;gaussian blur;image restoration;kalman filter;normal statistical distribution;simulation;standard test image;utility functions on indivisible goods	Stuart Citrin;Mahmood R. Azimi-Sadjadi	1992	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.199918	kalman filter;computer vision;fast kalman filter;image processing;computer science;control theory;mathematics;extended kalman filter;statistics	Vision	54.73259890580637	-67.23780673839228	146079
55ff41dac91bf8a1f6ec96b4916656f517b82806	compressive imaging reconstruction using adaptive directional total variation based on structure orientation fields	compressive imaging	Compressive imaging presents the challenge of reconstructing an image accurately from its under-sampling measurements. The total variation model, used as the regularization for imaging reconstruction, ignores structure direction, which tends to result in over-smoothing of homogenous regions and staircase artifacts in textured regions. This paper proposes to solve this problem using a compressive reconstruction method with adaptive directional total variation (DTV) based on the structure orientation field (SOF). This approach builds the estimation model of the structure tensor upon the Huber norm as the penalty term, reducing the degradation influence and reaching a quick solution using the primal-dual algorithm. The method then uses the structure tensor to describe the SOF and to establish compressive reconstruction in which the SOF restricts the DTV regularization. Finally, by analyzing the structure of the objective function, this approach adopts the linearized alternating direction method of multipliers to address the challenges presented by the reconstruction model. The experimental results show that the proposed method requires fewer iterations and demonstrates better reconstruction performance than existing methods, as indicated by examining texture details for visual fidelity and quality as measured through other objective criteria.		Huifeng Tao;Xing Yang;Songfeng Yin;Yongshun Ling	2016	J. Electronic Imaging	10.1117/1.JEI.25.6.063023	computer science	Vision	57.13161290208378	-70.55592570477555	146100
7aa7360cbe142ffc76f53b32310b5920626e3219	multiresolution image fusion based on the wavelet-based contourlet transform	laplacian pyramid transform multiresolution image fusion wavelet based contourlet transform directional filter bank multidirection decomposition multiscale decomposition perfect reconstruction;multiscale and multidirection decomposition image fusion wavelet transform contourlet transform directional filter bank redundancy;filter bank;multiscale and multidirection decomposition;image resolution;multiplication operator;image resolution image fusion wavelet transforms laplace equations filter bank image sensors signal resolution pixel programmable logic arrays educational institutions;multiscale decomposition;image fusion;multiresolution image fusion;wavelet based contourlet transform;programmable logic arrays;image sensors;wavelet transforms channel bank filters image fusion image reconstruction image resolution;wavelet transforms;laplacian pyramid transform;laplacian pyramid;laplace equations;wavelet transform;redundancy;channel bank filters;image reconstruction;pixel;signal resolution;contourlet transform;directional filter bank;fusion rule;multidirection decomposition;perfect reconstruction	The wavelet-based contourlet transform (WBCT) is a new directional transform. This transform uses the wavelet transform and the directional filter bank (DFB) to obtain a multiscale and multidirection decomposition of image. The wavelet transform and the DFB are non-redundant and perfect reconstruction. So the WBCT can be regarded as a non-redundant version of the contourlet transform. A new image fusion scheme based on the WBCT was presented. Firstly, the WBCT is used to perform a multiscale and multidirection decomposition of each image. Then the WBCT coefficients of fused image are constructed using multiple operators according to different fusion rules. The experimental results show that this new fusion scheme is effective and the fused images are better than that of using the Laplacian pyramid transform, the wavelet transform and the contourlet transform.	coefficient;contourlet;filter bank;image fusion;laplacian matrix;machine vision;multiresolution analysis;wavelet transform	Lei Tang;Zonggui Zhao	2007	2007 10th International Conference on Information Fusion	10.1109/ICIF.2007.4407989	wavelet;computer vision;contourlet;speech recognition;s transform;harmonic wavelet transform;second-generation wavelet transform;continuous wavelet transform;pattern recognition;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;wavelet transform	Vision	59.03233719487284	-67.32299291173196	146197
59280edddbb699a542af1daf62bb1b0ee51c2847	image decomposition combining staircase reduction and texture extraction	oscillations;texture;image processing;higher order;denoising;oscillating patterns;dual methods;image decomposition;staircase reduction	This paper proposes a natural and efficient way to achieve staircase reduction in texture extraction models of image processing. Moreover, we propose a precise framework for this amalgamation. In a sense, we utilize the best of both worlds: (I) the use of higher order derivatives through a variant of the Chambolle-Lions inf convolution energy (an image decomposition model in itself) along with (II) approximations to Meyer’s G and E norms including the H−1 negative norm for ameliorating staircasing in image decomposition and restoration problems.	approximation;circuit restoration;convolution;image processing	Tony F. Chan;Selim Esedoglu;Frederick E. Park	2007	J. Visual Communication and Image Representation	10.1016/j.jvcir.2006.12.004	computer vision;mathematical optimization;higher-order logic;image processing;computer science;noise reduction;mathematics;texture;oscillation;algorithm	Vision	55.02337762371775	-69.97033904975899	146726
0f6624a28bbc7d4aba8b7da835bf5763c6f540cb	an adaptive diffusion scheme for image restoration and selective smoothing	image restoration;inhomogeneous pdes;anisotropic diffusion;scale space;adaptive regularization	Anisotropic partial differential equation (PDE)-based image restoration schemes employ a local edge indicator function typically based on gradients. In this paper, an alternative pixel-wise adaptive diffusion scheme is proposed. It uses a spatial function giving better edge information to the diffusion process. It avoids the over-locality problem of gradient-based schemes and preserves discontinuities coherently. The scheme satisfies scale space axioms for a multiscale diffusion scheme; and it uses a well-posed regularized total variation (TV) scheme along with Perona-Malik type functions. Median-based weight function is used to handle the impulse noise case. Numerical results show promise of such an adaptive approach on real noisy images.	circuit restoration;image restoration;smoothing	V. B. Surya Prasath;Arindama Singh	2012	Int. J. Image Graphics	10.1142/S0219467812500039	image restoration;computer vision;mathematical optimization;mathematical analysis;scale space;computer science;mathematics;geometry;anisotropic diffusion	Vision	55.31810402775535	-70.18652304731103	147296
20e26e72fab73c1ee2f55f944240b7357c0e2d9d	half-quadratic algorithm for ℓp - ℓq problems with applications to tv-ℓ1 image restoration and compressive sensing		In this paper, we consider the `p-`q minimization problem with 0 < p, q ≤ 2. The problem has been studied extensively in image restoration and compressive sensing. In the paper, we first extend the half-quadratic technique from `1-norm to `p-norm with 0 < p < 2. Based on this, we develop a half-quadratic algorithm to solve the `p-`q problem. We prove that our algorithm is indeed a majorize-minimize algorithm. From that we derive some convergence results of our algorithm, e.g. the objective function value is monotonic decreasing and convergent. We apply our algorithm to TV-`1 image restoration and compressive sensing in magnetic resonance (MR) imaging applications. The numerical results show that our algorithm is fast and efficient in restoring blurred images that are corrupted by impulse noise, and also in reconstructing MR images from very few k-space data.	algorithm;algorithmic efficiency;circuit restoration;compressed sensing;computation;image restoration;impulse noise (audio);iterative reconstruction;loss function;numerical analysis;optimization problem;rate of convergence;resonance	Raymond H. Chan;Hai-Xia Liang	2011		10.1007/978-3-642-54774-4_4		ML	56.5405019781624	-71.70634105223608	147309
36d018bab08efc1b02ace0104f77c166306e1428	learning to display high dynamic range images	libre mercado;cuantificacion senal;analisis imagen;metodo adaptativo;quantization;evaluation performance;optimisation;analisis escena;analyse scene;learning algorithm;performance evaluation;image processing;optimizacion;learning;quantifier;implementation;evaluacion prestacion;procesamiento imagen;dynamic method;methode adaptative;algorithme apprentissage;traitement image;approche deterministe;high dynamic range imaging;marche concurrentiel;competitive learning;aprendizaje;deterministic approach;objective function;apprentissage;signal quantization;methode dynamique;quantification signal;quantificateur;adaptive method;imaging;adaptive learning;enfoque determinista;dynamic range;formation image;metodo dinamico;image analysis;optimization;low dynamic range;formacion imagen;open market;high dynamic range;reseau neuronal;implementacion;cuantificador;algoritmo aprendizaje;analyse image;red neuronal;dynamic range compression;learning based image processing;neural network;scene analysis	In this paper we present a novel method to map high dynamic range scenes to low dynamic range images for visualization. We formulate the problem as a quantization process and employ an adaptive learning strategy to ensure that the low dynamic range displays not only faithfully reproduce the original scenes but also are visually pleasing. This is achieved by the use of a competitive learning neural network that employs a frequency sensitive competitive learning mechanism. An L2 objective function ensures that the mapped low dynamic image preserves the relative visual contrast impressions of the original scene. A frequency sensitive competitive mechanism facilitates the full and even utilization of the limited displayable values. We present experimental results to demonstrate the effectiveness of the method in displaying a variety of high dynamic range scenes.	algorithm;artificial neural network;competitive learning;high dynamic range;high-dynamic-range rendering;loss function;optimization problem;performance;range imaging	Jiang Duan;Guoping Qiu;Graham D. Finlayson	2004	Pattern Recognition	10.1016/j.patcog.2007.02.012	computer vision;dynamic range;image analysis;simulation;dynamic range compression;open market operation;quantization;image processing;computer science;artificial intelligence;competitive learning;deterministic system;implementation;adaptive learning	Vision	62.26417231791232	-68.19740577207087	147447
bae91d6788e87b9e61a955bba53163f4b2a424f3	hierarchical beta process with gaussian process prior for hyperspectral image super resolution		Hyperspectral cameras acquire precise spectral information, however, their resolution is very low due to hardware constraints. We pro- pose an image fusion based hyperspectral super resolution approach that employes a Bayesian representation model. The proposed model accounts for spectral smoothness and spatial consistency of the representation by using Gaussian Processes and a spatial kernel in a hierarchical formulation of the Beta Process. The model is employed by our approach to first infer Gaussian Processes for the spectra present in the hyperspectral image. Then, it is used to estimate the activity level of the inferred processes in a sparse representation of a high resolution image of the same scene. Finally, we use the model to compute multiple sparse codes of the high resolution image, that are merged with the samples of the Gaussian Processes for an accurate estimate of the high resolution hyperspectral image. We per- form experiments with remotely sensed and ground-based hyperspectral images to establish the effectiveness of our approach.	gaussian process;super-resolution imaging	Naveed Akhtar;Faisal Shafait;Ajmal S. Mian	2016		10.1007/978-3-319-46487-9_7	computer vision;machine learning;pattern recognition;mathematics	ML	68.06975688285239	-66.79598582783309	148285
7a22a9b67d288cd1ddc9fa54d67e4f0e17791e7e	global image editing using the spectrum of affinity matrices	eigenvalues and eigenfunctions;graph theory;matrix algebra;polynomials;image representation	In this work we introduce a new image editing tool, based on the spectrum of a global filter computed from image affinities. Recently, we have shown that the global filter derived from a fully connected graph representing the image, can be approximated using the Nyström extension [1]. This filter is computed by approximating the leading eigenvectors of the filter. These orthonormal eigenfunctions are highly expressive of the coarse and fine details in the underlying image, where each eigenvector can be interpreted as one scale of a data-dependent multiscale image decomposition. In this filtering scheme, each eigenvalue can boost or suppress the corresponding signal component in each scale. Our analysis shows that the mapping of the eigenvalues by an appropriate polynomial function endows the filter with a number of important capabilities, such as edge-aware sharpening, denoising and tone manipulation.	affinity analysis;approximation algorithm;connectivity (graph theory);data dependency;global serializability;image editing;noise reduction;polynomial;processor affinity	Hossein Talebi Esfandarani;Peyman Milanfar	2013	2013 IEEE Global Conference on Signal and Information Processing	10.1109/GlobalSIP.2013.6737005	mathematical optimization;combinatorics;discrete mathematics;mathematics	Vision	54.86708999290424	-68.62659455816275	148341
308c3796afb4b5f298ed4a3a6fb5dd665465e1cf	faithful recovery of vector valued functions from incomplete data	total variation;pattern recognition;value function	On March 11, 1944, the famous Eremitani Church in Padua (Italy) was destroyed in an Allied bombing along with the inestimable frescoes by Andrea Mantegna et al. contained in the Ovetari Chapel. In the last 60 years, several attempts have been made to restore the fresco fragments by traditional methods, but without much success. We have developed an efficient pattern recognition algorithm to map the original position and orientation of the fragments, based on comparisons with an old gray level image of the fresco prior to the damage. This innovative technique allowed for the partial reconstruction of the frescoes. Unfortunately, the surface covered by the fragments is only 77 m, while the original area was of several hundreds. This means that we can reconstruct only a fraction (less than 8%) of this inestimable artwork. In particular the original color of the blanks is not known. This begs the question of whether it is possible to estimate mathematically the original colors of the frescoes by making use of the potential information given by the available fragments and the gray level of the pictures taken before the damage. Can one estimate how faithful such restoration is? In this paper we retrace the development of the recovery of the frescoes as an inspiring and challenging real-life problem for the development of new mathematical methods. We introduce two models for the recovery of vector valued functions from incomplete data, with applications to the fresco recolorization problem. The models are based on the minimization of a functional which is formed by the discrepancy with respect to the data and additional regularization constraints. The latter refer to joint sparsity measures with respect to frame expansions for the first functional and functional total variation for the second. We establish the relations between these two models. As a byproduct we develop the basis of a theory of fidelity in color recovery, which is a crucial issue in art restoration and compression.	algorithm;chapel;circuit restoration;color;discrepancy function;grayscale;pattern recognition;real life;sparse matrix;theory	Massimo Fornasier	2007		10.1007/978-3-540-72823-8_11	pattern recognition;data mining;statistics	Vision	63.68559195447585	-73.60430383637384	148871
174b4538b4b73109315b2e758092eb2989933550	structure-preserving dual-energy ct for luggage screening	noise reduction structure preserving dual energy ct inversion spde ct inversion computed tomography luggage screening metal artifact mitigation precise object localization artifact reduction material identification accuracy security applications photoelectric images compton pixel property images dual energy x ray tomographic data optimization problem projection process metal aware data weighting real dual energy data;metals computed tomography materials x ray imaging mathematical model image reconstruction signal to noise ratio;x ray microscopy compton effect computerised tomography national security optimisation;national security dual energy x ray tomography reconstruction structure preserving	We propose a new structure-preserving dual-energy (SPDE) CT inversion technique for luggage screening, which can mitigate metal artifacts and provide precise object localization. Such artifact reduction can increase material identification accuracy in security applications. Our main objective is formation of enhanced photoelectric and Compton pixel property images from dual-energy X-ray tomographic data. We achieve this aim by incorporating three important elements in a single unified framework. First, we generate our images as the solution of a joint optimization problem, which explicitly models the projection process. Second, we include metal aware data weighting to reduce streaks and metal artifacts. Third, we estimate a regularized joint boundary field and apply it to both the photoelectric and Compton images in order to improve object localization as well as smoothing inside the objects. We evaluate the performance of the method using real dual-energy data. We demonstrate a significant reduction in noise and metal artifacts.	ct scan;mathematical optimization;optimization problem;photoelectric effect;pixel;smoothing;unified framework	Limor Martin;W. Clem Karl;Prakash Ishwar	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6853782	computer vision;simulation	Robotics	55.67066952348369	-75.09192510774652	149391
4f900f75207d504e2552ce3858a57de6072aa026	morphological image enlargements	mathematical morphology;edge enhancement;image processing;low pass;linear interpolation;motion vector	This paper presents a method based in mathematical morphology to enlarge images. It does not make the low pass assumption which is common to all linear interpolation methods and which does not often hold for images. Pixels in smooth areas are properly interpolated by linear methods while those at the edges are not. The method begins with a linear interpolation and a gradient computation. The gradient serves as a measure of confidence about the linear interpolation. Then, the proposed algorithm processes the pixels in a certain order: first pixels with high confidence (smooth zones) of the image and those with a low one (edges) at the end. By doing so, it preserves both slow variations and sharp edges. The method can be applied to other image processing problems, such as edge enhancement or motion vector estimation, where there is an image and confidence information about each pixel.		Antonio Albiol;Jean Paul Frédéric Serra	1997	J. Visual Communication and Image Representation	10.1006/jvci.1997.0367	computer vision;mathematical optimization;discrete mathematics;mathematical morphology;bilinear interpolation;low-pass filter;image processing;computer science;morphological gradient;stairstep interpolation;mathematics;edge enhancement;linear interpolation;nearest-neighbor interpolation;image scaling	Vision	55.692208011459144	-66.33228226581427	149986
16f0bc1e7323b11cefec74833bb5b3a6df7b9535	constrained nonnegative matrix factorization for robust hyperspectral unmixing		Hyperspectral unmixng (HU) is an essential step for hyperspectral image (HSI) analysis. In real HSI, there often are abnormal fluctuations existing in specific bands, which can be described as sparse noise. This type of corruption will seriously disrupt the hyperspectral image quality, causing extra difficulties during unmixing process. However, the influence of sparse noise is often ignored by existing unmixing methods, which leads to the reduction of robustness and accuracy for HU tasks. Therefore, we propose a new unmixing model which takes noise corruption into consideration. By designing and imposing constraints considering the sparsity of noise, properties of endmember and abundance on nonnegative matrix factorization (NMF), the proposed method can resist the sparse noise and achieve more robust and accurate unmixing results. Adequate experiments have been conducted on both synthetic and real hyperspectral data. And the results confirm the superiority of proposed method compared to state-of-the-art methods.	experiment;horizontal situation indicator;image quality;non-negative matrix factorization;sparse matrix;synthetic intelligence	Fan Feng;Chenwei Deng;Wenzheng Wang;Jiahui Dai;Zhenzhen Li;Baojun Zhao	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8517818	endmember;robustness (computer science);computer vision;task analysis;image quality;sparse matrix;cluster analysis;computer science;hyperspectral imaging;non-negative matrix factorization;pattern recognition;artificial intelligence	Vision	68.13620227612623	-66.43466442534789	150265
5659e45a46fb8e24b6116dec777d621e73a9800e	two deterministic half-quadratic regularization algorithms for computed imaging	minimisation;auxiliary variable;gaussian noise;computed imaging;spect reconstruction deterministic half quadratic regularization algorithms computed imaging image processing problems roughness penalty smoothing edge preserving regularization auxiliary variable discontinuities half quadratic optimization optimization deterministic strategy alternate minimizations artur legend;image processing problems;spect reconstruction;medical image processing minimisation smoothing methods edge detection image reconstruction single photon emission computed tomography deterministic algorithms;image processing;half quadratic;edge preserving regularization;alternate minimizations;edge detection;markov random fields;minimization methods;sufficient conditions;computer applications;legend;deterministic algorithms;half quadratic optimization;smoothing methods;discontinuities;smoothing;image reconstruction;medical image processing;signal processing;deterministic half quadratic regularization algorithms;single photon emission computed tomography;alternating minimization;optimization;artur;image reconstruction smoothing methods tomography image processing sufficient conditions gaussian noise markov random fields minimization methods signal processing computer applications;roughness penalty;deterministic strategy;tomography	Many image processing problems are ill-posed and must be regularized. Usually, a roughness penalty is imposed on the solution. The difficulty is to avoid the smoothing of edges, which are very important attributes of the image. The authors first give sufficient conditions for the design of such an edge-preserving regularization. Under these conditions, it is possible to introduce an auxiliary variable whose role is twofold. Firstly, it marks the discontinuities and ensures their preservation from smoothing. Secondly, it makes the criterion half-quadratic. The optimization is then easier. The authors propose a deterministic strategy, based on alternate minimizations on the image and the auxiliary variable. This yields two algorithms, ARTUR and LEGEND. The authors apply these algorithms to the problem of SPECT reconstruction. >		Pierre Charbonnier;Laure Blanc-Féraud;Gilles Aubert;Michel Barlaud	1994		10.1109/ICIP.1994.413553	iterative reconstruction;gaussian noise;computer vision;minimisation;mathematical optimization;combinatorics;edge detection;image processing;computer science;signal processing;mathematics;tomography;computer applications;smoothing;classification of discontinuities	Theory	55.63947667037742	-72.60342197037478	150588
9533a46cb18a64d0fc87bb762cbaf9d75efaff60	image restoration by estimating frequency distribution of local patches		In this paper, we propose a method to solve the image restoration problem, which tries to restore the details of a corrupted image, especially due to the loss caused by JPEG compression. We have treated an image in the frequency domain to explicitly restore the frequency components lost during image compression. In doing so, the distribution in the frequency domain is learned using the cross entropy loss. Unlike recent approaches, we have reconstructed the details of an image without using the scheme of adversarial training. Rather, the image restoration problem is treated as a classification problem to determine the frequency coefficient for each frequency band in an image patch. In this paper, we show that the proposed method effectively restores a JPEG-compressed image with more detailed high frequency components, making the restored image more vivid.		Jaeyoung Yoo;Sang-ho Lee;Nojun Kwak	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00699	cross entropy;computer vision;frequency domain;computer science;image restoration;frequency band;artificial intelligence;image compression;feature extraction;jpeg;pattern recognition	Vision	59.9423310754071	-66.37248806026604	150674
345c6bd3cf2e61c45c54716f7c7411a854d362e1	video denoising using low rank tensor decomposition	denoising;video	Reducing noise in a video sequence is of vital important in many real-world applications. One popular method is block matching collaborative filtering. However, the main drawback of this method is that noise standard deviation for the whole video sequence is known in advance. In this paper, we present a tensor based denoising framework that considers 3D patches instead of 2D patches. By collecting the similar 3D patches non-locally, we employ the low-rank tensor decomposition for collaborative filtering. Since we specify the non-informative prior over the noise precision parameter, the noise variance can be inferred automatically from observed video data. Therefore, our method is more practical, which does not require knowing the noise variance. The experimental on video denoising demonstrates the effectiveness of our proposed method.	noise reduction;video denoising	Lihua Gui;Gaochao Cui;Qibin Zhao;Dongsheng Wang;Andrzej Cichocki;Jianting Cao	2016		10.1117/12.2268435	video denoising	Vision	59.65570370377954	-68.76835491473118	150890
5924ea471f262ff6705b518b838893ab30a6dfd9	robust brain mri denoising and segmentation using enhanced non-local means algorithm	brain mri;rician noise;segmentation;non local means;denoising	Image denoising is an integral component of many practical medical systems. Non-local means (NLM) is an effective method for image denoising which exploits the inherent structural redundancy present in images. Improved adaptive non-local means (IANLM) is an improved variant of classical NLM based on a robust threshold criterion. In this paper, we have proposed an enhanced non-local means (ENLM) algorithm, for application to brain MRI, by introducing several extensions to the IANLM algorithm. First, a Rician bias correction method is applied for adapting the IANLM algorithm to Rician noise in MR images. Second, a selective median filtering procedure based on fuzzy c-means algorithm is proposed as a postprocessing step, in order to further improve the quality of IANLM-filtered image. Third, different parameters of the proposed ENLM algorithm are optimized for application to brain MR images. Different variants of the proposed algorithm have been presented in order to investigate the influence of the proposed modifications. The proposed variants have been validated on both T1-weighted (T1-w) and T2-weighted (T2-w) simulated and real brain MRI. Compared with other denoising methods, superior quantitative and qualitative denoising results have been obtained for the proposed algorithm. Additionally, the proposed algorithm has been applied to T2-weighted brain MRI with multiple sclerosis lesion to show its superior capability of preserving pathologically significant information. Finally, impact of the proposed algorithm has been tested on segmentation of brain MRI. Quantitative and qualitative segmentation results verify that the proposed algorithm based segmentation is better compared with segmentation produced by other contemporary techniques. VC 2014 Wiley Periodicals, Inc. Int J Imaging Syst Technol, 24, 52–66, 2014; Published online in Wiley Online Library (wileyonlinelibrary.com). DOI: 10.1002/ima.22079	algorithm;effective method;john d. wiley;median filter;netware loadable module;noise reduction;non-local means;pixel;redundancy (engineering);resonance	Muhammad Aksam Iftikhar;Abdul Jalil;Saima Rathore;Mutawarra Hussain	2014	Int. J. Imaging Systems and Technology	10.1002/ima.22079	computer vision;computer science;artificial intelligence;machine learning;noise reduction;segmentation;non-local means	Vision	57.121099850910866	-68.38019299576202	151011
8251310f2da31b7317abd6effb522d48a759c921	the local low-dimensionality of natural images		We develop a new statistical model for photographic images, in which the local responses of a bank of linear filters are described as jointly Gaussian, with zero mean and a covariance that varies slowly over spatial position. We optimize sets of filters so as to minimize the nuclear norm of matrices of their local activations (i.e., the sum of the singular values), thus encouraging a flexible form of sparsity that is not tied to any particular dictionary or coordinate system. Filters optimized according to this objective are oriented and band-pass, and their responses exhibit substantial local correlation. We show that images can be reconstructed nearly perfectly from estimates of the local filter response covariance alone, and with minimal degradation (either visual or MSE) from low-rank approximations of these covariances. As such, this representation holds much promise for use in applications such as denoising, compression, and texture representation, and may form a useful substrate for hierarchical decompositions.	approximation;data compression;dictionary;elegant degradation;noise reduction;sparse matrix;statistical model	Olivier J. Hénaff;Johannes Ballé;Neil C. Rabinowitz;Eero P. Simoncelli	2014	CoRR		econometrics;mathematical optimization;machine learning;mathematics;statistics	ML	59.18155450068592	-69.10079193947631	151511
7910c1b8eb07d1e357ac4c71125cbd759abfa861	robust multiscale am-fm demodulation of digital images	image processing;multidimensional demodulation;gaussian processes;robustness demodulation digital images frequency estimation image reconstruction birth disorders multidimensional systems frequency modulation power harmonic filters signal analysis;multidimensional frequency modulation;instantaneous frequency;multidimensional amplitude modulation;indexing terms;sinusoidal instantaneous frequency component robust multiscale am fm demodulation digital images am fm demodulation algorithms filterbanks separable filters robust qea vakman methods mean square error am fm harmonic reconstruction gaussian amplitude;frequency modulated;mean square error;digital filters;multidimensional frequency modulation multidimensional demodulation multidimensional amplitude modulation;mean square error methods;mean square error methods digital filters gaussian processes image processing;digital image;amplitude modulated	In this paper, we introduce new multiscale AM-FM demodulation algorithms that provide significant improvements in accuracy over previously reported approaches. The improvements are due to the use of new filterbanks based on separable filters supported in just two quadrants. The QEA, robust-QEA and Vakman methods are improved with this new filterbanks. A number of 2-D AM-FM examples are presented, where we observe significant accuracy improvements. For Lena, the mean-square-error for the AM-FM harmonic reconstruction is reduced by 88.31%. Similarly, for a AM-FM synthetic example of sinusoidal phase and Gaussian amplitude, the mean-square-error is reduced by: (i) 70.86% for the reconstruction, (ii) 99.66% for the instantaneous amplitude and (iii) 96.52% for the sinusoidal instantaneous frequency component.	algorithm;fm broadcasting;instantaneous phase;lenna;mean squared error;synthetic data	Víctor Murray;V. PaulRodríguez;Marios S. Pattichis	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4378992	instantaneous phase;computer vision;speech recognition;digital filter;index term;image processing;computer science;gaussian process;mathematics;mean squared error;digital image;statistics	Robotics	62.485157616547895	-70.84019205241894	151536
ba05c75d407a7414d19dde5549d0468489530b84	segmentation based sparse reconstruction of optical coherence tomography images	image reconstruction image segmentation dictionaries retina interpolation noise reduction image resolution;layer segmentation optical coherence tomography retina ophthalmic imaging image reconstruction sparse representation denoising interpolation	We demonstrate the usefulness of utilizing a segmentation step for improving the performance of sparsity based image reconstruction algorithms. In specific, we will focus on retinal optical coherence tomography (OCT) reconstruction and propose a novel segmentation based reconstruction framework with sparse representation, termed segmentation based sparse reconstruction (SSR). The SSR method uses automatically segmented retinal layer information to construct layer-specific structural dictionaries. In addition, the SSR method efficiently exploits patch similarities within each segmented layer to enhance the reconstruction performance. Our experimental results on clinical-grade retinal OCT images demonstrate the effectiveness and efficiency of the proposed SSR method for both denoising and interpolation of OCT images.	algorithm;dictionaries as topic;dictionary;interpolation imputation technique;iterative reconstruction;noise reduction;patch (computing);retina;sparse approximation;sparse matrix;tomography, optical coherence;x-ray computed tomography;biologic segmentation;subsynaptic reticulum	Leyuan Fang;Shutao Li;David Cunefare;Sina Farsiu	2017	IEEE Transactions on Medical Imaging	10.1109/TMI.2016.2611503	image texture;computer vision;computer science;segmentation-based object categorization;image segmentation;optics;scale-space segmentation;computer graphics (images)	Vision	56.6402735355141	-70.60195589899007	152284
0f1e95a0ac039fc5974d63efd6b0869b17c68d66	shearlet-based deconvolution	automatic control;libre mercado;routing protocols;degradation;contourlets;validacion cruzada;performance evaluation;protocole transmission;approximation error;approximation algorithms;curvelets;methode echelle multiple;implementation;distributed discontinuities;wavelets deconvolution generalized cross validation shearlets;image restoration;metodo escala multiple;connaissances explicites;testing;generalized cross validation;marche concurrentiel;desconvolucion;explicit knowledge;algorithme;wavelet transforms;algorithm;protocolo transmision;generalized cross validation shearlet based deconvolution distributed discontinuities curvelets contourlets m channel implementation approximation inversion operator;deconvolution degradation noise reduction image restoration approximation error approximation algorithms automatic control testing performance evaluation cameras;noise reduction;validation croisee;shearlet based deconvolution;shearlets;deconvolution;multiscale method;protocole routage;cross validation;open market;implementacion;approximation inversion operator;wavelet transforms deconvolution image restoration;m channel implementation;wavelets;cameras;algoritmo;transmission protocol	In this paper, a new type of deconvolution algorithm is proposed that is based on estimating the image from a shearlet decomposition. Shearlets provide a multidirectional and multiscale decomposition that has been mathematically shown to represent distributed discontinuities such as edges better than traditional wavelets. Constructions such as curvelets and contourlets share similar properties, yet their implementations are significantly different from that of shearlets. Taking advantage of unique properties of a new M-channel implementation of the shearlet transform, we develop an algorithm that allows for the approximation inversion operator to be controlled on a multiscale and multidirectional basis. A key improvement over closely related approaches such as ForWaRD is the automatic determination of the threshold values for the noise shrinkage for each scale and direction without explicit knowledge of the noise variance using a generalized cross validation (GCV). Various tests show that this method can perform significantly better than many competitive deconvolution algorithms.	approximation algorithm;assumed;blind deconvolution;cholesky decomposition;circular convolution;coefficient;contourlet;cross reactions;curvelet;estimated;exhibits as topic;familial generalized lipodystrophy;forward algorithm;ganciclovir;hidden markov model;markov chain;matrix regularization;microsoft visio;new type;norm (social);population parameter;projections and predictions;qr decomposition;sample variance;shearlet;ventricular septal defects;wavelet analysis;wavelet transform;negative regulation of urea catabolic process;triangulation	Vishal M. Patel;Glenn R. Easley;Dennis M. Healy	2009	IEEE Transactions on Image Processing	10.1109/TIP.2009.2029594	wavelet;image restoration;computer vision;econometrics;mathematical optimization;approximation error;shearlet;degradation;open market operation;computer science;deconvolution;explicit knowledge;automatic control;noise reduction;mathematics;software testing;routing protocol;implementation;cross-validation;statistics;wavelet transform	Vision	54.05737306895353	-74.08335584940552	152305
59c6802264078bb6d21eef315aa29634e5ebcc5a	visual neural network model for welding deviation prediction based on weld pool centroid		To solve the problem in the process of weld seam tracking, a new prediction model for welding deviation based on the weld pool image centroid has been proposed in the paper. First, some weld images under different weld currents were captured by a vision sensor. A composite filter system, which is composed of narrow-band and neutral filters, is used to reduce the disturbance of weld arc. So, several clear weld pool images can be obtained. Then a frontier of weld pool is chosen to be the processing region. Median filter and gray transformation operations are used to enhance the contrast of processing region. On this basis, the variation trend of centroid difference xe and welding deviation e were analyzed. The centroid difference value xe and the weld current I were determined to be welding status parameters. Moreover, a BP neural network was set up, which was composed of three layers. Next, elastic gradient descent method was used to be the training function. So a prediction model between the welding statu...	network model;pool (computer science)	Dukun Ding	2018	IJPRAI	10.1142/S0218001418590140	artificial intelligence;pattern recognition;mathematics;weld pool;arc (geometry);median filter;welding;centroid;artificial neural network;gradient descent;control theory	AI	57.545364571470465	-66.59956144122032	152399
85f2da4c35ffbda28ccfb12ad62ae85dc0cb4833	image reconstruction from sparse projections using s-transform	transform;ct;image transform projections sparse reconstruction;bepress selected works;reconstruction;image;sparse;sparse sampling;total variation;era2015;s transform;sparse projection;projections	Sparse projections are an effective way to reduce the exposure to radiation during X-ray CT imaging. However, reconstruction of images from sparse projection data is challenging. This paper introduces a new sparse transform, referred to as S-transform, and proposes an accurate image reconstruction method based on the transform. The S-transform effectively converts the ill-posed reconstruction problem into a well-defined one by representing the image using a small set of transform coefficients. An algorithm is proposed that efficiently estimates the S-transform coefficients from the sparse projections, thus allowing the image to be accurately reconstructed using the inverse S-transform. The experimental results on both simulated and real images have consistently shown that, compared to the popular total variation (TV) method, the proposed method achieves comparable results when the projections is sparse, and substantially improves the quality of the reconstructed image when the number of the projections is relatively high. Therefore, the use of the proposed reconstruction algorithm may permit reduction of the radiation exposure without trade-off in imaging performance.	algorithm;ambiguous name resolution;coefficient;effective method;high- and low-level;iterative reconstruction;linkage (software);pixel;reconstruction conjecture;s transform;sparse approximation;sparse matrix;well-posed problem	Jiahai Liu;Wanqing Li;Yue Min Zhu;Ruiyao Jiang	2011	Journal of Mathematical Imaging and Vision	10.1007/s10851-011-0307-x	computer vision;mathematical optimization;haplogroup ct;s transform;machine learning;image;sparse approximation;mathematics;total variation	Vision	57.68023349669972	-69.99591790646885	152676
555d20c567d17d443417e08ab85cc4c549173a99	total variation based oversampling of noisy images	calculo de variaciones;image processing;procesamiento imagen;oversampling;image bruitee;traitement image;reduccion ruido;imagen sonora;calcul variationnel;surechantillonnage;imagen borrosa;blurred image;noise reduction;noisy image;reduction bruit;total variation;rapport signal bruit;relacion senal ruido;image floue;point of view;signal to noise ratio;variational models;variational calculus	We proposea variationalmodelwhich permitsto simultaneouslydeblur andoversamplean image.Indeed,after somerecallson an existing variationalmodelfor imageoversampling,weshow how to modify it in orderto properly achieve our two goals.We discussthemodificationbothundera theoretical point of view (theanalysisof thepreservation of somestructuralelements)and thepracticalpointof view of experimentalresults.Wealsodescribethealgorithm usedto computea solutionto this model.	oversampling;point of view (computer hardware company)	François Malgouyres	2001		10.1007/3-540-47778-0_10	computer vision;oversampling;image processing;computer science;calculus;noise reduction;mathematics;signal-to-noise ratio;total variation;algorithm;calculus of variations	Vision	54.34395350169715	-67.47855287563772	153000
2591e03b092dd4ddbcbe6775c03bdbde5b675c5f	poisson noise removal for image demosaicing		Most color image cameras today acquire only one out of the R, G, B values per pixel by means of a color filter array (CFA) in the hardware producing the so called ‘CFA image’. In-built software routines are required to undertake the task of obtaining the rest of the color information at each pixel through a process termed demosaicing. The most common CFA pattern is the well-known Bayer pattern which consists of repeating 2×2 filter sub-arrays. Studies in [2] have demonstrated that raw CFA images captured by a camera are corrupted predominantly by Poisson noise. This noise adversely affects the results of a demosaicing algorithm. While there exist several approaches in the literature to perform demosaicing, most of them do not fully account for the Poisson nature of the noise in the raw CFA images. In this paper, we present two simple but principled methods that infer dictionaries in situ from the noisy CFA images, both taking into account the Poisson nature of the noise. These dictionaries are used to denoise the noisy CFA images prior to demosaicing by exploiting the patch-level non-local similarity present in CFA images formed under periodic patterns such as the Bayer pattern and the sparsity of the coefficients of a linear combination of dictionary elements to express these patches. The denoised CFA image can be given as input to any off-the-shelf demosaicing routine to generate the full RGB image from the denoised CFA data. Our first approach, which we term the ‘Poisson Penalty Approach’ (PPA), is based on the direct minimization of an energy function which is the sum of the negative log likelihood of the Poisson noise model and a weighted sparsity-promoting term. Patches from the noisy CFA image are expressed as a non-negative sparse linear combination of dictionary columns, also constrained to be non-negative. Here, the dictionary as well as the sparse coefficients are learned in situ from the noisy patches in the CFA image. This minimization produces a denoised CFA image. Our second approach is termed the ‘Variance Stabiliser Approach’. It is well known that the so-called Anscombe transform [1] stabilizes the variance of Poisson distributed random variables of different mean values (and hence different variances), i.e. the transform makes these variances approximately equal. To denoise a Poisson corrupted CFA image Y using this approach, we first compute its Anscombe transform given by Z = 2 √ Y +3/8, denoise Z using a dictionary-based image denoising algorithm suited for the Gaussian noise model with a fixed, known variance (which equals 1 in this case), and obtain the final image as W = Z2/4−3/8. The specific denoising algorithm we use is the spatially varying PCA approach with a Wiener filter. We have performed extensive experiments on both synthetic and real data. Some results on real data captured using a Canon camera are shown in Figure 1 and comparisons are drawn between the noisy image displayed by the camera (after in-built demosaicing without denoising), the Poisson Penalty Approach, the Variance Stabilizer Approach, and a commerically available tool called NeatImage which denoises the RGB image after demoisaicing the noisy CFA image. Both the approaches clearly outperform the results obtained using NeatImage. Our methods have been tested on Bayer pattern CFA images but would work equally well on any other periodic CFA pattern.	algorithm;approximation;bayer filter;coefficient;color filter array;color image;column (database);demosaicing;dictionary;existential quantification;experiment;mathematical optimization;noise reduction;pixel;shot noise;sparse matrix;synthetic data;whole earth 'lectronic link;wiener filter	Sukanya Patil;Ajit Rajwade	2016			computer science;pattern recognition;computer vision;shot noise;artificial intelligence;demosaicing	Vision	59.03198077074541	-71.22068447357653	153812
ad3830184d3cf36c31a8b4a164329e992753a8ab	statistical imaging and complexity regularization	gaussian noise;poisson noise;optimisation;ill posed inverse problem;image coding;predictive coding statistical imaging statistical ill posed inverse problems image restoration poisson noise gaussian noise nonlinear transformations distortion measure image space nonasymptotic bounds estimation performance resolvability index image compressibility bounds observational models asymptotic imaging experiment convergence rates rate distortion theory optimal codebooks complexity regularized image estimators poisson imaging application;ultrasonic imaging extraterrestrial measurements additive white noise awgn contracts gaussian noise inverse problems distortion measurement image resolution image coding;image resolution;data compression;nonparametric estimation;convergence of numerical methods;image restoration;rate distortion theory;maximum likelihood estimate;statistical analysis;prediction theory;inverse problem;image compression;nonlinear transformation;computational complexity;statistics;image resolution statistical analysis image coding image restoration data compression parameter estimation gaussian noise poisson distribution inverse problems convergence of numerical methods rate distortion theory optimisation computational complexity prediction theory;minimum description length principle;parameter estimation;statistical analysis computational complexity image coding data compression inverse problems;compressibility statistical imaging complexity regularization statistical ill posed inverse problems distortion measure image space nonasymptotic bounds estimation performance index of resolvability;poisson distribution;inverse problems	We apply the complexity–regularization principle to statistical ill-posed inverse problems in imaging. The class of problems studied includes restoration of images corrupted by Gaussian or Poisson noise and nonlinear transforms. We formulate a natural distortion measure in image space and develop nonasymptotic bounds on estimation performance in terms of an index of resolvability that characterizes the compressibility of the true image. These bounds extend previous results that were obtained in the literature under simpler observational models. We present a connection between complexity-regularized estimation and rate-distortion theory, which suggests a method for constructing optimal codebooks. However, the design of computationally tractable complexity–regularized image estimators is quite challenging; we present some of the issues involved and illustrate them with a Poisson-imaging application.	acronis true image;circuit restoration;cobham's thesis;codebook;distortion;matrix regularization;nonlinear system;rate–distortion theory;signal-to-noise ratio;well-posed problem	Pierre Moulin;Juan Liu	2000	IEEE Trans. Information Theory	10.1109/18.857789	mathematical optimization;combinatorics;inverse problem;mathematics;statistics	Vision	54.40150342945032	-74.56089974168223	153997
5f1ffd275f80727ab9497595293e4ceeccfed248	local active content fingerprinting: optimal solution under linear modulation	modulation distortion awgn transform coding feature extraction transforms;awgn;transform coding;distortion;robustness and constrained projection active content fingerprint modulation local descriptor;feature extraction;transforms;modulation	This papers presents an analysis on Active Content Fingerprint (aCFP) for local (patch based) image descriptors. A generalization is proposed, the reduction of the aCFP with linear modulation to a constrained projection problem is shown and the optimal solution is given. The constrained projection problem addresses the linear modulation by a constraint on the properties of the resulting local descriptor. A computer simulation using local image patches, extracted from publicly available data sets is provided, demonstrating the advantages under several signal processing distortions.	computer simulation;distortion;fingerprint (computing);modulation;signal processing;visual descriptor	Dimche Kostadinov;Slava Voloshynovskiy;Maurits Diephuis;Taras Holotyak	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7533094	additive white gaussian noise;transform coding;distortion;feature extraction;computer science;theoretical computer science;pattern recognition;mathematics;modulation	Vision	58.475316565269246	-70.68976795625943	154235
ebd3f9b5597c513272c58121a0c60e51797283b5	robust image restoration via adaptive low-rank approximation and joint kernel regression	redundancy approximation methods sparse matrices image restoration dictionaries noise robustness;image restoration;redundancy;dictionaries;regression analysis approximation theory image denoising image restoration iterative methods learning artificial intelligence matrix algebra minimisation;robustness;super resolution task robust image restoration adaptive low rank approximation adaptive joint kernel regression algorithm corrupted matrix rank minimization nonlocal self similarity regional redundancy measure nonlocal patch rank measure adaptive image restoration method content awareness iterative outlier removal outlier handling denoise quality online dictionary learning scheme offline dictionary learning scheme image deblurring task;approximation methods;sparse matrices;noise	In recent years, image priors based on nonlocal self-similarity and low-rank approximation have been proven as powerful tools for image restoration. Many restoration methods group similar patches as a matrix and recover the underlying low-rank structure from the corrupted matrix via rank minimization. However, both the nonlocally redundant and low-rank properties are highly content dependent, and whether they can faithfully characterize a wide range of natural images still remains unclear. In this paper, we analyze these two properties and provide quantifications of them in a data-driven and parametric way, respectively, obtaining the new measures of regional redundancy and nonlocal patch rank. Leveraging these prior leads to an adaptive image restoration method with content-awareness. In particular, our method iteratively removes outliers and recovers latent fine details. To handle outliers, we propose an adaptive low-rank and sparse matrix approximation algorithm to encourage the estimated nonlocal rank in the patch matrix. The guidance of regional redundancy further gives rise to the “denoise” quality. In the detail recovery step, we propose an adaptive joint kernel regression algorithm using the redundancy measure to determine the confidence of each regression group. It also bridges the gap between our online and offline dictionary learning schemes. Experiments on synthetic and real-world images show the efficacy of our method in image deblurring and super-resolution tasks, especially when subject to practical outliers such as rain drops.	addresses (publication format);approximation algorithm;biologic preservation;circuit restoration;deblurring;dictionary [publication type];experiment;gradient;handling (psychology);image restoration;impulse noise (audio);kernel;low-rank approximation;machine learning;noise reduction;nonlocal lagrangian;normal statistical distribution;online and offline;pixel;principal component analysis;quantitation;quantum nonlocality;redundancy (information theory);self-similarity;singular value decomposition;sparse matrix;super-resolution imaging;synthetic intelligence;triple modular redundancy	Chen Huang;Xiaoqing Ding;Chi Fang;Di Wen	2014	IEEE Transactions on Image Processing	10.1109/TIP.2014.2363734	image restoration;computer vision;mathematical optimization;sparse matrix;computer science;noise;machine learning;pattern recognition;mathematics;redundancy;statistics;robustness	ML	58.11774874004659	-70.8213454836809	154862
9f57d0fc253999feed112970124bb55aa4c0ab0e	probability contour guided depth map inpainting and superresolution using non-local total generalized variation		This paper proposes an image-guided depth super-resolution framework to improve the quality of depth map captured by low-cost depth sensors, like the Microsoft Kinect. First, a contour-guided fast marching method is proposed to preprocess the raw depth map for recovering the missing data. Then, by using the non-local total generalized variation (NL-TGV) regularization, a convex optimization model is constructed to up-sample the preprocessed depth map to a high-resolution one. To preserve the sharpness of depth discontinuities, the color image and its multi-level segmentation information are utilized to assign the weights within the NL-TGV through a novel weight combining scheme. The texture energy from color image and local structure coherence around neighbor pixels in low-resolution depth map are applied to adjust the combination weights for further suppressing texture-transfer. Quantitative and qualitative evaluations of the proposed method on the Middlebury datasets and real-sensor datasets show the promising results in quality.	calculus of variations;color image;convex optimization;depth map;fast marching method;fast multipole method;image resolution;inpainting;kinect;lr parser;mathematical optimization;missing data;nl (complexity);pixel;preprocessor;sensor;structured-light 3d scanner;super-resolution imaging	Hai-Tao Zhang;Jun Yu;Zengfu Wang	2017	Multimedia Tools and Applications	10.1007/s11042-017-4791-x	computer vision;inpainting;artificial intelligence;computer science;missing data;pixel;pattern recognition;convex optimization;color image;segmentation;fast marching method;depth map	Vision	56.46530261418558	-70.36998104712792	155353
12c23aa977c739b82b64abade2169151f2c95a5a	hyperspectral image segmentation and unmixing using hidden markov trees	multiscale segmentation;abundance vectors;hidden markov tree;pixel hyperspectral imaging hidden markov models bayesian methods image segmentation signal to noise ratio estimation;joint bayesian endmember extraction;image segmentation;sticky hierarchical dirichlet process hyperspectral imaging multiscale segmentation;bayesian methods;indexing terms;abundance vectors hyperspectral image segmentation unmixing hidden markov trees joint bayesian endmember extraction;hyperspectral image segmentation;hidden markov trees;hidden markov models;estimation;unmixing;pixel;sticky hierarchical dirichlet process;image segmentation hidden markov models;hierarchical dirichlet process;hyperspectral imaging;signal to noise ratio;hyperspectral image	This paper is concerned with joint Bayesian endmember extraction and linear unmixing of hyperspectral images using a spatial prior on the abundance vectors. We hypothesize that hyperspectral images are composed of two types of regions. For the first type, the material proportions of adjacent pixels are similar and can be jointly characterized by a single vector, and in the second, neighboring pixels have very different abundances and are characterized by unique mixing proportions. Using this hypothesis we propose a new unmixing algorithm which simultaneously segments the image into such regions and performs unmixing. The experimental results show that the new algorithm can lead to improved MSE of both the extracted endmembers and the estimated abundances in low SNR cases.	algorithm;hidden markov model;image segmentation;markov chain;pixel;signal-to-noise ratio	Roni Mittelman;Alfred O. Hero	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5653020	computer vision;estimation;index term;bayesian probability;computer science;hyperspectral imaging;machine learning;pattern recognition;image segmentation;signal-to-noise ratio;pixel;hidden markov model;statistics;hierarchical dirichlet process	Robotics	61.463822780260564	-72.07155804256351	155371
0e0cddf06d5d02be911c573a91a8b122766a0bdd	a novel remote sensing image fusion algorithm using ica bases	discrete wavelet transforms;psnr;ica fusion remote sensing image fusion algorithm ica bases low resolution multispectral images ms images high resolution panchromatic image pan image stationary wavelet transform independent component analysis wavelet decomposition level correlation coefficient cc peak signal to noise ratio psnr mean structural similarity measure mssim edge stability mean square error esmse swt method ihs fusion pca fusion dwt fusion swt fusion;discrete wavelet transforms psnr principal component analysis spatial resolution;principal component analysis;wavelet transforms image fusion independent component analysis least mean squares methods principal component analysis remote sensing;wavelet analysis remote sensing image fusion mutlimodal image fusion stationary wavelet transform independent component analysis;spatial resolution	Image fusion is the method of gathering information from images obtained from sensors of different modalities in order to create an image that is more detailed than the input images. This paper presents a novel method for fusion of low-resolution multispectral (MS) images with a high-resolution panchromatic (PAN) image using Stationary Wavelet Transform (SWT) and Independent Component Analysis (ICA). Here, the MS image is modified using the PAN image at the wavelet decomposition level, followed by performing fast ICA algorithm on the individual bands, and thereby providing better resolution to the output image. In this paper, we also use parameters like correlation coefficient (CC), peak signal-to-noise ratio (PSNR), mean structural similarity measure (MSSIM) and edge stability mean square error (ESMSE) to measure the quality of the fused image. From experimental results, we observed that SWT method efficiently preserves spectral information, while ICA efficiently preserves spatial information. The proposed method is found to show better performance compared to other standard fusion methods like IHS fusion, PCA fusion, DWT fusion, SWT fusion and ICA fusion.	algorithm;coefficient;discrete wavelet transform;fast fourier transform;fastica;image fusion;image resolution;independent computing architecture;independent component analysis;information;mean squared error;multispectral image;peak signal-to-noise ratio;principal component analysis;sensor;similarity measure;standard widget toolkit;stationary process;stationary wavelet transform;structural similarity	C. S. Manu;C. V. Jiji	2015	2015 Eighth International Conference on Advances in Pattern Recognition (ICAPR)	10.1109/ICAPR.2015.7050690	computer vision;speech recognition;pattern recognition;mathematics;stationary wavelet transform;image fusion	Vision	58.99008722123409	-67.50427790410048	155611
7d77bea4f6df8cd7bcac98bd24b407d469628aeb	generalized lasso with under-determined regularization matrices	solution path;diagonally dominant;generalized lasso;robust lasso;deconvolution;total variation;lasso	This paper studies the intrinsic connection between a generalized LASSO and a basic LASSO formulation. The former is the extended version of the latter by introducing a regularization matrix to the coefficients. We show that when the regularization matrix is even- or under-determined with full rank conditions, the generalized LASSO can be transformed into the LASSO form via the Lagrangian framework. In addition, we show that some published results of LASSO can be extended to the generalized LASSO, and some variants of LASSO, e.g., robust LASSO, can be rewritten into the generalized LASSO form and hence can be transformed into basic LASSO. Based on this connection, many existing results concerning LASSO, e.g., efficient LASSO solvers, can be used for generalized LASSO.		Junbo Duan;Charles Soussen;David Brie;Jérôme Idier;Mingxi Wan;Yu-Ping Wang	2016	Signal processing	10.1016/j.sigpro.2016.03.001	mathematical optimization;computer science;deconvolution;diagonally dominant matrix;machine learning;lasso;mathematics;total variation;elastic net regularization;statistics	ML	56.02188294008008	-71.66600895144838	155913
769ec2f8370470c7a1ee5296309ec127824d79d5	algorithms for imaging inverse problems under sparsity regularization	constrained optimization;gaussian noise;poisson noise;compressed sensing;imaging convergence algorithm design and analysis vectors optimization deconvolution;inverse problems gaussian noise image reconstruction image restoration;image restoration;augmented lagrangian method;inverse problem;image reconstruction;vector optimization;unconstrained optimization imaging inverse problems sparsity regularization admm alternating direction method of multipliers augmented lagrangian methods standard image restoration standard image reconstruction linear observations poisson noise gaussian noise constrained optimization synthesis regularization;algorithm design;direct method;inverse problems	This paper reviews our recent work on the application of a class of techniques known as ADMM (alternating direction method of multipliers, which belongs to the family of augmented Lagrangian methods) to several imaging inverse problems under sparsity-inducing regularization. After reviewing ADMM, a formulation that allows handling problems with more than two terms is introduced; this formulation is then applied to a variety of problems, namely: standard image restoration/reconstruction from linear observations (e.g., compressive sensing, deconvolution, inpainting) with Gaussian or Poisson noise, using either analysis or synthesis regularization, and unconstrained or constrained optimization. We also show how the proposed framework can be used to address hybrid analysis-synthesis regularization. In all these cases, the proposed approach inherits the theoretic convergence guarantees of ADMM and achieve state-of-the-art speed.	algorithm;augmented lagrangian method;circuit restoration;compressed sensing;constrained optimization;convex optimization;deconvolution;image restoration;inpainting;mathematical optimization;matrix regularization;sparse matrix;the matrix;theory;tomographic reconstruction	Mário A. T. Figueiredo;Jos&#x00E9; M. Bioucas-Dias	2012	2012 3rd International Workshop on Cognitive Information Processing (CIP)	10.1109/CIP.2012.6232892	mathematical optimization;combinatorics;mathematical analysis;augmented lagrangian method;mathematics	ML	56.27045456989737	-72.63329215610413	157208
c2c30f6e948164e76717b9a15d3537fbfd5fc9ce	efficient total variation minimization methods for color image restoration	minimisation;libre mercado;television;color total variation minimization scheme;traitement signal;minimization algorithm;minimization;convergence;restauration image;methode essai;image processing;total variation minimization;color;image colour analysis image denoising image restoration minimisationcolor image restoration color total variation minimization scheme deblurred color image denoising minimization algorithm total variation minimization methods;procesamiento imagen;image restoration;color image restoration;total variation tv color image restoration deblurring denoising;traitement image;journal;marche concurrentiel;reduccion ruido;algorithme;algorithm;restauracion imagen;imagen borrosa;image color analysis;image colour analysis;blurred image;minimization algorithm total variation minimization methods color image restoration color total variation minimization scheme deblurred color image denoising;signal processing;noise reduction;minimization methods color image restoration layout mathematics convergence colored noise additive noise filters testing;test methods;reduction bruit;alternating minimization;total variation;image denoising;denoising;image floue;total variation tv;open market;test method;deblurred color image denoising;imagen color;procesamiento senal;algorithm design and analysis;image couleur;color image;deblurring;minimisation image colour analysis image denoising image restoration;total variation minimization methods;algoritmo;metodo ensayo	In this paper, we consider and study a total variation minimization model for color image restoration. In the proposed model, we use the color total variation minimization scheme to denoise the deblurred color image. An alternating minimization algorithm is employed to solve the proposed total variation minimization problem. We show the convergence of the alternating minimization algorithm and demonstrate that the algorithm is very efficient. Our experimental results show that the quality of restored color images by the proposed method are competitive with the other tested methods.	algorithm;appendix;arabic numeral 0;circuit restoration;color image;convergence (action);fixed point (mathematics);fixed-point number;image restoration;infinity;loss function;noise reduction;norm (social);optimization problem;social inequality;total variation diminishing	You-Wei Wen;Michael K. Ng;Yu-Mei Huang	2008	IEEE Transactions on Image Processing	10.1109/TIP.2008.2003406	computer vision;image processing;computer science;signal processing;noise reduction;mathematics;test method;computer graphics (images)	Vision	55.82703462820211	-72.24823027822438	157239
25d8aab0bf84eb2ab78451071062f8d5632dc979	a color image restoration with adjacent parallel lines inhibition	minimisation;image restoration colored noise image reconstruction approximation algorithms inverse problems image color analysis;approximation theory image colour analysis image restoration image denoising minimisation parameter estimation;image restoration;energy function;approximation theory;image colour analysis;interacting line elements deblurring color image restoration adjacent parallel line inhibition image blur image noise ill posed problem energy function minimization boolean variables intensity color field color intensity line variables estimation discontinuities convex approximation technique;image denoising;parameter estimation;color image	We focus our attention on the problem of restoring a color image corrupted by blur and noise. This problem is ill-posed in the Hadamard sense. By regularization techniques, it is possible to define the solution of the problem as the minimum of an energy function. To improve the quality of the reconstruction, some Boolean variables, associated with the discontinuities of the intensity color field, are introduced. Such variables, called line variables, have to be estimated and can be used in the energy function in an implicit way. Moreover, we impose that adjacent discontinuities have not to be parallel. To minimize the energy function, a CATILED (convex approximation technique for interacting line elements deblurring) algorithm is used. The experimental results confirm the good qualities of this technique.	circuit restoration;color image;image restoration	Ivan Gerace;Roberta Pandolfi	2003		10.1109/ICIAP.2003.1234081	demosaicing;color histogram;image texture;image restoration;computer vision;minimisation;mathematical optimization;feature detection;color image;image gradient;binary image;mathematics;color balance;estimation theory;statistics;approximation theory	Vision	57.83130673639326	-71.0428068912	157245
6de3bd6f55a60a5de9124c26598291fa0a9638e2	an expanded theoretical treatment of iteration-dependent majorize-minimize algorithms	minimisation;iterative method;sensitivity and specificity;traitement signal;minimization;sage;optimisation;optimization transfer;cost function signal processing algorithms image processing jacobian matrices signal processing statistics convergence algorithm design and analysis maximum likelihood estimation expectation maximization algorithms;convergence;image processing;optimizacion;cost function;image processing iteration dependent majorize minimize algorithms majorize minimize optimization tangent majorant function convergence condition upper curvature bounds expectation maximization algorithm signal processing;optimization technique;convergence of numerical methods;procesamiento imagen;funcion coste;traitement image;positron emission tomography;metodo iterativo;upper bound;minimizacion costo;image enhancement;minimisation cout;image interpretation computer assisted;cost minimization;expectation maximization;likelihood functions;methode iterative;signal processing;iteration dependent majorize minimize algorithms;reproducibility of results;expectation maximization algorithm;algorithms image enhancement image interpretation computer assisted likelihood functions positron emission tomography reproducibility of results sensitivity and specificity;ensemble convexe;signal processing convergence of numerical methods expectation maximisation algorithm image processing minimisation;signal and image processing;algorithme em;convergence condition;upper curvature bounds;sage expectation maximization em majorize minimize mm optimization transfer;fonction cout;algorithms;optimization;algoritmo em;convex set;signal processing algorithms;borne superieure;majorize minimize optimization;em algorithm;procesamiento senal;tangent majorant function;article;algorithm design and analysis;expectation maximization em;conjunto convexo;cota superior;majorize minimize mm;expectation maximisation algorithm	The majorize-minimize (MM) optimization technique has received considerable attention in signal and image processing applications, as well as in statistics literature. At each iteration of an MM algorithm, one constructs a tangent majorant function that majorizes the given cost function and is equal to it at the current iterate. The next iterate is obtained by minimizing this tangent majorant function, resulting in a sequence of iterates that reduces the cost function monotonically. A well-known special case of MM methods are expectation-maximization algorithms. In this paper, we expand on previous analyses of MM, due to Fessler and Hero, that allowed the tangent majorants to be constructed in iteration-dependent ways. Also, this paper overcomes an error in one of those earlier analyses. There are three main aspects in which our analysis builds upon previous work. First, our treatment relaxes many assumptions related to the structure of the cost function, feasible set, and tangent majorants. For example, the cost function can be nonconvex and the feasible set for the problem can be any convex set. Second, we propose convergence conditions, based on upper curvature bounds, that can be easier to verify than more standard continuity conditions. Furthermore, these conditions allow for considerable design freedom in the iteration-dependent behavior of the algorithm. Finally, we give an original characterization of the local region of convergence of MM algorithms based on connected (e.g., convex) tangent majorants. For such algorithms, cost function minimizers will locally attract the iterates over larger neighborhoods than typically is guaranteed with other methods. This expanded treatment widens the scope of the MM algorithm designs that can be considered for signal and image processing applications, allows us to verify the convergent behavior of previously published algorithms, and gives a fuller understanding overall of how these algorithms behave.	algorithm design;ct scan;converge;convex set;expectation–maximization algorithm;feasible region;image processing;iteration;iterative method;large;local convergence;loss function;mm algorithm;mathematical optimization;optimization problem;provable security;revision procedure;scientific publication;scott continuity;stationary process;tomography;verifying specimen	Matthew W. Jacobson;Jeffrey A. Fessler	2007	IEEE Transactions on Image Processing	10.1109/TIP.2007.904387	computer vision;econometrics;mathematical optimization;expectation–maximization algorithm;image processing;computer science;machine learning;mathematics;algorithm;statistics	ML	55.10777710726578	-73.4926396067486	157267
8b7695303477d8117ebf4c475a4c35f97734fefd	high resolution micro-computed tomography imaging and modelling of porous copper sample	meshing process μcomputed tomography image reconstruction;porous materials computational fluid dynamics computational geometry computerised tomography edge detection image reconstruction image resolution median filters mesh generation;image segmentation;meshing quality high resolution microcomputed tomography imaging porous copper metal modelling solid fluid contact heat transfer medium enhancement cooling density improvement x ray computed tomography μct scanning near exact image reconstruction metallic foams image enhancement techniques gaussian filter median filter edge detect filter image modelling image geometry external solid air pore structure representation internal solid air pore structure representation simpleware icem cfd tetragonal mesh surface mesh hexagonal mesh internal part pore features image models;geometry;materials;image enhancement;image reconstruction;image reconstruction copper image enhancement materials geometry image segmentation;copper	Porous metals have always been meticulously studied as they have a bright prospect in the future due to their extremely large solid-fluid contact that enhances heat transfer medium and improves cooling density. In this study, X-ray (Computed tomography) μCT-Scanning usually employed in medical imaging was used on 1.0 cm3 porous copper sample to permit near-exact reconstruction of the sample to be made and past studies applied on metallic foams. Then, sets of image enhancement techniques, “Gaussian”, “median” and “edge detect” filter were used to distinguish the sample from the noises that occur on the tomogram from the μCT-scanning such as “salt and pepper”. These steps of image modelling has allowed a more accurate geometry of the sample to be obtained compared to normal practices that assumes symmetrical model geometry for a porous material. A clear representation of the solid-air pore structure externally and internally was obtained. Meshing of this sample was done using SIMPLEWARE and refined using ICEM CFD. Due to the complexity of the sample structure, small pore size and high number of pore per inch (PPI) of the sample, a mix of tetragonal mesh for the surface and hexagonal mesh on the internal part of the sample was done and allowed the pore features to be preserved from the original image models during the meshing. However the current procedures using SIMPLEWARE only allowed a meshing quality of about 0.08751 to be obtained but this can be improved by refining the mesh parameters.	ct scan;computer cooling;high-resolution computed tomography;image editing;image resolution;median filter;medical imaging;pixel density	Mohamad Tzhaquib Fadhlullah Thafarallah;Wan Ahmad Najmi Wan Mohamed;Nor Amalina Nordin;Hairil Rashmizal bin Abdul Razak	2014	2014 IEEE International Conference on Control System, Computing and Engineering (ICCSCE 2014)	10.1109/ICCSCE.2014.7072764	crystallography;materials science;computer vision;engineering drawing	Robotics	56.177469267676926	-77.54357343889698	157889
75224c681d1595865f179c15cd94911ed47b8274	image super-resolution by tv-regularization and bregman iteration	iterative refinement;time dependent;high resolution;edge preserving;bregman iteration;low resolution;downsampling;total variation restoration;numerical scheme;super resolution;total variation;numerical experiment;variational models;spatial resolution	In this paper we formulate a new time dependent convolutional model for superresolution based on a constrained variational model that uses the total variation of the signal as a regularizing functional. We propose an iterative refinement procedure based on Bregman iteration to improve spatial resolution. The model uses a dataset of low resolution images and incorporates a downsampling operator to relate the high resolution scale to the low resolution one. We present an algorithm for the model and we perform a series of numerical experiments to show evidence of the good behavior of the numerical scheme and quality of the results.	algorithm;bregman divergence;decimation (signal processing);experiment;gaussian blur;ibm notes;image resolution;iteration;iterative refinement;matrix regularization;numerical analysis;numerical method;refinement (computing);super-resolution imaging;synthetic data;variational principle;white noise	Antonio Marquina;Stanley Osher	2008	J. Sci. Comput.	10.1007/s10915-008-9214-8	econometrics;mathematical optimization;image resolution;calculus;mathematics	Vision	56.50121082137936	-72.10246867065675	158043
44fb59b8a426351d4d37eec8ad2083d05f158626	recovery of image blocks using the method of alternating projections	feature extraction image restoration video coding hilbert spaces image resolution interpolation correlation methods discrete cosine transforms;image and video transmission;proyeccion ortogonal;espace hilbert;traitement signal;interpolation;detection forme;spatial domain interpolation algorithm image block recovery alternating projection method block loss restoration block based image video coding hilbert space restored damaged pixels orthogonal projection hybrid edge based average median interpolation technique interblock correlation interpolation approach fast discrete cosine transformation jpeg image peak signal to noise ratio image restoration;image coding;restauration image;error concealment;espacio hilbert;image processing;image resolution;hilbert spaces;video signal processing;edge detection;information transmission;correction erreur;transformation cosinus discrete;interpolacion;simulation;projection method;compresion senal;procesamiento imagen;simulacion;projection onto convex sets;image restoration;endommagement;spatial structure;correlation methods;shape detection;traitement image;deterioracion;alternating projections;discrete cosine transform;compression signal;deteccion contorno;algorithme;hilbert space;algorithm;detection contour;restauracion imagen;codage image;video coding;reconstruction image;deteccion forma;compression image;senal video;methode projection;signal video;algorithms artificial intelligence computer graphics image enhancement image interpretation computer assisted information storage and retrieval numerical analysis computer assisted pattern recognition automated reproducibility of results sensitivity and specificity signal processing computer assisted;image compression;codage video;image transmission;reconstruccion imagen;jpeg;discrete cosine transforms;projections onto convex sets pocs;error correction;feature extraction;image reconstruction	"""A technique for block-loss restoration in block-based image and video coding, dubbed recovery of image blocks using the method of alternating projections (RIBMAP), is developed. The algorithm is based on orthogonal projections onto constraint sets in a Hilbert space. For the recovery of a linear dimension N size block, a total of 8N vectors are extracted from the surrounding area of an N /spl times/ N missing block. These vectors form a library from which the best matching spatial information for the missing block is extracted. Recovery vectors, including both undamaged and restored damaged pixels, are introduced. The vectors are used to find highly correlated information relating to the lost pixels. To assure continuity with the surrounding undamaged area, three additional convex constraints are formulated. Adherance to these sets is imposed using alternating projections. Simulation results using orthogonal projections demonstrate that RIBMAP recovers spatial structure faithfully. Simulation comparisons with other procedures are presented: Ancis and Giusto's hybrid edge-based average-median interpolation technique, Sun and Kwok's projections onto convex sets-based method, Hemami and Meng's interblock correlation interpolation approach, Shirani et al.'s modified interblock correlation interpolation scheme, and Alkachouh and Bellanger's fast discrete cosine transformation-based spatial domain interpolation algorithm. Characteristic of the results are those of the """"Lena"""" JPEG image when one fourth of periodically spaced blocks in the image have errors. The peak signal-to-noise ratio of the restored image is 28.68, 29.99, 31.86, 31.69, 31.57, and 34.65 dB using that of Ancis and Giusto, Sun and Kwok, Hemami and Meng, Shirani et al., Alkachouh and Bellanger, and RIPMAP, respectively."""	algorithm;anisotropic filtering;atrioventricular block;block size (cryptography);circuit restoration;convex set;data compression;discrete cosine transform;extraction;hilbert space;interpolation imputation technique;jpeg;lenna;library (computing);matching;masks;moving picture experts group;peak signal-to-noise ratio;pixel;projections and predictions;scott continuity;simulation;whittaker–shannon interpolation formula	Jiho Park;Dong-Chul Park;Robert J. Marks;Mohamed A. El-Sharkawi	2005	IEEE Transactions on Image Processing	10.1109/TIP.2004.842354	computer vision;mathematical optimization;image processing;interpolation;computer science;theoretical computer science;signal processing;mathematics;hilbert space	Vision	58.50054866426883	-66.19161371264154	158341
1474475e62791e30a393f8bbf76aa10c2598437b	quantitative analysis of nonlinear embedding	nonlinear embedding dimensionality reduction manifold learning;high dimensionality;manifolds;nickel;manifold learning quantitative analysis nonlinear embedding embedding quality neighboring preservation rate;evaluation method;manifold learning;dimensionality reduction;vectors;matrix decomposition;data visualization;quantitative analysis;intrinsic low dimensional manifold;manifolds vectors jacobian matrices algorithm design and analysis matrix decomposition data visualization;algorithms artificial intelligence computer simulation nonlinear dynamics;learning artificial intelligence;jacobian matrices;dimensional reduction;nonlinear embedding;algorithm design;algorithm design and analysis;quantitative evaluation	A lot of nonlinear embedding techniques have been developed to recover the intrinsic low-dimensional manifolds embedded in the high-dimensional space. However, the quantitative evaluation criteria are less studied in literature. The embedding quality is usually evaluated by visualization which is subjective and qualitative. The few existing evaluation methods to estimate the embedding quality, neighboring preservation rate for example, are not widely applicable. In this paper, we propose several novel criteria for quantitative evaluation, by considering the global smoothness and co-directional consistence of the nonlinear embedding algorithms. The proposed criteria are geometrically intuitive, simple, and easy to implement with a low computational cost. Experiments show that our criteria capture some new geometrical properties of the nonlinear embedding algorithms, and can be used as a guidance to deal with the embedding of the out-of-samples.	algorithm;algorithmic efficiency;biologic preservation;cluster analysis;computation;crowding;embedding;global optimization;imagery;mycosis fungoides;nonlinear programming;nonlinear system;radial basis function;scientific visualization;silo (dataset);subject reduction;trusted computer system evaluation criteria;usability;statistical cluster	Junping Zhang;Qi Wang;Li He;Zhi-Hua Zhou	2011	IEEE Transactions on Neural Networks	10.1109/TNN.2011.2171991	algorithm design;mathematical optimization;topology;computer science;machine learning;mathematics;data visualization	Vision	58.75107531771989	-70.67328833101854	158522
c22081decc16431d4b681ece1d745cdd22e19c55	enhancing signal discontinuities with shearlets: an application to corner detection		Shearlets are a relatively new and very effective multi- resolution framework for signal analysis able to capture efficiently the anisotropic information in multivariate problem classes. For this reason, Shearlets appear to be a valid choice for multi-resolution image process- ing and feature detection. In this paper we provide a brief review of the theory, referring in particular to the problem of enhancing signal discontinuities. We then discuss the specific application to corner detec- tion, and provide a novel algorithm based on the concept of a cornerness measure. The appropriateness of the algorithm in detecting good match- able corners is evaluated on benchmark data including different image transformations.	corner detection	Miguel A. Duval;Francesca Odone;Ernesto De Vito	2015		10.1007/978-3-319-23234-8_11	speech recognition;computer science;artificial intelligence;algorithm	Vision	60.67707206723506	-67.14219640230252	158537
2a9630b927fb5db6806fb664a808fcaf1f13ed9d	minimizing the total variation under a general convex constraint for image restoration	transformation ondelette;gaussian noise;image restoration mathematical model signal restoration wavelet packets dictionaries mathematical analysis deconvolution focusing gaussian noise maximum a posteriori estimation;restauration image;image processing;image denoising total variation minimization general convex constraint image restoration variational approach wavelet approach deconvolution mathematical analysis gaussian noise;variational techniques;restoration;procesamiento imagen;image denoising wavelet transforms variational techniques image restoration gaussian noise;image;estimation a posteriori;image restoration;mathematical analysis;wavelet packet;traitement image;a posteriori estimation;desconvolucion;reduccion ruido;algorithme;wavelet transforms;algorithm;restauracion imagen;estimacion a posteriori;noise reduction;reduction bruit;deconvolution;total variation;image denoising;transformacion ondita;signal;maximum a posteriori;generalized convexity;wavelet;wavelet transformation;deblurring;algoritmo	In this paper, we present a general framework for image restoration; despite its simplicity, certain variational and certain wavelet approaches can be formulated within this framework. This permits the construction of a natural model, with only one parameter, which has the advantages of both approaches. We give a mathematical analysis of this model, describe our algorithm and illustrate this by some experiments.	algorithm;circuit restoration;experiment;image restoration;license;mathematics;population parameter;variational principle;wavelet	François Malgouyres	2002	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/TIP.2002.806241	gaussian noise;wavelet;image restoration;computer vision;mathematical optimization;image processing;computer science;deconvolution;maximum a posteriori estimation;image;pattern recognition;noise reduction;mathematics;total variation;statistics;wavelet transform;signal	Vision	55.42594332878268	-72.45423164434143	158776
15f8cf26521b5fe058fa4093a39fa640b04737b6	synthetic aperature ladar: a model-based approach		Synthetic aperture LADAR (SAL) allows high resolution imaging of distant objects. Basic SAL image processing is based on fast Fourier transform (FFT) techniques originally developed for use in radar. These techniques can amplify noise and limit resolution. More advanced reconstruction techniques have been proposed for synthetic aperture radar (SAR), but have not been adapted for SAL. In addition, both conventional SAL and advanced SAR algorithms reconstruct the complex-valued reflection coefficient instead of the real-valued reflectance which leads to speckled images. In this paper, we present a model-based iterative reconstruction (MBIR) algorithm designed specifically for SAL. Rather than estimating the reflection coefficient, we propose estimating its variance which is equal to the reflectance, a function that more closely resembles conventional optical images. A Bayesian framework is used to find the maximum a posteriori (MAP) estimate for the reflectance using a Q-Generalized Gaussian Markov random field (QGGMRF) prior model. The QGGMRF is able to model complex correlations between neighboring pixels which promotes a smooth and more natural looking image. The expectation-maximization (EM) algorithm is used to derive a surrogate for the MAP cost function. Finally, the proposed MBIR algorithm is tested on both simulated and experimental data. Results show significant and consistent improvements over existing reconstruction techniques in terms of image contrast, speckle reduction, autofocus, and low signal-to-noise ratio performance.	aperture (software);expectation–maximization algorithm;fast fourier transform;image processing;image resolution;iterative method;iterative reconstruction;loss function;markov chain;markov random field;numerical aperture;pixel;radar;reflection coefficient;signal-to-noise ratio;synthetic intelligence	Casey J. Pellizzari;Russell E. Trahan;Hanying Zhou;Skip Williams;Stacie E. Williams;Bijan Nemati;Michael Shao;Charles A. Bouman	2017	IEEE Transactions on Computational Imaging	10.1109/TCI.2017.2663320	iterative reconstruction;artificial intelligence;mathematics;experimental data;fast fourier transform;inverse synthetic aperture radar;image processing;computer vision;maximum a posteriori estimation;synthetic aperture radar;reflection coefficient	Vision	61.265466026397156	-71.14294351701075	158814
af2fd13aae3a0239227d90fb599bbb06cbfd4146	an adaptive directional haar framelet-based reconstruction algorithm for parallel magnetic resonance imaging	proximity operator;65t60;65k10;tight frame;68u10;parallel mri;total variation;haar wavelet system	Parallel magnetic resonance imaging (pMRI) is a technique to accelerate the magnetic resonance imaging process. The problem of reconstructing an image from the collected pMRI data is ill-posed. Regularization is needed to make the problem well-posed. In this paper, we first construct a 2-dimensional tight framelet system whose filters have the same support as the orthogonal Haar filters and are able to detect edges of an image in the horizontal, vertical, and ±45 directions. This system is referred to as directional Haar framelet (DHF). We then propose a pMRI reconstruction model whose regularization term is formed by the DHF. This model is solved by a fast proximal algorithm with low computational complexity. The regularization parameters are updated adaptively and determined automatically during the iteration of the algorithm. Numerical experiments for in-silico and in-vivo data sets are provided to demonstrate the superiority of the DHF-based model and the efficiency of our proposed algorithm for pMRI reconstruction.	algorithm;computational complexity theory;distributed hash table;experiment;frame (linear algebra);haar wavelet;iteration;matrix regularization;numerical method;resonance;video-in video-out;wavelet transform;well-posed problem	Yan-Ran Li;Raymond H. Chan;Lixin Shen;Yung-Chin Hsu;W Y I Tseng	2016	SIAM J. Imaging Sciences	10.1137/15M1033964	mathematical optimization;mathematical analysis;machine learning;mathematics;geometry;total variation	ML	56.74685315466864	-74.00507865485406	158837
07f2759ed354918b2f600244a92537f2019cd6fd	image reconstruction by parametric cubic convolution	frequency response;cubic equations;image reconstruction;numerical method;interpolation;image processing	A parametric implementation of cubic convolution image reconstruction is presented which is generally superior to the standard algorithm and which can be optimized to the frequency content of the image.	convolution;cubic function;iterative reconstruction	Stephen K. Park;Robert A. Schowengerdt	1982	Computer Graphics and Image Processing	10.1016/0146-664X(82)90063-6	iterative reconstruction;computer vision;mathematical optimization;frequency response;discrete mathematics;template matching;image gradient;image processing;numerical analysis;interpolation;free boundary condition;bicubic interpolation;mathematics;cubic function;anisotropic diffusion;kernel;image scaling	Graphics	55.63824078644105	-68.37808528222003	158944
d61185aa56448e11ed331614b9115f5d6c06f4ee	an image super resolution reconstruction algorithm based on undecimated morphological wavelet	undecimated morphological wavelet umw;interpolation;image reconstruction image resolution signal resolution signal processing algorithms wavelet transforms maximum likelihood detection;wavelet transforms image reconstruction image resolution;coefficient sequence image superresolution reconstruction algorithm undecimated morphological wavelet horizontal domain information vertical domain information diagonal domain information;image registration;super resolution reconstruction;image registration super resolution reconstruction undecimated morphological wavelet umw interpolation	The paper proposes an image super resolution reconstruction (SRR) algorithm based on Undecimated Morphological Wavelet (UMW). As UMW can maintain more gradient information and eliminate more artificial blocks, it is used to decompose low resolution (LR) images. High frequency coefficients from the decomposition contain the gradients that reflect the horizontal, vertical, and diagonal domain information. Then both the low frequency and high frequency coefficients are interpolated to produce an extended group of coefficient sequence. Finally, the extended coefficients are fused and using inverse transformation to produce a super resolution image. Experimental results show that that the proposed algorithm have good performance in the image reconstruction quality and the computational efficiency.	algorithm;coefficient;gradient descent;image resolution;interpolation;iterative reconstruction;lr parser;simpl;stationary wavelet transform;super-resolution imaging	Wei Liu;Yongsheng Liang;Zeng-Bin Chen;Jihong Zhang	2015	2015 IEEE International Conference on Digital Signal Processing (DSP)	10.1109/ICDSP.2015.7251944	computer vision;mathematical optimization;pattern recognition;mathematics	Robotics	58.38326914211059	-66.76180320571552	159044
b974acde6cdcbc9569352a7ad9d87a7fc6c3edac	hyper-spectral impulse denoising: a row-sparse blind compressed sensing formulation	noise correlation manganese robustness magnetic resonance imaging image reconstruction electroencephalography;dictionary learning hyper spectral denoising impulse noise compressed sensing;manganese;image reconstruction;magnetic resonance imaging;robustness;correlation;electroencephalography;split bregman approach hyperspectral impulse denoising row sparse blind compressed sensing bcs impulse noise intraband spatial correlation interband spectral correlation hyperspectral images gaussian denoising dictionary learning techniques;spectral analysis compressed sensing image denoising impulse noise optimisation;noise	This paper addresses the problem of impulse denoising from hyper-spectral images. Impulse noise is sparse; removing impulse noise requires minimizing an l1-norm data fidelity term. Prior studies have exploited the intra-band spatial correlation (leading to sparsity in transform domain) and inter-band spectral-correlation (joint-sparsity) of hyper-spectral images for Gaussian denoising. In this work, we propose to learn the joint-sparsity promoting dictionary adaptively from the data for impulse denoising problems. Unlike dictionary learning techniques, the sparsifying dictionary is not learnt in an offline training phase. We follow the Blind Compressed Sensing (BCS) framework - dictionary learning and denoising proceeds simultaneously. The optimization problem that arises out of our formulation is solved using the Split Bregman approach. The proposed algorithm, when compared against prior techniques (on real hyper-spectral datasets) shows more than 5dB improvement in PSNR on average.	algorithm;bregman divergence;compressed sensing;dictionary;gaussian blur;impulse noise (audio);infinite impulse response;machine learning;mathematical optimization;noise reduction;online and offline;optimization problem;peak signal-to-noise ratio;sparse matrix;taxicab geometry	Angshul Majumdar;Naushad Ansari;Hemant Kumar Aggarwal	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178172	basis pursuit denoising;speech recognition;electroencephalography;noise;manganese;magnetic resonance imaging;machine learning;pattern recognition;mathematics;video denoising;correlation;robustness	Vision	57.698231041491084	-71.65354938391359	159176
702ed4b9c007c5fd77894f1a00863302e43b65a9	robust image deblurring using hyper laplacian model	deblurring process;blind image;inaccurate kernel;blur kernel;blind image deblurring;hyper laplacian model;input kernel;real-world image;robust image;non-gaussian noise;clear image;blurry image	In recent years, many image deblurring algorithms have been proposed, most of which assume the noise in the deblurring process satisfies the Gaussian distribution. However, it is often unavoidable in practice both in non-blind and blind image deblurring, due to the error on the input kernel and the outliers in the blurry image. Without proper handing these outliers, the recovered image estimated by previous methods will suffer severe artifacts. In this paper, we mainly deal with two kinds of non-Gaussian noise in the image deblurring process, inaccurate kernel and compressed blurry image, and find that handling the noise as Laplacian distribution can get more robust result in these cases. Based on this point, the new non-blind and blind image deblurring algorithms are proposed to restore the clear image. To get more robust deblurred result, we also use 8 direction gradients of the image to estimate the blur kernel. The new minimization problem can be efficiently solved by the Iteratively Reweighted Least Squares(IRLS) and the experimental results on both synthesized and real-world images show the efficiency and robustness of our algorithm.	deblurring	Yuquan Xu;Xiyuan Hu;Silong Peng	2012		10.1007/978-3-642-37484-5_5	image restoration;computer vision;mathematical optimization;pattern recognition;mathematics	Vision	58.93914951386207	-69.93345838882239	159245
f64149fe7396647689d4f6e31394f0807e4ac911	a new interband multiwavelet decomposition for exact coding of multicomponent images	image resolution;earth;image simulation;earth image resolution;perfect reconstruction	In this paper, we are interested in using multiwavelet transforms in the context of progressive and lossless coding of multi component images. More precisely, multiwavelet decompositions that map integers to integers are considered since they guarantee a perfect reconstruction in the absence of quantizers. Generally, these decompositions are performed separately on each spectral component of a multicomponent image. Therefore, they fail to exploit the spectral redundancies. Our main contribution in this paper consists in modifying multiwavelet decompositions in order to take into account simultaneously the spatial and the spectral redundancies contained in a multicomponent image. Simulation tests carried out on natural multicomponent images show that the the generalized interband decomposition outperforms the state-of-art lossless coders.	lossless compression;simulation;singular value decomposition	Amel Benazza-Benyahia;Jean-Christophe Pesquet;Noura Azzabou	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5745414	computer vision;mathematical optimization;image resolution;theoretical computer science;mathematics;earth	Robotics	60.06641050453136	-67.80992054352977	160219
a1982c4b8d96456011095b28d63875e06f265057	improved total variation-based image reconstruction algorithm for linear scan cone-beam computed tomography	computed tomography;reconstruction algorithms;image restoration;distortion;algorithms	Linear scan cone-beam computed tomography (LSCBCT) is a technically simple computed tomography (CT) configuration and is powerful enough to inspect long objects. As one kind of limitedangle problem, the image reconstruction of LSCBCT is ill posed. However, the total variation minimization (TVM)-projection on convex sets (POCS) reconstruction algorithm, which is based on the TVM and POCS, has been proven effective for solving the limited-angle problem. While applying the TVM-POCS algorithm to the LSCBCT reconstruction, the reconstructed image is distorted near the edges of the object. To solve this problem, an improved iterative reconstruction algorithm was developed. The improved method integrated simultaneous algebraic reconstruction technique, TVM, and C-V model. The C-V model can detect objects whose boundaries are not necessarily defined by gradient. The developed algorithm can reduce artifacts by piecewise constant modification and get a more accurate image. Numerical simulations are presented to illustrate the effectiveness of the algorithm. Moreover, the developed algorithm can be applied to other x-ray CT reconstruction problems. © 2013 SPIE and IS&T [DOI: 10.1117/1.JEI.22.3.033015]	algorithm;bregman method;ct scan;cone beam computed tomography;convex set;gradient;iterative method;iterative reconstruction;numerical analysis;numerical linear algebra;simulation;simultaneous algebraic reconstruction technique;v-model;well-posed problem	Wei Yu;Li Zeng;Baodong Liu	2013	J. Electronic Imaging	10.1117/1.JEI.22.3.033015	iterative reconstruction;image restoration;computer vision;mathematical optimization;distortion;telecommunications;computer science;algebraic reconstruction technique;mathematics;geometry;computed tomography;tomographic reconstruction	Vision	56.759540409907764	-74.07268130515367	160340
0226aa82ee4d0dc7c184946ae626d7aebbf4b14a	quality improvement of compressed color images by model-based chrominance estimation	mrf;quality improvement;estimation algorithm;markov random field;compressed color images;jpeg;chrominance estimation;jpeg2000;human vision system;conditional probability;density functional;color image	In compressed color images, colors are usually represented by luminance and chrominance components. Considering characteristics of human vision system, chrominance components are generally represented more coarsely than luminance component. Aiming at possible recovery of chrominance components, we propose a model-based chrominance estimation algorithm where color images are modeled by a Markov random field. Chrominance components of a pixel are estimated by maximizing a conditional probability density function given its luminance component and its neighboring color vectors. Experimental results show that the proposed chrominance estimation algorithm is effective for quality improvement of compressed color images such as JPEG and JPEG2000.		Hideki Noda;Shun Haraguchi;Michiharu Niimi	2009		10.1007/978-3-642-10467-1_127	rgb color model;computer vision;quality management;materials recovery facility;color image;conditional probability;computer science;operating system;chrominance;pattern recognition;jpeg;jpeg 2000;mathematics;statistics;computer graphics (images)	Vision	59.941583633668266	-69.17738307742277	160343
b30e0884d38ee4462f941e85d51ad61aa0917b08	generalized em-type reconstruction algorithms for emission tomography	resolution noise tradeoff generalized em type reconstruction algorithms emission tomography reconstruction estimators shepp maximum likelihood estimator vardi maximum likelihood estimator quadratic weighted least squares estimator anderson wls estimator liu multiobjective estimator wang multiobjective estimator surrogate function expectation maximization de pierro trick convergence proof theoretical analysis cost function monotonic algorithms self constraining algorithms convergent algorithms image quality;regularization technique;q measurement;convergence;surrogate function;image resolution;surrogate function global convergence kuhn tucker kt conditions regularization technique;cost function;convergence cost function image reconstruction algorithm design and analysis maximum likelihood estimation q measurement reconstruction algorithms;reconstruction algorithms;global convergence;maximum likelihood estimation;journal;image reconstruction;medical image processing;emission tomography;medical image processing convergence emission tomography expectation maximisation algorithm image denoising image reconstruction image resolution maximum likelihood estimation;image denoising;algorithm design and analysis;kuhn tucker kt conditions;expectation maximisation algorithm	We provide a general form for many reconstruction estimators of emission tomography. These estimators include Shepp and Vardi's maximum likelihood (ML) estimator, the quadratic weighted least squares (WLS) estimator, Anderson's WLS estimator, and Liu and Wang's multi-objective estimator, and others. We derive a generic update rule by constructing a surrogate function. This work is inspired by the ML-EM (EM, expectation maximization), where the latter naturally arises as a special case. A regularization with a specific form can also be incorporated by De Pierro's trick. We provide a general and quite different convergence proof compared with the proofs of the ML-EM and De Pierro. Theoretical analysis shows that the proposed algorithm monotonically decreases the cost function and automatically meets nonnegativity constraints. We have introduced a mechanism to provide monotonic, self-constraining, and convergent algorithms, from which some interesting existing and new algorithms can be derived. Simulation results illustrate the behavior of these algorithms in term of image quality and resolution-noise tradeoff.	appendix;approximation algorithm;converge;electron microscopy;expectation–maximization algorithm;experiment;feasible region;generic drugs;image quality;inspiration function;integer (number);least squares;loss function;mandibular right second molar tooth;matrix regularization;numerical analysis;numerous;operating system;ordered pair;positive integer;reconstruction conjecture;simulation;social inequality;t-lymphocyte subsets;tomography;type inference	Yueyang Teng;Tie Zhang	2012	IEEE Transactions on Medical Imaging	10.1109/TMI.2012.2197758	iterative reconstruction;algorithm design;econometrics;mathematical optimization;radiology;image resolution;convergence;computer science;mathematics;maximum likelihood;statistics	ML	55.3379578672479	-73.55393952494204	160411
bd1cd34e145b92a130072a9bfc9c5a8d669e5057	volume measurement based tensor completion	minimization;tensile stress;admm tensor volume tensor completion low rank matrix nuclear norm;indexes;current measurement;robustness;volume measurement;optimization;tensile stress volume measurement robustness current measurement minimization optimization indexes	This paper presents a new tensor completion method named minimum volume constraint tensor completion. Unlike the nuclear norm penalization based methods, our method extends the conception of the matrix volume to the tensor volume, and uses the volume measurement as the penalization to address the tensor completion problem. The alternating direction method of multipliers (ADMM) algorithm is then employed to solve the optimization problem of the proposed model. Experimental results on several popular databases show superior performance of our method compared to the nuclear norm penalization based methods in terms of the accuracy and robustness.	algorithm;augmented lagrangian method;database;mathematical optimization;optimization problem;penalty method;the matrix	Jianchun Xie;Jian Yang;Ying Tai;Jianjun Qian	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532676	database index;mathematical optimization;computer science;calculus;mathematics;geometry;stress;robustness	Robotics	55.978431567307815	-73.0467017627106	160543
d04dbe72ee76eca87aa9fb632d271cd45d56b53c	image inpainting using directional tensor product complex tight framelets		Different from an orthonormal basis, a tight frame is an overcomplete and energy-preserving system with flexibility and redundancy. Many image restoration methods employing tight frames have been developed and studied in the literature. Tight wavelet frames have been proven to be useful in many applications. In this paper we are particularly interested in the image inpainting problem using directional complex tight wavelet frames. Under the assumption that frame coefficients of images are sparse, several iterative thresholding algorithms for the image inpainting problem have been proposed in the literature. The outputs of such iterative algorithms are closely linked to solutions of several convex minimization models using the balanced approach which simultaneously combines the l1-regularization for sparsity of frame coefficients and the l2-regularization for smoothness of the solution. Due to the redundancy of a tight frame, elements of a tight frame could be highly correlated and therefore, their corresponding frame coefficients of an image are expected to close to each other. This is called the grouping effect in statistics. In this paper, we establish the grouping effect property for frame-based convex minimization models using the balanced approach. This result on grouping effect partially explains the effectiveness of models using the balanced approach for several image restoration problems. Since real-world natural images usually have two layers consisting of cartoons and textures, methods using simultaneous cartoon and texture inpainting are popular in the literature by using two combined tight frames: one tight frame (often built from wavelets, curvelets or shearlets) provides sparse representations for cartoons, and the other tight frame (often built from discrete cosine transform) offers sparse approximation for textures. Inspired by recent development on directional tensor product complex tight framelets (TP-CTFs) and their impressive performance for the image denoising problem, in this paper we propose an iterative thresholding algorithm using a single tight frame derived from TP-CTFs for the image inpainting problem. Experimental results show that our proposed algorithm can handle well both cartoons and textures simultaneously and performs comparably and often better than several well-known frame-based iterative thresholding algorithms for the image inpainting problem without noise. For the image inpainting problem with additive zero-mean i.i.d. Gaussian noise, our proposed algorithm using TP-CTFs performs superior than other known state-of-the-art frame-based image inpainting algorithms.	algorithm;circuit restoration;code;coefficient;convex optimization;discrete cosine transform;elastic net regularization;expect;frame (linear algebra);image processing;image restoration;inpainting;iterative method;matlab;mpls-tp;noise reduction;numerical method;sparse approximation;sparse matrix;thresholding (image processing);tight binding;utility functions on indivisible goods;wavelet	Yi Shen;Bin Han;Elena Braverman	2014	CoRR		computer vision;mathematical optimization;machine learning;mathematics	Vision	57.220273820374544	-70.85321050108755	161043
2b04a4c6f5569533bda66f75b09029cde96f91b4	poisson image reconstruction with total variation regularization	photonics;poisson noise;convergence;optimization framework;partition based regularization poisson image reconstruction optimization framework nonnegative image intensities linear projections poisson noise poisson inverse problems medical imaging total variation regularization term piecewise smooth objective function regularized quadratic surrogate natural nonnegativity constraints computational tractability wavelet sparsity;piecewise smooth;computational tractability;poisson inverse problems;total variation regularization term;convex optimization;poisson image reconstruction;natural nonnegativity constraints;sparse approximation;nonnegative image intensities;total variation regularization;indexing terms;satisfiability;photon limited imaging;objective function;linear projections;inverse problem;wavelet sparsity;medical image;partition based regularization;image reconstruction;medical imaging;spirals;regularized quadratic surrogate;sparse approximation photon limited imaging poisson noise total variation convex optimization;total variation;optimization;approximation methods;tomography;inverse problems;inverse problems image reconstruction;image reconstruction optimization spirals photonics tomography approximation methods convergence	This paper describes an optimization framework for reconstructing nonnegative image intensities from linear projections contaminated with Poisson noise. Such Poisson inverse problems arise in a variety of applications, ranging from medical imaging to astronomy. A total variation regularization term is used to counter the ill-posedness of the inverse problem and results in reconstructions that are piecewise smooth. The proposed algorithm sequentially approximates the objective function with a regularized quadratic surrogate which can easily be minimized. Unlike alternative methods, this approach ensures that the natural nonnegativity constraints are satisfied without placing prohibitive restrictions on the nature of the linear projections to ensure computational tractability. The resulting algorithm is computationally efficient and outperforms similar methods using wavelet-sparsity or partition-based regularization.	algorithm;algorithmic efficiency;computation;iterative reconstruction;loss function;mathematical optimization;matrix regularization;medical imaging;optimization problem;sparse matrix;total variation denoising;wavelet;well-posed problem	Rebecca M Willett;Zachary T. Harmany;Roummel F. Marcia	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5649600	medical imaging;mathematical optimization;combinatorics;mathematical analysis;convex optimization;computer science;inverse problem;mathematics;tomography	Vision	56.35126401829155	-73.17072061288641	161222
277aa36ceb735a8858efc521556c56552a70ddad	minimizing tgv-based variational models with non-convex data terms		We introduce a method to approximately minimize variational models with Total Generalized Variation regularization (TGV) and non-convex data terms. Our approach is based on a decomposition of the functional into two subproblems, which can be both solved globally optimal. Based on this decomposition we derive an iterative algorithm for the approximate minimization of the original non-convex problem. We apply the proposed algorithm to a state-of-the-art stereo model that was previously solved using coarse-to-fine warping, where we are able to show significant improvements in terms of accuracy.	approximation algorithm;benchmark (computing);calculus of variations;convex function;convex optimization;decomposition (computer science);iterative method;lifting scheme;matrix regularization;maxima and minima;variational principle	Rene Ranftl;Thomas Pock;Horst Bischof	2013		10.1007/978-3-642-38267-3_24	mathematical optimization;iterative method;regular polygon;image warping;mathematics;regularization (mathematics)	ML	53.786613612963826	-71.78672022316552	161579
bf9f8918a940daea9b628bd2e69db6302a9cba43	multi-sensor image fusion with scdpt transform	average gradient image fusion scdpt the structure similarity regional energy;wavelet transforms gradient methods image fusion inverse transforms;image fusion;wavelet transforms;laplace equations;image edge detection;filter banks;shiftable complex directional pyramid transform multi sensor image fusion algorithm source image decomposition low pass sub band coefficients band pass directional sub band coefficients structural similarity ssim regional energy regional average gradient regional variance scdpt inverse transform wavelet transform laplacian pyramid transform gradient pyramid transform image information contour feature details contour texture details;image fusion image edge detection wavelet transforms filter banks laplace equations	A multi-sensor image fusion algorithm based on SCDPT transform is proposed in this paper. SCDPT transform is used to decompose source images in each scale and direction to get low-pass sub-band coefficients and band-pass directional sub-band coefficients. The principle for low-pass sub-band coefficients is based on structural similarity (SSIM), regional energy and regional average gradient, while the principle for directional band-pass sub-band coefficients is based on SSIM and regional variance. Finally, fused image is obtained by SCDPT inverse transform. The proposed method is compared to the wavelet transform, Laplacian pyramid transform and gradient pyramid transform. Our algorithm not only has more flexible directional and shift invariance, but also is able to accurately capture the image information of the contour feature and texture details.	algorithm;coefficient;feature (computer vision);gradient;image fusion;image texture;laplacian matrix;low-pass filter;sensor;source-to-source compiler;structural similarity;wavelet transform	Qian Hu;Junping Du;Pengcheng Han;Qingping Li;Zhenghong Zhang	2013	2013 15th IEEE International Conference on Communication Technology	10.1109/ICCT.2013.6820480	wavelet;computer vision;constant q transform;mathematical optimization;contourlet;s transform;harmonic wavelet transform;second-generation wavelet transform;image gradient;continuous wavelet transform;fractional fourier transform;pattern recognition;mathematics;stationary wavelet transform;image fusion;discrete wavelet transform;top-hat transform;wavelet transform	Robotics	58.79576601424448	-67.40360820616021	161635
c79bc612c88c0c9bf87ef22d9dabd4c4962a0e63	x-rays image analysis for defects detection and characterization in metallic samples	image processing;metals;x ray imaging;markov processes radiography inspection x ray imaging image processing parameter estimation noncrystalline defects flaw detection metals;inspection;radiography;geometric information x rays image analysis defects detection characterization metallic samples quality absorption granularity radiography three step inspection method localization shape determination parameters measurement regularization robust method adapted markovian modelizations;flaw detection;image analysis x ray detection x ray detectors inspection noise shaping radiography noise robustness shape measurement electromagnetic wave absorption noise reduction;robust method;noncrystalline defects;quantitative analysis;markov processes;parameter estimation;x rays	A way to assess metallic samples quality is to locate possible defects and to characterize them by parameters like depth, surface and volume. X-Rays are well adapted to achieve this quantitative analysis for apparent as well as for internal defects. The important absorption and the granularity of some metals lead to very noisy radiographies. In this paper, we propose a robust thrmstep inspection method : defects detection and localization, precise defects shape determination and defects parameters measurement. Our approach, based on the combination of two specific radiographies, and on a relevant regularization, reduces noise effects and leads to a robust method. The regularization is based on the introduction of relevant geometric information by adapted Markovian modelizations. The method has been tested on a wide range of samples and compared to a destructive inspection. The paper is illustrated by the results obtained, at the different inspection steps, for a particular sample.	free-orbit experiment with laser interferometry x-rays;image analysis;matrix regularization	Jean-Marc Dinten;Pascal Dziopa;Anne Koenig	1994		10.1109/ICIP.1994.413584	computer vision;radiography;inspection;image processing;quantitative analysis;mathematics;markov process;estimation theory;statistics	Vision	57.285768326529784	-77.88862809416705	161785
1953aea3620a88531b543a327c67d89ff53355bf	descriptive experiment design restructured mvdr beamforming technique for enhanced imaging with unfocused sar systems		We address a new technique for feature-enhanced radar imaging with unfocused/fractional SAR sensor systems that unifies the minimum risk inspired descriptive experiment design regularization (DEDR) framework with the robust minimum variance distortionless response (MVDR) beamforming approach for recovery of the spatial spectrum pattern (SSP) of the wavefield scattered from the remotely sensed scene referred to as its reconstructed radar image. The new framework incorporates into the DEDR strategy for feature- enhanced SSP estimation the convergence guaranteed composite projector onto the convex solution set and solves the overall reconstructive imaging inverse problem employing the developed DEDR-restructured robust MVDR method.	beamforming;design of experiments	Yuriy Shkvarko;Joel A. Amao Oliva	2014		10.1007/978-3-319-12568-8_117	speech recognition;telecommunications	EDA	68.0748937535871	-66.8097197271927	162632
6c5429d01a50a7e659b44a829053b0ee82dbbb01	sar image processing using adaptive stack filter	traitement signal;filtering;evaluation performance;routing protocols;fonction booleenne;filtrage;speckle;non linear filtering;performance evaluation;protocole transmission;image processing;maximum likelihood;non linear filter;binary image;evaluacion prestacion;filtrado;radar abertura sintetica;maximum vraisemblance;speckle noise;procesamiento imagen;traitement signal radar;boolean function;selective sampling;data filtering;classification;traitement image;reduccion ruido;protocolo transmision;maximum likelihood classification;funcion booliana;signal processing;noise reduction;radar imaging;sar image;image binaire;reduction bruit;filtro adaptable;imagen binaria;stack filters;protocole routage;filtro no lineal;imagerie radar;filtre adaptatif;procesamiento senal;radar ouverture synthetique;adaptive filter;radar signal processing;maxima verosimilitud;filtre non lineaire;diagrama mancha;synthetic aperture radar;transmission protocol	Stack filters are a special case of non-linear filters. They have a good performance for filtering images with different types of noise while preserving edges and details. A stack filter decomposes an input image into several binary images according to a set of thresholds. Each binary image is filtered by a Boolean function. The Boolean function that characterizes an adaptive stack filter is optimal and is computed from a pair of images consisting of an ideal noiseless image and its noisy version. In this work the behavior of adaptive stack filters on synthetic aperture radar (SAR) data is evaluated. With this aim, the equivalent number of looks for stack filtered data are calculated to assess the speckle noise reduction capability of this filter. Then a classification of simulated and real SAR images is carried out on data filtered with a stack filter trained with selected samples. The results of a maximum likelihood classification of these data are evaluated and compared with the results of classifying images previously filtered using the Lee and the Frost filters. 2009 Elsevier B.V. All rights reserved.	aperture (software);binary image;call stack;image processing;noise reduction;nonlinear system;patch (computing);pattern recognition;stack overflow;synthetic data;synthetic intelligence	María E. Buemi;Julio Jacobo-Berlles;Marta Mejail	2010	Pattern Recognition Letters	10.1016/j.patrec.2009.02.008	filter;adaptive filter;speckle pattern;speckle noise;computer vision;synthetic aperture radar;binary image;telecommunications;image processing;biological classification;computer science;signal processing;noise reduction;mathematics;maximum likelihood;routing protocol;boolean function;radar imaging;composite image filter	Vision	54.328133914043995	-66.52068866686966	163121
9768170131b581455e2747722aee6f959ae6b142	the dyadic lifting schemes and the denoising of digital images	information systems;lifting scheme;signal processing;image denoising;digital image;dyadic wavelet;applied mathematics	The dyadic lifting schemes, which generalize Sweldens lifting schemes, have been proposed for custom-design of dyadic and bi-orthogonal wavelets and their duals. Starting with dyadic wavelets and exploiting the control provided in the form of free parameters, one can custom-design dyadic as well as bi-orthogonal wavelets adapted to a particular application. To validate the usefulness of the schemes, two construction methods have been proposed for designing dyadic wavelet filters with higher number of vanishing moments; using these design techniques, spline dyadic wavelet filters have been customdesigned for the denoising of digital images, which exhibit enhanced denoising effects.	digital image;dyadic transformation;image processing;lifting scheme;noise reduction;simulation;spline (mathematics);unsharp masking;wavelet	Turghunjan Abdukirim Türüki;Koichi Niijima;Shigeru Takano	2008	IJWMIP	10.1142/S0219691308002380	computer vision;mathematical optimization;discrete mathematics;computer science;signal processing;mathematics;lifting scheme;information system;digital image	Graphics	55.7444615285363	-68.44879318912659	163165
a06409989323e2bffb74eaf9a7a2d33513f3efc3	nonlinear interpolators for old movie restoration	simulations nonlinear interpolators old movie restoration rational function filter image sequence frames restoration digitized old movies random defects defect localization algorithm morphological operator hardware implementation;phase detection;rational interpolation;interpolation;local algorithm;motion pictures;motion pictures image restoration interpolation filters hardware signal restoration image sequences phase detection signal processing laboratories;perforation;image restoration interpolation image sequences digital simulation;filters;nonlinear interpolators;random defects;image restoration;old movie restoration;linear operator;digitized old movies;signal processing;image sequence;image sequence frames restoration;defect localization algorithm;signal restoration;rational function;hardware implementation;digital simulation;rational function filter;morphological operator;hardware;image sequences	A nonlinear interpolat,or using a rational function filter is applied to the restoration of image sequence frames of digitized old movies. Samples to be interpolated are due to stationary and random defects. The interpolator is preceded by a defect localization algorithm. The performance of the proposed interpolator has been assessed on several sequences and compared with a classical morphological operator. The hardware implementation of the proposed rational interpolator is also considered. Simu1a.tions show that the interpolated frames with the proposed operator are free from blockiness and jaggedness which are very difficult to avoid when using linear operators.	algorithm;circuit restoration;computational complexity theory;computer architecture;continuation;international standard book number;interpolation;kernel density estimation;nonlinear programming;nonlinear system;perf (linux);software bug;stationary process;with high probability	Lazhar Khriji;Moncef Gabbouj;Stefano Marsi;Giovanni Ramponi;Etienne Decencière	1999		10.1109/ICIP.1999.817093	phase detector;image restoration;computer vision;rational function;interpolation;computer science;theoretical computer science;signal processing;mathematics;linear map;computer graphics (images)	Vision	65.92644856641368	-69.89202126183825	163374
a9e89079166e91ca3ba6d287945ffeeaed1fdf49	combining total variation with nonlocal self-similarity constraint for compressed sensing mri	nonlocal self similarity constraint image detail preservation image edge preservation noise reduction nl self similarity constraint local tv model wavelet sparsity regularization local tv regularization improved cs reconstruction method traditional total variation based cs method magnetic resonance image reconstruction signal recovery method magnetic resonance imaging technique acquisition time undersampling k space data compressed sensing mri;image reconstruction tv magnetic resonance imaging psnr reconstruction algorithms compressed sensing biomedical imaging;medical image processing biomedical mri compressed sensing edge detection image denoising image reconstruction;mri reconstruction compressed sensing nonlocal self similarity total variation	Undersampling k-space data is an efficient way to reduce the acquisition time of magnetic resonance imaging (MRI) technique. As a promising signal recovery method, compressed sensing (CS) is able to reconstruct magnetic resonance images using a few samples and therefore has great potential in speeding up MRI process. The traditional total variation (TV) based CS approaches tend to over-smooth local image details. This paper proposes an improved CS reconstruction method for MR images by combining local TV regularization, wavelet sparsity regularization and nonlocal (NL) self-similarity constraint together. The experimental results demonstrate that the local TV model and NL self-similarity constraint are complementary to each other, making the proposed approach highly effective in reducing noise and preserving image edges and details.	aharonov–bohm effect;compressed sensing;detection theory;matrix regularization;nl (complexity);quantum nonlocality;resonance;self-similarity;sparse matrix;undersampling;wavelet	Jian-Ping Huang;Wanyu Liu;Lihui Wang;Yue Min Zhu	2014	2014 IEEE 11th International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2014.6868057	computer vision;mathematical optimization;pattern recognition;mathematics	Vision	56.48320054444033	-70.73318512252801	163496
27c787a0e2cfd5ba7e610cb5472256fa9ad75e2d	progressive compressed sensing and reconstruction of multidimensional signals using hybrid transform/prediction sparsity model	compressed sensing cs;compressed sensing;initialization strategy progressive compressed sensing multidimensional correlated signal reconstruction hybrid transform prediction sparsity model signal representation linear projections multidimensional signal acquisition computational complexity 2d signals 3d signals iterative local signal reconstruction hybrid transform prediction correlation model;multidimensional signals;signal detection;prediction algorithms;image scanning;iterative methods;remote sensing compressed sensing cs hyperspectral imaging image scanning linear predictor multidimensional signals;computational complexity;image reconstruction;remote sensing;signal representation;signal reconstruction;correlation;hyperspectral imaging;image reconstruction remote sensing correlation compressed sensing prediction algorithms hyperspectral imaging;signal representation compressed sensing computational complexity iterative methods signal detection signal reconstruction;linear predictor	Compressed sensing (CS) is an innovative technique allowing to represent signals through a small number of their linear projections. Hence, CS can be thought of as a natural candidate for acquisition of multidimensional signals, as the amount of data acquired and processed by conventional sensors could create problems in terms of computational complexity. In this paper, we propose a framework for the acquisition and reconstruction of multidimensional correlated signals. The approach is general and can be applied to D dimensional signals, even if the algorithms we propose to practically implement such architectures apply to 2-D and 3-D signals. The proposed architectures employ iterative local signal reconstruction based on a hybrid transform/prediction correlation model, coupled with a proper initialization strategy.	algorithm;compressed sensing;image scanner;iteration;iterative method;progressive scan;rate of convergence;sensor;signal reconstruction;sparse matrix;vergence	Giulio Coluccia;Simeon Kamdem Kuiteing;Andrea Abrardo;Mauro Barni;Enrico Magli	2012	IEEE Journal on Emerging and Selected Topics in Circuits and Systems	10.1109/JETCAS.2012.2214891	iterative reconstruction;signal reconstruction;computer vision;prediction;linear prediction;computer science;hyperspectral imaging;machine learning;pattern recognition;iterative method;computational complexity theory;compressed sensing;correlation;detection theory	AI	63.75380269657656	-71.7294864157355	163660
f7023d355e883d75a7bf5a65b4518b9f579dbcb5	on preconditioning multiwavelet systems for image compression	preconditioning;image compression;prefilter;postfilter;wavelets;multiwavelet transform	This paper presents a study on applications of multiwavelet analysis to image compression. The biorthogonality and perfect reconstruction conditions are applied to multiwavelet filters. As a multiwavelet filter bank has multiple channels of inputs, the data structure of inputs to the multiwavelet system should be taken into consideration in multiwavelet decomposition and reconstruction algorithms. We investigate the data initialization problem by considering prefilters and postfilters that may give more efficient representations of the decomposed data. The interpolation postfilter and prefilter are formulated, which are capable to provide a better approximate image at each coarser resolution level. A design process is given to obtain both filters having compact supports, if exist. Image compression performances of three multiwavelet systems are studied in comparison to those of single wavelet systems.	image compression;preconditioner	Wonkoo Kim;Ching-Chung Li	2003	IJWMIP	10.1142/S0219691303000049	wavelet;computer vision;mathematical optimization;speech recognition;image compression;mathematics;preconditioner;algorithm;statistics	Robotics	55.536879741093	-68.50444632965227	164198
f71a682b76fe5aa37bc9a30ca1e3abafdae386d9	correlated wavelet shrinkage: models of local random fields across multiple resolutions	image denoising wavelet transforms image resolution statistical analysis;image resolution;wavelet shrinkage;wavelet transforms;statistical analysis;statistics hidden markov models bayesian methods additive noise root mean square wavelet domain signal resolution design engineering systems engineering and theory wavelet transforms;image denoising;image denoising correlated wavelet shrinkage local random fields image resolutions wavelet joint statistics wavelet correlation models nonlinear shrinkage algorithms;random field	This paper proposes a novel correlated shrinkage method based on wavelet joint statistics. Our objective is to demonstrate effectiveness of the wavelet correlation models [Z. Azimifar et al., 2004] in estimating the original signal from a noising observation. Simulation results are given to show the advantage of the new correlated shrinkage function. In comparison with the popular nonlinear shrinkage algorithms, it improves the denoised results.	algorithm;nonlinear system;simulation;wavelet	Zohreh Azimifar;Paul W. Fieguth;Ed Jernigan	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1530352	multiresolution analysis;wavelet;econometrics;random field;harmonic wavelet transform;image resolution;second-generation wavelet transform;pattern recognition;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;statistics;wavelet transform	Robotics	60.37397329966611	-70.19366943813806	164409
b06ce2757495fcd7a9e5695c28fea83229789c47	fast iteratively reweighted least squares for lp regularized image deconvolution and reconstruction	minimisation deconvolution image reconstruction iterative methods least squares approximations;image restoration;iteratively reweighted least squares image restoration image reconstruction compressive sensing nonconvex nonsmooth regularization;compressive sensing;image reconstruction;iteratively reweighted least squares;deconvolution image reconstruction minimization transforms image restoration approximation methods imaging;recovery quality iteratively reweighted least squares regularized linear inverse problem minimization irls method image deconvolution image reconstruction majorization minimization alternating direction method of multipliers admm reweighted linear equations shrinkage operator multiplicative fashion;nonconvex nonsmooth regularization	Iteratively reweighted least squares (IRLS) is one of the most effective methods to minimize the lp regularized linear inverse problem. Unfortunately, the regularizer is nonsmooth and nonconvex when 0 <; p <; 1. In spite of its properties and mainly due to its high computation cost, IRLS is not widely used in image deconvolution and reconstruction. In this paper, we first derive the IRLS method from the perspective of majorization minimization and then propose an Alternating Direction Method of Multipliers (ADMM) to solve the reweighted linear equations. Interestingly, the resulting algorithm has a shrinkage operator that pushes each component to zero in a multiplicative fashion. Experimental results on both image deconvolution and reconstruction demonstrate that the proposed method outperforms state-of-the-art algorithms in terms of speed and recovery quality.	algorithm;augmented lagrangian method;computation;deconvolution;iteratively reweighted least squares;linear equation	Xu Zhou;Rafael Molina;Fugen Zhou;Aggelos K. Katsaggelos	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025357	iterative reconstruction;image restoration;iteratively reweighted least squares;computer vision;econometrics;mathematical optimization;computer science;mathematics;compressed sensing;statistics	Robotics	57.07885291404084	-72.51021800632141	164560
e54b1f2cf4c7cbfa28a63f34658bd7cd40250d68	optimum recursive filtering of noisy two-dimensional data with sequential parameter identification	stochastic approximation asymmetric half plane model autoregressive moving average image enhancement optimum recursive filtering sequential parameter identification;recursive estimation;filtering;kernel;optimum recursive filtering;image coding;image processing;convolution;kalman filters;estimation algorithm;parameter identification;image enhancement;moving average;autoregressive moving average;karhunen loeve transforms;filtering algorithms;stochastic processes;asymmetric half plane model;stochastic approximation;fourier transforms;transforms;parameter estimation;correlation;filtering parameter estimation image coding fourier transforms kernel convolution equations image processing karhunen loeve transforms stochastic processes;minimum mean square error;sequential parameter identification;data models	A two-dimensional recursive estimation algorithm based on the asymmetric half-plane model is described for the problem of MMSE (minimum mean-square error) filtering. The optimum filtering problem is solved by formulating the asymmetric half-plane ARMA (autoregressive moving average) model for two-dimensional data. The sequential parameter identification from the noisy two-dimensional data is also discussed, utilizing the stochastic approximation. Experiments were performed for real image data, combining the proposed parameter identification and estimation algorithms. The results show that this method gives considerable improvement in SNR.	algorithm;autoregressive model;population parameter;recursion (computer science);signal-to-noise ratio;stochastic approximation	Young-Ho Yum;Song B. Park	1983	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.1983.4767396	filter;kalman filter;autoregressive–moving-average model;minimum mean square error;fourier transform;data modeling;stochastic approximation;mathematical optimization;kernel;image processing;pattern recognition;mathematics;convolution;moving average;estimation theory;correlation;statistics	ML	61.16466586822525	-70.94254624774052	165076
6c0e3c803a945ca2367bc2d35cb69aaafad96b05	multimodal sensor medical image fusion based on mutual-structure for joint filtering using sparse representation			image fusion;multimodal interaction;sparse approximation;sparse matrix	Weisheng Li;Xiaofan Xu;Jiao Du	2018	Int. J. Imaging Systems and Technology	10.1002/ima.22251	filter (signal processing);computer vision;artificial intelligence;computer science;sparse approximation;image fusion	Vision	59.58468454351132	-68.12074948821707	165162
8021292c50ce09bf79f9cdd625ec5798eb486e1e	intrinsic constraints in space-time filtering: a new approach to representing uncertainty in low-level vision	image sampling;cramer rao;desigualdad cramer rao;filtering uncertainty optical filters image motion analysis parameter estimation biomedical optical imaging optical sensors data mining image sampling sampling methods;filtering;theorie echantillonnage;teoria muestreo;vision ordenador;image motion analysis;movimiento;temporal sampling rates;spatial sampling rates;motion uncertainty;image processing;incertidumbre;flux optique;space time filtering;uncertainty;filtrado frecuencia espacial;optical filters;sistema informatico;procesamiento imagen;picture processing;computer system;space time;motion;satisfiability;spatial filters;data mining;traitement image;computer vision;cramer rao inequality;erreur estimation;intrinsic constraints;frecuencia temporal;mouvement;estimacion parametro;pattern recognition;error estimacion;inegalite cramer rao;vision ordinateur;optical flow;systeme informatique;spatial filters parameter estimation pattern recognition picture processing;optical sensors;incertitude;frequence temporelle;estimation error;temporal sampling rates spatial sampling rates cramer rao inequality parameter estimation intrinsic constraints picture processing space time filtering low level vision optical flow motion uncertainty;parameter estimation;estimation parametre;biomedical optical imaging;sampling methods;low level vision;frequency domain;spatial frequency filtering;vision faible;lower bound;filtrage frequence spatiale;temporal frequency;sampling theory	Describes how, in the process of extracting the optical flow through space-time filtering, one has to consider the constraints associated with the motion uncertainty, as well as the spatial and temporal sampling rates of the sequence of images. The motion uncertainty satisfies the Cramer-Rao (CR) inequality, which is shown to be a function of the filter parameters. On the other hand, the spatial and temporal sampling rates have lower bounds, which depend on the motion uncertainty, the maximum support in the frequency domain, and the optical flow. These lower bounds on the sampling rates and on the motion uncertainty are constraints that constitute an intrinsic part of the computational structure of space-time filtering. The author shows that if he uses these constraints simultaneously, the filter parameters cannot be arbitrarily determined but instead have to satisfy consistency constraints. By using explicit representations of uncertainties in extracting visual attributes, one can constrain the range of values assumed by the filter parameters. >		Radu S. Jasinschi	1992	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.120330	filter;sampling;computer vision;econometrics;cramér–rao bound;uncertainty;image processing;computer science;motion;space time;optical flow;optical filter;mathematics;upper and lower bounds;estimation theory;frequency domain;statistics;satisfiability	Vision	63.38939636519419	-68.01733286025416	165242
2adde6e8c5d746224554c042ea7cbd2f7dc85389	image denoising using wavelet bayesian network models	gaussian noise;belief networks;bayesian network;bayesian methods wavelet transforms noise reduction hidden markov models joints gsm image denoising;bayesian methods;joints;maximum likelihood estimation;wavelet transforms;wavelet transform;hidden markov models;image representation;noise reduction;decorrelation;image denoising;wavelet transform image denoising bayesian network;gsm;white gaussian noise image denoising wavelet bayesian network models simplest inverse problem wavelet coefficients maximum a posterior estimator map estimator wavelet coefficients graphical model construction training images wavelet properties interscale data dependency wavelet representation robust bayesian network peak signal to noise ratio performance psnr performance;white noise;inverse problems;white noise belief networks decorrelation gaussian noise image denoising image representation inverse problems maximum likelihood estimation wavelet transforms	A number of techniques have been developed to deal with image denoising, which is regarded as the simplest inverse problem. In this paper, we propose an approach that constructs a Bayesian network from the wavelet coefficients of a single image such that different Bayesian networks can be obtained from different input images. Then, we utilize the maximum-a-posterior (MAP) estimator to derive the wavelet coefficients. Constructing a graphical model usually requires a large number of training images. However, we demonstrate that by using certain wavelet properties, namely, interscale data dependency, decorrelation between wavelet coefficients, and sparsity of the wavelet representation, a robust Bayesian network can be constructed from one image to resolve the denoising problem. Our experiment results show that, in terms of the peak-signal-to-noise-ratio (PSNR) performance, the proposed approach outperforms state-of-art algorithms on several images with various amounts of white Gaussian noise.	algorithm;autostereogram;bayesian network;coefficient;data dependency;decorrelation;graphical model;noise reduction;peak signal-to-noise ratio;sparse matrix;wavelet	Jinn Ho;Wen-Liang Hwang	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288080	wavelet;computer vision;computer science;machine learning;pattern recognition;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;hidden markov model;statistics;wavelet transform	Robotics	61.1666489670347	-70.65773008361633	165421
15c2f21a506b172b9926149a80bd0c4d4c705b3a	adaptive kernel-based image denoising employing semi-parametric regularization	wavelet based technique;gaussian noise;optimisation;kernel;hilbert spaces;helium;semiparametric regularization;optimisation gaussian noise hilbert spaces image denoising;additive noise;gaussian noise adaptive kernel based image denoising semiparametric regularization reproducing kernel hilbert spaces rkhs theory additive noise digital image optimization task celebrated representer theorem wavelet based technique;celebrated representer theorem;optimization task;hilbert space;reproducing kernel hilbert spaces rkhs;reproducing kernel hilbert spaces;image denoising kernel additive noise gaussian noise hilbert space noise reduction digital images pattern analysis partial differential equations;partial differential equations;reproducing kernel hilbert space;noise reduction;semi parametric representer theorem;adaptive kernel based image denoising;representation theorem;pattern analysis;rkhs theory;image denoising;digital image;denoising;semi parametric representer theorem denoising kernel reproducing kernel hilbert spaces rkhs;digital images;noise removal;algorithms artifacts computer simulation data interpretation statistical image enhancement image interpretation computer assisted models biological models statistical pattern recognition automated reproducibility of results sensitivity and specificity	The main contribution of this paper is the development of a novel approach, based on the theory of Reproducing Kernel Hilbert Spaces (RKHS), for the problem of noise removal in the spatial domain. The proposed methodology has the advantage that it is able to remove any kind of additive noise (impulse, gaussian, uniform, etc.) from any digital image, in contrast to the most commonly used denoising techniques, which are noise dependent. The problem is cast as an optimization task in a RKHS, by taking advantage of the celebrated Representer Theorem in its semi-parametric formulation. The semi-parametric formulation, although known in theory, has so far found limited, to our knowledge, application. However, in the image denoising problem, its use is dictated by the nature of the problem itself. The need for edge preservation naturally leads to such a modeling. Examples verify that in the presence of gaussian noise the proposed methodology performs well compared to wavelet based technics and outperforms them significantly in the presence of impulse or mixed noise.	additive white gaussian noise;biologic preservation;digital image;hilbert space;kernel (operating system);mathematical optimization;noise reduction;normal statistical distribution;representer theorem;semi-thue system;semiconductor industry;utility functions on indivisible goods;wavelet	Pantelis Bouboulis;Konstantinos Slavakis;Sergios Theodoridis	2010	IEEE Transactions on Image Processing	10.1109/TIP.2010.2042995	computer vision;mathematical optimization;mathematical analysis;discrete mathematics;computer science;noise reduction;mathematics;digital image;hilbert space	Vision	55.5317790808472	-72.47115230713041	165802
9d2dc898112ce8186f6905153da751702ed36e31	hybrid wavelet measurement matrices for improving compressive imaging		Compressive sensing principle claims that a compressible signal can be recovered from a small number of random linear measurements. However, the design of efficient measurement basis in compressive imaging remains as a challenging problem. In this paper, a new set of hybrid wavelet measurement matrices is proposed to improve the quality of the compressive imaging, increase the compression ratio and reduce the processing time.The performance of these hybrid wavelet matrices for imagemodeling and reconstruction is evaluated and compared with other traditional measurement matrices such as the random measurement matrices, Walsh and DCT matrices. The compressive imaging approach chosen in this study is the block compressive sensing with smoothed projected Landweber reconstruction technique. The simulation results indicate that the imaging performance of the proposed hybrid wavelet measurement matrices is approximately 2–3dB better than that obtained using Gaussian matrix especially at higher compression ratios.	compressed sensing;discrete cosine transform;discrete wavelet transform;hadamard transform;landweber iteration;simulation;smoothing;wavelet	Rasha Shoitan;Zaki Nossair;Ibrahim Isamil;Ahmed Tobal	2017	Signal, Image and Video Processing	10.1007/s11760-016-0894-5	mathematical optimization;electrical engineering	Mobile	59.388770493806405	-69.09143278745834	166576
2210aca1515b9df389d74518b9fad2d31df9230a	riemannian anisotropic diffusion for tensor valued images	juser;positive definite;anisotropic diffusion;websearch;eigenvalues;magnetic resonance image;numerical scheme;riemannian metric;image processing techniques;diffusion process;synthetic data;publications database;geometric structure;diffusion tensor	Tensor valued images, for instance originating from diffusion tensor magnetic resonance imaging (DT-MRI), have become more and more important over the last couple of years. Due to the nonlinear structure of such data it is nontrivial to adapt well-established image processing techniques to them. In this contribution we derive anisotropic diffusion equations for tensor-valued images based on the intrinsic Riemannian geometric structure of the space of symmetric positive tensors. In contrast to anisotropic diffusion approaches proposed so far, which are based on the Euclidian metric, our approach considers the nonlinear structure of positive definite tensors by means of the intrinsic Riemannian metric. Together with an intrinsic numerical scheme our approach overcomes a main drawback of former proposed anisotropic diffusion approaches, the so-called eigenvalue swelling effect. Experiments on synthetic data as well as real DT-MRI data demonstrate the value of a sound differential geometric formulation of diffusion processes for tensor valued data.	anisotropic diffusion	Kai Krajsek;Marion I. Menzel;Michael Zwanger;Hanno Scharr	2008		10.1007/978-3-540-88693-8_24	diffusion mri;fundamental theorem of riemannian geometry;symmetric tensor;mathematical analysis;topology;eigenvalues and eigenvectors;diffusion process;magnetic resonance imaging;tensor contraction;mathematics;geometry;positive-definite matrix;anisotropic diffusion;ricci flow;statistics;synthetic data	Vision	54.06022732647424	-70.85271970021961	166750
cdc485a00f12d1b40bc780de410c1e434c293f78	clinical evaluation of wavelet compression of digitized chest x-rays	transformation ondelette;radiology;image storage;medical imagery;data compression;lossy compression;radiologia;radiologie;mean error;clinical evaluation;stockage image;image compression;imagerie medicale;compression ratio;algorithms;imageneria medical;compresion dato;transformacion ondita;information system;chest x ray;communication;comunicacion;almacenamiento imagen;wavelets;chest;systeme information;wavelet transformation;compression donnee;x rays;sistema informacion	In this paper we assess lossy image compression of digitalized chest x-rays using radiologist assessment of anatomic structures and numerical measurements of image accuracy. Forty chest x-rays were digitized and compressed using an irreversible wavelet technique at 10, 20, 40 and 80:1. These were presented in a blinded fashion with an uncompressed image for subjective A-B comparison of 11 anatomic structures as well as overall quality. Mean error, RMS error, maximum pixel error, and number of pixels within 1 percent of original value were also computed for compression ratios from 10:1 to 80:1. We found that at low compression there was a slight preference for compressed images. There was no significant difference at 20:1 and 40:1. There was a slight preference on some structures for the original compared with 80:1 compressed images. Numerical measures demonstrated high image faithfulness, both in terms of number of pixels that were within 1 percent of their original value, and by the average error for all pixels. Our findings suggest that lossy compression at 40:1 or more can be used without perceptible loss in the demonstration of anatomic structures.© (1997) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	wavelet transform	Bradley J. Erickson;Armando Manduca;Kenneth R. Persons	1997		10.1117/12.273909	computer vision;simulation;mathematics;computer graphics (images)	Vision	56.76845459365441	-79.73271718826916	166877
d42cb8dd3b55e0e86e851f50647613ae5455bccb	asymmetric, space-variant point spread function model for a spherical lens optical system	linear convolution;single spherical lens;gradient descent parameter adjustment;fraunhofer diffraction;image storage;modelling optical transfer function optical images aberrations fraunhofer diffraction lenses image restoration gradient methods fourier transform optics adaptive signal processing;space variant point spread function model;field angle;optical transfer function;fourier transform;asymmetric analytical model;image restoration;nyquist sampling;triangle function;brain modeling;sidelobe asymmetry;adaptive signal processing;aberrations;optical arrays;gradient descent;digital image restoration;point spread function;lenses;adaptive system;gradient methods;optical diffraction;optical images;fourier transform optics;rectangular aperture;optical sensors;digital image;lenses optical sensors brain modeling image restoration digital images sensor arrays apertures optical diffraction optical arrays image storage;central lobe shifting;nyquist sampling spherical lens optical system space variant point spread function model asymmetric analytical model single spherical lens rectangular aperture central lobe shifting sidelobe asymmetry diffraction theory gradient descent parameter adjustment field angle digital image restoration linear convolution triangle function fourier transform seidel aberration coefficient fraunhofer diffraction;digital images;spherical lens optical system;sensor arrays;analytical model;apertures;diffraction theory;seidel aberration coefficient	An asymmetric analytical model is presented for the spacevariant (SV) point-spread-function (PSF) of an optical system consisting of a single spherical lens with a rectangular aperture. The model is an improvement over a prior model in that additional parameters allow shifting of the central lobe and asymmetry of the sidelobes. Both of these effects are seen in diffraction-based PSF models as field angle is increased. The model is useful for applying certain SV restoration methods to digital images formed by simple optical systems, since it significantly reduces the memory required to store PSF sample functions. The proposed model uses eight parameters for any specific field position. The model is adapted to PSFs developed from diffraction theory, using an adaptive system with gradient descent parameter adjustment. Data is presented that characterizes the accuracy of the adapted model to a physical PSF as a function of field angle.	acoustic lobing;adaptive system;circuit restoration;digital image;function model;gradient descent;systemverilog	Thomas P. Costello;Wasfy B. Mikhael	1999		10.1109/ISCAS.1999.779943	computer vision;electronic engineering;computer science;adaptive system;mathematics;digital image	Vision	63.287281115104555	-70.74960761629238	167050
039b4500c7aba0a85b8971a08684d4f96442a9c3	reflection removal using rgb-d images		This paper proposes a novel reflection removal method for RGB-D images that achieve reflection removal and depth map recovery simultaneously. In general, there is a strong structure correlation between an RGB image and a depth map in gradient domain. Based on this fact, we introduce a novel regularization for RGB-D images named the multi-modal structure tensor total variation (MSTV). A proposed minimization problem based on MSTV which is constructed by two minimization problems, reflection removal and depth map recovery, is solved by using alternating direction method of multipliers (ADMM). Experimental results show the effectiveness of our method by applying it to both artificial and real-world images.		Toshihiro Shibata;Yuji Akai;Ryo Matsuoka	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451639	computer vision;rgb color model;structure tensor;regularization (mathematics);correlation;stress (mechanics);artificial intelligence;convex function;depth map;computer science	Robotics	56.9433583015007	-71.09010692010601	167155
9d74fa7eb581a9c1ffdb884d082106a6e2d3d4c1	image denoising with higher order total variation and fast algorithms		In this paper, we propose an efficient higher order total variation regularization scheme for image denoising problem. By relaxing the constraints appearing in the traditional infimal convolution regularization, the proposed higher order total variation can remove the staircasing effects caused by total variation as well as preserve sharp edges and finer details well in the restored image. We characterize the solution of the proposed model using fixed point equations (via the proximity operator) and develop convergent proximity algorithms for solving the model. Our numerical experiments demonstrate the efficiency of the proposed method.	algorithm;fast fourier transform;noise reduction	Wenchao Zeng;Xueying Zeng;Zhen Yue	2015		10.1007/978-3-319-21963-9_37	step detection;computer vision;mathematical optimization;combinatorics;total variation denoising;non-local means	ML	55.290050988654365	-70.48854471372717	167186
769e2fac8846825c0f3596b71610bb32a0e8fb10	separation surfaces in the spectral tv domain for texture decomposition	eigenvalues and eigenfunctions;spatially varying texture total variation spectral tv image decomposition image enhancement nonlinear eigenfunction analysis;surface texture;computer vision;surface treatment;eigenvalues and eigenfunctions tv surface texture transforms surface treatment image decomposition computer vision;transforms;television image texture;tv;image decomposition;adaptive scale range separation surfaces spectral tv domain texture decomposition image decomposition spectral total variation 3d domain spatially varying separation textures separation gradually varying pattern size pattern contrast illumination tv spectral framework spectral domain texture stratum	In this paper, we introduce a novel notion of separation surfaces for image decomposition. A surface is embedded in the spectral total-variation (TV) 3D domain and encodes a spatially varying separation scale. The method allows good separation of textures with gradually varying pattern size, pattern contrast, or illumination. The recently proposed TV spectral framework is used to decompose the image into a continuum of textural scales. A desired texture, within a scale range, is found by fitting a surface to the local maximal responses in the spectral domain. A band above and below the surface, referred to as the texture stratum, defines for each pixel the adaptive scale range of the texture. Based on the decomposition, an application is proposed, which can attenuate or enhance textures in the image in a very natural and visually convincing manner.	complement total hemolytic ch100:acnc:pt:ser/plas:qn;embedded system;embedding;maximal set;pixel;texture filtering;triune continuum paradigm	Dikla Horesh;Guy Gilboa	2016	IEEE Transactions on Image Processing	10.1109/TIP.2016.2587121	image texture;surface finish;computer vision;mathematical optimization;computer science;mathematics;television;texture compression;texture filtering	Vision	55.03747689275364	-68.43976787052806	167256
891d86bc6d34134ed60fc514d4d7da2539cc6a23	a multidimensional smoothing algorithm with applications to digital color printer calibration	tensile stress;image processing;cost function;multidimensional;printers;multidimensional systems smoothing methods printers calibration multidimensional signal processing signal processing image processing process control data engineering tensile stress;printer calibration;multidimensional data;tensor;smoothing methods;tensor multidimensional curve fitting smooth printer calibration;smooth;image color analysis;signal and image processing;tensors curve fitting image processing printers smoothing methods;cost function multidimensional smoothing algorithm digital color printer calibration image processing data fitting criteria tensor based algorithm smooth curve fitting;curve fitting;table lookup;data fitting;calibration;tensors	Multidimensional data smoothing has applications in signal and image processing, control, and other engineering disciplines. The two important data fitting criteria are the accuracy of the fit and the smoothness of the fit. In this paper, we discuss a tensor based algorithm for multidimensional smooth curve fitting where the cost function is a combination of two terms: one dealing with accuracy and the other one with the smoothness of the solution. We apply the proposed algorithm to digital color printer calibration in 1-d, 2-d and 3-d calibration techniques.	algorithm;curve fitting;image processing;loss function;printer (computing);smoothing	Sohail A. Dianat;Bruce Brewington;Lalit K. Mestha	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414029	computer vision;econometrics;tensor;image processing;computer science;mathematics;statistics;curve fitting	Robotics	61.47771081947946	-68.23154979500708	167674
f9dc3bdedbad9a9f3b3818de6e9de267c0ce3f6a	nonconvex compressive sensing reconstruction for tensor using structures in modes	sparse and low rank reconstruction;tensile stress sensors sparse matrices image reconstruction compressed sensing memory management matrix decomposition;tensor reconstruction;hyperspectral imaging nonconvex compressive sensing reconstruction cs reconstruction methods tensor reconstruction sparse structures magnetic resonance imaging;compressive sensing;magnetic resonance imaging compressed sensing concave programming hyperspectral imaging image reconstruction image representation;article;sparse and low rank reconstruction compressive sensing tensor reconstruction	This paper focuses on the reconstruction of a tensor captured using Compressive Sensing (CS). Instead of processing the signals via vectorization as is done in conventional CS, in tensor CS high dimensional signals are kept in their original formats, which benefits hardware implementation and eases memory requirements. In addition, more structures exist in a tensor along its various dimensions than in its vectorized format. Utilizing these various structures, this paper proposes a general reconstruction approach for tensor CS. Employing the proximity operator of a nonconvex norm function, a special case for a tensor with low rank and sparse structures is elaborated, which is shown to outperform the state-of-art tensor CS reconstruction methods when applied to magnetic resonance imaging and hyper-spectral imaging.	automatic vectorization;compressed sensing;low-rank approximation;requirement;resonance;sparse matrix	Xin Ding;Wei Chen;Ian J. Wassell	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472560	computer vision;mathematical optimization;computer science;machine learning;mathematics;compressed sensing	Vision	56.99816211430609	-73.25120724806976	167690
13143ce2734197f66451c3228a953acfb5efb3fb	non-convex low-rank approximation for hyperspectral image recovery with weighted total varaition regularization		Low-rank representation has been widely used as a powerful tool in hyperspectral image (HSI) recovery. The existing studies involving low-rank problems are commonly under the nuclear norm penalization. However, nuclear norm minimization tends to over-shrink the components of rank, which leads to modeling bias. In this paper, a new nonconvex penalty is introduced to obtain an unbiased low-rank approximation. In Addition, local spatial neighborhood weighted spectral-spatial total variation (TV) regularization is introduced to preserve spatial structural information. And sparse l1-norm is used as a constraint to sparse noise. Finally, a novel HSI non-convex low-rank relaxation restoration model is proposed. A number of experiments show that the proposed method can effectively remove the mixed-noise, and result in an unbiased estimate with better robustness.	circuit restoration;experiment;horizontal situation indicator;linear programming relaxation;low-rank approximation;penalty method;relaxation (approximation);robustness (computer science);sparse matrix;taxicab geometry	Hanyang Li;Peipei Sun;Hongyi Liu;Zenbin Wu;Zhihui Wei	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8517993	image restoration;computer vision;robustness (computer science);noise reduction;gaussian noise;artificial intelligence;mathematical optimization;matrix norm;low-rank approximation;matrix decomposition;regularization (mathematics);computer science	Vision	57.050519337869794	-71.68839102415629	167984
53ed19934668f217d802086f7bcaa7d427bbc82f	an algorithm for parallel reconstruction of jointly sparse tensors with applications to hyperspectral imaging		A wide range of Compressive Sensing (CS) frameworks have been proposed to address the task of color and hyperspectral image sampling and reconstruction. Methods for reconstruction of jointly sparse vectors that leverage joint sparsity constraints such as the Multiple Measurement Vector (MMV) approach have been shown to outperform Single Measurement Vector (SMV) frameworks. Recent work has shown that exploiting joint sparsity while simultaneously preserving the high-dimensional structure of the data results in further performance improvements. We introduce a parallelizable extension of a previously proposed serial tensorial MMV approach which, like its predecessor, exploits joint sparsity constraints multiple data dimensions simultaneously, but that is parallelizable in nature. We demonstrate empirically that the proposed method provides better reconstruction fidelity of hyperspectral imagery and that it is also more computationally efficient than the current state of the art.	algorithm;algorithmic efficiency;compressed sensing;computation;multiuse model view;sampling (signal processing);sparse matrix	Qun Li;Edgar A. Bernal	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2017.33	parallelizable manifold;tensor;pattern recognition;iterative reconstruction;compressed sensing;computer science;machine learning;hyperspectral imaging;matrix decomposition;artificial intelligence	Vision	67.47862083487774	-67.59460838342456	168297
95f42c12dcf3823da7625f6e8e28567fe4b40055	a study of using l1-norm with image watermarking on svd domain	bit error rates image watermarking svd domain robust singular value decomposition l1 norm sub space ll norm regression;watermarking;image coding;bit error rate;singular value decomposition;watermarking noise robustness singular value decomposition equations information science degradation bit error rate digital images copyright protection cryptography;watermarking image coding regression analysis singular value decomposition;regression analysis;image watermarking;missing data	This paper presents a study of image watermarking using robust singular value decomposition in L1-norm sub-space. The watermarked image attacked by noise is greatly degraded. This results in the effects of transparency and robustness of the watermarked image. Although the watermarking in SVD domain is sensitive to noise and outliers, incorporating L1-norm regression to the watermarking algorithm can help handling the missing data caused by noise and help increasing the robustness of the proposed algorithm. Experimental results show that the proposed algorithm can not only excellently reduce the bit error rates of the recovered watermark but also retain the transparency property of the watermarked image.	algorithm;bit error rate;digital watermarking;missing data;singular value decomposition;taxicab geometry;watermark (data file);watermarking attack	Amnach Khawne;Orachat Chitsobhuk;Toshiyuki Nakamiya	2007	Third International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP 2007)	10.1109/IIH-MSP.2007.64	computer vision;mathematical optimization;bit error rate;missing data;digital watermarking;theoretical computer science;mathematics;singular value decomposition;regression analysis;statistics	Robotics	59.101763545836306	-69.88687702588302	168420
a24840f8e164fcd6a8ed63a189cfdf6078453e26	high-quality non-blind motion deblurring	cross bilateral filter nonblind motion deblurring method gaussian scale mixture gsmfoe model experts fields model smoothness constraint iterative re weight least square algorithm residual deconvolution suite;bilateral filtering;least squares approximations;kernel;gaussian processes;image deblurring;motion estimation;image restoration;gaussian scale mixture;noise measurement;iterative methods;image edge detection;kernel deconvolution image restoration gsm pixel laboratories iterative algorithms filters convolution frequency;pixel;deconvolution;gsm;iteratively re weighted least squares;motion estimation gaussian processes image restoration iterative methods least squares approximations;kernel estimate;noise	Traditional non-blind motion deblurring methods are sensitive to kernel estimate errors and image noise, thus suffering from either ringing artifacts, enlarged image noise, or over-smoothed image details. We introduce a robust non-blind deblurring algorithm that produces high quality results even from many challenging images with noisy kernels. We adopt the Gaussian Scale Mixture Fields of Experts (GSM FOE) model and the smoothness constraint as image prior, and use the iterative re-weight least-square (IRLS) algorithm to produce the temporal result. The residual deconvolution suite is used to restore the lost image details. We denoise the result using our std-controlled cross bilateral filter. The experimental results are much better than those of previous approaches.	algorithm;bilateral filter;deblurring;deconvolution;display resolution;image noise;iterative method;iteratively reweighted least squares;noise reduction;ringing artifacts;smoothing	Chao Wang;Lifeng Sun;Zhuoyuan Chen;Shiqiang Yang;Jianwei Zhang	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414143	gsm;image restoration;computer vision;mathematical optimization;kernel;computer science;noise measurement;noise;deconvolution;motion estimation;gaussian process;mathematics;iterative method;bilateral filter;pixel;statistics	Vision	59.69977043438776	-71.13341688455233	169015
05f00ab01354f9d51cbb0750971e2f729d4d8a61	a convex formulation for color image segmentation in the context of passive emitter localization	parallel computing;nvidia geforce gtx 280;paper;image segmentation;image processing;emitter localization;convex programming;computer graphics;color;probability density function;image fusion;gpu convex formulation color image segmentation passive emitter localization information fusion object image extraction euler lagrange theorem graphics processing unit aerial platform bearing based localization;gpu;bearing based localization;data mining;energy function;image extraction;cuda;convex formulation;aerial platform;color image segmentation data mining concurrent computing context information processing ergonomics graphics parallel processing tv;euler lagrange;image color analysis;image colour analysis;pixel;parallel computer;nvidia;graphic processing unit;algorithms;total variation;passive emitter localization;information fusion;parallel implementation;image segmentation computer graphics convex programming image colour analysis image fusion;euler lagrange theorem;graphics processing unit;information fusion object;successive over relaxation;total variation image segmentation emitter localization parallel computing;color image;color image segmentation	In many tasks in information fusion objects of interest need to be extracted from color images. Often the only available information is the color of a specific object. In this paper we present a novel method for segmenting images into two regions, foreground (e.g. object) and background. We introduce a convex energy functional based on total variation that is subsequently solved using the Euler-Lagrange theorem and a parallel implementation of successive over-relaxation. The main achievement of this formulation is that, due to the convex formulation, the algorithm is guaranteed to find the global optimum from all possible solutions. Furthermore, this algorithm can be heavily parallized using the graphics processing unit (GPU). In the following we will show how to use the thus obtained image information in the context of passive emitter localization from aerial platforms. It will be shown that the fusion of image-and bearing-based localization results can strongly improve the bearings-only results.	aerial photography;algorithm;color image;computer graphics;euler;euler–lagrange equation;global optimization;graphics processing unit;image segmentation;internationalization and localization;lambert's cosine law;linear programming relaxation;maxima and minima;simulation;successive over-relaxation;variational principle	Marek Schikora;Miriam Häge;Eicke Ruthotto;Klaus Wild	2009	2009 12th International Conference on Information Fusion		computer vision;computer science;theoretical computer science;computer graphics (images)	Robotics	54.82319412391751	-72.11505408162199	169139
33bfdc9416930e98c32b61889bc977a25f0af0bb	a new epma image fusion algorithm based on contourlet-lifting wavelet transform and regional variance	image fusion;contourlet transform	Combined with image processes under microbeam analysis,and based on analyzing the feature of electron probe deeply,the paper presents an optimized method based on Contourlet-lifting wavelet transform and regional variance.firstly,to get the fused image before Contourlet fusion based on regional variance,and then the processing image and the target image will be fused under lifting wavelet transform based on regional variance,finally get the fusion result. The experiment shows that,the processed multi-source electron probe image is much more comprehensive and accurate than any other single source image,and it will facilitate further processing and analyzing images such as micro-surface target recognition and	algorithm;contourlet;electron;image analysis;image fusion;image processing;lifting scheme;matlab;multi-source;wavelet transform	Xiang Li;Xuan Zhan	2010	JSW		wavelet;computer vision;contourlet;speech recognition;second-generation wavelet transform;continuous wavelet transform;stationary wavelet transform;image fusion;discrete wavelet transform;lifting scheme;top-hat transform	Vision	58.97428146343531	-67.54549071906811	169304
2251c85d6e9582fe15c84c3acd1372fee4fdbb8d	alternating proximal algorithm for blind image recovery	minimisation;smooth convex data fidelity term;concave programming;alternating proximal algorithm;minimization;convergence;blind restoration;iterative proximal algorithm;blind reconstruction;convex programming;prior information;image restoration;nonsmooth analysis;variational formulation;objective function;iterative methods;wavelets blind restoration blind reconstruction proximal methods nonlinear optimization;linear degradation operator;proximal methods;image reconstruction;deconvolution;alternating minimization;blind image recovery;minimisation concave programming convex programming image reconstruction image restoration iterative methods;signal to noise ratio;minimization convergence image restoration signal to noise ratio deconvolution image reconstruction;blind restoration blind image recovery iterative proximal algorithm associated nonconvex minimization standard alternating minimization techniques alternating proximal algorithm smooth convex data fidelity term nonsmooth convex regularization terms linear degradation operator image restoration blind reconstruction;nonsmooth convex regularization terms;nonlinear optimization;standard alternating minimization techniques;wavelets;associated nonconvex minimization	We consider a variational formulation of blind image recovery problems. A novel iterative proximal algorithm is proposed to solve the associated nonconvex minimization problem. Under suitable assumptions, this algorithm is shown to have better convergence properties than standard alternating minimization techniques. The objective function includes a smooth convex data fidelity term and nonsmooth convex regularization terms modeling prior information on the data and on the unknown linear degradation operator. A novelty of our approach is to bring into play recent nonsmooth analysis results. The pertinence of the proposed method is illustrated in an image restoration example.	algorithm;circuit restoration;convex function;elegant degradation;image restoration;iterative method;loss function;optimization problem;relevance;subderivative;variational principle	Jérôme Bolte;Patrick L. Combettes;Jean-Christophe Pesquet	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5652173	iterative reconstruction;wavelet;image restoration;computer vision;minimisation;mathematical optimization;combinatorics;convex optimization;convergence;nonlinear programming;computer science;deconvolution;machine learning;mathematics;iterative method;signal-to-noise ratio;statistics	Vision	56.39408951132408	-72.64982667205736	169431
37cfc55d38bb86af71c0bbb4248b8328385b0a92	pattern-coupled sparse bayesian learning for inverse synthetic aperture radar imaging	electronic mail;bayes methods;synthetic aperture radar expectation maximisation algorithm gaussian processes learning artificial intelligence radar computing radar imaging;scattering;covariance matrices;radar imaging;imaging;pattern coupled sparse bayesian learning method hyperparameters map estimate maximum a posterior estimate em algorithm expectation maximization algorithm 2d pattern coupled hierarchical gaussian prior block sparse structure isar imaging inverse synthetic aperture radar imaging;pattern coupled sparse bayesian learning block sparse structure expectation maximization em isar;signal processing algorithms;bayes methods imaging signal processing algorithms radar imaging electronic mail scattering covariance matrices	We propose a pattern-coupled sparse Bayesian learning method for inverse synthetic aperture radar (ISAR) imaging by exploiting a block-sparse structure inherent in ISAR target images. A two-dimensional pattern-coupled hierarchical Gaussian prior is proposed to model the pattern dependencies among neighboring scatterers on the target scene. An expectation-maximization (EM) algorithm is developed to infer the maximum a posterior (MAP) estimate of the hyperparameters, along with the posterior distribution of the sparse signal. Numerical results are provided to illustrate the effectiveness of the proposed algorithm.	expectation–maximization algorithm;numerical linear algebra;numerical method;sparse matrix;synthetic intelligence	Huiping Duan;Lizao Zhang;Jun Fang;Lei Huang;Hongbin Li	2015	IEEE Signal Processing Letters	10.1109/LSP.2015.2452412	medical imaging;computer vision;machine learning;pattern recognition;scattering;radar imaging	Vision	61.18444820747181	-71.7793674101531	169839
e56e7a53b2adb964ba8543ba845edaec2c71ddb6	dct-based image up-sampling using anchored neighborhood regression		Down-sampling in the discrete cosine transform (DCT) domain is preferable for images coded by DCT transform, such as JPEG/MJPEG/H.264, etc. Recent researches show that the truncated high-frequency DCT coefficients during the DCT down-sampling process can be estimated by learning the correlations between low-frequency and high-frequency DCT coefficients. In this paper, we propose to utilize the powerful super-resolution framework using sparse dictionaries with anchored neighborhood regression to significantly improve the accuracy of the estimated high-frequency DCT coefficients. Experimental results show that the proposed framework outperforms the state-of-the-art DCT-based up-sampling methods in terms of PSNR (0.3–1.63dB) and SSIM values for standard image datasets Set5 and Set14, while the computational time of the proposed method is 23× times faster than the state-of-the-art learning-based method using k-NN MMSE due to pre-computation of the ridge regression during the training process.	coefficient;computation;dictionary;discrete cosine transform;h.264/mpeg-4 avc;jpeg;k-nearest neighbors algorithm;peak signal-to-noise ratio;precomputation;sampling (signal processing);sparse matrix;structural similarity;super-resolution imaging;time complexity	Kwok-Wai Hung;Jianmin Jiang;Qinglong Chang;Xu Wang	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296561	computer vision;motion jpeg;discrete cosine transform;structural similarity;pattern recognition;artificial intelligence;sampling (statistics);jpeg;computer science	Vision	59.491425531445984	-68.83696543628085	169962
b3b42b1c059986625c235b7dbe6bc23c3e1c16fd	shrinking gradient descent algorithms for total variation regularized image denoising	total variation;rof model;image denoise;gradient method	Total variation regularization introduced by Rudin, Osher, and Fatemi (ROF) is widely used in image denoising problems for its capability to preserve repetitive textures and details of images. Many efforts have been devoted to obtain efficient gradient descent schemes for dual minimization of ROF model, such as Chambolle’s algorithm or gradient projection (GP) algorithm. In this paper, we propose a general gradient descent algorithm with a shrinking factor. Both Chambolle’s and GP algorithm can be regarded as the special cases of the proposed methods with special parameters. Global convergence analysis of the new algorithms with various step lengths and shrinking factors are present. Numerical results demonstrate their competitiveness in computational efficiency and reconstruction quality with some existing classic algorithms on a set of gray scale images.	algorithm;experiment;gradient descent;grayscale;local convergence;noise reduction;numerical analysis;numerical method;radio over fiber;total variation denoising	Mingqiang Li;Congying Han;Ruxin Wang;Tiande Guo	2017	Comp. Opt. and Appl.	10.1007/s10589-017-9931-8	mathematical optimization;mathematics;grayscale;gradient method;noise reduction;algorithm;total variation denoising;gradient descent	ML	56.14208794371007	-71.57022537001973	169974
00c98dac84657377a4ca50ab270ca69b584f70eb	optimal choice of regularization parameter in image denoising	digital radiography;settore inf 01 informatica;total variation regularization;bayesian filtering;denoising;settore ing inf 06 bioingegneria elettronica e informatica	The Bayesian approach applied to image denoising gives rise to a regularization problem. Total variation regularizers have been introduced with the motivation of being edge preserving. However we show here that this may not always be the best choice in images with low/medium frequency content like digital radiographs. We also draw the attention on the metric used to evaluate the distance between two images and how this can influence the choice of the regularization parameter. Lastly, we show that hypersurface regularization parameter has little effect on the filtering quality.	matrix regularization;noise reduction	Mirko Lucchese;Iuri Frosio;N. Alberto Borghese	2011		10.1007/978-3-642-24085-0_55	regularization perspectives on support vector machines;regularization;computer vision;econometrics;mathematical optimization;computer science;noise reduction;mathematics;total variation denoising;statistics	ML	55.39374164276018	-71.9664510623126	170807
0db8921f7a9501b23f8ad7bc209314e7759cf40f	b-spline signal processing. ii. efficiency design and applications	metodo cuadrado menor;traitement signal;spline;methode moindre carre;least squares approximations;interpolation;image processing;least squares method;recursive filtering algorithms;splines mathematics digital filters filtering and prediction theory image processing least squares approximations signal processing;interpolacion;algoritmo recursivo;cubic spline image pyramid b spline signal processing design recursive filtering algorithms smoothing spline least squares approximation digital filters;procesamiento imagen;cubic spline image pyramid;least squares approximation;low pass filter;filtrage recursif;digital filter;traitement image;splines mathematics;b spline signal processing;process design;filtering and prediction theory;filtro paso bajo;laplacian pyramid;smoothing methods;algorithme recursif;filtre passe bas;filtering algorithms;efficient implementation;image edge detection;filtro numerico;computational complexity;smoothing;senal numerica;signal processing;digital filters;alisamiento;signal numerique;design;low pass filters;recursive algorithm;esplin cubico;spline cubique;filtrado recursivo;digital signal;signal processing algorithms;procesamiento senal;data structure;lissage;smoothing spline;recursive filtering;spline interpolation;cubic spline;spline signal processing process design signal processing algorithms low pass filters smoothing methods image edge detection filtering algorithms interpolation computational complexity;filtre numerique	For pt.I see ibid., vol.41, no.2, p.821-33 (1993). A class of recursive filtering algorithms for the efficient implementation of B-spline interpolation and approximation techniques is described. In terms of simplicity of realization and reduction of computational complexity, these algorithms compare favorably with conventional matrix approaches. A filtering interpretation (low-pass filter followed by an exact polynomial spline interpolator) of smoothing spline and least-squares approximation methods is proposed. These techniques are applied to the design of digital filters for cubic spline signal processing. An efficient implementation of a smoothing spline edge detector is proposed. It is also shown how to construct a cubic spline image pyramid that minimizes the loss of information in passage from one resolution level to the next. In terms of common measures of fidelity, this data structure appears to be superior to the Gaussian/Laplacian pyramid. >		Michael Unser;Akram Aldroubi;Murray Eden	1993	IEEE Trans. Signal Processing	10.1109/78.193221	spline interpolation;spline;computer vision;mathematical optimization;digital filter;data structure;low-pass filter;image processing;monotone cubic interpolation;interpolation;computer science;cubic hermite spline;signal processing;mathematics;thin plate spline;least squares;statistics	Embedded	54.74677169878471	-66.4724391895791	170974
7ad5f832548eabc8fff8de1d466c4111cc2f6514	detection of microcalcifications using a nonuniform noise model	noise estimation;quantum noise;robust estimator;high frequency	For the detection of microcalcifications in mammograms, ac- curate noise estimation is of crucial importance. In this paper we present a method that makes a robust estimate of the signal dependent image noise, by taking into account quantum noise and detector inhomogene- ity. In digital mammograms, high frequency image noise is dominated by quantum noise, which in raw images can be described by a square root model where the noise is proportional to the pixel value. However, due to detector inhomogeneity, the anode heel effect and other sources of varia- tion, noise properties vary across an image. We developed a method that deals with these effects in a general way, by making a nonuniform noise model that is pixel value and location dependent. This is established by subdividing the image into tiles. In each tile a square root model of the noise is estimated, that by interpolation gives a model of the noise as a function of pixel value and location. Results indicate some improvement in microcalcification detection, when this model is used to estimate noise in images acquired with an inhomogeneous detector.		Guido van Schie;Nico Karssemeijer	2008		10.1007/978-3-540-70538-3_53	gradient noise;gaussian noise;image noise;computer vision;electronic engineering;colors of noise;dark-frame subtraction;value noise;noise temperature;engineering;noise measurement;electrical engineering;shot noise;noise floor;salt-and-pepper noise	Robotics	62.492213691651116	-69.40782438102255	171466
9e449acc4e8d63631c8b02a8d0e976212195c411	texture minification using quad-trees and fipmaps		The paper extends the recently published methods for image reconstruction and texture minification using the generalized ripmap method, named fipmap, and quad-trees. Fipmap is based on the technique of partitioned iterated function systems, used in fractal image compression. The quad-tree texture reconstruction algorithm works well for many standard cases. The special cases can be solved using the fipmap minification. The approach was applied for textures from architectural image sequences and the results are very promising.	algorithm;anisotropic filtering;fractal compression;image compression;iterated function system;iteration;iterative reconstruction;quadtree	Alexander Bornik;Andrej Ferko	2002		10.2312/egs.20021013	computer vision;minification;quadtree;mathematics;artificial intelligence	Vision	55.92564770261333	-69.73974073563232	171698
94f580abc1a1edf8af6968fd29f7c609c15c1a58	image reduction with local reduction operators	electronic mail;image resolution;080108 neural;mathematical operators image reconstruction image resolution;local reduction operator;image reduction;natural images;weak localization;mathematical operators;evolutionary and fuzzy computation;image reconstruction;pixel;image reconstruction image reduction local reduction operator averaging function;averaging function;algorithm design and analysis;noise harmonic analysis pixel algorithm design and analysis image reconstruction matlab electronic mail;matlab;noise;harmonic analysis	In this work we propose an image reduction algorithm based on weak local reduction operators. We use several averaging functions to build these operators and we analyze their properties. We present experimental results where we apply the algorithm and weak local reduction operators in procedures of reduction, and later, reconstruction of images. We analyze these results over natural images and noisy images.	algorithm;salt (cryptography);salt-and-pepper noise	Daniel Paternain;Humberto Bustince;Javier Fernández;Gleb Beliakov;Radko Mesiar	2010	International Conference on Fuzzy Systems	10.1109/FUZZY.2010.5584891	iterative reconstruction;algorithm design;computer vision;mathematical optimization;image resolution;computer science;noise;theoretical computer science;harmonic analysis;mathematics;pixel;weak localization	EDA	57.682866161488896	-66.91088729338941	171733
f2d71d53d04d039483dedccd9daf09ef034640e3	a denoising approach via wavelet domain diffusion and image domain diffusion		This paper presents a new image denoising algorithm based on wavelet transform and nonlinear diffusion. Although the wavelet domain diffusion methods are very effective in image denoising, the salient artifacts are still produced. On the other hand, the image domain diffusion methods output the denoised image with fewer artifacts. So, unlike the previous denoising methods employing wavelet transform and diffusion scheme, the proposed method implements the diffusion not only in the wavelet domain but also in the image domain. The new method is called image denoising method combining the wavelet domain diffusion and the image domain diffusion (WDD-IDD). In the process of denoising, the initial denoised image is obtained by carrying out the wavelet domain isotropic diffusion. And then, the final denoised image is produced by applying the image domain anisotropic diffusion on the initial denoised image. It is noted that the image domain anisotropic diffusion scheme is constructed based on the feature of initial denoised image. In addition, to exemplify the power of the proposed method, the processing is restricted to the non-subsampled shearlet transform (NSST) domain which can better capture the geometry features of image than other wavelet transform. The tests show that the proposed WDD-IDD produces a better result in term of peak signal-to-noise ratio (PSNR), structural similarity (SSIM), and visual effect compared to the related methods.	algorithm;anisotropic diffusion;exemplification;grayscale;image processing;noise reduction;nonlinear system;nonlocal lagrangian;peak signal-to-noise ratio;shearlet;structural similarity;visual effects;wavelet transform;wiener filter	Xiaobo Zhang	2016	Multimedia Tools and Applications	10.1007/s11042-016-3778-3	computer vision;mathematical optimization;second-generation wavelet transform;fast wavelet transform;anisotropic diffusion;non-local means	Vision	56.33122976309783	-68.73799697777731	171822
2f98f255c9e22ae2096e7ec195353ada7d2144b9	low level image processing and analysis using radius filters	order statistics;impulse noise;speckle noise;image frequency decomposition;rank filters;edge detection and sharpening	A novel class of nonlinear L-filters based on local statistics is introduced, which will be called Radius Filters (RFs). The underlying idea for the construction of RFs is the sorting of the values of the input signal belonging in a sliding window, according to their distance from their mean value, in a vector T r that carries significant signal information, while the filter output is defined as a linear combination of the elements of the vector T r . RFs are simple, intuitive and easy to implement while they offer a unique signal formation, which provides insight and in depth signal interpretation for the proper filter design. Moreover, they may considered as a generalization of the OS Filters, since OS filters may be easily produced from the RFs. A number of low level image processing applications of the RFs such as impulse noise removal, speckle noise removal, adapting filtering, edge detection in noisy images, image sharpening and image frequency decomposition are outlined.	image processing	Kostas D. Tsirikolias	2016	Digital Signal Processing	10.1016/j.dsp.2015.12.001	speckle noise;computer vision;order statistic;impulse noise;mathematics;statistics	Graphics	55.096541067165774	-67.45005155581136	171832
f3e6fac353c9dd5596cb735964a2ee3d08e1e256	identification of geometrical properties of red blood cells from light scattering images using svd	boundary element method;image processing;light scattering;singular value decomposition;human red blood cell	In this paper an automatic identification method of geometrical properties of Red Blood Cells (RBCs) using light scattering images, is presented. A small number of features are estimated by the pixels’ intensity projection into an RBC-space. The basis of the RBC-space is derived using the singular vectors of a set of known RBCs. The nearest neighbor rule is used to classify any image projection to the known RBC coordinates. Since, the dimension ability of the RBC-space is significantly lower than the whole scattering image-space, it is easier to compare projections than original images. Considering the above idea, a Singular Value Decomposition (SVD) approach is implemented in this work. The database includes 1188 simulated scattering images, obtained by means of the Boundary Element Method (BEM). The identification accuracy of the actual RBC shape is estimated using three feature sets in the presence of additive white Gaussian noise from 60 to 10 dB SNR, giving a mean error rate less than 1 percent of the actual RBC shape.Moreover, an open-class classification problem was solved using RBC scattering images with new shapes and landscape images. © 2012 Elsevier Ltd. All rights reserved.	additive white gaussian noise;automatic identification and data capture;boundary element method;role-based collaboration;signal-to-noise ratio;singular value decomposition;subsurface scattering;utility functions on indivisible goods	George Apostolopoulos;Stephanos V. Tsinopoulos;Evangelos Dermatas	2013	Mathematical and Computer Modelling	10.1016/j.mcm.2012.12.030	computer vision;boundary element method;image processing;mathematics;geometry;light scattering;optics;singular value decomposition;algebra	Vision	63.80808891772013	-69.7468742365738	171917
aa688bbe58bc6650d44f5b82201fff8be29dbd04	deconvolution of medical ultrasound images via parametric inverse filtering	medical ultrasound images;parametric inverse filtering;biological tissues;image resolution;blind deconvolution;fourier transform;ultrasound;image restoration;ultrasound transducers;finite frequency bandwidth;shift invariant subspace;frequency response;statistical properties;ultrasound imaging;medical image processing biological tissues biomedical ultrasonics deconvolution fourier transforms image resolution image restoration;tissue reflectivity;medical image processing;finite dimensional principal shift invariant subspace;fourier transforms;deconvolution;biomedical image processing;deconvolution biomedical imaging ultrasonic imaging medical diagnostic imaging filtering image resolution image restoration frequency bandwidth biomedical transducers;biomedical ultrasonics;tissue reflectivity blind deconvolution techniques medical ultrasound images parametric inverse filtering ultrasound transducers finite frequency bandwidth image resolution image restoration fourier transform finite dimensional principal shift invariant subspace;in silico;blind deconvolution techniques	The finite frequency bandwidth of ultrasound transducers and the non-negligible width of transmitted acoustic beams are the most significant factors that limit the resolution of medical ultrasound imaging. As a result, in order to recover diagnostically important image details, which are often obscured due to the resolution limitations, an image restoration procedure should be applied. The current study addresses the problem of reconstructing ultrasound images by means of the blind deconvolution techniques. Particularly, the proposed deconvolution method is based on inversely filtering the complex-valued ultrasound images with a restoration kernel, whose Fourier transform is modeled as a member of a finite-dimensional, principal shift-invariant subspace. This approach presents a novel and very versatile way of modeling the frequency response of the inverse filter, in which the latter is defined by a few parameters, which can be estimated from the data using some reasonable assumptions on statistical properties of the tissue reflectivity. The effectiveness of the proposed method is demonstrated through a number of in silico and in vivo examples	acoustic cryptanalysis;bandwidth (signal processing);blind deconvolution;circuit restoration;frequency response;image restoration;inverse filter;medical ultrasound;transducer;video-in video-out	Oleg V. Michailovich;Allen R. Tannenbaum	2006	3rd IEEE International Symposium on Biomedical Imaging: Nano to Macro, 2006.	10.1109/ISBI.2006.1624891	fourier transform;computer vision;computer science;mathematics;blind deconvolution;optics	Vision	54.40378524607454	-79.76084704845337	172209
5428b6f957d79d1d5312c3f862bf728c522ee0ec	regularity classes for locally orderless images	processus gauss;regularite;parametre dispersion;scale parameter;regularidad;methode echelle multiple;fonction reguliere;regularity;metodo escala multiple;espacio escala;classification;maximal order;funcion matematica;scale space;paradigm;tolerancia;derivee;paradigme;mathematical function;funcion regular;multiscale method;tolerance;fonction mathematique;gaussian process;paradigma;proceso gauss;derivada;clasificacion;smooth function;espace echelle;derivative;parametro dispersion	Gaussian scale space permits one to compute image derivatives. The limitation to some finite order is not inherent in the paradigm itself (Gaussian blurred functions are always smooth), but is caused by the interplay of (at least) two external factors. One is the ratio of the Gaussian scale parameter versus the atomic scale that limits physically or perceptually meaningful sizes (e.g. pixel size, or in general any scale at which the image is “locally orderless”). The second factor involved is the fiducial level of tolerance. Together these factors conspire to determine a maximal order beyond which differential structure becomes meaningless. Thus they give rise to the notion of regularity classes for images akin to the conceptual C-classification pertaining to mathematical functions. We study the relationship between the maximal differential order k, the ratio of inner scale to atomic scale, and the prescribed tolerance level, and draw several conclusions that are of practical interest when considering image derivatives.	fiducial marker;image derivatives;maximal set;pixel;programming paradigm;scale space	Luc Florack;Remco Duits	2003		10.1007/3-540-44935-3_18	smoothness;scale space;scale parameter;biological classification;derivative;calculus;gaussian process;mathematics;geometry;function;statistics	Vision	63.56401264811649	-68.28188465879616	172228
1624f8a1ea3381847deb3fe905d470a100f5b2a7	global solutions of variational models with convex regularization	global solution;convex optimization;65k15;68u10;total variation;variational models;49m20;49m29;variational methods;calibrations	We propose an algorithmic framework to compute global solutions of variational models with convex regularity terms that permit quite arbitrary data terms. While the minimization of variational problems with convex data and regularity terms is straight forward (using for example gradient descent), this is no longer trivial for functionals with non-convex data terms. Using the theoretical framework of calibrations the original variational problem can be written as the maximum flux of a particular vector field going through the boundary of the subgraph of the unknown function. Upon relaxation this formulation turns the problem into a convex problem, however, in higher dimension. In order to solve this problem, we propose a fast primal dual algorithm which significantly outperforms existing algorithms. In experimental results we show the application of our method to outlier filtering of range images and disparity estimation in stereo images using a variety of convex regularity terms.	binocular disparity;calculus of variations;computer vision;convex optimization;convex set;diffusing update algorithm;filter (signal processing);gradient descent;graphics;interaction;ishikawa diagram;linear programming relaxation;markov random field;mathematical optimization;matrix regularization;maxima and minima;optimization problem;variational principle	Thomas Pock;Daniel Cremers;Horst Bischof;Antonin Chambolle	2010	SIAM J. Imaging Sciences	10.1137/090757617	convex analysis;subderivative;mathematical optimization;conic optimization;combinatorics;mathematical analysis;convex optimization;calibration;convex combination;variational analysis;linear matrix inequality;convex hull;mathematics;total variation;proper convex function	ML	54.42006874404052	-72.17401984113624	172236
24dd00bf11b7d449b99aa96020d7a4e6ac521049	robust surface reconstruction via triple sparsity	alternate minimization strategy robust surface reconstruction triple sparsity prior image reconstruction imaging applications optimization method double sparse prior residual gradient surface gradient;minimisation image reconstruction;surface reconstruction;sparsity;non convex regularization sparsity surface reconstruction;noise surface reconstruction image reconstruction robustness noise measurement estimation optimization;non convex regularization	Reconstructing a surface/image from corrupted gradient fields is a crucial step in many imaging applications where a gradient field is subject to both noise and unlocalized outliers, resulting typically in a non-integrable field. We present in this paper a new optimization method for robust surface reconstruction. The proposed formulation is based on a triple sparsity prior: a sparse prior on the residual gradient field and a double sparse prior on the surface gradients. We develop an efficient alternate minimization strategy to solve the proposed optimization problem. The method is able to recover a good quality surface from severely corrupted gradients thanks to its ability to handle both noise and outliers. We demonstrate the performance of the proposed method on synthetic and real data. Experiments show that the proposed solution outperforms some existing methods in the three possible cases: noise only, outliers only and mixed noise/outliers.	dhrystone;experiment;gradient;mathematical optimization;optimization problem;photometric stereo;sparse matrix	Hicham Badri;Hussein M. Yahia;Driss Aboutajdine	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.293	mathematical optimization;surface reconstruction;machine learning;pattern recognition;mathematics;sparsity-of-effects principle;statistics	Vision	56.957083748917846	-72.73103953221785	172624
725261f06bc9303d335e062ef7d3feda0cf6f81c	image denoising with nonsubsampled wavelet-based contourlet transform	i level binary tree decomposition;contourlet transform image denoising geometric image transform wavelet transform nonsubsampled directional filter bank i level binary tree decomposition;wavelet transforms filtering theory image denoising;filter bank;image processing;natural images;image denoising wavelet transforms filter bank noise reduction image processing anisotropic magnetoresistance additive noise fuzzy systems binary trees psnr;wavelet based contourlet transform;wavelet transforms;shift invariance;wavelet transform;noise reduction;transforms;nonsubsampled directional filter bank;contourlet transform;nonsubsampled directional filter banks;directional filter bank;geometric image transform;image denoising;denoising;shift invariance denoising nonsubsampled directional filter banks contourlet transform;multi resolution;shift invariant;filtering theory;noise;binary tree	Contourlet has shown its charming performance in image processing. The multi-resolution and multi-direction characteristics ensure the transform a good ability in handling the abundant texture information in natural images. However, the contourlet is not shift-invariant because of the subsampled filter structure. Thus, the contourlet will cause the visual artifact in image denoising applications. In this paper, we construct a geometric image transform by combining 2D wavelet transform and nonsubsampled directional filter banks. The nonsubsampled filter banks enable the proposed method to be shift-invariance. The nonsubsampled directional filter banks are implemented via an i-level binary tree decomposition. In order to assess the applicability of the proposed method, we extend it to image denoising. The numerical results show that this method can obtain higher PSNR and restrain the visual artifact compared with using wavelet transform and contourlet transform.	binary tree;contourlet;filter bank;image processing;noise reduction;numerical analysis;peak signal-to-noise ratio;tree decomposition;visual artifact;wavelet transform	Zhe Liu;Huanan Xu	2008	2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2008.458	computer vision;contourlet;speech recognition;image processing;computer science;pattern recognition;noise reduction;wavelet transform	Robotics	57.17183219750082	-67.54451430849707	172634
91f88e53f0960df90201e8a2366a0381e7e50a35	bayesian selection of scaling laws for motion modeling in images	turbulence characterization scaling laws turbulent motion nonparametric regularizer optic flow estimation constrained minimization lagrangian duality optimal regularization parameter simulated bidimensional turbulence multiscale motion model;minimization;turbulent motion;lagrangian duality;image motion analysis;turbulence characterization;image resolution;motion estimation atmospheric turbulence bayes methods flow simulation image sequences meteorology;self similar process;scaling law;bayes methods;navier stokes equations;atmospheric turbulence;optic flow estimation;bayesian methods;optical flow estimation;motion estimation;simulated bidimensional turbulence;scaling laws;integrated optics;constrained minimization;dual problem;optimal regularization parameter;optical imaging;first order;piv;image sequence;mathematical model;nonparametric regularizer;optical sensors;flow simulation;estimation de mouvement;bayesian methods optical sensors motion estimation image sequences meteorology inverse problems apertures image motion analysis lagrangian functions navier stokes equations;multiscale motion model;meteorology;lagrangian functions;apertures;inverse problems;data models;image sequences;flot optique;turbulence	Based on scaling laws describing the statistical structure of turbulent motion across scales, we propose a multiscale and non-parametric regularizer for optic-flow estimation. Regularization is achieved by constraining motion increments to behave through scales as the most likely self-similar process given some image data. In a first level of inference, the hard constrained minimization problem is optimally solved by taking advantage of lagrangian duality. It results in a collection of first-order regularizers acting at different scales. This estimation is non-parametric since the optimal regularization parameters at the different scales are obtained by solving the dual problem. In a second level of inference, the most likely self-similar model given the data is optimally selected by maximization of Bayesian evidence. The motion estimator accuracy is first evaluated on a synthetic image sequence of simulated bi-dimensional turbulence and then on a real meteorological image sequence. Results obtained with the proposed physical based approach exceeds the best state of the art results. Furthermore, selecting from images the most evident multiscale motion model enables the recovery of physical quantities, which are of major interest for turbulence characterization.	duality (optimization);expectation–maximization algorithm;experiment;first-order reduction;hierarchical database model;image scaling;matrix regularization;model selection;motion estimation;optical flow;regular expression;self-similarity;synthetic intelligence;turbulence	Patrick Héas;Étienne Mémin;Dominique Heitz;Pablo D. Mininni	2009	2009 IEEE 12th International Conference on Computer Vision	10.1109/ICCV.2009.5459353	turbulence;data modeling;aperture;computer vision;mathematical optimization;duality;image resolution;bayesian probability;computer science;inverse problem;motion estimation;first-order logic;optical imaging;mathematical model;mathematics;geometry;self-similar process;statistics	Vision	66.90112511780892	-68.90576188833069	172693
fe7ab57f706f3d9c84f271ac584f46b33a801755	model-based tikhonov-miller image restoration	model based method;optimal model;image restoration;image restoration additive noise degradation least squares methods stability image reconstruction vectors convolution statistics mathematical model;tikhonov miller image restoration;reconstruction error minimisation;miller tikhonov restoration method;optimization model;tikhonov regularisation;a priori model;local information;tikhonov regularisation tikhonov miller image restoration miller tikhonov restoration method local information optimal model reconstruction error minimisation model based method a priori model	We propose a new Miller-Tikhonov restoration method where an a priori model of the solution is included. In sharp contrast with the classical method, this approach incorporates local informations. We show that the optimal model can be directly calculated from the data or a priori given and adjusted by minimizing the reconstruction error.	circuit restoration;email;image restoration;ringing (signal)	V. Barakat;B. Guilpart;Robert Goutte;Rémy Prost	1997		10.1109/ICIP.1997.647768	image restoration;computer vision;econometrics;mathematical optimization;computer science;mathematics;tikhonov regularization;statistics	Vision	59.25558773400941	-70.42498611868628	173093
8befd2dc0e4bc0be69dd467059fadba248577472	spatially adaptive regularized iterative high-resolution image reconstruction algorithm	video;image restoration;image quality;remote sensing;image sensors;medical imaging		algorithm;image resolution;iterative method;iterative reconstruction	Won Bae Lim;Min Kyu Park;Moon Gi Kang	2001				Vision	58.50548767011816	-71.87442470189586	173281
1c05af12eb46a637b9b37c552f26d6b7f046950d	spectral partitioning for structure from motion	eigenvalues and eigenfunctions;eigenvalues and eigenfunctions computer vision hessian matrices optimisation image motion analysis;reprojection error;optimisation;image motion analysis;bottom up;video camera spectral partitioning large scale optimization problems hessian based partitioning reprojection error eigenvector domain knowledge heuristics structure from motion matrix partitioning algorithms fiedler vector newton raphson method computer vision;proceedings;newton raphson method;general techniques;eigenvalues;eigenvector;computer vision;eigenvalue;domain knowledge;hessian based partitioning;machine vision;spectral partitioning;optimization;post print;heuristics;fiedler vector;hessian;matrix partitioning algorithms;large scale optimization problems;video camera;structure from motion;large scale optimization;eigenvectors;hessian matrices;optimization methods	We propose a spectral partitioning approach for large-scale optimization problems, specifically structure from motion. In structure from motion, partitioning methods reduce the problem into smaller and better conditioned subproblems which can be efficiently optimized. Our partitioning method uses only the Hessian of the reprojection error and its eigenvectors. We show that partitioned systems that preserve the eigenvectors corresponding to small eigenvalues result in lower residual error when optimized. We create partitions by clustering the entries of the eigenvectors of the Hessian corresponding to small eigenvalues. This is a more general technique than relying on domain knowledge and heuristics such as bottom-up structure from motion approaches. Simultaneously, it takes advantage of more information than generic matrix partitioning algorithms.	algorithm;binary space partitioning;cluster analysis;computer form factor;encode;hessian;heuristic (computer science);integer factorization;international standard book number;international symposium on fundamentals of computation theory;mathematical optimization;numerical linear algebra;reprojection error;sparse matrix;structure from motion;top-down and bottom-up design;virtual reality headset	Drew Steedly;Irfan A. Essa;Frank Dellaert	2003		10.1109/ICCV.2003.1238457	computer vision;mathematical optimization;machine vision;eigenvalues and eigenvectors;computer science;theoretical computer science;machine learning;mathematics	Vision	59.08655283357516	-73.63956564531827	173418
320424c6d8fdf510dca547e426a114b0a1252f25	parallelizable inpainting and refinement of diffeomorphisms using beltrami holomorphic flow	image texture image enhancement image resolution;image resolution;texture mapping;video processing;computer graphic;image texture;image enhancement;medical image;image sequence;super resolution;medical imaging parallelizable inpainting diffeomorphism refinement diffeomorphism inpainting beltrami holomorphic flow algorithm image sequence inpainting diffeomorphism super resolution global parameterization local parameterization texture mapping video processing computer graphics	In this paper, we propose novel algorithms for inpainting and refinement of diffeomorphisms. We first represent a diffeomorphism by its Beltrami coefficient. Then it is possible to refine and inpaint the diffeomorphism by processing this Beltrami coefficient. With the inpainted/refined Beltrami coefficient, we construct a new diffeomorphism using the exact Beltrami holomorphic flow algorithm proposed in this paper. We apply our algorithms on several practical applications, which include the inpainting of a highly distorted diffeomorphism, the inpainting of image sequences of deforming shapes, the super-resolution of diffeomorphisms and the global parameterization of cortical surfaces by combining local parameterizations. Experiments show that our algorithm can solve these problems with natural and smooth results. We demonstrate how our proposed method can be widely applied in areas from texture mapping to video processing, and from computer graphics to medical imaging.	algorithm;co-ment;computer graphics;experiment;graphics processing unit;image resolution;inpaint;inpainting;jaccard index;matthews correlation coefficient;maximum flow problem;medical imaging;piecewise linear continuation;polygon mesh;refinement (computing);super-resolution imaging;texture mapping;video processing	Tsz Wai Wong;Xianfeng Gu;Tony F. Chan;Lok Ming Lui	2011	2011 International Conference on Computer Vision	10.1109/ICCV.2011.6126521	image texture;texture mapping;computer vision;topology;image resolution;computer science;mathematics;geometry;video processing;superresolution	Vision	54.31765141827807	-69.14138371947894	173474
590e8355d9b8f399a8e14309cad31a13c43e9f2d	semi-supervised statistical region refinement for color image segmentation	eigenvalue problem;image segmentation;prior knowledge;mixture model;pattern recognition;semi supervised grouping;region merging;color image segmentation	9 Some authors have recently devised adaptations of spectral grouping algorithms to integrate prior knowledge, as constrained eigenvalues problems. In this paper, we improve and adapt a recent statistical region merging approach to this task, as a non11 parametric mixture model estimation problem. The approach appears to be attractive both for its theoretical benefits and its experimental results, as slight bias brings dramatic improvements over unbiased approaches on challenging digital pictures. 13 2004 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.	algorithm;algorithmic efficiency;color image;computation;digital media;image segmentation;interaction;mixture model;pattern recognition;pixel;refinement (computing);semi-supervised learning;semiconductor industry;statistical region merging;time complexity	Richard Nock;Frank Nielsen	2005	Pattern Recognition	10.1016/j.patcog.2004.11.009	computer vision;computer science;machine learning;segmentation-based object categorization;pattern recognition;mixture model;image segmentation;scale-space segmentation	Vision	56.6706759425303	-69.46921271755708	173513
80210f7e316b4685af574b07aa6eea0ef934dc9c	on prediction error compressive sensing image reconstruction for face recognition		Abstract This paper explores the scope of spatial domain sparse representation for the application to develop a fast and robust remote end face recognition (FR) scheme in the framework of compressive sensing (CS). At the source end, error images as the difference between the original and the predicted images, are obtained using the different predictors that offer compressive measurements. Sub-sample measurements of the sparse error image and part of the original image are then transmitted. At the destination end, the test image is obtained from its partial information and CS reconstructed error image. Principal Component Analysis is used to extract the important features from the reconstructed image followed by FR. Performance of the proposed method is studied using collaborative representation based classifier with regularized least square method, applied on two databases, AR and ORL and an accuracy of 93.99% for the former and 91.5% for the latter is observed.	compressed sensing;facial recognition system;iterative reconstruction	Suparna Biswas;Jaya Sil;Santi Prasad Maity	2018	Computers & Electrical Engineering	10.1016/j.compeleceng.2017.11.009	iterative reconstruction;compressed sensing;computer science;mean squared prediction error;facial recognition system;computer vision;sparse approximation;principal component analysis;standard test image;least squares;artificial intelligence;pattern recognition	Vision	59.913227264532146	-68.67456380749881	173551
71b81c57e855b45ddf5a34573779c9d7ac7e1011	a parallel linearized admm with application to multichannel tgv-based image restoration		A parallel linearized alternating direction method of multipliers (PLADMM) is proposed to solve large-scale imaging inverse problems, which involve the sum of several linear-operator-coupled nonsmooth terms. In the proposed method, the proximity operators of the nonsmooth terms are called individually at each iteration and the auxiliary variables existing in the classical ADMM are excluded. Therefore, the proposed method possesses a highly parallel structure and most of its substeps can be executed simultaneously. The application to multichannel total generalized variation (TGV) based image restoration shows the effectiveness of the proposed method.	augmented lagrangian method;circuit restoration;image restoration;iteration	Chuan He;Chang-Hua Hu;Xuelong Li	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296469	acceleration;image restoration;computer vision;operator (computer programming);mathematical optimization;artificial intelligence;computer science;inverse problem;convex function;convergence (routing)	Robotics	56.69101093702297	-71.86826508212054	174045
007f9e107294750061751159cefde6f8369d3f1b	image reconstruction using projections from a few views by discrete steering combined with dart	image restoration;tomography;x rays	In this paper, we propose an algebraic reconstruction technique (ART) based discrete tomography method to reconstruct an image accurately using projections from a few views. We specifically consider the problem of reconstructing an image of bottles filled with various types of liquids from X-ray projections. By exploiting the fact that bottles are usually filled with homogeneous material, we show that it is possible to obtain accurate reconstruction with only a few projections by an ART based algorithm. In order to deal with various types of liquids in our problem, we first introduce our discrete steering method which is a generalization of the binary steering approach for our proposed multi-valued discrete reconstruction. The main idea of the steering approach is to use slowly varying thresholds instead of fixed thresholds. We further improve reconstruction accuracy by reducing the number of variables in ART by combining our discrete steering with the discrete ART (DART) that fixes the values of interior pixels of segmented regions considered as reliable. By simulation studies, we show that our proposed discrete steering combined with DART yields superior reconstruction than both discrete steering only and DART only cases. The resulting reconstructions are quite accurate even with projections using only four views.© (2012) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.	dart (programming language);iterative reconstruction	Junghyun Kwon;Samuel Moon-Ho Song;Brian Kauke;Douglas P. Boyd	2012		10.1117/12.909721	image restoration;computer vision;simulation;tomography;computer graphics (images)	Vision	56.41043138127521	-75.72969239278865	174584
5b2169b52ba1d2f2e4605ec8e7290c50ff606ef5	edge preserving image restoration using convex optimization	optimisation;image restoration;convex optimization;image enhancement;multiple constraints;inverse problems image restoration optimisation image enhancement;edge preservation edge preserving image restoration convex optimization original image blurred noisy image l sub 2 norm based regularized inverse solutions restored images l sub 1 norm gradient regularizer ellipsoid algorithm gradient constraint;inverse problems;image restoration ellipsoids signal processing algorithms constraint optimization image reconstruction iterative algorithms signal restoration digital signal processing smoothing methods optimization methods	Restoring an original image from its blurred and noisy version is usually ill-posed. 12 norm based regularized inverse solutions l e a d to restored images with the edges blurred. Some recent work has focused on using the 11 norm of the gradient as a regularizer. Alternatively convgx optimization approaches have been used to provide flexibility to impose multiple constraints on the solution. In this paper, we combine these methods using a convex optimization approach based on the ellipsoid algorithm to impose, among other constraints, an 11 norm of the gradient constraint. This results in a restored image with good edge preservation capabilities.	algorithm;circuit restoration;convex optimization;ellipsoid method;gradient;image restoration;mathematical optimization;shadow volume;well-posed problem	Kadagattur G. Srinidhi;Dana H. Brooks	1998		10.1109/ICIP.1998.723685	image restoration;computer vision;mathematical optimization;combinatorics;convex optimization;computer science;inverse problem;mathematics;geometry	Vision	56.17262788109574	-72.65903440248807	174667
bf164597e460bc5bac7668ede18d10f70711664d	a coded aperture compressive imaging array and its visual detection and tracking algorithms for surveillance systems	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;coded aperture;compressive imaging;uk phd theses thesis;compressive sensing;life sciences;motion detection and tracking;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	In this paper, we propose an application of a compressive imaging system to the problem of wide-area video surveillance systems. A parallel coded aperture compressive imaging system is proposed to reduce the needed high resolution coded mask requirements and facilitate the storage of the projection matrix. Random Gaussian, Toeplitz and binary phase coded masks are utilized to obtain the compressive sensing images. The corresponding motion targets detection and tracking algorithms directly using the compressive sampling images are developed. A mixture of Gaussian distribution is applied in the compressive image space to model the background image and for foreground detection. For each motion target in the compressive sampling domain, a compressive feature dictionary spanned by target templates and noises templates is sparsely represented. An l(1) optimization algorithm is used to solve the sparse coefficient of templates. Experimental results demonstrate that low dimensional compressed imaging representation is sufficient to determine spatial motion targets. Compared with the random Gaussian and Toeplitz phase mask, motion detection algorithms using a random binary phase mask can yield better detection results. However using random Gaussian and Toeplitz phase mask can achieve high resolution reconstructed image. Our tracking algorithm can achieve a real time speed that is up to 10 times faster than that of the l(1) tracker without any optimization.	clinical use template;closed-circuit television;coded aperture;coefficient;compressed sensing;dictionary [publication type];image resolution;imaging system;iterative reconstruction;masks;mathematical optimization;mixture model;motion detector;normal statistical distribution;online and offline;real-time clock;requirement;sampling (signal processing);sampling - surgical action;simulation;sparse matrix;standard test image;toeplitz hash algorithm	Jing Chen;Yongtian Wang;Hanxiao Wu	2012		10.3390/s121114397	computer vision;simulation;speech recognition;telecommunications;computer science;bioinformatics;electrical engineering;optics;compressed sensing	Vision	54.5617402593114	-77.11222001753075	174838
cedb7dd53b8e367522d936995de482c4dbf04067	fuzzy diffusion filter with extended neighborhood	image filter;extended neighborhood;diffusivity;fuzzy similarity	Anisotropic diffusion filters, which are motivated from heat diffusion between mediums, have become a widely used technique in the field of image processing. In the initial proposals of anisotropic diffusion filters, 4-neighborhood values with diffusivity functions are computed independently for each spatial location because of numerical approximation. However, anisotropic diffusion filters could not be used in real-time image and video processing applications because they need diffusivity parameters, which must be specified by users in every sampling period. In this study, a fuzzy adaptive diffusion filter using extended neighborhood without diffusivity functions has been developed. The fuzzy adaptive diffusion filter does not require any parameter chosen by user and therefore they could be employed in real-time applications. In the fuzzy adaptive diffusion filter, a similarity transformation by means of relation matrix and fuzzy logic is carried out. Accordingly, the similarity image, output of transformation, is directly used as a heat diffusion coefficient in the diffusion filter. Results show that the fuzzy adaptive diffusion filter is very efficient for removing noise in image while preserving edges.		Çetin Elmas;Recep Demirci;Ugur Güvenc	2013	Expert Syst. Appl.	10.1016/j.eswa.2012.05.042	adaptive filter;computer vision;mathematical optimization;kernel adaptive filter;thermal diffusivity;control theory;mathematics;anisotropic diffusion;composite image filter	Vision	55.606117042331604	-67.65238323578807	175187
a12df5141c97b97ea065b08e1289542f01f356a5	adaptive motion-compensated filtering of noisy image sequences	filtering;compensacion;filtrage;movimiento;image processing;adaptive filters filtering image sequences spatiotemporal phenomena layout nonlinear filters motion estimation pixel cameras mean square error methods;weighted averaging;adaptive filtering;filtrado adaptable;filtrado;ruido;procesamiento imagen;motion estimation;noise suppression;motion;image bruitee;traitement image;linear minimum mean square error;motion compensated;filtering and prediction theory;imagen sonora;interference suppression;adaptive filters;supresion;compensation;noisy image;mouvement;digital filters;bruit;image sequence;filtro adaptable;motion estimation adaptive filters digital filters filtering and prediction theory image sequences interference suppression;secuencia imagen;filtrage adaptatif;lmmse filtering adaptive spatiotemporal filter adaptive weighted averaging filter noise suppression image sequences motion trajectory performance low signal to noise ratios local linear minimum mean square error;filtre adaptatif;signal to noise ratio;adaptive filter;sequence image;noise;suppression;image sequences	The authors propose a novel adaptive spatiotemporal filter, called the adaptive weighted averaging (AWA) filter, for effective noise suppression in image sequences without introducing visually disturbing blurring artifacts. Filtering is performed by computing the weighted average of image values within a spatiotemporal support along the estimated motion trajectory at each pixel. The weights are determined by optimizing a well defined mathematical criterion, which provides an implicit mechanism for deemphasizing the contribution of the outlier pixels within the spatiotemporal filter support to avoid blurring. The AWA filter is therefore particularly well suited for filtering sequences that contain segments with abruptly changing scene content due to, for example, rapid zooming and changes in the view of the camera. The performance of the proposed AWA filter is compared with that of the spatiotemporal, local linear minimum mean square error (LMMSE) filtering. The results demonstrate that the proposed AWA filter-outperforms the LMMSE filter, especially in the cases of low signal-to-noise ratios and abruptly varying scene content. >		Mehmet K. Özkan;M. Ibrahim Sezan;A. Murat Tekalp	1993	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.257217	adaptive filter;computer vision;image processing;computer science;root-raised-cosine filter;control theory;mathematics	Vision	55.284563989505614	-66.3858273140995	175449
03d721bdbbdb48262b47fddad83837663f70d014	partition belief median filter based on dempster-shafer theory for image processing	pulse noise;gaussian noise;methode recursive;evaluation performance;least mean square;methode moindre carre moyen;enfoque credal;lms algorithm;performance evaluation;image processing;median filter;least mean squares methods;teoria de la evidencia;implementation;signal entree;evaluacion prestacion;impulse noise;ruido gaussiano;metodo recursivo;procesamiento imagen;recursive method;evidence theory;credal approach;bruit impulsion;filtrage recursif;traitement image;input signal;senal entrada;theorie de l evidence;teoria dempster shafer;weighted sums;filtro mediano;dempster shafer theory;bruit gaussien;impulsive noise;dempster shafer;mixture of gaussians;filtrado recursivo;implementacion;ruido impulso;filtre median;approche credibiliste;recursive filtering;theorie dempster shafer	A novel median-type filter controlled by evidence fusion is proposed for removing noise from images. The fusion of evidence based on the Dempster-Shafer evidence theory, providing a way to deal with the uncertainty in the evidence fusion, indicates to what extent a noise is considered. The filter proposed here is obtained as a weighted sum of the current pixel value and the output of the median filter, and the weight is set based on the belief value of the input signal sequence. The efficient step-like function is used to partition the belief space, and the least mean square (LMS) algorithm is applied to obtain the optimal weight for each block. Moreover, to improve the performance, the new filter is recursively implemented. Experimental results have demonstrated that the proposed filter can outperform many well-accepted median-based filters in preserving image details while effectively suppressing impulsive noises, and it also works satisfactorily in reducing Gaussian as well as the mixture of Gaussian and impulsive noise.	image processing;median filter	Tzu-Chao Lin	2008	Pattern Recognition	10.1016/j.patcog.2007.06.011	adaptive filter;nonlinear filter;median filter;raised-cosine filter;computer vision;least mean squares filter;dempster–shafer theory;kernel adaptive filter;image processing;computer science;artificial intelligence;root-raised-cosine filter;mathematics;filter design;bilateral filter;gaussian filter;algorithm;statistics;salt-and-pepper noise;m-derived filter	Vision	54.79506266671995	-66.28822177950369	176113
a3cb71e6cfee03a37c63b3a7f8f796f77f67678f	primal-dual algorithm based on gauss-seidel scheme with application to multiplicative noise removal	computacion informatica;primal dual;multiplicative noise;ciencias basicas y experimentales;gauss seidel iteration;matematicas;total variation;linearized augmented lagrangian;grupo a	Due to the strong edge preserving ability and low computational cost, the total variation (TV) regularization has been developed as one promising approach to solve the multiplicative denoising problem. In recent years, many efficient algorithms have been proposed for computing the numerical solution of TV-based convex variational models. Among these methods, the (linearized) augmented Lagrangian algorithm (ALM) and the primal-dual hybrid gradient (PDHG) algorithm are two of the most effective and most widely used techniques. In this paper, inspired by the connection of the ALM and PDHG algorithms, we develop an improved primal-dual algorithm for multiplicative noise removal. In the proposed algorithm, an auxiliary variable, which is updated by the Gauss-Seidel scheme, is introduced to accelerate the original primal-dual framework. The global convergence property of the proposed algorithm is also investigated. Numerical experiments on the multiplicative denoising show that the proposed algorithm outperforms the current state-of-the-art methods.	diffusing update algorithm;gauss–seidel method;multiplicative noise	Dai-Qiang Chen;Xinpeng Du;Yan Zhou	2016	J. Computational Applied Mathematics	10.1016/j.cam.2015.04.003	mathematical optimization;combinatorics;mathematical analysis;calculus;multiplicative noise;mathematics;pohlig–hellman algorithm;total variation;algebra	HPC	56.25817530332629	-71.38400585937816	176673
9c1046b335e9850214fcedb534ecb999c020f288	blind inpainting using ℓ0 and total variation regularization	minimisation;gaussian noise;iterative process;speckle;poisson observation models;impulse noise;rayleigh multiplicative model;minimization scheme;total variation regularization;noise measurement;additives;iterative methods;logarithmic transformation;image reconstruction;transforms;total variation;l 0 regularization;tv;blind inpainting	In this paper, we address the problem of image reconstruction with missing pixels or corrupted with impulse noise, when the locations of the corrupted pixels are not known. A logarithmic transformation is applied to convert the multiplication between the image and binary mask into an additive problem. The image and mask terms are then estimated iteratively with total variation regularization applied on the image, and    $\ell _{0}$    regularization on the mask term which imposes sparseness on the support set of the missing pixels. The resulting alternating minimization scheme simultaneously estimates the image and mask, in the same iterative process. The logarithmic transformation also allows the method to be extended to the Rayleigh multiplicative and Poisson observation models. The method can also be extended to impulse noise removal by relaxing the regularizer from the    $\ell _{0}$    norm to the    $\ell _{1}$    norm. Experimental results show that the proposed method can deal with a larger fraction of missing pixels than two phase methods, which first estimate the mask and then reconstruct the image.	inpainting;matrix regularization;total variation denoising	Manya V. Afonso;João Miguel Raposo Sanches	2015	IEEE Trans. Image Processing	10.1109/TIP.2015.2417505	iterative reconstruction;speckle pattern;gaussian noise;minimisation;mathematical optimization;food additive;impulse noise;noise measurement;calculus;iterative and incremental development;mathematics;iterative method;data transformation;total variation;statistics	Vision	58.42586127558122	-71.75197757361221	176783
22b5b9828d142615749c0f4cbcee3ba513f9d6ee	compressed sensing for dose reduction in stem tomography		We designed a complete acquisition-reconstruction framework to reduce the radiation dosage in 3D scanning transmission electron microscopy (STEM). Projection measurements are acquired by randomly scanning a subset of pixels at every tilt-view (i.e., random-beam STEM or “RB-STEM”). High-quality images are then recovered from the randomly downsampled measurements through a regularized tomographic reconstruction framework. By fulfilling the compressed sensing requirements, the proposed approach improves the reconstruction of heavily-downsampled RB-STEM measurements over the current state-of-the-art technique. This development opens new perspectives in the search for methods permitting lower-dose 3D STEM imaging of electron-sensitive samples without degrading the quality of the reconstructed volume. A Matlab code implementing the proposed reconstruction algorithm has been made available online.	3d scanner;algorithm;compressed sensing;decimation (signal processing);electron;matlab;pixel;randomness;requirement;tomographic reconstruction;tomography	Luca Donati;Masih Nilchian;Michael Unser;Sylvain Trepout;Cédric Messaoudi;S. Marcoy	2017	2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)	10.1109/ISBI.2017.7950459	medical physics	Embedded	55.1114007478767	-76.90383350491267	176853
f9a52977a9d44d0748ab133b8bca462b0d61c024	a medical image enhancement method using adaptive thresholding in nsct domain combined unsharp masking	adaptive thresholding;nsct;unsharp masking;medical image enhancement	In the process of medical image formation, the medical image is often interfered by various factors, and it is deteriorated by some new noise that may reduce the quality of the obtained image, which affect the clinical diagnosis seriously. A new medical image enhancement method is proposed in this article. Firstly, the initial medical image is decomposed into the NSCT domain with a lowfrequency sub-band, and several high-frequency sub-bands. Secondly, linear transformation is adopted for the coefficients of the lowfrequency sub-band. An adaptive thresholding method is used for denoising the coefficients of the high-frequency sub-bands. Then, all sub-bands were reconstructed into spatial domains using the inverse transformation of NSCT. Finally, unsharp masking was used to enhance the details of the reconstructed image. The results of experiment show that the proposed method is superior to other methods in image entropy, EME, and PSNR. VC 2015 Wiley Periodicals, Inc. Int J Imaging Syst Technol, 25, 199–205, 2015; Published online in Wiley Online Library (wileyonlinelibrary.com). DOI: 10.1002/ima.22137	ct scan;coefficient;encrypted media extensions;futures studies;image editing;image formation;john d. wiley;medical imaging;microsoft research;noise reduction;peak signal-to-noise ratio;structural similarity;thresholding (image processing);unsharp masking	Lu Liu;Zhenhong Jia;Jie Yang;Nikola K. Kasabov	2015	Int. J. Imaging Systems and Technology	10.1002/ima.22137	unsharp masking;computer vision;speech recognition;computer science;mathematics;thresholding;computer graphics (images)	Vision	58.16310521401207	-67.13648985733492	177383
f72ec48391baa2f5ef9b073b9a5bf38540479930	a general framework for quadratic volterra filters for edge enhancement	volterra series;metodo cuadrado menor;nonlinear filters;filtre volterra quadratique;two dimensional case;methode moindre carre;edge enhancement;least squares approximations;kernel;image numerique;constant output;highpass filter;1d case;image processing;least squares method;least squares design quadratic volterra filters edge enhancement noise local image brightness local mean estimator highpass filter 1d case two dimensional case sinusoidal inputs constant output filter characterization;high pass filter;two dimensions;edge detection;filter characterization;optimal filter;information filtering;serie volterra;procesamiento imagen;high pass filters;traitement image;linear filtering;two dimensional digital filters;least squares design;frequency response;brightness;laplace equations;image enhancement;filtre passe haut;filtro optimal;local mean estimator;computational complexity;quadratic volterra filters;least square;imagen numerica;humans;amelioration contour;digital image;sinusoidal inputs;filtre optimal;local image brightness;information filters;nonlinear filters laplace equations image enhancement information filtering information filters humans kernel brightness frequency response design methodology;filtro paso alto;noise;design methodology;least squares approximations image enhancement noise brightness high pass filters edge detection nonlinear filters computational complexity two dimensional digital filters	An inherent problem in most image enhancement schemes is the amplification of noise, which, due to Weber's law, is mostly visible in the darker portions of an image. Using a special class of quadratic Volterra filters, we can adapt the enhancement process in a computationally efficient way to the local image brightness because these filters are approximately equivalent to the product of a local mean estimator and a highpass filter. We analyze and derive this subclass of quadratic Volterra filters by investigating the 1-D case first, and then we generalize the results to two dimensions. An important property of these filters is that they map sinusoidal inputs to constant outputs, which allows us to develop a new filter characterization that is more intuitive for our application than the 4-D frequency response. This description finally leads to a novel least-squares design methodology. Image enhancement results using our Volterra filters are superior to those obtained with standard linear filters, which we demonstrate both quantitatively and qualitatively.		Stefan Thurnhofer;Sanjit K. Mitra	1996	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.503911	network synthesis filters;computer vision;mathematical optimization;image processing;computer science;control theory;mathematics;prototype filter;high-pass filter;m-derived filter	Vision	55.03384649666985	-66.97710332736858	177507
f11cee1eb4d64a279062daf695db034a8b236a74	a depth estimating method from a single image using foe crf	minimum mean square error mmse;non stationary spatially variance;conditional random field crf;field of experts foe	A high-order conditional random field (CRF) for depth estimation from a single image is proposed in this paper. Instead of formulating the problem with the Guassian or Laplacian CRF modeling techniques, which cannot exploit the full potential offered by the probabilistic modeling, this paper proposes a depth estimation CRF model with field of experts (FoE) as the prior. The minimum mean square error (MMSE) criteria is used to infer depth. Moreover, it is assumed that the variance of depth estimation error varies spatially in depth estimation model. This allows the proposed method to enjoy the benefits offered by the flexible prior and have the advantages of making use of the non-stationary variance probability model. Experimental results indicate that the proposed method outperforms state-of-the-art approaches in terms of RMSE-error and log10-error.	autostereogram;conditional random field;depth map;mean squared error;stationary process	Xiaoyan Wang;Chunping Hou;Liangzhou Pu;Yonghong Hou	2014	Multimedia Tools and Applications	10.1007/s11042-014-2130-z	machine learning;pattern recognition;statistics	Vision	61.42851850442209	-70.98893896841915	177602
2dfc46083874561f1c90212a0709c2e1d389a548	parameter selections for tikhonov regularization image restoration	regularization parameter;tikhonov regularization image restoration regularization parameter selection method snr gcv method upre method l curve criterion one dimensional regularization method blurred image restoration inverse degeneration process image recovery one dimensional normal degeneration atmospheric turbulence image degradation model;singular value decomposition;image restoration;image restoration regularization parameter singular value decomposition;image restoration degradation atmospheric modeling matrix decomposition signal to noise ratio linear systems	The model of image degradation due to atmospheric turbulence can be decomposed into two one-dimensional normal degenerations in horizontal and vertical directions successively. The recovery is an inverse process of degeneration. Each column of blurred image was restored by one-dimensional regularization method, then each row of restored image in vertical direction was recovered with same method. The regularization parameter was selected with the L-curve criterion, GCV and UPRE method respectively, when the degenerated image was restored in every column, then in every row, and different recovery results were obtained with different parameter selections. Simulation results show that if the blurred image has high SNR, three types of regularization parameter selection methods reached similar accuracy in image restoration, the GCV method which don't need a priori variance of the noise is more stable and effective than other two methods.	circuit restoration;elegant degradation;image restoration;signal-to-noise ratio;simulation;turbulence	Bin Zhang;Fei Jin	2013	2013 Ninth International Conference on Natural Computation (ICNC)	10.1109/ICNC.2013.6818202	regularization;mathematical optimization;mathematical analysis;mathematics;geometry;tikhonov regularization	Vision	58.793490414480615	-70.0107437985512	177664
da1f680e92d3f8c3250d83ea2de19f4d52295fec	choice of a 2-d causal autoregressive texture model using information criteria	2 d autoregressive model;parametric model;lattice parameter;prediction error;image processing;estimation method;information criterion;information criteria;matrix inversion;estimation algorithm;autoregressive model;kullback divergence;order selection by information criteria;lattice representation;bayesian information criterion;reflection coefficient;characterization of textures;order selection	In the context of parametric modeling for image processing, we derive an estimation method for both the order and the parameters of 2-D causal autoregressive model with different geometries of support. Model parameters are estimated from a lattice representation, i.e. based on reflection coefficients. Lattice parameter estimation algorithms offer advantages compared to the Yule–Walker method: they do not require matrix inversion and their computation are robust and fast. For order selection, information criterion (IC) methods are the most commonly used. Therefore our order selection method is based on the combination of an IC and the prediction errors of models computed from the lattice parameter estimation algorithm. In this paper, we favour two consistent criteria compared to the non-consistent Akaike criterion: the first criterion is a 2-D extension of Bayesian information criterion; the second criterion, noted /b, extended here to the 2-D case, is a generalization drawn on Rissanen s works. Simulations are provided on synthetic and natural textures with quarter plane support and non-symmetrical half plane support. We validate our results on natural textures using the Kullback divergence. The results show the interest of the combination of 2-DFLRLS algorithm and /b criterion to characterize natural textures. 2002 Elsevier Science B.V. All rights reserved.	algorithm;amiga walker;ar (unix);autoregressive model;bayesian information criterion;causal filter;coefficient;computation;computer simulation;eisenstein's criterion;estimation theory;image processing;kullback–leibler divergence;lattice constant;parametric model;supervised learning;synthetic intelligence;texture filtering;texture mapping	Olivier Alata;Christian Olivier	2003	Pattern Recognition Letters	10.1016/S0167-8655(02)00301-X	econometrics;parametric model;image processing;mean squared prediction error;pattern recognition;reflection coefficient;mathematics;autoregressive model;lattice constant;bayesian information criterion;statistics	Vision	61.5032878055569	-70.740401922106	177830
85626c3cc41749ec13664c73723175ce38a68801	scale selection for anisotropic diffusion filter by markov random field model	noise estimation;markov random field model;scale space;image denoising;anisotropic diffusion filter;scale selection;tree reweighted message passing	The selection of stopping time (i.e., scale) significantly affects the performance of anisotropic diffusion filter for image denoising. This paper designs a Markov random field (MRF) scale selection model, which selects scales for image segments, then the denoised image is the composition of segments at their optimal scales in the scale space. Firstly, statistics-based scale selection criteria are proposed for image segments. Then we design a scale selection energy function in the MRF framework by considering the scale coherence between neighboring segments. A segment-based noise estimation algorithm is also developed to estimate the noise statistics efficiently. Experiments show that the performance of MRF scale selection model is much better than the previous global scale selection schemes. Combined with this scale selection model, the anisotropic diffusion filter is comparable to or even outperform the stateof-the-art denoising methods in performance. & 2010 Elsevier Ltd. All rights reserved.	algorithm;anisotropic diffusion;experiment;markov chain;markov random field;mathematical optimization;message passing;noise reduction;nonlocal lagrangian;pixel;scale space;smoothing;wavelet	Jian Sun;Zongben Xu	2010	Pattern Recognition	10.1016/j.patcog.2010.02.019	computer vision;scale space;computer science;machine learning;pattern recognition;mathematics;anisotropic diffusion;statistics	Vision	56.788600707935394	-68.79898458559907	177846
3db300c41c4bfdab1bee57b6c817ed6d1ac78fe5	finding optimal convex gray-scale structuring elements for morphological multiscale representation	least mean square;lms algorithm;multiscale representation;signal representation	Recent papers in multiscale morphological filtering, particularly, have renovated the interest in signal representation via multiscale openings. Although most of the analysis was done with flat structuring elements, extensions to grayscale structuring elements (GSE) are certainly possible. In fact, we have shown that opening a signal with convex and symmetric GSE does not introduce additional zero-crossings as the filter moves to a coarser scales. However, the issue of finding an optimal GSE is still an open problem. In this paper, we present a procedure to find an optimal GSE under the least mean square (LMS) algorithm subject to three constraints: The GSE must be convex, symmetric, and non-negative. The use of the basis functions simplifies the problem formulation. In fact, we show that the basis functions for convex and symmetric GSE are concave and symmetric, thus alternative constraints are developed. The results of this algorithm are compared with our previous work.© (1996) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	grayscale;morphological parsing	Aldo W. Morales;Sung-Jea Ko	1996		10.1117/12.235825	mathematical optimization;theoretical computer science;mathematics;algorithm	Vision	54.624482832771626	-69.07598960414539	178070
216be5faed9516c7cd8da471471e5cb9436f5a48	compressive sensing by learning a gaussian mixture model from measurements	sensors;training;noise measurement;vectors;estimation;mixture models compressed sensing covariance matrices gaussian processes maximum likelihood estimation;covariance matrices;compressive hyperspectral imaging compressive sensing gaussian mixture model closed form minimum mean squared error reconstruction gmm signal model maximum marginal likelihood estimator mmle linear compressive measurements low rank covariance matrices high speed video;image reconstruction;covariance matrices training vectors noise measurement image reconstruction sensors estimation;hyperspectral imaging compressive sensing gaussian mixture model gmm mixture of factor analyzers mfa maximum marginal likelihood estimator mmle inpainting high speed video	Compressive sensing of signals drawn from a Gaussian mixture model (GMM) admits closed-form minimum mean squared error reconstruction from incomplete linear measurements. An accurate GMM signal model is usually not available a priori, because it is difficult to obtain training signals that match the statistics of the signals being sensed. We propose to solve that problem by learning the signal model in situ, based directly on the compressive measurements of the signals, without resorting to other signals to train a model. A key feature of our method is that the signals being sensed are treated as random variables and are integrated out in the likelihood. We derive a maximum marginal likelihood estimator (MMLE) that maximizes the likelihood of the GMM of the underlying signals given only their linear compressive measurements. We extend the MMLE to a GMM with dominantly low-rank covariance matrices, to gain computational speedup. We report extensive experimental results on image inpainting, compressive sensing of high-speed video, and compressive hyperspectral imaging (the latter two based on real compressive cameras). The results demonstrate that the proposed methods outperform state-of-the-art methods by significant margins.	calcium-sensing receptor;compressed sensing;condition number;convergence (action);elegant degradation;estimated;expectation–maximization algorithm;experiment;gm(m);google map maker;hospital admission;inpainting;low-rank approximation;mfa message structure;marginal model;master of fine arts;mean squared error;mixture model;multi-factor authentication;normal statistical distribution;partial;speedup;bismuth subsalicylate 35 mg/ml oral suspension	Jianbo Yang;Xuejun Liao;Xin Yuan;Patrick Llull;David J. Brady;Guillermo Sapiro;Lawrence Carin	2015	IEEE Transactions on Image Processing	10.1109/TIP.2014.2365720	iterative reconstruction;econometrics;estimation;sensor;noise measurement;pattern recognition;mathematics;statistics	ML	54.01322832115139	-76.63730420742918	178373
dfa44200bf52d2031a6d0b3b44fef421c24e78f8	stereo depth estimation using synchronous optimization with segment based regularization	metodo regularizacion;metodo adaptativo;optimisation;vision ordenador;problema mal planteado;image segmentation;optimizacion;vision estereoscopica;regularization method;probleme mal pose;vision stereoscopique;disparity;methode regularisation;correspondence problem;methode adaptative;disparidad;computer vision;segment based regularization;ill posed problem;adaptive method;stereo;vision ordinateur;optimization;depth estimation;stereopsis;local minima;disparite;anisotropic smoothing	Stereo correspondence is inherently an ill-posed problem, which is addressed by regularization methods. This paper introduces a novel stereo correspondence method that uses two synchronous interdependent optimizations. The regularization of the correspondence problem is done adaptively by considering the image segments and the intermediate disparity maps of the two optimizations. Our adaptive regularization allows inter-segment diffusion at the beginning of the optimizations to be robust against local minima. When the two optimizations start producing similar disparity maps, our regularization prevents inter-segment diffusion to recover the depth discontinuities. Our experimental results showed that the proposed algorithm can handle sharp discontinuities well and provides disparity maps with accuracy comparable to the state of the art stereo methods. 2010 Elsevier B.V. All rights reserved.	algorithm;binocular disparity;correspondence problem;depth perception;interdependence;map;mathematical optimization;maxima and minima;well-posed problem	Tarkan Aydin;Yusuf Sinan Akgül	2010	Pattern Recognition Letters	10.1016/j.patrec.2010.07.012	computer vision;mathematical optimization;computer science;stereopsis;maxima and minima;mathematics;geometry;image segmentation;correspondence problem;stereophonic sound	Vision	53.97407787416349	-70.41051359826061	179068
b10f1acdea340f84fcf90b12792999d422733331	nonlocal-means image denoising technique using robust m-estimator	dinesh j peteri v k govindan abraham t mathew 降噪技术 均值图像 非局部 估计 稳健 均值滤波器 加权平均数 重量计算 nonlocal means image denoising technique using robust m estimator;image processing;denoising technique;weighted averaging;exponential function;estimating function;nonlocal means filter;linear model;brain atlas;image denoising;robust m estimators	Edge preserved smoothing techniques have gained importance for the purpose of image processing applications. A good edge preserving filter is given by nonlocal-means filter rather than any other linear model based approaches. This paper explores a different approach of nonlocal-means filter by using robust M-estimator function rather than the exponential function for its weight calculation. Here the filter output at each pixel is the weighted average of pixels with surrounding neighborhoods using the chosen robust M-estimator function. The main direction of this paper is to identify the best robust M-estimator function for nonlocal-means denoising algorithm. In order to speed up the computation, a new patch classification method is followed to eliminate the uncorrelated patches from the weighted averaging process. This patch classification approach compares favorably to existing techniques in respect of quality versus computational time. Validations using standard test images and brain atlas images have been analyzed and the results were compared with the other known methods. It is seen that there is reason to believe that the proposed refined technique has some notable points.	aharonov–bohm effect;algorithm;brain atlas;computation;image processing;linear model;noise reduction;nonlocal lagrangian;pixel;quantum nonlocality;relevance;similarity measure;smoothing;standard test image;time complexity	J. Dinesh Peter;V. K. Govindan;Abraham T. Mathew	2010	Journal of Computer Science and Technology	10.1007/s11390-010-9351-z	mathematical optimization;image processing;exponential function;linear model;pattern recognition;non-local means;statistics	Vision	57.22486874237964	-67.6305901054313	179070
a8f46ad9d41fa642f27805b16e5c8f4685a09617	non-convex non-local flows for saliency detection		We propose and numerically solve a new variational model for automatic saliency detection in digital images. Using a non-local framework we consider a family of edge preserving functions combined with a new quadratic saliency detection term. Such term defines a constrained bilateral obstacle problem for image classification driven by p-Laplacian operators, including the so-called hyper-Laplacian case (0 < p < 1). The related non-convex non-local reactive flows are then considered and applied for glioblastoma segmentation in magnetic resonance fluid-attenuated inversion recovery (MRI-Flair) images. A fast convolutional kernel based approximated solution is computed. The numerical experiments show how the non-convexity related to the hyperLaplacian operators provides monotonically better results in terms of the standard metrics.	approximation algorithm;bilateral filter;computation;computational complexity theory;computer vision;concave function;digital image;experiment;gradient;hyper-heuristic;iteration;numerical analysis;numerical linear algebra;obstacle problem;problem solving;quantization (signal processing);resonance;sparse matrix;thresholding (image processing);variational principle	Iván Ramírez;Gonzalo Galiano;Emanuele Schiavi	2018	CoRR		digital image;kernel (linear algebra);pattern recognition;operator (computer programming);monotonic function;artificial intelligence;salience (neuroscience);regular polygon;computer science;contextual image classification;obstacle problem	Vision	54.70633116367445	-71.32865459705144	179397
298017b2d5cfa00d96c87cdc9b17142a0f8d802c	stair blue noise sampling	anti aliasing;pair correlation function;blue noise;sampling	A common solution to reducing visible aliasing artifacts in image reconstruction is to employ sampling patterns with a blue noise power spectrum. These sampling patterns can prevent discernible artifacts by replacing them with incoherent noise. Here, we propose a new family of blue noise distributions, Stair blue noise, which is mathematically tractable and enables parameter optimization to obtain the optimal sampling distribution. Furthermore, for a given sample budget, the proposed blue noise distribution achieves a significantly larger alias-free low-frequency region compared to existing approaches, without introducing visible artifacts in the mid-frequencies. We also develop a new sample synthesis algorithm that benefits from the use of an unbiased spatial statistics estimator and efficient optimization strategies.	adaptive sampling;algorithm;aliasing;artifact (error);cobham's thesis;colors of noise;data structure;image plane;iterative reconstruction;mathematical optimization;mike lesser;noise power;sampling (signal processing);spatial analysis;spectral density	Bhavya Kailkhura;Jayaraman J. Thiagarajan;Peer-Timo Bremer;Pramod K. Varshney	2016	ACM Trans. Graph.	10.1145/2980179.2982435	gradient noise;gaussian noise;sampling;colors of noise;value noise;telecommunications;noise measurement;radial distribution function;mathematics;optics;statistics	Graphics	66.01740774815313	-71.22546307843506	179819
eb615185bc7813cdd4ede6fda32c683bf7dd361d	deblocking scheme for jpeg-coded images using sparse representation and all phase biorthogonal transform		—For compressed images, a major drawback is that those images will exhibit severe blocking artifacts at very low bit rates due to adopting Block-based Discrete Cosine Transform (BDCT). In this paper, a novel deblocking scheme using sparse representation is proposed. A new transform called All Phase Biorthogonal Transform (APBT) was proposed in recent years. APBT has the similar energy compaction property with Discrete Cosine Transform (DCT). It has very good column properties, high frequency attenuation characteristics, low frequency energy aggregation, and so on. In this paper, we use it to generate the over-completed dictionary for sparse coding. For Orthogonal Matching Pursuit (OMP), we select an adaptive residual threshold by combining blind image blocking assessment. Experimental results show that this new scheme is effective in image deblocking and can avoid over-blurring of edges and textures. We can obtain deblocked images in the receiver.	blocking (computing);data compaction;deblocking filter;dictionary;discrete cosine transform;graphics processing unit;image quality;jpeg;k-svd;matching pursuit;neural coding;openmp;singular value decomposition;sparse approximation;sparse matrix;video post-processing	Liping Wang;Cheng-You Wang;Xiao Zhou	2016	JCM	10.12720/jcm.11.12.1095-1101	deblocking filter;theoretical computer science;distributed computing;jpeg;sparse approximation;computer science;biorthogonal system	Vision	58.66731727074424	-68.46946343581382	179825
d8fe30c6d1df039df3936161b80857b6a3740bee	entropy-based wavelet de-noising method for time series analysis	de noising;uncertainty;wavelet transform;time series analysis;information entropy	The existence of noise has great influence on the real features of observed time series, thus noise reduction in time series data is a necessary and significant task in many practical applications. When using traditional de-noising methods, the results often cannot meet the practical needs due to their inherent shortcomings. In the present paper, first a set of key but difficult wavelet de-noising problems are discussed, and then by applying information entropy theories to the wavelet de-noising process, i.e., using the principle of maximum entropy (POME) to describe the random character of the noise and using wavelet energy entropy to describe the degrees of complexity of the main series in original series data, a new entropy-based wavelet de-noising method is proposed. Analysis results of both several different synthetic series and typical observed time series data have verified the performance of the new method. A comprehensive discussion of the results indicates that compared with traditional wavelet de-noising methods, the new proposed method is more effective and universal. Furthermore, because it uses information entropy theories to describe the obviously different characteristics of noises and the main series in the series data is observed first and then de-noised, the analysis process has a more reliable physical basis, and the results of the new proposed method are more reasonable and are the global optimum. Besides, the analysis process of the new proposed method is simple and is easy OPEN ACCESS Entropy 2009, 11 1124 to implement, so it would be more applicable and useful in applied sciences and practical engineering works.	entropy (information theory);global optimization;noise reduction;principle of maximum entropy;synthetic intelligence;theory;time series;wavelet	Yan-fang Sang;Jichun Wu;Qing-Ping Zhu;Ling Wang	2009	Entropy	10.3390/e11041123	econometrics;uncertainty;calculus;time series;cascade algorithm;mathematics;stationary wavelet transform;maximum entropy spectral estimation;statistics;entropy;wavelet transform	DB	64.60576205258067	-72.87136709758025	180043
eb833affca15be252a8b237fefdb6a2f8671e174	super-resolution reconstruction of hyperspectral imagery using an spectral unmixing based representational model	biological system modeling;spectral unmixing super resolution hyperspectral images intrinsic representation;image reconstruction;regression analysis geophysical image processing hyperspectral imaging image resolution maximum likelihood estimation;real hyperspectral images super resolution reconstruction spectral unmixing based representational model hyperspectral image super resolution spatial correlation spectral correlation linear spectral mixture model endmembers spatial information spatial autoregression model maximum likelihood estimation framework maximization algorithm;optimization;hyperspectral imaging;adaptation models;spatial resolution image reconstruction hyperspectral imaging biological system modeling adaptation models optimization;spatial resolution	Efficient super-resolution of hyperspectral images (HSI) relies on the representational model (RM) that is capable of capturing the spatial and spectral correlation in hyperspectral images. In this paper, the spectral information in hyperspectral images is explained by linear spectral mixture model (LSMM), which expressed the observed pixels as a linear combination of endmembers, and the spatial information is captured by a spatial auto-regression model. The two component is combined in the maximum likelihood estimation (MLE) framework and solved by the expectation and maximization (EM) algorithm. Experiments on both simulated and real hyperspectral images demonstrate that the proposed method is not only capable of providing an accurate and effective super-resolution reconstruction of the image, but also capable of resisting the influence of noise.	autoregressive model;expectation–maximization algorithm;horizontal situation indicator;mixture model;pixel;super-resolution imaging;tomographic reconstruction	Xiao Sun;Linlin Xu;Longshan Yang;Yujia Chen;Yuan Fang;Junhuan Peng	2016	2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2016.7729410	iterative reconstruction;full spectral imaging;computer vision;image resolution;geology;hyperspectral imaging;pattern recognition;physics;remote sensing	Vision	67.97611960997884	-66.714383250158	180321
a23208afc609a2bc996e4a3e99da00ec7908ede8	image reconstruction for compressed sensing based on joint sparse bases and adaptive sampling	compressed sensing;adaptive sampling;dct;double-density dual-tree complex wavelet transform	In this paper, we focus on tackling the problem that one sparse base alone cannot represent the different content of the image well in the image reconstruction for compressed sensing, and the same sampling rate is difficult to ensure the precise reconstruction for the different content of the image. To address this challenge, this paper proposed a novel approach that utilized two sparse bases for the representation of image. Moreover, in order to achieve better reconstruction result, the adaptive sampling has been used in the sampling process. Firstly, DCT and a double-density dual-tree complex wavelet transform were utilized as two different sparse bases to represent the image alternatively in a smoothed projected Landweber reconstruction algorithm. Secondly, different sampling rates were adopted for the reconstruction of different image blocks after segmenting the entire image. Experimental results demonstrated that the images reconstructed with the two bases were largely superior to that reconstructed with a single base, and the PSNR could be improved further after using the adaptive sampling.	adaptive sampling;algorithm;complex wavelet transform;compressed sensing;discrete cosine transform;iterative reconstruction;landweber iteration;mathematical optimization;peak signal-to-noise ratio;sampling (signal processing);smoothing;sparse matrix;windows aero	Huihui Li;Yan Zeng;Ning Yang	2017	Machine Vision and Applications	10.1007/s00138-017-0882-y	compressed sensing;computer science;pattern recognition;artificial intelligence;iterative reconstruction;reconstruction algorithm;computer vision;sampling (signal processing);sampling (statistics);discrete cosine transform;complex wavelet transform;adaptive sampling	Vision	59.543406536533396	-68.49877881986448	180712
ce0f4fc67a1518615c178fa35321224e18630bf0	fast algorithm for color texture image inpainting using the non-local ctv model	non local model;color texture images inpainting;the split bregman algorithm;mumford shad model;ctv model	The classical TV (Total Variation) model has been applied to gray texture image denoising and inpainting previously based on the non local operators, but such model can not be directly used to color texture image inpainting due to coupling of different image layers in color images. In order to solve the inpainting problem for color texture images effectively, we propose a non local CTV (Color Total Variation) model. Technically, the proposed model is an extension of local TV model for gray images but we take account of the coupling of different layers in color images and make use of concepts of the non-local operators. As the coupling of different layers for color images in the proposed model will increase computational complexity, we also design a fast Split Bregman algorithm. Finally, some numerical experiments are conducted to validate the performance of the proposed model and its algorithm.	algorithm;bregman divergence;color image;computational complexity theory;experiment;inpainting;nl (complexity);noise reduction;numerical analysis	Jinming Duan;Zhenkuan Pan;Baochang Zhang;Wanquan Liu;Xue-Cheng Tai	2015	J. Global Optimization	10.1007/s10898-015-0290-7	computer vision;artificial intelligence;mathematics;computer graphics (images)	Vision	55.64169392717285	-70.4662233355889	182038
ae5e0963483bf851d965ee3865795a145069d0b2	sar image despeckling based on a novel total variation regularization model and gf-3 data		We propose a novel total variation (TV) regularization model for synthetic aperture radar (SAR) image despeckling. A method combined with second-order and fourth-order TV sparse constraints is applied to reconstruct and despeckle SAR image. Experiments have been carried out in echo domain using GF-3 spaceborne SAR raw data. We introduce the target-to-clutter ratio (TCR) to quantitatively assess the SAR image quality and the effectiveness of target detection and multiplicative noise suppression. Experimental results show that the proposed second-order and fourth-order TV method can further effectively suppress SAR image speckles without compromising the details of image features according to both subjective visual assessment of image quality and objective evaluation using TCR.	aperture (software);clutter;image quality;matrix regularization;multiplicative noise;noise reduction;sparse matrix;synthetic data;total variation denoising;transmission control room;zero suppression	Qingjun Zhang;Tengfei Li;Yu Zhu;Lv Zheng	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518099	iterative reconstruction;raw data;computer vision;feature (computer vision);image quality;total variation denoising;artificial intelligence;synthetic aperture radar;multiplicative noise;speckle pattern;computer science	Vision	67.76585769977179	-66.3496456710068	182053
b5b7930b27b5d238c37eeb8ae09bef555f8a63c5	estimation of the variance of noise in digital images using a median filter	damping;distortion;smoothing methods;estimation;filtration;correlation coefficient;digital images	This paper proposes a method for estimating noise based on the assumption that in the process of smoothing with median filter only noise damping takes place. Knowledge of noise damping coefficient and the selection of the appropriate area allows for more accurate estimation of the variance of the normal noise distribution.	coefficient;digital image;median filter;smoothing	Grzegorz Mikolajczak;Jakub Peksinski	2016	2016 39th International Conference on Telecommunications and Signal Processing (TSP)	10.1109/TSP.2016.7760927	gradient noise;damping;gaussian noise;median filter;image noise;econometrics;mathematical optimization;estimation;distortion;value noise;telecommunications;filtration;computer science;noise measurement;mathematics;digital image;statistics;salt-and-pepper noise	Robotics	57.09040583053048	-67.51547953616287	182540
c0244cbab9ed06ff6cb518bd55629768dcaad208	robust tensor approximation with laplacian scale mixture modeling for multiframe image and video denoising		Sparse and low-rank models have been widely studied in the literature of signal processing and computer vision. However, as the dimensionality of dataset increases (e.g., multispectral images, dynamic MRI images, and video sequences), the optimality of vector and matrix-based data representations and modeling tools becomes questionable. Inspired by recent advances in sparse and low-rank tensor analysis, we propose a novel robust tensor approximation (RTA) framework with the Laplacian Scale Mixture (LSM) modeling for three-dimensional (3-D) data and beyond. Our technical contributions are summarized as follows: first, conceptually similar to robust PCA, we consider its tensor extension here—i.e., low-rank tensor approximation in the presence of outliers modeled by sparse noise; second, built upon previous work on tensor sparsity, we propose to model tensor coefficients with an LSM prior and formulate a maximum a posterior estimation problem for noisy observations. Both unknown sparse coefficients and hidden LSM parameters can be efficiently estimated by the method of alternating optimization; and third, we have derived closed-form solutions for both subproblems and developed computationally efficient denoising techniques for multiframe images and video. Experimental results on three datasets have shown that the proposed algorithm can better preserve the sharpness of important image structures and outperform several existing state-of-the-art image/video denoising methods (e.g., BM4D/VBM4D and tensor dictionary learning).		Weisheng Dong;Tao Huang;Guangming Shi;Xin Li	2018	IEEE Journal of Selected Topics in Signal Processing	10.1109/JSTSP.2018.2873047	mathematical optimization;tensor;mixture model;robustness (computer science);computer science;signal processing;curse of dimensionality;matrix (mathematics);video denoising;laplace operator	Vision	58.33752051632518	-70.36437134558157	182622
54e75428128ffd84dd0ce967ab66773fb68b0376	wavelet noise reduction based on energy features	energy feature;energy levels;device noise profile;wavelet transform;noise reduction;spatial relationships	This paper proposes a new algorithm based on energy features for noise reduction using wavelets. The device noise profile is obtained by the noise images taken from the imaging device so that it can represent the device's noise in multi-scale and multi-band. The energy feature takes advantage of the inter-scale relationship and spatial relationship of wavelet transformation. The wavelet coefficients are shrunk by the likelihood of noise or signal based on its energy level. The de-noised images are obtained by wavelet reconstruction. The results and comparison against common used methods show that the performance of our method is very promising despite simple structure.	noise reduction;wavelet noise	Guoyi Fu;S. Ali Hojjat;Alan C. F. Colchester	2008		10.1007/978-3-540-69812-8_8	spatial relation;gradient noise;wavelet noise;gaussian noise;wavelet;image noise;computer vision;speech recognition;value noise;computer science;noise measurement;energy level;noise;noise reduction;mathematics;wavelet packet decomposition;stationary wavelet transform;salt-and-pepper noise;wavelet transform	EDA	58.03164593082781	-67.92096286204695	182731
eb04068416ade86de63cf9d9939e14d0bc9b96f9	trainable nonlinear reaction diffusion: a flexible framework for fast and effective image restoration	analytical models;loss specific training;paper;image processing;image super resolution;diffusion processes;image restoration;computer vision;cuda;computational modeling;nonlinear reaction diffusion;package;mathematical model;nvidia;image denoising;nonlinear reaction diffusion loss specific training image denoising image super resolution jpeg deblocking;jpeg deblocking nonlinear reaction diffusion loss specific training image denoising image super resolution;jpeg deblocking;signal denoising;image restoration computational modeling analytical models diffusion processes mathematical model image denoising;nvidia geforce gtx 780 ti	Image restoration is a long-standing problem in low-level computer vision with many interesting applications. We describe a flexible learning framework based on the concept of nonlinear reaction diffusion models for various image restoration problems. By embodying recent improvements in nonlinear diffusion models, we propose a dynamic nonlinear reaction diffusion model with time-dependent parameters (i.e., linear filters and influence functions). In contrast to previous nonlinear diffusion models, all the parameters, including the filters and the influence functions, are simultaneously learned from training data through a loss based approach. We call this approach TNRD—Trainable Nonlinear Reaction Diffusion. The TNRD approach is applicable for a variety of image restoration tasks by incorporating appropriate reaction force. We demonstrate its capabilities with three representative applications, Gaussian image denoising, single image super resolution and JPEG deblocking. Experiments show that our trained nonlinear diffusion models largely benefit from the training of the parameters and finally lead to the best reported performance on common test datasets for the tested applications. Our trained models preserve the structural simplicity of diffusion models and take only a small number of diffusion steps, thus are highly efficient. Moreover, they are also well-suited for parallel computation on GPUs, which makes the inference procedure extremely fast.	autostereogram;circuit restoration;computation;computer vision;deblocking filter;graphics processing unit;high- and low-level;image restoration;inference;jpeg;noise reduction;nonlinear system;normal statistical distribution;parallel computing;super-resolution imaging	Yunjin Chen;Thomas Pock	2017	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2016.2596743	image restoration;computer vision;simulation;image processing;computer science;mathematical model;package;computational model;computer graphics (images)	Vision	54.00729580490271	-69.56666689331291	182787
34af473baa3acbaa5f3b8acba786988a03e83df8	video despeckling using shearlet tensor-based anisotropic diffusion		Abstract This paper provides an effective method for video speckle noise reduction based on 3D Tensor-based Anisotropic Diffusion technique in the Shearlet domain (V-STAD). The proposed model exploits the multi-scale geometric representation and the sparsity property of the shearlet transform to apply a robust tensorial diffusion at each shearlet coefficients. In fact, the robustness of diffusion tensor image smoothing was not well investigated. Therefore, we adopted the robust Tukeyu0027s biweight function in the proposed tensor-based anisotropic diffusion. By this way, the filter benefits from robust statistics and sparse directional image representation property of the shearlet transform in addition to the intrinsic temporal correlations between frames to be adopted to the anisotropic nature of diffusion tensor. The experimental results demonstrate promising despeckling solution as compared to well-known state-of-the-art video denoising methods, like VBM3D and VIDOLSAT. The proposed method has clearly shown superior despeckling capability and it simultaneously demonstrated better local image structures preservation without introducing artifacts.		Olfa Moussa;Nawres Khlifa;Noureddine Ben Abdallah	2018	Computer Aided Geometric Design	10.1016/j.cagd.2018.09.005	tensor;mathematical optimization;mathematics;anisotropic diffusion;smoothing;effective method;video denoising;diffusion mri;speckle noise;shearlet	EDA	56.70559098857583	-68.90832447991275	182853
348c4a8b4914a2d4a99b1b0c6f5c89c09f872192	texture analysis using probabilistic models of the unimodal and multimodal statistics of adaptive wavelet packet coefficients	model selection;gaussian mixture;bayes methods;statistical analysis wavelet analysis wavelet packets histograms bayesian methods statistics frequency wavelet coefficients image processing image analysis;natural images;maximum likelihood estimation;wavelet packet;image texture;wavelet transforms;probabilistic model;texture analysis;maximum likelihood estimation image texture gaussian distribution wavelet transforms bayes methods;map estimation;generalized gaussian;multimodal histograms texture analysis probabilistic models unimodal statistics multimodal statistics adaptive wavelet packet coefficients natural images constrained gaussian mixture generalized gaussian mixture bayesian methodology map estimates texture discrimination wavelet packet bases gaussian subband statistics leptokurtotic subband statistics;gaussian distribution	Although subband histograms of the wavelet coefficients of natural images possess a characteristic leptokurtotic form, this is no longer true for wavelet packet bases adapted to a given texture. Instead, three types of subband statistics are observed: Gaussian, leptokurtotic, and interestingly, in some subbands, multimodal histograms. These subbands are closely linked to the structure of the texture, and guarantee that the most probable image is not flat. Motivated by these observations, we propose a probabilistic model that takes them into account. Adaptive wavelet packet subbands are modelled as Gaussian, generalized Gaussian, or a constrained Gaussian mixture. We use a Bayesian methodology, finding MAP estimates for the adaptive basis, for subband model selection, and for subband model parameters. Results confirm the effectiveness of the proposed approach, and highlight the importance of multimodal subbands for texture discrimination and modelling.	coefficient;gaussian blur;model selection;multimodal interaction;network packet;statistical model;wavelet	Roberto Cossu;Ian H. Jermyn;Josiane Zerubia	2004	2004 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2004.1326615	normal distribution;image texture;statistical model;machine learning;pattern recognition;mathematics;wavelet packet decomposition;maximum likelihood;model selection;statistics;wavelet transform	Vision	60.78302857968473	-69.950292756222	183307
9f2feadea41e3a121f81c25e49585c138a901593	pixel-wise decay parameter adaption for nonlocal means image denoising	image denoising;denoising;radiometric corrections	Abstract. The globally fixed decay parameter is generally adopted inthe traditional nonlocal means method for similarity computation,which has a negative influence on its restoration performance. Toaddress this problem, we propose to adaptively tune the decayparameter for each image pixel using the golden section searchmethod based on the pixel-wise minimum mean square error,which can be estimated using the prefiltered result and the estimatednoise component. The quantitative and subjective comparisons ofrestoration performance among the proposed method and severalstate-of-the-art methods indicate that it can achieve a better perfor-mance in noise reduction, artifact avoidance, and detail preservation.© 2013 SPIE and IS&T [DOI: 10.1117/1.JEI.22.4.043034] 1 IntroductionNoise will be generated during signal acquisition and datatransmission due to malfunction, transmission errors, andstoragefaults. Noisewillaffectimagequality,therebyaffect-ing the accuracy of subsequent image processing tasks suchas segmentation and registration. Therefore, denoising isa crucial step to facilitate subsequent image processing.Numerous denoising methods have been proposed, suchas the spatial adaptivemethods,	noise reduction;nonlocal lagrangian;pixel	Yi Zhan;Mingyue Ding;Xuming Zhang	2013	J. Electronic Imaging	10.1117/1.JEI.22.4.043034	computer vision;mathematical optimization;computer science;noise reduction;mathematics;non-local means;statistics	Vision	57.29495066568131	-68.71344325017527	183497
94a159b2e0ada9c550c957d399fa45098ea3e5a5	study of the principal component analysis method for the correction of images degraded by turbulence		This article analyzes and discusses a well-known paper [D. Li, R.M. Mersereau and S. Simske, IEEE Letters on Geoscience and Remote Sensing, 3:4 (2007), pp. 340–344] that applies principal component analysis in order to restore image sequences degraded by atmospheric turbulence. We propose a variant of this method and its ANSI C implementation. The proposed variant applies to image sequences acquired with short as well as long exposure times. Examples of restored images using sequences of real atmospheric turbulence are presented. Real atmospheric turbulent image dataset acquisition is described and made available for download.	ansi c;download;earth sciences;lithium;principal component analysis;silo (dataset);turbulence	Tristan Dagobert;Yohann Tendero;Stéphane Landeau	2018	IPOL Journal	10.5201/ipol.2018.47		Vision	66.05618843934667	-66.67062009253564	184297
853e5c7b81c2f4c0c01b99cdbf9537cc4e57c6f4	single image super-resolution using coupled dictionary learning and cross domain mapping		In this paper, a new algorithm for single image super resolution using coupled wavelet and spatial domain dictionary pairs is proposed. The standard deviation parameter, which is approximately scale invariant for low and high resolution patch pairs is employed for clustering. A pair of online coupled dictionaries is learned for each cluster using a low resolution image. The standard deviation measure of a low resolution patch is used to select the appropriate cluster dictionary pair for reconstructing the high resolution counterpart. Experimental results show that the performance of the proposed algorithm is superior to the existing methods in terms of objective and subjective quality measures. The objective image quality is measured in terms of PSNR and SSIM. This paper also proposed an extended algorithm, based on selective sparse representation over a set of coupled dictionary pair. The extended algorithm applies the coupled dictionary based sparse framework for patches having high standard deviation. Whereas, low complexity patch collaging method is used to super resolve low standard deviation valued patches. It is found empirically that a large percentage of patches have low standard deviation values. Moreover, the selective approach significantly reduces the computational complexity without losing the overall reconstruction quality.	algorithm;approximation algorithm;autostereogram;cluster analysis;computational complexity theory;dictionary;image quality;image resolution;machine learning;peak signal-to-noise ratio;reduction (complexity);sparse approximation;sparse matrix;structural similarity;super-resolution imaging;wavelet	Hemant S. Goklani;S. Shravya;Jignesh N. Sarvaiya	2017	Multimedia Tools and Applications	10.1007/s11042-017-5084-0	wavelet;computer science;pattern recognition;structural similarity;image quality;cluster analysis;standard deviation;k-svd;computational complexity theory;artificial intelligence;sparse approximation	AI	58.9200676530902	-66.49667397489058	184946
1bbc9c74443712908ccdb8f015a10e85a3ebb86e	consistent multi-modal non-rigid registration based on a variational approach	similarity metric;euler lagrange equation;consistent registration;magnetic resonance image;finite difference method;finite difference scheme;variational approach;medical image;non rigid registration;implicit finite difference method;image registration;b spline based deformation;mutual information;multi modal image registration;iteration method	In this paper, a novel variational approach for multi-modal image registration based on consistent non-rigid transforms is proposed. The forward and backward transforms are computed in a variational framework simultaneously. A consistency energy is added into the variational registration framework and an iterative method assures that the forward and backward transforms are close approximate inverses of each other. This new scheme can provide more constraints on the forward and backward transforms and make them smoother. This reduces the chance of being stuck into a local minimum. We can incorporate the consistent transforms together with several kinds of information based similarity metrics such as mutual information and correlation ratio into the registration framework. An implicit finite difference scheme is used to solve the Euler–Lagrange equation numerically. Our proposed method is evaluated by using simulated multi-modal magnetic resonance images (MRI) and real medical images. The results of our proposed consistent registration and those using only forward or backward registration are compared. Improvements at the edge area can be easily seen from the registration results. 2005 Elsevier B.V. All rights reserved.	approximation algorithm;euler;euler–lagrange equation;finite difference method;image registration;iterative method;maxima and minima;modal logic;mutual information;numerical analysis;resonance;variational principle	Zhijun Zhang;Yifeng Jiang;Hung-Tat Tsui	2006	Pattern Recognition Letters	10.1016/j.patrec.2005.10.018	mathematical optimization;mathematical analysis;discrete mathematics;finite difference method;image registration;magnetic resonance imaging;mathematics;iterative method;mutual information	Vision	55.263559557720306	-71.57893311782159	185231
f6b1bb164be904a0803b28cb6a24c2984e408e55	depth map enhancement using adaptive moving least squares method with a total variation minimization		Accurate and fast depth map acquisition and enhancement is an important issue in the area of computer vision and image processing. In this study, we present a novel method for enhancing noisy depth maps using adaptive total variation minimization, which facilitates noise smoothing and boundary sharpening for a given depth map image but without previous information. We filter the noise in the depth map with a refined total variation minimization technique. Our experimental results demonstrate that the proposed method outperforms other competitive methods in both objective and subjective comparisons of depth map enhancement and denoising.	approximation;basis pursuit denoising;computer vision;deblurring;depth map;heightmap;image processing;linear least squares (mathematics);moving least squares;noise reduction;smoothing;synthetic intelligence;total variation diminishing	Sang Min Yoon;Jungho Yoon	2015	Multimedia Tools and Applications	10.1007/s11042-015-2905-x	computer vision;mathematical optimization;machine learning	Vision	56.85035083509442	-70.21097791201696	185245
92bf2ae0c5b40525593d9b371d06da9461d82010	a hybrid reconstruction method for quantitative pat	diffuse optical tomography;interior data;hybrid reconstruction algorithm;boundary cauchy data;numerical minimization;quantitative photoacoustic tomography;inverse problem;diffusion equation;65m32;74j25;92c55;vector field method;49n45	The objective of quantitative photoacoustic tomography (qPAT) is to reconstruct the diffusion and absorption properties of a medium from data of absorbed energy distribution inside the medium. Mathematically, qPAT can be formulated as an inverse coefficient problem for the diffusion equation. Past research showed that if the boundary values of the coefficients are known, then the interior values of the coefficients can be uniquely and stably reconstructed with two well-chosen data sets. We propose a hybrid numerical reconstruction procedure for qPAT that uses both interior energy data and boundary current data. We show that these data allow the unique reconstruction of the boundary and interior values of the coefficients. The numerical implementation is based on reformulating the inverse coefficient problem as a nonlinear optimization problem. An explicit reconstruction scheme is utilized to eliminate the unknown coefficients inside the medium so that we only need to minimize over the boundary values, which have significantly fewer degrees of freedom. Numerical simulations with synthetic data are presented to validate the method.	acoustic cryptanalysis;coefficient;diffuse optical imaging;iteration;mathematical optimization;matthews correlation coefficient;multispectral image;nonlinear programming;nonlinear system;numerical analysis;numerical linear algebra;numerical method;optimization problem;phantom reference;photoacoustic imaging;simulation;synthetic data;tomography	Kui Ren;Hao Gao;Hongkai Zhao	2013	SIAM J. Imaging Sciences	10.1137/120866130	diffuse optical imaging;diffusion equation;mathematical optimization;inverse problem;calculus;mathematics;geometry	Robotics	56.97751929663139	-74.82016392299325	185330
fa92679a5ae3e60637d8cadbb6787726b8df2cd2	state tracking of composite delaminations with a bayesian filter	flash thermography;flash thermography bayesian inference nondestructive evaluation;computed tomography delamination bayes methods aerospace industry time measurement computational modeling data models;bayesian inference;tracking filters aerospace materials bayes methods composite materials computerised tomography condition monitoring delamination mechanical energy storage mechanical engineering computing nondestructive testing production engineering computing;nondestructive evaluation;thermography based condition tracking state tracking composite delaminations bayesian filter nondestructive evaluation nde scan lifetime history aerospace industry bayesian inference damage state probabilistic estimates flash thermography nde data computed tomography ct data high resolution volume model ground truth	We propose a method for tracking the condition of a composite part using Bayesian filtering of nondestructive evaluation (NDE) data over the lifetime of the part. NDE provides information about the state of a part or material without destroying or degrading the part. The Bayesian process builds on the lifetime history of NDE scans and can give better estimates of material condition compared to the most recent scan alone, which is the common practice in the aerospace industry. Bayesian inference provides probabilistic estimates of damage state that are updated as each new set of NDE data becomes available. The method is tested on simulated data and then on an experimental data set. Flash thermography NDE data was collected over the lifetime of a part to serve as a time history of that part. Computed tomography (CT) data was also collected after each damage event and provided a high resolution volume model of damage that acted as 'truth'. After each time point, the condition estimate was compared to 'ground truth' from CT to evaluate the performance of the thermography-based condition tracking.	bayesian network;ct scan;ground truth;tomography;tracking system	Elizabeth D. Gregory;Stephen D. Holland	2015	2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)	10.1109/ICMLA.2015.189	computer vision;nondestructive testing;bayesian inference;statistics	Vision	56.11997615285789	-76.49594285402716	185894
7aa4d47d85e146876b34c16e8a33205564002308	projection selection for binary tomographic reconstruction using global uncertainty		Binary tomography focuses on the problem of reconstructing homogeneous objects from a small number of their projections. In many applications, incomplete projection data holds insufficient information for the correct reconstruction of the original object. In this paper, we provide an optimization based method to select the “most informative” projection set, using information of global uncertainty. Beside the projection data we assume no further knowledge of the image to be reconstructed. Still, we achieve approximately as accurate reconstruction results, as it is possible to gain with a former method that uses blueprint images to find the optimal set of projections. We give experimental results for validating our approach on artificial images of various structures.	tomographic reconstruction	Gábor Lékó;Péter Balázs;László G. Varga	2018		10.1007/978-3-319-93000-8_1	artificial intelligence;pattern recognition;computer vision;blueprint;computer science;tomography;tomographic reconstruction;small number;homogeneous;binary number	Vision	57.18679196722124	-76.85652930362008	186126
cdabdee008c8a279fe774f3e6467ab77af037e56	uncertainty estimation in vascular networks		Reconstructing vascular networks is a challenging task in medical image processing as automated methods have to deal with large variations in vessel shape and image quality. Recent methods have addressed this problem as constrained maximum a posteriori (MAP) inference in a graphical model, formulated over an overcomplete network graph. Manual control and adjustments are often desired in practice and strongly benefit from indicating the uncertainties in the reconstruction or presenting alternative solutions. In this paper, we examine two different methods to sample vessel network graphs, a perturbation and a Gibbs sampler, and thereby estimate marginals. We quantitatively validate the accuracy of the approximated marginals using true marginals, computed by enumeration.	approximation algorithm;bayes factor;bayesian network;burn-in;experiment;gibbs sampling;graph property;graphical model;image processing;image quality;information;medical imaging;model selection;norm (social);perturbation function;rendering (computer graphics);sampling (signal processing)	Markus Rempfler;Bjoern Andres;Bjoern H. Menze	2017		10.1007/978-3-319-67675-3_5	image processing;mathematical optimization;enumeration;image quality;maximum a posteriori estimation;mathematics;inference;graphical model;graph;gibbs sampling	ML	54.529820795972086	-74.95020770182695	186388
e5f535455dd626c9f9cd22df729620e04f832cc5	diffusion-based inpainting for coding remote-sensing data		Inpainting techniques based on partial differential equations (PDEs), such as diffusion processes, are gaining growing importance as a novel family of image compression methods. Nevertheless, the application of inpainting in the field of hyperspectral imagery has been mainly focused on filling in missing information or dead pixels due to sensor failures. In this letter, we propose a novel PDE-based inpainting algorithm to compress hyperspectral images. The method inpaints separately the known data in the spatial and spectral dimensions. Then, it applies a prediction model to the final inpainting solution to obtain a representation much closer to the original image. Experimental results over a set of hyperspectral images indicate that the proposed algorithm can perform better than a recent proposed extension to prediction-based standard CCSDS-123.0 at low bit rate, better than JPEG 2000 Part 2 with the DWT 9/7 as a spectral transform at all bit rates, and competitive to JPEG 2000 with principal component analysis, the optimal spectral decorrelation transform for Gaussian sources.	algorithm;decorrelation;defective pixel;discrete wavelet transform;image compression;inpainting;jpeg 2000;numerical partial differential equations;principal component analysis	Naoufal Amrani;Joan Serra-Sagrist&#x00E0;;Pascal Peter;Joachim Weickert	2017	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2017.2702106	mathematics;iterative reconstruction;remote sensing;computer vision;inpainting;artificial intelligence;pixel;image compression;principal component analysis;decorrelation;jpeg 2000;hyperspectral imaging;pattern recognition	Vision	59.260135425891306	-68.61451394091512	186539
a2cb67cdd874a57b8de6164e1a779baf361ccd37	directional image decomposition using retargeting pyramid	laplace transforms computer vision image denoising;streaming media image decomposition laplace equations wavelet transforms noise reduction image denoising;image denoising retargeting pyramid rp multiscale image decomposition laplacian pyramid low pass filtering process content aware image resizing computer vision research contourlet based directional image decomposition lp based contourlet transform	Retargeting pyramid (RP) is an alternative method for multiscale image decomposition to the well-known Laplacian pyramid (LP). RP can be obtained by replacing the low-pass filtering process in LP with content-aware image resizing (a.k.a. retargeting), which is a developing technique for computer vision researches. Furthermore, we use RP for contourlet-based directional image decomposition. In experimental results, the proposed decomposition outperforms the LP-based contourlet transform for image denoising.	computer vision;contourlet;image processing;image scaling;laplacian matrix;low-pass filter;noise reduction;pyramid (geometry);rp (complexity);retargeting;seam carving;whole earth 'lectronic link	Yuichi Tanaka;Keiichiro Shirai	2012	Proceedings of The 2012 Asia Pacific Signal and Information Processing Association Annual Summit and Conference		computer vision;pyramid;feature detection;contourlet;pattern recognition;mathematics;non-local means;computer graphics (images)	Vision	56.06329652606431	-67.77566101609878	186952
4c371c54e58baa15a744418f010f057e4fe4df5a	image denoising based on translation invariant directional lifting	translation invariant directional lifting;translation invariant;visual artifacts;gibbs phenomena;translation invariance;image coding;psnr;data compression;motion estimation;cycle spinning;gabor filters;image texture;wavelet transforms;accuracy;gabor filter;estimation;image compression;2 d gabor filters image denoising adaptive directional lifting image compression directional selectivity visual artifacts gibbs phenomena translation invariant directional lifting cycle spinning computational complexity orientation estimation technique;image edge detection;computational complexity;adaptive directional lifting;noise reduction;2 d gabor filters;anisotropic magnetoresistance;motion estimation data compression gabor filters image coding image denoising image texture;image denoising noise reduction computational complexity image coding space technology anisotropic magnetoresistance image edge detection intelligent structures psnr decorrelation;directional selectivity;2 d gabor filters adaptive directional lifting image denoising translation invariance;decorrelation;direction selectivity;space technology;image denoising;orientation estimation;intelligent structures;orientation estimation technique;spinning	Adaptive Directional Lifting (ADL) has been successfully implemented in image compression and denoising due to the feature of simple structure and flexible directional selectivity. However, image denoising by means of ADL introduces many visual artifacts caused by Gibbs phenomena due to the lack of translation invariance. In this paper, we propose a translation invariant directional lifting (TI-DL) by employing the cycle-spinning based technique to reduce artifacts in denoising results. Moreover, the inefficiency and high computational complexity of the orientation estimation technique in ADL strongly influences the performance. In order to achieve better denoising results, in this paper, 2-D Gabor filters are adopted for orientation estimation to achieve better orientation estimation results with lower complexity. Experimental results demonstrate that the proposed method achieves state-of-art denoising performance in terms of both objective (PSNR) and subjective (SSIM) evaluation.	computational complexity theory;gabor filter;image compression;lambda lifting;lifting scheme;noise reduction;peak signal-to-noise ratio;selectivity (electronic);stationary wavelet transform;structural similarity;visual artifact	Xiaotian Wang;Guangming Shi;Lili Liang	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495480	data compression;magnetoresistance;image texture;gibbs phenomenon;computer vision;mathematical optimization;estimation;decorrelation;peak signal-to-noise ratio;visual artifact;spinning;image compression;computer science;pattern recognition;motion estimation;noise reduction;mathematics;accuracy and precision;space technology;computational complexity theory;statistics;wavelet transform	Robotics	62.454841885308014	-66.76310390794309	186992
475ca3b6273a31713c7b42f8e9363c1d45cb6f14	wavelet based de-noising using logarithmic shrinkage function	wavelet transform;sparsity;compressed sensing;shrinkage functions;logarithmic shrinkage;image de-noising;impulsive noises;median and wiener filtering	Noise in signals and images can be removed through different de-noising techniques such as mean filtering, median filtering, total variation and filtered variation techniques etc. Wavelet based de-noising is one of the major techniques used for noise removal. In the first part of our work, wavelet transform based logarithmic shrinkage technique is used for de-noising of images, corrupted by noise (during under-sampling in the frequency domain). The logarithmic shrinkage technique is applied to under-sampled Shepp–Logan Phantom image. Experimental results show that the logarithmic shrinkage technique is 7–10% better in PSNR values than the existing classical techniques. In the second part of our work we de-noise the noisy, under-sampled phantom image, having salt and pepper, Gaussian, speckle and Poisson noises through the four thresholding techniques and compute their correlations with the original image. They give the correlation values close to the noisy image. By applying median or wiener filter in parallel with the thresholding techniques, we get 30–35% better results than only applying the thresholding techniques individually. So, in the second part we recover and de-noise the sparse under-sampled images by the combination of shrinkage functions and median filtering or wiener filtering.	wavelet	Hayat Ullah;Muhammad Amir;Ihsan Ul Haq;Shafqat Ullah Khan;M. K. A. Rahim;Khan Bahadar Khan	2018	Wireless Personal Communications	10.1007/s11277-017-4927-3	wiener filter;frequency domain;filter (signal processing);wavelet;wavelet transform;median filter;shrinkage;thresholding;pattern recognition;artificial intelligence;mathematics	Mobile	57.33851733574741	-67.59539938821513	187276
17ef6e937e7fff2e2334fcb46f9b73a68a3c354f	improved adaptive brovey as a new method for image fusion		An ideal fusion method preserves the Spectral information in fused image and adds spatial information to it with no spectral distortion. Among the existing fusion algorithms, the contourlet-based fusion method is the most frequently discussed one in recent publications, because the contourlet has the ability to capture and link the point of discontinuities to form a linear structure. The Brovey is a popular pan-sharpening method owing to its efficiency and high spatial resolution. This method can be explained by mathematical model of optical remote sensing sensors. This study presents a new fusion approach that integrates the advantages of both the Brovey and the cotourlet techniques to reduce the color distortion of fusion results. Visual and statistical analyzes show that the proposed algorithm clearly improves the merging quality in terms of: correlation coefficient, ERGAS, UIQI, and Q4; compared to fusion methods including IHS, PCA, Adaptive IHS, and Improved Adaptive PCA.	algorithm;coefficient;contourlet;distortion;image fusion;mathematical model;sensor	Hamid Reza Shahdoosti	2018	CoRR		pattern recognition;merge (version control);contourlet;correlation coefficient;spatial analysis;artificial intelligence;computer science;classification of discontinuities;distortion;image resolution;image fusion	Vision	60.66237840579206	-66.54748372455691	188245
875b3154ad2f613b5446cb560d9902baca2a35de	the statistical methods of pixel-based image fusion techniques	remote sensing image;high resolution;correlation modeling;standard deviation;root mean square error;image fusion;low resolution;statistical method;data fusion;matching;indexation;resolution enhancement;pattern recognition;satellite image;statistical techniques;multi resolution;signal to noise ratio;correlation coefficient;pixel based fusion;statistical fusion	There are many image fusion methods that can be used to produce high-resolution mutlispectral images from a high-resolution panchromatic (PAN) image and low-resolution multispectral (MS) of remote sensed images. This paper attempts to undertake the study of image fusion techniques with different Statistical techniques for image fusion as Local Mean Matching (LMM), Local Mean and Variance Matching (LMVM), Regression variable substitution (RVS), Local Correlation Modeling (LCM) and they are compared with one another so as to choose the best technique, that can be applied on multi-resolution satellite images. This paper also devotes to concentrate on the analytical techniques for evaluating the quality of image fusion (F) by using various methods including Standard Deviation ( ), Entropy( ), Correlation Coefficient ( ), Signal-to Noise Ratio ( ), Normalization Root Mean Square Error (NRMSE) and Deviation Index ( ) to estimate the quality and degree of information improvement of a fused image quantitatively.	coefficient;image fusion;image resolution;latent class model;mean squared error;multispectral image;pixel	Firouz Abdullah Al-Wassai;N. V. Kalyankar;Ali A. Al-Zaky	2011	CoRR		computer vision;image resolution;computer science;pattern recognition;mathematics;statistics	Vision	65.76005039926555	-66.20005487512674	188696
d5249ca32e4a39139bb47d4b7359a2d01e4d1757	a bayesian framework for image segmentation with spatially varying mixtures	bayesian framework;variational approximation;bayes estimation;multinomial distribution;processus gauss;bayesian methods image segmentation clustering algorithms maximum likelihood estimation gaussian processes monte carlo methods approximation methods degradation gaussian distribution rate distortion theory;mcmc algorithm;densite probabilite;problema valor limite;inexact variational approximation;degradation;gaussian mixture;mise a jour;image segmentation;image processing;probability density;modelo markov;gaussian processes;spatially varying finite mixture model;bayes methods;methode bayes;boundary value problem;methode approchee;spatially varying finite mixture model bayesian model dirichlet compound multinomial distribution gauss markov random field prior gaussian mixture image segmentation;metodo imagen;procesamiento imagen;dirichlet problem;bayesian methods;metodo aproximado;natural images;estimation a posteriori;approximate method;spatially varying mixtures;probabilistic approach;gauss markov random field;maximum likelihood estimation;probleme dirichlet;traitement image;probability vectors;a posteriori estimation;densidad probabilidad;rate distortion theory;actualizacion;etat actuel;methode de melange fini;estimacion bayes;finite mixture method;maximum a posteriori expectation maximization algorithm;gaussian mixture model;markov model;campo aleatorio;expectation maximization;estimacion a posteriori;loi multinomiale;dirichlet parameters;markov chain monte carlo;dirichlet compound multinomial probability density;dual carrier modulation;enfoque probabilista;approche probabiliste;gaussian mixture models;dirichlet compound multinomial distribution;problema dirichlet;state of the art;image method;segmentation image;signal classification;algoritmo mcmc;monte carlo methods bayes methods expectation maximisation algorithm gaussian processes image segmentation markov processes;algorithme em;classification signal;clustering algorithms;estado actual;teoria mezcla	A new Bayesian model is proposed for image segmentation based upon Gaussian mixture models (GMM) with spatial smoothness constraints. This model exploits the Dirichlet compound multinomial (DCM) probability density to model the mixing proportions (i.e., the probabilities of class labels) and a Gauss-Markov random field (MRF) on the Dirichlet parameters to impose smoothness. The main advantages of this model are two. First, it explicitly models the mixing proportions as probability vectors and simultaneously imposes spatial smoothness. Second, it results in closed form parameter updates using a maximum a posteriori (MAP) expectation-maximization (EM) algorithm. Previous efforts on this problem used models that did not model the mixing proportions explicitly as probability vectors or could not be solved exactly requiring either time consuming Markov Chain Monte Carlo (MCMC) or inexact variational approximation methods. Numerical experiments are presented that demonstrate the superiority of the proposed model for image segmentation compared to other GMM-based approaches. The model is also successfully compared to state of the art image segmentation methods in clustering both natural images and images degraded by noise.	approximation;blast e-value;bayesian network;cluster analysis;coefficient;coherence (physics);dicom;dirichlet kernel;expectation–maximization algorithm;experiment;feature vector;fractures, open;gauss;google map maker;graph - visual representation;image segmentation;leucaena pulverulenta;markov chain monte carlo;markov random field;mean shift;mixture model;monte carlo method;multinomial logistic regression;noise reduction;normal statistical distribution;numerical analysis;phenylephrine hydrochloride 10 mg oral tablet;pixel;population parameter;probability;segmentation action;variational principle;biologic segmentation;methylene chloride;statistical cluster	Christophoros Nikou;Aristidis Likas;Nikolas P. Galatsanos	2010	IEEE Transactions on Image Processing	10.1109/TIP.2010.2047903	econometrics;image processing;pattern recognition;mixture model;gaussian process;mathematics;statistics	ML	53.92217555014356	-73.16651876691238	188794
2bd4aba4c3114d91c911bc4455262dfb6307b2fc	dual constrained tv-based regularization	gaussian noise;graph theory;minimization;compressed sensing;image restoration proximal algorithm inverse problems image denoising convex optimization;image restoration dual constrained tv based regularization total variation minimization image denoising compressive sensing inverse problems gilboa osher projection algorithms chambolle projection algorithms projection variable constraint fast parallel proximal algorithm image deblurring flexible graph data representation;image deblurring;tv image restoration minimization noise reduction gaussian noise signal to noise ratio;image restoration;convex optimization;data representation;computer vision;image restoration data structures graph theory image denoising;inverse problem;data structures;noise reduction;total variation;proximal algorithm;image denoising;tv;signal to noise ratio;inverse problems	Algorithms based on the minimization of the Total Variation are prevalent in computer vision. They are used in a variety of applications such as image denoising, compressive sensing and inverse problems in general. In this work, we extend the TV dual framework that includes Chambolle's and Gilboa-Osher's projection algorithms for TV minimization in a flexible graph data representation by generalizing the constraint on the projection variable. We show how this new formulation of the TV problem may be solved by means of a fast parallel proximal algorithm, which performs better than the classical TV approach for denoising, and is also applicable to inverse problems such as image deblurring.	algorithm;compressed sensing;computer vision;data (computing);deblurring;noise reduction	Camille Couprie;Hugues Talbot;Jean-Christophe Pesquet;Laurent Najman;Leo J. Grady	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5946561	computer vision;mathematical optimization;convex optimization;computer science;inverse problem;graph theory;machine learning;mathematics	Vision	56.15437705690026	-72.7414990513479	189500
ca855ce6b5feb910ecba39a86227b250c84a05aa	wavelet-vaguelette restoration in photon-limited imaging	linear shift invariant;poisson noise;nuclear medicine imaging wavelet vaguelette restoration photon limited imaging linear shift invariant inverse problems intensity image recovery poisson noise astronomical imaging medical imaging wavelet vaguelette decomposition wavelet based filtering prefiltered wavelet transform;image restoration;wavelet transforms;wavelet transform;filtering theory medical image processing image restoration inverse problems noise wavelet transforms radioisotope imaging;inverse problem;medical image processing;inverse problems image restoration degradation biomedical imaging wavelet domain frequency estimation frequency domain analysis filtering wavelet transforms nuclear medicine;nuclear medicine;radioisotope imaging;frequency domain;filtering theory;noise;inverse problems	This paper studies linear shift-invariant inverse problem s arising in photon-limited imaging. The problem we consider is the re covery of an intensity image from a distorted version degrad ed with Poisson noise. This problem arises in medical and astro nomical imaging. It is shown that the wavelet-vaguelette decom p sition (WVD) can provide much better estimates of the underlyi ng intensity compared to classical frequency domain methods. The paper combines recently developed wavelet-based filtering techniques for photon imaging with new results in WVD methods for inverse problems. Furthermore, we show that the WVD can be interpreted as a prefiltered wavelet transform, and that it c an be very efficiently computed. The new method is applied to nucle ar medicine imaging.	astropy;circuit restoration;wavelet transform	Robert D. Nowak;Michael J. Thul	1998		10.1109/ICASSP.1998.678124	computer vision;mathematical optimization;inverse problem;mathematics;wavelet transform	Vision	54.30021086910862	-79.57620013657977	189589
6e9f2c7e2f822f479ca68c56966681512f6c39a5	map filtering in the diversity-enhanced wavelet domain applied to ecg signals denoising	filtering;ecg signals denoising;diversity enhanced wavelet domain;snr;maximum likelihood estimation;maximum a posteriori filtering;automatic signal segmentation procedures;statistical properties;wavelet transforms;automatic signal segmentation procedures map filtering diversity enhanced wavelet domain ecg signals denoising maximum a posteriori filtering statistical properties wavelet coefficients signal to noise ratio snr;electrocardiography;signal processing;noise reduction;map filtering;filtering wavelet domain electrocardiography signal denoising noise reduction wavelet coefficients signal processing signal to noise ratio wavelet transforms wiener filter;wiener filter;signal to noise ratio;wavelet domain;wavelet transforms electrocardiography filtering theory maximum likelihood estimation medical signal processing signal denoising;medical signal processing;wavelet coefficients;filtering theory;signal denoising	An effective denoising method for ECG signals affected by real sources of noise is proposed in this paper. The method is based on a maximum a-posteriori (MAP) filtering in the diversity-enhanced wavelet domain, under realistic a-priori assumptions regarding the statistical properties of the wavelet coefficients of the ECG signal. In order to evaluate the performance of the method, we studied the signal-to-noise ratio (SNR) improvement factor and the degree of the denoising influence on the automatic signal segmentation procedures. The method was tested in both synthetic and real noise conditions and it showed very promising results	coefficient;noise reduction;signal-to-noise ratio;synthetic intelligence;wavelet	Marius Oltean;Jean-Marc Boucher;Alexandru Isar	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1660563	computer vision;speech recognition;computer science;signal processing;pattern recognition;mathematics;signal-to-noise ratio;video denoising;statistics	Robotics	62.15518465193254	-70.79597406762139	190227
8970edaa6dec51bf86934b81738d9ef65d7f2098	comparison of the main forms of half-quadratic regularization	minimisation;computers;rate of convergence;experimental analysis;convergence;conference_paper;half quadratic;cost function;computer graphics;speed of convergence;convergence reconstruction images minimization regularized cost functions half quadratic regularization multiplicative regularization additive regularization;image reconstruction convergence cost function councils erbium mathematics acceleration degradation inverse problems noise reduction;image reconstruction;minimisation convergence image reconstruction	Rights ©2002 IEEE. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE.	server (computing)	Michael K. Ng;Mila Nikolova	2002		10.1109/ICIP.2002.1038032	iterative reconstruction;minimisation;econometrics;mathematical optimization;convergence;computer science;compact convergence;calculus;mathematics;rate of convergence;computer graphics;algorithm;statistics;experimental analysis of behavior	Vision	55.8626709586205	-72.83444574067936	190479
2b16fcb3965cb5c51efc78d0cf6970cfa22aa034	precomputed romp for light transport acquisition	light transport acquisition;matching pursuit algorithms;light transport;orthogonal matching pursuit;compressed sensing;image coding;concurrent computing;sensors;near to sparse representation;computer graphics;image matching;light scattering;precomputed romp method;photography;layout;state estimation;indexes;parallel processing image matching image representation lighting;regularized orthogonal matching pursuit;image representation;layout image coding computational efficiency matching pursuit algorithms parallel processing light scattering photography concurrent computing state estimation computer graphics;compressive sensing;pixel;pseudosingle pixel projection;optimization;lighting;parallel processing light transport acquisition compressive sensing sparse representation near to sparse representation regularized orthogonal matching pursuit precomputed romp method pseudosingle pixel projection;efficient estimation;computational efficiency;sparse representation;high dimension;parallel processing;cameras	In this article, we propose an efficient and accurate compressive sensing-based method for estimating the light transport characteristics of real world scenes. Although compressive sensing allows efficient estimation of a high-dimension signal with a sparse or near-to-sparse representation from a small number of samples, the computational cost of the compressive sensing in estimating the light transport characteristics is relatively high. Moreover, these methods require a relatively smaller number of images compared with other techniques although they still need 500–1000 images to estimate an accurate light transport matrix. Our proposed method — precomputed ROMP (Regularized Orthogonal Matching Pursuit) — improves the performance of the compressive sensing by providing an appropriate initial state, which allows us to more accurately estimate the matrix with fewer images. This improvement was achieved through two steps: 1) pseudo-single pixel projection by multi-line projection — measuring coarse light transport characteristics to utilize them as an initial state, 2) ROMP with initial signal — refining coarse light transport characteristics with the compressive sensing theory with the initial signal. Precomputed ROMP was carried out by parallel processing. With these two steps, we were able to estimate the light transport characteristics more accurately, much faster, and with a lesser number of images.	algorithmic efficiency;compressed sensing;general-purpose computing on graphics processing units;graphics processing unit;light transport theory;matching pursuit;mike lesser;parallel computing;pixel;precomputation;romp;sparse approximation;sparse matrix;the matrix	Satoshi Yamamoto;Yasumasa Itakura;Masashi Sawabe;Gimpei Okada;Norimichi Tsumura;Toshiya Nakaguchi	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops	10.1109/CVPRW.2010.5543460	parallel processing;computer vision;concurrent computing;computer science;theoretical computer science;compressed sensing;computer graphics (images)	Vision	60.1141901149454	-72.85235961230075	190505
102eae35aa682db5232d3f24ca00604c1e19e2a6	"""comments on """"nonstationary assumptions for gaussian models in images"""""""	histograms;general and miscellaneous mathematics computing and information science;image coding;image processing;numerical solution;maximum likelihood;high pass filter;probability density function;signal design;image restoration;digital filter;numerical solution 990200 mathematics computers;least square fit;laplace equations;maximum likelihood fit;digital filters;least square;transmitters;low pass filters;digital images;pulse modulation;noise;covariance matrix	Evidence is reviewed suggesting that a Laplacian probability density function constitutes a more valid model for high-pass filtered imagery than the Gaussian model postulated in earlier work. In addition, it is shown that this discrepancy does not either seriously weaken the applicability of this class of images to a major image restoration method or challenge any other basic conclusions of the previous work.		H. Joel Trussell;Richard P. Kruger	1978	IEEE Trans. Systems, Man, and Cybernetics	10.1109/TSMC.1978.4310026	econometrics;mathematical optimization;digital filter;image processing;computer science;theoretical computer science;machine learning;mathematics;statistics	Vision	53.926279578917615	-68.00336522447033	190633
0eb0445cf7bc8e52c8043f6c232216d735dd7101	image atlas construction via intrinsic averaging on the manifold of images	image features;general karcher means;biological patents;biomedical journals;measurement;manifolds;generic algorithm;text mining;convex programming;approximation algorithms;europe pubmed central;convex optimization problem;citation search;geometry;construction industry;convex optimization;citation networks;straightforward euclidean means;riemannian manifolds;rotation invariance;smooth manifold m modulo image rotations;image manifold;research articles;abstracts;image reconstruction;riemannian manifold;image quality;open access;nearest neighbor;realization step;life sciences;clinical guidelines;optimization;approximation methods;full text;computer vision manifolds nearest neighbor searches image reconstruction image quality biomedical imaging equations symmetric matrices geometry;image quality image atlas construction image manifold straightforward euclidean means riemannian manifolds general karcher means localization step realization step smooth manifold m modulo image rotations convex optimization problem;rest apis;cameras;localization step;orcids;image atlas construction;europe pmc;biomedical research;image reconstruction convex programming geometry;bioinformatics;literature search	In this paper, we propose a novel algorithm for computing an atlas from a collection of images. In the literature, atlases have almost always been computed as some types of means such as the straightforward Euclidean means or the more general Karcher means on Riemannian manifolds. In the context of images, the paper's main contribution is a geometric framework for computing image atlases through a two-step process: the localization of mean and the realization of it as an image. In the localization step, a few nearest neighbors of the mean among the input images are determined, and the realization step then proceeds to reconstruct the atlas image using these neighbors. Decoupling the localization step from the realization step provides the flexibility that allows us to formulate a general algorithm for computing image atlas. More specifically, we assume the input images belong to some smooth manifold M modulo image rotations. We use a graph structure to represent the manifold, and for the localization step, we formulate a convex optimization problem in Rk (k the number of input images) to determine the crucial neighbors that are used in the realization step to form the atlas image. The algorithm is both unbiased and rotation-invariant. We have evaluated the algorithm using synthetic and real images. In particular, experimental results demonstrate that the atlases computed using the proposed algorithm preserve important image features and generally enjoy better image quality in comparison with atlases computed using existing methods.	algorithm;approximation;atlases;biologic preservation;business continuance volume;cervical atlas;computation (action);computer vision;convex optimization;coupling (computer programming);graph - visual representation;ibm notes;image quality;imaging, three-dimensional, computer assisted;internet information services;mathematical optimization;medical imaging;modulo operation;optimization problem;synthetic intelligence;united states national institutes of health;buciclovir;manifold	Yuchen Xie;Jeffrey Ho;Baba C. Vemuri	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2010.5540035	iterative reconstruction;image quality;computer vision;combinatorics;convex optimization;genetic algorithm;topology;manifold;computer science;machine learning;mathematics;geometry;k-nearest neighbors algorithm;feature;measurement	Vision	54.83957880821277	-73.95927159070622	190857
253198283eaf97ac9ccb05d1ec7f9407c85f9057	modified bm3d algorithm for image denoising using nonlocal centralization prior	shrinkage;sparsity;nonlocal;block matching;denoising;wavelet	Block matching and 3D collaborative filtering (BM3D) has shown great power for image denoising. For grouped image blocks, this letter proposes to remove the 1D transform inter-blocks and introduce the nonlocal centralization prior to better utilize both local sparsity of wavelet coefficients and nonlocal similarity of grouped blocks. Three nonlocal shrinkage functions are developed under different norm restrictions of wavelet coefficients intra- and inter-blocks. Such shrinkage functions are verified to be competitive or better than the BM3D algorithm and other state-of-the-art denoising methods. We remove the 1D transform across grouped blocks and introduce the nonlocal centralization prior.Both local sparsity of wavelet coefficients and nonlocal similarity can be better utilized.Nonlocal shrinkage functions are developed under different norm restrictions.Such shrinkage functions are verified to be competitive or better than the BM3D algorithm.	algorithm;noise reduction;nonlocal lagrangian	Hua Zhong;Ke Ma;Yang Zhou	2015	Signal Processing	10.1016/j.sigpro.2014.08.014	wavelet;econometrics;mathematical optimization;computer science;noise reduction;mathematics;sparsity-of-effects principle;shrinkage;statistics	ML	57.10460210974868	-70.56775398131009	190870
40bc3f955d4c5e34a0d0ca26ece039d96e81ab87	adaptive unsharp masking for contrast enhancement	nonlinear filters;image preprocessing;contrast enhanced;adaptive operator;medium detail areas;electronic mail;image contrast enhancement;filter coefficients adaptive algorithm adaptive unsharp masking image contrast enhancement image dynamics noise amplification reduction smooth areas adaptive directional filtering directional characteristics high detail areas medium detail areas sharp transitions experimental results adaptive operator image preprocessing zooming linear highpass filter adaptive recursive two dimensional filter gauss newton algorithm;recursive filters;high pass filters;polynomials;linear highpass filter;two dimensional digital filters;image dynamics;nonlinear distortion;adaptive algorithm;laplace equations;image enhancement;adaptive filters;zooming;two dimensional digital filters image enhancement adaptive filters adaptive signal processing high pass filters noise recursive filters;adaptive signal processing;adaptive unsharp masking;smooth areas;adaptive directional filtering;noise reduction;statistics;high detail areas;sharp transitions;cities and towns;noise amplification reduction;gauss newton algorithm;filter coefficients;laplace equations electronic mail adaptive algorithm noise reduction nonlinear filters adaptive filters cities and towns nonlinear distortion polynomials statistics;experimental results;adaptive recursive two dimensional filter;directional characteristics;noise	A new scheme of unsharp masking for image contrast enhancement is presented in this paper. An adaptive algorithm is introduced so that a sharpening action is performed only in locations where the image exhibits signiicant dynamics. Hence, the ampliication of noise in smooth areas is reduced. An adaptive directional l-tering is also performed so as to provide suitable emphasis to the diierent directional characteristics of the detail. Because it is capable of treating high-detail and medium-detail areas diierently, this algorithm also avoids unpleasant overshoot artifacts in regions of sharp transitions. Experimental results demonstrating the usefulness of the adaptive operator in an application involving preprocessing of images for enhancement prior to zooming are also included in the paper.	adaptive algorithm;overshoot (signal);preprocessor;unsharp masking	Andrea Polesel;Giovanni Ramponi;V. John Mathews	1997		10.1109/ICIP.1997.647756	adaptive filter;computer vision;computer science;control theory;mathematics	Vision	55.623852838449835	-66.45934958664178	190906
96e22e86768c9569e1185927670de7a63fa5f9e0	compression of time series by extracting major extrema	major minima and maxima;lossy compression;time series	We formalize the notion of important extrema of a time series, that is, its major minima and maxima; analyze basic mathematical properties of important extrema; and apply these results to the problem of time-series compression. First, we define numeric importance levels of extrema in a series, and present algorithms for identifying major extrema and computing their importances. Then, we give a procedure for fast lossy compression of a time series at a given rate, by extracting its most important minima and maxima, and discarding the other points.	algorithm;lossy compression;maxima and minima;time series	Eugene Fink;Harith Suman Gandhi	2011	J. Exp. Theor. Artif. Intell.	10.1080/0952813X.2010.505800	lossy compression;mathematical optimization;computer science;time series	ML	64.6894039138823	-73.1156462308304	191402
076a131f4eaa20d4882b4bd38afac3f9e9b21fb0	the use of entropy minimization for the solution of blind source separation problems in image analysis	entropy minimization;image processing;blind source separation;stochastic optimization;inverse problem;image reconstruction;image quality;pattern recognition;a priori information;image analysis;information entropy;correlation coefficient;inverse problems	Entropy minimization is closely associated with pattern recognition. The present contribution uses a direct minimization of an entropy like function to solve the blind source separation problem for image reconstruction. The mixture patterns are decomposed using SVD and then global stochastic optimization is used to find the first irreducible image pattern. Further images are then subsequently reconstructed, by imposing a 2D correlation coefficient for dissimilarity to prevent repeated images, until all images are exhaustively enumerated. Three test cases are used, including (1) a set of three black and white texturally different photographs (2) a set of three RGB geometrically similar photographs and (3) an underdetermined problem involving an imbedded watermark. Cases 1 and 2 are easily solved with outstanding image quality. Both searches are conducted in an unsupervised manner—no a priori information is used. In Case 3, the watermark is enhanced after targeting the region for entropy minimization. The present results have a wide variety of applications, including image and spectroscopic analysis. 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.	blind signal separation;coefficient;image analysis;image editing;image quality;irreducibility;iterative reconstruction;mathematical optimization;pattern recognition;semantics (computer science);singular value decomposition;source separation;stochastic optimization;test case;unsupervised learning	Liangfeng Guo;Marc Garland	2006	Pattern Recognition	10.1016/j.patcog.2005.09.006	computer vision;mathematical optimization;image analysis;image processing;computer science;inverse problem;stochastic optimization;machine learning;pattern recognition;mathematics	Vision	63.71706545925614	-72.9812989873366	191409
921bcb3f22e888fc79dddbc12a39afec2f7f79af	parameter estimation in super-resolution image reconstruction problems	image resolution;parameter estimation image resolution image reconstruction image sensors sensor arrays high resolution imaging optical imaging degradation pixel optical devices;low resolution;iterative methods image reconstruction image resolution maximum likelihood estimation;maximum likelihood estimation;iterative methods;maximum likelihood estimate;image reconstruction;super resolution;maximum likelihood estimation parameter estimation super resolution image reconstruction hyperparameters high resolution image image resolution iterative calculation;high resolution imager;parameter estimation	In this paper we consider the estimation of the unknown hyperparameters for the problem of reconstructing a high-resolution image from multiple undersampled, shifted, degraded frames with subpixel displacement errors. We derive mathematical expressions for the iterative calculation of the maximum likelihood estimate (mle) of the unknown hyperparameters given the low resolution observed images. Experimental results are presented for evaluating the accuracy of the proposed method.	displacement mapping;estimation theory;image resolution;iterative method;iterative reconstruction;pixel;super-resolution imaging	Javier Abad;Miguel Vega;Rafael Molina;Aggelos K. Katsaggelos	2003		10.1109/ICASSP.2003.1199573	computer vision;image resolution;pattern recognition;mathematics;maximum likelihood;sub-pixel resolution;statistics	Vision	59.035107690337	-71.92569698462133	191416
0d014e4432ad8f1a8ff397c38e361cb639ebd84d	multiscale variational decomposition and its application for image hierarchical restoration	image restoration;gradient descent;primal dual algorithm;variation;image decomposition;multiscale	Variational decomposition has been widely used in image denoising, however, it can’t distinguish texture from noise well. Replacing the fixed parameter in the (BV, G) decomposition with a monotone increasing sequence, and iteratively taking the residual of the previous step as the input to decompose, we propose a multiscale variational decomposition model in this paper. Unlike the fixed-scale decomposition, the new model can decompose the input image into a sum of a series of features with different scales. So, texture can be distinguished from noise. In addition, we prove the nontrivial property and the convergence of this multiscale decomposition, and introduce a hybrid iteration algorithm that combines the first-order primal–dual algorithm with the gradient decent method to numerically solve the multiscale decomposition model. Numerical results validate the effectiveness of the proposed model. Furthermore, we apply this multiscale decomposition for image hierarchical restoration. Compared with the classical hierarchical (BV, L2) decomposition, hierarchical wavelet decomposition and fixed-scale (BV, G) decomposition, our model has better performance for both synthetic and real images in terms of PSNR and MSSIM.	circuit restoration;variational principle	Liming Tang;Chuanjiang He	2016	Computers & Electrical Engineering	10.1016/j.compeleceng.2015.08.012	gradient descent;image restoration;computer vision;mathematical optimization;combinatorics;discrete mathematics;computer science;mathematics;theme and variations;set partitioning in hierarchical trees	Vision	55.67313389319964	-70.46301232384143	191608
fe9ac9bcb623e1d781618fdfc08a59e5416acb5f	a total variation regularization based super-resolution reconstruction algorithm for digital video	artefacto;baja resolucion;equation non lineaire;signal image and speech processing;television;preconditionnement;iterative method;traitement signal;ecuacion no lineal;degradation;euler lagrange equation;ecuacion euler lagrange;high resolution;estimation mouvement;algorithm performance;iterative decoding;image processing;equation euler lagrange;degradacion;estimacion movimiento;virgule fixe;laplacian;low resolution;basse resolution;procesamiento imagen;motion estimation;preconditioning;total variation regularization;traitement image;coma fija;artefact;fixed point;metodo iterativo;algorithme;algorithm;laplacien;haute resolution;laplaciano;quantum information technology spintronics;senal video;signal video;imagen borrosa;mathematical models;resultado algoritmo;erreur estimation;methode iterative;blurred image;senal numerica;signal processing;image sequence;euler equations;alta resolucion;performance algorithme;signal numerique;error estimacion;video signal;super resolution;precondicionamiento;nonlinear equations;secuencia imagen;digital video;estimation error;image floue;digital signal;superresolution;reconstruction algorithm;non linear equation;procesamiento senal;article;superresolucion;sequence image;algoritmo	Super-resolution (SR) reconstruction technique is capable of producing a high-resolution image from a sequence of low-resolution images. In this paper, we study an efficient SR algorithm for digital video. To effectively deal with the intractable problems in SR video reconstruction, such as inevitable motion estimation errors, noise, blurring, missing regions, and compression artifacts, the total variation (TV) regularization is employed in the reconstruction model. We use the fixed-point iteration method and preconditioning techniques to efficiently solve the associated nonlinear Euler-Lagrange equations of the corresponding variational problem in SR. The proposed algorithm has been tested in several cases of motion and degradation. It is also compared with the Laplacian regularization-based SR algorithm and other TV-based SR algorithms. Experimental results are presented to illustrate the effectiveness of the proposed algorithm.	algorithm;calculus of variations;compression artifact;digital video;elegant degradation;euler;euler–lagrange equation;fixed-point iteration;image resolution;laplacian matrix;list of variational topics;motion estimation;nonlinear system;preconditioner;robustness (computer science);super-resolution imaging;total variation denoising	Michael K. Ng;Huanfeng Shen;Edmund Y. Lam;Liangpei Zhang	2007	EURASIP J. Adv. Sig. Proc.	10.1155/2007/74585	computer vision;mathematical optimization;image resolution;image processing;computer science;calculus;signal processing;mathematics;geometry;superresolution	Vision	56.03939083035021	-71.65016441599967	191629
1964a81301c6709a13bb3dc5913ea34ff109ad34	unsupervised edge-preserving image restoration via a saddle point approximation	gaussian noise;partition function;punto silla;regularisation;gibbs parameter estimation;restauration image;modelo markov;gibbs sampler;maximum likelihood;edge preserving regularization;point col;reconstruction;maximum vraisemblance;ruido gaussiano;prior distribution;image restoration;regularization;restauracion imagen;markov model;campo aleatorio;posterior distribution;computational complexity;bruit gaussien;estimacion parametro;ley a posteriori;unsupervised image restoration;preservation;regularizacion;parameter estimation;estimation parametre;modele markov;image modeling;loi a posteriori;preservacion;maxima verosimilitud;champ aleatoire;reconstruccion;random field;saddle point	This paper proposes a fast method to estimate the Gibbs hyperparameters of an MRF image model with explicit lines during the restoration process. It consists of a mixed-annealing algorithm for the maximization of the posterior distribution with respect to the image field, periodically interrupted to compute, via ML estimation, a new set of parameters. We first consider the weak membrane model and show that, by adopting a saddle point approximation for the partition function, these new parameters are defined as those that maximize the conditional prior distribution of the lines given the intensities, evaluated on the current estimate of the whole image field. In this way the computation of the expectations involved in the ML estimation can be performed by analytical summation over the binary line elements alone, with a strong reduction of the computational complexity. The approach can be extended to the general case of self-interacting line models, by substituting the analytical computations with a binary, short-range Gibbs sampler. q 1999 Elsevier Science B.V. All rights reserved.	approximation;circuit restoration;computation;computational complexity theory;expectation–maximization algorithm;experiment;gibbs sampling;image restoration;interaction;interrupt;iteration;iterative method;markov random field;partition function (mathematics);sampling (signal processing);simulated annealing;unsupervised learning	Luigi Bedini;Anna Tonazzini;Salvatore Minutoli	1999	Image Vision Comput.	10.1016/S0262-8856(98)00154-1	gaussian noise;image restoration;regularization;econometrics;mathematical optimization;random field;prior probability;gibbs sampling;computer science;mathematics;maximum likelihood;saddle point;markov model;posterior probability;partition function;estimation theory;computational complexity theory;preservation;statistics	Vision	54.236912622984924	-73.14040441538363	192168
4e254b6b23b4fed1406f4a98d4bbb7846267c05a	smoothness-constrained face photo-sketch synthesis using sparse representation	minimisation;approximated optimal solution smoothness constrained face photo sketch synthesis sparse representation face sketch photo synthesis law enforcement drawing techniques artist depictions ill posed nature mosaic effects energy function minimization large scale convex optimization problem l 1 norm constraint iterative optimization approach;image segmentation;convex programming;iterative methods;face recognition;image representation;minimisation convex programming face recognition image representation image segmentation iterative methods;face dictionaries optimization vectors encoding training face recognition	Face photo-sketch and sketch-photo synthesis have important usages in law enforcement. It is challenging to synthesize face sketches from photos because the drawing techniques and styles of artists' depictions are hard to be learned. To synthesize face photos from sketches is also hard due to its ill-posed nature. In order to avoid mosaic effects in the existed photo-sketch methods, we propose a smoothness-constrained photo-sketch synthesis method via sparse representation. The work is an extension of the previous work[1]. The method is modeled as the minimization of an energy function, a large scale convex optimization problem with l1-norm constraint. Since previous optimization methods are infeasible to solve our problem, we propose an iterative optimization approach, which decomposes the large scale optimization into a sequence of small scale optimizations and solve them iteratively to obtain the approximated optimal solution. The same synthesis strategy can be also used to synthesize photos from sketches. Experiments show its effectiveness.	approximation algorithm;coefficient;convex optimization;dictionary;experiment;iterative method;mathematical optimization;optimization problem;sparse approximation;sparse matrix;taxicab geometry;well-posed problem	Liang Chang;Xiaoming Deng;Mingquan Zhou;Fuqing Duan;Zhongke Wu	2012	Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)		facial recognition system;computer vision;minimisation;mathematical optimization;convex optimization;computer science;machine learning;mathematics;iterative method;image segmentation;statistics	Vision	58.87076410060199	-73.44527848965296	192298
63276373c997d9dadcdfcf5c5a2e647d01bc0620	learning proximal operators: using denoising networks for regularizing inverse imaging problems		While variational methods have been among the most powerful tools for solving linear inverse problems in imaging, deep (convolutional) neural networks have recently taken the lead in many challenging benchmarks. A remaining drawback of deep learning approaches is their requirement for an expensive retraining whenever the specific problem, the noise level, noise type, or desired measure of fidelity changes. On the contrary, variational methods have a plug-and-play nature as they usually consist of separate data fidelity and regularization terms. In this paper we study the possibility of replacing the proximal operator of the regularization used in many convex energy minimization algorithms by a denoising neural network. The latter therefore serves as an implicit natural image prior, while the data term can still be chosen independently. Using a fixed denoising neural network in exemplary problems of image deconvolution with different blur kernels and image demosaicking, we obtain state-of-the-art reconstruction results. These indicate the high generalizability of our approach and a reduction of the need for problemspecific training. Additionally, we discuss novel results on the analysis of possible optimization algorithms to incorporate the network into, as well as the choices of algorithm parameters and their relation to the noise level the neural network is trained on.	algorithm;artificial neural network;calculus of variations;convolutional neural network;deblurring;deconvolution;deep learning;demosaicing;energy minimization;experiment;gaussian blur;mathematical optimization;matrix regularization;noise (electronics);noise reduction;numerical analysis;plug and play;proximal operator;stacking	Tim Meinhardt;Michael Möller;Caner Hazirbas;Daniel Cremers	2017	2017 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2017.198	machine learning;operator (computer programming);inverse problem;algorithm design;deconvolution;computer science;demosaicing;deep learning;artificial neural network;regularization (mathematics);artificial intelligence	Vision	56.39831383946385	-72.79155002669057	193278
1287b4168e0175e4761d8d9c549235a3fc9a20cf	analysis of image compression by minimum relative entropy (mre) and restoration through weighted region growing techniques for medical images	minimum relative entropy;medical image;image compression;region growing	— In recent years, there is a spurt of publications on Information Theory based Image Compression Techniques. Specific regional characteristics of the images are associated with Information clusters. In this paper a novel approach of information theory based Minimum Relative Entropy (MRE) and Entropy methods for image compression are discussed. A two stage compression process is performed through homogenous MRE method, and heterogeneous MRE. The compressed images are reconstructed through Region growing techniques. The performance of image compression and restoration is analyzed by the estimation of parametric values such as Mean Square Error (MSE) and Peak Signal to Noise Ratio (PSNR). Higher the PSNR better the reconstruction process. Six radiographic medical images of various sizes are analyzed and Maximum PSNR of 33 is achieved.	algorithm;circuit restoration;data compression ratio;entropy (information theory);image compression;information theory;iterative reconstruction;kullback–leibler divergence;mean squared error;newton's method;peak signal-to-noise ratio;radiography;region growing	R. Sukanesh;R. Harikumar;N. S. Balaji;S. R. Balasubramaniam	2007	Engineering Letters		computer vision;data compression ratio;mathematical optimization;image compression;computer science;machine learning;pattern recognition;mathematics;region growing	Vision	57.844411971644185	-68.37838279258054	193369
b95b1fac5caf924aaf9d0d66f4995dd7e9786d78	image restoration with surface-based fourth-order partial differential equation	equation derivee partielle;partial differential equation;4230;restauration image;sensors;edge detection;0130c;image restoration;energy function;reduccion ruido;detection contour;local structure;partial differential equations;noise reduction;reduction bruit;denoising;diffusion	ABSTRACT This paper presents an edge-preserving fourth order partial dierential equation (PDE) for image restorationderived from a new surface-based energy functional. The corresponding fourth order PDE can preserve edges andavoid the staircase eect. The proposed model contains a function of gradient norm as an edge detector, whichcontrols the diusion speed according to the local structure of the image and preserves more details. Denoisingresults are given and we have also compared our method with some related PDE models.Keywords: image restoration, fourth order partial dierential equation, edge-preserving, staircase eect, geom-etry framework 1. INTRODUCTION A digital image degenerated by Gaussian noise can be described by the formulau 0 (x,y )= u(x,y )+ (x,y ). (1)The goal of image denoising is to restore the true image u from its observation version u 0 . Assume the noiselevel is approximately known, i.e., (u  u 0 ) 2 dxdy = 2 . (2)Wavelet-based methods, statistical methods, and diusion lter methods have successfully been used toremove noise from digital images(see	circuit restoration;image restoration	Bibo Lu;Qiang Liu	2010		10.1117/12.863521	computer vision;computer science;noise reduction;partial differential equation	Vision	55.20084648933435	-69.63210282255022	193426
8d83981d5104fd3927d92695c5da2545300f0e4c	compressive blind image deconvolution	constrained optimization inverse methods compressive sensing blind image deconvolution;concave programming;real passive millimeter wave images compressive blind image deconvolution bid regularization framework compressive sensing based imaging systems cs based imaging systems blurred images constrained optimization technique cs reconstruction algorithm compressive bid problem nonconvex lp quasinorm simultaneous autoregressive regularization term image blur regularization term blurred synthetic images;compressed sensing;image reconstruction;deconvolution;millimetre wave imaging compressed sensing concave programming deconvolution image reconstruction;millimetre wave imaging	We propose a novel blind image deconvolution (BID) regularization framework for compressive sensing (CS) based imaging systems capturing blurred images. The proposed framework relies on a constrained optimization technique, which is solved by a sequence of unconstrained sub-problems, and allows the incorporation of existing CS reconstruction algorithms in compressive BID problems. As an example, a non-convex lp quasi-norm with 0 <; p <; 1 is employed as a regularization term for the image, while a simultaneous auto-regressive regularization term is selected for the blur. Nevertheless, the proposed approach is very general and it can be easily adapted to other state-of-the-art BID schemes that utilize different, application specific, image/blur regularization terms. Experimental results, obtained with simulations using blurred synthetic images and real passive millimeter-wave images, show the feasibility of the proposed method and its advantages over existing approaches.	algorithm;bid protein;binary integer decimal;body dysmorphic disorders;calcium-sensing receptor;compressed sensing;constrained optimization;constraint (mathematics);deconvolution;gaussian blur;mathematical optimization;optimization problem;penalty method;simulation;synthetic data;synthetic intelligence;visually impaired persons;millimeter	Bruno Amizic;Leonidas Spinoulas;Rafael Molina;Aggelos K. Katsaggelos	2013	IEEE Transactions on Image Processing	10.1109/TIP.2013.2266100	iterative reconstruction;image restoration;computer vision;mathematical optimization;computer science;deconvolution;mathematics;blind deconvolution;compressed sensing	Vision	56.6594530726568	-73.83423620547242	193824
254c0c174bfc7aa5a5825c1bce455c264515b1c3	blurred image classification based on adaptive dictionary		Two frameworks for blurred image classification based on adaptive dictionary are proposed. Given a blurred image, instead of image deblurring, the semantic category of the image is determined by blur insensitive sparse coefficients calculated depending on an adaptive dictionary. The dictionary is adaptive to an assumed space invariant Point Spread Function (PSF) estimated from the input blurred image. In one of the proposed two frameworks, the PSF is inferred separately and in the other, the PSF is updated combined with sparse coefficients calculation in an alternative and iterative manner. The experimental results have evaluated three types of blur namely defocus blur, simple motion blur and camera shake blur. The experiment results confirm the effectiveness of the proposed frameworks.	box blur;coefficient;computer vision;deblurring;dictionary;gaussian blur;gradient;iterative method;machine learning;sparse matrix	Guangling Sun;Guoqing Li;Jie Yin	2013	CoRR	10.5121/ijma.2013.5101	image restoration;computer vision;speech recognition;pattern recognition	Vision	57.39625798786214	-69.74884165634737	193987
690eb5d473a9f1661380dc61f8f63320c9a5e7a1	optimal parameters based stochastic dot model for tone compensation of dither matrix	tone compensation;optimal parameters;printer model;munsell value	The image outputs of a laser printer are accompanied with nonlinear phenomena such as dot gains and losses. As a result, a laser printer model is necessary to suppress such nonlinear distortions. This paper proposes the optimal parameters based stochastic dot model (OPSDM) and applies it to the tone compensation of dither matrices. In the proposed model, Munsell value is taken as intermediate value and a measured, and recurrent parameters of model are modified to obtain the optimal parameters of the stochastic dot model of printer by calculating the minimum Munsell value error between the measure result and simulated result of model. Finally, the dither matrix thresholds are modified to make the simulated result of every gray level with dither matrix close to the given level. The results of experimental indicate that the modified dither matrix with proposed model can significantly restrain the influence of the nonlinear characteristics of printer. & 2015 Elsevier B.V. All rights reserved.	algorithm;apply;distortion;dither;grayscale;laser printing;machine learning;motion compensation;nonlinear system;printer (computing);stripes	Hai Su;Juhua Liu;Yaohua Yi;Bo Du	2016	Neurocomputing	10.1016/j.neucom.2015.08.102	telecommunications;control theory	AI	57.06435076575112	-66.26259477740372	194279
d5720b4330c8ef0d97885e06a4e81c59218cfe0c	robust deformation estimation in wood-composite materials using variational optical flow		Wood-composite materials are widely used today as they homogenize humidity related directional deformations. Quantification of these deformations as coefficients is important for construction and engineering and topic of current research [7, 8], but still a manual process. This work introduces a novel computer vision approach that automatically extracts these properties directly from scans of the wooden specimens, taken at different humidity levels during the long lasting humidity conditioning process. These scans are used to compute a humidity dependent deformation field for each pixel, from which the desired coefficients can easily be calculated. The overall method includes automated registration of the wooden blocks, numerical optimization to compute a variational optical flow field which is further used to calculate dense strain fields and finally the engineering coefficients and their variance throughout the wooden blocks. The methods regularization is fully parameterizable which allows to model and suppress artifacts due to surface appearance changes of the specimens from mold, cracks, etc. that typically arise in the conditioning process.		Markus Hofinger;Thomas Pock;Thomas Moosbrugger	2018	CoRR		artificial intelligence;humidity;deformation (mechanics);pattern recognition;acoustics;computer science;pixel;mold;conditioning process;regularization (mathematics);optical flow	Vision	56.48850095030075	-77.01583862351673	194602
120c2089d9b3392471d161e9542359e5aee7a499	an imaging device that uses the wavelet transformation as the image reconstruction algorithm	wavelet transform;image reconstruction	Abstract#R##N##R##N#Many imaging devices have been constructed that use fourier transform techniques for image reconstruction as well as for image analysis. The basis functions in the fourier transform space are sinusoids. These are not localized. Therefore it should not be expected that highly localized behavior of a signal be characterized well using functions distributed evenly throughout the interval of the integral transform. Also, the phase cancellations of a conventional impulse signal are not handled well by fourier techniques. On the other hand, the basis functions of the wavelet transformation are highly localized, i.e., these exhibit compact support, yet are orthonormal. The multispectral decomposition algorithm of the wavelet transformation is used to analyze the signal returning from the reflections of a single ultrasonic transducer with a focused beam and operated in the focal zone. The choice of frequency depends upon the mutually antagonistic factors of penetration (αf–2) and resolution (αf). The signal sent into the sample should not be the impulse response signal used in conventional devices; rather, it should be a time-reversed replica of a single or a linear combination of the most highly localized basis function wavelets. The returning signal is a sequence of translates of the wavelets plus perhaps some lower-resolution wavelets. The translations are proportational to the time of flight of the signal. The wavelet transformation is superb at discriminating the population of each of these translates which is identically the A-scan signal. The transmitted signal, which is a time-reversed single wavelet or a linear combination of wavelets, is not easy to produce. Inexpensive ultrasound transducers have resonances which make it difficult to produce any desired wave form. Wavelet shape is far from arbitrary. Precise wave shaping is performed by measuring the impulse response function of the transducer; then, the desired wave shape is convolved with the inverse of the measured impulse response function of the transducer. This produces the signal to be presented to the pulse generation circuit. Care is taken to damp the impulse response so that there are no zeros. The received signal is sampled at an even rate which is carefully chosen to match the time delay of one wavelet translate to the next. B-scan, C-scan, and volume imaging are easily accomplished using a sequence of A-scan data, all by conventional techniques. A single A-scan requires less than 2 ms to perform and reconstruct with a high-speed arithmetic unit which was designed in conjunction with this work and is now commercially available.	algorithm;iterative reconstruction;wavelet transform	John H. Letcher	1992	Int. J. Imaging Systems and Technology	10.1002/ima.1850040204	iterative reconstruction;wavelet;computer vision;mathematical optimization;harmonic wavelet transform;second-generation wavelet transform;continuous wavelet transform;impulse response;computer science;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;fast wavelet transform;lifting scheme;gabor wavelet;wavelet transform	Graphics	54.93028859729409	-79.96780339639498	194857
67622f06205c70609e7e793bd13786890828879f	parametric dictionary-based velocimetry for echo piv		We introduce a novel motion estimation approach for Echo PIV for the laminar and steady flow model. We mathematically formalize the motion estimation problem as a parametrization of a dictionary of particle trajectories by the physical flow parameter. We iteratively refine this unknown parameter by subsequent sparse approximations. We show smoothness of the adaptive flow dictionary that is a key for a provably convergent numerical scheme. We validate our approach on real data and show accurate velocity estimation when compared to the state-of-the-art cross-correlation method.	approximation;black box;cross-correlation;dictionary;iteration;motion estimation;numerical analysis;reconstruction conjecture;sparse matrix;velocity (software development);z/vm	Ecaterina Bodnariuc;Stefania Petra;Christian Poelma;Christoph Schnörr	2016		10.1007/978-3-319-45886-1_27	velocimetry	Vision	56.110334687469994	-74.43741835721973	194937
a95c7f52541392f601ee1d67118b503e55b44ed9	finite mixture models and stochastic expectation-maximization for sar amplitude probability density function estimation based on a dictionary of parametric families	probability;stochastic expectation maximization;probability density function;stochastic processes probability density function amplitude estimation dictionaries data analysis parametric statistics synthetic aperture radar statistical analysis parameter estimation optimization methods;estimation algorithm;method of log cumulants finite mixture model stochastic expectation maximization parametric probability density function estimation remote sensing data analysis pixel intensity synthetic aperture radar sar amplitude data analysis theoretical heuristic model land cover typology innovative estimation algorithm optimal model iterative methodology;image texture;iterative methods;remote sensing by radar;data analysis;remote sensing data;sar image;radar cross sections;iterative methods remote sensing by radar synthetic aperture radar probability data analysis radar cross sections image texture;parameter estimation;cumulant;finite mixture model;land cover;optimization model;synthetic aperture radar	"""In remotely sensed data analysis, a crucial problem is represented by the need to develop accurate models for the statistics of the pixel intensities. This paper deals with the problem of parametric probability density function (PDF) estimation in the context of Synthetic Aperture Radar (SAR) amplitude data analysis. Several theoretical and heuristic models for the PDFs of SAR data have been proposed in the literature, that have been proved to be effective for different land-cover typologies, thus making the choice of a single optimal SAR parametric PDF a hard task. In this paper, an innovative estimation algorithm is described, that faces such a problem by adopting a finite mixture model (FMM) for the amplitude PDF, with mixture components belonging to a given dictionary of SAR-specific PDFs. The method automatically integrates the procedures of selection of the optimal model for each component, of parameter estimation, and of optimization of the number of components by combining the Stochastic Expectation Maximization (SEM) iterative methodology with the recently developed """"method-of-log-cumulants"""" (MoLC) for parametric PDF estimation. Experimental results on several real SAR images are reported, showing that the proposed method accurately models the statistics of SAR amplitude data."""	dictionary;estimation theory;expectation–maximization algorithm;fast multipole method;heuristic;iterative method;mathematical optimization;mixture model;pixel;portable document format;stochastic gradient descent	Josiane Zerubia;Sebastiano B. Serpico	2004	IGARSS 2004. 2004 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2004.1368708	image texture;econometrics;probability density function;synthetic aperture radar;pattern recognition;probability;mixture model;iterative method;estimation theory;data analysis;statistics;cumulant	Vision	61.22922021371879	-72.458687970315	195074
f1357b6e8c38a3ceaa7f145a16d37673aae1c2a1	shearlet-based total variation diffusion for denoising	artefacto;television;iterative method;traitement signal;metodo adaptativo;shearlet representation;evaluation performance;performance evaluation;curvelets;evaluacion prestacion;pollution measurement;optimal estimation;simulacion numerica;distributed discontinuities;gibbs type artifacts;methode adaptative;projected adaptive total variation scheme;shearlet based diffusion method;noise measurement;artefact;regularization;reduccion ruido;metodo iterativo;total variation curvelets denoising diffusion regularization shearlets;error analysis;noise level;methode iterative;image representation;signal processing;noise reduction;shearlet transform;simulation numerique;adaptive method;shearlet transform shearlet based diffusion method image denoising distributed discontinuities gibbs type artifacts shearlet representation residual coefficients projected adaptive total variation scheme;transforms;performance analysis;reduction bruit;shearlets;noise reduction tv wavelet coefficients performance analysis noise measurement pollution measurement signal processing image denoising error analysis noise level;total variation;transforms image denoising image representation;image denoising;denoising;tv;algorithms artifacts image enhancement image interpretation computer assisted reproducibility of results sensitivity and specificity;procesamiento senal;diffusion;wavelet coefficients;residual coefficients;numerical simulation	We propose a shearlet formulation of the total variation (TV) method for denoising images. Shearlets have been mathematically proven to represent distributed discontinuities such as edges better than traditional wavelets and are a suitable tool for edge characterization. Common approaches in combining wavelet-like representations such as curvelets with TV or diffusion methods aim at reducing Gibbs-type artifacts after obtaining a nearly optimal estimate. We show that it is possible to obtain much better estimates from a shearlet representation by constraining the residual coefficients using a projected adaptive total variation scheme in the shearlet domain. We also analyze the performance of a shearlet-based diffusion method. Numerical examples demonstrate that these schemes are highly effective at denoising complex images and outperform a related method based on the use of the curvelet transform. Furthermore, the shearlet-TV scheme requires far fewer iterations than similar competitors.	algorithm;coefficient;curvelet;estimated;euler;euler–lagrange equation;inpainting;iteration;morphologic artifacts;noise reduction;nonlinear system;numerical method;programming paradigm;projections and predictions;shearlet;steady state;total disc replacement;wavelet	Glenn R. Easley;Demetrio Labate;Flavia Colonna	2009	IEEE Transactions on Image Processing	10.1109/TIP.2008.2008070	computer vision;mathematical optimization;shearlet;computer science;signal processing;noise reduction;mathematics;television;statistics	ML	55.70637593867039	-70.87089827726825	195524
fc1871021b5850e22ce368b088530a7d180bbd90	2d filtering of curvilinear structures by ranking the orientation responses of path operators (rorpo)		We present a filtering method for 2D curvilinear structures, called RORPO (Ranking the Orientation Responses of Path Operators). RORPO is based on path operators, a recently developed family of mathematical morphology filters. Compared with state of the art methods, RORPO is non–local and well adapted to the intrinsic anisotropy of curvilinear structures. Since RORPO does not depend on a linear scale-space framework, it tends to preserve object contours without a blurring effect. Due to these properties, RORPO is a useful low-level filter and can also serve as a curvilinear prior in segmentation frameworks. In this article, after introducing RORPO, we develop the 2D version of the algorithm and present a few applications. Source Code The C++ implementation of RORPO 2D along with an online demo are available on the IPOL webpage of this article1	algorithm;c++;high- and low-level;intrinsic dimension;linear scale;mathematical morphology;scale space;web page	Odyssée Merveille;Benoît Naegel;Hugues Talbot;Laurent Najman;Nicolas Passat	2017	IPOL Journal	10.5201/ipol.2017.207	operator (computer programming);filter (signal processing);anisotropy;discrete mathematics;curvilinear coordinates;mathematical morphology;ranking;mathematical optimization;mathematics	ML	54.96442189139951	-69.3621926148898	196174
20fc53835b1c552dfc866a931748b7c3b529153b	2d instantaneous frequency-based method for motion estimation using total variation	motion estimation;lucas kanade method 2d instantaneous frequency based method motion estimation digital videos two dimensional instantaneous frequency information amplitude modulation frequency modulation method optical flow vectors iteratively reweighted norm for total variation algorithm irn tv algorithm synthetic videos three dimensional am fm based method horn schunck method;motion estimation amplitude modulation frequency modulation image sequences;optical imaging;estimation;videos motion estimation optical imaging optical signal processing biomedical optical imaging adaptive optics estimation;optical signal processing;amplitude modulation frequency modulation motion estimation optical flow;biomedical optical imaging;adaptive optics;videos	We present a first approach to a new method to compute the motion estimation in digital videos using the two-dimensional instantaneous frequency information computed using amplitude-modulation frequency-modulation (AM-FM) methods. The optical flow vectors are computed using an iteratively reweighted norm for total variation (IRN-TV) algorithm. We compare the proposed method using synthetic videos versus a previous three-dimensional AM-FM based method and available motion estimation methods such as a phase-based, Horn-Schunck and the Lucas-Kanade methods. The results are promising producing a full density estimation with more accurate results than the other methods.	am broadcasting;algorithm;fm broadcasting;gramática de la lengua castellana;horn clause;instantaneous phase;kanade–lucas–tomasi feature tracker;linear algebra;matlab;modulation;motion estimation;optical flow;synthetic intelligence	Víctor Murray;Paul Rodríguez;Marios S. Pattichis	2014	2014 IEEE Global Conference on Signal and Information Processing (GlobalSIP)	10.1109/GlobalSIP.2014.7032273	computer vision;electronic engineering;motion estimation;mathematics;optics	Robotics	58.61578538827499	-72.15815844469775	196492
42cbfb88cffd5832c4a676d56090b8e19652062e	fractional-order variational regularization for image decomposition	texture image decomposition total variation minimization fractional variational regularization structure;variational techniques evolutionary computation gradient methods image restoration image texture minimisation;会议论文;mathematical model image decomposition tv image edge detection digital signal processing minimization psnr;staircase edges fractional order variational regularization image decomposition geometric objects image texture convex functional evolution equations gradient descent flow image restoration models block artifacts	We propose new models for image decomposition which separate an image into a cartoon, consisting only of geometric objects, and an oscillatory component, consisting of textures or noise. The proposed models are given in a fractional variational formulation, the role of which is to better handle the texture details of image. We compute this decomposition by minimizing a convex functional which depends on the two variable u and v, alternatively in each variable. The resulting evolution equations are the gradient descent flow that minimizes the overall functional. The proposed models have been applied to real images with promising results; unlike the existing TV-based image restoration models, the proposed models don't suffer from block artifacts, staircase edges and false edge near the edges.	calculus of variations;circuit restoration;convex function;gradient descent;image restoration;manifold regularization	Lingling Jiang;Haiqing Yin	2014	2014 19th International Conference on Digital Signal Processing	10.1109/ICDSP.2014.6900821	mathematical optimization;topology;image gradient;mathematics;geometry	Vision	54.968337857707816	-70.56175484203945	196500
63e2c3c50fd5e884c233f2d5a8005f3e02697692	multi-focus pixel-based image fusion in dual domain	mathematical morphology;mathematical morphology dual domain image fusion m dirlots slant texture joint measurement;image fusion;slant texture;dual domain;mathematical morphology processing multifocus pixel based image fusion redundant transform dual domain multiple directional lapped orthogonal transforms m dirlot high pass subbands;joint measurement;transforms image fusion mathematical morphology;image fusion image edge detection discrete wavelet transforms computational efficiency image reconstruction redundancy;m dirlots	Redundant-transform-based image fusion approaches require a lot of memory and computations. This paper proposes an effective and efficient multi-focus image fusion technique in dual domain using a redundant transform. The proposed scheme captures high-frequency informations, e.g. edges and slant textures, of images efficiently, and reduce the computational cost. The proposed scheme extracts the high-frequency information of images with multiple directional lapped orthogonal transforms (M-DirLOTs) through the following procedure: (1) reconstruct the detail image using high-pass subbands, (2) execute a fusion operation in spatial domain through joint measurement and (3) improve the performance by mathematical morphology processing. The proposed method overcomes some disadvantages of traditional transform-based and spatial-based fusion techniques. Experimental results show that the proposed method is able to significantly improve the fusion performance.	computation;computational complexity theory;image fusion;mathematical morphology;pixel	Zhiyu Chen;Shogo Muramatsu	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7471974	computer vision;speech recognition;mathematical morphology;computer science;mathematics;image fusion;top-hat transform;computer graphics (images)	Robotics	59.271521142663026	-67.19243595812772	196536
010cf12e4c6064573f08776039083b30203a6fc0	frequency equalized compounding for effective speckle reduction in medical ultrasound imaging	medical ultrasound imaging;frequency compounding;speckle noise;dynamic quadrature demodulation	Frequency compounding (FC) is commonly used to reduce the speckle variance in order to enhance contrast resolution by averaging two or more uncorrelated sub-band images. However, due to the frequency dependent attenuation, the contrast resolution cannot be enhanced to the theoretical limit when imaging deep-lying tissue. In this paper, we propose the frequency equalized compounding (FEC) method to achieve contrast enhancement in the area of imaging as a whole. In this proposed method, a sub-band signal is divided into several zones along the imaging depth (or time), and the center frequencies and weighting factors for each zone are estimated; the estimated values are used in dynamic quadrature demodulation (DQDM) and image compounding respectively. The performance of the proposed method was evaluated through simulations and experiments. During the evaluation, the contrast resolution was quantified by speckle's signal-to-noise ratio (SSNR) in speckle regions and contrast-to-noise ratio (CNR) in hyper- and hypoechoic regions. Theoretical values of the SSNR and the CNR by the FC were computed by multiplying the SSNR and CNR values measured from the original image by N, where N is the number of sub-bands used in the compounding. From in vitro phantom experiments, it was learned that the SSNR and CNR values from the proposed method were similar to the theoretical values; the maximum and minimum errors from the theoretical value were 9% and 1% while those of the conventional FC (CFC) method were 25% and 7%. Similar results were obtained from the in vivo experiments with RF data acquired from the liver and the kidney. In addition, signal-to-noise ratio (SNR) improvement was measured. The SNR also improved due to the DQDM; maximum improvements for the in vitro and the in vivo experiments were 2.3 dB and 4.8 dB higher the results from the CFC method. These results demonstrate that the proposed FEC method can improve the contrast resolution up to a theoretically achievable value and may be useful in imaging technically difficult patients.	medical ultrasound	Changhan Yoon;Gi-Duck Kim;Yang-Mo Yoo;Tai-Kyong Song;Jin Ho Chang	2013	Biomed. Signal Proc. and Control	10.1016/j.bspc.2013.08.007	speckle noise;telecommunications;optics	ML	55.94684482230505	-79.17272246375818	196569
0bc5f08d1723be2aafd0a9497866e6ed0e0607a8	stochastic truncated wirtinger flow algorithm for phase retrieval using boolean coded apertures		X-ray crystallography is an experimental technique to estimate the 3D atomic positions of the elements present in a crystal. This technique constructs the 3D structure from the phase of diffracted and patterned X-rays (DPX). Multiple intensity DPX measurements are acquired to solve the phase retrieval problem. The feasibility of implementing this technique depends on solving the phase retrieval problem using expensive multiple valued patterns and the Truncated Wirtinger Flow Algorithm. This paper presents a Stochastic Truncated Wirtinger Flow Algorithm (STWF) which solves the phase retrieval problem based on DPX measurements low-cost boolean block-unblock coded apertures. Several simulations are realized to demonstrate the convergence of the STWF algorithm and the optimal parameters of the boolean coded apertures. The results indicate that given the DPX measurements, the quality of reconstructed phase images using STWF attained up 24:63dB of PSNR.	algorithm;digital picture exchange;maximum flow problem;peak signal-to-noise ratio;phase retrieval;simulation	Samuel Pinilla;Camilo Noriega;Henry Arguello	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7953318	mathematical optimization;combinatorics;discrete mathematics;mathematics	Robotics	55.565056880329415	-76.64665004221831	196941
698f47a23f6534943de4ca44f01a1f0f46839319	full band watermarking in dct domain with weibull model	image watermarking;dct;weibull distribution;signal detection	In the framework of maximum-likelihood detection for image watermarking schemes, the conventional Generalized Gaussian Distribution (GGD), Cauchy and Student’s t distributions often fail to model the pulse-like distributions, such as Discrete Cosine Transform (DCT) coefficient distribution. Meanwhile DCT DC coefficients are often neglected in the image watermarking schemes. In this paper an improved full band image watermarking algorithm with utilization of Weibull distribution modeling the DCT AC and DC coefficients is proposed. Experiments indicate that compared with other popluar distributions such as the GGD, the Weibull model gives a closer fit on the distribution of AC coefficients in absolute domain with a smaller Kullback-Leibler (KL) divergence and lower Mean Square Error (MSE). The watermarking scheme with Weibull modeling the DCT AC coefficients (Weibull-AC) exhibits strong robustness under the attack of scaling and median filtering. The watermarking scheme with Weibull modeling the DCT DC coefficients (Weibull-DC) yields a better detection accuracy for bright and more detailed images. Combining the above two advantages, the proposed Weibull based full band watermarking in DCT domain (Weibull-FB) further improves its robustness under the attack of JPEG compression and achieves 10.47 % overall increment in the detection accuracy compared with the baseline system while maintaining good invisibility in the view of structural similarity (SSIM).	algorithm;baseline (configuration management);coefficient;digital watermarking;discrete cosine transform;image scaling;jpeg;kullback–leibler divergence;mean squared error;median filter;structural similarity	Luan Dong;Qin Yan;Yong Lv;Shuyu Deng	2015	Multimedia Tools and Applications	10.1007/s11042-015-3115-2	mathematical optimization;theoretical computer science	ML	60.99933319488441	-68.76492812515879	198120
ccc2ecba1ed743abc3a9505abb4b58b36e33072e	a comparison of x-lets in denoising cdna microarray images	wavelet microarray denoising x let transform;lab on a chip bioinformatics curvelet transforms discrete wavelet transforms image denoising;ssim index cdna microarray image denoising microarray technology bioinformatics gene expression levels image capturing processes gaussian noise stationary wavelet transform swt complex wavelet transform cwt curvelet transform curv contourlet transform cnt sparse transforms microarray image analysis ordinary discrete wavelet transform dwt contourlet sd decomposition steerable pyramid stp shearlet transform shr x let transform bayesshrink method hard thresholding soft thresholding intrascale dependency statistical modeling x let coefficients global thresholding local thresholding structural similarity index;noise reduction psnr continuous wavelet transforms discrete wavelet transforms	Microarray technology has become a power tool in the field of bioinformatics. It is used to measure gene expression levels and similar to any other image capturing processes is prone to noise. There are different kinds of noise, during preparation, hybridization and scanning in microarray images which usually are modeled by Gaussian noise. Since introduction of wavelets in 1970s, many more forms and extensions of this transform have been developed and used, such as stationary wavelet transform (SWT), complex wavelet transform (CWT), curvelet transform (CURV) and contourlet transform (CNT). By developing of more sparse transforms, it is important to have a perspective of how efficient the transforms are in different applications, such as microarray image analysis. In this paper, we compare the efficiency of common sparse transforms including ordinary discrete wavelet transform (DWT), SWT, CWT, CURV, CNT, Contourlet-SD decomposition, steerable pyramid (STP) and shearlet transform (SHR) for microarray image denoising. Therefore after converting microarray image into x-let transform, BayesShrink method, soft and hard thresholding are used to perform denoising of these images. Both local and general thresholds are calculated for each subband in order to evaluate the effect of incorporating intrascale dependency on top of sparsity property in statistical modeling of x-let's coefficients. Our simulation results show that CWT and SHR outperforms the others when using global thresholding and SWT is the preferred transform when using local thresholding. Although STP and SHR have better performance for some criteria like structural similarity (SSIM) index, but CWT is faster.	bioinformatics;coefficient;complex wavelet transform;contourlet;curvelet;dna microarray;discrete wavelet transform;image analysis;noise reduction;pyramid (image processing);shr;shearlet;simulation;sparse matrix;standard widget toolkit;stationary process;stationary wavelet transform;statistical model;structural similarity;thresholding (image processing)	Rouzbeh Shams;Hossein Rabbani;Saeed Gazor	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6854119	wavelet;computer vision;constant q transform;speech recognition;s transform;harmonic wavelet transform;second-generation wavelet transform;continuous wavelet transform;pattern recognition;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;wavelet transform	Visualization	58.10065592932625	-68.06328810634066	198291
18da8df766ccc1c6e383b48f0692a63e4c285a04	motion estimation using adaptive blocksize observation model and efficient multiscale regularization	motion analysis;bayesian motion estimation;multiscale smoothing algorithm;probability;prior model;image matching;optimization process;bayes methods;motion estimation bayesian methods apertures motion measurement measurement errors simulated annealing markov random fields algorithm design and analysis covariance matrix costs;motion estimation;adaptive matching algorithm pdf models adaptive blocksize observation model efficient multiscale regularization bayesian motion estimation motion field model optimization process sequential approach simulated annealing prior model bayesian method initialisation markov random field modeling multiscale smoothing algorithm;simulated annealing;bayesian method;probability motion estimation adaptive signal processing simulated annealing bayes methods markov processes image matching smoothing methods;smoothing methods;adaptive matching algorithm;adaptive signal processing;markov random field modeling;efficient multiscale regularization;markov processes;sequential approach;motion field model;bayesian method initialisation;adaptive blocksize observation model;pdf models	Bayesianmotion estimationrequirestwo pdf models: observation modelandmotion field (prior) model. The optimizationprocessfor this methodusessequential approach, e.g. simulatedannealing. This paperproposesadapti ve blocksizeobservation modelandmultiscaleregularization for theprior modelandtheoptimizationprocess.Thepurposesare to increasethe speedand to improve the result. Theproposedframeworkcaninitialize thebayesianmethod. Theresultin thispapershowsoneof thepossibilityof its usage.Many strategiescanbederivedfrom this framework to work for itself or to supporttheMarkov randomfield modeling for motionestimation.	motion estimation;portable document format	Stephanus Suryadarma Tandjung;Teddy Surya Gunawan;Man-Nang Chong	2000		10.1109/ICIP.2000.899490	adaptive filter;computer vision;mathematical optimization;simulated annealing;bayesian probability;computer science;machine learning;pattern recognition;motion estimation;probability;mathematics;markov process;statistics	Vision	60.76747754704517	-72.28499931052444	198755
557360d2aba884aacc204c51745358583f95597d	improving image restoration with soft-rounding	image restoration histograms kernel visualization noise measurement transforms convolution;text analysis image enhancement image resolution image restoration least squares approximations;general natural image restoration regularized least square restoration framework quadratic penalty function binary text image restoration binary barcode image restoration multiple distinct pixel value soft rounding enhanced restoration method visual quality psnr ssim	Several important classes of images such as text, barcode and pattern images have the property that pixels can only take a distinct subset of values. This knowledge can benefit the restoration of such images, but it has not been widely considered in current restoration methods. In this work, we describe an effective and efficient approach to incorporate the knowledge of distinct pixel values of the pristine images into the general regularized least squares restoration framework. We introduce a new regularizer that attains zero at the designated pixel values and becomes a quadratic penalty function in the intervals between them. When incorporated into the regularized least squares restoration framework, this regularizer leads to a simple and efficient step that resembles and extends the rounding operation, which we term as soft-rounding. We apply the soft-rounding enhanced solution to the restoration of binary text/barcode images and pattern images with multiple distinct pixel values. Experimental results show that soft-rounding enhanced restoration methods achieve significant improvement in both visual quality and quantitative measures (PSNR and SSIM). Furthermore, we show that this regularizer can also benefit the restoration of general natural images.	algorithm;barcode;circuit restoration;experiment;image restoration;least squares;peak signal-to-noise ratio;penalty method;pixel;rounding;structural similarity	Xing Mei;Honggang Qi;Bao-Gang Hu;Siwei Lyu	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.60	image restoration;computer vision;mathematical optimization;pattern recognition;mathematics	Vision	57.89206757674546	-70.67298552737824	199117
cc5ce3b24a1b3e611f267e029e59d48b903471bd	a fast and adaptive method for determining $k_{1}$ , $k_{2}$ , and $k_{3}$ in the tensor decomposition-based anomaly detection algorithm		"""In our previous work, a tensor decomposition-based anomaly detection algorithm has been proposed. However, determining <inline-formula> <tex-math notation=""""LaTeX"""">$K_{1}$ </tex-math></inline-formula>, <inline-formula> <tex-math notation=""""LaTeX"""">$K_{2}$ </tex-math></inline-formula>, and <inline-formula> <tex-math notation=""""LaTeX"""">$K_{3}$ </tex-math></inline-formula> (i.e., the major principal component numbers along the three modes of hyperspectral data) has not been settled satisfactorily. In this letter, a fast and adaptive method for determining <inline-formula> <tex-math notation=""""LaTeX"""">$K_{1}$ </tex-math></inline-formula>, <inline-formula> <tex-math notation=""""LaTeX"""">$K_{2}$ </tex-math></inline-formula>, and <inline-formula> <tex-math notation=""""LaTeX"""">$K_{3}$ </tex-math></inline-formula> is proposed. In the proposed method, the determination problem is converted into an optimization problem by constructing the energy function by maximizing the anomalous degree of the reconstructed anomaly data in both spectral and spatial domains. In order to reduce the computational complexity, a fast initialization strategy is introduced to initialize those parameters in the feature space directly. In addition, to avoid the problem of parameter selection, an adaptive strategy is utilized. Furthermore, <inline-formula> <tex-math notation=""""LaTeX"""">$K_{1}$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=""""LaTeX"""">$K_{2}$ </tex-math></inline-formula> are considered to be independent, making the degree of freedom of the three parameters conform with the actual. Experiments with three hyperspectral data sets reveal that the proposed method works effectively."""	algorithm;anomaly detection;computational complexity theory;experiment;feature vector;horizontal situation indicator;mathematical optimization;optimization problem;principal component analysis;time complexity	Xing Zhang;GongJian Wen	2018	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2017.2759963	tensor;mathematics;anomaly detection;degrees of freedom (statistics);principal component analysis;stress (mechanics);computational complexity theory;algorithm;feature vector;optimization problem	Vision	68.16344721284615	-69.3829391327143	199189
78ec00002eb6f9864ef5c633482d14626e1b092d	shape from shading using ritz method with tent basis	minimisation;ritz method;linear combination;spline;perturbation method;shading information;shape from shading;tensile stress;cost function;reflectivity;bismuth;imaging model;weighting coefficients;discretization;weighted basis functions;surface reconstruction;splines mathematics;iterative methods;brightness;shape;tent basis;surface height;image reconstruction;signal representation;shape cost function surface reconstruction brightness tensile stress image reconstruction reflectivity bismuth lighting spline;perturbation method shape from shading ritz method tent basis shading information cost function surface height imaging model light direction linear combination discretization signal representation weighted basis functions weighting coefficients;lighting;light direction;image modeling	A robust approach to recover the shape of an object from shading information is presented. The scheme is derived based on the perturbation method that minimizes a cost function for computing a surface height from a single image given the imaging model and light direction. To describe the desired solution variables as a linear combination of a set of basis functions, the Ritz method is applied in our approach to discretize the cost function associated with the regularized shape from shading problem. This kind of discretization is simple and the concept of signal representation by a set of weighted basis functions can be clearly stated. To enhance the robustness of alternative scheme, two new constraints are added into the cost function. They constrain the behavior of high-order change rate between the variables, and bound the variables more tightly. Thus, the changes of variables will be more regular. Hence, new constraints strengthen the relations between the input image and the reconstructed surface height. After finding the weighting coefficients using a perturbation method, the solution of the SFS problem will finally be decided.	photometric stereo;shading	Chen-Kai Huang;Chuang Pal;Wen-Thong Chang	1996		10.1109/ICPR.1996.546057	iterative reconstruction;spline;minimisation;mathematical optimization;mathematical analysis;photometric stereo;surface reconstruction;linear combination;shape;ritz method;bismuth;discretization;lighting;mathematics;geometry;reflectivity;iterative method;stress;brightness;statistics	Vision	55.2222809969671	-70.81064895010044	199281
eff84f7312eeb594a126d462f549def7b31843b4	edge preservation of impulse noise filtered images by improved anisotropic diffusion	impulsive noise reduction;anisotropic diffusion;second order difference;edge preservation	This paper provides a robust scheme for random valued impulsive noise reduction along with edge preservation by anisotropic diffusion with improved diffusivity. The defective impulse noisy pixels are detected by Laplacian based second order pixel difference operation where these defective pixels are replaced by appropriate values with regard of the gray level of their four directional neighbors. This de-noised image undergoes the diffusion operation where diffusion coefficient function is modified to make it adaptive by incorporating local gray level variance information. The proposed modified diffusion scheme effectively restore the edges and fine details destroyed during impulse noise reduction process. The effect of proposed diffusion scheme has been studied on various images and the results are compared with some existing diffusion methods which are independently used for impulse noise reduction and edge preservation. The results shows that the prior removal of impulsive noise before the application of diffusion process is advantageous over the direct application of diffusion for removing the impulsive noise. In addition, the results of the proposed diffusion scheme are compared with some of the median filter based methods which are effectively used for impulse noise reduction without caring of edge preservation. The proposed diffusion scheme sufficiently preserves the edges without boosting of impulsive noise components on images corrupted up to 50 % of the impulsive noise density.	anisotropic diffusion;circuit restoration;coefficient;defective pixel;grayscale;impulse noise (audio);mathematical optimization;median filter;noise reduction;signal-to-noise ratio;smoothing;thresholding (image processing)	Nafis Uddin Khan;K. V. Arya;Manisha Pattanaik	2013	Multimedia Tools and Applications	10.1007/s11042-013-1620-8	computer vision;mathematical optimization;value noise;computer science;anisotropic diffusion;salt-and-pepper noise	Vision	56.8596300095298	-66.24886095412374	199311
860e6472c25b7f870dd5955cc51ac79eeeaeed66	clustered compressed sensing via bayesian framework	bayesian framework;clustered prior;bayes methods image reconstruction compressed sensing noise measurement magnetic resonance imaging mathematical model signal processing;mean square error methods bayes methods compressed sensing magnetic resonance imaging;compressive sensing;clustred lasso;psnr clustered compressed sensing bayesian framework magnetic resonanse images signal recovery synthetic images medical images mri images mean square error mse pick signal to noise ratio;sparse prior;lasso;posterior;clustred lasso bayesian framework sparse prior clustered prior posterior compressive sensing lasso	This paper provides clustered compressive sensing (CCS) based signal processing using Bayesian framework. Images like magnetic resonanse images (MRI) are usually very weak due to the presence of noise and due to the weak nature of the signal itself. Compressed sensing (CS) paradigm can be applied in order to boost such signal recoveries. We applied CS paradigm via Bayesian framework. That is incorporating the different prior information such as sparsity and the special structure that can be found in such sparse signal improves signal recovery. The method is applied on synthetic and medical images including MRI images. The results show that applying the clustered compressive sensing out performs the non clustered but only sparse counter parts when it comes to mean square error(MSE), pick signal to noise ratio (PSNR) and other performance metrics.	algorithm;cluster analysis;compressed sensing;detection theory;lars bak (computer programmer);lasso;least squares;mean squared error;neural coding;noise (electronics);peak signal-to-noise ratio;programming paradigm;signal processing;sparse matrix;synthetic intelligence;waist–hip ratio	Solomon Tesfamicael;Faraz Barzideh	2015	2015 17th UKSim-AMSS International Conference on Modelling and Simulation (UKSim)	10.1109/UKSim.2015.21	computer science;machine learning;pattern recognition;statistics	Robotics	53.9177767304803	-75.38962033312774	199711
b57d5cdbc9cb2d8c91d57c4fed505f042693ed5a	regularized generalized inverse accelerating linearized alternating minimization algorithm for frame-based poissonian image deblurring	poisson noise;90c90;image deblurring;65t60;68u10;linearized alternating minimization;framelet;regularized generalized inverse	Blurred images corrupted by Poisson noise frequently appear in medical and astronomical applications, and variational models based on sparsity priors such as the total variation and the framelet have been devoted to the problem of recovering the Poissonian images. However, developing efficient algorithms for solving the associated optimization problems is still an attractive research area due to the ill-posedness of the blurring operator and the complexity of the data-fidelity term. In this paper, we propose an accelerating linearized alternating minimization algorithm called GILAM for solving the frame-based variational model for Poissonian image deblurring. In the proposed method, a modification of the generalized inverse is introduced to overcome the negative effect of the ill-posed blurring operator, and, further, a discrepancy function is used to adjust the value of the regularization parameter automatically. The global convergence of the proposed algorithms is also investigated. Numerical experiment...	algorithm;deblurring	Dai-Qiang Chen	2014	SIAM J. Imaging Sciences	10.1137/130932119	mathematical optimization;mathematical analysis;shot noise;mathematics;quantum mechanics;statistics	Vision	56.39753350439045	-72.21834633350585	199756
158350bea89c90890c83e8431d6e4d7575b33ea9	image reconstruction from projection under periodicity constraints using genetic algorithm	image reconstruction;genetic algorithm	In this paper we study the problem of image reconstruction from a small number of projections. This type of problem arises in material science during developing the program for the reconstruction of crystalline structure from their projection image, obtained by high-resolution transmission electron microscopy. The problem has large number of solutions due to few projections. To reduce the number of solutions we can use some priori information about the object. This priori information is called constraints. One of these constraints is periodicity constraint. We use genetic algorithm to optimize the solution, which is an evolutionary technique to solve the problem.	genetic algorithm;iterative reconstruction;quasiperiodicity	Narender Kumar;Tanuja Srivastava	2010		10.1007/978-3-642-14834-7_8	iterative reconstruction;mathematical optimization;genetic algorithm;computer science;machine learning;pattern recognition	AI	57.73588555711744	-76.76305130726648	199950
