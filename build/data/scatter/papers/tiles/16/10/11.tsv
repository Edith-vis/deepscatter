id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
11ad3e03c99b1ac9e200927372b8c5748328fc10	optimization and quality estimation of circuit design via random region covering method		Random region covering is a global optimization technique that explores the landscape by introducing multiple random starting points to initiate the local optimization solvers. This study applies the random region covering technique to circuit design automation and proposes a theory to explain why this technique is efficient at searching for the global optimum. In addition to analyzing the efficiency of the random region covering algorithm, the theory gives a probability-based estimation of the goodness of the optimization result. To enhance the efficiency of the random region covering technique, this work evaluates the boundary of top performance regions and proposes a modified random region covering method that only performs the global optimization on the top design region. The results from a large number of mathematical experiments verify the proposed methodology. The optimized designs of a class-E power amplifier and a wide load range operational amplifier outperform both manual designs and other state-of-the-art optimization techniques.	algorithm;analysis of algorithms;audio power amplifier;benchmark (computing);best, worst and average case;circuit design;covering space;experiment;global optimization;local convergence;machine learning;mathematical optimization;operational amplifier;randomized algorithm	Zhaori Bi;Dian Zhou;Sheng-Guo Wang;Xuan Zeng	2017	ACM Trans. Design Autom. Electr. Syst.	10.1145/3084685	automation;operational amplifier;global optimization;local search (optimization);computer science;mathematical optimization;amplifier;circuit design;global optimum	EDA	26.86439764235372	48.43168685230614	79376
053626d83a212c7a8e6f36e12df82ca1c09b2311	rate-matched regenerating code in hostile networks	regenerating code;errorcorrection;malicious nodes rate matched regenerating code hostile networks distributed storage systems minimum storage regeneration code msr code mbr code minimum bandwidth regeneration code;bandwidth servers error correction codes encoding security symmetric matrices next generation networking;adversary;adversary regenerating code mds code errorcorrection;encoding;mds code	Regenerating code is a class of code very suitable for distributed storage systems, which can maintain optimal bandwidth and storage space. Two types of important regenerating code have been constructed: the minimum storage regeneration (MSR) code and the minimum bandwidth regeneration (MBR) code. However, in hostile networks where adversaries can compromise storage nodes, the storage capacity of the network can be significantly affected. In this paper, we propose a rate-matched MSR code that can combat against this kind of adversaries in hostile networks. We optimize the code parameters for given system requirements. Our comprehensive analysis shows that our code can detect and correct malicious nodes with higher storage efficiency compared to the normal error correction MSR code.	algorithm;clustered file system;error detection and correction;requirement;storage efficiency;system requirements	Jian Li;Tongtong Li;Jian Ren	2015	2015 IEEE International Conference on Communications (ICC)	10.1109/ICC.2015.7249235	low-density parity-check code;telecommunications;computer science;adversary;code rate;distributed computing;locally testable code;computer security;encoding	HPC	35.68597812484591	59.0518713128614	79419
af56a150dfb358d7d5c507a8d67fe8c9523b95a6	an experimental word decode and drive system for a magnetic film memory with 20-ns read-cycle time	electronic circuits;cycle time;design principle;conductive films;eddy currents;circuit noise;integrated circuit;decoding;building block;magnetic films;pulse amplifiers;high spped;coupling circuits;film memory;word decode system electronic circuits film memory high spped;magnetic film;storage capacity;decoding magnetic films conductive films circuit noise noise cancellation pulse amplifiers eddy currents driver circuits space vector pulse width modulation coupling circuits;noise cancellation;driver circuits;cross section;space vector pulse width modulation;high speed;word decode system;memory model	Design principles of a word selection and drive scheme for a 20-ns nondestructive-READ-cycle-time flat-film memory with 150 000 bits storage capacity, are presented and verified by cross-sectional tests. The word lines are matched at one end and driven from the other by a high-speed driver building block compatible with integrated circuit technology which is connected to a decode matrix. Suitable decode matrix line drivers for providing pulses of different width and amplitude for READ operations up to 50 MHz and WRITE operations up to 20 MHz, respectively, are described. Experiments with a cross-sectional setup for 64 outputs prove the feasibility of the system. Its operation in connection with a memory model indicates an access time of 30 ns.		Dieter Seitzer	1967	IEEE Trans. Electronic Computers	10.1109/PGEC.1967.264813	memory model;electronic circuit;electronic engineering;telecommunications;cycle time variation;computer science;engineering;eddy current;electrical engineering;integrated circuit;active noise control;cross section	Visualization	30.705604319135183	53.04923254193238	79460
7a636d522332f878cb5dca42dd0ce3bd0d63409d	an approach for dynamic selection of synthesis transformations based on markov decision processes	high quality scripts;probability;history;logic design;applied synthesis steps dynamic selection synthesis transformations markov decision processes logic synthesis systems loosely related function preserving transformations complete synthesis run transformation sequences synthesis scripts tool developer high quality scripts transition probability;transition probability;transformation sequences;prototypes;markov processes delay optimization benchmark testing history prototypes context;applied synthesis steps;loosely related function preserving transformations;dynamic selection;synthesis transformations;logic synthesis;logic synthesis systems;markov process;probability logic design markov processes;optimization;synthesis scripts;expert knowledge;markov processes;markov decision process;tool developer;markov decision processes;context;benchmark testing;complete synthesis run	Modern logic synthesis systems apply a sequence of loosely-related function-preserving transformations to gradually improve the circuit with respect to certain criteria such as area, performance, power, etc. For the quality of a complete synthesis run, the application order of the transformations for the individual steps are critical as they can produce vastly different outcomes. In practice, the transformation sequences is encoded in synthesis scripts which are derived manually based on experience and intuition of the tool developer. These scripts are static in the sense that transformations are applied independently of the result of previous transformations or the current status of the design. Despite the importance of obtaining high quality scripts, there are only a few attempts to optimize them. In this paper, we present a novel method to select transformations dynamically during the synthesis run leveraging the theory of Markov Decision Processes. The decision to select a particular transformation is based on transition probabilities, the history of the applied synthesis steps, and expectations for future steps. We report experimental results obtained from an implementation of the approach using the logic synthesis system ABC.	display resolution;iteration;linear programming;logic synthesis;markov chain;markov decision process;mathematical optimization;prototype	Tobias Welp;Andreas Kuehlmann	2011	2011 Design, Automation & Test in Europe	10.1109/DATE.2011.5763328	markov decision process;logic synthesis;computer science;artificial intelligence;machine learning;markov process;algorithm;statistics	EDA	26.391787103544488	46.51295548497872	79651
c0b59132d3497954d33e610e941423844f4b1fe2	accurate calculation of bit-level transition activity using word-level statistics and entropy function	entropy;logic cad;power consumption;signal processing;switching circuits;lsbs;triple-bit type model;accurate calculation;bit level switching activity;bit level transition activity;conditional entropy;correlation factor;data path operators;dual bit type model;entropy based calculation;entropy function;input statistics;multiplier output;polynomial approximation;sigmoidal model;sign bits;switching activity;word level statistics	Accurate models for the calculation of bit level switching activity in data path operators, by combining the dual bit type model with the entropy based calculation are presented. Given the input statistics, the conditional entropy per bit is estimated by means of a sigmoidal model. A polynomial approximation to the entropy of the sign bits has been developed. Entropy and switching activity of primary inputs are used to estimate the output activity. The Triple-Bit Type (TBT) model is introduced to describe the behavior of the LSBs of the multiplier output, which do not behave as white noise. The error between the results of the proposed models and the measurements of entropy and switching activity is about 1% for signals with correlation factor less than 0.8.	bit-level parallelism	Efstathios D. Kyriakis-Bitzaros;Spiridon Nikolaidis;Anna Tatsaki	1998		10.1109/ICCAD.1998.743077	model checking;approximate entropy;algorithm design;entropy;mathematical optimization;electronic circuit;joint entropy;electronic engineering;discrete mathematics;binary entropy function;transfer entropy;formal verification;maximum entropy probability distribution;computer science;principle of maximum entropy;dissipation;theoretical computer science;signal processing;probability;mathematics;white noise;joint quantum entropy;differential entropy;cross entropy;maximum entropy spectral estimation;microelectronics;entropy rate;algorithm;conditional entropy;functional verification;statistics;systems design	ML	27.950384389301924	52.008746655435246	79942
462a7133fb3a6addc36d7081217558dd43c354ea	on metric properties of maps between hamming spaces and related graph homomorphisms	schrijver s ź function;schrijver s θ function;graph homomorphism;error correcting codes;projective geometry over f 2	A mapping of k-bit strings into n-bit strings is called an (α, β)-map if kbit strings which are more than αk apart are mapped to n-bit strings that are more than βn apart. This is a relaxation of the classical problem of constructing error-correcting codes, which corresponds to α = 0. Existence of an (α, β)-map is equivalent to existence of a graph homomorphism H̄(k, αk) → H̄(n, βn), where H(n, d) is a Hamming graph with vertex set {0, 1}n and edges connecting vertices differing in d or fewer entries. This paper proves impossibility results on achievable parameters (α, β) in the regime of n, k → ∞ with a fixed ratio n k = ρ. This is done by developing a general criterion for existence of graph-homomorphism based on the semidefinite relaxation of the independence number of a graph (known as the Schrijver’s θ-function). The criterion is then evaluated using some known and some new results from coding theory concerning the θ-function of Hamming graphs. As an example, it is shown that if β > 1/2 and n k – integer, the n k -fold repetition map achieving α = β is asymptotically optimal. Finally, constraints on configurations of points and hyperplanes in projective spaces over F2 are derived.	alexander schrijver;code;email;error detection and correction;forward error correction;graph homomorphism;hamming space;linear programming relaxation;map;mceliece cryptosystem;welch's method;window function	Yury Polyanskiy	2017	J. Comb. Theory, Ser. A	10.1016/j.jcta.2016.08.005	combinatorics;discrete mathematics;topology;mathematics;graph homomorphism;algebra	Theory	38.43661835717147	52.96370350236059	79973
cf13bf2e62cd7d0624bc2687c19500dda70c06ce	wavelet analysis for the detection of parametric and catastrophic faults in mixed-signal circuits	wavelet analysis;mahalanobis distance;parametric fault;and mixed signal;fault free circuit;circuit design;testing;spectrum;euclidean distance;product line;mahalanobis distance wavelet analysis catastrophic fault mixed signal circuits parametric fault wavelet transform fault free circuit euclidean distance;power supply;network analysis;wavelet transforms;wavelets circuit design circuit testing mahalanobis distance microcontroller based mc based testing mixed signal testing power supply current measurements;mixed signal testing;mixed signal circuits;wavelet transform;current measurement;power supply current measurements;fault detection;catastrophic fault;test methods;testing fault detection wavelet analysis current measurement electrical fault detection;mixed analogue digital integrated circuits;circuit testing;wavelet transforms circuit testing fault diagnosis mixed analogue digital integrated circuits network analysis;root mean square;computer simulation;wavelets;electrical fault detection;fault diagnosis;microcontroller based mc based testing	Methods for testing both parametric and catastrophic faults in analog and mixed-signal circuits are presented. They are based on the wavelet transform (WT) of the measured signal, be it supply current (IPS) or output voltage (VOUT) waveform. The tolerance limit, which affects fault detectability, for the good or reference circuit is set by statistical processing data obtained from a set of fault-free circuits. In the wavelet analysis, two test metrics, one based on a discrimination factor using normalized Euclidean distances and the other utilizing Mahalanobis distances, are introduced. Both metrics rely on wavelet energy computation. Simulation results from the application of the proposed test methods in testing known analog and mixed-signal circuit benchmarks are given. In addition, experimental results from testing actual circuits and from production line testing of a commercial electronic circuit are presented. These results show the effectiveness of the proposed test methods employing the two test metrics against three other test methods, namely, a test method based on the root-mean-square value of the measured signal, a test method utilizing the harmonic magnitude components of the measured signal spectrum, and a method based on the WT of the measured signal.	algorithm;artificial neural network;catastrophic interference;cluster analysis;coefficient;computation;computational complexity theory;electronic circuit;fault coverage;fault detection and isolation;ips panel;mixed-signal integrated circuit;reference circuit;spice 2;simulation;spectral density;support vector machine;waveform;wavelet transform	Alexios Spyronasios;Michael G. Dimopoulos;Alkis A. Hatzopoulos	2011	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2011.2115550	computer simulation;wavelet;embedded system;electronic engineering;real-time computing;engineering;automatic test pattern generation;mathematics;statistics;wavelet transform	EDA	25.02649546492063	52.59075860400358	80309
1ab8c0771ce48f37dbea003b1b2742d6e70b89a0	secret sharing schemes based on additive codes over gf(4)	minimal access structure;94a62;generalized t design;11t71;secret sharing scheme;additive codes;access structure	A secret sharing scheme (SSS) was introduced by Shamir in 1979 using polynomial interpolation. Later it turned out that it is equivalent to an SSS based on a Reed–Solomon code. SSSs based on linear codes have been studied by many researchers. However there is little research on SSSs based on additive codes. In this paper, we study SSSs based on additive codes over GF(4) and show that they require at least two steps of calculations to reveal the secret. We also define minimal access structures of SSSs from additive codes over GF(4) and describe SSSs using some interesting additive codes over GF(4) which contain generalized 2-designs.	access structure;hexacode;linear code;polynomial interpolation;qr code;reed–solomon error correction;shamir's secret sharing;utility functions on indivisible goods	Jon-Lark Kim;Nari Lee	2016	Applicable Algebra in Engineering, Communication and Computing	10.1007/s00200-016-0296-5	combinatorics;discrete mathematics;theoretical computer science;mathematics	Theory	38.1909432686846	54.803152319205765	80464
64cca405efc68c28321b8cb22345d82fc6b4f107	pseudorandom rounding for truncated multipliers	multiplier;truncated multipliers;erreur;multiple precision;multiplications rounding truncated multipliers multiple precision floating point numbers pseudorandom rounding;numero seudo aleatorio;truncamiento;rounding;statistical properties;least significant bit;multiplicateur;nombre pseudoaleatoire;troncature;floating point numbers;multiplications;pseudorandom number;digital arithmetic;floating point;coma flotante;error;multiplicacion;truncation;pseudorandom rounding;multiplication;information science numerical simulation chemicals magnetic flux logic large scale integration laboratories computer architecture quantum computing;multiplicador;error aproximado;rounding error;numerical simulation;virgule flottante;erreur arrondi	An economical, unbiased, overflow-free rounding scheme for multiplication of multiple-precision floating-point numbers is proposed. The new rounding scheme, which we call “the pseudorandom rounding,” saves multiplications of lower bits, and makes use of statistical properties of bits around the least significant bit of products in order to compensate for truncated parts. The method is deterministic, and inputs are commutable. The validity of the rounding is verified by numerical simulation.	computer simulation;least significant bit;most significant bit;pseudorandom number generator;pseudorandomness;rounding	N. Yoshida;Eiichi Goto;Shuichi Ichikawa	1991	IEEE Trans. Computers	10.1109/12.83650	computer simulation;arithmetic;computer science;floating point;theoretical computer science;operating system;round-off error;mathematics;rounding;algorithm	Theory	30.821899834402338	48.317848768189414	82676
c0cce98afb706e6d4ab83fb8a5991e632099bb44	improving wireless product testing: an opportunity for university and industry collaboratio	collaboration;collaboration radio frequency probes research and development circuit testing calibration production integrated circuit testing built in self test circuit topology;probes;circuit topology;research and development;built in self test;radio frequency;integrated circuit testing;production;circuit testing;calibration	Intersil Corporation Wireless Division has used University R&D in the past to generate ideas for WAT PCM level self-testing with some success, but has not yet capitalized on all the opportunities possible through university collaborations. Generally the design R&D area has generated the largest payback with new theory, measurement techniques or calibrations, and circuit topology or ideas as the main areas of University Industry teamwork.	circuit topology	Jim Paviol	2003		10.1109/TEST.2003.1271129	topology;electronic engineering;calibration;engineering;electrical engineering;circuit design;radio frequency;computer engineering;collaboration	DB	28.753724006799125	53.319271899890865	83284
a1ebbaf843e32f80e4767a0b07b0563ae74f5dc9	nondestructive readout of metallic-tape computer cores	fabrication;application software;magnetic cores application software fabrication magnetization crystallization circuits toroidal magnetic fields magnetic materials earth satellites;earth;magnetic materials;crystallization;satellites;circuits;toroidal magnetic fields;magnetic cores;magnetization;high speed	The subject of this investigation is nondestructive readout of metallic-tape memory cores by the application of a magnetomotive force spatially in quadrature to the direction of remanent flux. A simple method of fabrication is proposed and empirical data for the design of the nondestructive read systems is obtained. The use of nondestructive readout is not limited to digital computer circuits and no attempt has been made to use this method in any particular application; an experimental shift register was built, however, to test the method in a practical application. The nature of the system permits high-speed low-current-level operation in either digital or analog applications.		Lloyd M. Lambert	1959	IRE Trans. Electronic Computers	10.1109/TEC.1959.5222060	electronic circuit;magnet;electronic engineering;application software;magnetization;electrical engineering;earth;crystallization;nuclear magnetic resonance;fabrication;physics;satellite;quantum mechanics	HCI	30.65407794047864	53.01286985929531	85110
4210ade8fcd12e9d42699c751e3d64af11f14858	optimization of rf circuits by expert system monitored genetic computation	expert systems;circuit design;genetics;vco designs rf circuit optimization expert system genetic computation automated rfic circuit design genetic algorithm lna designs;vco designs;genetic algorithm;voltage controlled oscillators circuit optimisation expert systems genetic algorithms low noise amplifiers radiofrequency integrated circuits;radio frequency expert systems monitoring design optimization radiofrequency integrated circuits circuit synthesis genetic algorithms automatic control control systems engines;voltage controlled oscillators;genetic algorithms;rf circuit optimization;radiofrequency integrated circuits;low noise amplifiers;circuit optimisation;automated rfic circuit design;genetic computation;lna designs;circuit optimization;expert system	A novel approach for automated RFIC circuit design & optimization is presented. It consists of genetic algorithm (GA) optimizers, which are controlled by an expert system. The combination of the two engines alleviates individual shortcomings and enhances the operation of the overall system. In this paper, the overall system and individual blocks of the expert system are described while results on the optimization of LNA and VCO designs are presented	circuit design;computation;expert system;genetic algorithm;launch numerical aperture;mathematical optimization;rfic;radio frequency;voltage-controlled oscillator	G. Konstantopoulos;K. Papathanasiou;A. Samelis	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1693816	control engineering;electronic engineering;genetic algorithm;computer science;engineering;electrical engineering;expert system	Arch	27.302197103695907	48.63649068167467	85393
a305164d6c792d1ea1dd88482ea56d62be17e302	universal waveforms processor		This paper presents a flexible physical layer (PHY) implementation based on the National Instruments (NI) USRP-RIO software-defined radio (SDR) platform. The implementation allows to reconfigure important parameters of the physical layer during run-time to create a multitude of modern waveforms. In addition, a first performance evaluation of the transceiver is given. The source code of the field programmable gate array (FPGA) design is freely available as open source.	etsi satellite digital radio;field-programmable gate array;noise shaping;open-source software;phy (chip);performance evaluation;pulse shaping;real-time clock;run time (program lifecycle phase);subcarrier;transceiver;universal software radio peripheral;waveform;window function	Martin Danneberg;Ahmad Nimr;Nicola Michailow;Shahab Ehsanfar;Maximilian Matthe;Ana Belen Martinez;Dan Zhang;Gerhard P. Fettweis	2018	2018 European Conference on Networks and Communications (EuCNC)	10.1109/EuCNC.2018.8442548	field-programmable gate array;source code;transceiver;physical layer;waveform;phy;electronic engineering;computer science;orthogonal frequency-division multiplexing	EDA	30.25692091544775	59.74815963629398	85576
868a262da9ee8a8d42d1a582c340086916a40674	serial data driven cyclic redundancy check generator for low power rfid applications	recuperacion de reloj;desciframiento;reconstitution rythme;componente pasivo;passive component;composant passif;identificacion por radiofrecuencia;systeme embarque;decodage;decoding;telecommunication sans fil;implementation;logique combinatoire;puce rfid;stockage donnee;rfid tag;identification par radiofrequence;crc;embedded systems;combinatory logic;data storage;consumo electricidad;emetteur recepteur;low power;logica combinatoria;telecomunicacion sin hilo;senal numerica;rfid;electric power consumption;low power electronics;emisor receptor;signal numerique;registro dispersion;radio frequency identification;almacenamiento datos;horloge;dispositif a memoire;digital signal;digital storage;circuit integre radiofrequence;radiofrequency integrated circuits;implementacion;stockage numerique;electronique faible puissance;registre decalage;clock;memory devices;consommation electricite;transceiver;clock recovery;shift register;data driven;reloj;wireless telecommunication	Cyclic Redundancy Checks (CRCs) are commonly used for their effective error-detecting capabilities in various wireless transceivers, digital networks, data storage devices, embedded designs, RFID systems, etc. The conventional clock-driven and combinational logic based CRC hardware implementations are known to consume considerable power especially those used in wireless passive devices. This paper proposes a low-power implementation of a novel serial data driven CRC for passive RFID tags. This CRC design is not explicitly clocked, but driven by the incoming encoded data as opposed to clock recovery. The associated CRC computations are done in parallel to the data decoding procedure unlike the traditional shift register implementations. Simulation results of the data driven CRC-16 design show significant reduction in power consumption and the area occupied as compared to the typical CRC-16 implementations used in RFID applications.	cyclic redundancy check	Vyasa Sai;Ajay Ogirala;Marlin H. Mickle	2012	J. Low Power Electronics	10.1166/jolpe.2012.1221	radio-frequency identification;embedded system;electronic engineering;telecommunications;computer science;engineering;operating system	EDA	29.347212947738605	55.8382894808972	86575
32a483cc98f8b15e6fbd79986ebb7220a29d74ea	reconfigurable low complexity fir filters for software radio receivers	radio receivers;radio receivers computational complexity fir filters;finite impulse response filter software radio receivers channel bank filters sampling methods reconfigurable architectures land mobile radio mobile communication clocks frequency;software defined radio;low complexity;finite impulse response filters;software radio;reconfigurable architecture;finite impulse response;binary common subexpression elimination;computational complexity;binary common subexpression elimination reconfigurable low complexity fir filters software radio receivers sdr channelizers finite impulse response filters;reconfigurable low complexity fir filters;fir filter;software radio receivers;common subexpression elimination;fir filters;high speed;sdr channelizers	The most computationally demanding block of a software defined radio (SDR) receiver is the channelizer which operates at the highest sampling rate. Reconfigurability and low complexity are the two key requirements of the SDR channelizers. Two new reconfigurable architectures of low complexity finite impulse response (FIR) filters for channelizers are proposed in this paper. Our methods are based on the binary common subexpression elimination (BCSE) algorithm. The proposed architectures are capable of operating at a high speed clock frequency of 109.7 MHz based on Xilinx's Virtex II 2v2000ff896-6 FPGA for a 12-bit FIR filter coefficient. Design examples show that our method offers an average reduction of 23% in the number of addition operations compared to the conventional FIR filter implementations	12-bit;algorithm;clock rate;coefficient;common subexpression elimination;etsi satellite digital radio;field-programmable gate array;finite impulse response;reconfigurability;reduction (complexity);requirement;sampling (signal processing);virtex (fpga)	Raveendranatha P. Mahesh;A. Prasad Vinod	2006	2006 IEEE 17th International Symposium on Personal, Indoor and Mobile Radio Communications	10.1109/PIMRC.2006.254336	embedded system;real-time computing;computer science;finite impulse response	EDA	31.710149478416387	57.95145928077767	88174
66be1a3bfd2ac36429947f97bbbe413f16cb3ba8	new binary locally repairable codes with locality 2 and uneven availabilities for hot data			locality of reference	Kangseok Lee;Hosung Park;Jong-Seon No	2018	Entropy	10.3390/e20090636	mathematical optimization;mathematics;locality;binary number	AI	36.101420319340235	57.240779915326335	88937
0a99caa09dbc58c6e3fbe571c34343449c927e19	using the loopy belief propagation in siguo	loopy belief propagation	A burst mode architecture to provide burst mode access to a plurality of data words in a flash memory is described. The burst mode architecture includes a first circuit, a control circuit coupled to the first circuit, and a data buffer selectively coupled to the first circuit by the control circuit. The first circuit accesses a plurality of data words, beginning with an initial access of a first data word and a second data word. The control circuit generates a timing signal having pulses and a second signal. The second signal is generated upon completion of the initial access of the first data word and the second data word. The first circuit follows the initial access with subsequent accesses of the plurality of data words responsively to the second signal and the timing signal. The data buffer has an output and produces the first data word at the output and successively produces, with each successive pulse of the timing signal following an initial period of time, the second data word, and subsequent data words at the output. The subsequent data words correspond to the subsequent accesses of the plurality of data words.	belief propagation;casio loopy;software propagation	Zhengyou Xia;Yongping Zhu;Hui Lu	2007	ICGA Journal		algorithm;belief propagation;simulation;burst mode (photography);data buffer;word (computer architecture);computer science;pulse (signal processing);flash memory	ML	31.91295917964768	51.68323177777597	89688
f91d7c3ab2c8e7fcc73034b9fe945e879f55a4d7	combined qualitative-quantitative steady-state diagnosis of continuous-valued systems	measurement sequence processing;explanation;steady state fault diagnosis systems engineering and theory fault detection predictive models nonlinear equations information analysis artificial intelligence switches computer science;value system;explanation diagnostic reasoning;empirical analysis;system modeling;analytic constraint equation system models;ordinal information;systems engineering and theory;engineering system;qualitative quantitative steady state diagnosis;partial explanation model;complex engineering systems;fault detection;artificial intelligence;nonlinear equations;predictive models;incremental algorithm;computer science;diagnostic reasoning;measurement sequence processing qualitative quantitative steady state diagnosis continuous valued systems complex engineering systems analytic constraint equation system models partial explanation model ordinal information;switches;continuous valued systems;information analysis;fault diagnosis;steady state;qualitative and quantitative analysis	This paper discusses systematic methods for diagnosis of complex engineering systems combining qualitative and quantitative analysis of analytic constraint equation system models to generate more precise and accurate candidates. Candidates generated from a qualitative steady-state partial explanation model are refined with available ordinal information on the magnitude of component parameter deviations. In addition, an incremental algorithm is implemented to efficiently process sequences of measurements. Empirical analysis demonstrates that accuracy and resolution of minimal candidate generation are improved by including ordinal information. This avoids the practical problems encountered when reasoning with pure quantitative information.	algorithm;mit engineering systems division;ordinal data;steady state	Gautam Biswas;Ravi Kapadia;Xudong Yu	1997	IEEE Trans. Systems, Man, and Cybernetics, Part A	10.1109/3468.554680	mathematical optimization;systems modeling;nonlinear system;network switch;computer science;artificial intelligence;machine learning;control theory;mathematics;predictive modelling;value system;data analysis;steady state;fault detection and isolation;algorithm;statistics	AI	24.78589307682469	48.8237491239802	90333
cad759d09ecc5e757124cb18c0c546d3fb5d314e	optimal configurations for peer-to-peer user-private information retrieval	ramanujan graphs;systemvetenskap informationssystem och informatik;information systems;combinatoric configurations;private information retrieval;discrete mathematics;expander graphs;p2p;projective plane;expander graph;finite projective plane;datavetenskap datalogi;diskret matematik;computer science;peer to peer;ramanujan graph	User-private information retrieval systems should protect the user’s anonymity when performing queries against a database, or they should limit the servers capacity of profiling users. Peer-to-peer user-private information retrieval (P2P UPIR) supplies a practical solution: the users in a group help each other in doing their queries, thereby preserving their privacy without any need of the database to cooperate. One way to implement the P2P UPIR uses combinatoric configurations to administrate the keys needed for the private communication between the peers. This article is devoted to the choice of the configuration in this system. First of all we characterize the optimal configurations for the P2P UPIR and see the relationship with the projective planes as described in finite geometry. Then we give a very efficient construction of such optimal configurations, i.e. finite projective planes. We finally check that the involved graphs are Ramanujan graphs, giving an additional justification of the optimality of the constructed configurations. © 2010 Elsevier Ltd. All rights reserved.	algorithm;peer-to-peer;personally identifiable information;private information retrieval;ramanujan graph;samsung sgr-a1	Klara Stokes;Maria Bras-Amorós	2010	Computers & Mathematics with Applications	10.1016/j.camwa.2010.01.003	mathematical optimization;combinatorics;mathematical analysis;discrete mathematics;expander graph;theoretical computer science;mathematics;algebra	DB	35.65908173865451	50.751090929508365	90428
edd3a1871e0f709011c7687f351981572ac305fc	a continuous-time hexagonal field-programmable analog array in 0.13μm cmos with 186mhz gbw	digital circuit;continuous time;cmos integrated circuits;integrated circuit design cmos integrated circuits field programmable analogue arrays;frequency 186 mhz field programmable analog array continuous time hexagonal fpga integrated circuit design rapid prototyping digital circuit analog circuit size 0 13 mum;building block;field programmable analog arrays filters parasitic capacitance tunable circuits and devices signal design switches bandwidth semiconductor device measurement transconductors distortion measurement;field programmable analogue arrays;development tool;analog circuits;integrated circuit design;analog circuit;frequency 186 mhz;rapid prototyping;field programmable analog array;continuous time hexagonal fpga;digital circuits;size 0 13 mum;non recurring engineering	The high non-recurring engineering costs of integrated circuit design make rapid-prototyping an important field of microelectronics. For digital circuits, FPGAs are highly developed and have become indispensable as both development tools and application platforms. In contrast, the nature of analog circuits is much more complex and does not readily allow a mapping of any arbitrary analog behavior to a generic set of basic building-blocks and routing nodes. Therefore, field-programmable analog arrays (FPAAs) are very specific to the target application and have poor performance when compared to single-purpose ASICs.	analogue electronics;application-specific integrated circuit;cmos;digital electronics;field-programmability;field-programmable analog array;field-programmable gate array;integrated circuit design;programming tool;rapid prototyping;routing	Joachim Becker;Fabian Henrici;Stanis Trendelenburg;Maurits Ortmanns;Yiannos Manoli	2008	2008 IEEE International Solid-State Circuits Conference - Digest of Technical Papers	10.1109/ISSCC.2008.4523061	mixed-signal integrated circuit;embedded system;electronic engineering;analogue electronics;computer science;engineering;electrical engineering;digital electronics	EDA	26.976481663656838	51.500198153728995	90948
0deb553b17aa308b2189752fe6081e10993cfc75	a coarse-grained array based baseband processor for 100mbps+ software defined radio	wireless terminals;mimo ofdm;software defined radio;bridging fault simulation;software radio;power 310 mw;low power;resistive bridging faults;leakage power;telecommunication standards;power 25 mw coarse grained array based baseband processor software defined radio sdr wireless terminals communication standards tsmc 90 g process dual vt standard cells flow mimo ofdm frequency 400 mhz power 310 mw;ofdm modulation;communication standards;sdr;cost effectiveness;digital signal processing chips;energy budget;telecommunication standards digital signal processing chips mimo communication ofdm modulation software radio;frequency 400 mhz;coarse grained array based baseband processor;baseband software radio vliw throughput communication standards clocks frequency registers energy consumption process design;coarse grained;high performance;dual vt standard cells flow;mimo communication;tsmc 90 g process;power 25 mw	The Software-Defined Radio (SDR) concept aims to enabling cost-effective multi-mode baseband solutions for wireless terminals. However, the growing complexity of new communication standards applying, e.g., multi-antenna transmission techniques, together with the reduced energy budget, is challenging SDR architectures. Coarse-Grained Array (CGA) processors are strong candidates to undertake both high performance and low power.  The design of a candidate hybrid CGA-SIMD processor for an SDR baseband platform is presented. The processor, designed in TSMC 90G process according to a dual-VT standard-cells flow, achieves a clock frequency of 400MHz in worst case conditions and consumes maximally 310mW active and 25mW leakage power (typical conditions) when delivering up to 25,6GOPS (16-bit). The mapping of a 20MHz 2x2 MIMO-OFDM transmit and receive baseband functionality is detailed as an application case study, achieving 100Mbps+ throughput with an average consumption of 220mW.	16-bit;baseband processor;best, worst and average case;central processing unit;clock rate;etsi satellite digital radio;mimo;mimo-ofdm;simd;spectral leakage;throughput	Bruno Bougard;Bjorn De Sutter;Sebastien Rabou;David Novo;Osman Allam;Steven Dupont;Liesbet Van der Perre	2008	2008 Design, Automation and Test in Europe	10.1145/1403375.1403549	embedded system;electronic engineering;real-time computing;telecommunications;computer science;engineering;electrical engineering;software-defined radio	EDA	31.259225231320126	59.20479663080742	91137
78ebd7719f3527a938180a6f3c2b611bf26ccb75	growing binary trees in a random environment	cambio estado;vertices multi access communication tree extinction conditions binary trees random environment two state markovian environment;two state markovian environment;sorting;random tree;working environment noise;senior members;multi access communication;random variables;vertices;trees mathematics;indexing terms;changement etat;binary trees;trees mathematics information theory markov processes multi access systems random processes;stability;algorithme;algorithm;stochastic processes;arbol binario;multi access systems;arbre binaire;random processes;tree extinction conditions;arbre aleatoire;binary trees multiaccess communication working environment noise senior members sorting stochastic processes communication channels stability terrorism random variables;change of state;markov processes;environnement aleatoire;communication channels;information theory;random environment;terrorism;multiaccess communication;algoritmo;binary tree	We study a class of binary trees that grow in a random environment, where the state of the environment can change at every vertex of the trees. The trees considered are single-type and two-type binary trees that grow in a two-state Markovian environment. For each kind of tree, the conditions on the environment process for extinction of the tree are determined, and the problem of calculating the expected number of vertices of the tree is addressed. Diierent ways of growing the trees are compared.	binary tree	Ilan Kessler;Moshe Sidi	1993	IEEE Trans. Information Theory	10.1109/18.179356	random binary tree;optimal binary search tree;stochastic process;red–black tree;combinatorics;discrete mathematics;tree rotation;binary expression tree;geometry of binary search trees;binary tree;link/cut tree;theoretical computer science;range tree;k-d tree;k-ary tree;mathematics;2–3–4 tree;weight-balanced tree;ternary search tree;tree traversal;metric tree;statistics	Theory	34.32300922477816	50.13113456491729	91196
fdb80b43d4591f5eb29318295e2c5e73171eb72d	a true random number generator based on meta-stable state			hardware random number generator;random number generation	Lingyan Fan;Yongping Long;Jianjun Luo;Liangliang Zhu;Hailuan Liu	2018	IEICE Electronic Express	10.1587/elex.14.20171122	electronic engineering;theoretical computer science;computer science;random number generation	Logic	31.17722093883966	47.12805129583642	94203
81361d9954cd2a78c8515dabab751d29fcc1024d	digital window comparator dft scheme for mixed-signal ics	signal level evaluation;chip;mixed signal asic;window comparator;go nogo test;dft	The possibility of using window comparators for on-chip and potentially also on-line response evaluation of analogue circuits is investigated. No additional analogue test inputs are required. The additional circuitry can be either realised by means of standard digital gates taken from an available library or by full custom designed gates. With only a few gates an observation window can be realized, tailored to the application needs. With this approach, the test overhead can be kept extremely low. Due to the low gate capacitance also the load on the observed nodes is very low. Simulation results for some examples show that 100% of all assumed layout-realistic faults could be detected.	comparator;mixed-signal integrated circuit;window function	Daniela De Venuto;Michael J. Ohletz;Bruno Riccò	2002	J. Electronic Testing	10.1023/A:1014937424827	chip;embedded system;electronic engineering;computer hardware;telecommunications;computer science;engineering;discrete fourier transform	Theory	26.47228231985507	52.56242361294122	94365
3604df9ba5cdb5208f3e3322dd58fe37d62937bf	method of downsizing the 4k uncompressed video signal generator		In this paper, the method of downsizing 4K uncompressed video signal generator is introduced. It is required large data transmission bandwidth to output uncompressed 4K video signal. A SSD is known as a storage device with large data transmission bandwidth. The conventional technology for producing 4K uncompressed video output was redundant arrays of inexpensive disks technology using multiple SSDs. There was a problem that the size of 4K uncompressed video signal generator apparatus became too large by many SSDs. We have figured out the solution to such problem, by means of the NAND flash control method without SSDs.	4k resolution;bandwidth (signal processing);flash memory;raid;solid-state drive;uncompressed video	Kosho Suzuki;Takayuki Tajiri;Nobumichi Takaba;Kiyoshi Okuizumi;Jun Nakamura	2017	2017 IEEE 6th Global Conference on Consumer Electronics (GCCE)	10.1109/GCCE.2017.8229406	field-programmable gate array;signal generator;uncompressed video;data transmission;nand gate;computer science;computer hardware;bandwidth (signal processing)	EDA	33.599253984585836	51.61960360535749	94584
9ad505f4ffdc8c14e25b35e40691b4f62cc14e2e	atsc 3.0 implementation in gnu radio companion		High quality television services entail more complex systems and higher costs of prototyping, development and testing methods. The concept of Software-Defined Radio, which addresses flexibility and adaptability to new systems as they are researched, is an option to study the behavior of some technologies that will be used in future systems. This paper presents an overview of the new Advanced Television Systems Committee (ATSC) system, named ATSC 3.0. Finally, it proposes an implementation of ATSC 3.0 modulator using the GNU Radio Companion (GRC) software and validates the implementation transmitting and receiving the signal with a third party hardware and software.	atsc 3.0;atsc standards;advanced television systems committee;complex systems;etsi satellite digital radio;gnu radio;governance, risk management, and compliance;modulation;open-source software;transmitter;universal software radio peripheral	Victor M. Dionisio;Cristiano Akamine	2017	2017 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)	10.1109/BMSB.2017.7986220	modulation;adaptability;computer science;complex system;embedded system;software;software-defined radio	Arch	29.994123685853634	60.055266125084856	94645
41d0c970fa1dba1743c9fa5347701566236914cb	high capacity embedding methods of qr code error correction		In this paper, two methods about how to embed message into QR code are investigated. According to different application scenarios, two different embedding ways are given. The first proposed embedding way is to modify a continuous region based on the arrangement of codewords in QR code and the mechanism of QR code error correction which can reach the maximum error correction capability as well as scan the QR code altered by a QR code reader. The second embedding way is designed to modify each column separately in coding regions which can be decoded correctly as well. Although the second embedding way couldn’t reach high capacity, it can be applied in many occasions while the first embedding way couldn’t. Based on the proposed two embedding methods and the analysis of the error correction mechanism, we conclude the general rules about how to embed message into QR code. The experiment results show the effectiveness of our methods.	qr code	Song Wan;Yuliang Lu;Xuehu Yan;Wanmeng Ding;Hanlin Liu	2016		10.1007/978-3-319-72998-5_8	theoretical computer science;error detection and correction;computer science;distributed computing;embedding	NLP	35.371103648973694	54.3878966982682	94875
7a8ea05148acb5a824dcc54fa001306076b9b434	optical random number generation - harvesting entropy from noise and chaos	computers;chaos noise entropy random number generation;generators;chaos;optical noise;random number generation;random number generation optical noise entropy chaos computers generators;entropy	Random number generators have become an essential part of our modern life, in ways that average consumers seldom appreciate. Much of modern communication and digital storage relies on cryptography to ensure privacy, security, and authentication. Random number generation is a critical process that underlies all of these methods. The rapidly increasing demand for bandwidth, storage, and computation, combined with the growing specter of cyberthreats ensure that our need for reliable and unpredictable random numbers will only grow in the future. Just as optics has become the dominant mode of digital communication, optical methods are being increasingly applied to the problem of random number generation. In this presentation, we review the current status of optical random number generation, and discuss the broader problem of harvesting and quantifying entropy in systems that combine noise and chaos.	authentication;computation;cryptography;random number generation	Joseph D. Hart;Rajarshi Roy;Thomas E. Murphy	2017	2017 51st Annual Conference on Information Sciences and Systems (CISS)	10.1109/CISS.2017.7926165	entropy;combinatorics;hardware random number generator;random number generation;theoretical computer science;mathematics;statistics	Theory	33.28244145381923	47.713038771957265	96006
2ca2b755b9d50827730643046e2520b117217b69	an adaptive cross-correlation derivative algorithm for ultra-low power time delay measurement	ultra low power;audio signal processing;integrated circuit;cross correlation;time delay;low power;delay effects power measurement time measurement charge coupled devices energy consumption hardware design languages counting circuits analytical models integrated circuit modeling integrated circuit measurements;0 5 micron adaptive cross correlation derivative algorithm ultra low power time delay measurement low power integrated circuit inter aural time difference logic structure cmos process;cmos logic circuits;low power electronics;time of arrival estimation;time of arrival estimation audio signal processing cmos logic circuits low power electronics	In this paper, we report a low power integrated circuit that implements an adaptive version of the cross-correlation derivative algorithm for the estimation of inter-aural time difference. The architecture and logic structure as well as measured results reporting the performance of the IC -fabricated in a standard CMOS 0.5 mum process - are shown.	algorithm;cmos;cross-correlation;integrated circuit	F. N. Martin Pirchio;Pedro Julián;Pablo Sergio Mandolesi;Alfonso Chacon-Rodriguez	2007	2007 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2007.378799	embedded system;electronic engineering;real-time computing;asynchronous circuit;logic gate;audio signal processing;computer science;electrical engineering;operating system;integrated circuit;cross-correlation;integrated injection logic;power optimization;low-power electronics;statistics	Arch	25.424049463846742	54.10534076266688	96929
726c179fbc0e2d1ae9acd5ed8563bb1809809c38	transition density: a new measure of activity in digital circuits	digital circuit;densidad relativa;vlsi circuits digital circuits circuit activity transition density average switching rate circuit reliability average power dissipation electromigration failures hot electron degradation density propagation algorithm prototype density simulator;modelizacion;average switching rate;switching;concepcion circuito;senal estocastica;degradation;fiabilidad;reliability;design process;densite;integrated circuit;transition density;density measurement;relative density;density propagation algorithm;switching circuits;electromigration failures;stochastic signal;circuit design;circuito integrado;arithmetic logic unit;runtime;experimental result;circuit numerique;failure analysis;modelisation;reliability assessment;circuit simulation;virtual prototyping;circuit analysis;density measurement digital circuits switching circuits runtime power dissipation electromigration degradation virtual prototyping circuit simulation circuit analysis;vlsi circuits;prototype density simulator;circuit reliability;digital integrated circuits;fiabilite;power dissipation;vlsi circuit analysis computing circuit reliability digital circuits digital integrated circuits electromigration failure analysis hot carriers switching;conmutacion;circuito numerico;resultado experimental;vlsi;signal stochastique;hot carriers;conception circuit;electromigration;average power dissipation;stochastic model;unite arithmetique logique;digital circuits;resultat experimental;hot electron degradation;circuit analysis computing;modeling;commutation;unidad aritmetica logica;circuit integre;circuit activity	"""Reliability assessment is an important part of the design process of digital integrated circuits. We observe that a common thread that runs through most causes of run-time failure is the extent of circuit activity, i.e., the rate at which its nodes are switching. We propose a new measure of activity, called the transition density, which may be de ned as the \average switching rate"""" at a circuit node. Based on a stochastic model of logic signals, we also present an algorithm to propagate density values from the primary inputs to internal and output nodes. To illustrate the practical signi cance of this work, we demonstrate how the density values at internal nodes can be used to study circuit reliability by estimating (1) the average power & ground currents, (2) the average power dissipation, (3) the susceptibility to electromigration failures, and (4) the extent of hot-electron degradation. The density propagation algorithm has been implemented in a prototype density simulator. Using this, we present experimental results to assess the validity and feasibility of the approach. In order to obtain the same circuit activity information by traditional means, the circuit would need to be simulated for thousands of input transitions. Thus this approach is very e cient and makes possible the analysis of VLSI circuits, which are traditionally too big to simulate for long input sequences. Submitted to the IEEE Transactions on Computer-Aided Design, 1991."""	algorithm;computer-aided design;digital electronics;electromigration;electron;elegant degradation;hot-carrier injection;integrated circuit;markov chain;prototype;simulation;software propagation;very-large-scale integration	Farid N. Najm	1993	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.205010	embedded system;electronic engineering;computer science;engineering;electrical engineering;very-large-scale integration;digital electronics	EDA	25.856192862515645	55.67494593296298	97401
7de98e857bb62c08c9f16ba282f25658cf0fff93	design and implementation of ofdma-based wlan prototype system	hardware description languages;frequency division multiple access;wireless lan analogue digital conversion digital analogue conversion fast fourier transforms field programmable gate arrays frequency division multiple access hardware description languages ofdm modulation radio transceivers;analogue digital conversion;ofdm modulation;fast fourier transforms;nuht ofdm fpga synchronization wlan;wireless lan;digital analogue conversion;nuht 1 0 phy standard ofdma based wlan prototype system ofdma based wireless lan prototype system fpga technology verilog hdl hardware description language hardware platform base band processor digital to analog converter dac analog to digital converter adapter board adc adapter board rf front end hardware modules ofdma transceiver training sequences pilot inserting cyclic prefix adding cp adding ifft fft synchronization channel equalization virtex 5 fpga development boards dual channel high performance adc licensed its band next ultra high throughput wlan;field programmable gate arrays;radio transceivers	This paper presents design and implementation results of an OFDMA-based Wireless LAN (WLAN) prototype system using FPGA technology and Verilog HDL hardware description language. The whole hardware platform is composed of base band processor, Digital-to-Analog Converter (DAC)/Analog To Digital Converter (ADC) adapter board and RF front end. The hardware modules of the OFDMA transceiver presented in this paper include training sequences, pilot inserting, Cyclic Prefix (CP) adding, IFFT/FFT, synchronization, and channel equalization. The final implementation consists of two identical Virtex-5 FPGA development boards, offering dual channel high performance ADCs and DACs, required for operation in the licensed ITS band of 5.8GHz. The system is used to test and verify Next Ultra High Throughput WLAN (NUHT) 1.0 PHY standard.	analog-to-digital converter;baseband;digital-to-analog converter;fast fourier transform;field-programmable gate array;hardware description language;multi-channel memory architecture;phy (chip);prototype;radio frequency;throughput;transceiver;verilog;wireless access point	Damin Wu;Nan Bao;Weiwei Xia;Lianfeng Shen	2012	2012 International Conference on Wireless Communications and Signal Processing (WCSP)	10.1109/WCSP.2012.6542868	embedded system;electronic engineering;telecommunications;engineering	EDA	30.81463898829017	58.93033071786382	98497
30e3b439d5ad6ac139292a2777220f7b2e5f2be5	low-power and high-speed cordic-based split-radix fft processor for ofdm systems	ofdm systems;digital audio broadcast;cordic;split radix;fft;digital video broadcast;fast fourier transform;low power;power consumption;asymmetric digital subscriber line;high speed;ultra wide band	This paper presents a CORDIC (Coordinate Rotation Digital Computer)-based split-radix fast Fourier transform (FFT) core for OFDM systems, for example, Ultra Wide Band (UWB), Asymmetric Digital Subscriber Line (ADSL), Digital Audio Broadcasting (DAB), Digital Video Broadcasting - Terrestrial (DVB-T), Very High Bitrate DSL (VHDSL), and Worldwide Interoperability for Microwave Access (WiMAX). The high-speed 128/256/512/1024/2048/4096/8192-point FFT processor has been implemented by 0.18 @mm (1p6m) at 1.8 V, in which all the control signals are generated internally. This programmable FFT processor outperforms the conventional ones in terms of both power consumption and core area.	cordic;fast fourier transform	Tze-Yun Sung;Hsi-Chin Hsin;Yi-Peng Cheng	2010	Digital Signal Processing	10.1016/j.dsp.2009.08.008	embedded system;fast fourier transform;telecommunications;computer science	Arch	31.120367663848114	58.463225903845206	98499
9b4889acb585bfadf1e67864efc53acbae485cb7	quantum reed-muller codes	information theory quantum communication reed muller codes error correction codes;error correction codes;reed muller code;indexing terms;quantum physics;quantum communication;quantum error correcting codes;quantum information theory reed muller codes quantum error correcting codes;quantum error correction;reed muller codes;information theory	A set of quantum error correcting codes based on classical Reed-Muller codes is described. The codes have parameters [[n, k, d]] = [[2r, 2r −C(r, t)− 2 ∑t−1 i=0 C(r, i), 2 t + 2t−1]]. The study of quantum information is currently stimulating much interest. Most of the basic concepts of classical information theory have counterparts in quantum information theory, and among these is the idea of an error correcting code. An error correcting code is a means of storing information (whether quantum or classical) in a set of bits (ie either qubits or classical bits) in such a way that the information can be extracted even after a subset of the bits has changed in an unknown way. Such codes are a fundamental part of the study of classical information channels. The possibility of quantum error correction was only recently discovered [1, 2]. Importantly, it was shown that efficient quantum codes exist for arbitrarily large amounts of quantum information [3, 4]. The word ‘efficient’ refers to the fact that the rate k/n of the code need not fall off as n increases, for a given ratio d/n, where d is the minimum distance of the code. This and other features makes quantum error correction the best prospect for enabling quantum information to be transmitted or stored with a small amount of error, and consequently the best prospect for controlling noise in a quantum information processor. The subject of quantum error correction may be considered to have two distinct parts. The first part is to show how to apply error correction in a physical situation, and the second is to find good quantum error correcting codes. This paper is concerned with the second part, that of finding codes. Following [5], I will use the notation [[n, k, d]] to refer to a quantum error correcting code for n qubits having 2 codewords and minimum distance d (previously	code word;error detection and correction;forward error correction;information processor;information theory;physical information;quantum error correction;quantum information;qubit;reed–muller code;reed–solomon error correction	Andrew M. Steane	1999	IEEE Trans. Information Theory	10.1109/18.771249	arithmetic;reed–muller code;concatenated error correction code;low-density parity-check code;information theory;theoretical computer science;linear code;luby transform code;mathematics;quantum convolutional code;raptor code;reed–solomon error correction;algorithm;statistics	Theory	38.9791621860694	57.46598251379738	98922
0e2bcf6019636d6b17939c51d7efcaefb1d81e4d	error-correcting codes for automatic control	automatic control;error correction codes error correction;control theory;error correction codes;on line estimation;real time;error correction codes automatic control communication system control base stations application software computer science mathematics polynomials engines space exploration;indexing terms;error correction code;error correction;shannon coding theorem automatic control control theory applications infinite state graph polynomially growing expansion channel noise constructive error correcting code online error correcting code online estimation online control control applications	Systems with automatic feedback control may consist of several remote devices, connected only by unreliable communication channels. It is necessary in these conditions to have a method for accurate, real-time state estimation in the presence of channel noise. This problem is addressed, for the case of polynomial-growth-rate state spaces, through a new type of error-correcting code that is online and computationally efficient. This solution establishes a constructive analog, for some applications in estimation and control, of the Shannon coding theorem.	algorithmic efficiency;automatic control;error detection and correction;feedback;forward error correction;noise (electronics);polynomial;real-time clock;shannon coding;shannon–hartley theorem	Rafail Ostrovsky;Yuval Rabani;Leonard J. Schulman	2005	IEEE Transactions on Information Theory	10.1109/SFCS.2005.33	fx.25 forward error correction;concatenated error correction code;turbo code;constant-weight code;low-density parity-check code;error detection and correction;computer science;theoretical computer science;cyclic code;automatic control;coding gain;linear code;mathematics;forward error correction;error floor;error exponent;algorithm	Theory	38.35382660238096	60.250125436697154	99194
713f29175742d507888d850d4ad1c80e522c8778	digital radar signal simulator design for combat management system integration into warships	computers;computational modeling	This paper presents an approach on development of digital radar signal simulator based on low cost 16-bit RISC based microcontroller. Typical, mostly used, radar output signals are given and then signal-video relationships are explained in addition to requirements of tactical display calibration with respect to stages of combat management system deployment. Later, generation of radar output signal methodology is given in detail in terms of utilization of hardware accelerators and peripherals within the microcontroller. Furthermore, the usage of signals and generated discrete radar signals are given as well as digital radar video which simulates radar video returns from target or terrain. Finally, practical results of testing and calibrating of tactical display with such a solution are discussed.	16-bit;hardware acceleration;management system;microcontroller;peripheral;radar;requirement;software deployment;system deployment;system integration	Serkan Turan;Sarp Ertürk	2016	2016 UKSim-AMSS 18th International Conference on Computer Modelling and Simulation (UKSim)	10.1109/UKSim.2016.29	embedded system;man-portable radar;electronic engineering;radar engineering details;simulation;radar lock-on;engineering;digital radio frequency memory	EDA	29.37886924078308	59.71614223561596	99550
739abda477e1550900d41004a4820d169c67353e	matroid bounds on the region of entropic vectors	network coding entropic vectors matroids polymatroids;vectors random variables network coding complexity theory entropy manganese encoding;matrix algebra;matroid bounds multiple source multicast network coding capacity regions multiterminal information theory substantial reduction binary representable matroid rank ternary representable matroid rank quaternary representable matroid rank extremal polymatroids shannon outer bound integer valued vector entropic vector region;vectors;computational complexity;entropy;vectors combinatorial mathematics computational complexity entropy matrix algebra;combinatorial mathematics	Several properties of the inner bound on the region of entropic vectors obtained from representable matroids are derived. In particular, it is shown that: I) It suffices to check size 2 minors of an integer-valued vector to determine if it is a valid matroid rank; II) the subset of the extreme rays of the Shannon outer bound (the extremal polymatroids) that are matroidal are also the extreme rays of the cone of matroids; III) All matroid ranks are convex independent; and IV) the extreme rays of the conic hull of the binary/ternary/quaternary representable matroid ranks inner bound are a subset of the extreme rays of the conic hull of matroid ranks. These properties are shown to allow for substantial reduction in the complexity of calculating important rate regions in multiterminal information theory, including multiple source multicast network coding capacity regions.	code;computation;entropic vector;information theory;linear network coding;matroid rank;matroid representation;multicast;multiseat configuration;shannon (unit)	Congduan Li;John MacLaren Walsh;Steven Weber	2013	2013 51st Annual Allerton Conference on Communication, Control, and Computing (Allerton)	10.1109/Allerton.2013.6736606	matroid;mathematical optimization;combinatorics;discrete mathematics;graphic matroid;oriented matroid;mathematics;weighted matroid;matroid partitioning	Theory	35.88234946939643	58.00011586878686	101012
3f83d49da0d78ba2cc20dfc70314eb7d288656ba	secret sharing schemes from binary linear codes	melas codes;bch codes;error correction code;linear code;cyclic code;bch code;secret sharing scheme;irreducible cyclic codes;access structures	In principle, every linear code can be used to construct a secret sharing scheme. However, in general, determining the access structure of the scheme is very hard. On the other hand, finding error correcting codes that produce secret sharing schemes with efficient access structures is also difficult. In this paper, we study a set of minimal codewords for certain classes of binary linear codes, and then determine the access structure of secret sharing schemes based on these codes. Furthermore, we prove that the secret sharing schemes obtained are democratic in the sense that every participant is involved in the same number of minimal access sets.	code;secret sharing	Zhihui Li;Ting Xue;Hong Lai	2010	Inf. Sci.	10.1016/j.ins.2010.07.029	block code;reed–muller code;polynomial code;concatenated error correction code;turbo code;combinatorics;discrete mathematics;low-density parity-check code;theoretical computer science;cyclic code;shamir's secret sharing;linear code;mathematics;homomorphic secret sharing;secure multi-party computation;bch code;raptor code;code;group code;reed–solomon error correction;statistics	Crypto	38.33299788684194	54.82338413916526	101126
d085fc536e72e2bc42a059284e7060338251021b	optimal source codes for geometrically distributed integer alphabets (corresp.)	source coding huffman codes;geometric distribution;huffman codes;source code;source coding	Let P(i)= (1 - \theta)\theta^i be a probability assignment on the set of nonnegative integers where \theta is an arbitrary real number, 0  . We show that an optimal binary source code for this probability assignment is constructed as follows. Let l be the integer satisfying \theta^l + \theta^{l+1} \leq 1  and represent each nonnegative integer i as i = lj + r when j = \lfloor i/l \rfloor , the integer part of i/l , and r = [i] mod l . Encode j by a unary code (i.e., j zeros followed by a single one), and encode r by a Huffman code, using codewords of length \lfloor \log_2 l \rfloor , for r  , and length \lfloor \log_2 l \rfloor + 1 otherwise. An optimal code for the nonnegative integers is the concatenation of those two codes.	code	Robert G. Gallager;David C. van Voorhis	1975	IEEE Trans. Information Theory	10.1109/TIT.1975.1055357	combinatorics;discrete mathematics;computer science;theoretical computer science;mathematics;statistics;source code	Theory	38.850137479949304	53.882983945814814	101433
f9d848c14386480fc3114ee134eade2f57e03f6d	cad methodology for analog static cmos design automation	second order;computer aided design;design automation;supply voltage cad methodology analog static cmos design automation computer aided design ekv model;cad;ekv model;analog static cmos design automation;integrated circuit design;accuracy;cmos analogue integrated circuits;transistors;solid modeling;supply voltage;integrated circuit modeling;mathematical model;integrated circuit design cad cmos analogue integrated circuits;cad methodology;integrated circuits;design automation voltage equations space technology mosfets integrated circuit modeling cmos technology semiconductor device modeling constraint optimization design optimization	The computer-aided design (CAD) methodology proposed in this paper, automates analog static CMOS design. This methodology is based on the EKV model which is continuous over the inversion range. This methodology provides accurate description of current drain (error < 10%) with integration of second order effects in charts for simplicity. It explores the whole solution space. Thus, circuits are sized without inversion level constraint and can be optimized unambiguously for given design requirements and given technology. The methodology is illustrated on a classic self-biased compact current reference. The circuit is optimized in supply voltage. The simulation in 0.15 mum technology gives a minimum supply voltage of 800 mV for a current target of 50 nA with 10% accuracy.	automation;cmos;chart;feasible region;mathematical software;requirement;simulation	Francois Rudolff;Edith Kussener;Gaëtan Bracmard	2007	2007 14th IEEE International Conference on Electronics, Circuits and Systems	10.1109/ICECS.2007.4511132	control engineering;electronic engineering;engineering;computer engineering	EDA	27.124963453398667	49.715772955916655	101681
6fe00ce8e9773c4d324057dff5ec233ee783ce54	on the construction of prefix-free and fix-free codes with specified codeword compositions	approximate algorithm;prefix free code;approximation algorithm;polynomial time algorithm;algorithm;fix free code;information theory;uniform distribution	We investigate the construction of prefix-free and fix-free codes with specified codeword compositions. We present a polynomial time algorithm which constructs a fix-free code with the same codeword compositions as a given code for a special class of codes called distinct codes. We consider the construction of optimal fix-free codes which minimize the average codeword cost for general letter costs with uniform distribution of the codewords and present an approximation algorithm to find a near optimal fix-free code with a given constant cost.	approximation algorithm;code word;p (complexity);prefix code	Ali Kakhbod;Morteza Zadimoghaddam	2011	Discrete Applied Mathematics	10.1016/j.dam.2011.08.003	block code;reed–muller code;polynomial code;concatenated error correction code;prefix code;turbo code;combinatorics;discrete mathematics;constant-weight code;kraft's inequality;information theory;parity-check matrix;theoretical computer science;covering code;cyclic code;locally decodable code;linear code;mathematics;uniform distribution;reed–solomon error correction;approximation algorithm	Theory	38.92560675494006	54.17452792630132	101747
f1735816b526fcd534a2405cd8171649f1f3f034	an energy-efficient true random number generator based on current starved ring oscillators		True random number generators (TRNGs) are pivotal in cryptography, Markov Chain Monte Carlo analysis, neural network simulation, industrial testing, gambling, etc., where deterministic pseudo-random number sequences are inadequate to produce satisfactory results. The demand for fast low-power TRNG is growing as such sophisticated applications are increasingly moving into mobile. This paper presents an energy-efficient on-chip TRNG design. Its random digital bits are extracted from the jitter noise of two free running current starved ring oscillators (ROs). The current starved ROs exhibit a larger jitter noise than the regular inverter based ROs because the jitter is boosted by lowering the oscillation frequency and the drain current of the transistors in the ROs. In addition, the jitter source ROs, which are the most power-hungry components of conventional oscillator based TRNGs, are biased in the subthreshold region in our proposed design to reduce their power consumption. Simulation results based on 65nm 1.2V CMOS technology show that the proposed TRNG consumes only 123 μW at a throughput rate of 96 Mbps. It outperforms the state-of-art on-chip TRNGs with a figure-of-merit of 1.28 pJ/bit. Its generated bit sequence passes all the fifteen randomness tests of the National Institute of Standards and Technology (NIST) statistical test suite.	artificial neural network;authentication protocol;bitstream;cmos;cryptographic nonce;cryptography;data rate units;digital forensics framework (dff);electronic oscillator;hardware random number generator;low-power broadcasting;markov chain monte carlo;monte carlo method;power inverter;program counter;pseudorandomness;quantization (signal processing);random number generation;randomness tests;simulation;test suite;throughput;tier 1 network;transistor	Yuan Cao;Chip Hong Chang;Yue Zheng;Xiaojin Zhao	2017	2017 Asian Hardware Oriented Security and Trust Symposium (AsianHOST)	10.1109/AsianHOST.2017.8353992	electronic engineering;computer science;nist;random number generation;throughput (business);subthreshold conduction;jitter;randomness tests;logic gate;cmos	Arch	24.83532534017413	60.12345829048958	101749
0e6c5f313ddf94367ffd08bc864cfed229a9fe86	probabilistic analysis of linear programming decoding	multicast scheduling;tanner graph probabilistic analysis linear programming decoding low density parity check codes ldpc codes random code ensemble probabilistic bit flipping channels;probabilidad error;desciframiento;analytical models;finite element methods;graph theory;metodo caso peor;random graph;canal binaire;channel coding;object recognition;random code ensemble;randomized algorithms;resource augmentation;graphe biparti;error correcting code;error correction codes;technological innovation;history;iterative decoding;decodage;decoding;code controle parite;correlation based;factor graph;low density parity check code;grafo bipartido;parity check codes;codigo corrector error;probabilistic bit flipping channels;routing;color;linear programming decoding;metodo combinatorio;grafo aleatorio;graphe de tanner;discrete mathematics;prediction algorithms;code aleatoire;methode combinatoire;graphe aleatoire;low density parity check ldpc codes;probabilistic approach;operations research;toxicology;maximum likelihood estimation;materials;sufficient conditions;inversion de bit;codage canal;sum product algorithm binary symmetric channel bsc channel coding error control coding expanders factor graphs linear programming decoding low density parity check ldpc codes randomized algorithms;polynomials;books;algorithme;upper bound;low density parity check codes;iterative methods;algorithm;binary symmetric channel bsc;canal binario;programacion lineal;vectors;channel capacity;probabilistic analysis;enfoque probabilista;approche probabiliste;error correction;maximum likelihood decoding;linear code;binary sequences;error control coding;ldpc code;non clairvoyant scheduling;methode cas pire;linear programming;randomized algorithm;joining processes;expanders;random codes;programmation lineaire;hypercubes;linear program;binary symmetric channel;combinatorial method;optimization	"""We initiate the probabilistic analysis of linear programming (LP) decoding of low-density parity-check (LDPC) codes. Specifically, we show that for a random LDPC code ensemble, the linear programming decoder of Feld-man et al. succeeds in correcting a constant fraction of errors with high probability. The fraction of correctable errors guaranteed by our analysis surpasses all prior non-asymptotic results for LDPC codes, and in particular exceeds the best previous finite-length result on LP decoding by a factor greater than ten. This improvement stems in part from our analysis of probabilistic bit-flipping channels, as opposed to adversarial channels. At the core of our analysis is a novel combinatorial characterization of LP decoding success, based on the notion of a generalized matching. An interesting by-product of our analysis is to establish the existence of """"almost expansion"""" in random bipartite graphs, in which one requires only that almost every (as opposed to every) set of a certain size expands, with expansion coefficients much larger than the classical case."""	coefficient;linear programming decoding;low-density parity-check code;probabilistic analysis of algorithms;with high probability	Constantinos Daskalakis;Alexandros G. Dimakis;Richard M. Karp;Martin J. Wainwright	2007	IEEE Transactions on Information Theory	10.1109/TIT.2008.926452	combinatorics;low-density parity-check code;linear programming;theoretical computer science;factor graph;mathematics;randomized algorithm;algorithm;statistics	Theory	39.15971640930486	57.429942587098495	102209
27b3be5cd89f7dfaa668bc1243a356c6619d5a2b	identification of traitors in algebraic-geometric traceability codes	list decoding;watermarking;algebraic geometric codes;error correction codes;soft decision list decoding algebraic geometric traceability codes fingerprinting scheme error correction codes traitor identification;decoding;fingerprint recognition watermarking data security error correction codes decoding signal processing algorithms filtering algorithms degradation robustness councils;soft decision decoding;traitor tracing;error correction code;minimum distance;fingerprinting;decoding algebraic geometric codes watermarking error correction codes	In a fingerprinting scheme, a distributor places marks in each copy of a digital object. Placing different marks in different copies uniquely identifies the recipient of each copy and therefore allows the tracing of the source of an unauthorized redistribution. A widely used approach to the fingerprinting problem is the use of error-correcting codes with a suitable minimum distance. With this approach, the set of embedded marks in a given copy is precisely a codeword of the error correcting code. The focus of this paper is in the identification of traitors when the error-correcting code is an algebraic-geometric (AG) code. The authors present a tracing algorithm that employs the Guruswami-Sudan soft-decision list decoding algorithm to find all provably identifiable dishonest users.	algorithm;authorization;code word;decision list;embedded system;error detection and correction;fingerprint (computing);forward error correction;goppa code;list decoding;soft-decision decoder;traceability;virtual artifact	Marcel Fernandez;Miguel Soriano	2004	IEEE Transactions on Signal Processing	10.1109/TSP.2004.833858	block code;reed–muller code;list decoding;concatenated error correction code;fingerprint;turbo code;low-density parity-check code;error detection and correction;sequential decoding;digital watermarking;computer science;theoretical computer science;serial concatenated convolutional codes;bcjr algorithm;linear code;mathematics;berlekamp–welch algorithm;error floor;computer security;reed–solomon error correction;algorithm;statistics	Embedded	36.819031144636014	55.67531304152448	102561
2e2ff518028cd41bfafcd49c8dddf3f5370da51d	generalized-and efficient techniques for the design of cmi and other encoders	logic arrays;transition density;optical fibre communication;state machine;clocks timing optical receivers logic arrays circuits national electric code high speed optical techniques optical feedback wideband delay;fiber optic;efficient implementation;codes;timing codes optical fibre communication logic arrays digital circuits;digital circuits;bnzs codes cmi encoders coded mark inversion line code wideband fiber optic systems dc balance guaranteed transition density timing recovery digital state machines gate arrays discrete logic components manchester codes rz codes;timing	The coded mark inversion (CMI) line code is becoming popular for wide-band fiber-optic systems. The advantages of the CMI include DC balance and guaranteed transition density, which simplifies timing recovery. Previous CMI encoder implementations typically relied on a mixture of digital, analog, and delay-line techniques. The encoders proposed in this paper are digital state machines that allow elegant, accurate, and efficent implementation with gate arrays or discrete logic components. A general approach is then discussed for designing state machines of the type described here. Such state machines may be used to implement other codes, such as Manchester, RZ, and BnZS codes.	analog delay line;carrier recovery;computer memories inc.;encoder;line code;logic gate;markov chain;optical fiber	Steven S. Gorshe	1997	IEEE Trans. Communications	10.1109/26.592609	electronic engineering;real-time computing;telecommunications;computer science;optical fiber;finite-state machine;code;digital electronics	EDA	29.971244558105433	54.179581107273705	102859
9d7f755a46d6eef1313fce249dcdf2cbb78f18d5	locally repairable and locally regenerating codes obtained by parity-splitting of hashtag codes		We construct an explicit family of locally repairable and locally regenerating codes whose existence was proven i n a recent work by Kamath et al. about codes with local regenerat ion (but no explicit construction was given). This explicit family of codes is based on HashTag codes. HashTag codes are recently defined vector codes with different vector lengthα (also called a sub-packetization level) that achieve the optimal repairbandwidth of MSR codes or near-optimal repair bandwidth depending on the sub-packetization level. We applied the technique of paritysplitting code construction. Additionally, we show that the lower bound on the size of the finite field where these codes are constructed, given in the same work of Kamath et al., can be lower. Finally, we discuss the importance of having two ways for node repair with locally regenerating HashTag codes: repair only with local parity nodes or repair with both local and global parity nodes. To the best of the authors’ knowledge, this is t he first work where this is discussed. Namely, we give a practica l example where several optimization metrics such as the repa ir bandwidth, the number of I/O operations, the access time for the contacted parts and the size of the stored file determine t he repair procedure.	access time;code (cryptography);hashtag;input/output;mathematical optimization	Danilo Gligoroski;Katina Kralevska;Rune Erlend Jensen;Per Simonsen	2017	CoRR		block code;reed–muller code;discrete mathematics;online codes;linear code;mathematics;algorithm	Theory	36.1608640691875	58.25574194988791	103391
1f5890603a1015cdd5e7e0d230ce85f9b4e911f0	design of computing circuits using spatially localized dna majority logic gates		DNA is considered as a good computing device because of the predictability of the double helical structure and the Watson-Crick binding thermodynamics associated with them. DNA circuits can be considered as a possible replacement of silicon transistor based circuits, in implantable medical devices, bio-nanorobots, SMART drugs etc. In this paper, we are proposing a novel five input majority logic gate using DNA strands. A spatially localized architecture is used for implementing the majority logic in an Origami substrate. The localization of the strands, give the advantage of re-using the same strands repeatedly in the circuit without causing any spurious reactions. The proposed majority gate is tested using Visual DSD software. The addition of a five input majority gate into the set of DNA logic gates family will give more flexibility for the designer while developing the complex digital computing circuits. We also implemented a full adder circuit using majority gates. A general condition for obtaining n-input majority gate is also discussed.	apl;adder (electronics);british informatics olympiad;computer;dna barcoding;diagram;document structure description;history of the transistor;image scaling;logic gate;majority function;nanorobotics;programming language;smart;simulation;thomas j. watson research center	Aby K. George;Harpreet Singh	2017	2017 IEEE International Conference on Rebooting Computing (ICRC)	10.1109/ICRC.2017.8123683	dna;architecture;software;predictability;electronic circuit;transistor;logic gate;computer science;adder;electronic engineering	EDA	28.092066494110423	46.59665704335759	103904
50c99a55bf8fda5c01e15287ef53f69944813cb6	rapid prototyping hardware platforms for the development and testing of ofdm based communication systems	power amplifier nonlinearity;digital signal processing;field programmable gate array;rapid prototyping hardware platform;digital audio broadcast;radiofrequency amplifiers;wireless local area network;communication system;software prototyping;signal design;prototypes;power amplifier;automatic digital signal processing;fpga;program verification;compensation algorithm;prototypes hardware system testing ofdm transceivers digital signal processing software prototyping signal processing algorithms signal design algorithm design and analysis;algorithm verification;software radio;power amplifiers;rapid prototyping;design method;ofdm based communication system;hardware software partitioning;phase testing;ofdm modulation;ofdm;compensation algorithm rapid prototyping hardware platform ofdm based communication system digital transceiver phase testing rf stage power amplifier nonlinearity software radio hardware software partitioning automatic digital signal processing dsp processor coding algorithm verification field programmable gate array fpga;system testing;digital signal processing chips;transceivers;dsp processor coding;phase coding;telecommunication signalling;field programmable gate arrays;signal processing algorithms;rf stage;wireless systems;system simulation;telecommunication signalling ofdm modulation software radio transceivers radiofrequency amplifiers power amplifiers digital signal processing chips phase coding program verification field programmable gate arrays;algorithm design and analysis;hardware;digital transceiver;orthogonal frequency division multiplex	Implementation of modern digital transceivers requires an expertise in numerous fields. Conventional transceiver design methods are no longer sufficient to guarantee a fast conversion from initial concept to final product. Moreover, in the testing phase, system simulations alone cannot provide the full insight into the system parameters and performance, especially at the RF stages, where the modeling of power amplifier non-linearities is a highly complex task. To address these design gaps, this paper utilizes software radio solutions. Specifically, it elaborates on transceiver architectural methods at the baseband involving hardware/software partitioning, as well as automatic digital signal processing (DSP) coding strategies that allow for rapid prototyping, testing and verification of algorithms developed in the design simulation stages. In particular, DSP processor and field programmable gate array (FPGA)-based testbeds are described that offer different advantages in the transceiver rapid prototyping methodology. These testbeds were designed to eventually be used in experiments geared towards demonstrating the effectiveness of compensation algorithms for wireless systems like wireless local area network (WLAN) and digital audio broadcasting (DAB), where orthogonal frequency division multiplexed (OFDM) signaling is deployed.	algorithm;audio power amplifier;baseband;digital signal processing;experiment;field-programmable gate array;frequency divider;high-level programming language;multiplexing;radio frequency;rapid prototyping;simulation;software prototyping;testbed;transceiver	Craig Jamieson;Scott Melvin;Jacek Ilow	2005	3rd Annual Communication Networks and Services Research Conference (CNSR'05)	10.1109/CNSR.2005.51	embedded system;real-time computing;telecommunications;computer science;field-programmable gate array	EDA	30.190148503295333	59.63535515294452	104141
3d38db8bafb8f91dbe11dafddbf2ab9468abf7c4	on-line current testing for a microprocessor based application with an off-chip sensor	automatic control;microprocessors;test mode;sensor phenomena and characterization;performance evaluation;embedded systems integrated circuit testing microprocessor chips cmos digital integrated circuits integrated circuit measurement;microprocessors circuit testing voltage production current measurement sensor phenomena and characterization counting circuits automatic control performance evaluation monitoring;sensor dedicated pin;p1149 1 driven test on line current testing microprocessor based application test activation modes iddq measurement intel 386 ex embedded microprocessor off chip current sensor direct test mode sensor dedicated pin test mode;intel 386 ex;chip;embedded systems;counting circuits;current measurement;direct test mode;monitoring;cmos digital integrated circuits;voltage;embedded microprocessor;on line current testing;integrated circuit testing;iddq measurement;production;p1149 1 driven test;circuit testing;off chip current sensor;microprocessor based application;test activation modes;integrated circuit measurement;microprocessor chips	This work presents a prototype architecture that provides on-line IDDQ measurement for a microprocessor-based system. It has been implemented using an IDDQ testable microprocessor (the Intel 386/sup TM/ EX embedded microprocessor) and an off-chip current sensor. Three current test activation modes are supported. A direct test mode through a sensor dedicated pin, a test mode where the microprocessor controls the off-chip sensor, and a P1149.1 driven test. Measurements and architecture operation are detailed.	microprocessor	Bartomeu Alorda;Ivan de Paúl;Jaume Segura;Tess Miller	2000		10.1109/OLT.2000.856617	chip;embedded system;electronic engineering;voltage;computer hardware;telecommunications;computer science;engineering;automatic control;iddq testing	EDA	24.71991978394667	53.62423309950766	104938
45882694821cc5ef74d608ec453d2f922d6f3aa3	optimal covering codes for finding near-collisions	optimal solution;digital expansions;cryptographic hash function;general methods;hash functions;coding theory;memoryless near collisions;direct sum construction;hash function;covering codes	Recently, a new generic method to find near-collisions for cryptographic hash functions in a memoryless way has been proposed. This method is based on classical cycle-finding techniques and covering codes. This paper contributes to the coding theory aspect of this method by giving the optimal solution to a problem which arises when constructing a suitable code as the direct sum of Hamming and trivial codes.	algorithm;bcrypt;coding theory;covering code;cryptographic hash function;cryptography;ecrypt;hamming code;mathematical optimization;microtransaction;optimization problem	Mario Lamberger;Vincent Rijmen	2010		10.1007/978-3-642-19574-7_13	block code;security of cryptographic hash functions;double hashing;combinatorics;discrete mathematics;hash function;perfect hash function;collision resistance;theoretical computer science;hash chain;hash-based message authentication code;mathematics;rolling hash;cryptographic hash function;fowler–noll–vo hash function;mdc-2;swifft;hash tree;hash filter	Theory	37.96912121278951	54.415743109191716	105368
352897149d5d3540b92e77d5db7ff52f6a31f6aa	secret sharing schemes based on linear codes can be precisely characterized by the relative generalized hamming weight		This paper precisely characterizes secret sharing schemes based on arbitrary linear codes by using the relative dimension/length profile (RDLP) and the relative generalized Hamming weight (RGHW). We first describe the equivocation Δm of the secret vector s = [s1, . . . ,sl ] given m shares in terms of the RDLP of linear codes. We also characterize two thresholds t1 and t2 in the secret sharing schemes by the RGHW of linear codes. One shows that any set of at most t1 shares leaks no information about s, and the other shows that any set of at least t2 shares uniquely determines s. It is clarified that both characterizations for t1 and t2 are better than Chen et al.’s ones derived by the regular minimum Hamming weight. Moreover, this paper characterizes the strong security in secret sharing schemes based on linear codes, by generalizing the definition of stronglysecure threshold ramp schemes. We define a secret sharing scheme achieving the α-strong security as the one such that the mutual information between any r elements of (s1, . . . ,sl) and any α−r+1 shares is always zero. Then, it is clarified that secret sharing schemes based on linear codes can always achieve the α-strong security where the value α is precisely characterized by the RGHW. key words: secret sharing scheme, linear code, relative generalized Hamming weight, relative dimension/length profile	entity–relationship model;hamming code;hamming distance;hamming weight;linear code;minimum weight;mutual information;ramp simulation software for modelling reliability, availability and maintainability;secret sharing;window function	Jun Kurihara;Tomohiko Uyematsu;Ryutaroh Matsumoto	2012	IEICE Transactions		combinatorics;discrete mathematics;theoretical computer science;linear code;hamming code;mathematics;statistics	Crypto	39.07500127653817	55.21410394557693	106025
abf4c99df020a86db146a516f8fbb812566980a2	developing a domain model for relay circuits	domain model		domain model;relay	Anne Elisabeth Haxthausen	2009	Int. J. Software and Informatics		computer science;theoretical computer science;domain model;a symbolic analysis of relay and switching circuits;electronic circuit;relay	SE	30.154734107489517	50.855129132829234	106109
0179b3d394a45c827747ea574d45450099ba655e	a mixed tcad/electrical simulation laboratory to open up the microelectronics teaching	hardware design languages;design automation;microelectronics engineer curriculum;semiconductor process modeling;microelectronics teaching;physics;doping;integrated circuit design;digital ic design;computational modeling;cad tools laboratory;partial differential equations;transistors;solid modeling;integrated circuit modeling;technology cad electronics;electronic engineering education;analog ic design;spice tcad electrical simulation laboratory microelectronics teaching microelectronics engineer curriculum device physics analog ic design digital ic design cad tools laboratory;technology cad electronics electronic engineering education integrated circuit design spice;microelectronics;spice;computer simulation;device physics;tcad;laboratories microelectronics education computational modeling design automation hardware design languages physics spice computer simulation partial differential equations;electrical simulation laboratory	In classical microelectronics engineer curriculum, students have to acquire skills in device physics as well as in analog and digital IC design. As a rule, each course is illustrated in a dedicated CAD tools laboratory. Over time, this method leads to a compartmentalizing of the teaching by putting conceptual barrier in the student's mind. This article proposes a laboratory in which students can mix TCAD electrical and SPICE simulation. With this tutorial, the instructor is able to highlight the possible interdependence between these tools but overall he demonstrates students that associated teachings are linked and share the same objectives.	computer-aided design;integrated circuit design;interdependence;mind;spice;simulation;traffic collision avoidance system	Jean Marc Gallière;Jerome Boch	2009	2009 IEEE International Conference on Microelectronic Systems Education	10.1109/MSE.2009.5270836	computer simulation;electronic engineering;electronic design automation;engineering;electrical engineering;solid modeling;doping;computational model;microelectronics;partial differential equation;transistor;computer engineering;integrated circuit design	EDA	27.912674984321903	52.99109551575145	106295
1d16bc283b0cc6f628917f7a1aa046b396271db9	new 2-d eye-opening monitor for gb/s serial links	decision feedback equalization dfe;intersymbol interference cmos circuits and systems decision feedback equalization dfe eye opening monitor eom gb s serial links;intersymbol interference;microprocessor chips cmos integrated circuits jitter;gb s serial links;eye opening monitor eom;cmos circuits and systems;voltage 1 2 v gb s serial links 2d on chip eye opening monitor on chip eom rectangular 2d eom ibm cmos technology data link channel length attenuation spectre cadence design systems bsim four device models data jitter size 130 nm	This paper presents a new 2-D on-chip eye-opening monitor (EOM) for Gb/s serial links. A comprehensive review of the state-of-the-art of on-chip EOMs is provided and their pros and cons are investigated. A new hexagon 2-D EOM that outperforms the widely used rectangular 2-D EOMs is introduced and the implementation details are presented. The effectiveness of the proposed EOM is evaluated by embedding it in a serial link implemented in an IBM 130 nm 1.2 V CMOS technology. For the purpose of comparison, a rectangular 2-D EOM is also included in the same data link. The data link with a variable channel length and attenuation is analyzed using Spectre from Cadence Design Systems with BSIM four device models. Simulation results of the data link demonstrate that the proposed EOM outperforms the rectangular EOM by providing a tightened control of data jitter at the edge of data eyes and by eliminating unnecessary errors flagged by the rectangular EOM.	bit error rate;cmos;enterprise output management;gigabyte;serial communication;simulation	Alaa R. Al-Taee;Fei Yuan;Andy Gean Ye;Saman Sadr	2014	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2013.2267805	electronic engineering;real-time computing;telecommunications;computer science;intersymbol interference	Visualization	29.98882840930066	55.71044898396441	106521
2a7eb658d7fabead1e0022baabde08eea0773305	linear secret sharing from algebraic-geometric codes	secret sharing;elliptic curve;secure multi party computation;multi party computation;linear secret sharing scheme;threshold scheme;indexing terms;verifiable secret sharing;error correction code;linear code;secret sharing scheme;geometric structure;algebraic curve	It is well-known that the linear secret-sharing scheme (LSSS) can be constructed from linear error-correcting codes (Brickell [1], R.J. McEliece and D.V.Sarwate [2],Cramer, el.,[3]). The theory of linear codes from algebraic-geometric curves (algebraic-geometric (AG) codes or geometric Goppa code) has been well-developed since the work of V.Goppa and Tsfasman, Vladut, and Zink( see [17], [18] and [19]). In this paper the linear secret-sharing scheme from algebraicgeometric codes, which are non-threshold schemes for curves of genus greater than 0, are presented . We analysis the minimal access structure, dmin and dcheat([8]), (strongly) multiplicativity and the applications in verifiable secret-sharing (VSS) scheme and secure multi-party computation (MPC) of this construction([3] and [10-11]). Our construction also offers many examples of the self-dually GF (q)-representable matroids and many examples of new ideal linear secret-sharing schemes addressing to the problem of the characterization of the access structures for ideal secret-sharing schemes([3] and [9]). The access structures of the linear secret-sharing schemes from the codes on elliptic curves are given explicitly. From the work in this paper we can see that the algebraic-geometric structure of the underlying algebraic curves is an important resource for secret-sharing, matroid theory, verifiable secret-sharing and secure multi-party computation.	access structure;algebraic equation;formal verification;forward error correction;genus (mathematics);goppa code;grammatical framework;linear algebra;matroid;printer (computing);reed–solomon error correction;secure multi-party computation;verifiable secret sharing	Hao Chen	2005	CoRR		combinatorics;discrete mathematics;error detection and correction;index term;computer science;theoretical computer science;linear code;mathematics;secure multi-party computation;algebraic curve;elliptic curve;secret sharing;verifiable secret sharing;statistics	Crypto	37.77591448981129	54.70526329716574	106756
4368ba17e286637a232b79302624bbddc88d94e1	cross-mbcr: exact minimum bandwith coordinated regenerating codes		We study the exact and optimal repair of multiple failures in codes for distributed storage. More particularly, we provide an explicit construction of exact minimum bandwidth coordinated regenerating codes (MBCR) for n = d + t, k, d ≥ k, t ≥ 1. Our construction differs from existing constructions by allowing both t > 1 (i.e., repair of multiple failures) and d > k (i.e., contacting more than k devices during repair).	clustered file system;code	Steve Jiekak;Nicolas Le Scouarnec	2012	CoRR		combinatorics;theoretical computer science;mathematics;algorithm	Theory	35.934511837441384	58.36341776261988	107288
54dfeb98d179fae149a1922c4c2b629385727744	on the density of best coverings in hamming spaces	best covering;hamming space	Nous présentons des constructions pour des recouvrements d'espaces de Hamming binaires de dimension n par des sphères de rayon 1. Nous montrons que la densité minimale μn de tels recouvrements est au plus 3/2. Le comportement asymptotique de μn quand n tend vers l'infini n'est pas connu.	spaces;window function	M. Beveraggi;Gérard D. Cohen	1986		10.1007/3-540-19368-5_4	binary code;linear code;topology;hamming code;mathematics	Theory	38.49820748694833	50.73255406008786	107610
4a373cde4ef6df4a57efd83b6a474c61814349cc	the limits of error correction with lp decoding	minimisation;lp norm;minimization error correction decoding corrupted measurements coding matrix unknown error vector;minimization;compressed sensing;decoding;support vector machines;coding matrix;corrupted measurements;unknown error vector;error correction;linear programming;error correction error correction codes decoding sparse matrices signal analysis read only memory;encoding;information theory;minimisation decoding error correction	An unknown vector f in R<sup>n</sup> can be recovered from corrupted measurements y = Af + e where A<sup>m×n</sup>(m ≥ n) is the coding matrix if the unknown error vector e is sparse. We investigate the relationship of the fraction of errors and the recovering ability of l<inf>p</inf>-minimization (0 < p ≤ 1) which returns a vector x that minimizes the “l<inf>p</inf>-norm” of y-Ax. We give sharp thresholds of the fraction of errors that is recoverable. If e is an arbitrary unknown vector, the threshold strictly decreases from 0.5 to 0.239 as p increases from 0 to 1. If e has fixed support and fixed signs on the support, the threshold is ⅔ for all p in (0, 1), and 1 for p = 1.	data recovery;error detection and correction;linear programming decoding;schedule (computer science);sparse matrix	Meng Wang;Weiyu Xu;Ao Tang	2010	2010 IEEE International Symposium on Information Theory	10.1109/ISIT.2010.5513613	support vector machine;minimisation;error detection and correction;information theory;linear programming;theoretical computer science;machine learning;mathematics;compressed sensing;lp space;encoding;statistics	Theory	37.42603353178815	59.69134867206483	107849
614d61694e144b7e00f344fbe32c4bca3bc5c3cd	finding the most fault-tolerant flat xor-based erasure codes for storage systems	search space fault tolerant flat xor based erasure code storage system;storage system;fault tolerant;search space;storage management;erasure code;fault tolerance fault tolerant systems hamming distance space exploration force parity check codes systematics;fault tolerant computing;codes;storage management codes fault tolerant computing	We describe the techniques we developed to efficiently find the most fault-tolerant flat XOR-based erasure codes for storage systems. These techniques substantially reduce the search space for finding fault-tolerant codes (e.g., by a factor of over 52 trillion in one case). This reduction in the search space has allowed us to find the most fault-tolerant codes for larger codes than was previously thought feasible. The result of our effort to find the most fault-tolerant flat XOR-based erasure codes for storage systems has yielded a corpus of 49,215 erasure codes that we are making public.	erasure code;exclusive or;fault tolerance	Jay J. Wylie	2011	2011 Conference Record of the Forty Fifth Asilomar Conference on Signals, Systems and Computers (ASILOMAR)	10.1109/ACSSC.2011.6190329	block code;concatenated error correction code;real-time computing;online codes;fountain code;theoretical computer science;tornado code;linear code;luby transform code;mathematics;distributed computing	HPC	37.02722770878911	58.28619943739905	108050
1f18c1bd75f142cbaa634c66ceebfe9d59016373	on the optimality of randomized time division and superposition coding for the broadcast channel	electronic mail;decoding;receivers;regions;encoding;conferences	This paper shows that the slope at each corner point of the capacity region of the general broadcast channel coincides with that of the randomized time division (hence the Marton) inner bound and the Nair-El Gamal (as well as the Körner-Marton) outer bound. We then show that the optimal superposition coding inner bound by Bandemer, El Gamal, and Kim can be simplified to the convex closure of the union of the Cover-Bergmans UX region and the Cover-van der Meulen UV region. Generalizing a result by Hajek and Pursely on the skewed binary symmetric broadcast channel, we show that for binary input broadcast channels, the UV region reduces to time division further simplifying the superposition coding inner bound. Finally we establish necessary and sufficient conditions for the optimality of the superposition inner bound for skewed binary broadcast channels.	a/ux;broadcast domain;convex hull;randomized algorithm	Chandra Nair;Hyeji Kim;Abbas El Gamal	2016	2016 IEEE Information Theory Workshop (ITW)	10.1109/ITW.2016.7606810	telecommunications;theoretical computer science;mathematics;communication	Theory	37.56189944851013	56.379888590647354	108465
726c61f6019ca485f3e6e21196000d11a2ec28ea	on the non-existence of minimum storage regenerating codes with repair-by-transfer property	single node failure repair minimum storage regenerating code nonexistence repair by transfer property distributed storage code storage overhead bandwidth consumption;maintenance engineering systematics bandwidth encoding interference computer aided software engineering;systematics;maintenance engineering;interference;encoding digital storage;computer aided software engineering;bandwidth;encoding	Regenerating codes are the family of distributed storage codes that achieve the optimal trade-off between storage overhead and the bandwidth consumption for the repair of a single node failure. Recent research reveals that, with certain regenerating codes, the amount of disk read for the repair can be reduced to as much as the repair bandwidth (i.e., the amount of data to be transferred), which makes encoding at the helper nodes unnecessary. In literature, such a property is often referred to as the repair-by-transfer property. Searching for regenerating codes with the repair-by-transfer property has been of great interest. In this manuscript, we prove that there does not exist a minimum storage regenerating (MSR) code with the repair-by-transfer property for the case of k ≥ 3, β <; d - k + 1, where k is the number of storage nodes required to reconstruct the original file, d is the number of helper nodes, and β is the number of symbols transmitted from each helper node.	asynchronous i/o;clustered file system;code (cryptography);input/output;overhead (computing);qr code;stochastic matrix	Yubin Chen;Yan Wang	2015	IEEE Communications Letters	10.1109/LCOMM.2015.2487971	maintenance engineering;real-time computing;telecommunications;computer science;theoretical computer science;distributed computing;interference;systematics;computer-aided software engineering;bandwidth;encoding;statistics	Theory	36.0391573056696	58.63971900327582	109240
a0899e131f45181a0b05a9f9860ee09257c064b1	extracting interconnect capacitance sensitivity to linewidth variation	process variation;metals;integrated circuit layout;very large scale integration;linewidth variation;interconnect;vlsi integrated circuit interconnections integrated circuit layout;layout;data mining;capacitance extraction;capacitance extraction interconnect process variation;sensitivity;integrated circuit interconnections;vlsi;capacitance;vlsi layout;parasitic capacitance very large scale integration conductors ieee members shape etching cleaning geometry dielectrics capacitance measurement;interconnect capacitance sensitivity;vlsi layout linewidth variation interconnect capacitance sensitivity process variation very large scale integration;conductors	VLSI interconnect capacitance is becoming more significant and also increasingly subject to process variation. A simple technique to extract from layout the sensitivity of interconnect parasitic capacitance to linewidth process variations is proposed based on 2.5D capacitance models and implemented in the Magic VLSI layout editor. The derivative of each extracted capacitance with respect to variation in every level is obtained. Coincident edges in layout result in distinct “shrinking” and “bloating” derivatives. The derivatives form a gradient that may be multiplied by a vector of the variations on each level to give the total expected deviation from nominal capacitance. The gradient allows the process sensitivity of each capacitance to be determined by simply inspecting the netlist.	2.5d;gradient;netlist;very-large-scale integration	Nick Huang;Andrew Labun	2009	2009 Canadian Conference on Electrical and Computer Engineering	10.1109/CCECE.2009.5090117	electronic engineering;computer science;engineering;electrical engineering;very-large-scale integration;engineering drawing	EDA	24.876249576437136	56.37559019592712	109672
9f63af00435420f60a46faa9f06241555da8daf9	boolean and ternary complementary pairs	candidate zero pattern;weight w;ternary complementary pair;ternary case;zero autocorrelation;length n;odd weight;boolean complementary pair;new pair;small length	A ternary complementary pair, TCPðn;wÞ; is a pair of ð0;71Þ-sequences of length n with zero autocorrelation and weight w: These are of theoretical interest in combinatorics as well as of practical consequence in coding, transmitting and processing various kinds of signals. When one attempts to construct a TCP of given length and weight, the first thing to decide is where to place the zeros, if any. Thus arise Boolean complementary pairs, BPðn;wÞ—pairs of ð0; 1Þsequences of length n with zero autocorrelation over Z2 and a total of w 1’s. The unique pair of ð0; 1Þ-sequences having the same support as a TCPðn;wÞ is a BPðn;wÞ (but the converse is not necessarily true); thus, Boolean complementary pairs establish candidate zero patterns for ternary complementary pairs. This cleanly separates the construction of ternary complementary pairs into two stages: deciding where to put the zeros, and determining the sign of the nonzero entries. We obtain some necessary conditions for the existence of Boolean complementary pairs. We conduct an exhaustive survey of pairs of small lengths and construct some infinite classes clearly of fundamental importance in the theory. We completely characterize all pairs of even weight and give a product construction for pairs of odd weight that gives a greater variety of new pairs than similar product methods used in the ternary case. r 2003 Elsevier Inc. All rights reserved.	autocorrelation;transmitter	Robert Craigen	2003	J. Comb. Theory, Ser. A	10.1016/S0097-3165(03)00118-3	arithmetic;combinatorics;discrete mathematics;mathematics	Theory	38.945047478555125	51.65689836625439	110076
4c6123a83a5098e1d43bdb465792a8427bf96757	mos vlsi circuit simulation by hardware accelerator using semi-natural models	hardware accelerator;circuit simulation	The accelerator is destined to circuit-level simulation of digital and analog/digital MOS VLSI'c containing of up to 100 thousand transistors (with 16 Mb RAM host-machine). The acceleration factor obtains of 3-4 order with respect to the Spice-2 program on VAX-11/780. The basic idea of the accelerator is to use real transistors instead of their mathematical models. In addition the accelerator concurrently uses 16processors and programmable communications between processors and distributed memory, the waveform relaxation method and Spice-like input language. for the multivariant circuit simulation, optimization, statistical analysis, and for the percent of parametrical defect determination. A radical solution to these problems would be to use a hardware accelerator. There are several commercial implementation of specific hardware accelerators for VLSI circuit simulation: the SPICE ACCELERATOR system by Weitek Corporation (has the acceleration factor 10-40 in comparison with Spice-2 program on VAX11/780); the autonomic system series for circuit simulation SX-250, SX-1000 and SX-2500 by Shiva Multisystem Corporation (a factor of 20 to 100); the accelerator project constructed by Brunel University is assumed to increase the simulation rate to 100 times as much. The accelerator project Awsim-2 [1] has the accelerator factor to 50000, but it uses the greatly simplified mathematical models. It is assumed, that VLSI circuit does not contain the float capacitors, such as drain-gate capacitors, all of MOS-transistor capacitors are linear, source and drain resistances are neglected. These assumptions are of principle importance and they are the essential signs to differ the classical circuit simulation from its simplified variants. That is why these accelerators may not be considered as a circuits simulators without any conditionality. In most practical cases VLSI digital circuits time-domain verification is carried out using switch level timing simulation. Switch-level verification and simulation both have low reliability, because they are based on significant simplification of the problem. Many of the essential features are not taken i nto account, for example: switch delay's dependence upon the input signals combination; effects of long transmission lines, non linearity of load capacitors and waveforms peculiarity, parasitic feedbacks, the influence of temperature and radiation. The attempts of taking these effects into account without solving the system of nonlinear differential equations lead to loss of reliability. On the other hand, the standard ways of solution for such a system of equations requires too many CPU time. These problems become more serious This paper describes the project of hardware accelerator, which is an equivalent to SPICEsimulation and is about 1000...10000 faster than SPICE-2 with VAX11/780. The accelerator will take up to 16 Mbyte RAM as the number of transistors becomes about 100,000. Besides, the use of the real transistors instead of their Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission.  1994 ACM 0-89791-687-5/94/0009 3.50 mathematical models allows to achieve th 99,9% fidelity of volt-ampere characteristics for MOStransistors inside the wide interval of temperature, radiation, light, humidity, pressure and vibration influence. The real devices as the models are subjected themselves by the external influence without terminating of simulation process. concurrence of a 16-processor system with the programmable communications between processors and distributed memory (fig. 1). Each processor consists of the processor for transistor simulation and processor [3] for two-pole elements simulation (fig. 2). The interprocessor commutator was organized in the form of the closed surface (fig. 2) and it allows to realize the overwhelming majority of connection between processors in accordance with simulated subcircuit. 1. BASIC PRINCIPLES OF ACCELERATOR DESIGN The high operation characteristics of accelerator arise from the original simulation methods based on the use of real circuit elements instead of their own models. In several points it is similar to Realmodel, Realchip, Realfast systems (Daisy Systems corp., Valid Logic Systems Inc.), VEETest (Hewlett-Packard Co.), PDM (Plessy Semiconductors Ltd.), but in our case we have circuit (not logic) level simulation. Figure 2. The commutator structure; square and round is processor for transistor and twopole simulation Fig. 1. The architecture of hardware accelerator with host-machine On the hardware/software level we use independent choice of timesteps for different subcircuits; piecewise-linear approximation of waveforms that reduces the expenditures of memory and increases the channel capacity (bandwidth) of interconnection between hostmachine and accelerator; subcircuit simulation with different speed-up (and precision). We use also event control of the simulation process; partition of VLSI into subcircuits across the weak feedback paths; SPICE-input using the The basic principles of the accelerator include exploiting the real transistors instead of their mathematical models and modification of Waveform Relaxation method (WR) [2]. Besides, the widely known methods of modeling have been used such as hardware realization of VLSI circuit element models in each processor; subcircuits language construction for the circuit description language. where δ , εsi, Cox , φF, are the widely known parameter of Spice program models; Ww , Wn the channel width of transistors Tn and Tw; W is the channel width of transistor to be simulated. The values of other parameters and variables follow from fig. 3 and the above equations. 2. MOS-TRANSISTOR MODEL The accelerator employs the semi-natural models of MOS transistor (fig. 3) which are based on the real (reference) transistors Tn and Tw , manufactured by the same technology as VLSI circuit is to be designed. In this case the most of transistor parameters can be considered as a build-in parameters of the models and we are not need in identification stage. We must only control the transistor width and length as well as drain and source resistances and capacitances. Wn = 0.8 μ Ww= 4.0 μ γ Spice 2.0 4.0 6.0 8.0 50 100 0.0 W, μ 0.6	autonomic computing;basic;central processing unit;channel capacity;commutator (electric);daisy digital talking book;digital electronics;distributed memory;electrical element;electronic circuit simulation;hardware acceleration;horner's method;integrated circuit;interconnection;konix multisystem;level of detail;linear approximation;linear programming relaxation;logic level;mathematical model;mathematical optimization;megabyte;newman's lemma;nonlinear system;random-access memory;relaxation (approximation);relaxation (iterative method);reliability engineering;spice 2;semiconductor industry;software bug;timing closure;transistor model;transmission line;vax-11;very-large-scale integration;waveform;xfig	Victor V. Denisenko	1994			embedded system;computer architecture;computer engineering	EDA	28.100693422233753	49.96903236368637	110094
1658b857f8472f55fddae06342c4eca76c592eea	a lower bound on list size for list decoding	list;list decoding;approximation asymptotique;error correcting code;lista;codigo corrector error;alfabeto;optimisation combinatoire;aleatorizacion;hamming distance;error correction code;distance hamming;borne inferieure;liste;randomisation;asymptotic approximation;randomization;combinatorial optimization;code correcteur erreur;distancia hamming;lower bound;alphabet;aproximacion asintotica;cota inferior;optimizacion combinatoria	A <i>q</i>-ary error-correcting code <i>C</i> ⊆ {1,2,...,<i>q</i>}<i>n</i> is said to be list decodable to radius ρ with list size <i>L</i> if every Hamming ball of radius ρ contains at most <i>L</i> codewords of <i>C</i>. We prove that in order for a <i>q</i> -ary code to be list-decodable up to radius (1-1/<i>q</i>)(1- ε)<i>n</i>, we must have <i>L</i> = Ω(1/ ε<sup>2</sup>) . Specifically, we prove that there exists a constant <i>cq</i> > 0 and a function <i>fq</i> such that for small enough ε > 0, if <i>C</i> is list-decodable to radius (1-1/<i>q</i>)(1- ε)<i>n</i> with list size <i>cq</i>/ ε<sup>2</sup>, then <i>C</i> has at most <i>fq</i>( ε) codewords, independent of <i>n</i> . This result is asymptotically tight (treating <i>q</i> as a constant), since such codes with an exponential (in <i>n</i> ) number of codewords are known for list size <i>L</i> = <i>O</i>(1/ ε<sup>2</sup>). A result similar to ours is implicit in Blinovsky ( Problems of Information Transmission, 1986) for the binary (<i>q</i>=2) case. Our proof is simpler and works for all alphabet sizes, and provides more intuition for why the lower bound arises.	ball project;code word;forward error correction;list decoding;time complexity;window function	Venkatesan Guruswami;Salil P. Vadhan	2005	IEEE Transactions on Information Theory	10.1007/11538462_27	arithmetic;combinatorics;error detection and correction;combinatorial optimization;computer science;mathematics;algorithm;statistics	Theory	38.279152038472766	56.34385038068594	110196
1b167fff2f71d44f5fa0b34e69cda1eed638e38b	hover erasure codes for disk arrays	xor based erasure code;parameter constraint;bit error rate;disc storage;erasure code;reed solomon codes;code standards;reliability theory;hover erasure code;fault tolerant computing disc storage;parameter constraint hover erasure code disk array xor based erasure code vertical parity arrangement maximum distance separable;fault tolerant computing;maximum distance separable;fault tolerant systems;vertical parity arrangement;fault tolerance;terminology;fault tolerant systems bit error rate reliability theory code standards reed solomon codes fault tolerance terminology;disk array	We present a new family of XOR-based erasure codes primarily targeted for use in disk arrays. These codes have a unique data/parity layout, with both horizontal and vertical parity arrangements giving rise to the name HoVer codes. We give constructions that tolerate up to four disk failures. Though the codes are only approximately maximum distance separable (MDS), they have performance advantages over other codes at many common array sizes. In addition, they have fewer parameter constraints than many other codes which enable greater choices and flexibility in efficiency and performance trade-offs	best practice;blue gene;disk array;erasure code;exclusive or;experiment;ibm research - almaden;mds matrix;parity bit;programming language implementation	James Lee Hafner	2006	International Conference on Dependable Systems and Networks (DSN'06)	10.1109/DSN.2006.40	block code;erasure code;reliability engineering;fault tolerance;parallel computing;real-time computing;online codes;disk array;bit error rate;reliability theory;fountain code;computer science;theoretical computer science;tornado code;linear code;terminology	DB	37.1358721201681	58.27249118454954	110610
38970048a288732bb543ec98a7e52bb69bfec412	high-speed tournament givens rotation-based qr decomposition architecture for mimo receiver	radio receivers;computer architecture matrix decomposition mimo hardware educational institutions clocks receivers;clocks;systolic arrays;flip flops;receivers;computer architecture;systolic arrays flip flops matrix decomposition mimo communication radio receivers;matrix decomposition;mimo;mimo communication;hardware;size 0 25 mum high speed tournament givens rotation based qr decomposition architecture mimo receiver high speed hardware architecture tournament based complex givens rotation t cgr zero insertion process triangular systolic array approach flip flops tsmc technology tacr tsa based architecture matrix decomposition	This paper presents a high-speed hardware architecture of an improved Givens rotation-based QR decomposition, named tournament-based complex Givens rotation (T-CGR). In the proposed approach, more than one pivots are selected and zero-insertion processes of Givens-rotations are performed in parallel like tournament in order to increase the throughput. As a result, the QR decomposition performance significantly increases compared to the triangular systolic array (TSA) approach. Moreover, the circuit area was reduced due to the smaller number of flip-flops for holding the computed results during the decomposition process. The proposed QR decomposition hardware was implemented using TSMC 0.25 um technology. The experimental results show that the proposed architecture achieves 73.00% speed-up over the TACR/TSA-based architecture for the 8 × 8 matrix decomposition.	flops;flip-flop (electronics);givens rotation;mimo;qr decomposition;systolic array;throughput;unified model	Min-Woo Lee;Ji-Hwan Yoon;Jongsun Park	2012	2012 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2012.6271699	embedded system;electronic engineering;parallel computing;real-time computing;computer science;mathematics;radio receiver;matrix decomposition;statistics;mimo	Arch	32.40152973670931	58.350583852360344	110707
2e46107705e02b7e039eada89aea06c824f2a684	the second generation motis mixed-mode simulator	existing motis mixed-mode simulator;new simulator;timing verification;timing simulation;mixed-mode environment;floating capacitor;current modeling capability;bidirectional transmission gate;general mos circuit;unit delay simulation;production;semiconductor device modeling;logic circuits;resistors;chip;local time;capacitance;switches;propagation delay	This paper describes the second generation MOTIS mixed-mode simulator. In particular, it extends the current modeling capabilities to include resistors, floating capacitors, and bidirectional transmission gates. It employs a relaxation algorithm with local time-step control for timing simulation, and a switch level approach for unit delay simulation. It provides logic and timing verification for general MOS circuits in a mixed-mode environment. The new simulator is being used for production chips, and it is more accurate, flexible, and efficient than the existing MOTIS mixed-mode simulator.	algorithm;linear programming relaxation;mixed-signal integrated circuit;relaxation (iterative method);second generation multiplex plus;simulation;static timing analysis;timing closure	Chin-Fu Chen;Chi-Yuan Lo;Hao N. Nham;Prasad Subramaniam	1984	21st Design Automation Conference Proceedings		resistor;chip;propagation delay;electronic engineering;semiconductor device modeling;real-time computing;logic gate;network switch;computer science;engineering;electrical engineering;local time;capacitance;computer network	EDA	26.280192653719794	54.04235822151255	110722
f50768c80e8f37e4ca15506e9c4764f62a4ae2c8	spectral leakage-driven loopback scheme for prediction of mixed-signal circuit specifications		The rising cost of production testing for a system-on-a-chip (SoC) is one of the crucial matters to chip makers, due to long test time and costly automated-test-equipment. This paper proposes a spectral leakage-driven built-in self-test (BIST) scheme to precisely predict the nonlinearity of mixed-signal circuits in the loopback mode, thereby accomplishing cost-effectiveness (compared to previous BIST-based works). A digitally synthesized single-tone sinusoidal stimulus used for conventional harmonic testing is incoherently sampled by a device under test (DUT). The DUT output signal exhibits the correlation between the DUT harmonics and the spectral leakage introduced by the incoherent sampling. The DUT output signal is then fed to another DUT through a loopback path, so that the harmonics of a pair of DUTs are correlated with the spectral leakage on the loopback response; the magnitude of the spectral leakage is considered as a weighting factor on the harmonic magnitude of those DUTs. The correlation is quantitatively modeled as characteristic equations in (15), and postprocessing predicts the harmonics of the two individual DUTs, by simultaneously solving the characteristic equations using on-chip DSP core available in an SoC. Simulation and hardware measurements validated that this paper can be practically used for production testing by showing less than 0.3 and 0.6 dB of the prediction errors, respectively.	built-in self-test;device under test;loopback;mixed-signal integrated circuit;nonlinear system;sampling (signal processing);simulation;spectral leakage;system on a chip	Byoungho Kim;Jacob A. Abraham	2019	IEEE Transactions on Industrial Electronics	10.1109/TIE.2018.2829667	mixed-signal integrated circuit;a-weighting;electronic circuit;digital signal processing;engineering;loopback;spectral leakage;control theory;harmonics;harmonic	EDA	25.85735202180293	52.781721032933184	111203
0a17186e44caa9b738ef75f855b20d0a4de35285	on-chip rise-time measurement	cmos integrated circuits;built-in self test;integrated circuit testing;mixed analogue-digital integrated circuits;time measurement;cmos circuits;built-in self-testing;design-for-test;mixed-signal testing;on-chip instrumentation;rise-time measurement;65;bist;built-in self-testing;dft;design-for-test;mixed-signal testing;on-chip instrumentation;rise-time measurement	The on-chip rise-time measurement method for an exponentially decaying signal is proposed. By employing a differentiator, the method circumvents the challenges associated with directly measuring a signal's rise time. The method can be used as the on-chip test instrument as part of a built-in self-testing (BIST) framework, or independently. CMOS circuits and layouts for implementation of the proposed method on a 0.25-/spl mu/m technology have been developed.	built-in self-test;cmos;database trigger;differentiator;mumps;rise time;time complexity	San L. Lin;Samiha Mourad	2004	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2004.834060		EDA	25.621839988176674	52.87808134477836	111389
148a8bf5f4e88d04df5a25467dc30fda3943a15f	combinatorial channels from partially ordered sets	telecommunication channels integer programming linear programming;deletion errors;thesis;coding theory;constant weight binary codes combinatorial channels rank n elements parallelogram property confusion graph integer linear program fractional relaxation sphere packing upper bound q ary vectors hamming errors;combinatorics partially ordered set;measurement upper bound encoding binary codes lattices	A combinatorial channel specifies a set of possible channel outputs for each channel input. A ranked partially ordered set, or ranked poset, gives us a notion of up errors and down errors. This allows us to define a variety of combinatorial channels. There is a family of channels that have the rank-n elements of the poset as the input, and introduce s total errors, each performing a different mixture of up errors and down errors. If a ranked poset has the “parallelogram property,” the family of channels all have the same confusion graph and thus the same codes. Furthermore, there is a natural metric on each rank of the poset. In the common confusion graph of the channel, vertices are adjacent if and only if their distance in this metric is at most 2s. Although all of the channels in the family have the same set of codes, each channel corresponds to a different integer linear program that characterizes the set of codes. Because each integer linear program has a different fractional relaxation, each leads to a different sphere-packing upper bound for the codes. We take advantage of this phenomenon by optimizing across the family of channels to obtain the best bound. This formulation includes many of classical error models, including erasures and substitutions in q-ary vectors, Hamming errors in constant weight binary codes, insertions and deletions in q-ary strings, the error model of subspace codes, the natural error model for compositions, and various errors models for permutations.	binary code;hamming distance;linear programming relaxation;set packing;shannon capacity of a graph	Daniel Cullina;Negar Kiyavash	2015	2015 IEEE Information Theory Workshop (ITW)	10.1109/ITW.2015.7133160	block code;combinatorics;discrete mathematics;pure mathematics;linear code;hamming code;mathematics	Theory	38.48196682744174	53.86511904965457	112865
99bee893e420010f51d79092b98cd378dcaf4359	encoding an arbitrary state in a [7, 1, 3] quantum error correction code	fault tolerant;pauli operator error environment;quantum fault tolerance;quantum computer;quantum physics;error correction;quantum error correcting codes;error probability;quantum error correction	We calculate the fidelity with which an arbitrary state can be encoded into a [7, 1, 3] Calderbank-Shor-Steane quantum error correction code in a nonequiprobable Pauli operator error environment with the goal of determining whether this encoding can be used for practical implementations of quantum computation. The determination of usability is accomplished by applying ideal error correction to the encoded state which demonstrates the correctability of errors that occurred during the encoding process. We also apply single-qubit Clifford gates to the encoded state and determine the accuracy with which these gates can be implemented. Finally, fault tolerant noisy error correction is applied to the encoded states allowing us to compare noisy (realistic) and perfect error correction implementations. We find the encoding to be usable for the states |0〉, |1〉, and |±〉 = |0〉 ± |1〉. These results have implications for when non-fault tolerant procedures may be used in practical quantum computation and whether quantum error correction must be applied at every step in a quantum protocol.	computation;error detection and correction;fault tolerance;quantum computing;quantum error correction;qubit;shor's algorithm;usability;user error	Sidney D. Buchbinder;Channing L. Huang;Yaakov S. Weinstein	2013	Quantum Information Processing	10.1007/s11128-012-0414-7	fault tolerance;discrete mathematics;error detection and correction;theoretical computer science;probability of error;quantum capacity;mathematics;quantum computer;quantum algorithm;physics;algorithm;quantum mechanics;quantum error correction	PL	38.47836270271218	59.590924344439784	113051
69c720e498831be8c2b5feca7e0478ee0c15b073	a digital audio signal processor for cellular phone application	audio signal processing;radio receivers;conference;cellular radio;turn off;power supply;memory access;low power;land mobile radio;cmos digital integrated circuits;digital filters;mobile communication;audio signals;digital signal processing chips;power consumption;high performance;parallel processing;pipeline processing	| A salient digital audio signal processor for mobile communication receiver is described in this paper. With this IC, the complete audio signal processing system of an AMPS or a TACS cellular phone is easily implemented. The processor can be also applied to cellular radio, high performance cordless telephone, etc. In this paper, as an example of application, the implementation of AMPS audio signal processing system is presented. To save the power consumption, some special consideration of the low power architectures has been made. The DSP core uses 4 stage pipelining without a routine memory access stage to reduce the power consumption and execution speed. The parallel calculations of data in the DSP and separated lter blocks also contribute to reduce the clock frequency and to save power. It also provides the power-down mode that turns o the IC except the internal bus interface in the standby mode. It uses 3.3V power supply, 10MHz 4-phase clock. The data sampling rate is 10K-samples per second.	audio signal processor;audio signal processing;bus (computing);clock rate;digital signal processor;mobile app;mobile phone;parallel computing;pipeline (computing);power supply;sampling (signal processing);sleep mode;transcranial alternating current stimulation	Jeongsik Yang;Chanhong Park;Beomsup Kim	1995		10.1145/224818.224896	embedded system;parallel processing;electronic engineering;real-time computing;digital filter;mobile telephony;telecommunications;audio signal processing;computer science;audio signal;radio receiver;audio signal flow	Mobile	32.37359258667837	52.153962932726365	113902
94dc61a4bfeea9e22b9ad29c6fedc8742aef3c8a	area and energy efficient vlsi architectures for low-density parity-check decoders using an on-the-fly computation	array ldpc;low density parity check ldpc codes;semi parallel architecture;layered decoding;decoder architecture;irregular ldpc;thesis;offset min sum;book;vector processing;turbo decoding message passing;parallel processing;block ldpc	Area and Energy Efficient VLSI Architectures for Low -Density Parity-Check Decoders Using an On-the-Fly Computation. (December 2006) Kiran Kumar Gunnam, M.S., Texas A&M University Co-Chairs of Advisory Committee: Dr. Gwan Choi Dr. Scott Miller The VLSI implementation complexity of a low density parity check (LDPC) decoder is largely influenced by the interconnect and the storage requirements. This dissertation presents the decoder architectures for regular and irregular LDPC codes that provide substantial gains over existing academic and commercial implementations. Several structured properties of LDPC codes and decoding algorithms are observed and are used to construct hardware implementation with reduced processing complexity. The proposed architectures utilize an on-the-fly computation paradigm which permits scheduling of the computations in a way that the memory requirements and re-computations are reduced. Using this paradigm, the run-time configurable and multi-rate VLSI architectures for the rate compatible array LDPC codes and irregular block LDPC codes are designed. Rate compatible array codes are considered for DSL applications. Irregular block LDPC codes are proposed for IEEE 802.16e, IEEE 802.11n, and IEEE 802.20. When compared with a recent implementation of an 802.11n LDPC decoder, the proposed decoder reduces the logic complexity by 6.45x and memory complexity by 2x for a given data throughput. When compared to the latest reported multi-rate decoders, this decoder design has an area	algorithm;algorithmic efficiency;computation;digital subscriber line;ieee 1471;low-density parity-check code;parity bit;programming paradigm;requirement;scheduling (computing);throughput;very-large-scale integration	Kiran K. Gunnam	2006			parallel computing;real-time computing;computer science;theoretical computer science;error floor	Arch	32.84986931008363	55.373737098364074	113940
c97c58bdc9365cc77d1f8774d613768644bec437	a reconfigurable mpsoc-based qam modulation architecture	communication system;dynamic load balancing;dynamic load balancing reconfigurable architecture qam modulation mpsoc multilevel modulation technique data radio communication system parallel architecture multirate modulation dynamic reconfiguration hardware based resource allocation;mpsoc design methodology;dynamic reconfiguration;resource allocation;reconfigurable architectures;nickel;resource management;high data rate;quadrature amplitude modulation resource management nickel heuristic algorithms hardware throughput;mpsoc design methodology qam modulators;data communication;parallel architectures;system on chip;heuristic algorithms;system on chip data communication multiprocessing systems parallel architectures quadrature amplitude modulation reconfigurable architectures resource allocation;multiprocessing systems;quadrature amplitude modulation;qam modulators;throughput;hardware;design methodology	QAM is a widely used multi-level modulation technique, with a variety of applications in data radio communication systems. Most existing implementations of QAM-based systems use high levels of modulation in order to meet the high data rate constraint of emerging applications. This work presents the architecture of a highly-parallel MPSoC-based QAM modulator that offers multi-rate modulation. The proposed MPSoC architecture is modular and provides flexibility via dynamic reconfiguration of the QAM, offering high data rates (more than 1 Gbps), even at low modulation levels (16-QAM). Furthermore, the proposed QAM implementation integrates a hardware-based resource allocation algorithm for dynamic load balancing.	algorithm;data rate units;load balancing (computing);mpsoc;modulation	Christos Ttofis;Agathoklis Papadopoulos;Theocharis Theocharides;Maria K. Michael;Demosthenes Doumenis	2010	2010 18th IEEE/IFIP International Conference on VLSI and System-on-Chip	10.1109/VLSISOC.2010.5642606	system on a chip;nickel;throughput;electronic engineering;parallel computing;real-time computing;quadrature amplitude modulation;design methods;telecommunications;resource allocation;computer science;resource management;communications system	EDA	31.350417961560247	59.203808656202355	114492
06cceda13b63728a2b823a716487e9a0691fc450	design of a multi gbps single carrier digital baseband for 60ghz applications and its fpga implementation	quadrature phase shift keying;parity check codes;baseband field programmable gate arrays receivers throughput radio frequency parallel processing application specific integrated circuits;frequency 60 ghz multigbps single carrier digital baseband system architecture fpga mapping millimeter wave digital baseband wireless communication 802 11ad single carrier real time hardware mac beam forming front end fpga prototyping system qpsk modulation ldpc code rate bit rate 2 5 gbit s;radiocommunication;field programmable gate arrays;radiocommunication field programmable gate arrays parity check codes quadrature phase shift keying	This paper describes the system architecture, design methodology and subsequent FPGA mapping of a millimeter wave digital baseband for wireless communication in the 60GHz spectral band. The baseband is designed to be compliant with the 802.11ad Single Carrier and Control PHY draft specifications and supports a data rate of 2.5Gbps at the physical layer. The demanding throughput and latency requirements are achieved with a parallel implementation. However, due to limited capacity of the FPGAs present in our prototype platform and complex partitioning requirements, only a scaled down version of the full single carrier baseband that operates at 1/10th the throughput of the specification could be mapped. A minimal real-time hardware MAC was also incorporated and coupled with a 60GHz RF beam-forming front-end to demonstrate file transfer between two independent FPGA prototyping systems. A system throughput of 59Mbps was achieved at the application layer using π/2 QPSK modulation with a 13/16 LDPC code rate.	baseband;code rate;fpga prototyping;field-programmable gate array;file transfer;low-density parity-check code;modulation;phy (chip);prototype;radio frequency;real-time clock;requirement;systems architecture;throughput;uncompressed video	Surendra Guntur;Feike Jansen;Jan Hoogerbrugge;Lotfi Abkari;Eric Vos	2013	2013 23rd International Conference on Field programmable Logic and Applications	10.1109/FPL.2013.6645573	embedded system;telecommunications;computer science;line code;field-programmable gate array	Mobile	31.159166712684062	59.048246195897185	114579
408f9af56a14c46062263774ab216ae50508ff39	text compression using variable-to fixed-length encodings	text compression	Abstract#R##N##R##N#Many methods have been suggested for representing text for storage on magnetic media or for transmission down telecommunication channels with fewer bits then are required by a conventional fixed-length character representation. These methods are reviewed, and attention is drawn to the advantages of techniques in which variable-length character strings are represented by a fixed number of bits. Such techniques are described in more detail. The advantages and disadvantages of implementing text compression in storage and telecommunications are discussed, and an indication is given of the types of hardware which may be used. The extent to which text may be compressed with the methods discussed, and approximate timings, are stated.		David Cooper;Michael F. Lynch	1982	JASIS	10.1002/asi.4630330105	computer science;theoretical computer science;database;world wide web;information retrieval;algorithm	NLP	35.00384560392596	56.16337227100995	115316
842a27c34e370330b2a4e95620a27ca0197c2214	analog circuits soft fault diagnosis using rényi's entropy	component tolerance;lagrange multiplier method;analog circuit;renyi s entropy;soft fault	An analog circuit soft fault diagnosis method using Renyi's entropy is proposed. This method focuses on extracting the entropy information contained in the probability density function (PDF) of the output of the circuit under test (CUT), which is sensitive to the parameters of circuits. Firstly, using the Lagrange multiplier method with Renyi's entropy deduces PDF. Then the parameter ? of Renyi's entropy is estimated adaptively by employing the output of CUT through the maximum likelihood estimation method. Finally, the value of Renyi's entropy can be calculated using the PDF and ? parameter. The divergence between the Renyi's entropy corresponding to the fault and fault free circuits is adopted to detect the fault. The method can detect soft fautls, including the single fault and multiple faults, without complicated models and mass of data, and also without interrupting the inherent connections. We conduct experiments respectively on two circuits that are implemented on an actual circuit board. The effectiveness of the proposed method is demonstrated by the result of the experiment.	rényi entropy	Xuan Xie;Xifeng Li;Dongjie Bi;Qizhong Zhou;Sanshan Xie;Yongle Xie	2015	J. Electronic Testing	10.1007/s10836-015-5520-x	mathematical optimization;electronic engineering;binary entropy function;analogue electronics;maximum entropy probability distribution;computer science;stuck-at fault;mathematics;lagrange multiplier;statistics	EDA	25.512413558295936	55.0287692939951	115807
213da3a79bb00099bdb26d1606139ca7fc23615b	two-dimensional information theory and coding - with application to graphics data and high-density storage media		This self-contained introduction to two-dimensional (2-D) theory and coding provides the key techniques for modelling data and estimating their information content. Throughout, special emphasis is placed on applications to transmission, storage, compression, and error protection of graphic information. The book begins with a self-contained introduction to information theory, including concepts of entropy and channel capacity, which requires minimal mathematical background knowledge. It then introduces error-correcting codes, particularly Reed– Solomon codes, the basic methods for error-correction, and codes applicable to data organized in 2-D arrays. Common techniques for data compression, including compression of 2-D data based on application of the basic source coding, are also covered, together with an advanced chapter dedicated to 2-D constrained coding for storage applications. Numerous worked examples illustrate the theory, whilst end-of-chapter exercises test the reader’s understanding, making this an ideal book for graduate students and also for practitioners in the telecommunications and data-storage industries.	channel capacity;code;data compression;entropy (information theory);error detection and correction;forward error correction;graphics;information theory;reed–solomon error correction;self-information;while	Jørn Justesen;Søren Forchhammer	2010				ML	38.573343791966174	58.26406134775267	117406
11132032dc9bdc620520c700a806657b142a50cc	fast shape optimization of metalization patterns for power-mosfet based driver	heuristic algorithm;power mosfet;behavior modeling;shape optimization		power mosfet;shape optimization	Bo Yang;Shigetoshi Nakatake	2009	IEICE Transactions		behavioral modeling;heuristic;mathematical optimization;computer science;shape optimization;power mosfet;mathematics	EDA	28.428894977219677	48.300223679009335	117714
91976c6134fa99fa2bb4f41c74d2e6105a10d4eb	flexible, efficient multimode mimo detection by using reconfigurable asip	detectors;detection algorithms;multiple input multiple output mimo coarse grained reconfigurable architecture cgra markov chain monte carlo mcmc detection minimum mean square error mmse;arrays;mimo vectors algorithm design and analysis detectors detection algorithms arrays;vectors;size 65 nm reconfigurable asip software flexibility hardware configurability partially reconfigurable application specific instruction set processor wireless receiver algorithm multiple input multiple output modulation configuration antenna configuration coarse grained reconfigurable architecture cgra matrix operation noniterative mimo detection algorithm markov chain monte carlo cmos technology rasip based multimode mimo detection;signal detection application specific integrated circuits cmos integrated circuits instruction sets markov processes matrix algebra mimo communication modulation monte carlo methods radio receivers reconfigurable architectures;mimo;algorithm design and analysis	The combination of software flexibility and hardware configurability makes partially reconfigurable application-specific instruction-set processor (rASIP) an attractive architecture, which matches the needs of computation-intensive and fast-evolving wireless receiver algorithms. This paper describes the design of a multimode multiple-input-multiple-output (MIMO) detector by using rASIP, which supports multiple MIMO detection algorithms with different antenna and modulation configurations. The rASIP is mainly constructed using a coarse-grained reconfigurable architecture (CGRA) coupled with a processor. In MIMO detection, some important computation steps (e.g., preprocessing) or even the whole detection algorithm is realized using matrix operations. Therefore, for the rASIP, the CGRA is designed to efficiently support different matrix operations used in MIMO detection, and the processor is integrated with special instructions to implement the control path required by different algorithms. Feasibility of the proposed approach is shown by implementing three noniterative MIMO detection algorithms. To evaluate the flexibility of the proposed approach, a Markov Chain Monte Carlo based MIMO detection is also realized by mapping part of the algorithm by using matrix operations on the CGRA. Postlayout results of the rASIP are generated for the implemented detection algorithms on a 65-nm CMOS technology. Compared with some selected designs based on programmable architectures and dedicated application-specified integrated circuits (ASICs), we show that following the proposed approach, the rASIP-based multimode MIMO detection, is about 1.6-5.4 times more efficient than the programmable architectures, and it approaches the throughput performance to the dedicated ASICs.	algorithm;application-specific instruction set processor;application-specific integrated circuit;cmos;channel state information;computation;mimo;markov chain monte carlo;modulation;monte carlo method;preprocessor;standard cell;throughput	Xiaolin Chen;Andreas Minwegen;Bilal Syed Hussain;Anupam Chattopadhyay;Gerd Ascheid;Rainer Leupers	2015	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2014.2361206	embedded system;algorithm design;detector;electronic engineering;real-time computing;computer science;theoretical computer science;statistics;mimo	EDA	32.08413927935499	58.743421545813234	117857
088fcb5c88e695cd730ade0de720ecb9eda00b2b	on the generalization of error-correcting wom codes	optimal codes;silicon;flash memory;generalization error;error correction codes;electronic mail;decoding;optic disk;binary variables error correcting wom codes write once memory code storage media regular data updating capability electronic memory multilevel cells floating codes optimal codes;application software;geometry;natural extension;upper bound;storage media;distance measurement;write once memory code;error correction code;threshold voltage;error correction;error correction codes digital storage;linear code;binary variables;regular data updating capability;computer science;digital storage;error correcting wom codes;floating codes;encoding;flash memory error correction codes threshold voltage computer science electronic mail application software file systems linear code geometry error correction;state transition;file systems;electronic memory;multilevel cells	WOM (write once memory) codes are codes for efficiently storing and updating data in a memory whose state transition is irreversible. Storage media that can be classified as WOM includes flash memories, optical disks and punch cards. Error-correcting WOM codes can correct errors besides its regular data updating capability. They are increasingly important for electronic memories using MLCs (multi-level cells), where the stored data are prone to errors. In this paper, we study error-correcting WOM codes that generalize the classic models. In particular, we study codes for jointly storing and updating multiple variables - instead of one variable - in WOMs with multi-level cells. The error-correcting codes we study here are also a natural extension of the recently proposed floating codes. We analyze the performance of the generalized error- correcting WOM codes and present several bounds. The number of valid states for a code is an important measure of its complexity. We present three optimal codes for storing two binary variables in n q-ary cells, where n = 1,2,3, respectively. We prove that among all the codes with the minimum number of valid states, the three codes maximize the total number of times the variables can be updated.	code (cryptography);complexity;error detection and correction;flash memory;forward error correction;punched card;reed–solomon error correction;state (computer science);state transition table;write once, compile anywhere;write once, run anywhere	Anxiao Jiang	2007	2007 IEEE International Symposium on Information Theory	10.1109/ISIT.2007.4557417	block code;reed–muller code;concatenated error correction code;turbo code;parallel computing;error detection and correction;online codes;fountain code;computer hardware;computer science;theoretical computer science;linear code;hamming code;expander code;luby transform code;mathematics;statistics	Theory	37.66474373472396	58.88966590052488	118271
5c2d8e7e2bc552e3103281b801006f723261cb59	architecture and fpga implementation of a 10.7 gbit/s otn regenerator for optical communication systems	protocols;bit rate 10 7 gbit s fpga technology optical communication systems serializer deserializer hardwired modules high data rate protocols high speed telecommunication systems otn otu2 regenerator programmable logic device 3r regeneration otu overhead processing error correction otu2 received frames digital logic xilinx virtex 6 device optical transport network;optical fibre networks;optical logic;optical repeaters;field programmable gate arrays;repeaters adaptive optics optical fibers protocols forward error correction optical amplifiers;protocols field programmable gate arrays optical fibre networks optical logic optical repeaters	As the newest FPGA technologies provide high operating frequencies, in conjunction with serializer/deserializer hardwired modules enabling the operation of high data rate protocols, these devices are being increasingly used in high-speed telecommunication systems. This paper presents the architecture and development of an OTN OTU2 Regenerator that operates in 10.7 Gbit/s, fully implemented in a programmable logic device. 3-R regeneration, OTU overhead processing and error correction are realized in the OTU2 received frames by the digital logic developed. The aim of this work is to present the implementation feasibility of an OTN processor in FPGAs with applications in optical communication systems. A prototype was implemented in a Xilinx Virtex-6 device. Synthesis and timing results are also reported.	boolean algebra;data rate units;error detection and correction;field-programmable gate array;gigabit;logic gate;overhead (computing);programmable logic device;prototype;serdes	Rodrigo Bernardo;Luis R. Monte;Eduardo Mobilon;Valentino Corso;Arley H. Salvador;Carolina G. Neves;Cleber A. Nakandakare;Daniele R. da Silva;Luis P. F. de Barros;Ronaldo F. da Silva	2012	22nd International Conference on Field Programmable Logic and Applications (FPL)	10.1109/FPL.2012.6339193	embedded system;communications protocol;telecommunications;computer science;simple programmable logic device;field-programmable gate array	EDA	30.861833266321707	57.08766947380757	118727
d962ded11295753e4b1db18d736984c38aad5626	methodology for improved event-driven system-level simulation of an rf transceiver subsystem for wireless socs		Digital assisted analog and RF concepts used in multi-standard multi-band wireless SoCs require a strong interaction between the analog and digital subsystems. Therefore functional verification of modern complex SoCs has evolved into a very challenging part in todays design flows. As an approach for an expedient verification process, an event-driven method based on SystemVerilog HDL is presented to perform time-efficient verification simulations of an entire RF transceiver frontend. To take into account signal properties, system behavior and certain error scenarios, methods like RF signal representation in the equivalent baseband, electrical signal modeling and true event-driven filter modeling are applied. A comparison to conventional simulation methodologies is given.	analogue electronics;baseband;event-driven programming;hardware description language;level design;netlist;noise (electronics);nonlinear system;radio frequency;requirement;schematic;specification language;system on a chip;system-level simulation;systemverilog;test case;transceiver;transistor	Fabian Speicher;Christoph Beyerstedt;Markus Scholl;Tobias Saalfeld;Vahid Bonehi;Moritz Schrey;Ralf Wunderlich;Stefan Heinen	2018	2018 13th International Conference on Design & Technology of Integrated Systems In Nanoscale Era (DTIS)	10.1109/DTIS.2018.8368550	systemverilog;computer science;electronic engineering;real-time computing;phase-locked loop;signal;system-level simulation;radio frequency;design flow;functional verification;baseband	EDA	27.44072668596387	53.24008833623693	118795
faee6b3b67f899c26a1e0d2b4011deedcf18a74f	toward cnn chip-specific robustness	image processing algorithms;turing machines;image segmentation;image processing;cost function;cnn chip;very large scale integration;implementation;cellular neural network;semiconductor device measurement;very large scale integrated;cellular neural nets;gray scale;chip;very large scale integration vlsi cellular neural network cnn implementation;large scale integration;design method;chip specific optimization;chip specific robustness;digital systems;vlsi cellular neural nets image segmentation image sequences;cellular neural networks robustness design methodology gray scale turing machines large scale integration proposals semiconductor device measurement cost function optimization methods;template optimization cnn chip template design methods image processing algorithms very large scale integration chip specific optimization chip specific robustness;vlsi;design;robustness;global optimization;template design methods;cellular neural networks;sista;proposals;templates;template optimization;optimization methods;image sequences;design methodology;cnn	The promising potential of cellular neural networks (CNN) has resulted in the development of several template design methods. The CNN universal machine (CNN-UM), a programmable CNN, has made it possible to create image-processing algorithms that run on this platform. However, very large-scale integration implementations of CNN-UMs presented parameter deviations that do not occur on ideal CNN structures. Consequently, new design methods were developed aiming at more robust templates. Although these new templates were indeed more robust, erroneous behavior can still be observed. An alternative for chip-independent robustness is chip-specific optimization, where the template is targeted to an individual chip. This paper describes a solution proposal in this sense to automatically tune templates in order to make the chip react as an ideal CNN structure. The approach uses measurements of actual CNN-UM chips as part of the cost function for a global optimization method to find an optimal template given an initial approximation. Further improvements are achieved by generating chip-specific robust templates by doing a search for the best template among the optimal ones. The tuned templates are therefore customized versions that are expected to be much less sensitive to imperfections on the operation of CNN-UM chips. Results are presented for the binary and grayscale cases, including the case of grayscale output. It is expected that as this technique matures, it will give CNN-UM chips enough reliability to compete with digital systems in terms of robustness in addition to advantages of speed.	algorithm;approximation;artificial neural network;bitwise operation;digital electronics;global optimization;gradient;grayscale;image processing;integrated circuit;loss function;mathematical optimization;mixed-signal integrated circuit;performance tuning;robustness (computer science);self-tuning;software bug;turing machine;very-large-scale integration	Samuel Xavier de Souza;Müstak E. Yalçin;Johan A. K. Suykens;Joos Vandewalle	2004	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2004.827618	computer vision;electronic engineering;cellular neural network;design methods;image processing;computer science;theoretical computer science;machine learning;very-large-scale integration;global optimization	EDA	27.65281288097243	47.20610815419486	118828
38cf6b8f983a845346774e4eae47d81dbd850661	a decision procedure for the unique decipherability of multivalued encodings (corresp.)	decoding;variable length code;variable length coding vlc decoding;decision procedure;variable length coding vlc	EN -‘I WJl*= ii i MhJA:(h)N -’ r=l s=l bring about an extreme change from the original source message, that is to say, a perfect loss of distance. There has been considerable discussion [2]-[7] since Sardinas and Patterson [l] introduced a decision method for unique decomposability. Other decision methods for the fundamental problem of the decipherability of variable-length codes have been developed by Markov [7], Even [4], and others for the single-valued case, and studies have been carried out on their implementation by deterministic generalized sequential machines (gsm) [9]. However, in the case of multivalued encodings, unique decipherability is not equivalent to unique decomposability, and some new ideas are needed to solve this problem. N-l N-l * x x exp i2n(fis-~r)EN-2X(fk_i) j=o I=0	decision problem;exptime;emoticon;markov chain;multivalued dependency;variable-length code	Kohei Sato	1979	IEEE Trans. Information Theory	10.1109/TIT.1979.1056040	combinatorics;discrete mathematics;variable-length code;mathematics;algorithm;statistics	Theory	38.157984710594164	57.59993328230472	119438
673ef310329b07600f832dba6e2926b97895b036	verification of the rf subsystem within wireless lan system level simulation	analog rf subsystem;rf receiver part;wireless lan system level;system performance;rf subsystem;different simulation tool;system level simulation;digital part;digital system part;mobile communication system;complete system;rf part;embedded systems;network synthesis;mobile communication;radio frequency;system testing;radio receivers;signal processing	Today's mobile communication systems use sophisticated signal processing to achieve high transmission rates. Therefore a high complexity in the digital system part as well as very accurate signal processing in the analog RF subsystem is needed. So far analog and digital part are developed separately. The increased performance requirements demand now a common verification of the complete system including analog and digital parts. At the design of an IEEE 802.11a wireless LAN receiver it is demonstrated, how the RF receiver part is tested in the system level simulation. Different simulation tools are used. The simulation results show the impact of properties of the RF part on the system performance. Experiences form the tool evaluation are presented.	digital electronics;radio frequency;requirement;signal processing;simulation	Uwe Knöchel;Thomas Markwirth;Jürgen Hartung;Ralf Kakerow;Radhakrishna Atukula	2003			network synthesis filters;embedded system;electronic engineering;mobile telephony;telecommunications;computer science;signal processing;radio receiver;system testing;radio frequency;computer network	Mobile	29.203890996984573	59.24095453560725	119589
2a7d16bbf796e293c79168ee9dde8f5dfa629657	chaos-based cryptography: end of the road?	chaos cryptography degradation state space methods signal generators information security australia reliability theory h infinity control;chaotic system;digital chaotic systems;chaos;cryptography chaos;chaotic dynamics;cryptography;chaos based cryptography;nonlinear dynamics;digital chaotic systems chaos based cryptography nonlinear dynamics	Chaos-based cryptography emerged in the early 1990s as an innovative application of nonlinear dynamics in the chaotic regime. Even if in theory chaotic dynamics was thought to evolve into a new revolution in cryptography, in real-life an efficient and reliable chaos-based cryptosystem didn't emerge. The main but not the only reason is the dynamic degradation of digital chaotic systems, a subject that became very popular in the last few years. This paper presents a new theoretical background related to this issue that proves the inefficiency of chaos-based encryption algorithms. Even more, in one of the two relevant case studies presented, another myth is demolished: the analog encryption base on synchronized chaos.	algorithm;chaos theory;coding theory;cryptography;cryptosystem;elegant degradation;encryption;nonlinear system;real life	Daniel Curiac;Daniel T. Iercan;Octavian Dranga;Florin Dragan;Ovidiu Banias	2007	The International Conference on Emerging Security Information, Systems, and Technologies (SECUREWARE 2007)	10.1109/SECUREWARE.2007.4385313	control of chaos;neural cryptography;nonlinear system;cryptography;theoretical computer science;synchronization of chaos;distributed computing;computer security	EDA	33.465799636008484	47.664219340545266	119810
87773af879fe7a4ce1e0d7a4086072b092f23f46	nonlinear driver models for timing and noise analysis	galerkin method;nonlinear network analysis;modeling technique;nonlinear circuit simulations nonlinear driver modeling delay noise analysis galerkin finite element method logic block characterization static timing analysis piecewise linear driver models;finite elements method;nonlinear models;delay noise;logic design;timing analysis delay noise finite elements method noise analysis nonlinear models;noise integrated circuit modelling nonlinear network analysis logic design galerkin method finite element analysis timing;finite element method;65;piece wise linear;integrated circuit modelling;nonlinear circuits;timing analysis;static timing analysis;finite element analysis;noise analysis;timing driver circuits circuit noise performance analysis noise generators moment methods finite element methods logic circuits nonlinear circuits circuit simulation;noise;nonlinear model;timing	This paper presents a novel and flexible modeling technique to generate accurate linear and nonlinear driver models with applications in timing and noise analysis. The new technique, based on Galerkin's finite elements method, is very efficient because it relies on existing logic block characterization for timing, does not require additional nonlinear circuit simulations during modeling, and generates reusable models. The performance of the proposed modeling technique is exemplified in two different implementations: nonlinear driver models for delay noise analysis and piece-wise linear driver models for static-timing analysis.	electronic circuit simulation;finite element method;galerkin method;image noise;linear circuit;logic block;rise time;spice;simulation;static timing analysis;timing closure	Bogdan Tutuianu;Ross Baldick;Mark S. Johnstone	2004	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2004.835136	control engineering;electronic engineering;computer science;finite element method;control theory;mathematics;static timing analysis	EDA	26.839596542279683	52.95126605661178	120075
31388173409dcd03845272190ddc498d4b4706db	surface code continuous quantum error correction using feedback	error correction codes;lattices;slh parametrization surface code continuous quantum error correction quantum computing minimal 2 d qubit array measurement based feedback policy codeword fidelity quantum feedback network heisenberg picture quantum filtering formalism;quantum computing quantum mechanics lattices mathematical model error correction codes error correction couplings;quantum mechanics;error correction;mathematical model;couplings;quantum computing;quantum computing error correction	We investigated the use of feedback in a continuous error correction scheme for quantum computing based on the surface code. Using a minimal 2-D qubit array, we can implement a simple yet effective measurement-based feedback policy to preserve codeword fidelity and extend the timescale of coherent storage. The quantum feedback network was described in the Heisenberg-picture using quantum filtering formalism in conjunction with an efficient SLH parametrization.	code word;coherence (physics);error detection and correction;feedback;heisenberg picture;quantum computing;quantum error correction;qubit;semantics (computer science);toric code	Thien Nguyen;Charles D. Hill;Lloyd Christopher L. Hollenberg;Matthew R. James	2015	2015 54th IEEE Conference on Decision and Control (CDC)	10.1109/CDC.2015.7403339	quantum simulator;quantum operation;quantum information;error detection and correction;quantum probability;theoretical computer science;quantum network;quantum capacity;lattice;mathematical model;mathematics;quantum dissipation;quantum convolutional code;qubit;coupling;quantum channel;quantum computer;quantum process;quantum algorithm;one-way quantum computer;quantum phase estimation algorithm;quantum error correction	Visualization	38.29650410168416	59.93501058401516	120751
4859a14edd7de8017f25848f291a3205bac704a8	unified high-level synthesis and module placement for defect-tolerant microfluidic biochips	digital biochips;resource binding;automated design;clinical diagnostics;design automation;integrated circuit yield;unified high level synthesis;real life protein assay;placement;defect tolerant microfluidic biochips;simulated annealing;parallel recombinative simulated annealing;synthesis;biochip;high level synthesis;biosensing;permission;high level synthesis microfluidics biosensors microsensors;system integration;microfluidics;system level design;microfluidic array;droplet based microfluidic biochips;integrated circuit synthesis;circuit testing;high level synthesis microfluidics system level design integrated circuit synthesis biosensors integrated circuit reliability circuit testing permission integrated circuit yield design automation;module placement;defect tolerance;real life protein assay unified high level synthesis module placement defect tolerant microfluidic biochips biosensing clinical diagnostics droplet based microfluidic biochips operation scheduling resource binding digital biochips parallel recombinative simulated annealing microfluidic array;integrated circuit reliability;operation scheduling;microsensors;biosensors	"""Microfluidic biochips promise to revolutionize biosensing and clinical diagnostics. As more bioassays are executed concurrently on a biochip, system integration and design complexity are expected to increase dramatically. This problem is also identified by the 2003 ITRS document as a major system-level design challenge beyond 2009. We focus here on the automated design of droplet-based microfluidic biochips. We present a synthesis methodology that unifies operation scheduling, resource binding, and module placement for such """"digital"""" biochips. The proposed technique, which is based on parallel recombinative simulated annealing, can also be used after fabrication to bypass defective cells in the microfluidic array. A real-life protein assay is used to evaluate the synthesis methodology."""	3d modeling;biochip;electronic system-level design and verification;gnu nano;high- and low-level;high-level synthesis;level design;next-generation network;parallel computing;real life;scheduling (computing);simulated annealing;software bug;system integration;system on a chip	Fei Su;Krishnendu Chakrabarty	2005	Proceedings. 42nd Design Automation Conference, 2005.	10.1145/1065579.1065797	embedded system;electronic engineering;electronic design automation;computer science;engineering;biosensor	EDA	26.375564848125666	51.23614386023749	120946
f8f1e70c6a432f262fa92574883d3292e58d5357	statistical modeling and simulation of threshold variation under random dopant fluctuations and line-edge roughness	estensibilidad;analytical models;modelizacion;circuit design threshold variation random dopant fluctuation line edge roughness threshold voltage nanoscale transistor spice simulation statistical variation model etching process atomistic simulation device model leakage current saturation current gate size gate length fluctuation nonrectangular gate reverse narrow width effect postlithography gate geometry device output current;litografia;lithographie;postlithography gate geometry;metodo estadistico;diseno circuito;microelectronic fabrication;threshold variation;fabricacion microelectrica;variation atomistic simulation gate slicing line edge roughness non rectangular gate random dopant fluctuations threshold voltage;nanoelectronica;leakage current;voltage threshold;fluctuations;semiconductor process modeling;gate size;corriente escape;semiconductor doping;statistical variation model;computer model;rugosidad;compact design;concepcion compacta;circuit design;saturation current;random dopant fluctuation;gate slicing;programme spice;statistical method;transistor circuits circuit simulation etching leakage currents lithography mosfet nanoelectronics semiconductor device models semiconductor doping spice statistical analysis;spice simulation;statistical model;non rectangular gate;modelisation;grabado;lithography;circuit simulation;device output current;computational modeling;simulation methods;engraving;gate length fluctuation;etching process;nanoscale transistor;statistical analysis;atomistic simulation;random dopant fluctuations;courant fuite;conception compacte;leakage currents;methode statistique;roughness;threshold voltage;semiconductor device models;nanoelectronique;nanoelectronics;semiconductor process modeling fluctuations computational modeling spice predictive models threshold voltage analytical models lithography etching leakage current;transistor circuits;modele statistique;device model;rugosite;advanced technology;predictive models;mosfet;seuil tension	The threshold voltage (Vth) of a nanoscale transistor is severely affected by random dopant fluctuations and line-edge roughness. The analysis of these effects usually requires atomistic simulations which are too expensive in computation for statistical design. In this work, we develop an efficient SPICE simulation method and statistical variation model that accurately predict threshold variation as a function of dopant fluctuations and gate length change caused by lithography and the etching process. By understanding the physical principles of atomistic simulations, we: 1) identify the appropriate method to divide a nonuniform gate into slices in order to map those fluctuations into the device model; 2) extract the variation of Vth from the strong-inversion region instead of the leakage current, benefiting from the linearity of the saturation current with respect to Vth ; 3) propose a compact model of Vth variation that is scalable with gate size and the amount of dopant and gate length fluctuations; and 4) investigate the interaction with non-rectangular gate (NRG) and reverse narrow width effect (RNWE). The proposed SPICE simulation method is validated with atomistic simulation results. Given the post-lithography gate geometry, this approach correctly models the variation of device output current in all operating regions. Based on the new results, we further project the amount of Vth variation at advanced technology nodes, helping shed light on the challenges of future robust circuit design.	and gate;cmos;circuit design;computation;dopant;image scaling;mathematical optimization;molecular dynamics;multiprotocol label switching;spice;scalability;simulation;spectral leakage;statistical model;time complexity;transistor	Y. Ye;F. Liu;Min Chen;S. Nassif;Y. Cao	2011	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2010.2043694	lithography;electronic engineering;computer science;engineering;electrical engineering;predictive modelling	EDA	25.389868550296356	57.983245613197965	121395
583d5da41d815b535eb9b97b63390525d7f24a2c	representing small identically self-dual matroids by self-dual codes	partage secret;code lineaire;matroid;mathematiques discretes;aplicacion;secret sharing;codigo autodual;05b35;matematicas discretas;linear secret sharing scheme;self dual codes;94a62;schema partage secret;discrete mathematics;identically self dual matroids;generador;multiplicative linear secret sharing schemes;matroide;52b40;94b05;generator;linear code;code autodual;autodual code;complexite multiplicative;multiparty computation;application;generateur;reparto secreto;self dual code;codigo lineal	The matroid associated to a linear code is the representable matroid that is defined by the columns of any generator matrix. The matroid associated to a self-dual code is identically self-dual, but it is not known whether every identically self-dual representable matroid can be represented by a self-dual code. This open problem was proposed in [8], where it was proved to be equivalent to an open problem on the complexity of multiplicative linear secret sharing schemes. Some contributions to its solution are given in this paper. A new family of identically self-dual matroids that can be represented by self-dual codes is presented. Besides, we prove that every identically self-dual matroid on at most eight points is representable by a self-dual code.	column (database);dual code;dual matroid;generator matrix;linear code;matroid representation;secret sharing	Carles Padró;Ignacio Gracia	2005	IACR Cryptology ePrint Archive	10.1137/05064309X	matroid;combinatorics;discrete mathematics;graphic matroid;oriented matroid;linear code;mathematics;geometry;weighted matroid;matroid partitioning;secret sharing;electric generator;algorithm;statistics;algebra	Theory	37.13766813595946	53.39582817697741	121402
84626a829823db08b7ef38a75fe7c959f52dd984	application of regenerating codes for fault tolerance in distributed storage systems	regeneration data throughput code regeneration fault tolerance distributed storage systems special network coding technique fault tolerant storage systems data recovery single node failure coding theory regenerating codes reed solomon codes encoding;distributed processing;reed solomon codes;maintenance engineering;fault tolerant computing;distributed storage systems erasure coding;redundancy;erasure coding;distributed storage systems;encoding reed solomon codes redundancy maintenance engineering throughput bandwidth;reed solomon codes digital storage distributed processing encoding fault tolerant computing;bandwidth;digital storage;encoding;throughput	Recently, regenerating codes, a special network coding technique, were discovered for fault-tolerant storage systems with the promising advantage of efficient data recovery in the case of a single node failure and replacement (regeneration case). From the perspective of coding theory, regenerating codes are extensively studied, but there exists no reference on how to implement these codes in storage systems. We provide a comparison of Reed-Solomon codes and regenerating codes from an implementation point of view. The comparison includes the experimental evaluation of the encoding and the regeneration data throughput.	algorithmic efficiency;code;coding theory;computer data storage;data recovery;fault tolerance;linear network coding;overhead (computing);reed–solomon error correction;throughput	Kathrin Peter;Peter Sobe	2012	2012 IEEE 11th International Symposium on Network Computing and Applications	10.1109/NCA.2012.35	erasure code;maintenance engineering;throughput;parallel computing;real-time computing;online codes;fountain code;computer science;theoretical computer science;redundancy;bandwidth;encoding	OS	36.31583717399642	58.833077150344714	122170
1188053b93d7d32afc38c6aab088fa4a7ba045a3	hardware/software codesign of finite field datapath for low-energy reed-solomon codecs	television;software;traitement signal;satellite communication;energy latency reductions;degree reduction;basse energie;programmability;error correction codes;hardware software codesign;codecs;low power electronics hardware software codesign reed solomon codes codecs error correction codes digital signal processing chips scheduling;energy consumption hardware software codesign finite field datapath low energy reed solomon codecs error control coding digital audio digital tv software radio cd players satellite communications wireless communications domain specific digital signal processor programmability primitive polynomial heterogeneous digit serial approach multiply accumulate subarrays degree reduction subarrays finite field multiplications scheduling strategies energy latency reductions rs n k codes;etude theorique;low energy;logiciel;application software;systeme aide decision;signal design;baja energia;satellite communications;digital tv;finite field datapath;reed solomon codes;registro numerico;digital audio;rs n k codes;telecomunicacion via satelite;primitive polynomial;multiply accumulate subarrays;sistema ayuda decision;telecommunication par satellite;software radio;cd players;finite field;wireless communication;finite field multiplications;tratamiento numerico;hardware galois fields codecs application software reed solomon codes digital tv software radio wireless communication satellite communication signal design;programmable;decision support system;wireless communications;digital recording;heterogeneidad;energy consumption;code reed solomon;scheduling;error correction;senal numerica;signal processing;enregistrement numerique;low power electronics;error control coding;low energy reed solomon codecs;estudio teorico;reed solomon;signal numerique;radio communication;digital signal processor;codec;logicial;digital signal processing chips;digital processing;pgz reed solomon decoder;procesador;radiocommunication;codigo reed solomon;computer hardware	Reed–Solomon (RS) coders are used for error-control coding in many applications such as digital audio, digital TV, software radio, CD players, and wireless and satellite communications. Traditionally, RS coders have been implemented using dedicated hardware. This paper considers software-based implementation of RS codecs. Ahardware–software codesign approach is used to design the finite field datapath in a domain-specific digital signal processor (DSP) with low-energy RS codecs application in mind. These datapaths are designed to accommodate programmabilitywith respect to the primitive polynomial as well as the field degree . A novel heterogeneous digit-serial approach is proposed, where theheterogeneitycorresponds to the use of different digit sizes in the multiply-accumulate (MAC) and degree reduction (DEGRED) subarrays. The salient feature of this digit-serial approach is that only the digit cells are implemented in hardware and the finite field multiplications are performed digit-serially in software by dynamically scheduling the internal digit-level operations. Efficient scheduling strategies for digit-serial finite field multiplications are presented and applied to the design of low-energy high-performance RS codecs in software. Significant energy and energy-latency reductions can be achieved using the digit-serial datapaths, as compared with the traditional approach where a combined MAC-DEGRED (parallel multiplier) unit is used. It is concluded that for two-error-correcting RS( ) codes over finite field GF(2), datapath containing a parallel MAC unit (of digit size eight) and a DEGRED unit with digit size two (or four) leads to RS codecs with the least energy consumption and energy-latency products; with these datapath architectures and appropriate digit-serial scheduling strategies, more than 60% energy reduction and more than one-third energy-latency reduction can be achieved compared with the parallel multiplication datapath-based approach.	algorithm;code;codec;communications satellite;datapath;digital signal processor;error detection and correction;multiply–accumulate operation;primitive polynomial (field theory);reed–solomon error correction;scheduling (computing);signal processing	Leilei Song;Keshab K. Parhi;Ichiro Kuroda;Takao Nishitani	2000	IEEE Trans. VLSI Syst.	10.1109/92.831436	embedded system;electronic engineering;codec;finite state machine with datapath;parallel computing;decision support system;telecommunications;computer science;operating system;signal processing;finite field;reed–solomon error correction;communications satellite;algorithm;wireless	HPC	32.55984970277571	59.39486824526398	122335
d89382deb4b7a9fcaf6072817e10acb4ad88557d	synchronization of washing operations with droplet routing for cross-contamination avoidance in digital microfluidic biochips	biomedical science;routing microfluidics synchronization proteins design automation pharmaceutical technology clocks permission biochemistry molecular biophysics;reservoirs;design automation;droplet routing;clocks;routing;cross contamination avoidance;pharmaceutical technology;chip;synchronisation;bioassay;proteins;permission;synchronization;droplet based microfluidics;washing operation;microfluidics;transportation;molecular biophysics;synchronisation drops lab on a chip microfluidics;lab on a chip;contamination;digital microfluidic biochip;lab on chip;electrowetting;drops;biochemistry;bioassay synchronization washing operation droplet routing cross contamination avoidance digital microfluidic biochip biochemistry biomedical science;lab on chip droplet based microfluidics electrowetting	Digital microfluidic biochips are being utilized in many areas of biochemistry and biomedical sciences. Since cross-contamination between droplets of different biomolecules can lead to erroneous outcomes for bioassays, it is essential to avoid cross-contamination during droplet routing. We propose a wash-operation synchronization method to manipulate wash droplets to clean the residue that is left behind by sample and reagent droplets. We also synchronize wash-droplet routing with sample/reagent droplet-routing steps by controlling the arrival order of droplets at cross-contamination sites. The proposed method minimizes droplet-routing time without cross-contamination, and it is especially effective for tight chip-area constraints. A real-life application is used for evaluation.	real life;routing	Yang Zhao;Krishnendu Chakrabarty	2010	Design Automation Conference	10.1145/1837274.1837437	synchronization;electronic engineering;electronic design automation;lab-on-a-chip;telecommunications;computer science;engineering;molecular biophysics	EDA	26.54504337950689	51.2040597237185	123375
0e5a87e0532785b6aeb48344e431288d1e654dc7	two-party function computation on the reconciled data		In this paper, we initiate a study of a new problem termed function computation on the reconciled data, which generalizes a set reconciliation problem in the literature. Assume a distributed data storage system with two users A and B. The users possess a collection of binary vectors SA and SB, respectively. They are interested in computing a function φ of the reconciled data SA ∪ SB. It is shown that any deterministic protocol, which computes a sum and a product of reconciled sets of binary vectors represented as nonnegative integers, has to communicate at least 2n + n − 1 and 2n + n − 2 bits in the worst-case scenario, respectively, where n is the length of the binary vectors. Connections to other problems in computer science, such as set disjointness and finding the intersection, are established, yielding a variety of additional upper and lower bounds on the communication complexity. A protocol for computation of a sum function, which is based on use of a family of hash functions, is presented, and its characteristics are analyzed.	communication complexity;computation;computer data storage;computer science;hash function;sandy bridge;worst-case scenario	Ivo Kubjas;Vitaly Skachek	2017	2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton)	10.1109/ALLERTON.2017.8262764	discrete mathematics;computer science;theoretical computer science;mathematics;algorithm;statistics	Theory	35.50257534507632	58.01700144213039	124757
311bd36c5943d23c85cef5d8516c8310fd50f9d2	bck-algebras arising from block codes	bck algebras;block codes	In this paper, we will provide an algorithm which allows us to find a BCK-algebra starting from a given binary block code. BCK-algebras were first introduced in mathematics in 1966 by Y. Imai and K. Iseki, through the paper [4], as a generalization of the concept of set-theoretic difference and propositional calculi. The class of BCK-algebras is a proper sub-class of the class of BCI-algebras and there exist several generalizations of BCK-algebras as for example: generalized BCK-algebras [3], dual BCK-algebras [9] , BE-algebras [1], [8]. These algebras form an important class of logical algebras and have many applications to various domains of mathematics, such as: group theory, functional analysis, fuzzy sets theory, probability theory, topology, etc. For other details about BCK-algebras and about some new applications of them, One of the recent applications of BCK-algebras was given in the Coding Theory. In Coding Theory, a block code is an error-correcting code which encode data in blocks. In the paper [7], the authors constructed a finite binary block-codes associated to a finite BCK-algebra. At the end of the paper, they put the question if the converse of this statement is also true. In the present paper, we will prove that, in some circumstances, the converse of the above statement is also true.	algorithm;block code;coding theory;encode;error detection and correction;existential quantification;forward error correction;fuzzy set;propositional calculus;set theory;turing completeness	Cristina Flaut	2015	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-141469	block code;combinatorics;discrete mathematics;computer science;linear code;mathematics;group code;algebra	Theory	38.21708986525436	57.411771478551984	125121
eedd5263bbcbb42389dc8aaee8a82389bc0ed2bb	device interfacing: the weakest link in the chain to break into the giga bit domain?	second order;second order effects;peripheral interfaces;timing offsets;systematic error;paper technology;driver model;ate to device interactions;automatic test equipment;technology management;calibration radio frequency technology management test equipment guidelines bandwidth system testing hardware paper technology;eye diagrams;delay error;systematic errors;pin to pin dispersion;radio frequency;jitter error device interfacing gigabit digital i o buses signal delivery path second order effects systematic errors ate to device interactions ate accuracy error sources calibration level induced offsets driver model pin to pin dispersion eye diagrams timing offsets delay error;guidelines;signal delivery path;system testing;bandwidth;ate accuracy;test equipment;error sources;device interfacing;level induced offsets;timing jitter;calibration;gigabit digital i o buses;measurement errors;jitter error;hardware;timing jitter automatic test equipment peripheral interfaces measurement errors calibration	"""With approaching 1 Gb/s digital I/O buses and I/O margins in the range of 250 ps and less, the signal delivery path becomes increasingly more challenging to manage. Second order effects, systematic errors, and ATE to device interactions, which used to be """"noise level """" at 100 Mb/s rates, become more and more significant when attempting to make accurate measurements in the Gb/s domain. This paper provides an overview of the challenges, the magnitudes, the associated approaches, and practical methods to address them."""	gigabit	Ulrich Schoettmer;Chris Wagner;Tom Bleakley	2000		10.1109/TEST.2000.894312	embedded system;electronic engineering;real-time computing;telecommunications;engineering;electrical engineering;technology management;operating system;systematic error;statistics	Theory	27.795043283500622	54.18959046250982	125508
6eee0ebf0ee788a4ae404d5f7400e6266e50fe8b	fingerprinting schemes. identifying the guilty sources using side information	list;filigranage numerique;digital watermarking;list decoding;utilisation information;uso informacion;error correcting code;distance minimale;steganographie;lista;information use;codigo corrector error;information source;source information;tracing;minimal distance;steganography;empreinte digitale;esteganografia;error correction code;minimum distance;viterbi algorithm;filigrana digital;tracage;liste;fingerprint;huella digital;code correcteur erreur;side information;fuente informacion;distancia minima;trazado	In a fingerprinting scheme a distributor places marks in each copy of a digital object. Placing different marks in different copies, uniquely identifies the recipient of each copy, and therefore allows to trace the source of an unauthorized redistribution. A widely used approach to the fingerprinting problem is the use of error correcting codes with a suitable minimum distance. With this approach, the set of embedded marks in a given copy is precisely a codeword of the error correcting code. We present two different approaches that use side information for the tracing process. The first one uses the Guruswami-Sudan soft-decision list decoding algorithm and the second one a modified version of the Viterbi algorithm.	amiga walker;authorization;code word;decision list;embedded system;error detection and correction;fingerprint (computing);forward error correction;kinetic void;list decoding;roland gs;soft-decision decoder;traceability;virtual artifact;viterbi algorithm	Miguel Soriano;Marcel Fernandez;Josep Cotrina Navau	2005		10.1007/11551492_18	error detection and correction;telecommunications;computer science;theoretical computer science;algorithm;statistics	Theory	36.72824590505999	55.702942094496905	125860
295d7fc0df1c93422dcfb3ad950f8fec80cd0425	almost euclidean subspaces of ℓ1n via expander codes	compressed sensing;grupo de excelencia;51n20;linear constraint;error correction code;ciencias basicas y experimentales;matematicas;polynomial time;68p30;basis pursuit;68r05;bipartite graph	We give an explicit (in particular, deterministic polynomial time) construction of subspaces <i>X</i> ⊆ ℝ<i><sup>N</sup></i> of dimension (1 - <i>o</i>(1))<i>N</i> such that for every <i>x</i> ∈ <i>X</i>, {display equation}.  If we are allowed to use <i>N</i><sup>1/log log N</sup> ≤ <i>N</i>°<sup>(1)</sup> random bits and dim<i>(X)</i> ≥ (1 - <i>η</i>)<i>N</i> for any fixed constant η, the lower bound can be further improved to (log <i>N</i>)<sup>-°(1)</sup> √<i>N</i>||<i>x</i>||<sub>2</sub>. Our construction makes use of unbalanced bipartite graphs to impose local linear constraints on vectors in the subspace, and our analysis relies on expansion properties of the graph. This is inspired by similar constructions of error-correcting codes.	code;forward error correction;time complexity;unbalanced circuit	Venkatesan Guruswami;James R. Lee;Alexander A. Razborov	2007	Combinatorica	10.1007/s00493-010-2463-9	time complexity;combinatorics;discrete mathematics;error detection and correction;basis pursuit;bipartite graph;mathematics;geometry;compressed sensing	Theory	38.53424505160698	48.167611161554554	125906
1af51b4e9c34581c97661b24c71fe1f8a44e2f03	a variability-tolerant feedback technique for throughput maximization of trbgs with predefined entropy	sequences;random number generation;entropy;throughput maximization	In this paper a probabilistic feedback technique to maximize the throughput of a generic True Random Bit Generator (TRBG) circuit, under a given constraint on the entropy, is discussed. In the proposed solution, the throughput of the device is dynamically and adaptively varied by an on-line entropy detector, such to obtain, with an arbitrary confidence level, an entropy greater than a given worst-case value. The approach, which has a general validity, introduces a method for making maximum use of the TRBG random bit generation capabilities, maximizing the generation throughput while preserving its entropy. Differently from the classical ‘open loop’ TRBG design approach, in which the circuit parameter variability determines an uncertainty about the actual entropy of the device, with the proposed techniques the TRBG generation speed is varied under a given constraint on the entropy. The method can be applied to all those integrated TRBG circuits proposed in the literature and based on the uniform sampling of, e.g., random physical processes or chaotic dynamical systems.	autocorrelation;best, worst and average case;chaos theory;dynamical system;expectation–maximization algorithm;heart rate variability;online and offline;sampling (signal processing);spatial variability;throughput	Tommaso Addabbo;Massimo Alioto;Ada Fort;Santina Rocchi;Valerio Vignoli	2010	Journal of Circuits, Systems, and Computers	10.1142/S0218126610006505	entropy;mathematical optimization;binary entropy function;maximum entropy probability distribution;random number generation;theoretical computer science;sequence;mathematics;algorithm;conditional entropy;statistics	EDA	29.537358391816923	52.10351560737974	125998
55f51ab5732f7a1e2bdb0d622e9977cebde2928b	joint decoding of raid-ecc solutions for ssds		There are two independent layers of error correction mechanisms, i.e., error-correction codes (ECC) and redundant array of independent disks (RAID), to protect NAND-based Solid State Drives (SSDs) from serious noise/disturb. However, bit error rates of modern SSDs have increased to the point that more powerful ECCs or Maximal Distance Separate codes are only regarded as sufficient solutions. The theme of this work is to show that instead of designing new error correction mechanisms, it is possible to improve the NAND-based SSD reliability by designing a joint decoder exploring the inherent information between ECCs and RAIDs. We first show that it is possible to recover two-page failures using the inherent information with RAID 4/5; we then show that these ideas can be extended to RAID 6; finally, we conduct information theoretic study to explore how much information can be reliably stored with the help of RAID system in NAND-based SSD.	binary decoder;bit error rate;erasure code;error detection and correction;flash memory;forward error correction;maximal set;nand gate;solid-state drive;standard raid levels;stripes;theory	Ning Zheng;Qing Li	2017	2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton)	10.1109/ALLERTON.2017.8262769	error detection and correction;parallel computing;solid-state;standard raid levels;raid;computer science;distributed computing;nand gate;decoding methods	Theory	37.02540029504571	58.60520516838972	126084
f4cc6ec12fbb06249986d932e88ea6967cc0416a	time bounds on space computations	silicon;computers;electromagnetic scattering;minimization;object recognition;complexity theory;performance evaluation;time measurement;printers;input variables;interconnected systems;magnetic devices;geometry;surface roughness;distributed computing;delay effects;wires;size measurement;heating;interference;programmable logic arrays;physics computing;rough surfaces;upper bound;surface treatment;shape;logic gates;energy measurement;quantum mechanics;spatial distribution;distributed computing space technology physics computing interconnected systems capacitors magnetic cores size measurement time measurement computational efficiency tin;adders;capacitors;artificial intelligence;ink;parallel machines;space technology;vehicles;organizations;electrical engineering;quantum computing;magnetic cores;tin;computational efficiency;encoding;power transmission lines;resistance heating;space vehicles;noise;solids;spatial resolution;conductors	A physicomathematical basis is used to establish bounds TD(n) on the time needed to compute n-argument functions by spatially distributed primitive devices or composite systems D. The axioms used concern the speed, packing density, and noise threshold of the energy with which any computing device detects or alters the physical representation of information. The principal result is that TD(n) grows at least as n1/2. Composite systems consisting of spatially distributed identical components are examined in light of this bound. Inherent bounds on the computing time of n-argument functions are then combined with TD(n), resulting in a measure of computational efficiency which bounds computing time to processor size.	computer;set packing	Michael L. Dertouzos	1971	IEEE Transactions on Computers	10.1109/SWAT.1971.27	combinatorics;capacitor;image resolution;surface roughness;logic gate;shape;tin;computer science;organization;noise;theoretical computer science;cognitive neuroscience of visual object recognition;electrical conductor;solid;mathematics;interference;space technology;electric power transmission;silicon;upper and lower bounds;quantum computer;algorithm;adder;encoding;time	Visualization	30.71238277522139	52.928808486047856	126260
815cc293bb6f8abe6889e96bf270f2542c9a7228	ultrafast digital-optical arithmetic using wave-optical computing	time of flight;optical computing;high speed	We propose a method for implementing digital-optical arithmetic with high accuracy at extremely high speed. To this end we use the superposition of photons running through a passive network of simple optical components. All possible solution are realized in parallel by superposition. Therefore, the overall computing time can be reduced to the sum of the time of flight through a very short optical path and the time needed for input and output.	optical computing	Tobias Haist;Wolfgang Osten	2008		10.1007/978-3-540-85673-3_3	electronic engineering;real-time computing;computer science;theoretical computer science	NLP	30.887683707690115	56.58745228364605	127160
6dc5ee7b392d39f102d99a513d1195665d427456	inequalities between the probability of a subspace and the probabilities of its cosets	vector spaces;fourier transform;vector space;group theory;coding theory;probability distribution;coding;vector spaces coding group theory probability functions;probability functions;lower bound	We consider an n-dimensional vector space over GF(q) which has a probability distribution def ined on it. The sum of the prohabilities over a proper k-dimensional suhspace is compared to a sum over a coset of this subspace. The difference of these set probabilit ies is related to a sum of the Fourier transforms of the distribution over a subset of the domain of the transforms. W e demonstrate the existence of a coset and both an upper and a lower bound on the difference associated with this coset. The bounds depend on the maximum and nonzero minimum of the transforms as def ined on a special subset of the transform domain. Two examples from coding theory are presented. The first deals with a q-ary symmetric channel while the second is concerned with a binary compound channel. ET V be an n-dimensional vector space over GF(q) and L suppose that we are interested a proper k-dimensional subspace of V,G. Hence we will be assuming throughout that k < n. A probability distribution is defined on V which satisfies the following symmetry property : P(v) = P(-v), vu E v. (1) We wish to compare the probability of the subspace G ,P(G), with that of a coset of G ,P(c + G), where c is a coset leader; explicitly f’(G) = c P(g) (24 %EG P(c + G) = c P(c + g), c E v. (2b) %EG Such comparisons arise naturally in coding theory [ 11, [2] (see especially [2, section 3.41). An example is the use of the Slepian decoding scheme in linear codes [3]. Another comparison of this type is contained in a recent paper by Crimm ins and Horwitz [4]. The probability distributions in these cases are generally the channel transition probability distributions. Instead of considering the distribution directly, we will emp loy its Fourier transform. Towards that end, we will denote the character group of V by V* [5], [6]. Iff(v) is a complex-valued function on V, its transform is defined as J(x) = “~pM4~ x E v”. (34 The sum is over all q” elements of V and the bar denotes complex conjugation. The inverse transform is f(v) = f x~*fwx(v)~ v E v. (3b) Manuscript received March 24, 197?; revised October 7, 1972. This work was supported in part by the Natlonal Science Foundat ion under Grant GK-32706. The author is with the Department of Electrical Engineering, University of W isconsin, Madison, W is. 53706. Here the sum is over all q” elements of the character group V*. We will need to consider a special subgroup of V* which is associated with G . The annihilator of G will be designated by the symbol A*, and is defined as follows A* = {x E V*: x(g) = + 1, Vg E G}. (4)	a* search algorithm;code;coding theory;coset leader;electrical engineering;eurographics;kleene star;markov chain;quaternions and spatial rotation;slepian–wolf coding	G. Robert Redinbo	1973	IEEE Trans. Information Theory	10.1109/TIT.1973.1055035	combinatorics;mathematical analysis;discrete mathematics;probability vector;vector space;symmetric probability distribution;mathematics;group theory;statistics	Theory	38.7592908951799	51.08033958355189	127645
eec250b7a5d2a93ecbb6742a06b01774c23468d9	new micro channel features	data integrity;clocks master slave nanoscale devices encoding timing;electromagnetic interference;64 bit micro channel architecture streaming data procedure address data parity synchronous exception signaling card seating power disturbance electromagnetic interference chck signal 32 bit;computer interfaces	Three features of the Micro Channel architecture are discussed. These features broaden the characteristics of that architecture to support advanced I/O devices or systems. These new features are a streaming data procedure (including 64-b transfers), address and data parity, and synchronous exception signaling. Streaming data is a procedure that provides the ability to transfer multiple data cycles within one bus envelope. The procedure distributes the device selection overhead across the total packet, nearly doubling (for 32 b) or quadrupling (for 64 b) the performance capability of the Micro Channel bus. Address and data parity are provided to improve the data integrity characteristics of the Micro Channel architecture. These mechanisms are particularly well suited to detecting typical errors, such as card seating problems, power disturbance, and electromagnetic interference. The Micro Channel architecture provides for exception signaling with the CHCK signal. The enhancements use previously reserved pins on the Micro Channel bus and retain compatibility with existing card and system board designs.<<ETX>>	arcade system board;data integrity;emoticon;input/output;interference (communication);micro channel architecture;network packet;overhead (computing);period-doubling bifurcation;sensor;stream (computing);streaming media	James O. Nicholson;Fred E. Strietelmeier	1990	Digest of Papers Compcon Spring '90. Thirty-Fifth IEEE Computer Society International Conference on Intellectual Leverage	10.1109/CMPCON.1990.63670	embedded system;electronic engineering;real-time computing;engineering	Arch	25.001922594237513	52.03570141231277	128052
4538159caf66d563baa74e5be6e6384a40d1ef25	error drifting reduction in enhanced fine granularity scalability	reference frame;video coding;enhancement layer;fine granular scalable;maximum accumulated mismatch error drifting error reduction enhanced fine granularity scalability psnr gain fading mechanism base layer enhancement layer fading factors reset mechanism accumulation error fgs coding scalable video coding reference frame prediction;scalability fading bit rate mpeg 4 standard decoding psnr streaming media codecs displays error correction;base layer	— Rank 2nd among a total of 106 graduating majors in Institute of Electronics. — Elected as honorary member of Phi Tau Phi society, Taiwan, 1999. — Received dragon thesis award from Acer foundation, Taiwan, 1999. — Received excellent work award in the competition of application system of microcomputer design, hosted by EECS college of National Chiao-Tung University, Taiwan, 1998. — Master dissertation: An Efficient Algorithm and Architecture Design for 2-D Separable Discrete Wavelet Transform.	algorithm;computer science;discrete wavelet transform;microcomputer;scalability;xeon phi	Wen-Hsiao Peng;Yen-Kuang Chen	2002		10.1109/ICIP.2002.1039887	reference frame;real-time computing;telecommunications;computer science	ML	32.97461127103146	55.31805264226919	128254
d03df58f117aa67bea8c1bb7287c036fadf9124e	data link layer considerations for future 100 gbps terahertz band transceivers		This paper presents a hardware processor for 100 Gbps wireless data link layer. A serial Reed-Solomon decoder requires a clock of 12.5 GHz to fulfill timings constraints of the transmission. Receiving a single Ethernet frame on a 100 Gbps physical layer may be faster than accessing DDR3 memory. Processing so fast streams on a state-of-the-art FPGA (field programmable gate arrays) requires a dedicated approach. Thus, the paper presents lightweight RS FEC engine, frames fragmentation, aggregation, and a protocol with selective fragment retransmission. The implemented FPGA demonstrator achieves nearly 120 Gbps and accepts bit error rate (BER) up to . Moreover, redundancy added to the frames is adopted according to the channel BER by a dedicated link adaptation algorithm. At the end, ASIC synthesis results are presented including detailed statistics of consumed energy per bit.	data rate units;transceiver	Lukasz Lopacinski;Marcin Brzozowski;Rolf Kraemer	2017	Wireless Communications and Mobile Computing	10.1155/2017/3560521	real-time computing;telecommunications;computer science;operating system;computer network	Mobile	30.810067580007573	57.247176203612135	128526
aee3166d596bce58d38498b5e8361c1a92e1faf9	repair duality with locally repairable and locally regenerating codes		We construct an explicit family of locally repairable and locally regenerating codes whose existence was proven in a recent work by Kamath et al. about codes with local regeneration but no explicit construction was given. This explicit family of codes is based on HashTag codes. HashTag codes are recently defined vector codes with different vector length α (also called a sub-packetization level) that achieve the optimal repair bandwidth of MSR codes or near-optimal repair bandwidth depending on the sub-packetization level. We applied the technique of parity-splitting code construction. We show that the lower bound on the size of the finite field for the presented explicit code constructions can be lower than the one given in the work of Kamath et al. Finally, we discuss the importance of having two ways for node repair with locally regenerating HashTag codes: repair only with local parity nodes or repair with both local and global parity nodes. To the best of the authors' knowledge, this is the first work where this duality in repair process is discussed. We give a practical example and experimental results in Hadoop where we show the benefits of having this repair duality.	access time;apache hadoop;code;hashtag;input/output;locality of reference;mathematical optimization;xfig	Danilo Gligoroski;Katina Kralevska;Rune Erlend Jensen;Per Simonsen	2017	2017 IEEE 15th Intl Conf on Dependable, Autonomic and Secure Computing, 15th Intl Conf on Pervasive Intelligence and Computing, 3rd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)	10.1109/DASC-PICom-DataCom-CyberSciTec.2017.162	duality (optimization);euclidean vector;discrete mathematics;parity (mathematics);finite field;bandwidth (signal processing);mathematics;upper and lower bounds	Theory	36.14453249405314	58.1770743156429	128872
29a094c8176d58be14ce195a375e1eeb87d2719d	testing of high-speed dacs using prbs generation with “alternate-bit-tapping”	testing generators polynomials computer architecture simulation circuit faults analog memory;generators;circuit faults;simulation;testing;polynomials;specific test patterns;digital to analog converters;computer architecture;eye diagram measurement high speed dac prbs generation alternate bit tapping digital to analog converters high speed synchronized input signals specific test patterns;prbs generation;alternate bit tapping;digital to analog converter;specification tests;analog memory;test generation;digital analogue conversion;eye diagram measurement;high speed;high speed synchronized input signals;high speed dac;dynamic behavior	Testing of high-speed Digital-to-Analog Converters (DACs) is a challenging task, as it requires large number of high-speed synchronized input signals with specific test patterns. To overcome this problem, we propose use of PRBS signals with an “Alternate-Bit-Tapping” technique and eye-diagram measurement as a solution to efficiently generate the test-vectors and test the DACs. This approach covers all levels and transitions necessary for testing the dynamic behavior of the DAC completely, in minimum possible time. Circuit level simulations are used to verify its usefulness in testing a 4-bit 20-GS/s current-steering DAC.	4-bit;diagram;digital-to-analog converter;linear-feedback shift register;modulation;noise reduction;pseudorandom binary sequence;simulation;test card	Mohit Singh;Mahendra Sakare;Shalabh Gupta	2011	2011 Design, Automation & Test in Europe	10.1109/DATE.2011.5763066	embedded system;electronic engineering;real-time computing;computer science;software testing;polynomial	EDA	24.759786840028454	51.06056191871861	129268
7d48977211a2456b60b79802fd5caa10d2194f70	real-time reconfigurable linear threshold elements implemented in floating-gate cmos	ultra low power;ultra low voltage;building block;reconfigurable architectures;floating gate;real time;neural chips real time systems reconfigurable architectures threshold elements threshold logic cmos logic circuits digital simulation;indexing terms;threshold logic;power supply;chip;circuit testing computer simulation laboratories current measurement measurement standards design methodology wiring power measurement semiconductor device measurement power supplies;neural chips;low power;design method;cmos logic circuits;neural networks real time reconfigurable linear threshold elements floating gate cmos computer simulations reconfigurable uv programmable floating gate ultra low power potential mosfet wiring fguvmos power supply voltages linear threshold element;threshold elements;computer simulation;digital simulation;neural network;real time systems	This paper describes using theory, computer simulations, and laboratory measurements a new class of real-time reconfigurable UV-programmable floating-gate (FGUVMOS) linear threshold elements operating with current levels typically in the pA to /spl mu/A range, in standard double-poly 0.6 /spl mu/m CMOS, providing an ultra low-power potential. A new design method based on using the same basic two-MOSFET circuits extensively is proposed, meant for improving the opportunities to make larger FGUVMOS circuitry than previously reported. By using the same basic circuitry extensively, instead of different circuitry for basic digital functions, the goal is to ease UV-programming and test and save circuitry on chip and I-O-pads. Matching of circuitry should also be improved by using this approach. Compact circuitry can be made, reducing wiring and active components compared to previously reported FGUVMOS. 2-MOSFET circuits able to implement CARRY, NOR, NAND, and INVERT functions are demonstrated by measurements on chip, working with power supply voltages ranging from 800 mV down to 93 mV. The basic linear threshold element proposed is considered as a potential basic building block in neural networks.		Snorre Aunet;Yngvar Berg;Trond Sæther	2003	IEEE transactions on neural networks	10.1109/TNN.2003.816351	computer simulation;chip;embedded system;real-time computing;index term;design methods;computer science;artificial neural network	EDA	25.921420342962335	47.5642899987909	129554
adf2517d43cda580a9e9c02cfc4b9d5af760d46f	fpga implementation of digital chaotic cryptography	digital circuit;field programmable gate array;order statistic;analisis estadistico;funcion no lineal;modelo autorregresivo;statistique ordre;non linear function;red puerta programable;digital filter;reseau porte programmable;system realization;autoregressive model;fixed point;circuit numerique;fpga implementation;analisis regresion;statistical analysis;criptografia;cryptography;analyse statistique;estadistica orden;circuito numerico;fonction non lineaire;analyse regression;cryptographie;regression analysis;systeme chaotique;realizacion sistema;realisation systeme;modele autoregressif;hardware implementation;chaotic systems	In this paper, we present the digital chaotic cryptography implementation on FPGA. The system realization uses AR filter and modulo function as a non linear component. The hardware implementation of FPGA-based which consumed 288 CLBs has been successfully developed for 24 bits fixed point system using 4 order filter. The maximum clock frequency used in the experiment is 6.747 MHz.	clock rate;cryptography;field-programmable gate array;fixed point (mathematics);modulo operation;realization (systems)	Dewi Utami;Hadi Suwastio;Bambang Sumadjudin	2002		10.1007/3-540-36087-5_28	order statistic;digital filter;telecommunications;cryptography;mathematics;fixed point;autoregressive model;algorithm;regression analysis;statistics	EDA	31.691316349342998	47.82725264224609	129863
3911d968a29d028d0f88a19d9e0d1b974cc6c51c	bounds on the rate of linear locally repairable codes over small alphabets		Locally repairable codes (LRC) have recently been a subject of intense research due to the theoretical appeal and their application in distributed storage systems. In an LRC, any coordinate of a codeword can be recovered by accessing only few other coordinates. For LRCs over small alphabet (such as binary), the optimal rate-distance trade-off is unknown. In this paper we provide the tightest known upper bound on the rate of linear LRCs of a given relative distance, an improvement over the best known bound by Cadambe and Mazumdar.	clustered file system;code word	Abhishek Agarwal;Arya Mazumdar	2016	2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)	10.1109/ALLERTON.2016.7852349	combinatorics;discrete mathematics;mathematics;algorithm	Theory	35.98637309222398	58.328856024246036	130119
ad158801aff5130577f46d9c7ed7baae0bfdbc53	optimal index codes with near-extreme rates	directed graphs;optimisation;broadcast index coding network coding side information;optimisation directed graphs linear codes network coding;linear codes;circuit packing bound optimal scalar linear index codes side information icsi near extreme transmission rates near extreme min ranks digraphs optimal scalar linear solution np complete problem;network coding;indexes polynomials educational institutions encoding color receivers electronic mail	The min-rank of a digraph was shown to represent the length of an optimal scalar linear solution of the corresponding instance of the Index Coding with Side Information (ICSI) problem. In this paper, the graphs and digraphs of near-extreme min-ranks are studied. Those graphs and digraphs correspond to the ICSI instances having near-extreme transmission rates when using optimal scalar linear index codes. In particular, it is shown that the decision problem whether a digraph has min-rank two is NP-complete. By contrast, the same question for graphs can be answered in polynomial time. In addition, a circuit-packing bound is revisited, and several families of digraphs, optimal with respect to this bound, whose min-ranks can be found in polynomial time, are presented.	code;decision problem;directed graph;maxima and minima;np-completeness;set packing;time complexity	Son Hoang Dau;Vitaly Skachek;Yeow Meng Chee	2014	IEEE Transactions on Information Theory	10.1109/TIT.2013.2295331	mathematical optimization;combinatorics;discrete mathematics;linear network coding;directed graph;mathematics;statistics	Theory	37.72003178876299	55.883854005883094	130780
442b3ad153d8dc593f9e9db17dcd3ac68506fb03	the rhino platform: charging towards innovation and skills development in software defined radio	front end;pilot study;reconfigurable computing;software defined radio;physical layer;skills development;software defined radio sdr;learning platform;experimentation;reconfigurable hardware	This paper presents a pilot study on the development of a reconfigurable computing platform for use in prototyping Software Defined Radio (SDR) applications and building technical knowledge in this specialist area. SDR is becoming an increasingly popular approach for building experimental radio and radar systems, giving researchers significant flexibility in choosing bandwidths, modulation and other operational parameters traditionally fixed by front-end hardware. The SDR approach involves constructing and testing radio applications on reusable platforms, thereby reducing costs and time spent changing physical layer hardware. This paper discusses use of the Reconfigurable Hardware Interface for comptiNg and radiO (RHINO) platform as a hardware platform for novice engineers to develop SDR skills and to prototype radio systems.	etsi satellite digital radio;field-programmable gate array;modulation;prototype;radar;reconfigurable computing	Simon Winberg;Alan Langman;Simon Scott	2011		10.1145/2072221.2072271	embedded system;simulation;engineering;computer engineering	Mobile	29.882172937886967	59.91238107655935	131344
ee7c7bd9f3e3987ba259bfb0a073ecb5c13cceec	discovery of good double and triple circulant codes using multiple impulse method		Introduction The design of good codes is of fundamental importance in a communication system. Furthermore, finding good linear or nonlinear codes may affect the sphere packing problems in Euclidean spaces [1]. When the code rate is 1/2 or 1/3; Double or Triple Circulant Codes (DCC & TCC) have been an interesting family of codes with high minimum distances. It is still hard to determine the minimum distances of long binary codes as well as their asymptotic relative minimum distances [2,4]. Besides, Karlin [5] and Pless [3] found many good codes by systematic double circulant codes over GF(2) and GF(3) using quadratic residues respectively. In [6], Gaborit proposes a double circulant code scheme which generalizes the constructions of Karlin and Pless over any field and for any length n=pm, where p is an odd prime. Furthermore, Karlin considered binary circulant [3p+1, p+1] and [3p, p] codes using quadratic residues and nonresidues [5]. Recently artificial intelligence techniques were introduced to solve this problem. Among related works, one idea used Genetic Algorithms (GA) to design constant weight codes [7], another one used GA for searching the minimum distance of BCH code [8]. Lacan et al. [9] introduced Genetic algorithms in the search of optimal error correcting codes, and in [10], authors give new good DCC constructed by GA. Here, we propose two heuristic search methods such as Genetic Algorithm, and random search of DCC and TCC when we use the MIM method published in [11] in order to determine the minimum distance, and we give an improvement by introducing the multiple impulse error using genetic algorithm , which we call MIM-GA.	alexander horned sphere;anna karlin;artificial intelligence;bch code;binary code;circulant matrix;code rate;cyclic code;error detection and correction;genetic algorithm;heuristic;nonlinear system;quadratic residue;random search;set packing;software release life cycle	Mohamed Askali;Saïd Nouh;Ahmed Azouaoui;Mostafa Belkasmi	2015	CoRR		block code;mathematical optimization;theoretical computer science;linear code;mathematics;algorithm	Theory	38.035707108168054	52.33505213478264	132101
e0accf053716c4de054aa96fe3ac29aaafe676db	guest editorial: evolutionary advancements in wireless software defined radio systems		The field of wireless communications over the years has been constantly making significant advancements in the domain of Software Defined Radios (SDRs). These reconfigurable radios are also capable to learn from their environment like the spectrum and then undertake rapid decisions based on its selfdefined policies leading to the development of wider range of new applications. Thus, new topics and concepts are not only been investigated in the Digital Signal Processing domain but also in the Cognitive Radio domain for an effective and efficient use of the spectrum. Using such technologies with widely accepted international standards like the Software Communications Architecture (SCA) in military software defined radios also requires that they go through the testing and certification processes. Thus, in this Special Issue of the Springer Journal of Signal Processing Systems, all these topics leading towards the evolution of SDRs are discussed. This Special Issue comprises of three sets containing a total of nine articles. The first set of articles put focus on the various cognitive radio technologies like Dynamic Spectrum Access and Spectrum Sensing. The still existing inefficient use of the spectrum is discussed in various SDR research communities. New and innovative ideas are continuously investigated to use the spectrum in the most efficient way in order to meet the ever growing demands of high data throughput. Use of Cognitive Radios is one such way to achieve it. These radios are able to sense the radio environment in order to identify portions of the spectrum that are unused at a specific time or location and are able to reconfigure themselves. Various algorithms and techniques are being investigated to achieve such results in a dynamic environment. The second set of articles discusses several advanced Digital Signal Processing techniques. All these techniques aim at optimizing operationally relevant criteria. These criteria vary from achieving higher robustness, lower bit error rates, longer communication ranges, and minimal energy consumption to faster deployments. The proofs of the achievements are given either by simulation or by prototype implementation on off-the-shelf low cost hardware. The third set of articles illustrates the importance test and certification of Software Defined Radios in the military domain using internationally accepted standard like the SCA. Standards like the SCA allow waveforms to be ported easily across heterogeneous platforms. It also provides neatly defined interfaces and Application Program Interfaces (APIs) aimed to increase portability and interoperability of radios. In the final phase of the development of SDRs, efficient ways of testing and agencies providing the certification of standards play a significant role. * Marc Adrat marc.adrat@fkie.fraunhofer.de	algorithm;application programming interface;bit error rate;cognitive radio;digital signal processing;etsi satellite digital radio;interoperability;prototype;simulation;software communications architecture;springer (tank);throughput	Sarvpreet Singh;Marc Adrat;Dania Marabissi;Claudio Armani	2016	Signal Processing Systems	10.1007/s11265-016-1102-0	telecommunications;computer science;electrical engineering	Networks	30.078332681574953	59.978121711206434	132122
15cf03a911d9ad4027ec10460a3e5228169ed4d6	calculating error of measurement on high-speed microprocessor test	microprocessors;test process speed;pulp manufacturing;ma manufacturing site;process capability;sorting;measurement error calculation;ma manufacturing site high speed microprocessor test test process measurement error calculation test process speed sorting error industry s fastest microprocessor digital equipment corporation;manufacturing industries;digital equipment corporation;error analysis computer testing integrated circuit testing very high speed integrated circuits measurement errors;error analysis;mining industry;manufacturing processes;computer testing;electronics industry;integrated circuit testing;industry s fastest microprocessor;circuit testing;very high speed integrated circuits;process improvement;sorting error;integrated circuit reliability;high speed microprocessor test;velocity measurement;processing speed;velocity measurement microprocessors sorting circuit testing manufacturing processes manufacturing industries mining industry pulp manufacturing electronics industry integrated circuit reliability;test process;high speed;measurement errors	Accuracy and precision are desirable properties of any test process. Understanding test process capability can help ensure that high speed microprocessors are binned at their proper speed. This paper discusses a practical example of how a designed experiment was used to determine the test process speed sorting error of measurement (EoM) of the Alpha AXP, the industry’s fastest microprocessor, tested at Digital Equipment Corporation’s Hudson, M A manufacturing site. Knowing EoM allowed effective guardbands to be established to guarantee specified performance in the context of the supplier’s and consumer’s risks. A series of test process improvements resulted from the follow-up work suggested by the experiment. Introduction In the integrated circuit industry, a great deal of time and resources are devoted to qualifying a manufacturing process to ensure that devices work as designed and are reliable. However, this same qualification process is generally not applied to measurement systems; whatever the results are, we assume they are correct. In a perfect test process, good parts will test good, bad ones will test bad. Additionally, the good parts are often sorted into bins, typically based on their speed. Because the fastest parts command a premium price, flawless speed binning is also a goal of any test process. Ideally, a tester’s resolution should be at least an order of magnitude better than a device specification limit. However, today’s microprocessors are pushing the limits of test systems. After finding some speed sorting inconsistencies in our test process, we wanted to identify, in a logical and structured way, parameters that contribute most to the non-repeatability so that these inconsistencies could be eliminated. Also, we had “guardbands” on our specifications that were based on device characterization, not test process variation data. We needed to know how much our process should be guardbanded to guarantee customer requirements while minimizing scrap cost. This paper is an example of the typical procedure we use at Digital for determining significant process factors and for setting test process guardbands. Planning the Study Before starting the experiment, we spent a good deal of time determining its purpose, brainstorming which test process facl.ors, and iesponses to include in the study, reviewing the. logistical impllications of performing the experiment, and drafting a final report including what we expected the results to be. Planning is essential for performing a well-designed experiment. It ensures that the experimenter has fully understood the problem situation and priorities ;IS well as constraints (schedules, budgets, etc.). Experimental Purpose amid Factors The purpose of this experiment was to characterize the speed variations nnduced by the following test process factoirs (See Addendum A for rr;lossarv of statistical terms): a tester 41 test loadboard 41 test method Parts were selected to cover the specified speed range. In this paper, speeds are reported in % target value units, representing device CPU cycle time. “Operatory’ was not used as an experimental factor. The measured device speed was the experimental response. The a risk level for the experiment was chosen to be 5%, i.e., if two testers would, in the long run, provide equal values for speed, then the experiment would conclude that “the testers are riot equal” only 5% of the time. Performing the Experiment We foiundl that we needed to balance the statistical requirements with the corrstraints of reality. The experiment was repeated five tirnes. We refer to each complete set of combinations as a “round.” Within each round, first a tester (1 or 2) was randomly selected. Then the loadboard (1 or 2) was chosen at random. For each tester part INTERNATIONAL TEST CONFERENCE 1994 0-7803-21 02-2/94 $4.0	addendum;constraint (mathematics);cybernetics and human knowing;dec alpha;design of experiments;experiment;fastest;hudson;integrated circuit;logistics;microprocessor;product binning;randomness;repeatability;requirement;resolution (logic);schedule (computer science);sorting;system of measurement	Tamorah Comard;Madhukar Joshi;Donald A. Morin;Kimberley Sprague	1994		10.1109/TEST.1994.528026	embedded system;electronic engineering;mining;process capability;computer science;sorting;engineering;electrical engineering;manufacturing;observational error	SE	27.755487789232784	54.065512465822586	132639
cc70fd01c2239a9d095b02d8563d56a16c5c6ec6	implementing concurrent error detection in infinite-impulse-response filters	infinite impulse response iir concurrent error detection ced filter;finite impulse response filter registers fault tolerant systems redundancy circuit faults;costing;fault tolerance;programmable filters;fir filters;error detection;size 45 nm concurrent error detection infinite impulse response filter electronic circuit transient error radiation manufacturing variation fault tolerance digital filter iir filter finite impulse response filter programmable coefficient fault injection experiment cost estimation;programmable filters costing error detection fault tolerance fir filters iir filters;iir filters	Advanced electronic circuits suffer errors caused by multiple sources. For example, radiation can induce transient errors, and manufacturing variations can cause some devices to sporadically suffer errors. Fault tolerance is therefore an important issue in advanced electronic circuits. Digital filters are commonly used in many applications, and therefore, their protection against errors has been widely studied. However, infinite-impulse-response (IIR) filters have received little attention as most of the existing works focus on finite-impulse-response filters. In this brief, a technique to implement concurrent error detection in IIR filters with programmable coefficients is proposed and evaluated. The protection effectiveness is assessed through fault injection experiments that show that it can efficiently detect errors. The cost is estimated using the synthesis results for a 45-nm library. The results show that the area overhead is much lower than that of a duplicated system that can also detect errors.	coefficient;digital filter;electronic circuit;error detection and correction;experiment;fault injection;fault tolerance;finite impulse response;infinite impulse response;overhead (computing)	Pedro Reviriego;Oscar Ruano;Juan Antonio Maestro	2012	IEEE Transactions on Circuits and Systems II: Express Briefs	10.1109/TCSII.2012.2208676	adaptive filter;fault tolerance;electronic engineering;real-time computing;error detection and correction;digital filter;computer science;engineering;2d filters;finite impulse response;linear filter;control theory;mathematics;prototype filter;activity-based costing	EDA	25.34319076724014	52.19794915264032	133031
9d221022a522cb16c61903ae8534c721d4c7cfc8	impact of all-digital pll on soc testing	digitally assisted analog circuit testing all digital pll soc testing on chip measurement pll testing;phase locked loops delay phase frequency detector jitter clocks testing system on a chip;clocks;phase frequency detector;testing;system on a chip;phase locked loops;system on chip;circuit testing;jitter;system on chip circuit testing phase locked loops	This paper, as a case study and tutorial, discusses testing methods for general PLL features and their operating margin. These methods can be applied all for analog-, digital- and PW-PLLs. There are various kinds of on-chip measurement macros which can be applied for the PLL testing, and for the direction toward digitally assisted analog circuit testing, it is shown that a digitally controlled variable delay and a time to digital converter have a big possibility for PLL testing using only digital signals.	analogue electronics;metal gear solid: peace walker;phase-locked loop;time-to-digital converter	Toru Nakura;Tetsuya Iizuka;Kunihiro Asada	2012	2012 IEEE 21st Asian Test Symposium	10.1109/ATS.2012.22	system on a chip;embedded system;electronic engineering;real-time computing;computer science;engineering;pll multibit	SE	25.435755029132494	53.05098278296869	134952
664f272857572d587948dc054812e79b9d5ebc26	modular construction a lattices from cyclotomic fields and their applications in information security		We present an overview of recent advances in the Area of information security using algebraic number fields. This overview indicates the importance of modular lattices in information security and in recently proposed methods for obtaining modular lattices using algebraic number fields. Obtaining Construction a unimodular lattices using cyclotomic number fields of prime orders have been addressed in the literature. Recently, a new lattice invariant called secrecy gain has been defined and it has been shown that it characterizes the confusion at the eavesdropper when using lattices in the Gaussian wiretap channels. There is a symmetry point, called weak secrecy gain, in the secrecy function of modular lattices. It is conjectured that the weak secrecy gain is the secrecy gain. It is known that d-modular lattices with high level d are more likely to have a large length for the shortest nonzero vector, which results in a higher weak secrecy gain. In search of such lattices, we prove that there is no modular lattices built using Construction A over cyclotomic fields of prime power order $p^{n}$, with $n > 1$. We also present a new framework based on Construction A lattices and cyclotomic number fields that gives a family of p-modular lattices with $p\equiv 1 (\mathrm {m}\mathrm {o}\mathrm {d}~4)$.		Hassan Khodaiemehr;Daniel Panario;Mohammad-Reza Sadeghi	2018	2018 15th International ISC (Iranian Society of Cryptology) Conference on Information Security and Cryptology (ISCISC)	10.1109/ISCISC.2018.8546855	discrete mathematics;algebraic number;computer security;prime (order theory);public-key cryptography;computer science;lattice (order);invariant (mathematics);algebraic number field;unimodular matrix;prime power	Crypto	38.219458001830006	53.67898784752155	135285
7310b49086fbaf896975a48a934e45a7e4f4eca2	regulation of uniformly k-limited t0l systems		A content-addressable memory (CAM) has an array of four-transistor memory cells arranged in rows corresponding to stored words and columns corresponding to a selected search word. Complementary column lines couple signals associated with the bits of the search word to the memory cells associated with all of the stored words in parallel. The memory cells of each row are coupled to a common sense line and cause a current to flow on the sense line in response to the search word not matching the data word associated with that row. Writing is accomplished by discharging one of the sense lines and applying signals representative of the desired word to be stored to the column lines. Since the ground lines are not unique to any row, they can be shared between adjacent rows or columns as best suits the layout of the circuit. A status bit is associated with each stored word and is used to selectively activate the sense amplifier associated with each row. The status bit is responsive to the signal on sense line and a separate control line, thus, simple comparisons can be used to selectively activate sense amplifiers. Finally, a unique control circuit associated with the most significant bit allows a selected segment of the content-addressable memory to be activated.		Dietmar Wätjen	1994	Elektronische Informationsverarbeitung und Kybernetik		discrete mathematics;arithmetic;control line;most significant bit;row;word (computer architecture);mathematics;amplifier;sense amplifier	Logic	31.784535889336816	51.3917120741044	137897
1c2f08828bffa012ba9d58c426f1e460cab9c32b	secure cooperative regenerating codes for distributed storage systems	storage management precoding security of data;maintenance engineering decision support systems security bandwidth upper bound encoding resilience;minimum bandwidth cooperative regenerating mbcr codes;coding for distributed storage systems;minimum storage cooperative regenerating mscr codes;cooperative repair;minimum bandwidth cooperative regenerating code distributed storage system dss eavesdropper minimum storage cooperative regenerating code min cut analysis dss secrecy graph representation data precoding;security	Regenerating codes enable trading off repair bandwidth for storage in distributed storage systems (DSS). Due to their distributed nature, these systems are intrinsically susceptible to attacks, and they may also be subject to multiple simultaneous node failures. Cooperative regenerating codes allow bandwidth efficient repair of multiple simultaneous node failures. This paper analyzes storage systems that employ cooperative regenerating codes that are robust to (passive) eavesdroppers. The analysis is divided into two parts, studying both minimum bandwidth and minimum storage cooperative regenerating scenarios. First, the secrecy capacity for minimum bandwidth cooperative regenerating codes is characterized. Second, for minimum storage cooperative regenerating codes, a secure file size upper bound and achievability results are provided. These results establish the secrecy capacity for the minimum storage scenario for certain special cases. In all scenarios, the achievability results correspond to exact repair, and secure file size upper bounds are obtained using min-cut analyses over a suitable secrecy graph representation of DSS. The main achievability argument is based on an appropriate precoding of the data to eliminate the information leakage to the eavesdropper.	attack (computing);bandwidth (signal processing);clustered file system;code (cryptography);distributed computing;emoticon;fano's inequality;graph (abstract data type);information leakage;machine-readable dictionary;maxima and minima;minimum cut;negativity (quantum mechanics);passive attack;polynomial;social inequality;spectral leakage;table (database)	Onur Ozan Koyluoglu;Ankit Singh Rawat;Sriram Vishwanath	2014	IEEE Transactions on Information Theory	10.1109/TIT.2014.2319271	telecommunications;computer science;information security;distributed computing;computer security	HPC	35.629666662898494	58.97536024568832	137916
3a9fcdf252fde926bb191f8dbe172c0b0b920b67	gnu radio signal processing models for dynamic multi-user burst modems		This paper presents a modern method for implementing burst modems in GNU Radio. Since burst modems are widely used for multi-user channel access and sharing in non-broadcast radio systems, this capability is critical to the development of numerous waveforms in GNU Radio. We focus on making such systems easy to develop and adapt to wide classes of modems and computationally efficient at runtime. We use the GNU Radio Event Stream scheduler to demonstrate concise implementations of burst PSK and FSK modems in GNU Radio and compare this with alternate approaches which have been attempted in GNU Radio.	algorithmic efficiency;gnu radio;modem;multi-user;pre-shared key;radio broadcasting;run time (program lifecycle phase);scheduling (computing);signal processing	Timothy J. O'Shea;Kiran Karra	2016	CoRR		embedded system;real-time computing;telecommunications;computer science	Mobile	30.26820339625038	59.7652932655054	138568
735f0e3fb4423175b90a8ff6b0fcb2ed4e383e51	data integration in web data extraction system	web data extraction;data integrity	An instruction fetch unit that employs sequential way prediction. The instruction fetch unit comprises a control unit configured to convey a first index and a first way to an instruction cache in a first clock cycle. The first index and first way select a first group of contiguous instruction bytes within the instruction cache, as well as a corresponding branch prediction block. The branch prediction block is stored in a branch prediction storage, and includes a predicted sequential way value. The control unit is further configured to convey a second index and a second way to the instruction cache in a second clock cycle succeeding the first clock cycle. This second index and second way select a second group of contiguous instruction bytes from the instruction cache. The second way is selected to be the predicted sequential way value stored in the branch prediction block corresponding to the first group of contiguous instruction bytes in response to a branch prediction algorithm employed by the control unit predicting a sequential execution path. Advantageously, a set associative instruction cache utilizing this method of way prediction may operate at higher frequencies (i.e., lower clock cycles) than if tag comparison were used to select the correct way.		Marcus Herzog	2009		10.1007/978-0-387-39940-9_1161	data mining;database;information retrieval	NLP	31.76016630536435	51.34644996829983	138641
ad036e712ecc41c1d61638e0511d24025772d58c	high level spectral-based analysis of power consumption in dsps systems	temporal correlation;digital signal processing;signals spectral distribution;spectral analysis correlation methods gaussian distribution power consumption signal processing;transfer functions;transition activity;linear approximation;feedback circuits;spectrum;correlation methods;dsp systems;power optimization spectral analysis power consumption dsp systems temporal correlation transition activity signals spectral distribution transfer function;power engineering and energy;energy consumption;transfer function;signal processing;cmos logic circuits;conference report;power optimization;statistics;energy consumption digital signal processing cmos logic circuits feedback circuits statistics power engineering and energy power system modeling linear approximation transfer functions gaussian distribution;power consumption;correlation;spectral analysis;power system modeling;efficient estimation;correlation coefficient;power demand;gaussian distributions;gaussian distribution	In this paper, an efficient technique to evaluate temporal correlation and transition activity at high level in DSP systems is presented. The method is based on the spectral distribution of signals and has the advantage to require information well known by designer, simple and easy to apply even for complex circuits. With only the knowledge of the input signal spectrum and the transfer function from the input to the circuit nodes allows an efficient estimation of the correlation coefficients. A model for computing transition activity in terms only of spectral distribution is also developed. The spectral analysis gives new criteria to obtain power optimization. Experimental results validating the method are also included in this paper	coefficient;digital signal processor;high-level programming language;mathematical optimization;power optimization (eda);spectral density estimation;transfer function	Arindam Calomarde;Diego Mateo;Antonio Rubio	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1693063	control engineering;computer vision;mathematical optimization;electronic engineering;signal processing;mathematics;transfer function;maximum entropy spectral estimation;statistics	EDA	27.925539314304856	52.062275740199496	139106
1f737d1490b00d7e546cb69140f78f3a5b18b2d8	non-binary polar codes with channel symbol permutations	codes;polarisation;random processes;alphabet permutations;arbitrary q-ary input randomized channel polarization;channel randomization concept;channel symbol permutations;kernel equation;nonbinary polar codes;unitary polar codes;polar codes;channel polarization	The polar codes generated by the kernel equation are termed as unitary polar codes. It has been shown that q-ary unitary polar codes polarize arbitrary q-ary input channels if q is a prime number. However, this is in general not true if q is not a prime number. To achieve polarization for arbitrary q-ary input channels, the conventional approaches were to modify the kernel using permutations of alphabet inside the kernel, or assign nonunitary entries in the kernel if q is a prime power. In this paper, we propose a different approach by using the concept of channel symbol permutations. We show that unitary polar codes can still polarize arbitrary q-ary input channels if the code symbols from the unitary polar code are randomly mapped to the channel symbols before transmission. By using the channel randomization concept, we show that unitary polar codes polarize arbitrary q-ary input randomized channels. If q is a prime power, some sequences of fixed channel symbol permutations are given that, with unitary polar codes, guarantee polarization for arbitrary q-ary input channels.	converge;kernel (operating system);polar code (coding theory);polarization (waves);randomized algorithm;randomness	Mao-Ching Chiu	2014	2014 International Symposium on Information Theory and its Applications		discrete mathematics;permutation;computer science;polar;binary number;symbol;communication channel	Arch	37.96288888102335	56.71321533151728	139421
6ea5ae6f0a39a6dad3bb9457c836bf139427f47e	algorithmic issues in coding theory	code lineaire;error correcting code;algorithmique;codigo corrector error;optimisation combinatoire;error correction code;algorithmics;coding theory;algoritmica;error correction;linear code;hamming code;theorie information;combinatorial optimization;code correcteur erreur;reed solomon code;information theory;optimizacion combinatoria;codigo lineal;teoria informacion	The goal of this article is to provide a gentle introduction to the basic definitions, goals and constructions in coding theory. In particular we focus on the algorithmic tasks tackled by the theory. We describe some of the classical algebraic constructions of error-correcting codes including the Hamming code, the Hadamard code and the Reed Solomon code. We describe simple proofs of their error-correction properties. We also describe simple and e cient algorithms for decoding these codes. It is our aim that a computer scientist with just a basic knowledge of linear algebra and modern algebra should be able to understand every proof given here. We also describe some recent developments and some salient open problems.	algorithm;coding theory;computer scientist;error detection and correction;forward error correction;graph coloring;hadamard code;hamming code;linear algebra;reed–solomon error correction	Madhu Sudan	1997		10.1007/BFb0058031	error detection and correction;information theory;combinatorial optimization;computer science;theoretical computer science;mathematics;algorithmics;algorithm;statistics	Theory	38.722886006626176	56.03712751361558	139462
33eb4fe29f59b7adc9057c2165f29657a19b57e5	complexity of acceptors for prefix codes (corresp.)	decoding;prefix code;automata;decoding automata;word length	For a given finite set of messages and their assigned probabilities, Huffman's procedure gives a method of computing a length set (a set of codeword lengths) that is optimal in the sense that the average word length is minimized. Corresponding to a particular length set, however, there may be more than one code. Let L(n) consist of all length sets with largest term n , and, for any \ell \in L(n) , let {\cal S}( \ell) be the smallest number of states in any finite-state acceptor which accepts a set of codewords with length set \ell . It is shown that, for all n > 1 , n^{2}/(16 \log_{2} n) \leq max {\cal S}(\ell) \leq 0(n^{2}).  \ell \in L(n)	prefix code	Donna J. Brown;Peter Elias	1976	IEEE Trans. Information Theory	10.1109/TIT.1976.1055546	arithmetic;prefix code;discrete mathematics;computer science;mathematics;automaton;algorithm;statistics	Theory	38.57986281757759	53.71051434258564	139878
2aa306c2a8925a5e58c9132206bf71cfc601329b	a novel data processing circuit in high-speed serial communication	high speed serial interface;usb2 0;nrzi;serializer;parallel processing;bit stuffing	A novel data processing circuit in high-speed serial communication has been demonstrated in this work. The circuit, including a serializer and a frequency divider, was developed to process the format of data transmission. The chip design is based on TSMC 0.25μm mixed signal model, and semi-custom design methodology was used. After pre- and post-layout simulation, results indicated that the speed of circuit has reached 480MHz. Moreover, the data are processed properly in agreement with USB2.0 specification.	serial communication	Yongjian Tang;Lenian He;Xiaolang Yan	2005		10.1109/ASPDAC.2005.1466564	electronic engineering;parallel computing;computer hardware;computer science	EDA	30.078194862163933	55.991924460741586	140196
7e76c98b7601b679e596ee010b8b7db4b23b57c0	superior execution time design of a space/spatial-frequency optimal filter for highly nonstationary 2d fm signal estimation		Multiple-clock-cycle, signal adaptive, and fully pipelined hardware design of the optimal (Wiener) space/spatial-frequency (S/SF) filter is developed in this paper. All implementation and verification details, as well as the extensive comparative analysis, are provided. The developed solution optimizes critical design performances related to the hardware complexity, in line with multiple-clock-cycle nature. Variable (signal adaptive) number of clock cycles, taken within the execution in different S/SF points, provides this solution to retain the optimized time requirements, as well as high resolution, selectivity, and estimation quality of the corresponding recently proposed signal adaptive filtering solution. However, as the major contribution, the fully pipelined implementation enables the developed design to additionally improve the time required for execution. The achieved improvement corresponds to a clock cycle per each S/SF point performed within the estimation that results in the significant comparative improvement in execution time of up to 50% in terms of S/SF points lying outside the local frequency of the estimated 2D frequency-modulated signal. The implementation is tested on a highly nonstationary multicomponent signal and is verified by a field programmable gate array circuit design.	adaptive filter;circuit design;clock signal;fm broadcasting;field-programmable gate array;image resolution;modulation;performance;pipeline (computing);qualitative comparative analysis;requirement;run time (program lifecycle phase);selectivity (electronic)	Veselin N. Ivanovi&#x0107;;Nevena R. Brnovi&#x0107;	2018	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2018.2815723	field-programmable gate array;electronic engineering;time–frequency analysis;cycles per instruction;mathematics;spatial frequency;circuit design;convolution;adaptive filter;selectivity	EDA	31.295953594680906	57.72757571513405	140933
de531b106f2474e916f6ac477090cf11191d184b	systematic methodology for high-level performance modeling of analog systems	least squares approximations;kernel;support vector machines analogue integrated circuits circuit cad genetic algorithms high level synthesis integrated circuit design least squares approximations;high level performance modeling;support vector machines;analog systems;training;geometry;construction industry;high level analog synthesis performance modeling least squares support vector machine;high level analog synthesis;circuit level implementation;high level synthesis;integrated circuit design;analogue integrated circuits;computational modeling;support vector machines design methodology circuit simulation analog circuits space technology least squares methods topology design optimization optimization methods switches;transistor size;integrated circuit modeling;performance model;genetic algorithm;predictive models;genetic algorithms;circuit cad;transistor size high level performance modeling analog systems least squares support vector machine circuit level implementation genetic algorithm generalization;performance modeling;generalization;least squares support vector machine	This paper presents a systematic methodology for construction of high-level performance models using least squares support vector machine. The transistor sizes of the circuit-level implementation of a component block along with a set of geometry constraints applied over them define the sample space. Optimal values of the model hyper parameters are computed using genetic algorithm. The novelty of the methodology is that the models constructed with this methodology are accurate, fast to evaluate with good generalization ability and low construction time. The present methodology has been compared with two other standard methodologies and the novelties are clearly demonstrated with experimental results.	genetic algorithm;high- and low-level;least squares support vector machine;mathematical optimization;performance;simulation;transistor	Soumya Pandit;Chittaranjan A. Mandal;Amit Patra	2009	2009 22nd International Conference on VLSI Design	10.1109/VLSI.Design.2009.26	electronic engineering;genetic algorithm;computer science;theoretical computer science;machine learning	EDA	26.836892616988322	49.5272469492631	141244
c7bcfd6a7f388dd5a144d573a101dc86b9fa3389	modeling and evaluating errors due to random clock shifts in quantum-dot cellular automata circuits	quantum dot cellular automata;clocked qca;building block;standard deviation;quantum dot cellular automata qca;phase shift;emerging nanotechnologies;settore ing inf 01 elettronica;gaussian distribution;numerical simulation	This paper analyzes the effect of random phase shifts in the underlying clock signals on the operation of several basic Quantum-dot Cellular Automata (QCA) building blocks. Such phase shifts can result from manufacturing variations or from uneven path lengths in the clocking network. We perform numerical simulations of basic building blocks using two different simulation engines available in the QCADesigner tool. We assume that the phase shifts are characterized by a Gaussian distribution with a mean value of iπ2 , where i is the clock number and a standard deviation, σ, which is varied in each simulation. Our results indicate that the sensitivity of building blocks to phase shifts depends primarily on the layout while the performance of all building blocks starts to drop once the phase shifts in the clocking network are characterized by a standard deviation, σ ≥ 4◦. A full adder was simulated to analyze the operation of a circuit featuring a combination of the building blocks considered here. Results are consistent with expectations and demonstrate that the Cout output of the full adder is better able to withstand the phase shifts in the clocking network than the Sum output which features a larger combination of the simulated building blocks.	adder (electronics);carry (arithmetic);clock rate;columbia (supercomputer);computer simulation;electrical engineering;email;fan-out;numerical analysis;qualitative comparative analysis;quantum cellular automaton;quantum dot cellular automaton;two-phase locking	Faizal Karim;Marco Ottavi;Hamidreza Hashempour;Vamsi Vankamamidi;Konrad Walus;André Ivanov;Fabrizio Lombardi	2009	J. Electronic Testing	10.1007/s10836-008-5088-9	normal distribution;computer simulation;electronic engineering;theoretical computer science;mathematics;phase;standard deviation;algorithm;statistics	Metrics	27.753388064365975	56.16646570819482	141545
28a425d90c5d1b0fc179026de363d1dd497d3729	an automated passive analog circuit synthesis framework using genetic algorithms	automated passive analog circuit synthesis framework;genetic algorithms automated passive analog circuit synthesis framework;network synthesis;search space;filters;genetic programming;design space;circuit topology;analog circuits;circuit simulation;computational modeling;analog circuits circuit synthesis genetic algorithms rlc circuits circuit topology genetic programming filters production computational modeling circuit simulation;rlc circuits;production;genetic algorithm;genetic algorithms;analogue circuits;passive networks analogue circuits genetic algorithms network synthesis;analog circuit synthesis;passive networks;circuit synthesis	In this work, we present a genetic algorithm based automated circuit synthesis framework for passive analog circuits. A procedure is developed for the simultaneous generation of both the topology and the component values for analog circuits comprising of R, L and C elements, from a given set of specifications. The novelty of the work pertains to two distinct advantages when compared to previous evolutionary search techniques having similar objectives. First of all, the selection procedure chooses prospective parent circuits for mating on the basis of comparable fitness values. Secondly, the crossover process comprises of exchanging well-defined subcircuits, encompassing the whole design space, between the two chosen parent circuits. This minimizes the production of faulty offspring circuits. Experiments conducted on two filter specifications show that the techniques adopted bring about a reduction in the search space and help in faster attainment of the design goal.	active galactic nucleus;analogue electronics;experiment;genetic algorithm;low-pass filter;prospective search;software release life cycle	Angan Das;Ranga Vemuri	2007	IEEE Computer Society Annual Symposium on VLSI (ISVLSI '07)	10.1109/ISVLSI.2007.22	electronic engineering;computer science;machine learning;algorithm	EDA	26.48941304595059	48.69845050342893	141601
bd1c150de6e6071da96e8a53b14a26858d65d6ff	some techniques for using pseudorandom numbers in computer simulation	simulation;boolean algebra;bit manipulation;random numbers;computer simulation	An alogorithm is described by which uniform pseudorandom integers may be used to construct binary “numbers” in which the probability that each bit in the word is a 1-bit and can assume any desired parameter value. Techniques for making use of such “numbers” in simulation programming are described.	1-bit architecture;algorithm;computer simulation;pseudorandomness	T. Donnelly	1969	Commun. ACM	10.1145/363156.363174	computer simulation;boolean algebra;discrete mathematics;bit manipulation;computer science;theoretical computer science;programming language;pseudorandom generator theorem;algorithm;simulation language	Theory	31.733627860283026	46.66142332228168	142309
34ae9e9b2c6c5a3f9faeff4d33fc88232c94e212	the impact of miller and coupling effects on single event transient in logical circuits	harden;期刊论文;single event transient;delay time;digital circuits;miller feedback;coupling effect;analytical model	With feature size scaling down, Miller feedback effects of gate-to-drain capacitance for transistors and coupling effects between interconnects will dramatically affect single event transient (SET) generation and propagation in combinational logic circuits. Two ways of ICs are arranged: linear and ''S'' types. For pulse width and delay time, SET propagations in two layouts of digital circuits are compared under considering the coupling effects between interconnects. An analytical model is used to describe the impact of Miller and coupling effects on SET propagation. A criterion for SET occurrence in digital circuits with effects of coupling and Miller feedback is presented. The influence of temperature and technology node on SET generation and propagation is analyzed. The results indicate that (1) the existence of these effects will improve the critical charge for SET generation and also reduce the estimated SER, and (2) the way of ''S'' type is more immune to SET than the scheme of linear.		Baojun Liu;Li Cai;Xiaokuo Yang;Hongtu Huang;Peng Bai;Weidong Peng	2012	Microelectronics Journal	10.1016/j.mejo.2011.11.002	electronic engineering;computer science;electrical engineering;digital electronics;algorithm	EDA	24.758333363392335	52.660425419429096	142731
487deb149412dd95dd64a9483de58b6fe0c6da9d	update-efficient error-correcting product-matrix codes	error correcting mbr code;generators;error correction codes;complexity theory;decoding;data reconstruction;reed solomon codes error correction codes matrix algebra product codes;reed solomon codes;product matrix codes;data reconstruction update efficient error correcting product matrix code distributed storage system error correcting mbr code error correcting regenerating code error correcting msr code product matrix msr code product matrix mbr code reed solomon code rs code vandermonde matrix encoding scheme decoding scheme;distributed storage;product matrix mbr code;maintenance engineering;distributed storage system;matrix algebra;update efficient error correcting product matrix code;vandermonde matrix;product matrix codes distributed storage regenerating codes reed solomon codes decoding;decoding generators encoding complexity theory bandwidth maintenance engineering error correction codes;bandwidth;product matrix msr code;error correcting msr code;encoding;encoding scheme;reed solomon rs codes;decoding scheme;reed solomon code;error correcting regenerating code;rs code;regenerating codes;product codes	Regenerating codes provide an efficient way to recover data at failed nodes in distributed storage systems. It has been shown that regenerating codes can be designed to minimize the per-node storage (called MSR) or minimize the communication overhead for regeneration (called MBR). In this work, we propose new encoding schemes for error-correcting MSR and MBR codes that generalize our earlier results on error-correcting regenerating codes. General encoding schemes for product-matrix MSR and MBR codes are derived such that the encoder based on Reed-Solomon (RS) codes is no longer limited to the Vandermonde matrix proposed earlier. Furthermore, MSR codes and MBR codes with the least update complexity can be found. A decoding scheme is proposed that utilizes RS codes to perform data reconstruction for MSR codes. The proposed decoding scheme has better error correction capability and incurs least number of node accesses when errors are present. A new decoding scheme is also proposed for MBR codes that is more capable and can correct more error-patterns. Simulation results are presented that exhibit the superior performance of the proposed schemes.	clustered file system;code (cryptography);encoder;error detection and correction;microsoft research;overhead (computing);reed–solomon error correction;simulation;vandermonde matrix	Yunghsiang Sam Han;Hung-Ta Pai;Rong Zheng;Pramod K. Varshney	2015	IEEE Transactions on Communications	10.1109/TCOMM.2015.2424416	block code;arithmetic;maintenance engineering;vandermonde matrix;concatenated error correction code;turbo code;online codes;fountain code;sequential decoding;computer science;theoretical computer science;serial concatenated convolutional codes;bcjr algorithm;tornado code;linear code;luby transform code;mathematics;berlekamp–welch algorithm;error floor;reed–solomon error correction;bandwidth;algorithm;encoding;statistics	DB	36.573797924846446	58.93631745954668	143651
acfb46b735c947dc0e94afb030e9d068b1302fd2	an fpga application with high speed serial transceiver running at sub nominal rate	logic design;physical layer;phase locked loops;phase locked loops field programmable gate arrays logic design local area networks transceivers;transmission systems field programmable gate arrays high speed serial transceiver physical layer circuit ethernet system phase locked loop circuit data recovery blind oversampling;transceivers;field programmable gate arrays;high speed;field programmable gate arrays transceivers optical receivers high speed optical techniques circuits optical devices ethernet networks optical design physical layer phase locked loops;local area networks	"""We describe an implementation of the physical layer circuit for 100Mb/s optical Ethernet using an FPGA device with embedded high-speed serial transceivers. The design is the foundation for implementing a dual-speed 100/1000 Mb/s Ethernet system in which all components except the optical ones are implemented in an FPGA. The low-speed mode is outside the transceiver's nominal range, so the PLL circuit in the receiver cannot reliably lock to the received clock. We solve the problem by using the data recovery technique known as blind oversampling. The design can easily be adapted to implement other transmission systems that may put the operating point of the transceiver outside its nominal range, extending the scope of applications in which """"FPGA to the optics"""" approach is viable."""	bit-level parallelism;data rate units;data recovery;embedded system;field-programmable gate array;image-line fl studio;network packet;operating point;oversampling;phase-locked loop;protocol stack;transceiver	Dusan Suvakovic;Ilija Hadzic	2005	International Conference on Field Programmable Logic and Applications, 2005.	10.1109/FPL.2005.1515727	local area network;embedded system;logic synthesis;real-time computing;phase-locked loop;synchronous ethernet;computer science;physical layer;field-programmable gate array;transceiver	EDA	30.89975521384058	56.73896443226571	144063
3f64d44e37d97d7adeb8d7f83cf93cca79b88d30	parity check codes for logic processors	parite;code reed muller;reed muller code;circuito logico;parity;circuit logique;paridad;procesador;code;processeur;logic circuit;codigo;processor			Rabab Kreidieh Ward	1986	Comput. J.	10.1093/comjnl/29.1.12	arithmetic;reed–muller code;logic gate;parity;theoretical computer science;code;algorithm;statistics	Theory	30.31124732163225	48.59995066773039	144180
e333389c56ba47120524b893c342b969f5d31341	a computer-aided investigation on the fundamental limits of caching		We present our recent effort, in the context of the caching systems, in developing a computer-aided approach in the investigation of information systems. Yeung's linear programming (LP) outer bound of the entropy space is our starting point, however our effort goes significantly beyond using it to prove information inequalities. A symmetry-reduced linear program is used to identify the boundary of the memory-transmission-rate tradeoff for several simple cases, for which we can obtain a set of tight outer bounds. General hypotheses on the optimal tradeoff are formed from these computed data, which are then analytically proved. This leads to a complete characterization of the optimal tradeoff for caching systems with only two users, and a partial characterization for systems with only two files. Next, we show that by carefully analyzing the joint entropy structure of the outer bounds for certain cases, a novel code construction can be reverse-engineered, and unachievability of linear codes can be proved for some other cases. Finally, we show that strong outer bounds can be computed through strategically relaxing the LP, which allows us to compute outer bounds for larger problem cases, despite the seemingly impossible computation scale.	artificial intelligence;cache (computing);cluster analysis;code;computation;computer data storage;domain-specific language;information system;information theory;joint entropy;linear programming;machine learning;reinforcement learning;reverse engineering;time complexity	Chao Tian	2017	2017 IEEE International Symposium on Information Theory (ISIT)	10.3390/e20080603	inequality;discrete mathematics;combinatorics;joint entropy;computer-aided;information system;computation;mathematical optimization;linear programming;mathematics	Theory	35.24519188616049	58.10406060480998	144650
a0d8732a0318e8a95782512b21235abbbfef7c89	interconnect capacitance extraction for system lcd circuits	system lcd;capacitance extraction;pattern matching;interconnect capacitance	This paper discusses interconnect capacitance extraction for system LCD circuits, where coupling capacitance is much significant since a ground plane locates far away unlike LSI interconnects. We focus on a pattern matching method with interpolation to implement an accurate and efficient capacitance extraction system, and present good implementations that are suitable for system LCD circuits. To reduce computational cost, interconnect structures are spatially divided into several sub-regions considering capacitance coupling range, and analyzed in each sub-region using a capacitance database pre-characterized by a 3-D field solver. This paper evaluates tradeoff curves between characterization cost and extraction accuracy for four division methods in lattice structures that are basic and common structures in LCD driver circuits. Experimental results reveal efficient division methods for accurate capacitance extraction.	algorithmic efficiency;electrical connection;electromagnetic field solver;interpolation;pattern matching	Yoshihiro Uchida;Sadahiro Tani;Masanori Hashimoto;Shuji Tsukiyama;Isao Shirakawa	2005		10.1145/1057661.1057700	embedded system;electronic engineering;parasitic capacitance;computer science;engineering;electrical engineering;pattern matching;programming language	EDA	29.05091271819965	50.77943119759642	144959
690a798b8cc9f8845db351e99ac2491610005ac3	reply to 'comments on integer sec-ded codes for low power communications'	safety security in digital systems;desciframiento;communication system;nombre entier;procesamiento informacion;algorithm analysis;decodage;decoding;procedimiento;time complexity;complexite calcul;correction erreur;temps lineaire;safety systems;complejidad lineal;tiempo lineal;linear complexity;potencia;systeme numerique;arithmetic code;systeme communication;codigo aritmetico;integer;codificacion;complejidad computacion;complexite temps;digital system;low power;computational complexity;informatique theorique;error correction;digital systems;entero;linear time;information processing;coding;systeme securite;integer sec ded codes;sistema numerico;puissance;analyse algorithme;code arithmetique;correccion error;complejidad tiempo;traitement information;communication;comunicacion;complexite lineaire;power;analisis algoritmo;codage;procedure;computer theory;informatica teorica	This paper is a reply to the comments on u0027Integer SEC-DED codes for low power communicationsu0027.	computer security;dedicated hosting service;hamming code	Aleksandar Radonjic;Vladimir Vujicic	2010	Inf. Process. Lett.	10.1016/j.ipl.2010.04.004	block code;list decoding;time complexity;concatenated error correction code;turbo code;information processing;telecommunications;computer science;serial concatenated convolutional codes;linear code;mathematics;error floor;algorithm	Web+IR	39.088169144318016	56.649631155560364	145289
a2437813cf1c01e5700c59c31ce0bcb9d21c2923	wom codes against inter-cell interference in nand memories	nand flash memory;write once storage;flash memory;write once storage flash memories interference nand circuits;high density;diamond wom codes write once memories intercell interference nand flash memories parasitic effects cell to cell interference rewriting capacity delta wom codes;interference;nand circuits;inter cell interference;interference ash logic gates programming channel coding vectors;flash memories	Flash memories, especially NAND flash memories, are prevalent in the memory market due to their outstanding features such as high density and so on. However, the problems coming with the density are the parasitic effects, of which the cell-to-cell interference is the dominant one. Besides interference, another challenge for NAND flash memories is its limited life time. Actually, flash memories can be regarded as “write-once memories (WOM)”, where cells can be changed from lower states to higher states, not vice versa. In this paper, we study WOM codes against cell-to-cell interference. We derive bounds of the rewriting capacity of WOM codes based on the new WOM codes, Delta-WOM, and constrained codes. We also explore efficient WOM code constructions: one construction is based on our Diamond-WOM codes construction, which can be proven to approach its known rewriting capacity; the other one is based on constrained codes.	adobe flash;code;flash memory;interference (communication);rewriting;write once, run anywhere	Qing Li	2011	2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton)	10.1109/Allerton.2011.6120334	electronic engineering;parallel computing;computer hardware;computer science	Theory	37.44445832293191	58.88025983919276	145428
be6919e80eb050c7f242c0331015aebb138dcadb	theoretical modeling and simulation of phase-locked loop (pll) for clock data recovery (cdr)		Modern communication and computer systems require rapid (Gbps), efficient and large bandwidth data transfers. Agressive scaling of digital integrated systems allow buses and communication controller circuits to be integrated with the microprocessor on the same chip. The Peripheral Component Interconnect Express (PCIe) protocol handles all communcation between the central processing unit (CPU) and hardware devices. PCIe buses require efficient clock data recovery circuits (CDR) to recover clock signals embedded in data during transmission. This paper describes the theoretical modeling and simulation of a phase-locked loop (PLL) used in a CDR circuit. A simple PLL architecture for a 5 GHz CDR circuit is proposed and elaborated in this work. Simulations were carried out using a Hardware Description Language, VerilogAMS. The effect of jitter on the proposed design is also simulated and evaluated in this work. It was found that the proposed design is robust against both input and VCO jitter. ABSTRAK: Sistem komunikasi dan komputer moden memerlukan pemindahan data yang cekap (Gbps), dan bandwidth yang besar. Pengecilan agresif menggunakan teknik sistem digital bersepadu membenarkan bas dan litar pengawal komunikasi disatukan dengan mikroprocessor dalam cip yang sama. Protokol persisian komponen sambung tara ekspres (PCIe) mengendalikan semua komunikasi antara unit pemprosesan pusat (CPU) dan peranti perkakasan. Bas PCIe memerlukan litar jam pemulihan data (CDR) yang cekap untuk mendapatkan kembali isyarat jam yang tertanam dalam data semasa transmisi. Karya ini menerangkan teori pemodelan dan simulasi gelung fasa terkunci (PLL) untuk CDR. Rekabentuk 5 GHz PLL yang mudah telah dicadangkan dalm kertas kerja ini. Simulasi telah dijalankan menggunakan perisian verilog-AMS. Simulasi mengunnakan kesan ketar dalam reka bentuk yang dicadangkan telah dinilai. Reka bentuk yang dicadangkan terbukti teguh mengatasi ganguan ketar di input dan VCO.	arnold tongue;bus (computing);central processing unit;clock recovery;computer simulation;data rate units;data recovery;embedded system;fasa studio;frequency band;front-end processor;hardware description language;image scaling;jam;low-pass filter;microprocessor;pci express;peripheral;phase-locked loop;rlc circuit;shamash;transfer function;verilog;verilog-ams;voltage-controlled oscillator;yang	Zainab Ashari;Anis Nurashikin Nordin	2012	CoRR		embedded system;electronic engineering;computer hardware;telecommunications;computer science	EDA	29.574717039362724	55.3942167082577	145853
c24563612d6ac8c205088f11217e9b33bb3eb5e0	an efficient estimation method of dynamic power dissipation on vlsi interconnects	metodo analitico;interconnection;capacitancia;tiempo subida;integrated circuit;dissipation energie;estudio comparativo;error relativo;circuit vlsi;programme spice;modele ordre reduit;circuito integrado;energy dissipation;temps montee;rise time;interconexion;etude comparative;relative error;vlsi circuit;analytical method;power dissipation;modelo orden reducido;interconnexion;comparative study;erreur relative;reduced order model;methode analytique;disipacion energia;capacitance;circuito vlsi;efficient estimation;spice;circuit integre;capacite electrique;timing	Up to the present, there have been many works to analyze interconnects on timing aspects, while less works have been done on power aspects. As resistance of interconnects and rise time of signals decrease, power dissipation associated with interconnects is ever-increasing. Hence, an efficient method to compute power dissipation on interconnects is necessary and in this paper we propose a simple yet accurate method to estimate dynamic power dissipation on interconnects. We propose a new reduced-order model to estimate power dissipation on large interconnects. Through the proposed model which is directly derived from total capacitance and resistance of interconnects, we show that the dynamic power dissipation on whole interconnects can be approximated, and propose an analytic method to compute the power dissipation. The results of the proposed method applied to various RC networks show that maximum relative error is within 7% in comparison with HSPICE results.	approximation algorithm;approximation error;cmos;electrical connection;rise time;spice 2;simulation;very-large-scale integration	Joong-ho Park;Bang-Hyun Sung;Seok-Yoon Kim	2006		10.1007/11802839_10	telecommunications;computer science;electrical engineering;dissipation	EDA	26.165040276420633	56.0247212460378	145913
1633c510ae68efd82506f9ffa44274aa3bcc4635	modeling the analog circuit design feature variety	transfer functions;circuit topology;analog circuits;abstracts;integrated circuit modeling;couplings;transfer functions couplings abstracts algorithm design and analysis integrated circuit modeling analog circuits circuit topology;algorithm design and analysis	This paper presents a method to model the design feature variety in analog circuits, such as the various topological structures present in a set of circuits devised for the same purpose (like OpAmps or OTAs). The insight is important to characterize the novel and similar features in a circuit, the conditions under which existing design features can be reused in new circuits, and the exploration of new conceptual designs. The paper introduces the four basic operators used in modeling: circuit comparison, circuit instantiation-abstraction, circuit combination, and design feature induction. A case study presents the circuit feature variety modeling for a set of modern OpAmps.	analogue electronics;circuit design;operational amplifier;universal instantiation	Cristian Ferent;Alex Doboli	2013	Proceedings of the 2013 Forum on specification and Design Languages (FDL)		equivalent circuit;mixed-signal integrated circuit;topology;physical design;algorithm design;electronic circuit;electronic circuit design;asynchronous circuit;analogue electronics;computer science;circuit design;linear circuit;design layout record;transfer function;coupling;circuit extraction;electronic circuit simulation;discrete circuit;integrated circuit design	EDA	26.095647578683646	49.466599750749765	146014
e8606ef986ce1dd43d2da190724c5f690de1f3d0	heterogeneous multi-core architecture for a 4g communication in high-speed railway	telecommunication power management 4g mobile communication broadband networks energy consumption field programmable gate arrays intelligent transportation systems long term evolution mimo communication mobility management mobile radio ofdm modulation railway communication telecommunication network reliability;ofdm computer architecture program processors long term evolution field programmable gate arrays modulation;long term evolution;computer architecture;ofdm;field programmable gate array fpga intelligent transportation systems its high speed railway hsr multiple input multiple output mimo long term evolution lte orthogonal frequency division multiplexing ofdm;field programmable gate arrays;xilinx zynq fpga platform 4g communication reliability heterogeneous multicore architecture high speed railway high mobility intelligent transportation system high mobility its broadband services 4g long term evolution standard hsr communication system users orthogonal frequency division multiplexing modulation embedded lte mimo ofdm system low energy consumption high performance processing field programmable gate array;program processors;modulation	The fast development of high-speed railway (HSR), as a high-mobility intelligent transportation system (ITS), and the growing demand of broadband services for HSR users, introduce new challenges to wireless communication systems. 4G Long Term Evolution (LTE) standard has been widely used to satisfy the HSR communication system needs. The key part of 4G LTE standard is the Orthogonal Frequency Division Multiplexing (OFDM) modulation. In order to achieve a reliable communication and meet the demands of high performance processing and low energy consumption of HSR, we propose a flexible heterogeneous multi-core architecture for embedded LTE MIMO-OFDM system using Field Programmable Gate Array (FPGA). In this paper, different multi-core configurations of the LTE MIMO-OFDM are explored and their performances are evaluated on the Xilinx Zynq FPGA platform. The consumed area, power, and execution times of the different configurations are analyzed and compared in order to propose the most efficient architecture for this application.	compaq lte;embedded system;field-programmable gate array;hierarchical state routing;intel core (microarchitecture);mimo;mimo-ofdm;modulation;multi-core processor;multiplexing;performance	Mariem Makni;Mouna Baklouti;Smaïl Niar;Morteza Biglari-Abhari;Mohamed Abid	2015	2015 10th International Design & Test Symposium (IDT)	10.1109/IDT.2015.7396731	embedded system;electronic engineering;real-time computing;orthogonal frequency-division multiplexing;telecommunications;computer science;engineering;electrical engineering;operating system;field-programmable gate array;computer network;modulation	Arch	31.382228747854313	59.33408025544085	146102
2ea231482f343834718f608c01c3716f532a5ebd	irregular mds array codes	resource management arrays redundancy generators systematics sensors wireless sensor networks	In this paper, we extend the concept of maximum-distance separable (MDS) array codes to a larger class of codes, where the array columns contain a variable number of data and parity symbols and the codewords cannot be arranged, in general, in a regular array structure with equal column length. These new codes, named irregular MDS array codes, find applications in problems of distributed data storage with multiple sources of information generating data at unequal rates. We solve the problem of finding optimal parity symbol allocations that achieve minimum redundancy for a given level of protection against block erasures. We provide a classification of irregular MDS array codes according to the parameters of their parity symbol allocation and we show how regular MDS array codes are a special case of this wider class. We derive necessary and sufficient conditions for such irregular array codes to be MDS and extend the concept of the lowest density generator matrix. Finally, we show how a simple constructive method allows to design irregular lowest density MDS array codes with alphabet size independent of the size of the array columns.	code word;column (database);computer data storage;generator matrix;singleton bound	Filippo Tosato;Magnus Sandell	2014	IEEE Transactions on Information Theory	10.1109/TIT.2014.2336656	real-time computing;computer science;theoretical computer science;mathematics	HPC	37.00567310323834	57.92823061393638	146234
cd85d2bdecd5671ef132079b0d2865912282672d	transistor size optimization in digital circuits using ant colony optimization for continuous domain	ant colony optimization;genetic algorithm;automatic transistor sizing;digital circuits	In this paper, ant colony optimization ACO algorithm is presented, as a tool to find transistor sizes in digital circuits. Performance of ACO has been tested on four digital circuits, of different complexity, to find optimum balance between power and delay of circuits. Optimization problem has been set up by first, formulating an objective function, to be minimized, for each circuit and then finding the values of variables of circuits, using optimization algorithm. For the purpose of examining the results, circuits are optimized using genetic algorithm GA also. Results show that, ACO performs better than GA, for all the four circuits, in finding optimized transistor sizes. Copyright © 2012 John Wiley & Sons, Ltd.	ant colony optimization algorithms;digital electronics;mathematical optimization;transistor	Himanshu Gupta;Bahniman Ghosh	2014	I. J. Circuit Theory and Applications	10.1002/cta.1879	mathematical optimization;electronic engineering;ant colony optimization algorithms;meta-optimization;genetic algorithm;computer science;engineering;electrical engineering;artificial intelligence;digital electronics;metaheuristic	EDA	26.828462514231834	48.637890522189856	146482
0b00e744f93e534d3c84ebee1bc987b13c8da6fa	distributed storage allocations and a hypergraph conjecture of erdős	storage allocation;graph theory;storage allocation encoding graph theory reliability;reliability;resource management upper bound information theory probabilistic logic random variables electrical engineering educational institutions;encoding;reliability distributed storage allocations hypergraph conjecture erdös	We study two variations of the distributed storage allocation problem. The goal is to allocate a given storage budget in a distributed storage system for maximum reliability. It was recently discovered that this problem is related to an old conjecture in extremal combinatorics, on the maximum number of edges in a hypergraph subject to a constraint on its maximum matching number. The conjecture was recently verified in some regimes. In this paper we assume that the conjecture is true and establish new results for the optimal allocation for a variety of parameter values. We also derive new performance bounds that are independent of the conjecture, and compare them to the best previously known bounds.	asymptotically optimal algorithm;chernoff bound;clustered file system;computer data storage;erdős number;extremal combinatorics;matching (graph theory);mathematical optimization	Yi-Hsuan Kao;Alexandros G. Dimakis;Derek Leong;Tracey Ho	2013	2013 IEEE International Symposium on Information Theory	10.1109/ISIT.2013.6620357	combinatorics;discrete mathematics;graph theory;theoretical computer science;reliability;mathematics;encoding;statistics	Theory	35.65750660820249	58.782372337492525	146488
67a4df102937a07a2e936d5eec13dfb0c39f7567	evolvable hardware system at extreme low temperatures	damage;developpement logiciel;topology;proceso concepcion;diseno circuito;adaptability;adaptabilite;degradation;design process;reconfigurable architectures;circuit design;topologie;evolvable hardware;endommagement;deterioracion;adaptabilidad;topologia;preparacion serie fabricacion;chip;low temperature;test preliminaire;fault tolerant system;chips;preliminary test;biomimetique;desarrollo logicial;transistors;software development;design analysis;sistema tolerando faltas;algorithme evolutionniste;systeme tolerant les pannes;algoritmo evolucionista;conception circuit;circuits;test preliminar;liquid nitrogen;field programmable transistor array fpta;process planning;evolutionary algorithm;damaging;preparation gamme fabrication;evolvable harware;architecture reconfigurable;biomimetics;processus conception	This paper describes circuit evolutionary experiments at extreme low temperatures, including the test of all system components at this extreme environment (EE). In addition to hardening-by-process and hardeningby-design, “hardening-by-reconfiguration ”, when applicable, could be used to mitigate drijts, degradation, or damage on electronic devices (chips) in EE, by using re-con$gurable devices and an adaptive selfreconJiguration of their circuit topology. Conventional circuit design exploits device characteristics within a certain temperaturehadiation range; when that is exceeded, the circuit function degrades. On a recon.gurable device, although component parameters change in EE, a new circuit design, suitable for new parameter values, may be mapped into the reconJigurable structure to recover the initial circuit function. This paper demonstrates this technique for circuit evolution and recovery at liquid nitrogen temperatures (-196.6 “c). In addition, preliminary tests are performed to assess the survivability limitations of the evolutionary processor at extreme low temperatures.	circuit design;circuit topology;elegant degradation;evolvable hardware;experiment	Ricardo Salem Zebulum;Adrian Stoica;Didier Keymeulen;Lukás Sekanina;Rajeshuni Ramesham;Xin Guo	2005		10.1007/11549703_4	chip;biomimetics;embedded system;electronic circuit;fault tolerance;adaptability;simulation;degradation;design process;computer science;engineering;artificial intelligence;software development;evolutionary algorithm;circuit design;liquid nitrogen;transistor	EDA	25.3190181838403	46.882414574408244	146559
e7eda6e81e141728ff8793ca331ecc19b9ef7e65	on a hyperplane arrangement problem and tighter analysis of an error-tolerant pooling design	tolerancia falta;metodo adaptativo;borne erreur;vector space;prueba grupada;methode adaptative;optimisation combinatoire;estimation erreur;error estimation;test groupe;fault tolerance;adaptive method;error tolerant;estimacion error;hiperplano;espace vectoriel;error bound;non adaptive pooling design;combinatorial optimization;hyperplane;espacio vectorial;hyperplane arrangement;tolerance faute;hyperplan;group testing;optimizacion combinatoria;limite error	In this paper, we formulate and investigate the following problem: given integers d, k andr where k > r ≥ 1, d ≥ 2, and a prime power q, arranged hyperplanes onFq to maximize the number of r-dimensional subspaces of Fq each of which belongs to at least one of the hyperplanes. The problem is motivated by the need to give tighter bounds for an error-tolerant pooling design based on finite vector spaces.	error-tolerant design	Hung Q. Ngo	2008	J. Comb. Optim.	10.1007/s10878-007-9084-2	mathematical optimization;fault tolerance;combinatorics;group testing;vector space;combinatorial optimization;hyperplane;mathematics;algorithm	Theory	38.32554055279661	50.33756674108269	146807
abfaacfcf14062b3d0c7c910c688b59af40cf11c	an offset compensation technique for latch type sense amplifiers in high-speed low-power srams	canal n;analytical models;modelizacion;circuit matching;cmos integrated circuits;circuit rapide;compensacion;metodo analitico;diseno circuito;process variation;mos devices;random access memory;offset voltage;transistor mismatch;cmos technology;static random access memory;fluctuation tension;canal p;tiempo subida;amplifiers;fluctuations;integrated circuit;adaptation circuit;forme onde;voltage fluctuation;very large scale integration;signal entree;submicron technology;amplifier;pmos transistors;circuit design;flip flops;metal oxide semiconductor transistor;circuit vlsi;pulse amplifiers;circuito integrado;design parameters;caracteristique temps tension;temps montee;network analysis;experimental result;rise time;caracteristica tiempo tension;modelisation;time voltage characteristic;power consumption amplifiers compensation low power electronics cmos integrated circuits sram chips flip flops high speed integrated circuits;input signal;n channel;tension electrique;p channel;senal entrada;feedback;vlsi circuit;compensation;low power;forma onda;srams;offset compensation technique;analytical method;sense amplifier enable signal;voltage;circuito rapido;adaptacion circuito;low power electronics;resultado experimental;puissance faible;puce memoire acces direct statique;mos transistor;methode analytique;fluctuacion tension;conception circuit;latches;amplificateur;waveform;full latch v sub dd biased sense amplifier offset compensation technique srams static random access memory offset voltage sense amplifier enable signal nmos transistors pmos transistors metal oxide semiconductor transistor power consumption design parameters;power consumption;circuito vlsi;full latch v sub dd biased sense amplifier;amplificador;resultat experimental;voltaje;analyse circuit;modeling;low power consumption;high speed;random access memory latches voltage analytical models very large scale integration fluctuations feedback pulse amplifiers cmos technology mos devices;high speed integrated circuits	The input referred offset voltage occurring in the full latch V/sub DD/ biased sense amplifier has been analyzed extensively. The process variations in the matched nMOS and pMOS transistors have been accounted by /spl plusmn/2.5% variation in V/sub T/ and /spl plusmn/5% variation in /spl beta/, from typical values. Effect of various design parameters on the sense amplifier offset has been studied and reported. It has been shown that the rise time of the sense amplifier enable signal (SAEN) has a profound effect on the offset voltage. The slower transition of SAEN signal is proposed to result in high speed as well as low-power consumption in SRAM application. An analytical model has been derived for simplified latch to model the effect of rise time of SAEN signal on offset voltage.	low-power broadcasting;nmos logic;norm (social);rise time;sense amplifier;simulation;static random-access memory;transistor	R. Singh;N. Bhat	2004	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2004.827566	embedded system;electronic engineering;input offset voltage;voltage;computer science;engineering;electrical engineering;amplifier;cmos	EDA	26.215621196239063	56.097737120057836	147099
06b48d59b8af975356175d98f3b8132403342292	tcg-s: orthogonal coupling of p/sup */-admissible representations for general floorplans	wireless;integrated circuit layout;physical design;solution structure;receivers;network topology;low power;permission information science costs constraint optimization circuit stability design engineering design automation modems very large scale integration algorithm design and analysis;cost evaluation tcg s orthogonal coupling p admissible representations floorplans packing perturbation schemes vlsi geometric relations position constraints incremental update;proceedings paper;vlsi;transitive closure;circuit layout cad;network topology circuit layout cad integrated circuit layout vlsi modules;modules;cmos	We extend in this paper the concept of the P-admissible floorplan representation to that of the P*-admissible one. A P*-admissible representation can model the most general floorplans. Each of the currently existing P*-admissible representations, SP, BSG, and TCG, has its strengths as well as weaknesses. We show the equivalence of the two most promising P*-admissible representations, TCG and SP, and integrate TCG with a packing sequence (part of SP) into a new representation, called TCG-S. TCG-S combines the advantages of SP and TCG and at the same time eliminates their disadvantages. With the property of SP, faster packing and perturbation schemes are possible. Inherited nice properties from TCG, the geometric relations among modules are transparent to TCG-S (implying faster convergence to a desired solution), placement with position constraints becomes much easier, and incremental update for cost evaluation can be realized. These nice properties make TCG-S a superior representation which exhibits an elegant solution structure to facilitate the search for a desired floorplan/placement. Extensive experiments show that TCG-S results in the best area utilization, wirelength optimization, convergence speed, and stability among existing works and is very flexible in handling placement with special constraints.	coupling (computer programming);experiment;incremental backup;mathematical optimization;set packing;turing completeness;vergence	Jai-Ming Lin;Yao-Wen Chang	2002	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1145/513918.514127	physical design;embedded system;mathematical optimization;electronic engineering;computer science;electrical engineering;theoretical computer science;modular programming;mathematics;integrated circuit layout;very-large-scale integration;cmos;transitive closure;engineering drawing;network topology;algorithm;wireless	EDA	25.78651110725077	48.6849210694716	147853
0ddfb51319cfebdac05e741cc38a63d030ff8ea8	circuit level verification of a high-speed toggle	toggle flip-flop;digital circuit;non-linear circuit-level model;non-linear dynamic;digital design;accurate linear approximation;original projection;vlsi fabrication technology;continuous model;circuit level verification;largest verification;high-speed toggle;switches;computer science;design automation;non linear dynamics;digital circuits;crosstalk;linear program;three dimensional;linear approximation;very large scale integration	As VLSI fabrication technology progresses to 65nm feature sizes and smaller, transistors no longer operate as ideal switches. This motivates verifying digital circuits using continuous models. This paper presents the verification of the high-speed, toggle flip-flop proposed by Yuan and Svensson [1]. Our approach builds on the projection based methods originally proposed by Greenstreet and Mitchell [2], [3]. While they were only able to demonstrate their approach with two- and threedimensional systems, we apply projection based analysis to a seven-dimensional model for the flip-flop. We believe that this is the largest verification to date of a digital circuit using non-linear circuit-level models. In this paper, we describe how we overcame problems of numerical errors and instability associated with the original projection based methods. In particular, we present a novel linear-program solver and new methods for constructing accurate linear approximations of non-linear dynamics. We use the toggle flip-flop as an example and consider how these methods could be extended to verify a standard cell library for digital design.	approximation;digital electronics;dynamical system;flops;feature toggle;flip-flop (electronics);instability;linear circuit;linear programming;logic synthesis;mitchell corporation;network switch;nonlinear system;numerical analysis;semiconductor device fabrication;solver;standard cell;transistor;very-large-scale integration	Chao Yan;Mark R. Greenstreet	2007	Formal Methods in Computer Aided Design (FMCAD'07)	10.1109/FAMCAD.2007.39	three-dimensional space;crosstalk;electronic design automation;nonlinear system;network switch;computer science;linear programming;theoretical computer science;very-large-scale integration;digital electronics;algorithm;linear approximation	EDA	25.433441972250325	49.880095732845795	147924
4dc8bd1e83a5c6c490adafee02be9378554d39bf	a surface model based on a fibre bundle of 1-parameter groups of hamiltonian lie algebra	lie algebra;copy protection lie algebras group theory image recognition image coding;image recognition;invariant set;image coding;3d imaging;algebra shape optical fiber communication power system modeling image recognition chaos systems engineering and theory power engineering and energy image coding image retrieval;copy protection;lie algebras;group theory;copyright protection fibre bundle hamiltonian lie algebra l parameter groups linear lie algebra euclidean motions recognition synthesis based coding 3d images;copyright protection;numerical integration;surface model;elementary functions;image retrieval	This paper extends a successful fibre-bundle surface model using l-parameter groups of a linear Lie algebra as fibres. The new model uses a fibre-bundle of l-parameter groups of a Hamilton Lie algebra, therefore is rich in descriptive power to represent bounded shapes as closed surfaces. A surface represented by this model is uniquely determined by a finite number of invariants. The complete invariant set of the model under action of Euclidean motions is obtained. Conditions for the surfaces to be bounded or closed are also given. Another feature is that the surfaces can be synthesized by elementary functions therefore free of numerical integration errors. This model can be used in recognition-synthesis-based coding of 3D images, image retrieving and copyright protection as well.	elementary function;hamiltonian (quantum mechanics);invariant (computer science);numerical analysis;numerical integration	Jinhui Chao;Fang Xing Li	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1529927	adjoint representation;stereoscopy;lie algebra;discrete mathematics;topology;numerical integration;elementary function;mathematics;simple lie group;graded lie algebra;lie conformal algebra;weight;algebra	Robotics	39.170973874092645	49.007879073504085	148948
1763c48f5a0f8f4acf45176c4c25bc99e01f37af	lengthening and extending binary private information retrieval codes		It was recently shown by Fazeli et al. that the storage overhead of a traditional t-server private information retrieval (PIR) protocol can be significantly reduced using the concept of a t-server PIR code. In this work, we show that a family of tserver PIR codes (with increasing dimensions and blocklengths) can be constructed from an existing t-server PIR code through lengthening by a single information symbol and code extension by at most ⌈	code;overhead (computing);personally identifiable information;private information retrieval;server (computing)	Hsuan-Yin Lin;Eirik Rosnes	2017	CoRR		mathematics;discrete mathematics;block code;theoretical computer science;linear code;private information retrieval;binary number;symbol;special case;code (cryptography)	Security	37.05702501064918	57.30566259686671	149470
3e16e65e44a4e298e2efb80e5a284695aa680e59	data flow entropy collector	random number generation data flow computing entropy human computer interaction;keyboards;random number generator;generators;human computer interaction;unpredictable random number generator;random number generation;data flow entropy collector;data mining;data flow entropy entropy source random number generator;entropy random number generation mice keyboards hardware sampling methods timing linux pulse modulation scientific computing;human computer interaction data flow entropy collector true random number generator unpredictable random number generator;streaming media;cryptography;true random number generator;entropy source;data flow computing;entropy;pseudo random number generator;data flow;hardware	Collecting entropy from various sources available within a regular PC and combining them in an entropy pool is often used as an alternative to pseudo random number generation. Cheaper than using a hardware true random number generator (TRNG), this method offers a good approximation of a TRNG, commonly known as an unpredictable random number generator. The source of unpredictability is ultimately the human-computer interaction that, when complex enough, is irreproducible by an adversary. The present paper proposes a novel approach to combining the entropy from various sources in a data flow manner, thus increasing the degree of unpredictability. The result is a highly unpredictable random number generator, of potentially good quality.	adversary (cryptography);approximation;dataflow architecture;hardware random number generator;human–computer interaction;pseudorandomness;random number generation	Alin Suciu;Kinga Marton;Zoltan Antal	2008	2008 10th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing	10.1109/SYNASC.2008.25	random number generation;computer science;theoretical computer science;algorithm;statistics	Arch	33.12224931076505	47.69286284134584	149624
47e126d8f86d378387c5457b52760cefdb57771c	a fpga architecture of blind source separation and real time implementation	tratamiento paralelo;systeme mimo;distributed system;field programmable gate array;reseau capteur;entrada salida;systeme reparti;mimo system;separacion ciega;traitement parallele;ingenierie connaissances;blind source separation;real time;speech processing;circuit vlsi;measurement system;tratamiento palabra;traitement parole;intelligence artificielle;red puerta programable;reseau porte programmable;senal vocal;chip;input output;blind separation;signal vocal;vlsi circuit;red sensores;sistema repartido;sistema mimo;envoi message;temps reel;cognition;fpga architecture;separacion senal;sensor array;message passing;separation aveugle;cognicion;tiempo real;artificial intelligence;real time implementation;separation source;inteligencia artificial;circuito vlsi;vocal signal;source separation;convolutive mixture;parallel processing;entree sortie;vlsi architecture;knowledge engineering	Blind source separation(BSS) of independent sources from their convolutive mixtures is a problem in many real-world multi-sensor applications. However, the existing BSS architectures are more often than not based upon software and thus not suitable for direct implementation on hardware. In this paper, we present a new VLSI architecture for the blind source separation of a multiple input mutiple output(MIMO) measurement system. The algorithm is based on feedback network and is highly suited for parallel processing. The implementation is designed to operate in real time for speech signal sequences. It is systolic and easily scalable by simple adding and connecting chips or modules. In order to verify the proposed architecture, we have also designed and implemented it in a hardware prototyping with Xilinx FPGAs.	blind signal separation;field-programmable gate array;source separation	Yong Kim;Hong Jeong	2005		10.1007/11499305_36	chip;input/output;embedded system;parallel processing;message passing;cognition;telecommunications;computer science;artificial intelligence;machine learning;knowledge engineering;system of measurement;speech processing;blind signal separation;sensor array;field-programmable gate array	Arch	34.71286106601807	49.0784514259552	149693
0428563ab95971cf45a5ecb5c86cc8339e208a23	word-length determination and scaling software for a signal flow block diagram	minimisation;performance measure;digital signal processing;quantization;constraint optimization;cost function;fixed point digital signal processing algorithms;signal processing digital signal processing software algorithms signal processing algorithms noise measurement quantization signal to noise ratio hardware cost function constraint optimization;search methods;hardware cost model;search method;signal flow graphs;signal to quantization noise ratio;signal flow block diagram;noise measurement;real input signal;fixed point;search methods word length determination scaling software signal flow block diagram fixed point digital signal processing algorithms fixed point performance measure signal to quantization noise ratio hardware cost model real input signal netlist preprocessing;scaling software;word length;signal processing;fixed point performance measure;word length determination;vlsi;software algorithms;digital signal processing chips;digital arithmetic;search problems;signal to noise ratio;circuit optimisation;signal flow graphs digital arithmetic vlsi digital signal processing chips digital simulation circuit analysis computing search problems minimisation circuit optimisation;signal processing algorithms;circuit analysis computing;netlist preprocessing;cost model;digital simulation;hardware	Software for word-length determination and scaling of a general signal flow block diagram is developed for aiding the development of fixed-point digital signal processing algorithms. A fixed-point performance measure, such as signal to quantization noise ratio, and a hardware cost model are used as constraints to the optimization. Simulation using a real input signal is conducted for the evaluation of fixed-point performance. The number of simulations required for the optimization process is greatly reduced by employing netlist preprocessing and efficient search methods.	algorithm;analysis of algorithms;diagram;digital signal processing;fixed point (mathematics);fixed-point arithmetic;image scaling;mathematical optimization;netlist;preprocessor;quantization (signal processing);simulation	Wonyong Sung;Ki-Il Kum	1994		10.1109/ICASSP.1994.389613	computer vision;minimisation;real-time computing;quantization;signal transfer function;computer science;noise measurement;theoretical computer science;digital signal processing;signal processing;fixed point;very-large-scale integration;signal-to-quantization-noise ratio;signal-to-noise ratio	EDA	30.562169718203915	49.73210835619163	150185
09a4696abae48e8cee67158257f893cf5ca150cf	modeling magnetic coupling for on-chip interconnect	far away coupling terms;coupling values;integrated circuit modelling integrated circuit interconnections inductance electric admittance;magnetic interaction;magnetic fields;pattern matching style solutions;window based extraction;chip;couplings inductance integrated circuit interconnections capacitance sparse matrices integrated circuit modeling frequency magnetic analysis magnetic susceptibility magnetic fields;large scale magnetic coupling;large scale;electric admittance;on chip interconnect;magnetic susceptibility;on the fly field solution;operating frequencies;integrated circuit modelling;pattern matching;integrated circuit interconnections;magnetic analysis;integrated circuit modeling;on the fly;matrix approximation;inductance;capacitance;interconnect modeling;couplings;frequency;inductance extraction large scale magnetic coupling on chip interconnect operating frequencies far away coupling terms window based extraction pattern matching style solutions coupling values on the fly field solution;susceptance;sparse matrices;inductance extraction;magnetic coupling	As advances in IC technologies and operat-ing frequencies make the modeling of on-chip magnetic interactions a necessity, it is apparent that extension of traditional inductance extraction approaches to full-chip scale problems is impractical. There are primarily two obstacles to performing inductance extraction with the same efficacy as full-chip capacitance extraction: 1) ne-glecting far-away coupling terms can generate an unsta-ble inductance matrix approximation; and 2) the penetrating nature of inductance makes localized extrac-tion via windowing extremely difficult. In this paper we propose and contrast three new options for stable and ac-curate window-based extraction of large-scale magnetic coupling. We analyze the required window sizes to con-sider the possibilities for pattern-matching style solu-tions, and propose three schemes for determining coupling values and window sizing for extraction via on-the-fly field solution.	approximation;inductive coupling;interaction;naruto shippuden: clash of ninja revolution 3;network analysis (electrical circuits);pattern matching;simulation;singular value decomposition;sparse matrix	Michael W. Beattie;Lawrence T. Pileggi	2001		10.1145/378239.378504	chip;susceptance;electronic engineering;magnetic susceptibility;inductive coupling;magnetic field;sparse matrix;inductance;engineering;electrical engineering;frequency;pattern matching;capacitance;coupling	EDA	25.809625558036387	57.653766959274904	150449
1c2e5707c5adcb1614ddef90237ce8bf0ead7448	cam puzzle: a power model and function-based circuit segment method of content addressable memory	static random access memory cell;cmos integrated circuits;high frequency data;cam;hspice;power analysis;static random access memory;data storage device;packet classification;decoding;integrated circuit layout;ip address lookup;concrete circuit layout;preliminary simulation;petty models;spice simulation;power distribution;function based circuit segment method;network routing;conventional cam;cam architecture;high frequency data storage circuits;circuit simulation;accuracy;data storage;power expressions;sram chips circuit simulation content addressable storage integrated circuit layout ip networks memory architecture network routing spice;power model;cam architecture cam puzzle power model function based circuit segment method data storage device static random access memory cell sram cell network routers ip address lookup packet forwarding packet classifications power distribution preliminary simulation concrete circuit layout power analysis method simulation time circuit simulation tool hspice high frequency data storage circuits camp petty models spice simulation power expressions conventional cam precomputation based content addressable memory pb cam power consumption cam peripheral circuits address decoder circuit data i o;logic gates;memory architecture;semiconductor device modeling;computer aided manufacturing;network routers;integrated circuit modeling;sram cell;content addressable memory;data i o;ip networks;address decoder circuit;logic gates computer aided manufacturing accuracy integrated circuit modeling decoding cmos integrated circuits semiconductor device modeling;accuracy cam spice simulation pb cam;power consumption;precomputation based content addressable memory;content addressable storage;cam peripheral circuits;simulation time;spice;cam puzzle;power modeling;circuit simulation tool;packet forwarding;camp;packet classifications;sram chips;pb cam;power analysis method	Content Addressable Memory (CAM) is a data storage device, utilizing the Static Random Access Memory (SRAM) cell. CAMs are very popular especially implemented in network routers for IP address lookup, packet forwarding and packet classifications. Up to now, there are many types of CAM to conform to these different implementations. For the purpose to estimate the efficiency and power distribution of all types of CAM, doing preliminary simulation is needed before doing concrete circuit layout. Therefore, a speedy and accurate power analysis method is necessary. Besides, the simulation time of nowadays circuit simulation tool like HSPICE is considerable, especially for high frequency data storage circuits like CAMs. This work established a brand new power model of CAM which is called CAM Puzzle (CAMP), combining the petty models which are extracted from SPICE simulation, simplified into easily analytical power expressions in order to analyse the conventional CAM and pre-computation based content addressable memory (PB-CAM). In addition, this work also estimates the power consumption of CAM peripheral circuits such as address decoder circuit and data I/O. Using CAMP, the power consumption of complete CAM architecture can be estimated with 82% model accuracy and 94% system accuracy compared to SPICE simulation in 0.18µm process.	address decoder;circuit diagram;computation;computer data storage;content-addressable memory;electronic circuit simulation;lookup table;network packet;packet analyzer;peripheral;precomputation;random access;router (computing);spice 2;static random-access memory	Kam-Tou Sio;Feipei Lai;Chin-Hung Peng	2010	2010 IEEE Asia Pacific Conference on Circuits and Systems	10.1109/APCCAS.2010.5774881	embedded system;routing;parallel computing;semiconductor device modeling;power analysis;cam;static random-access memory;logic gate;computer hardware;computer science;operating system;computer data storage;content-addressable memory;packet forwarding;accuracy and precision;integrated circuit layout;cmos;statistics;computer-aided manufacturing	EDA	26.801848621521934	54.63769564921477	150553
79e0bdaa9245588b942e44a5e8a73ae04664e047	managing dynamic reconfiguration on mimo decoder	dynamic change;power saving;decoding;cordic units;partial reconfiguration;dynamic reconfiguration;mimo decoding signal processing algorithms field programmable gate arrays wireless communication computer architecture design methodology reconfigurable architectures adaptive arrays software algorithms;reconfigurable architectures;vertical bell laboratories layered space time;coordinate rotation digital computer;fpga;mimo v blast;wireless communication;computer architecture;design method;dynamic partial reconfiguration method;adaptive arrays;space time codes;decoder architecure;software algorithms;digital arithmetic;space time codes decoding digital arithmetic field programmable gate arrays mimo communication reconfigurable architectures;field programmable gate arrays;signal processing algorithms;mimo;square root decoder;mimo square root design method;coordinate rotation digital computer dynamic partial reconfiguration method fpga mimo square root design method decoder architecure mimo v blast vertical bell laboratories layered space time square root decoder cordic units;layered space time;mimo communication;design methodology	This paper is about the implementation of a MIMO V-BLAST (vertical bell laboratories layered space-time) square root decoder in a FPGA using dynamic partial reconfiguration. The decoder architecture is based on four CORDIC (coordinate rotation digital computer) units. Among these CORDIC units, three are used in rotation mode and the fourth one is used in vectoring mode. The design implementation aims power saving and area efficiency allowing dynamically changing the interconnections between the fixed modules in the reconfigurable modules. This MIMO square root design method shows the configuration time improvement, area efficiency and flexibility of the decoder by using the dynamic partial reconfiguration method.	blast;bell laboratories layered space-time;bitstream;cordic;computer;field-programmable gate array;interconnection;mimo;modulation;random access;reconfigurable computing;throughput;virtex (fpga)	Hongzhi Wang;Jean-Philippe Delahaye;Pierre Leray;Jacques Palicot	2007	2007 IEEE International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2007.370387	embedded system;parallel computing;real-time computing;design methods;computer science;field-programmable gate array	Embedded	31.54865474194242	59.01486147933181	150933
e7ac5de4f30c58fb55dfdee80f383cda79344045	stochastic associative processor operated by random voltages	processeur associatif;evaluation performance;circuito lsi;tecnologia electronica telecomunicaciones;procesador asociativo;interconnection;performance evaluation;integrated circuit;variable aleatoire;evaluacion prestacion;variable aleatoria;programme spice;circuito integrado;random variables;and vector matching;manhattan distance;simulator;interconexion;stochastic computing;associative processor;lsi circuit;simulador;pattern matching;vector matching;interconnexion;random variable;simulateur;concordance forme;power consumption;tecnologias;consommation energie electrique;grupo a;circuit lsi;spice;circuit integre;spice simulator manhattan distance;spice simulator	The latest LSIs still lack performance in pattern matching and picture recognition. Living organisms, on the other hand, devote very little energy to processing of this type, suggesting that they operate according to a fundamentally different concept. There is a notable difference between the two types of processing: the most similar pattern is always chosen by the conventional digital pattern matching process, whereas the choice made by an organism is not always the same: both the most similar patterns and other similar patterns are also chosen stochastically. To realize processing of this latter type, we examined a calculation method for stochastically selecting memorized patterns that show greater similar to the input pattern. Specifically, by the use of a random voltage sequence, we executed stochastic calculation and examined to what extent the accuracy of the solution is improved by increasing the number of random voltage sequences. Although calculation of the Manhattan distance cannot be realized by simply applying stochastic computing, it can be done stochastically by inputting the same random voltage sequence to two modules synchronously. We also found that the accuracy of the solution is improved by increasing the number of random voltage sequences. This processor operates so efficiently that the power consumption for calculation does not increase in proportion to the number of memorized vector elements. This characteristic is equivalent to a higher accuracy being obtained by a smaller number of random voltage sequences: a very promising characteristic of a stochastic associative processor.		Michihito Ueda;Ichiro Yamashita;Kiyoyuki Morita;Kentaro Setsune	2007	IEICE Transactions	10.1093/ietele/e90-c.5.1027	random variable;embedded system;electronic engineering;telecommunications;computer science;electrical engineering;theoretical computer science;operating system;statistics	Theory	27.514054652125704	56.3256779473436	151172
5c13d390f9e47cab5f5b31ed823978f66cc6feb9	a markov chain-based yield formula for vlsi fault-tolerant chips	tolerancia falta;markov chain model;concepcion asistida;computer aided design;yield formula;fiabilidad;reliability;state space methods;probability;circuit faults;fault tolerant;integrated circuit;very large scale integration;markov chain model yield formula vlsi fault tolerant chips yield calculation method probability;circuit vlsi;logic circuits;circuito integrado;very large scale integration fault tolerance circuit faults statistics probability state space methods virtual manufacturing redundancy logic devices logic circuits;methode calcul;chip;metodo calculo;calculating method;vlsi circuit reliability markov processes probability statistical analysis;vlsi circuit;redundancy;statistical analysis;vlsi fault tolerant chips;circuit reliability;fiabilite;fault tolerance;statistics;vlsi;conception assistee;pastilla electronica;markov processes;circuito vlsi;puce electronique;virtual manufacturing;tolerance faute;yield calculation method;circuit integre;state transition;logic devices;markov chain	A new yield calculation method for the yield formula of fault-tolerant VLSI chips is introduced, which improves existing methods and puts together generalities, ease of computation, and predictability in approximation levels. The innovative part of the method is concerned with the evaluation of the probability that the chip is acceptable given n defects. This is done by introduction of a Markov chain model, in which each state represents an operating chip configuration and the state transitions take place by the presence of manufacturing defects. Comparisons are made with three existing yield calculation methods to prove the “ease of use,” the “accuracy,” and the “representativeness” characteristics of the new proposed one.	approximation;computation;fault tolerance;markov chain;software bug;usability;very-large-scale integration	Bruno Ciciani;Giuseppe Iazeolla	1991	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.68412	reliability engineering;embedded system;markov chain;fault tolerance;electronic engineering;computer science;electrical engineering;computer aided design;mathematics;very-large-scale integration;algorithm;statistics	EDA	24.72888476452703	55.19312074427123	151221
2be219976bf2650d5fbf51b515e83da92b376d10	generalized partial orders for polar code bit-channels		We study partial orders (POs) for the synthesized bit-channels of polar codes. First, we consider complementary bit-channel pairs whose Bhattacharyya parameters over the binary erasure channel (BEC) exhibit a symmetry property and provide insight into the alignment of polarized sets of bit-channels for the BEC and general binary-input memo-ryless symmetric (BMS) channels. Next, we give an alternative proof of a recently presented PO for bit-channels and use the underlying idea to extend the bit-channel ordering to some additional cases. In particular, the bit-channel ordering for a given code block-length is used to generate additional bit-channel ordering relationships for larger block-lengths, generalizing previously known POs. Examples are provided to illustrate the new POs.	battery management system;binary erasure channel;block (programming);polar code (coding theory)	Wei Wu;Bing Fan;Paul H. Siegel	2017	2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton)	10.1109/ALLERTON.2017.8262784	mathematical optimization;discrete mathematics;polar code;binary erasure channel;computer science;generalization;communication channel;polar;bhattacharyya distance	Theory	38.09656791624815	58.43635101636878	151588
d4cc45b38131d2aa1f288b554707b43dec35c915	architectural techniques for eliminating critical feedback paths	feedforward systems;bimode 3b4b line coder;cmos integrated circuits;circuit codeur;concepcion circuito;optical communication equipment;coding circuit;optical communication equipment cmos integrated circuits encoding feedback;throughput optical feedback feedforward systems output feedback feedback loop upper bound feedback circuits optical signal processing optical fiber communication circuit optimization;integrated circuit;architectural techniques;information transmission;2 micron;1 4 gbit s;circuit design;2 micron optical communication systems critical feedback paths architectural techniques small state feedback circuits coding signal processing systems optical communications cmos layout bimode 3b4b line coder 1 4 gbit s;signal processing systems;feedback circuits;circuito integrado;bucle enclavamiento;tecnologia mos complementario;optical communications;simulation estimation;output feedback;chip;upper bound;feedback;telecomunicacion optica;telecommunication optique;data dependence;feedback loop;signal processing;optical signal processing;optical communication systems;circuito codificacion;coding;optical telecommunication;locking loop;optical communication;conception circuit;cmos layout;transmision informacion;boucle verrouillage;transmission information;high throughput;technologie mos complementaire;machine etat fini;encoding;critical feedback paths;optical fiber communication;optical feedback;finite state machine;circuit integre;complementary mos technology;small state feedback circuits;circuit optimization;throughput	Circuits with feedback paths are significantly slower than comparable circuits without the feedback. The feedback also implies data dependency which voids usual parallel implementations, further exacerbating the throughput problem. This paper discusses a new high-throughput solution for systems wilh finite-level feedback values. As an example, we consider coding and signal processing systems for optical communications, which usually have very simple feedback. Our melhod uses architectural techniques, and requires no detail circuit tuning for high speedup. We demonstrate the method by realizing a 2 micron CMOS layout of a himode 3B4B line coder. Simulation estimates that, using standard cell design, the chip achieves a coding rate of 1.4 Gb/s. Other design options are discussed.	cmos;data dependency;feedback;high-throughput computing;signal processing;simulation;speedup;standard cell;throughput	Horng-Dar Lin;David G. Messerschmitt	1991	IEEE Journal on Selected Areas in Communications	10.1109/49.87641	real-time computing;telecommunications;computer science;signal processing;optical communication;computer network	EDA	30.182406898732605	55.485641615914965	151781
fa621616402aafb6a1faaa80e808a7d0d6c1255c	improved sketching of hamming distance with error correcting	data stream;probability of error;hamming distance;error correction	We address the problem of sketching the hamming distance of data streams. We present a new notion of sketching technique, Fixable sketches and we show that using such sketch not only we reduce the sketch size, but also restore the differences between the streams. Our contribution: For two streams with hamming distance bounded by k we show a sketch of size O(k log n) with O(log n) processing time per new element in the stream and how to restore all locations where the two streams differ in time linear in the sketch size. Probability of error is less than 1/n.	algorithm;edit distance;hamming distance;network packet;original order;time complexity	Ely Porat;Ohad Lipsky	2007		10.1007/978-3-540-73437-6_19	hamming weight;hamming distance;error detection and correction;hamming bound;theoretical computer science;probability of error;pattern recognition;hamming code;forward error correction;hamming(7,4);statistics	Theory	38.43778340552294	57.255663878529546	151838
97d66b42d7e2d16e6eca8496affa1018fcd00709	information compression and varshamov-gilbert bound	complexite;error correcting code;procesamiento informacion;information compression;complexite calcul;complejidad calculo;codigo corrector error;complejidad;computing complexity;complexity;compresion informacion;compression information;information processing;limite varshamov gilbert;traitement information;code correcteur erreur	Abstract   The program complexity to enumerate a finite set of words is found. The complexity is either an exponential or a linear function of the word length depending on whether the redundancy is either less or more than 100%. A corollary: the Varshamov-Gilbert bound of the group error correcting code cardinality is tight for almost any channel with additive noise. The proofs are based on the concept of the collision index.	gilbert cell;gilbert–varshamov bound	R. E. Krichevsky	1987	Inf. Comput.	10.1016/0890-5401(87)90008-3	complexity;error detection and correction;information processing;computer science;theoretical computer science;mathematics;algorithm	Theory	38.882442766219754	56.62085210966257	152412
54d2f3153dd35aa6642ea900eea6d34e6c3f1b30	noise bus modeling in network on chip		This paper considers the noise modeling of interconnections in on-chip communication. We present an approach to illustrate modeling and simulation of interconnections on chip microsystems that consist of electrical circuits connected to subsystems described by partial di®erential equations, which are solved independently. A model for energy dissipation in RLCmode is proposed for the switching current/voltage of such on-chip interconnections. The Waveform Relaxation (WR) algorithm is presented in this paper to address limiting in simulating NoCs due to the large number of coupled lines. We describe our approach to the modeling of on-chip interconnections. We present an applicative example of our approach with VHDL-AMS implementations and simulation results. This article analyzes the coupling noise, the bit error rate (BER) as well as the noise as a function of the rise/fall time of the signal source which can signi ̄cantly limit the scalability of the NoCs.	algorithm;applicative programming language;bit error rate;electrical connection;fall time;linear programming relaxation;lossy compression;lumped element model;network on a chip;scalability;simulation;system on a chip;transmission line;vhdl;vhdl-ams;very-large-scale integration;waveform graphics;x.690	Moez Balti	2018	Journal of Circuits, Systems, and Computers	10.1142/S0218126618501499	bit error rate;chip;rlc circuit;electrical network;transmission line;electronic engineering;scalability;modeling and simulation;network on a chip;computer science	EDA	29.013899588375097	55.09083792617368	152558
75b47952cf00141b585033c57972fa8f58346b94	an fpga-based all-digital 802.11b & 802.15.4 receiver for the software defined radio paradigm	baseband;digital receivers reconfigurable receiver wifi zigbee vlsi qpsk software defined radio fpga;baseband rf signals software defined radio paradigm all digital 802 11b receiver all digital 802 15 4 receiver fpga implementation configurable baseband receiver power consumption protocols coprocessor;zigbee field programmable gate arrays power consumption protocols radio receivers software radio telecommunication power management wireless lan;receivers ieee 802 15 standards baseband field programmable gate arrays phase shift keying;phase shift keying;receivers;field programmable gate arrays;ieee 802 15 standards	An FPGA implementation of an all-digital fully compliant IEEE 802.11b and 802.15.4 configurable baseband receiver is presented. This architecture can be integrated in systems implementing the Software Defined Radio (SDR) paradigm, relaxing the need for high power consumption general purpose processors. The receiver uses a single architecture that can be configured for receiving either standard at run time, exploiting similarities between both protocols, and may serve as a coprocessor for offloading the task of processing baseband RF signals. The system can be used as a platform for future low power devices to integrate into the SDR paradigm. Results showed that the architecture exceeds the specifications required by both standards, and has great performance in low SNR scenarios, making it an attractive alternative in wireless sensor networks with extremely low signal power levels.	baseband;bluetooth;cmos;carrier frequency offset;central processing unit;coefficient;coprocessor;etsi satellite digital radio;field-programmable gate array;finite-state machine;integrated circuit;lookup table;mobile app;phy (chip);power semiconductor device;programming paradigm;radio frequency;run time (program lifecycle phase);signal-to-noise ratio	Alfredo Espinoza-Rhoton;Luis F. Gonzalez-Perez;J. L. Ponce;Borrayo-S. Hector;Lennin C. Yllescas Lennin;Ram&#x00F3;n Parra-Michel;Hassan Aboushady	2014	2014 International Conference on ReConFigurable Computing and FPGAs (ReConFig14)	10.1109/ReConFig.2014.7032499	embedded system;real-time computing;computer science;phase-shift keying;software-defined radio;baseband;radio receiver design;field-programmable gate array	Mobile	30.417282151872698	59.604915921641876	152755
deec973ae94dae8dfe7e49183f930191658e7250	analysis of a type of digital chaotic cryptosystem	piecewise linear;chosen ciphertext attack digital chaotic cryptosystem 1 d piecewise linear map finite precision chaotic encryption finite precision chaotic decryption inverse system approach dynamical system n order delay line input terminal parameter mismatches security;piecewise linear techniques;delay lines cryptography chaos piecewise linear techniques;chaos;delay lines;dynamic system;inverse system;chaos cryptography chaotic communication decoding piecewise linear techniques mechanical engineering delay lines communication system security equations signal generators;cryptography;chosen ciphertext attack	A.type of digital chaotic cryptosystem was proposed in [l] which used a class of 1-D piecewise linear map to realize Tinite-precision chaotic encryption and decryption systems through the inverse system approach. In the general structure of encryption systems, a dynamical system () is used to connect the n-order delay line with the input terminal. The decoder is sensitive to all parameter mismatches, so it can provide high security of the encrypted signals. In this paper we show that this cryptosystem can not frustrate chosen-ciphertext attack. Consequently, it may be possible to recover the key with good accuracy. Fig. 1. Encoder	analog delay line;chosen-ciphertext attack;ciphertext;cryptography;cryptosystem;dynamical system;encoder;encryption;piecewise linear continuation	GuoJie Hu;ZhengJin Feng;Lin Wang	2002		10.1109/ISCAS.2002.1010263	inverse system;discrete mathematics;cryptography;theoretical computer science;control theory;mathematics	Crypto	34.84362333211576	46.857718701955555	152808
c003e99941c1c5952e42f07bca8b2194710e638a	attack on a chaos-based “true” random bit generator	generators;chaos;chaotic oscillator chaos based true random bit generator algebraic attack clone system security weakness master slave synchronization scheme secret parameters scalar time series;oscillators;random number generation;cloning;time series oscillators random number generation synchronisation;synchronization;chaos synchronization cloning generators oscillators random number generation	This paper presents an algebraic attack on a chaos-based “true” random bit generator (RBG). A clone system is proposed to analyze the security weaknesses of the RBG and its convergence is proved using master slave synchronization scheme. Secret parameters of the RBG are revealed where the only information available are the structure of the RBG and a scalar time series observed from the chaotic oscillator. Simulation and numerical results verifying the feasibility of the clone system are given. The RBG doesn't fulfill Diehard and NIST-800-22 statistical test suites, not only the next bit but also the same output bit stream of the RBG can be reproduced.	bitstream;chaos theory;cryptanalysis;linear algebra;numerical analysis;pseudorandomness;rendering (computer graphics);simulation;test suite;time series	Salih Ergün	2015	2015 IEEE 13th International New Circuits and Systems Conference (NEWCAS)	10.1109/NEWCAS.2015.7181993	real-time computing;theoretical computer science;mathematics;synchronization of chaos;distributed computing	Embedded	32.88378469808741	46.71844762775336	153137
5697590830b98a457205a3dafd51763698b9d291	fast passivity verification and enforcement via reciprocal systems for interconnects with large order macromodels	passive macromodels;algorithme rapide;modelizacion;analog simulation;simulation and modeling;passivity verification;integrated circuit interconnections very large scale integration packaging signal design design automation circuit simulation iterative algorithms scattering parameters electromagnetic coupling delay effects;interconnection;tabulated data;parametro s;integrated circuit;parametre s;packaging electronico;modeling and simulation;high frequency modules;circuit vlsi;circuito integrado;scattering parameter signal integrity vlsi interconnects vlsi packages passivity verification reciprocal systems;vlsi design;signal integrity;conception circuit integre;packaging electronique;interconexion;vlsi interconnects;modelisation;etat actuel;integrated circuit design;haute frequence;vlsi circuit;vlsi packages analog simulation and modeling high frequency modules passive macromodels scattering parameters signal integrity tabulated data vlsi interconnects;integrated circuit modelling;reciprocal systems;integrated circuit interconnections;fast algorithm;state of the art;electronic packaging;interconnexion;interconnected system;vlsi packages;s parameters;s parameter;sistema interconectado;vlsi;analog simulation and modeling;scattering parameter;estado actual;vlsi integrated circuit interconnections integrated circuit modelling s parameters;circuito vlsi;systeme interconnecte;simulacion analogica;alta frecuencia;modeling;high speed;high frequency;algoritmo rapido;circuit integre;simulation analogique;scattering parameters	With the ever increasing signal speeds, signal integrity issues of high-speed VLSI designs are presenting increasingly difficult challenges for state-of-the-art modeling and simulation tools. Consequently, characterization and passive macromodeling of high-speed modules such as interconnects, vias, and packages based on tabulated data are becoming important. This paper presents a fast algorithm for passivity verification and enforcement of large order macromodels of scattering parameter based multiport subnetworks. Numerous examples tested on this algorithm demonstrate a significant speed-up compared to the existing algorithms in the literature	algorithm;electrical connection;signal integrity;simulation;speaker wire;very-large-scale integration;via (electronics)	Dharmendra Saraswat;Ramachandra Achar;Michel S. Nakhla	2007	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2007.891085	embedded system;electronic engineering;engineering;electrical engineering;modeling and simulation;scattering parameters	EDA	26.82180410050544	55.54331231019793	153562
530c08819adac401e2c9bede6cdd9edda69d60cb	architectural trends in ghz speed dacs	mixers modems switches;integrated circuit design;digital analogue conversion;integrated circuit design digital analogue conversion;digital analog converter design architectural trend gigahertz frequency dac	Recent interests from the research community in building digitized transmitters has led to numerous architectural and circuit-level developments in the design of digital-to-analog converters (DACs) in the GHz space. Several challenges exist in terms of interface overhead and process capabilities that fundamentally influence the achievable speed and performance numbers. This paper aims to provide the reader with some of the emerging architectural innovations that address these challenges and aid in the transition of DACs into the GHz regime.	capability (systems engineering);digital-to-analog converter;overhead (computing);transmitter	S. Balasubramanian;Waleed Khalil	2012	NORCHIP 2012	10.1109/NORCHP.2012.6403097	electronic engineering;telecommunications;engineering;electrical engineering	EDA	26.910266351135665	60.14198290114155	153784
4a9cf7e86dbaddc0e8b30e984c13c79f5c733e13	efficient fault-tolerant design for parallel matched filters		Parallel matched filters (PMFs) are widely used in digital communication systems for efficient timing synchronization. In this brief, an efficient fault-tolerant design is proposed for PMFs. In the proposed scheme, the correlation between the different filters is used for fault detection and recovery, so that the redundant filters that are needed in existing solutions can be removed, which reduces the implementation cost. The evaluation of practically applied PMFs on an FPGA implementation shows that the proposed scheme can reduce significantly the overheads compared to existing protection methods. The proposed scheme can also be applied to other parallel processing systems with mutual correlation.	fault detection and isolation;fault tolerance;field-programmable gate array;matched filter;parallel computing	Zhen Gao;Ming Zhou;Pedro Reviriego;Juan Antonio Maestro	2018	IEEE Transactions on Circuits and Systems II: Express Briefs	10.1109/TCSII.2017.2713479	parallel computing;field-programmable gate array;fault tolerance;communications system;real-time computing;synchronization;fault detection and isolation;matched filter;computer science;parallel processing	EDA	31.448242609399053	54.527625633985735	153793
53cf6bc53b5e58e277fa4e449203ed768ab70792	data verification and reconciliation with generalized error-control codes	dna;second order;client server network updates;sequences;error correction codes sequences protocols upper bound complexity theory application software computer network reliability telecommunication network reliability dna amino acids;protocols;generalization error;minimum communication;data reconciliation;error correction codes;complexity theory;telecommunication network reliability;application software;lower bounds;upper bounds;file synchronization;communication complexity;graph coloring;data verification;set theory;multiset verification;client server network updates data verification data reconciliation generalized error control codes graph coloring problem upper bounds lower bounds communication complexity error correcting codes code size multiset verification minimum communication coding theory file synchronization;generalized error control codes;upper bound;error correcting codes;forward error correction;error correction code;graph coloring problem;coding theory;code size;amino acids;upper and lower bounds;communication complexity forward error correction set theory graph colouring;graph colouring;computer network reliability	We consider the problem of data reconciliation, which we model as two separate multisets of data that must be reconciled with minimum communication. Under this model, we show that the problem of reconciliation is equivalent to a variant of the graph coloring problem and provide consequent upper and lower bounds on the communication complexity of reconciliation. Further, we show by means of an explicit construction that the problem of reconciliation is, under certain general conditions, equivalent to the problem of finding error-correcting codes for a general class of errors. Under this equivalence, reconciling with little communication is linked to codes with large size, and vice versa. We show analogous results for the problem of multiset verification, in which we wish to determine whether two multisets are equal using minimum communication. As a result, a wide body of literature in coding theory may be applied to the problems of reconciliation and verification.	code;coding theory;communication complexity;error detection and correction;forward error correction;graph coloring;turing completeness	Mark G. Karpovsky;Lev B. Levitin;Ari Trachtenberg	2003	IEEE Transactions on Information Theory	10.1109/TIT.2003.813498	discrete mathematics;theoretical computer science;graph coloring;mathematics;distributed computing;upper and lower bounds	Theory	37.03397503596808	56.96672054352787	153887
50512617c9644caa8e09d1db460103108e604de4	generalized hashing and parent-identifying codes	satisfiability	Let C be a code of length n over an alphabet of q letters. For a pair of integers 2 ≤ t > u, C is (t,u)-hashing if for any two subsets T, U ⊂C, satisfying T ⊂ U, |T| = t, |U| = u, there is a coordinate 1 ≤ i ≤ n such that for any x ∈ T, y ∈ U - x, x and y differ in the ith coordinate. This definition, generalizing the standard notion of a t-hashing family, is motivated by an application in designing the so-called parent identifying codes, used in digital fingerprinting. In this paper, we provide lower and upper bounds on the best possible rate of (t, u)-hashing families for fixed t, u and growing n. We also describe an explicit construction of (t, u)-hashing families. The obtained lower bound on the rate of (t, u)-hashing families is applied to get a new lower bound on the rate of t-parent identifying codes.	code;hash function	Noga Alon;Gérard D. Cohen;Michael Krivelevich;Simon Litsyn	2003	J. Comb. Theory, Ser. A	10.1016/j.jcta.2003.08.001	arithmetic;theoretical computer science;mathematics;algorithm;satisfiability	Theory	38.26669900068474	52.706509570219225	155025
14f6fa72be68b904e83c3f5d796a78acd8146938	optimal quantum control via adaptive error correction	error correction codes;adaptive control;discrete time systems;data mining;process tomography;optimal control;computational modeling;error correction;optimal control adaptive control discrete time systems error correction codes;process control;optimization;quantum control;optimal control programmable control adaptive control error correction tomography quantum computing encoding quantum mechanics computer errors design optimization;encoding;tomography;integrated circuits;optimal error correcting encoding optimal quantum control adaptive error correction black box error correction quantum process tomography	A method is presented to integrate a complete “black-box” error correction scheme, that takes quantum process tomography as input and iterates the control until it finds an optimal error correcting encoding and recovery.	black box;coherent control;error detection and correction;quantum process;quantum tomography	Robert L. Kosut	2009	Proceedings of the 48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control Conference	10.1109/CDC.2009.5400129	control engineering;error detection and correction;optimal control;adaptive control;computer science;engineering;theoretical computer science;process control;control theory;mathematics;tomography;computational model;encoding	Visualization	38.21291674517146	60.13571097452062	155554
2711cbc38133ba14421332ef48071d4be40e253b	exact minimum-repair-bandwidth cooperative regenerating codes for distributed storage systems	distributed data;maintenance engineering distributed databases network coding systematics redundancy encoding upper bound;network coding distributed storage regenerating codes erasure codes repair bandwidth;network coding failure analysis;erasure code;distributed storage;distributed storage system;indexing terms;failure analysis;network coding;network coding exact minimum repair bandwidth cooperative regenerating code distributed storage system data reliability multiple storage node erasure code single failure recovery large storage network multiple failed node lower bound;erasure codes;lower bound;regenerating codes;repair bandwidth	In order to provide high data reliability, distributed storage systems disperse data with redundancy to multiple storage nodes. Regenerating codes is a new class of erasure codes to introduce redundancy for the purpose of improving the data repair performance in distributed storage. Most of the studies on regenerating codes focus on the single-failure recovery, but it is not uncommon to see two or more node failures at the same time in large storage networks. To exploit the opportunity of repairing multiple failed nodes simultaneously, a cooperative repair mechanism, in the sense that the nodes to be repaired can exchange data among themselves, is investigated. A lower bound on the repair-bandwidth for cooperative repair is derived and a construction of a family of exact cooperative regenerating codes matching this lower bound is presented.	clustered file system;download;erasure code;network packet;replication (computing);systematic code	Kenneth W. Shum;Yuchong Hu	2011	2011 IEEE International Symposium on Information Theory Proceedings	10.1109/ISIT.2011.6033778	erasure code;real-time computing;online codes;distributed data store;computer science;theoretical computer science;mathematics;distributed computing;statistics	HPC	35.91515878733057	58.624888884973494	155750
09570a537c18adc409da82764ba84f4106cf6cb0	theoretical limit of the compression for the information		The pit recording of file, the coefficient of compression are introduced. The theoretical limit of the information compression as minimal coefficient of compression for the given length of alphabet are found. Алфавит (А) – конечный набор символов любой природы. Текст – любая последовательность символов, возможно с повторениями. Основной текст – последовательность всех символов алфавита без повторения. Пит – величина, принимающая ровно p-значений.	coefficient	A. Lavrenov	2002	CoRR		data compression ratio;combinatorics;discrete mathematics;theoretical computer science;mathematics	ML	37.757183163364935	51.81081393274085	155920
bc46b1a7e5f5dd7039a04f01ed9634cfd3cf307f	digital block modeling and substrate noise aware floorplanning for mixed signal socs	noise figure noise level spice integrated circuit noise switching circuits noise generators circuit noise clocks integrated circuit modeling signal design;integrated circuit layout;system on chip integrated circuit layout integrated circuit noise mixed analogue digital integrated circuits;substrate noise aware floorplanning;switching noise;system on chip;mixed analogue digital integrated circuits;digital block modeling;digital block modeling system on chip mixed signal soc substrate noise aware floorplanning switching noise;integrated circuit noise;mixed signal soc;substrate noise	In this paper the authors discuss how the problem of substrate-coupled switching noise (dI/dt and dV/dt noise) in mixed signal SOCs can be minimized or resolved by using the combined substrate analysis capabilities of SNA coupled with encounter floorplanning and placement tools. Four aspects: substrate noise modeling and injection, propagation, detection and protection were covered in the paper.	floorplan (microelectronics);mixed-signal integrated circuit;software propagation;system on a chip	William H. Kao;Xiaopeng Dong	2007	2007 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2007.378354	system on a chip;embedded system;substrate coupling;electronic engineering;real-time computing;computer science;engineering;electrical engineering;low-noise amplifier;integrated circuit layout	EDA	24.98946388163676	54.41341948309328	156233
44c2df1ffb926318c1deccae1678f47d137f86e2	layout-aware laser fault injection simulation and modeling: from physical level to gate level	temporal logic cryptography fault simulation laser beam effects logic circuits silicon;temporal logic levels layout aware laser fault injection simulation cryptographic algorithms laser induced fault simulation flow laser spot parameters layout information;integrated circuit modeling circuit faults laser modes laser theory semiconductor lasers transient analysis mathematical model;simulation laser effects modeling security	Fault injection is a technique used by hackers to retrieve secret information in circuits implementing cryptographic algorithms. In particular, laser illuminations have been proven to be a very efficient mean to perform such attacks. In this paper, we present a complete laser-induced fault simulation flow geared towards the evaluation of the resistance of devices against such illuminations at design stage. For that, an accurate physical level modeling of the interaction between lasers and silicon is proposed taking into account both laser spot parameters and position and layout information. The models are abstracted at electrical and temporal/logic levels and included in a multi-level simulator.	algorithm;ambiguous name resolution;cryptography;dots per inch;experiment;fault injection;logic level;silicon on insulator;simulation;temporal logic;vii	Feng Lu;Giorgio Di Natale;Marie-Lise Flottes;Bruno Rouzeyre;Guillaume Hubert	2014	2014 9th IEEE International Conference on Design & Technology of Integrated Systems in Nanoscale Era (DTIS)	10.1109/DTIS.2014.6850665	electronic engineering;real-time computing;computer science;theoretical computer science;logic simulation	EDA	24.78056001271165	53.64923744619336	157115
cc218a4ca2ae551be8ed1f739c3b0a8ee0bfda20	binary locally repairable codes - sequential repair for multiple erasures	electronic mail;parity check codes;binary codes;linear codes;maintenance engineering;decision support systems;distributed databases	Locally repairable codes (LRC) for distributed storage allow two approaches to locally repair multiple failed nodes: 1) parallel approach, by which each newcomer access a set of $r$ live nodes $(r$ is the repair locality$)$ to download data and recover the lost packet; and 2) sequential approach, by which the newcomers are properly ordered and each newcomer access a set of $r$ other nodes, which can be either a live node or a newcomer ordered before it. An $[n,k]$ linear code with locality $r$ that allows local repair for up to $t$ failed nodes by sequential approach is called an $(n,k,r,t)$-exact locally repairable code (ELRC). In this paper, we present a family of binary codes which is equivalent to the direct product of $m$ copies of the $[r+1,r]$ single- parity-check code. We prove that such codes are $(n,k,r,t)$-ELRC with $n=(r+1)^m,k=r^m$ and $t=2^m-1$, which implies that they permit local repair for up to $2^m-1$ erasures by sequential approach. Our result shows that the sequential approach has much bigger advantage than parallel approach.	best, worst and average case;binary code;clustered file system;code rate;download;linear code;locality of reference;network packet;qr code	Wentu Song;Chau Yuen	2016	2016 IEEE Global Communications Conference (GLOBECOM)	10.1109/GLOCOM.2016.7841631	maintenance engineering;binary code;discrete mathematics;computer science;theoretical computer science;mathematics;algorithm	DB	35.93588277282328	58.54192557330765	158286
01297174978370067a1448387b0080caf0019ab8	new multiple covering codes by tabu search		"""The problem of finding multiple coverings words) of the space is considered. of upper and lower bounds for such codes were et aI., Bounds for multiple covering codes, Des. Codes Cryptogr. 3 (1993), The new codes found in this work improve on 27 upper bounds in those tables. The codes were found using tabu search. The of this method is and it is shown how it also can be used to search for large codes. The problem of finding good of Hamming spaces has attracted a lot of attention during the last decade. In this paper binary codes will be discussed. However, many of the results can be to codes over other alphabets. We consider codes over , where = {O, I} is the two-element Galois field. A code is a nonempty set G ~ F:f. In some particular cases we allow G to be a multiset. The rtamrmng distance d(x, y) between two words x, y E Ff is the number of coordinates in which they differ. The distance between a word x and a code G ~ Ff is d(x, G) mincEC d(x, c). A code G is said to be an (n, IGI, r, /1) multiple (Me) if for all x E Ff there is a set of codewords C' ~ G, such that IC'I /1 and d(x, c) r for all c E Cf. Furthermore, if we allow C and G f to be multisets we call the code a m1J,ltiple covering with repeated codewords (MeR). We are now interested in the functions K(n, r, /1) K(n, r, /1) min{M I there is an (n,M,r,/1) MC} and min{M I there is an (n, r, /1) MCR}. It is in practice impossible to determine exact values of these functions in the general case, so effort has been put into obtaining upper and lower bounds. Upper bounds are constructive: they are proved by finding a corresponding code. If /1 1 '""""This research was supported by the Academy of Finland. Austt~alasian Journal of Combinatorics 1995); pp.145-155 we are traditional covering codes, which have been studied; the most recent tables of upper and lower bounds on binary covering codes can be found in [15] and [12], respectively. Earlier results on multiple coverings include those by Clayton [3] and Van Wee et ai. [161. Hamalainen et at. [6] collected bounds on K(n, T, JL) and K(n, T, M) for n 16, M 4. At the end of their introduction mention some reasons why the codes in the paper can be considered reasonably good. Anyhow, in this paper it is shown how efficient and extensive computer searches have led to many as 27 on their upper bounds. New results on lower bounds for multiple covering codes can be found in [2]. In Section the optimization method used in the search, tabu search, is briefly discussed. A matrix method that can be used to find codes is and data structures employed to make the search more efficient are explained. It i8 also mentioned how these can be slightly modified and used in search for other types of In Section 3 the new upper bounds tabulated and compared with the best known old results. Codes corresponding to new bounds are listed in the Appendix."""	academy;binary code;clutter;code (cryptography);code word;data structure;hamming distance;mathematical optimization;matrix method;qr code;tabu search	Patric R. J. Östergård	1995	Australasian J. Combinatorics		guided local search;tabu search;mathematical optimization;computer science	Theory	38.831931845047	52.70371160715098	158477
20ef744e64fbe7710de6d78d5e3d76873bef298f	signature driven hierarchical post-manufacture tuning of rf systems for performance and power	tuning radio frequency transmitters computational modeling mixers power demand mathematical model;radiofrequency integrated circuits cmos integrated circuits integrated circuit modelling integrated circuit yield;computational modeling;radio frequency;tuning;self healing behavioral model design of experiments doe diagnosis post manufacture tuning response surface methodology rsm rf;transmitters;mathematical model;mixers;module level performance metrics signature driven hierarchical post manufacture tuning radiofrequency systems radiofrequency circuits cmos technology severe process variation manufacturing yield yield improvement module level performances system level response top down model diagnosis constrained optimization system level specifications module level tuning parameters;power demand	Integration of RF circuits in deeply scaled CMOS technologies and severe process variation in those technology nodes result in poor manufacturing yield. A post-manufacture tuning approach for yield improvement of RF systems is developed in this paper that uses hierarchical behavioral models of the RF systems. The proposed method first determines module-level performances from the system-level response (signature) to an applied test using top-down model diagnosis. Then, constrained optimization is performed to determine the best module-level tuning parameter values that satisfy system-level specifications (bottom-up analysis) in a power-conscious manner. Both top-down and bottom-up analysis techniques are supported by hierarchical RF behavioral models. The relationship between module-level tuning parameters and module-level performance metrics changes from instance to instance depending on amount of process variation and this factor is included in the proposed tuning procedure. A key benefit of the proposed approach is that only a single-test application is needed at a fixed number of tuning knob settings resulting in reduction in tuning time. The efficiency of the proposed technique is demonstrated using simulation results and hardware experiment.	audio power amplifier;bottom-up proteomics;cmos;computation;constrained optimization;control knob;mathematical optimization;performance;prototype;radio frequency;simulation;top-down and bottom-up design	Aritra Banerjee;Abhijit Chatterjee	2015	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2014.2309114	control engineering;transmitter;electronic engineering;telecommunications;computer science;engineering;electrical engineering;mathematical model;computational model;radio frequency	EDA	24.786627522814147	58.05363756026696	158600
f5cefc25efdf82f3dce70db82d047f0c876c46ea	minimal linear codes over finite fields		As a special class of linear codes, minimal linear codes have important applications in secret sharing and secure two-party computation. Constructing minimal linear codes with new and desirable parameters has been an interesting research topic in coding theory and cryptography. Ashikhmin and Barg showed that wmin/wmax > (q− 1)/q is a sufficient condition for a linear code over the finite field GF(q) to be minimal, where q is a prime power, wmin and wmax denote the minimum and maximum nonzero weights in the code, respectively. The first objective of this paper is to present a sufficient and necessary condition for linear codes over finite fields to be minimal. The second objective of this paper is to construct an infinite family of ternary minimal linear codes satisfying wmin/wmax ≤ 2/3. To the best of our knowledge, this is the first infinite family of nonbinary minimal linear codes violating Ashikhmin and Barg’s condition.	coding theory;cryptography;linear code;secret sharing;secure two-party computation	Ziling Heng;Cunsheng Ding;Zhengchun Zhou	2018	Finite Fields and Their Applications	10.1016/j.ffa.2018.08.010	combinatorics;ternary operation;cryptography;mathematics;finite field;secret sharing;linear code;prime power;coding theory	Theory	38.16569167638491	53.281402285742224	158855
88f1ee795d44091478de058879fa939d93bff238	accurate and computer efficient modelling of single event transients in cmos circuits	simulation ordinateur;concepcion asistida;cmos integrated circuits;simulation electrique;circuit integre cmos;analisis sensibilidad;computer aided design;metodo analitico;diseno circuito;simulacion electrica;radiation effects;duree impulsion;pulse width;arquitectura circuito;puerta logica;transistor level effects;cad tools;parametro circuito;transient pulse amplitude;circuit mos;electrical characteristic;circuit parameter;duracion impulso;cmos circuits;circuit design;error soft;programme spice;circuit architecture;circuito mos;circuito logico;computer efficient modelling;simulation circuit;parametre circuit;network analysis;erreur soft;porte logique;soft error rate;circuit simulation;modelo logico;logic gate level;transients circuit simulation cmos logic circuits integrated circuit modelling radiation effects sensitivity analysis;circuit logique;integrated circuit modelling;set mechanism;erreur estimation;sensitivity analysis;cmos logic circuits;analytical method;transient pulse width;caracteristique electrique;single event transient;architecture circuit;electrical simulation;conception assistee;error estimacion;analyse sensibilite;methode analytique;conception circuit;ionising particle hits;simulacion computadora;transients;soft error rate computer efficient modelling single event transients cmos circuits analytical modelling transient pulse amplitude transient pulse width set mechanism logic gate level sensitivity analysis ionising particle hits transistor level effects cad tools;estimation error;logic model;mos circuit;analytical modelling;computational efficiency;analyse circuit;logic circuit;logic gate;spice;computer simulation;caracteristica electrica;soft error;analisis circuito;modele logique;single event transients;transistor	A new analytical modelling approach to evaluate the impact of single event transients (SETs) on CMOS circuits has been developed. The model allows evaluation of transient pulse amplitude and width (duration) at the logic level, without the need to run circuit level (Spice-like) simulations. The SET mechanism in MOS circuits is normally investigated by Spice-like circuit simulation. The problem is that electrical simulation is time-consuming and must be performed for each different circuit topology, incident particle and track. The availability of a simple model at the logic gate level may greatly improve circuit sensitivity analysis. The electrical response of a circuit to an ionising particle hit depends on many parameters, such as circuit topology, circuit geometry and waveform shape of the charge injection mechanism. The proposed analytical model, which is accurate and computer efficient, captures these transistor-level effects of ionising particle hits and models them to the logic level of abstraction. The key idea is to exploit a model that allows the rapid determination of the sensitivity of any logic gate in a CMOS circuit, without the need to run circuit simulations. The model predicts whether or not a particle hit generates a SET, which may propagate to the next logic gate or memory element, making possible to analyse the sensitivity of each node in a complex circuit. Model derivation is strongly related to circuit electrical behaviour, being consistent with technology scaling. The model is suitable for integration into CAD tools, intending to make automated evaluation of circuit sensitivity to SET possible, as well as automated estimation of soft error rate.	cmos;circuit topology;computer-aided design;electronic circuit simulation;image scaling;logic gate;logic level;spice;soft error;transistor;waveform	Gilson I. Wirth;Michele G. Vieira;Fernanda Gusmão de Lima Kastensmidt	2007	IET Circuits, Devices & Systems	10.1049/iet-cds:20050210	equivalent circuit;computer simulation;control engineering;electronic engineering;logic optimization;asynchronous circuit;logic gate;computer science;engineering;electrical engineering;linear circuit;circuit extraction;synchronous circuit;register-transfer level;discrete circuit	EDA	26.06763684001703	55.76543858602712	158987
084025aa21be7fdd2acccee2530fb579fbd613e8	godpe: global optimization in small signal device model parameter extraction	computational complexity circuit cad integrated circuit design circuit optimisation c language semiconductor device models monte carlo methods;parameter extraction scattering parameters frequency circuit simulation equations gallium arsenide spice large hadron collider optimization methods;parameter extraction;integrated circuit design;c language;computational complexity;semiconductor device models;global optimization;cad gopde global optimization small signal device model device model parameter extraction c language peel algorithm optimisation complexity ic design;circuit cad;circuit optimisation;monte carlo methods;device modeling	A novel global optimization algorithm, GODPE in C, for small signal model parameter extraction is proposed. A “peel” algorithm is developed to relieve the complexity of the optimization.	algorithm;complexity;global optimization;mathematical optimization;small-signal model	Dongfeng Zhao;Ray R. Chen	1994		10.1109/ISCAS.1994.408819	electronic engineering;simulation;computer science;theoretical computer science;mathematics;circuit extraction;computational complexity theory;global optimization;monte carlo method;integrated circuit design	EDA	27.47909333843182	49.48918233826754	159276
eeca5dacd7b1a6d43d8dc608a53bce4f77717ff5	a reconfigurable digital platform for the real-time emulation of broadband copper access networks	digital signal processing;field programmable gate array;broadband networks;communication system;access network;dsl;emulation copper dsl instruments field programmable gate arrays physical layer insertion loss cables bandwidth testing;real time digital signal processor dsp digital subscriber line xdsl emulation field programmable gate array fpga physical layer of communication systems;broadband copper access networks;physical layer;real time;real time emulation;emulation;field programmable gate array fpga;digital subscriber lines;digital signal processor dsp;reconfigurable digital platform real time emulation broadband copper access networks digital signal processing techniques field programmable gate arrays digital subscriber line dsl;technology and engineering;digital subscriber line;digital subscriber line xdsl;digital signal processing techniques;telecommunication equipment broadband networks digital subscriber lines field programmable gate arrays;reconfigurable digital platform;digital signal processor;digital signal processing chips;physical layer of communication systems;field programmable gate arrays;copper;hard real time;telecommunication equipment;real time systems	This paper presents a reconfigurable platform for the real-time digital emulation of broadband copper access networks. The instrument, which uses hard real-time digital-signal-processing techniques on Xilinx Virtex II field-programmable gate arrays, is capable of accurately reproducing the physical layer of a digital subscriber line (DSL). The magnitude and the true phase of the insertion and return loss of the twisted pair access loop, which consists of cables with various characteristics and length, are digitally emulated over the full DSL bandwidth. The innovative character and advantages of digital emulation techniques over conventional analog loop emulation are demonstrated, and its performance is assessed using several test cases.	access network;digital signal processing;digital subscriber line;emulator;field-programmability;field-programmable gate array;local loop;real-time clock;real-time computing;real-time transcription;return loss;test case;twisted pair;virtex (fpga)	Koen Van Renterghem;Jo Pletinckx;Jan Vandewege;Serge Temmerman	2007	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2007.908322	embedded system;electronic engineering;real-time computing;digital subscriber line;telecommunications;computer science;engineering;hardware emulation	Networks	30.334200659744194	59.549236088694855	159516
f038cbe9a0bccc6977ec2d84faec6c471a31d400	a cad methodology for automatic topology selection & sizing	low dropout regulator cad methodology automatic topology selection system on chip analog topology selection sizing flow multilevel screening process geometric programming technology migration automatic node expression generation dc performance evaluation optimal error amplifier topology;network topology;integrated circuit design;system on chip circuit cad electronic design automation geometric programming integrated circuit design network topology;system on chip;circuit cad;geometric programming;topology transistors performance evaluation mathematical model integrated circuit modeling libraries convergence;electronic design automation	Over the years, as system on chip is increasingly incorporating analog functionalities, there is a need of a tool, which can automate the analog topology selection and sizing flow. We develop one efficient methodology to automatically select the best topology given only the user specifications. The proposed topology selection is a multilevel screening process inherited from geometric programming (GP) by sequentially introducing sub-sets of requirements. This helps to prune out unfit topologies at early stage and hence not costing much computational effort. Use of GP ensures fast convergence with optimal solution. Apart from the speed advantage, compared to the existing literature, methodology has better representation of topology performance expressions. We propose to use two levels of performance expressions. First one is technology independent and the second one is technology specific. This bifurcation of performance expressions help in quick technology migration. Third contribution is automatic node expression generation which are required for dc performance evaluation. The proposed methodology is used for selecting optimal error amplifier topology for low dropout regulator (LDO).	analog signal;bifurcation theory;computer-aided design;dropout (neural networks);error amplifier (electronics);geometric programming;low-dropout regulator;performance evaluation;regular expression;requirement;system on a chip;vergence	Supriyo Maji;Pradip Mandal	2011	2011 IEEE International SOC Conference	10.1109/SOCC.2011.6085101	system on a chip;control engineering;embedded system;topology optimization;electronic engineering;geometric programming;electronic design automation;computer science;engineering;engineering drawing;network topology;computer network;integrated circuit design	EDA	26.670657997257702	48.78818965594968	159722
1d9eee9d5b91d86cffb6180a10fb6843c4b9fb11	hard-fault detection and diagnosis during the application of model-based data converter testing	process variation;device under test;output response analysis;test and diagnosis methodologies;mixed signal and analog test;model based testing;converter testing;fault detection and diagnosis	The concept of model-based test was developed in order to reduce the production test effort for data converters (Cherubal and Chatterjee (IEEE Trans Circuits Syst part I 50(3):317–327, 2003); Stenbakken and Souders (1985) Modelling and test point selection for data converter testing. In: ITC, Int Test Conf, pp 813–817; Wegener and Kennedy (IEEE Trans Circuits Syst I 51(1):213–217, 2004); Wrixon and Kennedy (IEEE Trans Instrum Meas IM-48(5):978–985, 1999)). In applying this concept, a vector of model parameters is determined for each device under test (DUT). Typically, this model parameter vector is merely used to calculate the DUT performance characteristic which is then subject to specification-oriented testing. However, each element of the model parameter vector represents an independent error source which contributes to performance degradations; thus, the model parameter vector can be viewed as a signature of the error sources. In this work, analyzing the error source signature is used to devise a model-based methodology for hardfault detection and diagnosis. We investigate conditions under which hard-faults are detectable/diagnosable in spite of masking effects due to manufacturing process Responsible Editor: M. Lubaszewski C. Wegener (B) Infineon Technologies AG, 81726 Munich, Germany e-mail: carsten.wegener@infineon.com M. P. Kennedy Department of Microelectronic Engineering, University College Cork, Cork, Ireland e-mail: peter.kennedy@ucc.ie variations. In particular, we show that taking the model parameter vector as the fault signature is optimal as it minimizes the masking effects and thus maximizes detectability/diagnosibility.	cork encoding;current source;device under test;dictionary attack;electrical element;elegant degradation;email;fault detection and isolation;ingo wegener;lookup table;model-based testing;schematic;simulation;source-to-source compiler;test effort;test point	Carsten Wegener;Michael Peter Kennedy	2007	J. Electronic Testing	10.1007/s10836-007-5050-2	embedded system;electronic engineering;model-based testing;simulation;telecommunications;device under test;engineering;electrical engineering;control theory;process variation;algorithm;statistics	EDA	26.825564213099558	54.490424143809506	159749
3ee9166a1c73de82e94ae57b02d756df003cd3d5	trusted storage over untrusted networks	information theoretic techniques;untrusted networks;encoding network coding cryptography equations computational complexity;trusted storage;matrix algebra cryptography encoding;distributed storage;low complexity;matrix algebra;network coding;computational complexity;cryptography;vandermonde matrix;coding;information theoretic techniques trusted storage untrusted networks distributed storage coding algebraic structure vandermonde matrix;algebraic structure;encoding;information theoretic	We consider distributed storage over two untrusted networks, whereby coding is used as a means to achieve a prescribed level of confidentiality. The key idea is to exploit the algebraic structure of the Vandermonde matrix to mix the input blocks, before they are stored in different locations. The proposed scheme ensures that eavesdroppers with access to only one of the networks are unable to decode any symbol even if they are capable of guessing some of the missing blocks. Information-theoretic techniques allow us to quantify the achievable level of confidentiality. Moreover, the proposed approach is shown to offer low complexity and optimal rate.	byzantine fault tolerance;clustered file system;computational complexity theory;confidentiality;line code;linear network coding;theory;threat model;vandermonde matrix	Paulo F. Oliveira;Luísa Lima;Tiago T. V. Vinhoza;João Barros;Muriel Médard	2010	2010 IEEE Global Telecommunications Conference GLOBECOM 2010	10.1109/GLOCOM.2010.5683473	vandermonde matrix;discrete mathematics;distributed data store;computer science;cryptography;algebraic structure;theoretical computer science;distributed computing;coding;encoding;statistics	HPC	35.658140545959704	59.04337515097509	159807
e9d190a7367f7b0afb6762e2717bc06d2bd0554a	the unified theory of pseudorandomness: guest column	list decoding;error correction code;expander graph;randomness extractors	"""We survey the close connections between a variety of """"pseudorandom objects,"""" namely pseudorandom generators, expander graphs, list-decodable error-correcting codes, randomness extractors, averaging samplers, and hardness amplifiers."""	amplifier;code;error detection and correction;forward error correction;hardness of approximation;pseudorandom generator;pseudorandomness;randomness extractor	Salil P. Vadhan	2007	SIGACT News	10.1145/1324215.1324225	list decoding;combinatorics;discrete mathematics;error detection and correction;expander graph;theoretical computer science;expander code;mathematics;pseudorandomness;pseudorandom generator theorem	Theory	39.02123204026348	55.83171645417069	160272
86e934c6d68ac26ee3776a0e6aca5b38e00b6160	givens rotation-based qr decomposition for mimo systems		QR decomposition is an essential operation in various detection algorithms utilised in multiple-input multiple-output (MIMO) wireless communication systems. This study presents a Givens rotation-based QR decomposition for 4 × 4 MIMO systems. Instead of performing QR decomposition by coordinate rotation digital computer (CORDIC) algorithms, LUT compression algorithms are employed to rapidly evaluate the trigonometric functions. The proposed approach also provides greater accuracy compared with the CORDIC algorithms. QR decomposition is performed by complex Givens rotations cascaded with real Givens rotations. In complex Givens rotations, a modified triangular systolic array is adopted to reduce the delay units of the design and hence, reducing the hardware complexity. The proposed QR decomposition algorithm is implemented in TSMC 90 nm CMOS technology. It achieves the throughput of 53.5 million QR decompositions per second when operating at 214 MHz.	givens rotation;mimo;qr decomposition	Wen Fan;Amir Alimohammad	2017	IET Communications	10.1049/iet-com.2016.0789	qr decomposition;givens rotation	Networks	32.27393234582726	58.37533246343264	161010
083543db2396358d171dddd6e025f0aa4a5cf3eb	linear-time decoding of regular expander codes	decoding;error correcting codes;expander codes;ldpc codes	Sipser and Spielman (IEEE IT, [1996]) showed that any c,d)-regular expander code with expansion parameter >¾ is decodable in linear time from a constant fraction of errors. Feldman et al. (IEEE IT, [2007]) proved that expansion parameter >⅔ + 1/3c is sufficient to correct a constant fraction of errors in polynomial time using LP decoding.  In this work, we give a simple combinatorial algorithm that achieves even better parameters. In particular, our algorithm runs in linear time and works for any expansion parameter >⅔ − 1/6c. We also prove that our decoding algorithm can be executed in logarithmic time on a linear number of parallel processors.	algorithm;central processing unit;combinatorial optimization;expander code;lp-type problem;linear programming decoding;michael sipser;parallel computing;time complexity	Michael Viderman	2011	TOCT	10.1145/2493252.2493255	list decoding;combinatorics;discrete mathematics;low-density parity-check code;sequential decoding;expander code;mathematics;berlekamp–welch algorithm;error floor;algorithm	Theory	37.79185674224445	55.72535991288929	161333
6da1ac4dd2c1f50f0188ffd30773d43b3dd73160	a deterministic reduction for the gap minimum distance problem	approximate algorithm;random polynomials;approximation algorithm;polynomial time algorithm;minimum distance problem;coding theory;minimum distance;linear code;polynomial time;np complete	Determining the minimum distance of a linear code is one of the most important problems in algorithmic coding theory. The exact version of the problem was shown to be NP-complete in [14]. In [8], the gap version of the problem was shown to be NP-hard for any constant factor under a randomized reduction. It was shown in the same paper that the minimum distance problem is not approximable in randomized polynomial time to the factor 2log1-ε n unless NP ⊆ RTIME(2polylog(n)). In this paper, we derandomize the reduction and thus prove that there is no deterministic polynomial time algorithm to approximate the minimum distance to any constant factor unless P=NP. We also prove that the minimum distance is not approximable in deterministic polynomial time to the factor 2log1-εn unless NP ⊆ DTIME(2polylog(n)). As the main technical contribution, for any constant 2/3< ρ < 1, we present a deterministic algorithm that given a positive integer s, runs in time poly(s) and constructs a code C of length poly(s) with an explicit Hamming ball of radius ρ d(C) such that a projection at some s coordinates sends the codewords in the ball surjectively onto a linear subspace of dimension s, where d(C) denotes the minimum distance of C. The codes are obtained by concatenating Reed-Solomon codes with Hadamard codes.	approximation algorithm;code word;coding theory;concatenation;deterministic algorithm;folded reed–solomon code;hadamard transform;linear code;np-completeness;np-hardness;p (complexity);p versus np problem;polynomial;rp (complexity);randomized algorithm;time complexity;window function	Qi Cheng;Daqing Wan	2009	IEEE Transactions on Information Theory	10.1145/1536414.1536421	time complexity;mathematical optimization;combinatorics;discrete mathematics;np-complete;computer science;linear code;mathematics;approximation algorithm;algorithm;coding theory	Theory	37.72880281879473	55.48373521367913	163001
6026e765ec87af69c85df4aa13b20b6c6d7ec820	lower bounds on almost-separating binary codes	binary codes data security decoding error probability vectors concatenated codes fingerprint recognition;binary codes;traitor tracing;fingerprinting code lower bounds almost separating binary code automata synthesis technical diagnosis traitor tracing scheme;lower bound	Separating codes have been used in many areas as diverse as automata synthesis, technical diagnosis and traitor tracing schemes. In this paper, we study a weak version of separating codes called almost separating codes. More precisely, we derive lower bounds on the rate of almost separating codes. From the main result it is seen that the lower bounds on the rate for almost separating codes are greater than the currently known lower bounds for ordinary separating codes. Moreover, we also show how almost separating codes can be used to construct a family of fingerprinting codes.	automata theory;binary code;code word;concatenated error correction code;concatenation;fingerprint (computing);time complexity;traitor tracing;typical set	José Moreira;Gregory A. Kabatiansky;Marcel Fernandez	2011	2011 IEEE International Workshop on Information Forensics and Security	10.1109/WIFS.2011.6123141	block code;reed–muller code;concatenated error correction code;turbo code;discrete mathematics;online codes;fountain code;theoretical computer science;serial concatenated convolutional codes;bcjr algorithm;tornado code;linear code;expander code;luby transform code;mathematics;raptor code;error floor;algorithm	Theory	38.7416192863389	55.53564727975785	164276
bf7a63a764a421789ce9e9acb5e75401c65b444f	synthesis of feedback decoders for initialized encoders	decoder existence feedback decoder synthesis initialized encoder data processing encoder circuit decoder circuit decoder synthesis automation encoder specification;network synthesis;codecs;network synthesis codecs feedback;satisfiability solving craig interpolation decoder encoder finite state transition system;finite state transition system;satisfiability solving;feedback;logic synthesis;encoder;decoding interpolation history benchmark testing encoding algorithm design and analysis reachability analysis;craig interpolation;decoder	Encoding and decoding are common practice in data processing. Designing encoder and decoder circuitry manually can be error prone and time consuming. Although great progress has been made on automating decoder synthesis from its encoder specification, prior specification was limited to an uninitialized encoder only, whose decoder in turn cannot depend on the entire execution history of the encoder. Prior decoder existence condition is unnecessarily stringent as encoders are often initialized to some specific starting states. This paper shows how decoders of initialized encoders can be practically synthesized. Experimental results demonstrate effective decoder synthesis of initialized encoders, beyond existing methods' capabilities.	codec;cognitive dimensions of notations;computation;electronic circuit;encoder;interpolation;reachability;window function	Kuan-Hua Tu;Jie-Hong Roland Jiang	2013	2013 50th ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2463209.2488794	network synthesis filters;encoder;electronic engineering;codec;logic synthesis;real-time computing;soft-decision decoder;computer science;theoretical computer science;feedback;adaptive coding;decoder;one-hot;viterbi decoder;statistics	EDA	26.376076865716882	46.61455505103188	164417
028bf8b4fb457abc1e48f433ad104b0054e5090b	a clock network of distributed adplls using an asymmetric comparison strategy	phase detectors clocks digital phase locked loops;detectors;convergence;all digital phase locked loop;clocks;oscillators;digital phase locked loops;clock network;simulation;network convergence;phase locked loops;phase detectors;bang bang phase detector;asymmetric comparison strategy;clocks phase locked loops phase detection detectors convergence frequency synchronization integrated circuit interconnections robustness energy consumption feedback loop;synchronization;network convergence clock network asymmetric comparison strategy distributed all digital phase lock loop network bang bang phase detector;distributed all digital phase lock loop network;phase detector	In this paper, we describe an architecture of a distributed ADPLL (All Digitall Phase Lock Loop) network based on bang-bang phase detectors that are interconnected asymmetrically. It allows an automatic selection between two operating modes (uni- and bidirectional) to avoid mode-locking phenomenon, to accelerate the network convergence and to improve the robustness to possible network failures in comparison to simple unidirectional mode.	ambiguous name resolution;bang file;cmos;clock network;computer simulation;device configuration overlay;lock (computer science);network convergence;pdf/a;phase-locked loop;sensor;software propagation	Anton Korniienko;Éric Colinet;Gérard Scorletti;Eric Blanco;Dimitri Galayko;Jérôme Juillard	2010	Proceedings of 2010 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2010.5537932	phase detector;control engineering;synchronization;detector;electronic engineering;phase-locked loop;convergence;computer science;network convergence;control theory;oscillation	Arch	27.793057750296736	54.67746677301953	164445
7317f4a3f5c3da06dd321c1dfa80161660409eb8	a new digital front-end for flexible reception in software defined radio	elektroteknik och elektronik;lte;wlan;concurrency;multi standard;dfe;dvb h	Future mobile terminals are expected to support an ever increasing number of Radio Access Technologies (RAT) concurrently. This imposes a challenge to terminal designers already today. Software Defined Radio (SDR) solutions are a compelling alternative to address this issue in the digital baseband, given its high flexibility and low Non-Recurring Engineering (NRE) cost. However, the challenge still remains in the Digital Front-End (DFE), where many operations are too complex or energy hungry to be implemented as software instructions. Thus, new architectures are needed to feed the SDR digital baseband while keeping complexity and energy consumption at bay. In this article the architecture of a Digital Front-End Receiver (DFE-Rx) for the next-generation mobile terminals is presented. The flexibility needed for multi-standard support is demonstrated by detecting, synchronizing and reporting carrier-frequency offset, of multiple concurrent radio standards. Moreover, the proposed architecture has been fabricated in a 65 nm CMOS low power high-VT cell technology in a die size of 5 mm. The core module of the DFE-Rx, the synchronization engine, has been measured at 1.2 V and reports an average power consumption of 1.9 mW during Wireless Local Area Network (WLAN) reception and 1.6 mW during configuration, while running at 10 MHz. 2015 Elsevier B.V. All rights reserved.	algorithm;baseband;cmos;cpu power dissipation;etsi satellite digital radio;frequency offset;hardware acceleration;mobile phone;overhead (computing);rx microcontroller family;scalability;sensor	Isael Diaz;Chenxin Zhang;Lieven Hollevoet;Jim Svensson;Joachim Neves Rodrigues;Leif R. Wilhelmsson;Thomas Olsson;Liesbet Van der Perre;Viktor Öwall	2015	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2015.03.001	embedded system;parallel computing;real-time computing;concurrency;telecommunications;computer science;electrical engineering;lte advanced;operating system;computer network	HCI	30.48299079653182	59.357340002595464	165232
180873e251f498ab36818d23de4871b2ac4f76f8	a hierarchical design automation concept for analog circuits	integrated circuit modeling;statistics;mathematical model;optimization;spice;algorithm design and analysis;sociology	This paper presents a new approach to hierarchically synthesize analog circuits. In general, behavioral models are preferred at intermediate levels to reduce total synthesis time. However, there are problems associated with the usage of behavioral models such as significantly sacrificing the accuracy and costly preparation time for model generation. Therefore, a model-free approach is proposed, in which behavioral models are eliminated at higher level. Top level specifications and sub-block performances are optimized simultaneously during the synthesis process, where performance requirements of sub-blocks are arranged automatically. A third order low pass Butterworth filter is used as an example to show the effectiveness of the proposed approach.	algorithm;analogue electronics;bottom-up parsing;butterworth filter;cmos;iteration;mathematical optimization;nonlinear system;performance;requirement;simulation;software performance testing;top-down and bottom-up design	Gönenç Berkol;Engin Afacan;Günhan Dündar;E. V. Fernandez	2016	2016 IEEE International Conference on Electronics, Circuits and Systems (ICECS)	10.1109/ICECS.2016.7841150	electronic engineering;simulation;computer science;artificial intelligence	EDA	26.62020842785461	48.7845288369314	165329
5759a1b29984e28c1693c6c222818a336a67be7c	a new customized testing technique using a novel design of droplet motion detector for digital microfluidic biochip systems	testing microfluidics circuit faults capacitance electrodes layout routing;dispensers customized testing technique droplet motion detector digital microfluidic biochip systems dfmb system lab on chip system chemical droplet 2d planar electrode array safety critical nature reliability dependability assay execution capacitive sensing cell electrodes droplet presence droplet arrival bioassay;microfluidics;microsensors capacitive sensors chemical sensors chemical variables measurement drops lab on a chip microfluidics;test completion time droplet motion detector customized testing digital micro fluidics routing layout;lab on a chip;microsensors;drops;capacitive sensors;chemical variables measurement;chemical sensors	Digital microfluidic biochip (DFMB) systems have been developed as a promising platform for Lab-on-chip systems that manipulate individual droplet of chemicals on a 2D planar array of electrodes. Because of the safety critical nature of the applications these devices are intended for high reliability and thereby dependability becomes a major issue for the design of DMFBs. Therefore, such devices are required to be tested frequently both off-line (e.g., post-manufacturing) and prior to each assay execution. Under both scenarios, testing is accomplished by routing one or more test droplets across the chip and recording their arrival at the scheduled destination. In this paper, we have proposed a new design of a droplet motion detector based on capacitive sensing, which can be manufactured with the cell electrodes for detection of the presence (arrival) of a droplet at a predetermined location. Using this sensor, we have further proposed a customized testing technique for a specified layout with an objective of 1) optimizing the total number of test droplets for testing a particular bioassay, 2) optimizing the number of dispensers, 3) minimizing the overall test completion time, 4) detection of a specific segment at fault within the given layout, and 5) optimizing the number of locations where the detectors are to be activated. The test simulation has been carried out on two testbenches of Benchmark suite III and the results are found to be encouraging compared to the existing methods.	benchmark (computing);biochip;capacitive sensing;dependability;emoticon;functional testing;informatics;motion detector;online and offline;peripheral;routing;sensor;simulation	Pranab Roy;Hafizur Rahaman;Partha Sarathi Gupta;Parthasarathi Dasgupta	2013	2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2013.6637295	embedded system;electronic engineering;microfluidics;lab-on-a-chip;engineering;electrical engineering;capacitive sensing	EDA	26.527707371147795	51.22162756843526	165412
e38901929578ca7ce1f26623e98e099c0a1db58a	a criticism of the acw algorithm	data compression;acw algorithm;huffman algorithm	Data compression via the Huffman algorithm, which is a data compression technique, is the most efficient technique between single symbol data compression techniques. This algorithm is counted among statistical data compression techniques. Many efforts have been made for optimizing this technique and some algorithms have been presented, too. One of these algorithms is the ACW algorithm which is presented by Hussein Al-Bahadili et al. At first, they used the Huffman algorithm and then used the results of the Huffman algorithm as the input of their algorithm to calculate the optimum character wordlength. So, they decreased the stored characters via increasing the character wordlength for doing the compression. In this paper, we examined and criticized the ACW algorithm and presented some of its weaknesses via suitable counterexamples. At the end, it was concluded that the optimum character wordlength can never be calculated by extant techniques. © 2012 Elsevier Ltd. All rights reserved.	acorn business computer;algorithm;data compression;huffman coding	M. Saleh Mohammadi;Ali Naserasadi	2012	Computers & Mathematics with Applications	10.1016/j.camwa.2012.06.024	data compression;arithmetic;computer science;theoretical computer science;mathematics;algorithm;statistics;huffman coding	Theory	34.971787094602085	55.724221720872805	165729
551c1780113d7b35e25ec26c0a64823d8f6e7262	multi-map orbit hopping chaotic stream cipher	spread spectrum;stream cipher;digital communication;initial condition	Abstract In this paper we propose a multi-map orbit hopping chaotic stream cipher that utilizes the idea of spread spectrum mechanism for secure digital communications and fundamental chaos characteristics of mixing, unpredictable, and extremely sensitive to initial conditions. The design, key and subkeys, and detail implementation of the system are addressed. A variable number of well studied chaotic maps form a map bank. And the key determines how the system hops between multiple orbits, and it also determines the number of maps, the number of orbits for each map, and the number of sample points for each orbits. A detailed example is provided.	frequency-hopping spread spectrum;initial condition;map;secure digital;stream cipher	Xiaowen Zhang;Li Shu;Ke Tang	2006	CoRR		telecommunications;computer science;theoretical computer science;mathematics;distributed computing;stream cipher;spread spectrum;initial value problem	ML	35.31512036535459	46.84834375677105	165742
77e1e5dc0640e1a9e9b2190a4e365e79b703dac1	on a parity based group testing algorithm	combinatorial group testing	In traditional Combinatorial Group Testing the problem is to identify up to d defective items from a set of n items on the basis of group tests. In this paper we describe a variant of the group testing problem above, which we call parity group testing. The problem is to identify up to d defective items from a set of n items as in the classical group test problem. The main difference is that we check the parity of the defective items in a subset. The test can be applied to an arbitrary subset of the n items with two possible outcomes. The test is positive if the number of defective items in the subset is odd, otherwise it is negative. In this paper we extend Hirschberg et al.’s method to the parity group testing scenario.	binary logarithm;combinatorial search;encoder;hirschberg's algorithm;parity bit;search algorithm	Sándor Z. Kiss;Éva Hosszu;Lajos Rónyai;János Tapolcai	2015	Acta Cybern.	10.14232/actacyb.22.2.2015.12	computer science	ML	37.0412275861374	51.46663489064334	165794
c2e0139a30a9791f390f3c82023dd6b90265d5a4	digital microfluidic logic gates and their application to built-in self-test of lab-on-chip	bioassay protocol digital microfluidic logic gates lab on chip pulse sequence analysis built in self test method bist and or not capacitive sensing test outcome circuits biochip design;bist;or;multiple testing;prototypes;bioassay protocol;microfluidics biomems built in self test lab on a chip logic gates;capacitive sensing test outcome circuits;logic circuits;testing;testing dependability lab on chip logic gates microfluidics;chip;pulse sequence analysis;digital microfluidic logic gates;not;electrodes;built in self test;logic gates;built in self test method;dependability;microfluidics;logic testing;test methods;immune system;lab on a chip;robustness;circuit testing;sequence analysis;and;microfluidics logic gates built in self test electrodes circuit testing logic testing immune system prototypes logic circuits robustness;biochip design;logic gate;lab on chip;biomems	Dependability is an important system attribute for microfluidic lab-on-chip. Robust testing methods are therefore needed to ensure correct results. Previously proposed techniques for reading test outcomes and for pulse-sequence analysis are cumbersome and error prone. We present a built-in self-test (BIST) method for digital microfluidic lab-on-chip. This method utilizes digital microfluidic logic gates to implement the BIST architecture; AND, OR and NOT gates are used to compress multiple test-outcome droplets into a single droplet to facilitate detection with low overhead. These approaches obviate the need for capacitive sensing test-outcome circuits for analysis. We also apply the BIST architecture to a pin-constrained biochip design. A multiplexed bioassay protocol is used to evaluate the effectiveness of the proposed test method.	and gate;biochip;biological assay;built-in self-test;capacitive sensing;cognitive dimensions of notations;compresses (device);dependability;electrowetting;exclusive or;fault detection and isolation;functional testing;logic gate;mechanical computer;microelectromechanical systems;microfluidics;multiplexing;overhead (computing);sequence analysis;xor gate	Yang Zhao;Krishnendu Chakrabarty	2010	IEEE Transactions on Biomedical Circuits and Systems	10.1109/TBCAS.2010.2048567	embedded system;electronic engineering;logic gate;engineering;nanotechnology;computer engineering	EDA	26.259660009618372	51.27189250770082	165889
9887f42deff301bfe529ad412ea16fccefacb5c0	spice behavioral modeling of rf current injection in wire bundles	behavioral modeling		behavioral modeling;radio frequency;spice	Flavia Grassi;Giordano Spadacini;Sergio Amedeo Pignari	2014	IEICE Transactions		behavioral modeling;embedded system;computer science	EDA	25.044161816495272	54.21136646023556	166175
66e24a631dbc12dadf02e87c02f5d3d706a0d8ee	optimal modes of operation of pseudorandom sequence generators based on dlfsrs			block cipher mode of operation;pseudorandom number generator	Alberto Peinado;Jorge Munilla;Amparo Fúster-Sabater	2016	Logic Journal of the IGPL	10.1093/jigpal/jzw050	pseudorandom generators for polynomials;theoretical computer science;pseudorandom binary sequence;pseudorandom noise;pseudorandom number generator;pseudorandom generator theorem;algorithm	Crypto	31.391626553292387	47.08647744119	166441
2430dbbdae4cc87e868e947ce9f0828d574909df	efficient hardware generation of random variates with arbitrary distributions	random number generator;probability density;target distribution;application software;probability density function;random number generation;polynomials;hardware architecture;target distribution probability density function probability distribution hardware architecture random number generation software approximation generator;computer architecture;statistical distributions;computational modeling;hardware random number generation probability distribution sampling methods software algorithms application software computational modeling polynomials computer architecture probability density function;statistical distributions random number generation;probability distribution;software algorithms;software approximation generator;sampling methods;random numbers;hardware	This paper presents a technique for efficiently generating random numbers from a given probability distribution. This is achieved by using a generic hardware architecture, which transforms uniform random numbers according to a distribution mapping stored in RAM, and a software approximation generator that creates distribution mappings for any given target distribution. This technique has many features not found in current non-uniform random number generators, such as the ability to adjust the target distribution while the generator is running, per-cycle switching between distributions, and the ability to generate distributions with discontinuities in the probability density function	approximation;http 404;random number generation;random-access memory;run time (program lifecycle phase)	David B. Thomas;Wayne Luk	2006	2006 14th Annual IEEE Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2006.39	random variate;probability distribution;probability density function;stability;convolution of probability distributions;mixture distribution;computer science;theoretical computer science;degenerate distribution;circular distribution;stochastic simulation;hardware architecture;pseudo-random number sampling;convolution random number generator;uniform distribution;moment-generating function;probability integral transform	Theory	28.424169069070413	51.5437720036527	167356
0450fa59327d18af5f4ec641e15090b86ac4c5bc	sphere-packing bounds for convolutional codes	convolutional code;code convolutif;sphere packing;channel coding;convolutional codes;code treillis;computational complexity convolutional codes trellis codes;high rate convolutional codes general sphere packing bounds heller bound limited bit oriented trellis complexity efficient computer search;codigo treillis;trellis code;codage canal;codigo convolutivo;high rate codes;computational complexity;trellis codes;trellis complexity;convolutional codes councils information theory informatics	We introduce general sphere-packing bounds for convolutional codes. These improve upon the Heller (1968) bound for high-rate convolutional codes. For example, based on the Heller bound, McEliece (1998) suggested that for a rate (n - 1)/n convolutional code of free distance 5 with /spl nu/ memory elements in its minimal encoder it holds that n /spl les/ 2/sup (/spl nu/+1)/2/. A simple corollary of our bounds shows that in this case, n < 2/sup /spl nu//2/, an improvement by a factor of /spl radic/2. The bound can be further strengthened. Note that the resulting bounds are also highly useful for codes of limited bit-oriented trellis complexity. Moreover, the results can be used in a constructive way in the sense that they can be used to facilitate efficient computer search for codes.	convolutional code;encoder;memory bound function;search algorithm;set packing;trellis quantization	Eirik Rosnes;Øyvind Ytrehus	2004	IEEE Transactions on Information Theory	10.1109/TIT.2004.836672	combinatorics;convolutional code;telecommunications;theoretical computer science;mathematics;algorithm;statistics	Theory	38.91112833143895	56.85343400190939	167435
7b70c24d6e359081a6a0159f9bdf4cc5e022aa95	writing cosets of a convolutional code to increase the lifetime of flash memory	convolutional codes;error correction codes;hamming codes;communication complexity;encoding flash memories logic gates error correction codes vectors writing measurement;redundancy;hamming code convolutional code flash memory lifetime redundancy write process coded writes multilevel cell binary vector viterbi algorithm viterbi metrics decoding complexity encoding complexity error correction code;viterbi decoding;flash memories;viterbi decoding communication complexity convolutional codes error correction codes flash memories hamming codes redundancy	The goal of this paper is to extend the lifetime of Flash memory by reducing the frequency with which a given page of memory is erased. This is accomplished by increasing the number of writes that are possible before erasure is necessary. Redundancy is introduced into the write process to decrease the number of memory cells that are impacted by a given write, and to even out the impact of writing across an entire page of memory. Improvements are expressed in terms of write efficiency and lifetime gain. Write efficiency is the ratio of cells written to cells available, and lifetime gain is the ratio of coded writes to the baseline of uncoded writing. We use a physical model that allows multiple writes to a given region of memory. This can be realized with single level cells or with multi-level cells. Data is written to memory in the form of a coset of a convolutional code. The coset is represented by a binary vector that is selected by the Viterbi algorithm to minimize the number of cells impacted by the write (Hamming weight) and to even out the number of writes to each cell within a given page. Several different Viterbi metrics are evaluated. It is shown that page write efficiencies of over 85% and lifetime gains of over 500% are possible with only modest encoding and decoding complexity. It is also straightforward to integrate lifetime extension with standard methods of error correction by requiring that the coset representative be drawn from an error correcting code. An example is provided where single error correction is provided using a Hamming code.	baseline (configuration management);bit array;convolutional code;error detection and correction;flash memory;forward error correction;hamming code;hamming weight;memory cell (binary);viterbi algorithm	Adam N. Jacobvitz;A. Robert Calderbank;Daniel J. Sorin	2012	2012 50th Annual Allerton Conference on Communication, Control, and Computing (Allerton)	10.1109/Allerton.2012.6483234	concatenated error correction code;turbo code;convolutional code;parallel computing;real-time computing;constant-weight code;sequential decoding;computer science;theoretical computer science;cyclic code;serial concatenated convolutional codes;linear code;hamming code;forward error correction;error floor	Embedded	37.42592320707335	58.790265656063504	167676
8600ef4dc4a9f1c55f7d14d185096dc7561367cd	a computer-aided design framework for modeling and simulation of vlsi interconnections and packaging	simulation ordinateur;encapsulation;modelizacion;software;concepcion asistida;module multipuce;presentation information;computer aided design;microelectronic fabrication;representation graphique;fabricacion microelectrica;architecture systeme;interconnection;integrated circuit;logiciel;modeling and simulation;implementation;representacion grafica;simulation;information layout;circuit vlsi;circuito integrado;encapsulacion;integration;modelisation;ejecucion;multichip module;vlsi circuit;object oriented;integracion;electronic packaging;interconnexion;conception assistee;oriente objet;logicial;arquitectura sistema;tool integration;simulacion computadora;circuito vlsi;system architecture;modeling;orientado objeto;presentacion informacion;computer simulation;graphics;cad framework;circuit integre;interconeccion;fabrication microelectronique	The higher speed requirement and rising complexity of interconnect and packaging structure in a VLSI system have increased the necessity of applying modeling and simulation techniques to develop CAD tools for analysis and design. To effectively manage design data and CAD tools involved for modeling and simulation of electronic packaging, a framework which provides different levels of services and abstractions is essential. This paper describes a computer-aided design framework which provides three levels of services for the aforementioned purposes. The first level of the framework supports CAD tool integrations and simulation management. A common graphical user interface is provided for the simulation environment. In the second level, design data representation and management are stressed. We applied an object-oriented approach to develop design libraries and encapsulate CAD tools. The third level of the framework emphasizes system level modeling and simulation for multiple chip systems. The underlying architecture and implementation of the framework are explained, design examples given.	best, worst and average case;cmos;centralized computing;characteristic impedance;clock rate;computer-aided design;data (computing);database;delay calculation;electrical connection;electronic packaging;fan-out;graphical user interface;intel quickpath interconnect;library (computing);lint (software);logic gate;multi-chip module;nmos logic;pf (firewall);power inverter;propagation delay;simulation;software propagation;spectral density estimation;transistor;transmission line;velocity (software development);very-large-scale integration;voltage source;von luschan's chromatic scale;wiring;xfig	Pochang Hsu;Jerzy W. Rozenblit	1994	Integration	10.1016/0167-9260(94)00010-7	computer simulation;embedded system;electronic engineering;systems modeling;encapsulation;computer science;engineering;electrical engineering;graphics;operating system;integrated circuit;interconnection;computer aided design;modeling and simulation;electronic packaging;programming language;object-oriented programming;implementation;computer engineering	EDA	28.70635725374029	55.07692815597581	167722
19dfa74906fdd8074e559ac640f814d93b7a687c	on graphs and codes preserved by edge local complementation	05c75;binary linear codes;edge local complementation;94a15;isomorphism;graphs;94b05;error correction code;05c30;linear code;hamming code;bipartite graph;information theory;pivot;exhaustive search	Orbits of graphs under local complementation (LC) and edge local complementation (ELC) have been studied in several different contexts. For instance, there are connections between orbits of graphs and errorcorrecting codes. We define a new graph class, ELC-preserved graphs, comprising all graphs that have an ELC orbit of size one. Through an exhaustive search, we find all ELC-preserved graphs of order up to 12 and all ELC-preserved bipartite graphs of order up to 16. We provide general recursive constructions for infinite families of ELC-preserved graphs, and show that all known ELC-preserved graphs arise from these constructions or can be obtained from Hamming codes. We also prove that certain pairs of ELC-preserved graphs are LC equivalent. We define ELC-preserved codes as binary linear codes corresponding to bipartite ELC-preserved graphs, and study the parameters of such codes.	binary golay code;brute-force search;carrier-to-noise ratio;code word;enumerated type;forward error correction;graph (discrete mathematics);graph theory;hamming code;iterative method;recursion;ternary golay code;μ-recursive function	Lars Eirik Danielsen;Matthew G. Parker;Constanza Riera;Joakim Grahl Knudsen	2015	Des. Codes Cryptography	10.1007/s10623-013-9876-6	strong perfect graph theorem;1-planar graph;pathwidth;split graph;combinatorics;discrete mathematics;cograph;error detection and correction;topology;bipartite graph;graph product;dense graph;information theory;metric dimension;factor graph;hamming graph;brute-force search;linear code;trapezoid graph;hamming code;mathematics;geometry;odd graph;maximal independent set;graph isomorphism;isomorphism;modular decomposition;graph;partial k-tree;chordal graph;indifference graph;statistics	Theory	38.60945830040652	52.07194663255418	167828
3c4b67f9bcc4af8c216066de7a6656a313f74838	algorithms for the minimum weight of linear codes		We outline the algorithm for computing the minimum weight of a linear code over a finite field that was invented by A. Brouwer and later extended by K.-H. Zimmermann. We show that matroid partitioning algorithms can be used to efficiently find a favourable (and sometimes best possible) sequence of information sets on which the Brouwer-Zimmermann algorithm operates. We present a new algorithm for computing the minimum weight of a linear code. We use a large set of codes to compare our new algorithm with the Brouwer-Zimmermann algorithm. We find that for about one third of codes in this sample set, our algorithm requires to generate fewer codewords than the Brouwer-Zimmermann algorithm.	algorithm;brouwer fixed-point theorem;code word;linear code;matroid partitioning;minimum weight	Petr Lisonek;Layla Trummer	2016	Adv. in Math. of Comm.		linear code;error floor	Theory	39.10072059833391	52.9694913647576	169398
462ba1cbfab7fea9f09decfd505f01c8fbff4d0d	circuit design and technological limitations of silicon rfics for wireless applications	circuit design;electrical engineering and computer science;thesis	Semiconductor technologies have been a key to the growth in wireless communication over the past decade, bringing added convenience and accessibility through advantages in cost, size, and power dissipation. A better understanding of how an IC technology affects critical RF signal chain components will greatly aid the design of wireless systems and the development of process technologies for the increasingly complex applications that lie on the horizon. Many of the evolving applications will embody the concept of adaptive performance to extract the maximum capability from the RF link in terms of bandwidth, dynamic range, and power consumption-further engaging the interplay of circuits and devices is this design space and making it even more difficult to discern a clear guide upon which to base technology decisions. Rooted in these observations, this research focuses on two key themes: 1) devising methods of implementing RF circuits which allow the performance to be dynamically tuned to match real-time conditions in a power-efficient manner, and 2) refining approaches for thinking about the optimization of RF circuits at the device level. Working toward a 5.8 GHz receiver consistent with 1 GBit/s operation, signal path topologies and adjustable biasing circuits are developed for low-noise amplifiers (LNAs) and voltage-controlled oscillators (VCOs) to provide a facility by which power can be conserved when the demand for sensitivity is low. As an integral component in this effort, tools for exploring device level issues are illustrated with both circuit types, helping to identify physical limitations and design techniques through which they can be mitigated. The design of two LNAs and four VCOs is described, each realized to provide a fully-integrated solution in a 0.5 pm SiGe BiCMOS process, and each incorporating all biasing and impedance matching on chip. Measured results for these 5-6GHz circuits allow a number of poignant technology issues to be enlightened, including an exhibition of the importance of terminal resistances and capacitances, a demonstration of where the transistor fT is relevant and where it is not, and the most direct comparison of bipolar and CMOS solutions offered to date in this frequency range. In addition to covering a number of new circuit techniques, this work concludes with some new views regarding IC technologies for RF applications. Thesis Supervisor: Charles G. Sodini Title: Professor of Electrical Engineering and Computer Science		Donald A. Hitko	2002			electrical engineering technology;electronic engineering;engineering;electrical engineering;computer engineering	EDA	26.935138556564198	60.15207260687603	169513
c7e10c6bce0dbea5d5a8acfa3d648355fbb6b6dc	development of a cmos cell library for rf wireless and telecommunications applications	cmos integrated circuits;integrated circuit;software libraries;telecommunication equipment cmos integrated circuits cellular arrays circuit cad software libraries integrated circuit design mobile communication;cellular arrays;integrated circuit design;baseband circuits cmos cell library rf wireless applications cell library circuit elements high frequency sub system standard mosis processes design files circuit tutorials;mobile communication;circuit cad;high frequency;telecommunication equipment;libraries radio frequency circuit testing measurement standards integrated circuit measurements baseband	There is increasing interest in the use of CMOS circuits for highly integrated high frequency wireless telecommunications systems. This paper presents the results of on-going work into the development of a cell library that includes many of the circuit elements required for the high frequency sub-system of a communications integrated circuit. The cells were fabricated using standard MOSIS processes and measurement results are presented. The full design files, testing results and circuit tutorials describing the cells and how they interface with baseband circuits are available from the author.		Robert H. Caverly	1998		10.1109/GLSV.1998.665249	mixed-signal integrated circuit;physical design;electronic engineering;mobile telephony;telecommunications;computer science;engineering;electrical engineering;telecommunications equipment;integrated circuit;circuit design;diode-or circuit;high frequency;design layout record;application-specific integrated circuit;circuit extraction;cmos;pin compatibility;computer engineering;integrated circuit design	Mobile	28.948303105195933	58.40678975947693	169529
865aece520d4ffabe2b80b967bb95aa28707b29a	a partial order for the synthesized channels of a polar code	codes channel coding;degradation decoding indexes hamming weight transmitters receivers;covering relation partial order synthesized channel polar code binary input channel binary expansion	A partial order for the synthesized channels WN(i) of a polar code is presented that is independent of the underlying binary-input channel W. The partial order is based on the observation that WN(j) is stochastically degraded to WN(i) if j is obtained by swapping a more significant 1 with a less significant 0 in the binary expansion of i. We derive an efficient representation of the partial order, the so-called covering relation. The partial order is then combined with another partial order from the literature that is also independent of W. Finally, we give some remarks on how this combined partial order can be used to simplify code construction of polar codes.	binary number;paging;polar code (coding theory)	Christian M Schürch	2016	2016 IEEE International Symposium on Information Theory (ISIT)	10.1109/ISIT.2016.7541293	telecommunications;theoretical computer science;mathematics;algorithm	Logic	38.093396456254645	58.46020213368564	169549
7ca5019c37f107a3e13e8bb05980e33fef9db37e	asynchronous realization of algebraic integer-based 2d dct using achronix speedster spd60 fpga	speed increase;higher speed;achronix spd60 field;accuracy improvement;achronix speedster;high throughput;nm synchronous xilinx fpga;spd60 fpga;integer dct;novel quantization scheme;asynchronous realization;ai dct;high accuracy	1 Electrical and Computer Engineering, Auburn Science and Engineering Center (ASEC) 265, The University of Akron, Akron, OH 44325-3904, USA 2 Signal Processing Group, Department of Statistics, Federal University of Pernambuco, 50740-540 Recife, PE, Brazil 3 Department of Electrical and Computer Engineering, ICT 402, Schulich School of Engineering, University of Calgary, 2500 University Drive NW Calgary, Alberta, Calgary, AB, Canada T2N 1N4 4Advanced Micro Devices, 1 Commerce Valley Drive East, Markham, ON, Canada L3T 7X6		Nilanka T. Rajapaksha;Amila Edirisuriya;Arjuna Madanayake;Renato J. Cintra;Denis Onen;Ihab Amer;Vassil S. Dimitrov	2013	J. Electrical and Computer Engineering	10.1155/2013/834793	electronic engineering;real-time computing;trellis quantization;computer science;theoretical computer science	DB	32.99884344456662	55.44097933475775	169752
1355d796b5c22a5863c70cf889ea2238e3fdaa37	on the equivalence between distances and t-indistinguishabilities		iru dq| {> |> } # [/ wkhq g lv fdoohg d 80jhqhudol}hg glvwdqfh rq [ +vhh ^8`dqg ^6`,1 Erwk wkh svhxgr0 glvwdqfhv dqg wkh glvwdqfhv ghqhg rq oU. e| phdqv ri wkh rshudwlrq ./ wkh olqhdu rughu dqg h @ 3 duh h{dpsohv ri jhqhudol}hg glvwdqfhv1 Rq wkh rwkhu kdqg/ li 8 @ +V>>>v, lv d frppx0 wdwlyh rughuhg vhpljurxs zlwk d glvwlqjxlvkhg hoh0 phqw v/ d ixqfwlrq L = [ [ V lv fdoohg dq 80 lqglvwlqjxlvkdelolw| rshudwru ri ohyho v iru wkh vhw [ li/ iru dq| {> |> } # [ / lw yhulhv +vhh ^7`dqg	turing completeness	Enric Trillas;Elena Castiñeira;Ana Pradera	1999			equivalence (measure theory);alarm;electrical engineering;radio wave;shift register;computer science	Web+IR	30.706935567399945	46.919131986847006	169849
09da214556613f8b4d7df68e6e1e1d4e39435ff8	dct based ring oscillator physical unclonable functions	ro output decorrelation dct ring oscillator physical unclonable function post processing scheme puf discrete cosine transform;oscillators decorrelation discrete cosine transforms;discrete cosine transforms quantization signal ring oscillators noise inverters error correction probability density function;key generation physical unclonable function ring oscillator discrete cosine transform	A new post-processing scheme is proposed for Physical Unclonable Functions (PUFs) based on Ring Oscillators (ROs). The scheme uses the Discrete Cosine Transform (DCT) to decorrelate the RO outputs and improves on existing RO PUFs in terms of uniqueness and the number of extracted bits.	discrete cosine transform;ring oscillator;video post-processing	Onur Günlü;Onurcan Iscan	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6855199	discrete mathematics;modified discrete cosine transform;theoretical computer science;discrete cosine transform;mathematics	EDA	31.476263843180817	47.06475913277531	170010
d49230832ff9a22ce19dd9f0130eae10c18ed6e8	spocs: application specific signal processor for ofdm communication systems	application specific instruction set processor asip;digital audio broadcast;wireless local area network;communication system;communications;data processing;fft;digital video broadcast;chip;fast fourier transform;input output;bit manipulation;ofdm;assp;application specific instruction set processor;dsp;orthogonal frequency division multiplex	This paper presents an Application-Specific Signal Processor (ASSP) for Orthogonal Frequency Division Multiplexing (OFDM) Communication Systems, called SPOCS. The instruction set and its architecture are specially designed for OFDM systems, such as Fast Fourier Transform (FFT), scrambling/descrambling, puncturing, convolutional encoding, interleaving/deinterleaving, etc. SPOCS employs the optimized Data Processing Unit (DPU) to support the proposed instructions and the FFT Address Generation Unit (FAGU) to automatically calculate input/output data addresses. In addition, the proposed Bit Manipulation Unit (BMU) supports efficient bit manipulation operations. SPOCS has been synthesized using the SEC 0.18 μm standard cell library and has a much smaller area than commercial DSP chips. SPOCS can reduce the number of clock cycles over 8%~53% for FFT and about 48%~84% for scrambling, convolutional encoding and interleaving compared with existing DSP chips. SPOCS can support various OFDM communication standards, such as Wireless Local Area Network (WLAN), Digital Audio Broadcasting (DAB), Digital Video Broadcasting-Terrestrial (DVB-T), etc.	small private online course	Jae Hyun Baek;Sung Dae Kim;Myung Hoon Sunwoo	2008	Signal Processing Systems	10.1007/s11265-008-0240-4	embedded system;fast fourier transform;parallel computing;real-time computing;data processing;telecommunications;computer science	Embedded	30.785708352863146	58.59776801043696	170793
447435def699e36ddbdc51cda441e1e1032cfe98	letter: all digital spread spectrum modems for power line communications	spread spectrum;courant porteur sur ligne electrique;low voltage;power line communication;spread spectrum communication;baja tension;spectre etale;carrier transmission on power lines;basse tension;modems;modem	Through the use of band-spreading techniques electrical power distribution networks become reliable and universal data links. For cost and performance reasons all digital solutions for the. constmction of modems are paramount. The use of general purpose digital signal processors (DSP) is neither feasible nor cost-effective. After a review of appropriate signalling schemes the key components for integration into an ASIC will be outlined. Within the transmitter section of the ASIC a programmable address counter together with a sample memory provides precise and fast signal synthesis. In the receiver section a fast front-end is needed for correlation operations. Furthermore essential steps towards bit decision are performed. A typical ASIC, representing the heart of a power line modem, finally contains the following functional blocks: An 8 bit correlator with a minimum of four separate accumulators, a squarer with at least two additional accumulators forming the decision logic, and a programmable 10 bit address counter. VHDL-based ASIC design was performed and fabrication took place in October 1994 within the EUROCHIP project. An ASIC incorporates 5250 gates with a power consumption below 50 mW. Modems have been built with different signalling schemes for data rates up to 2400 bits/s. Measurements revealed typical bit error rates below lo4.	application-specific integrated circuit;bit error rate;central processing unit;digital signal processor;modem;power-line communication;program counter;transmitter;vhdl	Klaus M. Dostert	1996	European Transactions on Telecommunications	10.1002/ett.4460070605	electronic engineering;telecommunications;computer science;engineering;electrical engineering;spread spectrum	Arch	30.765312563486457	56.707363734416205	170992
f9a4725c67549016c3d3bf6205bbf3a784e36dee	a dsp-based feedback loop for mixed-signal vlsi testing	digital signal processing;software testing;embedded digital signal processing;signal generators;pins;instruments;adc servo loop test design dsp mixed signal vlsi testing embedded digital signal processing feedback loop testing hardware configuration software architecture;adc servo loop test design;very large scale integration;automatic test equipment;feedback loop testing;software architecture;feedback;feedback loop;synchronization;mixed signal vlsi testing;integrated circuit testing;vlsi;mixed analogue digital integrated circuits;system testing;digital signal processing chips;hardware configuration;feedback loop very large scale integration digital signal processing system testing hardware pins instruments software testing signal generators synchronization;dsp;hardware	Implementing a feedback loop system for a mixed-signal VLSI test system by using an embedded digital signal processing (DSP) unit provides superior flexibility in device testing applications. This paper describes such a DSP feedback loop within the context of a mixed-signal VLSI test system, while discussing a number of potential applications and their implications.	digital signal processor;feedback;mixed-signal integrated circuit;very-large-scale integration	Lakshmikantha S. Prabhu;Daniel A. Rosenthal	1997		10.1109/TEST.1997.639679	embedded system;electronic engineering;real-time computing;computer science;engineering;electrical engineering;digital signal processing;software engineering;very-large-scale integration	Robotics	24.734955420104285	52.13628656684095	171465
ea491efef3016c8cb7d40393200ca891ecf6622c	on an upper bound for mixed error-correcting codes	sphere packing;perfect code;error correcting code;error correction codes;code parfait;hamming codes;codigo perfecto;codigo corrector error;upper bounds;distinct alphabet upper bound mixed error correction code sphere packing bound;upper bound;error correction code;teoria codificacion;coding theory;upper bounds mixed error correcting codes perfect mixed codes;perfect mixed codes;upper bound error correction codes convolutional codes information theory protection linear matrix inequalities channel capacity block codes telecommunications milling machines;hamming codes error correction codes;theorie codage;code correcteur erreur;mixed error correcting codes	A mixed code is an error-correcting code in which different entries of the codewords can be chosen from different alphabets. In this correspondence an upper bound is given for the number of codewords in a mixed code where all the entries can come from distinct alphabets. This bound improves the sphere packing bound in several directions. The result is specialized to a simpler form in the case when only two distinct alphabets are used. Numerical results are presented to show that, in various cases, two different forms of the bound and alternative choices of a parameter may give the strongest bound.	alexander horned sphere;code word;error detection and correction;forward error correction;numerical linear algebra;set packing	Stephanie Perkins;Alexander L. Sakhnovich;Derek H. Smith	2006	IEEE Transactions on Information Theory	10.1109/TIT.2005.862107	combinatorics;discrete mathematics;error detection and correction;hamming bound;mathematics;algorithm;statistics	Theory	39.01526257344262	54.351055306167225	172337
2d890989001fee9985b9284d44ea49c69ea81e6d	efficient algorithm for the computation of on-chip capacitance sensitivities with respect to a large set of parameters	layout parameters;layout parasitic extraction;design automation;parasitic capacitance;efficient algorithm;cad;prototypes;design for manufacture;design rule checking;on chip capacitance sensitivities;wires;adjoint method;finite difference;data mining;system on a chip;process design;chip;capacitance extraction;sensitivity;finite difference scheme;vectors;process parameters;sensitivity analysis;beol cross section;design for manufacturability;cross section;capacitance;design automation parasitic capacitance algorithm design and analysis wiring data mining process design monte carlo methods system on a chip prototypes design methodology;wiring;cad methodology;adjoint method sensitivity analysis capacitance extraction;size 65 nm on chip capacitance sensitivities cad methodology design for manufacturability process parameters layout parameters layout parasitic extraction finite difference scheme beol parameters beol cross section;size 65 nm;sparse matrices;algorithm design and analysis;monte carlo methods;design for manufacture cad capacitance;beol parameters;design methodology;conductors	Recent CAD methodologies of Design-for-Manufacturability (DFM) have naturally led to a significant increase in the number of process and layout parameters that have to be taken into account in design-rule checking. Methodological consistency requires that a similar number of parameters be taken into account during layout parasitic extraction. Because of the inherent variability of these parameters, the issue of efficiently extracting deterministic parasitic sensitivities with respect to such a large number of parameters must be addressed. In this paper, we tackle this very issue in the context of capacitance sensitivity extraction. In particular, we show how the adjoint sensitivity method can be efficiently integrated within a finite-difference (FD) scheme to compute the sensitivity of the capacitance with respect to a large set of BEOL parameters. If np is the number of parameters, the speedup of the adjoint method is shown to be a factor of np/2 with respect to direct FD sensitivity techniques. The proposed method has been implemented and verified on a 65nm BEOL cross section having 10 metal layers and a total number of 59 parameters. Because of its speed, the method can be advantageously used to prune out of the CAD flow those BEOL parameters that yield a capacitance sensitivity less than a given threshold.	back end of line;computation;computer-aided design;cross section (geometry);design for manufacturability;emoticon;finite difference method;output-sensitive algorithm;sensitivity and specificity;simulation;solver;spatial variability;speedup	Tarek A. El-Moselhy;Ibrahim M. Elfadel;David Widiger	2008	2008 45th ACM/IEEE Design Automation Conference	10.1145/1391469.1391699	embedded system;electronic engineering;electronic design automation;computer science;engineering;electrical engineering;design for manufacturability;engineering drawing;statistics	EDA	24.660914243947452	57.55611910959964	172664
09436c021e015f703a1bed1c43a3110f533a37cd	vt position code communication technology and its implementation	computers;multi system data;protocols;new technology;communications technology electrons wires computer science circuits computer aided manufacturing hardware pins electromagnetic interference voltage;binary system coding mode;decoding;electronic communication;bottleneck problem;binary codes;wires;quantified time dot;second step voltage quantification method vt position code communication technology electronic communication technology time axis voltage axis quantified time dot multi steps voltage quantification bottleneck problem binary system coding mode single lined system automobile controller;vt position code communication technology;communication connection;communication connection vt position code communication technology multi system data;communications technology;communication technology;second step voltage quantification method;encoding;electronic communication technology;integrated circuits;single lined system automobile controller;voltage axis;multi steps voltage quantification;time axis	A kind of new electronic communication technology is presented in this paper. The technology quantifies the time axis and the voltage axis synchronously, uses the quantified time dot as the address of the communication, and realizes the transmission of the multi-system data via transmitting the multi-steps voltage quantification. This new technology can solve the bottleneck problem of the speed, circuit and electromagnetism in the electronic communication, change the binary system coding mode and communication connection form of the electronic device, debase the complexity of the devices connection, enhance the rate of the processing and the transmission, reduce the transmission quantity of the redundant information, simplify the transformation between the difference protocols. The single-lined system automobile controller based on the second-step voltage quantification method of this technology has been verified in the actual application, and obtained the invocation patent. The experimental results approve the validity and the robustness of the technology.	apache axis;binary-coded decimal;communications protocol;control system;electronic component;logical connective;programmer;robustness (computer science);switch;transmitter	Shiying Zhou;Guihe Qin;Yubo Jin	2008	2008 IEEE Conference on Robotics, Automation and Mechatronics	10.1109/RAMECH.2008.4681351	embedded system;information and communications technology;electronic engineering;real-time computing;computer science;engineering;control theory	Robotics	32.901954447058856	53.462739920306134	172907
7a950d455cbc1ef282e6f995a37ab2f6516dd7de	on-die ring oscillator based measurement scheme for process parameter variations and clock jitter	process parameter variation;high performance microprocessor;phase measurement;circuit noise;clock jitter measurement;clocks;oscillators;high performance microprocessor process parameter variations clock jitter measurement scheme;ring oscillator;size measurement;power cost;clocks jitter delay phase measurement oscillators size measurement area measurement;process parameters;clock jitter measurement on die ring oscillator based measurement process parameter variation low cost scheme parameter variation measurement high performance microprocessor power cost;process parameter variations;low cost scheme;clock jitter;on die ring oscillator based measurement;area measurement;oscillators circuit noise clocks jitter;jitter;parameter variation measurement;high performance;measurement scheme	We present a novel low cost scheme for the on-die measurement of either clock jitter, or process parameter variations. By re-using and properly modifying the Ring Oscillators (ROs) that are currently widely employed for process parameter variation measurement in high performance microprocessors, our proposed scheme can be easily set in either the process parameter variation measurement mode, or the clock jitter measurement mode, by acting on an external control signal. This way, during the test or debug phase, clock jitter can also be measured at negligible area and power costs with respect to process parameter variation measurement only. Our scheme is scalable in the provided clock jitter measurement resolution, while allowing the same process parameter variation measurement resolution as the currently employed RO based schemes. Moreover, due to its allowing both process parameter variation and clock jitter measurements, our scheme features accurate clock jitter measurement despite the possible presence of significant process parameter variations.	microprocessor;overhead (computing);ring oscillator;scalability	Martin Omaña;Daniele Giaffreda;Cecilia Metra;T. M. Mak;Simon Tam;Asifur Rahman	2010	2010 IEEE 25th International Symposium on Defect and Fault Tolerance in VLSI Systems	10.1109/DFT.2010.39	electronic engineering;real-time computing;jitter;clock angle problem;telecommunications;clock domain crossing;clock skew;computer science;ring oscillator;control theory;digital clock manager;oscillation	Embedded	27.785498157038496	55.409044936396555	172916
49d89162855791e800bd041f01db91eb3c7e957e	test and debug features of the rto7 chip	design for testability;digital test system philips rto7 chip receive chain digital demodulation bluetooth like radio communication test hardware debug hardware design for testability design for debug state dump analysis;philips rto7 chip;digital demodulation;receive chain;debug hardware;design for debug;testing digital filters radio frequency demodulation filtering band pass filters design for disassembly radiofrequency amplifiers streaming media communication standards;radiofrequency integrated circuits bluetooth design for testability integrated circuit testing radio links;bluetooth like radio communication;integrated circuit testing;radio communication;test hardware;bluetooth;radiofrequency integrated circuits;state dump analysis;digital test system;radio links	The Philips RTO7 chip consists of a complete receive chain from RF up to and including digital demodulation for Bluetooth-like radio communication. This paper describes both the implementation and verification of the test and debugs hardware for the digital core of the RTO7. The core-based DfT and DfD flow of the RTO7 is presented. The experimental results show that the RTO7 is both a fully testable and debuggable chip. State dump analysis results are also presented, showing that the state dumps obtained in the application are 100% stable, and match the state dumps made in simulation, and on the digital test system		Kees van Kaam;Bart Vermeulen;Henk Jan Bergveld	2005	IEEE International Conference on Test, 2005.	10.1109/TEST.2005.1583985	embedded system;electronic engineering;real-time computing;telecommunications;computer science;engineering;electrical engineering;operating system;design for testing;bluetooth	EDA	29.00240081624028	58.522865009945235	173074
d047f8a923e95f606e3172013637e151e78f2c8c	linear time recognition of optimal l-restricted prefix codes (extended abstract)	extended abstract;linear time recognition;optimal l-restricted prefix codes;prefix code;linear time	  Given an alphabet Σ = {a  1,...,a    n  } and a corresponding list of weights [w  1,...,w    n  ], an optimal prefix code is a prefix code for Σ that minimizes the weighted length of a code string, defined to be å</font >i=1n wi li\sum_{i=1}^{n} w_i l_i, where l    i   is the length of the codeword assigned to a    i  . This problem is equivalent to the following problem: given a list of weights [w  1,...,w    n  ], find an optimal binary code tree, that is, a binary tree T that minimizes the weighted path length  å</font >i=1n wi li\sum_{i=1}^{n} w_i l_i, where l    i   is the level of the i-th leaf of T from left to right. If the list of weights is sorted, this problem can be solved in O(n) by one of the efficient implementations of Huffman’s Algorithm [Huf52]. Any tree constructed by Huffman’s Algorithm is called  a Huffman tree.    	prefix code;time complexity	Ruy Luiz Milidiú;Eduardo Sany Laber	2000		10.1007/10719839_23	block code;prefix code;kraft's inequality;linear code	Theory	38.792096033191186	53.438952904679496	173613
9f9299f6ad88a415a95626229426d64d23ce78e7	a design for a low-power digital matched filter applicable to w-cdma	cmos integrated circuits;matched filters multiaccess communication energy consumption registers spread spectrum communication clocks power generation parallel processing computer simulation cmos technology;digital filters median filters power consumption code division multiple access matched filters cmos integrated circuits digital simulation;chip;wideband code division multiple access;code division multiple access;low power;direct sequence spread spectrum;digital filters;w cdma low power digital matched filter wideband code division multiple access direct sequence spread spectrum communication system correlation calculating unit asynchronous latch clock generation reception registers chip correlation operations computer simulations cmos standard cell array technology;matched filters;power consumption;matched filter;computer simulation;median filters;digital simulation	This paper presents a design for a low-power digital matched filter (DMF) applicable to Wideband-Code Division Multiple Access (W-CDMA), which is a Direct-Sequence Spread-Spectrum (DS-SS) communication system. The proposed architectural approach to reducing the power consumption focuses on the reception registers and the correlation-calculating unit (CCU), which dissipate the majority of the power in a DMF. The main features are asynchronous latch clock generation for the reception registers, parallelism of the correlation calculation operations and bit manipulation for chipcorrelation operations. A DMF is designed in compliance with the W-CDMA specifications incorporating the proposed techniques, and its properties are evaluated by computer simulations at the gate level using 0.18-μm CMOS standard cell array technology. The results of the simulations show a power consumption of 9.3 mW (@15.6MHz, 1.6V), which is only about 30 % of the power consumption of conventional DMFs.	bit manipulation;cmos;computer simulation;low-power broadcasting;matched filter;parallel computing;processor register;standard cell	Shoji Goto;Takashi Yamada;Norihisa Takayarna;Yoshifumi Matsushita;Yasoo Harada;Hiroto Yasuura	2002		10.1109/DSD.2002.1115371	computer simulation;embedded system;real-time computing;telecommunications;computer science;matched filter	EDA	30.186282835271456	56.23399480890666	173747
64f424a5a8b53a4ba47d5437c969d372163ee5a9	fast-accurate non-polynomial metamodeling for nano-cmos pll design optimization	optimisation;neural networks;neural nets;phase locked loops integrated circuit modeling optimization polynomials biological neural networks neurons;physical design;consumer electronics;polynomial;design optimization;polynomials;phase locked loops;nano cmos;semiconductor device models;pll;bee colony algorithm fast accurate nonpolynomial metamodeling nano cmos pll design optimization complex device models space exploration techniques consumer electronics complex nano cmos circuit physical design aware neural networks pll circuit;time to market;electronic engineering computing;design space exploration;semiconductor device models electronic engineering computing neural nets optimisation phase locked loops polynomials;circuit optimization metamodeling neural networks nano cmos pll polynomial modeling;metamodeling;modeling;performance optimization;circuit optimization;neural network;device modeling	At the nanoscale domain, the simulation, design, and optimization time of the circuits have increased significantly due to high-integration density, increasing technology constraints, and complex device models. This necessitates fast design space exploration techniques to meet the shorter time to market driven by consumer electronics. This paper presents non-polynomial metamodels (surrogate models) using neural networks to reduce the design optimization time of complex nano-CMOS circuit with no sacrifice on accuracy. The physical design aware neural networks are trained and used as metamodels to predict frequency, locking time, and power of a PLL circuit. Different architectures for neural networks are compared with traditional polynomial functions that have been generated for the same circuit characteristics. Thorough experimental results show that only 100 sample points are sufficient for neural networks to predict the output of circuits with 21 design parameters within 3% accuracy, which improves the accuracy by 56% over polynomial metamodels. The generated metamodels are used to perform optimization of the PLL using a bee colony algorithm. It is observed that the non-polynomial (using neural networks) metamodels achieve more accurate results than polynomial metamodels in shorter optimization time.	algorithm;artificial neural network;cmos;design space exploration;gnu nano;list of wireless mice with nano receivers;lock (computer science);mathematical optimization;metamodeling;phase-locked loop;physical design (electronics);polynomial;simulation	Oleg Garitselov;Saraju P. Mohanty;Elias Kougianos	2012	2012 25th International Conference on VLSI Design	10.1109/VLSID.2012.90	control engineering;embedded system;electronic engineering;real-time computing;phase-locked loop;computer science;engineering;artificial neural network;polynomial	EDA	24.609252583180957	59.06229950665819	174439
4fdec4ebacd44440c5bbe50ef4bd60c59edeeba7	iterative built-in testing and tuning of mixed-signal/rf systems	cmos integrated circuits;baseband;process variation;performance evaluation;radiofrequency integrated circuits built in self test cmos integrated circuits integrated circuit design integrated circuit testing mixed analogue digital integrated circuits;integrated circuit design;built in self test;radio frequency;tuning;system testing radio frequency circuit testing automatic testing circuit optimization circuits and systems cmos technology cmos process high speed electronics electronic equipment testing;transmitters;ofdm;integrated circuit testing;built in test;mixed analogue digital integrated circuits;yield loss;radiofrequency integrated circuits;high speed;truly self healing systems iterative built in testing high speed mixed signal rf circuits scaled cmos technologies post manufacture tuning high speed electronic circuits iterative test and tune procedures	Design and test of high-speed mixed-signal/RF circuits and systems is undergoing a transformation due to the effects of process variations stemming from the use of scaled CMOS technologies that result in significant yield loss. To this effect, postmanufacture tuning for yield recovery is now a necessity for many high-speed electronic circuits and systems and is typically driven by iterative test-and-tune procedures. Such procedures create new challenges for manufacturing test and built-in self-test of advanced mixed-signal/RF systems. In this paper, key test challenges are discussed and promising solutions are presented in the hope that it will be possible to design, manufacture and test “truly self-healing” systems in the near future.	built-in self-test;cmos;electronic circuit;iteration;mixed-signal integrated circuit;performance tuning;radio frequency;stemming	Abhijit Chatterjee;Donghoon Han;Vishwanath Natarajan;Shyam Kumar Devarakond;Shreyas Sen;Hyun Woo Choi;Rajarajan Senguttuvan;Soumendu Bhattacharya;Abhilash Goyal;Deuk Lee;Madhavan Swaminathan	2009	2009 IEEE International Conference on Computer Design	10.1109/ICCD.2009.5413136	transmitter;electronic engineering;orthogonal frequency-division multiplexing;telecommunications;computer science;engineering;electrical engineering;baseband;process variation;cmos;radio frequency;computer engineering;integrated circuit design	EDA	25.867088732104758	52.287250553276714	175535
a065392abf95d2c311ad5f8560320b856871d12c	architecture of run-time reconfigurable channel decoder	channel coding;convolutional codes;communication system;runtime decoding viterbi algorithm energy consumption wireless communication standards development reconfigurable architectures multiaccess communication digital video broadcasting gsm;decoding;software defined radio;reconfigurable architectures;multifunctional communication devices;turbo codes;wireline communication devices;multimode communication devices;software radio;software defined radio run time reconfigurable channel decoder wireline communication devices wireless communication devices multimode communication devices multifunctional communication devices reconfigurable architecture viterbi decoding turbo decoding;viterbi decoding channel coding reconfigurable architectures software radio turbo codes;satellite broadcasting;wireless communication;computer architecture;reconfigurable architecture;run time reconfigurable;registers;viterbi algorithm;multiprocessor architecture;viterbi decoder;power consumption;turbo decoding;viterbi decoding;wireless communication devices;run time reconfigurable channel decoder	Modern wireline and wireless communication devices are multimode and multifunctional communication devices. In order to support multiple standards on a single platform, it is necessary to develop a reconfigurable architecture that can provide the required flexibility and performance. The Channel decoder is one of the most compute intensive and essential elements of any communication system. Most of the standards require a reconfigurable Channel decoder that is capable of performing Viterbi decoding and Turbo decoding. Furthermore, the Channel decoder needs to support different configurations of Viterbi and Turbo decoders. In this paper, we propose a reconfigurable Channel decoder that can be reconfigured for standards such as WCDMA, CDMA2000, IEEE802.11, DAB, DVB and GSM. Different parameters like code rate, constraint length, polynomials and truncation length can be configured to map any of the above mentioned standards. A multiprocessor approach has been followed to provide higher throughput and scalable power consumption in various configurations of the reconfigurable Viterbi decoder and Turbo decoder. We have proposed A Hybrid register exchange approach for multiprocessor architecture to minimize power consumption.	code rate;convolutional code;digital video broadcasting;multi-function printer;multiprocessing;polynomial;scalability;throughput;truncation;viterbi decoder	Ritesh Rajore;S. K. Nandy;H. S. Jamadagni	2009	2009 IEEE International Conference on Communications	10.1109/ICC.2009.5198763	embedded system;real-time computing;soft-decision decoder;telecommunications;computer science;software-defined radio;viterbi decoder;statistics	Robotics	31.488001669152027	59.322588323405185	175711
3ca9e151c3bd816a051ab8d688033a6ec33cd6a9	on the interior points of the storage-repair bandwidth tradeoff of regenerating codes	unit data symbol interior points storage repair bandwidth regenerating codes minimum storage regeneration msr endpoints minimum bandwidth regeneration mbr endpoints exact repair code construction storage overhead average repair bandwidth information theory inequality prover;lead;file organisation	It is well known that it is possible to construct regenerating codes with exact repair that operate at the minimum storage regeneration (MSR) and minimum bandwidth regenerating (MBR) endpoints of the storage-repair bandwidth tradeoff. It has also been known for some time, that it is not possible to construct exact-repair codes that operate exactly at an interior point, except possibly, in a small region adjacent to the MSR point. There have been three recent results relating to code constructions for the interior points. In the first, a normalized version of the classical storage-repair bandwidth tradeoff is introduced. This shows that when measured against practical metrics such as storage overhead and average repair bandwidth per unit time and unit data symbol stored, interior-point constructions are of equal importance as constructions for MSR and MBR points. A second major result uses an information-theory inequality prover to establish that it is not possible in general, to construct codes that achieve the interior points even asymptotically. The third result presents a construction for regenerating codes that does better than space-sharing, compares well with codes for MSR and MBR points when viewed in the normalized tradeoff framework, and which is capable of achieving a single interior point. An overview of these results is presented here.	code;information theory;overhead (computing);social inequality	Birenjith Sasidharan;P. Vijay Kumar	2013	2013 51st Annual Allerton Conference on Communication, Control, and Computing (Allerton)	10.1109/Allerton.2013.6736605	mathematical optimization;theoretical computer science;mathematics;engineering drawing	Theory	35.79086879354932	58.08812736626496	175748
2f479a93e8ada043a8dd93f6e6767b925638c6df	distributed storage over unidirectional ring networks	storage management;euclidean division unidirectional ring networks topological structures storage theory data storage problems representative networks reconstructing bandwidth optimal reconstructing distributed storage scheme ordss repair problem failed storage node repair bandwidth;peer to peer computing	Data storage over networks with topological structures is one of issues discussed in storage theory. In this paper, we study data storage problems over unidirectional ring networks, one type of simple and representative networks. A lower bound on the reconstructing bandwidth (total communicated bandwidth in the network) for each user to download entire original data is proposed, and it is achievable for arbitrary parameters that are number of nodes, n, storage capacity per node, α, original data size, M. If a distributed storage scheme can achieve this lower bound with equality for every user, we call it an optimal reconstructing distributed storage scheme (ORDSS). Furthermore, the repair problem for a failed storage node in ORDSSes is under consideration and a tight lower bound on the repair bandwidth for each failed storage node is obtained. Particularly, we indicate the fact that for any ORDSS, every storage node can be repaired with repair bandwidth achieving the lower bound with equality. In addition, by using the concept of Euclidean division, we present an efficient approach to construct ORDSSes over the smallest finite field F2 for arbitrary parameters (n, α, M). Finally, an example is proposed to characterize the above approach.	clustered file system;computer data storage;download;scheme	Jiyong Lu;Xuan Guang;Fang-Wei Fu	2014	2014 International Symposium on Information Theory and its Applications		telecommunications;computer science;theoretical computer science;distributed computing;algorithm	Theory	35.80639633809095	58.37225220988747	175879
48c0f738945cfddb3704f01ab573d56c3600cdc2	rank bounds for design matrices with applications toc ombinatorial geometry and locally correctable codes	matrix scaling;matrix rigidity;high dimensionality;sylvester gallai;computational geometry;satisfiability;finite field;error correction code;computational complexity;combinatorial geometry	A (q,k,t)-design matrix is an m x n matrix whose pattern of zeros/non-zeros satisfies the following design-like condition: each row has at most q non-zeros, each column has at least k non-zeros and the supports of every two columns intersect in at most t rows. We prove that for m ≥ n, the rank of any (q,k,t)-design matrix over a field of characteristic zero (or sufficiently large finite characteristic) is at least n - (qtn/2k)2. Using this result we derive the following applications: Impossibility results for 2-query LCCs over large fields: A 2-query locally correctable code (LCC) is an error correcting code in which every codeword coordinate can be recovered, probabilistically, by reading at most two other code positions. Such codes have numerous applications and constructions (with exponential encoding length) are known over finite fields of small characteristic. We show that infinite families of such linear 2-query LCCs do not exist over fields of characteristic zero or large characteristic regardless of the encoding length. Generalization of known results in combinatorial geometry: We prove a quantitative analog of the Sylvester-Gallai theorem: Let v1,...,vm be a set of points in Cd such that for every i ∈ [m] there exists at least δ m values of j ∈ [m] such that the line through vi,vj contains a third point in the set. We show that the dimension of v1,...,vm is at most O(1/δ2). Our results generalize to the high-dimensional case (replaceing lines with planes, etc.) and to the case where the points are colored (as in the Motzkin-Rabin Theorem).	code word;column (database);forward error correction;run-length encoding;sylvester–gallai theorem;time complexity	Boaz Barak;Zeev Dvir;Amir Yehudayoff;Avi Wigderson	2010		10.1145/1993636.1993705	combinatorics;discrete mathematics;error detection and correction;computational geometry;pure mathematics;mathematics;geometry;computational complexity theory;finite field;algebra;satisfiability	Theory	38.91794997303227	52.09445954281346	176110
cda540b1f81576febecb012ba1a63677f5af72ad	a unified code	arithmetic coding;data compression;error detecting code;theorie communication;codigo detector error;securite donnee;teoria comunicacion;arithmetic code;statistical properties;codigo aritmetico;communication theory;compression ratio;error probability;code arithmetique;compresion dato;error detection;algoritmo optimo;algorithme optimal;optimal algorithm;security of data;compression donnee;code detecteur erreur	We have proposed a novel scheme based on arithmetic coding, an optimal data compression algorithm in the sense of shortest length coding. Our scheme can provide encryption, data compression, and error detection, all together in a one-pass operation. The key size used is 248 bits. The scheme can resist existing attacks on arithmetic coding encryption algorithms. A general approach to attacking this scheme on data secrecy is difficult. The statistical properties of the scheme are very good and the scheme is easily manageable in software. The compression ratio for this scheme is only 2 % worse than the original arithmetic coding algorithm. As to error detection capabilities, the scheme can detect almost all patterns of errors inserted from the channel, regardless of the error probabilities, and at the same time it can provide both encryption and data compression.	algorithm;arithmetic coding;crypt (unix);data compression;data integrity;encryption;error detection and correction;information security;key size;list of statistical packages;plaintext;run-length encoding;springer (tank);subset sum problem;world news connection	Xian Liu;Patrick G. Farrell;Colin Boyd	1999		10.1007/3-540-46665-7_9	data compression;arithmetic coding;discrete mathematics;error detection and correction;shannon–fano coding;theoretical computer science;probability of error;compression ratio;mathematics;context-adaptive binary arithmetic coding;computer security;algorithm;statistics;communication theory	Crypto	35.22527953278247	55.86936346486806	176286
0e62f3a72dc9e94af289a8a754d1619c32a68d34	designing fast fourier transform accelerators for orthogonal frequency-division multiplexing systems	fft;crema;ofdm;mimo;avatar	Designing accelerators for the real-time computation of Fast Fourier Transform (FFT) algorithms for state-of-the-art Orthogonal Frequency-Division Multiplexing (OFDM) demodulators has always been challenging. We have scaled-up a template-based CoarseGrain Reconfigurable Array device for faster FFT processing that generates special purpose accelerators based on the user input. Using a basic and a scaledup version, we have generated a radix-4 and mixedradix (2, 4) FFT accelerator to process different length and types of algorithms. Our implementation results show that these accelerators satisfy not only the execution time requirements of FFT processing for Single Input Single Output (SISO) wireless standards that are IEEE-802.11 a/g and 3GPP-LTE but also for Multiple Input Multiple Output (MIMO) IEEE-802.11n standard.	algorithm;avatar (computing);compaq lte;computation;fast fourier transform;mimo;multiplexing;real-time clock;requirement;run time (program lifecycle phase);simulation interoperability standards organization	Waqar Hussain;Fabio Garzia;Tapani Ahonen;Jari Nurmi	2012	Signal Processing Systems	10.1007/s11265-011-0642-6	fast fourier transform;real-time computing;orthogonal frequency-division multiplexing;computer science;theoretical computer science;prime-factor fft algorithm;statistics;mimo	HPC	30.81544189756267	58.821923005269554	176726
ad11656a0b6141f1af3d6bbaaea1333868db9665	on a multiplexing scheme for threshold logical elements	threshold logic	In this paper, a multiplexing scheme for increasing the reliability of logical networks consisting of threshold elements is discussed. This scheme is a slight modification of yon Neumann ' s (1956) multiplexing scheme. I t takes advantage of the fact tha t a threshold element can be used for the combined functions of a logic gate ( tha t performs digital logic) and of a restoring organ ( tha t nullifies the effect of errors). The general model of yon Neumann ' s multiplexing scheme is shown in Fig. 1, where x~ 1, x~ 2, . x~ k is a group of k independently generated versions of the signal x~, for i -1, 2 . . • n. The digital information carried by these k signals in each group is intended to be the same, although some m a y be in error. A group of two or more versions of the signal x~ are called the redundant signals of x~. So tha t a correct output from the logic gate will be generated when there are erroneous input signals, the restoring organs are inserted in front of the logic gate to assure tha t the inputs to the logic gate are correct. Figure 2(a) shows a simple example of such an error-correcting scheme where threshold elements are used both as the restoring organ and the logic gate. The input signals xl and xs are triplicated as xl I, xl 2, xl z and x ~ ~ s respectively. The threshold elements I and I I are 27 x2~ x2~ major i ty vote takers tha t will restore the correct signals x~ and x~ when there are single errors among the triplicated groups. Figure 2(b) shows a	boolean algebra;digital data;error detection and correction;logic gate;multiplexing	Chung Laung Liu;Jane W.-S. Liu	1965	Information and Control	10.1016/S0019-9958(65)90214-7	computer science;theoretical computer science;algorithm	Crypto	33.12132141837703	53.89514213086483	176906
45c55c7f6c7138dd7015db5614ec80c1f6c05807	analysis of jitter accumulation in interleaved phase frequency detectors for high-accuracy on-chip jitter measurements	cmos integrated circuits;bist;design for testability;jitter cmos integrated circuits;on chip measurement;phase frequency detector;chip;power supply noise;pll;cmos technology jitter accumulation interleaved phase frequency detectors high accuracy on chip jitter measurements spice simulation;jitter;bist jitter on chip measurement design for testability pll	This work presents the analysis of jitter accumulation in interleaved phase frequency detectors for high-accuracy on-chip jitter measurements. Jitter accumulation in phase frequency detector degrades the accuracy of on-chip jitter measurements, and required to be mitigated. In order to estimate the jitter accumulation in phase frequency detectors, SPICE simulation was performed with 65 nm CMOS technology. Simulation results show that, with a 50 mV power supply noise injection, jitter accumulation can be reduced from 1.03 ps to 0.49 ps (52% reduction) by using an interleaved architecture.	cmos;in-phase and quadrature components;phase frequency detector;power supply;spice;sensor;simulation;tree accumulation	Masato Sakurai;Kiichi Niitsu;Naohiro Harigai;Daiki Hirabayashi;Daiki Oki;Takahiro J. Yamaguchi;Haruo Kobayashi	2011	2011 International SoC Design Conference	10.1109/ISOCC.2011.6138668	embedded system;electronic engineering;real-time computing;jitter	EDA	25.436840453687644	54.07054493106074	177121
f416edd853e21f07bd23ce94908e3ed7ff216af9	vlsi implementation of very-high-order fir filters	quadrature phase shift keying;very large scale integration finite impulse response filter cmos technology computer networks cellular networks telephony circuits application software digital arithmetic hardware;residue number systems;computer network;chip;delay circuits fir filters cmos digital integrated circuits vlsi residue number systems quadrature phase shift keying;low power;cmos digital integrated circuits;fir filter;delay circuits;0 8 mum very high order fir filters vlsi implementation modulation schemes wireless computer networks cellular telephones low power cmos technology residue number system arithmetic 20 bit equivalent integer arithmetic tap implementation fqpsk kf modulation scheme linear phase filter;vlsi;residue number system;fir filters	Very-high-order FIR filters required for the new modulation schemes associated with wireless computer networks and cellular telephones can be implemented in VLSI circuitry using low-power CMOS technology and a novel application of Residue Number System (RNS) arithmetic. Through this approach 20-bit equivalent integer arithmetic can be obtained for filters with 8 to 256 taps with only a modest increase in hardware for filters above 8 taps. Simulations indicate that this new technique can increase dramatically the number of taps implemented on a single VLSI chip when compared with an FIR filter generated using FIRGEN.	finite impulse response;very-large-scale integration	Michael A. Soderstrand;Kamal Al-Marayati	1995		10.1109/ISCAS.1995.521403	embedded system;electronic engineering;real-time computing;telecommunications;computer science;finite impulse response;mathematics	Logic	32.03831434170947	56.74166899061742	177449
c677326c1fc5ba3aae0e1edde4ae7f1a8d276f01	implementation of a multirate resampler for multi-carrier systems on gpus	carrier aggregation;gpu-based radio;multirate signal processing;polyphase decimator;polyphase interpolator;polyphase resampler	Efficient sample rate conversion is of widespread importance in modern communication and signal processing systems. Although many efficient kinds of polyphase filterbank structures exist for this purpose, they are mainly geared toward serial, custom, dedicated hardware implementation for a single task. There is, therefore, a need for more flexible sample rate conversion systems that are resource-efficient, and provide high performance. To address these challenges, we present in this paper an all-software-based, fully parallel, multirate resampling method based on graphics processing units (GPUs). The proposed approach is well-suited for wireless communication systems that have simultaneous requirements on high throughput and low latency. Utilizing the multidimensional architecture of GPUs, our design allows efficient parallel processing across multiple channels and frequency bands at baseband. The resulting architecture provides flexible sample rate conversion that is designed to address modern communication requirements, including real-time processing of multiple carriers simultaneously.	graphics processing unit	Scott C. Kim;Shuvra S. Bhattacharyya	2017	Signal Processing Systems	10.1007/s11265-017-1239-5	latency (engineering);polyphase system;real-time computing;parallel computing;throughput;signal processing;wireless;filter bank;computer science;baseband;sample rate conversion	HPC	31.03271197456848	59.08969145336138	177742
8d8998556cf47d7ce9000fafee160822b894e249	multilevel logic transmission using disparate intensity levels of white light and programmable controllers	look up table;communication system;programmable controllers;optical transmitters;photocell;optical fibres;programmable receiver controller;light emitting diodes;resistance;light transmission;programmable controllers data communication light emitting diodes light transmission multivalued logic optical fibres photoelectric devices;data mining;data communication;receivers;assembly;white light;optical fibers;programmable transmitter controller;photoelectric devices;programmable control optical transmitters information analysis logic light emitting diodes information retrieval optical noise optical receivers communication system control costs;led;short length communication systems;cost effectiveness;short length communication systems multilevel logic transmission white light programmable transmitter controller photocell led programmable receiver controller look up table optical fibers copper wires;multilevel logic transmission;optical fiber;multivalued logic;copper;copper wires	This paper analyzes the transmission of a multilevel logic information by assigning a distinct level of Intensity of white light to a specific Byte. The criterion for allocation of Intensities is fed into the programmable transmitter controller. The receiver employs a Photocell to detect the transmitted intensity of white light emitted by an LED. The information at the receiving end is retrieved and processed by the programmable receiver controller. The technique discussed takes into consideration the channel's behavior and the involvement of noise excursions, thus settling for a unique Look up Table at the receiving end. The programmability of the transmitter and receiver controllers allows adjustment according to the variation in channel's behavior. The paper uses a normal dust free channel opaque in nature, thus providing a cost effective alternative to Optical Fibers and a loss effective alternative to Copper Wires for short length communication systems.	byte;transmitter	Rohit Sharma;Anupam Chahar;Nitish Paliwal;Appoorv Narula;Vigya Jindal	2009	2009 International Conference on Ultra Modern Telecommunications & Workshops	10.1109/ICUMT.2009.5345653	telecommunications;computer science;optical fiber	EDA	32.91844147376072	53.51083589779386	177773
9e10bfe4fc60c5cfcc596fa2bf672ddaa92b1f66	panel: analog characterization and test: the long road to realization	circuit noise;complexity theory;very large scale integration;automatic testing;temperature sensors;industries;frequency measurement;temperature dependence;analog circuits;signal processing;voltage;circuit testing;circuit testing analog circuits circuit noise automatic testing very large scale integration frequency measurement signal processing voltage temperature dependence temperature sensors;temperature measurement;voltage measurement;noise	Even though analog circuits have fewer components relative to their digital counterparts, test and characterization of analog circuits is comparatively difficult. The complexity stems from the facts that analog measurements are continuous rather than discrete; acceptable signal tolerances depend on process variations; circuit operation conditions are sensitive to voltage, temperature and frequency; and that there is no widely accepted analog fault model. Analog circuits also have higher susceptibility to noise making measurements and their interpretation difficult. It is common to develop “specification based” test and characterization plans for the analog parts of a design. However, that requires thorough knowledge of the circuitry. Further, it is difficult to decompose an analog circuit into sub-components and test each component, in a manner that is done in digital circuits using scan. To add to the complexity, test and characterization circuitry is harder to implement for analog components due to a form of “Heisenberg principle” where the test and characterization circuitry interferes with the observation. Even basic analog test has necessitated significant modifications to the IEEE 1149 standard. Finally, in many situations ATE equipments may not be able to provide the conditions required to properly measure analog parameters as defined by industry specifications.	analogue electronics;digital electronics;electronic circuit;fault model;formal specification;integrated circuit;jtag;race condition;uncertainty principle	Arani Sinha;Amitava Majumdar;Vasu Ganti	2009	2009 27th IEEE VLSI Test Symposium	10.1109/VTS.2009.65	control engineering;electronic engineering;voltage;analogue electronics;temperature measurement;computer science;engineering;noise;electrical engineering;signal processing;very-large-scale integration	EDA	25.125647781924716	52.956860998272354	178929
4a335d95d23fc4253973d9f53bbac21d7e2ad91a	using bit recycling to reduce the redundancy in plurally parsable dictionaries	decoding;redundancy;dictionaries;encoding;recycling;dictionaries recycling encoding redundancy decoding conferences;savari coding bit recycling plurally parsable dictionaries redundancy reduction optimal dictionary variable to fixed code uniquely parsable dictionary tunstall code random binary sources;conferences;dictionaries codes	Tunstall proposed an efficient algorithm for constructing the optimal dictionary of any particular size to obtain a variable-to-fixed code. More accurately, the algorithm constructs the optimal uniquely parsable dictionary. In fact, Savari showed that, if one allows herself to consider plurally parsable dictionaries, better codes may be constructed. Savari found a class of plurally parsable dictionaries that outperform the Tunstall code for memoryless, highly skewed, binary sources. This work addresses the redundancy in plurally parsable dictionaries and proposes the use of bit recycling as the means to reduce this redundancy, extending the range of random binary sources that may benefit from a plurally parsable dictionary at the same time. We present a theoretical analysis that evaluates the performance of variable-to-fixed codes based on the Tunstall dictionary and ones based on plurally parsable dictionaries, using Savari's coding on the one hand and coding with bit recycling on the other hand.	algorithm;code;dictionary;parsing;tunstall coding	Ahmad Al-Rababa'a;Danny Dubé	2015	2015 IEEE 14th Canadian Workshop on Information Theory (CWIT)	10.1109/CWIT.2015.7255153	speech recognition;computer science;theoretical computer science;algorithm	Theory	36.99682307434728	60.08127940799286	179923
d8db57d679700fbf5c1c4672be16e1ad3b4200d7	high level accuracy loss estimates for a class of analog/digital systems	digital systems degradation paper technology physics computing fluctuations production performance loss circuits embedded system digital filters;perturbation techniques mixed analogue digital integrated circuits performance index integrated circuit modelling embedded systems integrated circuit reliability;performance index;perturbation techniques;digital fft module high level accuracy loss estimates analog digital systems performance degradation mixed analog digital technology finite precision representations physical parameter fluctuations physical parameter production process implementation related perturbations performance loss index embedded system analog filter adc;embedded system;production process;embedded systems;integrated circuit modelling;digital systems;indexation;mixed analogue digital integrated circuits;integrated circuit reliability	The paper perfects and adapts Alippi's methodology (2002) to estimate the performance degradation of an application being implemented in a mixed analog/digital technology. Finite precision representations, deviations from the reference computation and fluctuations of physical parameters due to the production process for passive components are suitably abstracted by implementation-related perturbations and associated to the performance loss index. Such an index can be used to validate a candidate solution and identify the most sensitive modules composing the circuit/system. The effectiveness of the high level approach is shown on a nontrivial embedded system comprising an analog filter, an ADC and a digital FFT module.	analog signal;digital electronics	Cesare Alippi;Marco Stellini	2003		10.1109/ISCAS.2003.1206153	mixed-signal integrated circuit;control engineering;embedded system;electronic engineering;process performance index;real-time computing;successive approximation adc;computer science;engineering;scheduling	Vision	26.713167555535343	57.35682337132901	179953
e5eded02e9e163e43200038bb6018ab66f86c9b4	a model for energy quantization of single-electron transistor below 10nm	analytical models;single electron transistors coulomb blockade semiconductor device models;orthodox theory energy quantization discrete energy levels single electron transistor set;asic design energy quantization single electron transistor set device post coms vlsi coulomb island discrete energy level;energy quantization;single electron transistors;qualitative analysis;discrete energy levels;energy levels;logic gates;single electron transistor;semiconductor device models;logic gates analytical models;single electron transistor set;logic gate;low power consumption;orthodox theory;analytical model;coulomb blockade	Single-electronic transistor (SET) are considered as the attractive candidates for post-COMS VLSI due to their ultra-small size and low power consumption. Along with the size of coulomb island become smaller and smaller, the energy quantization of single electron transistor based on charge state come forth and from obviously to more obviously. A qualitative analysis to single-electron transistors base on charge state with discrete energy levels, is introduced in this paper. Compared with other analysis to single-electron transistor based on charge state without discrete energy levels, our result is close to fact. Through the comparison, it can be get that the former is accurate and close to fact compared with the simulator without discrete energy levels, and is very useful for the ASIC design of SET devices.	application-specific integrated circuit;electron;energy level;one-electron universe;post-quantum cryptography;quantization (physics);simulation;transistor;transistor–transistor logic;very-large-scale integration	Xiaobao Chen;Zuocheng Xing;Bingcai Sui	2011	2011 9th IEEE International Conference on ASIC	10.1109/ASICON.2011.6157239	electronic engineering;electrical engineering;physics;quantum mechanics	EDA	27.484128542034597	56.48389002936198	180915
687a9a25238b536c0ee9a341576175fa274ad940	a parallel multipole accelerated 3-d capacitance simulator based on an improved model	simulation ordinateur;modelizacion;algoritmo paralelo;concepcion asistida;computer aided design;concepcion circuito;capacidad parasita;parallel algorithm;integrated circuit;circuit design;circuito integrado;algorithme parallele;modelisation;integrated circuit design;capacite parasite;analyse performance;performance analysis;vlsi;conception assistee;spurious capacity;conception circuit;capacitance;circuit cad;simulacion computadora;circuit analysis computing;modeling;computer simulation;circuit integre;digital simulation;analisis eficacia;parallel algorithms	This paper gives an improved single-layer potential formula for extracting the parasitic capacitance of multiple conductors, embedded in the infinite or finite dielectrics, based on the MultiPole Accelerated (MPA) method. In fact, many capacitors should be considered to be bounded by a finite region. This indicates that the improvement is necessary to raise extraction accuracy. The nonuniform cube subdivision is used in the simulator. The reason for this is that it is an important base of further implementing the real adaptive calculation, for example h-version of the MPA. Also, it can save a lot of memory for ignoring most empty cubes. Only two lists of the cubes are used, thus simplifying the classification of five cube types. The simplified cell classification scheme eases book keeping in the parallel implementation. In developing a parallel MPA algorithm on the transputer network, a parallel machine of the MIMD type, we pay great attention to balancing workload and reducing communication for general nonuniform distribution of the particles. The results show that the total balance can be well achieved by balancing workload in every level of cubes. The pipeline communication mode shows higher efficiency if the processor number P used can well match problem size.	algorithm;analysis of algorithms;comparison and contrast of classification schemes in linguistics and metadata;embedded system;empty string;mimd;olap cube;parallel computing;simulation;subdivision surface;transputer	Zeyi Wang;Yanhong Yuan;Qiming Wu	1996	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/43.552078	computer simulation;embedded system;electronic engineering;simulation;computer science;engineering;electrical engineering;operating system;computer aided design;parallel algorithm;algorithm	EDA	29.118219256083876	49.708687283806775	180937
04a63d3b7831d826b13eb0dd62043187c643b2c7	power distribution and management	power distribution energy management frequency circuit noise semiconductor device measurement voltage monitoring sampling methods design optimization damping;damping;circuit noise;semiconductor device measurement;design optimization;power distribution;monitoring;voltage;sampling methods;frequency;energy management	There is an extreme range of power constraints in today’s electronic applications, ranging from highperformance servers to low-power personal-health-care products. Traditionally, chips in these domains have been designed with a fixed power-supply-voltage and frequency. With the explosion in variability in today’s nanoscale circuits, fixed operating points with large process variations require wide design margins, incurring a cost in power consumption or performance. Squeezing the most out of these designs involves optimizing the power distribution and management methods. To do this, two independent technologies are needed: the ability to measure accurately the chip behavior and the ability to use this information to control system parameters such as frequency and voltage.	computer performance;control system;low-power broadcasting;power supply;spatial variability	Alice Wang;Jos Huisken	2007	2007 IEEE International Solid-State Circuits Conference. Digest of Technical Papers	10.1109/ISSCC.2007.373405	damping;control engineering;sampling;electronic engineering;voltage;multidisciplinary design optimization;engineering;electrical engineering;frequency;control theory;physics;quantum mechanics;energy management	EDA	25.505531322109885	59.38084723484469	181174
e3689c1d4098483d96b15ba045089ce8b7f718d5	a fault-tolerant modulus replication complex fir filter	fault tolerance finite impulse response filter computer architecture wireless lan adaptive equalizers base stations radio frequency laboratories clocks adaptive filters;modulus replication residue number system fault tolerant modulus replication complex fir filter fault tolerant computation high throughput multirate equalizer wireless lan;fault tolerant;residue number systems;fault tolerant computing fir filters residue number systems wireless lan;fault tolerant computing;error correction;fir filter;residue number system;wireless lan;fir filters;replication complex;high throughput	In this paper we propose an architecture for the implementation of fault-tolerant computation for a high throughput multirate equalizer used in a 1 Gbps asymmetrical wireless LAN. Exploiting the algebraic structure of the modulus replication residue number system (MRRNS) minimizes the area overhead, and the area cost to correct a fault in a single computational channel is 82.7%. Generalized results for single error correction showing significant area savings are also presented.	computation;data rate units;equalization (communications);error detection and correction;fault tolerance;finite impulse response;first-order predicate;gigabit;modulus of continuity;modulus robot;overhead (computing);polynomial;residue number system;throughput	Ian Steiner;P. Chan;Laurent Imbert;Graham A. Jullien;Vassil S. Dimitrov;Grant McGibney	2005	2005 IEEE International Conference on Application-Specific Systems, Architecture Processors (ASAP'05)	10.1109/ASAP.2005.6	embedded system;real-time computing;finite impulse response	Robotics	32.09589773580172	56.69943734118495	182166
b7e4acb72ec66877d45322b10f7835f0a31e5906	a low-cost diagnosis methodology for pipelined a/d converters	a d converter;signal generators;process variation;low cost diagnosis methodology;circuit noise;wideband;mixed signal functional blocks;low area bist circuits;clocks;voltage circuit testing system on a chip signal generators clocks wideband time division multiplexing voltage controlled oscillators signal processing circuit noise;analog digital conversion;low area bist circuits low cost diagnosis methodology pipelined a d converters time division multiplexing scan based testing vco based measurement mixed signal functional blocks sample and hold amplifier digital to analog sub converters noise induced errors;time division multiplex;digital to analog sub converters;low complexity;self testing;function block;system on a chip;boundary scan testing;sample and hold amplifier;sample and hold circuits;circuit simulation;built in self test;vco based measurement;signal processing;analogue digital conversion;voltage;mixed analogue digital integrated circuits;noise induced errors;voltage controlled oscillators;circuit testing;time division multiplexing;digital analogue conversion;scan based testing;digital analog conversion;circuit analysis computing;high speed;pipelined a d converters;circuit analysis computing analogue digital conversion digital analogue conversion mixed analogue digital integrated circuits built in self test fault diagnosis time division multiplexing boundary scan testing voltage controlled oscillators sample and hold circuits;fault diagnosis;mixed analog digital integrated circuits	Pipelined A/D converters have intrinsic high-speed characteristics and are widely used in wideband communication and video systems. In this paper, we propose a low-cost diagnosis methodology for pipelined A/D converters which employs three techniques in the diagnosis process: (1) time-division-multiplexing (TDM), (2) scan based testing, and (3) VCO based measurement. The last technique is developed to diagnose the most critical mixed-signal functional blocks in the pipelined ADC including the sample-and-hold amplifier (SHA) and the digital-to-analog sub-converters (DASC). It provides a great capability to distinct signals with very small voltage difference and is insensitive to process variations and immune to noise induced errors. The diagnosis methodology is power- and area-efficient because it only needs low-complexity and low-area BIST circuits to accomplish the full diagnosis process. A 12-bit pipelined A/D converter with the proposed diagnosis scheme is designed and simulated using the TSMC 0.25um 1P5M technology to demonstrate the effectiveness of the proposed methodology.	12-bit;amplifier;analog-to-digital converter;built-in self-test;correctness (computer science);device under test;digital-to-analog converter;mixed-signal integrated circuit;multiplexing;sample and hold;voltage-controlled oscillator	Chih-Haur Huang;Kuen-Jong Lee;Soon-Jyh Chang	2004	13th Asian Test Symposium	10.1109/ATS.2004.8	system on a chip;embedded system;electronic engineering;voltage;computer science;engineering;electrical engineering;signal processing;process variation;time-division multiplexing;signal generator	EDA	25.816359053887492	52.178724645811805	182626
d789d3aa398cbf25d79a6dc2073c365d61fdf301	quantum error correction of time-correlated errors	quantum system;entangled state;quantum information;03 67 pp;quantum physics;error correction;quantum error correcting codes;time correlated;quantum error correction;03 67 a;stabilizer code;03 65 w	In this paper we make a distinction between time-correlated quantum errors which re-occur with a certain probability and new errors, uncorrelated with past errors. The obvious choice to deal with time-correlated errors, is to design a quantum error correcting code capable to correct (eu + ec) errors, where eu is the number of uncorrelated errors and ec the expected number of time-correlated errors. This solution is wasteful and, possibly unfeasible, due to the complexity of the quantum circuit for error correction. We propose an algorithm based upon the stabilizer formalism, which allows us to correct a single time-correlated error in addition to new, uncorrelated error(s). The algorithm can be applied to any quantum error correcting code designed to correct eu uncorrelated bit-flip or phase-flip error(s). 1 Quantum Error Correction Decoherence, the alteration of the quantum state as a result of the interaction with the environment, is probably the most challenging problem faced by quantum computation and quantum communication [11]. Quantum error correcting codes allows us to deal algorithmically with decoherence. We describe the effect of the environment upon a qubit as a transformation given by Pauli operators: (i) if the state of the qubit is unchanged then we apply the σI operator; (ii) a bit-flip error is the result of applying the transformation given by σx; (iii) a phase-flip error is the result of applying the transformation given by σz ; and (iv) a bitand phase-flip error is the result of applying the transformation given by σy = iσxσz. A quantum error correcting scheme takes advantage of the entanglement in two ways: • We entangle one qubit carrying information with (n−1) other qubits initially in state | 0〉 and create an n-qubit quantum codeword which is more resilient to errors. • We entangle the n qubits of the quantum codeword with ancilla qubits in such a way that we can measure the ancilla qubits to determine the error syndrome without altering the state of the n-qubit codeword, by performing a so called non-demolition measurement. The error syndrome tells if the individual qubits of the codeword have been affected by errors as well as the type of error. • Finally, we correct the error(s). Even though the no-cloning theorem prohibits the replication of a quantum state, we are able to encode a single logical qubit as multiple physical qubits and thus we can correct quantum errors [14]. For example, we can encode the state of a qubit | ψ〉 = α0 | 0〉 + α1 | 1〉	algorithm;ancilla bit;code word;computation;decoding methods;decoherence-free subspaces;encode;error detection and correction;forward error correction;lov grover;no-cloning theorem;quantum channel;quantum circuit;quantum computing;quantum decoherence;quantum entanglement;quantum error correction;quantum information science;quantum nondemolition measurement;quantum state;quantum system;qubit;semantics (computer science);software bug;stabilizer code	Feng Lu;Dan C. Marinescu	2007	Quantum Information Processing	10.1007/s11128-007-0058-1	quantum information;error detection and correction;theoretical computer science;quantum capacity;mathematics;quantum convolutional code;physics;algorithm;quantum mechanics;quantum error correction	Theory	38.5187697983494	59.54755151199258	183495
83de43ae64e2ddac3157dbdc448ef47bd523818a	bounds on the size of optimal difference triangle sets	traitement signal;upper bound interference convolutional codes polynomials conferences information theory informatics;interferencia;optimisation;convolutional codes;limite superieure;optimizacion;limite inferior;lower bounds;upper bounds;optimal difference triangle sets;tabla dato;interference;polynomials;upper bound;table donnee;brouillage;upper bounds lower bounds information theory optimal difference triangle sets;signal processing;optimization;informatics;electromagnetic interference;limite superior;data table;procesamiento senal;limite inferieure;lower bound;information theory;conferences	Applications of difference triangle sets are briefly described. New lower and upper bounds on the size of optimal difference triangle sets are given. u003e		Torleiv Kløve	1988	IEEE Trans. Information Theory	10.1109/18.2652	combinatorics;discrete mathematics;integer triangle;information theory;signal processing;triangle inequality;mathematics;geometry;upper and lower bounds;statistics	Theory	37.43827683677716	50.58897503161508	184000
0de6cff2187cfb7540b927483473c018cd48f867	alphabetic codes revisited	code alphabetique;characteristic;limite superieure;error correction codes;probability;loi probabilite;desigualdad;ley probabilidad;limite inferior;inequality;systeme binaire;longitud;relacion orden;alfabeto;inegalite;ordering;caracteristica;indexing terms;test;huffman codes;length;prefix code;fault diagnosis performance evaluation merging upper bound joining processes probability distribution information theory system testing sequential analysis random variables;ensayo;upper bound;probability law;essai;relation ordre;left to right;longueur;probability encoding error correction codes;probability distribution;sistema binario;upper bound optimal code length alphabetic code ordered probability distribution prefix code coding tree binary test problems characteristic inequality kraft inequality huffman code merging property lower bounds;caracteristique;code;limite superior;encoding;limite inferieure;codigo;lower bound;alphabet;binary system	An alphabetic code for an ordered probability distribution ( p k ) is a prefix code in which Pk is assigned to the kth codeword of the coding tree in the left-to-right order. This class of codes is applied to binary test problems. Many previous results on alphabetic codes are unified and enhanced. The characteristic inequality for alphabetic codes that is analogous to the Kraft Inequality for prefix codes is also derived. It is shown that if ( p k ) is in ascending or descending order, Lmi,, the expected length of an optimal alphabetic code, is the same as that of a Huffman code for the unordered distribution {pk} . An enhancement of Gilbert and Moore’s merging property of an optimal alphabetic code is proved. Two new lower bounds and a new upper bound on the expected length of an optimal alphabetic code are also proved, and a simple method is proposed for constructing good alphabetic codes when optimality is not critic a I. Index Terms --Alphabetic code, ordered probability distribution, binary testing, characteristic inequality.	binary classification;code word;huffman coding;kraft–mcmillan inequality;moore machine;prefix code;social inequality;sorting	Raymond W. Yeung	1991	IEEE Trans. Information Theory	10.1109/18.79913	arithmetic;prefix code;combinatorics;linear code;mathematics;upper and lower bounds;algorithm;statistics	Theory	39.06007972367865	56.79903985854283	184026
2ae25dd79b302f99e62e666de418f2e82dde37a3	on the two-dimensional binary (d, k) constrained array	d d 1 constrained array two dimensional binary d k constrained array capacity lower bound 0 k constrained array;entropy binary sequences;binary sequences;lower bound	Another description of the two-dimensional binary (d,k) constrained array is given. Based on this, a lower bound for the capacity of the (0,k) constrained array is presented, and it is reproved that the capacity of the (d,d+1) constrained array is equal to zero.		Xiao Ma;Xinmei Wang;Haitao Yue	2000		10.1109/GLOCOM.2000.891251	mathematical optimization;upper and lower bounds	NLP	39.031785912579785	55.19982956467171	184052
844ab32774b5ce5e857cfbbd315584131fd57190	applications of graph containers in the boolean lattice	sperner families;container method;error correcting codes;enumeration problems;boolean lattice	We apply the graph container method to prove a number of counting results for the Boolean lattice P(n). In particular, we: (i) Give a partial answer to a question of Sapozhenko estimating the number of t error correcting codes in P(n), and we also give an upper bound on the number of transportation codes; (ii) Provide an alternative proof of Kleitman’s theorem on the number of antichains in P(n) and give a two-coloured analogue; (iii) Give an asymptotic formula for the number of (p, q)-tilted Sperner families in P(n); (iv) Prove a random version of Katona’s t-intersection theorem. In each case, to apply the container method, we first prove corresponding supersaturation results. We also give a construction which disproves two conjectures of Ilinca and Kahn on maximal independent sets and antichains in the Boolean lattice. A number of open questions are also given.	code;error detection and correction;kahn process networks;maximal set;sperner's lemma	József Balogh;Andrew Treglown;Zsolt Adam Wagner	2016	Random Struct. Algorithms	10.1002/rsa.20666	combinatorics;discrete mathematics;boolean algebra;mathematics	Theory	38.834264567066185	52.91702292227882	184437
ffff24e5f29267f3693bcb0d9643fcb86a9e4775	on the capacity of constrained permutation codes for rank modulation	flash memories error correction codes;modulation ash error correction codes interference nonvolatile memory electronic mail indexes;multi permutations error correcting codes constrained codes kendall metric permutations;error correcting codes constrained permutation codes flash memories rank modulation scheme	Motivated by the rank modulation scheme, a recent study by Sala and Dolecek explored the idea of constraint codes for permutations. The constraint studied by them is inherited by the inter-cell interference phenomenon in flash memories, where high-level cells can inadvertently increase the level of lowlevel cells. A permutation σ ∈ S<sub>n</sub> satisfies the single-neighbor k-constraint if |σ(i + 1) - σ (i)| ≤ k for all 1 ≤ i ≤ n - 1. In this paper, this model is extended into two constraints. A permutation σ ∈ S<sub>n</sub> satisfies the two-neighbor k-constraint if for all 2 ≤ i ≤ n-1, |σ(i)-σ(i-1)| ≤ k or |σ(i + 1)-σ(i)|≤k, and it satisfies the asymmetric two-neighbor k-constraint if for all 2 ≤ i ≤ n - 1, σ(i - 1) - σ(i) <; k or σ(i + 1) - σ(i) <; k. We show that the capacity of the first constraint is (1 + ϵ)/2 in case that k = Θ(n<sup>ϵ</sup>) and the capacity of the second constraint is 1 regardless for any positive k. We also extend our results and study the capacity of these two constraints combined with error-correcting codes in the Kendall τ-metric.	code;constraint (mathematics);flash memory;forward error correction;high- and low-level;interference (communication);kendall tau distance;modulation	Sarit Buzaglo;Eitan Yaakobi	2016	IEEE Transactions on Information Theory	10.1109/TIT.2016.2519400	block code;concatenated error correction code;turbo code;combinatorics;discrete mathematics;low-density parity-check code;online codes;theoretical computer science;serial concatenated convolutional codes;bcjr algorithm;tornado code;linear code;expander code;luby transform code;mathematics;forward error correction;raptor code;error floor	Theory	37.807161238001456	58.38317935109225	184625
7d0dd39ca633a5263163a608ab9c08bd4a873e19	decomposition constructions for secret-sharing schemes	graph theory;partage secret;maximum degree;cle secrete;linear programming channel capacity graph theory cryptography;information science;information rates;indexing terms;indice informacion;information rates security terminology computer science information science mathematical model cryptography;decomposition construction;programacion lineal;information rate secret sharing schemes decomposition construction graph;channel capacity;criptografia;cryptography;graph;linear programming;mathematical model;information rate;programmation lineaire;cryptographie;terminology;computer science;secret sharing scheme;taux information;security;secret sharing schemes	The purpose of this paper is to decribe a very powerful decomposition construction for perfect secret sharing schemes. We give several applications of the construction, and improve previous results by showing that for any graph G of maximum degree d, there is a perfect secret sharing scheme for G with information rate 2=(d + 1). As a corollary, the maximum information rate of secret sharing schemes for paths on more than three vertices and for cycles on more than four vertices is shown to be 2=3.	peak information rate;secret sharing;vertex (geometry)	Douglas R. Stinson	1994	IEEE Trans. Information Theory	10.1109/18.272461	combinatorics;discrete mathematics;information science;computer science;linear programming;cryptography;graph theory;theoretical computer science;mathematical model;mathematics;graph;channel capacity;statistics	Crypto	37.40407995592226	54.27717751061656	185324
cc87d2dc791752c02556802eef31e9b3ae4499bf	simulation methodology and evaluation of through silicon via (tsv)-finfet noise coupling in 3-d integrated circuits	transient analysis digital signals integrated circuit noise leakage currents mosfet spice technology cad electronics three dimensional integrated circuits;size 22 nm planar technology leakage current noise transient tcad finfet model technology computer aided design high fidelity finfet spice model full wave electromagnetic simulation substrate circuit model digital signal superior gate control planar device noise immunity planar bulk technology substrate noise 3 d dies stacking short channel effect bulk finfet 3 d integrated circuit tsv finfet noise coupling through silicon via simulation methodology;through silicon vias noise substrates transistors integrated circuit modeling solid modeling couplings;transistors;solid modeling;integrated circuit modeling;through silicon via tsv 3 d integrated circuit ic computer aided design cad;substrates;couplings;noise;through silicon vias	Bulk FinFETs have emerged as the solution to short-channel effects at the 22-nm technology node and beyond. The capability of 3-D stacking of dies from various technologies will eventually enable stacking FinFET dies within 3-D integrated circuits. Within 3-D circuits, through silicon vias (TSVs) are a known source of substrate noise in planar bulk technologies. While FinFETs are expected to demonstrate superior noise immunity relative to planar devices due to superior gate control over and volume inversion of the active fin, the impact of TSV noise on FinFETs has not been previously quantified. To evaluate TSV-FinFET noise coupling, we develop in this paper a simulation methodology that extends the state of the art by accurately modeling substrate noise due to digital signals on nearby TSVs and improving the extraction of substrate circuit models from full-wave electromagnetic simulations. To overcome the lack of high-fidelity FinFET SPICE models that accurately capture the effects of substrate noise, we use high-fidelity technology computer-aided design (TCAD) FinFET models. Our results show that FinFETs exhibit an order of magnitude less leakage current noise transients, and two orders of magnitude less saturation current noise transients, relative to comparable planar technologies. Our findings are generalizable, showing that FinFETs are significantly more robust to substrate noise than equivalent planar devices.	amplifier;analogue electronics;computer-aided design;digital electronics;electronic circuit simulation;feedback;hfss;integrated circuit;lossy compression;qualitative comparative analysis;ring oscillator;spice;semiconductor device fabrication;simulation;spectral leakage;stacking;substrate (electronics);substrate coupling;through-silicon via;traffic collision avoidance system;transistor	Brad D. Gaynor;Soha Hassoun	2015	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2014.2341834	embedded system;substrate coupling;electronic engineering;engineering;noise;electrical engineering;solid modeling;coupling;transistor	EDA	24.977234072369196	56.97264461224338	186547
96244a98caf1d836a63454417009818309e42b72	an evolutionary algorithm-based approach to robust analog circuit design using constrained multi-objective optimization	design model;constrained optimization;analog circuit design;integrated circuit;pareto front;circuit design;design for manufacture;multi objective optimization;test bed;optimization problem;analog circuits;ultra wideband;statistical analysis;low noise amplifier;analog integrated circuits;design for manufacturability;circuit sizing;design for yield;genetic algorithm;genetic algorithms;evolutionary algorithm;evolutionary optimization;optimal algorithm;multi objective optimization problem;population based algorithms	The increasing complexity of circuit design needs to be managed with appropriate optimization algorithms and accurate statistical descriptions of design models in order to reach the design specifics, thus guaranteeing ”zero defects”. In the Design for Yield open problems are the design of effective optimization algorithms and statistical analysis for yield design, which require time consuming techniques. New methods have to balance accuracy, robustness and computational effort. Typical analog integrated circuit optimization problems are computationally hard and require the handling of multiple, conflicting, and non-commensurate objectives having strong nonlinear interdependence. This paper tackles the problem by evolutionary algorithms to produce tradeoff solutions. In this research work, Integrated Circuit (IC) design has been formulated as a constrained multi-objective optimization problem defined in a mixed integer/discrete/continuous domain. The RF Low Noise Amplifier, Leapfrog Filter, and Ultra Wideband LNA real-life circuits were selected as test beds. The proposed algorithm, A-NSGAII, was shown to produce acceptable and robust solutions in the tested applications, where state-ofart algorithms and circuit designers failed. The results show significant improvement in all the chosen IC design problems.	analogue electronics;evolutionary algorithm;integrated circuit design;interdependence;linear integrated circuit;low-noise amplifier;mathematical optimization;multi-objective optimization;nonlinear system;optimization problem;radio frequency;real life;robustness (computer science);ultra-wideband	Giuseppe Nicosia;Salvatore Rinaudo;Eva Sciacca	2008	Knowl.-Based Syst.	10.1016/j.knosys.2007.11.014	physical design;probabilistic-based design optimization;mathematical optimization;constrained optimization;genetic algorithm;computer science;multi-objective optimization;evolutionary algorithm;design for manufacturability	EDA	27.026607641284144	48.646726448941486	186871
573fd1cd997bb7f973814e1b2c0d63e197b2c642	synthesis procedure of configurable building block-based linear and nonlinear analog circuits		Synthesis and design methodology suitable for computer-aided design tools are now emerging as major challenge for analog system designers. This paper proposes a systematic synthesis procedure which provides reusability and programmability, for linear and nonlinear analog circuits using configurable building block (CBB) and analog cell (AC). Operational transconductance amplifier-based ACs are synthesized in terms of CBBs. Reusability and programmability of the synthesis methodology have been demonstrated by realizing different types of linear and nonlinear circuits and mapping those circuits in field programmable analog array. Finally, the proposed methodology has been integrated as an electronic design automation tool AnaSyn1.0. Performance of all the proposed circuits has been verified by SPICE simulation.	analogue electronics;common building block;computer-aided design;electronic design automation;field-programmable analog array;integrated circuit;nonlinear system;operational transconductance amplifier;spice;simulation	Mousumi Bhanja;Baidya Nath Ray	2017	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2017.2681062	transconductance;electronic engineering;computer science;operational transconductance amplifier;analogue electronics;analog multiplier;electronic circuit;control engineering;electronic design automation;field-programmable analog array;reusability	EDA	27.122458733204294	49.34064246738728	187722
5c6a79c16c342fc86692c821151990db64ef0ba2	static electromigration analysis for on-chip signal interconnects	switching scenarios;microprocessor;unidirectional current;electric charge;switching circuits;wire segments;signal analysis;on chip signal interconnects;integrated circuit reliability vlsi electromigration circuit analysis computing integrated circuit interconnections electric charge driver circuits switching circuits;circuit nodal formulation;charge transfer;physical design;indexing terms;industrial electromigration analysis tool;time domain analysis;electromigration signal analysis integrated circuit interconnections wire charge transfer current density time domain analysis root mean square equations circuit simulation;chip;wire;circuit simulation;integrated circuit interconnections;static electromigration analysis;multiple simultaneous switching drivers;high performance designs;vlsi;microprocessor static electromigration analysis on chip signal interconnects high performance designs charge transfer wire segments linear equations circuit nodal formulation switching scenarios unidirectional current bidirectional current multiple simultaneous switching drivers simultaneous switching drivers modeling industrial electromigration analysis tool;driver circuits;time domain;electromigration;simultaneous switching drivers modeling;root mean square;integrated circuit reliability;linear equations;static analysis;bidirectional current;circuit analysis computing;high performance;current density	With the increase in current densities, electromigration has become a critical concern in high-performance designs. Typically, electromigration has involved the process of time-domain simulation of drivers and interconnect to obtain average, root mean square (rms), and peak current values for each wire segment. However, this approach cannot be applied to large problem sizes where hundreds of thousands of nets must be analyzed, each consisting of many thousands of RC elements. The authors propose a static electromigration analysis approach. They show that the charge transfer through wire segments of a net can be calculated directly by solving a system of linear equations, derived from the nodal formulation of the circuit, thereby eliminating the need for time domain simulation. Also, they prove that the charge transfer through a wire segment is independent of the shape of the driver current waveform. From the charge transfer through each wire segment, the average current is obtained directly, as well as approximate rms and peak currents. The authors account for the different possible switching scenarios that give rise to unidirectional or bidirectional current by separating the charge transfer from the rising and falling transitions and also propose approaches for modeling multiple simultaneous switching drivers. They implemented the proposed static analysis approach in an industrial electromigration analysis tool that was used on a number of industrial circuits, including a large microprocessor. The results demonstrate the accuracy and efficiency of the approach.	approximation algorithm;electrical connection;electromigration;industrial pc;linear equation;mean squared error;microprocessor;rc circuit;simulation;static program analysis;system of linear equations;waveform	David Blaauw;Chanhee Oh;Vladimir Zolotov;Aurobindo Dasgupta	2003	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/TCAD.2002.805728	chip;physical design;embedded system;electromigration;electronic engineering;index term;root mean square;electric charge;time domain;computer science;engineering;electrical engineering;signal processing;linear equation;very-large-scale integration;charge-transfer complex;static analysis;current density;quantum mechanics	EDA	25.651974779377664	57.58240440112834	188365
786fa908c09bd19a14f45a7e9f017aaba5f3f994	construction of codes for dna computing by the greedy algorithm		In this paper we construct codes for DNA computing using the greedy algorithm over Z4. We obtain linear codes over Z4 with bounded GC content. We also consider the edit distance, we gave upper bounds for the edit distance and construct codes with bounded edit distance.	code;dna computing;edit distance;greedy algorithm;z4 (computer)	Nabil Bennenni;Kenza Guenda;T. Aaron Gulliver	2015	ACM Comm. Computer Algebra	10.1145/2768577.2768583	greedy randomized adaptive search procedure;mathematical optimization;combinatorics;greedy algorithm;theoretical computer science;mathematics	Theory	38.05198934497381	54.00843666180222	188647
50ed65099a4d6091517a5fffecd12f67e75a5de7	estimation of clock drift in hil testing by property-based conformance check	continuous systems;clock drift estimation;clocks;testing;hil measurements;program verification;computational modeling;signal data distortion;correlation timing clocks testing computational modeling data models;program testing;hardware in the loop testing;property based conformance check;program verification program testing;hil measurements clock drift estimation property based conformance check hardware in the loop testing measurement devices signal data distortion continuous systems;measurement devices;clock drift;correlation;data models;timing	Hardware-in-the-loop testing (HiL) requires exact timing of measurements of continuous signals. Although great effort is put into design and manufacturing of measurement devices, inaccuracies might occur. In industrial practice differently fast clocks can occur and lead to drifting sampling rates. This drift leads to stretched and distorted signal data. Based on a methodology for analysing test data of continuous systems, we suggest to estimate the drift in HiL measurements with a property-based approach. Data of a reference device and the device exhibiting the drift is compared with help of signal properties. The shifted occurrences of one property depends on the clock's drift.	clock drift;computation;conformance testing;cross-correlation;embedded system;fast fourier transform;hil bus;interpolation;polynomial;real-time computing;requirement;sampling (signal processing);simulation;test data	Jacob Palczynski;Carsten Weise;Stefan Kowalewski;Daniel Ulmer	2011	2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops	10.1109/ICSTW.2011.101	embedded system;data modeling;electronic engineering;real-time computing;computer science;software engineering;software testing;clock drift;computational model;correlation;hardware-in-the-loop simulation	SE	25.741899884224033	56.90329213068105	188759
6e577bbebb1490cdde8437547bce6dd42519b260	a semiconductor wafer representation database and its use in the preditor process editor and statistical simulator	analytical models;circuit simulation;computational modeling;permission;solid modeling;spatial databases;statistics;integrated circuit synthesis;statistics object oriented modeling computational modeling computer simulation spatial databases solid modeling circuit simulation permission analytical models integrated circuit synthesis;computer simulation;object oriented modeling	This paper describes a database, called the Chip PREDITOR Process Editor and Statistical Simulator D. M. H. Walker, C. S. Kellen, and A. J. Strojwaa Department of Electrical and Cwnputer Engineering Carnegie Mellon University Pittsburgh, PA 15213-3890 geometry and field Database (CDB), for representing the results of IC process and device simulation, and its use in the PREDITOR process editor and statistical simulator. Using fast analytical models, process simulation and device parameter extraction for a CMOS inverter takes 1.3sec on a DECstation 3100, with database access taking 14-34% of that time.	amiga walker;cmos;decstation;preditor;power inverter;simulation;wafer (electronics)	D. M. H. Walker;Chris S. Kellen;Andrzej J. Strojwas	1991		10.1145/127601.127736	computer simulation;simulation;computer science;theoretical computer science;solid modeling;computational model;statistics	EDA	29.554560493906532	51.584122854432074	188876
2d89a6650709d2713a1416b68045916d14b68fce	thermal investigations of integrated circuits and systems at therminic'02	integrated circuit		integrated circuit	Márta Rencz	2003	Microelectronics Journal	10.1016/S0026-2692(03)00199-X		EDA	32.68534937365544	48.988570371888535	188923
98099fa4f5bef705e0ba59aab2039a47216d1bd1	a low-voltage fully differential cmos high-speed track-and-hold circuit	cmos integrated circuits;capacitance voltage differential amplifiers cmos technology circuit simulation clocks circuit noise noise reduction noise cancellation costs;building block;high speed integrated circuits integrated circuit design integrated circuit modelling circuit simulation sample and hold circuits cmos integrated circuits low power electronics;sample and hold circuits;integrated circuit design;circuit simulation;low voltage;integrated circuit modelling;low power electronics;0 5 micron cmos low voltage fully differential high speed track and hold circuits circuit design building blocks;high speed;high speed integrated circuits	A new technique for realizing a low-voltage fully differential CMOS high-speed track-and-hold (T/H) circuit is presented. The design consideration of the building blocks is described in detailed. Simulation results are given to demonstrate the potential advantage of the new technique.	cmos;simulation	Tsung-Sum Lee;Chi-Chang Lu	2002		10.1109/APCCAS.2002.1114924	equivalent circuit;mixed-signal integrated circuit;physical design;embedded system;electronic engineering;electronics;adiabatic circuit;high impedance;computer science;engineering;electrical engineering;operating system;integrated circuit;circuit design;linear circuit;diode-or circuit;application-specific integrated circuit;integrated injection logic;circuit extraction;low voltage;electronic circuit simulation;cmos;pin compatibility;discrete circuit;low-power electronics;integrated circuit design	EDA	26.155784646698127	52.193278029027454	189061
b133035d3c58b664087ae8439ebc761893f1cd36	a novel concept for stateless random bit generators in cryptographic applications	ring oscillators;cryptographic device;delay elements;oscillations;random number generation cmos digital integrated circuits cryptography integrated circuit noise minimum entropy methods oscillators;cryptography jitter ring oscillators delay circuit testing throughput circuit noise noise generators entropy circuit simulation;stateless random bit generator;oscillator based generators;oscillators;ring oscillator;random number generation;minimum entropy methods;minimum entropy;0 12 micron stateless random bit generator cryptographic device ring oscillators delay elements oscillator based generators statistical quality minimum entropy cmos process;cmos process;statistical quality;circuit simulation;cmos digital integrated circuits;cryptography;0 12 micron;integrated circuit noise	A new, patent pending, concept for a random bit generator, suitable to be integrated in a cryptographic device, is presented. The proposed circuit exploits the relative jitter between two identical ring oscillators sharing the same delay elements and shows several advantages with respect to other oscillator-based generators reported in the technical literature. In particular, the generator is stateless and therefore easily testable accordingly to what is reported in (Bucci, 2005). Moreover, the generation throughput is automatically adapted to the available noise in the circuit thus guaranteeing the statistical quality (minimum entropy) of the generated bits. To validate the proposed circuit, simulation results on a 0.12mum CMOS process are reported	cmos;cryptography;simulation;stateless protocol;throughput	Marco Bucci;Luca Giancane;Raimondo Luzzi;Mario Varanonuovo;Alessandro Trifiletti	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1692586	electronic engineering;real-time computing;computer science;theoretical computer science;mathematics;oscillation;quantum mechanics	Arch	27.954185171161505	55.65050935541274	190024
f3b2a24b44752d30cd407dd8e5d29c806bc04505	on the geometry of stabilizer states	exterior product;stabilizer circuits;computational geometry;inner product;stabilizer states;quantum circuits;wedge product	Large-scale quantum computation is likely to require massive quantum error correction (QEC). QEC codes and circuits are described via the stabilizer formalism, which represents stabilizer states by keeping track of the operators that preserve them. Such states are obtained by stabilizer circuits (consisting of CNOT, Hadamard and Phase gates) and can be represented compactly on conventional computers using Θ(n2) bits, where n is the number of qubits [17]. As an additional application, the work in [1, 2] suggests the use of superpositions of stabilizer states to represent arbitrary quantum states. To aid in such applications and improve our understanding of stabilizer states, we characterize and count nearest-neighbor stabilizer states, quantify the distribution of angles between pairs of stabilizer states, study succinct stabilizer superpositions and stabilizer bivectors, explore the approximation of non-stabilizer states by single stabilizer states and short linear combinations of stabilizer states, develop an improved inner-product computation for stabilizer states via synthesis of compact canonical stabilizer circuits, propose an orthogonalization procedure for stabilizer states, and evaluate several of these algorithms empirically.	algorithm;approximation;bell state;computation;computer;controlled not gate;error detection and correction;formal system;quantum computing;quantum error correction;quantum state;quantum superposition;qubit;stabilizer code	Héctor J. García;Igor L. Markov;Andrew W. Cross	2014	Quantum Information & Computation		exterior algebra;computational geometry;control theory;mathematics	ML	36.621847812841175	47.952081770973564	190576
981874c8fc5dc93d11b6dc535fd02d3d9fa32871	variable-rate, static and universal generic linear network codes	static linear network codes;associated layered network universal generic linear network codes static linear network codes variable rate network codes acyclic network variable rate generic edge failures data generating edges;error correction codes;gain;linear codes;variable rate generic;edge failures;failure analysis;variable rate codes;acyclic network;network coding;vectors;variable rate network codes;sun;encoding vectors network coding robustness error correction codes gain sun;robustness;encoding;associated layered network;variable rate codes failure analysis linear codes;data generating edges;universal generic linear network codes	On an acyclic network, generic linear network codes are known to have the strongest optimality in terms of linear independence. This paper characterizes other properties of generic codes on a given acyclic network. We first show that every generic code is variable-rate generic with the additional property that the coding coefficients in relation to data-generating edges need not be changed. We next strengthen the concept of static generic codes to make the code preserve the generic property not only in the case of edge failures, but also in the case of adding some edges to be data-generating. Along this line, we define a new class of generic codes, called universal generic codes, such that the code is always generic with respect to any selection of data-generating edges. Last, we provide a unified characterization of static and universal generic codes on a given acyclic network in terms of the generic codes on an associated layered network.	code;coefficient;directed acyclic graph;generic programming	Qifu Tyler Sun	2012	2012 International Symposium on Network Coding (NetCod)	10.1109/NETCOD.2012.6261903	block code;combinatorics;discrete mathematics;generic property;theoretical computer science;generic point;linear code;mathematics	Theory	36.034568689367106	57.98960521745949	190728
205d87f9fb486abcbe2311911b32283c566a51b8	a novel specification based test pattern generation using genetic algorithm and wavelets	circuit under test;fault simulation;wavelet analysis test pattern generation genetic algorithm analog circuit mixed signal circuits integrated circuit testing faults detection transient signal;and mixed signal;automatic test pattern generation;pattern generation;specification based testing;genetic algorithms automatic test pattern generation integrated circuit testing analogue circuits mixed analogue digital integrated circuits transient analysis fault simulation wavelet transforms;transient analysis;wavelet transforms;integrated circuit testing;mixed analogue digital integrated circuits;test generation;genetic algorithm;genetic algorithms;analogue circuits;test pattern generators genetic algorithms circuit testing circuit faults costs electrical fault detection fault detection transient analysis signal analysis wavelet analysis	For analog and mixed signal circuits, the traditional tests are time consuming and also most expensive in terms of both test development and test implementation costs. So for testing these ICs new testing strategies are investigated. In this paper, a novel test generation methodology for the detection of faults in analog cores is proposed. A transient signal is taken as input stimulus for the circuit under test and the output response is analysed using wavelets. Specifications of the circuit are mapped to the performance space and the specifications are checked implicitly. The proposed fault oriented test generator also computes the optimal test patterns based on genetic algorithm.	genetic algorithm;matlab;mixed-signal integrated circuit;spice;sensor;test card;very-large-scale integration;wavelet	P. Kalpana;K. Gunavathi	2005	18th International Conference on VLSI Design held jointly with 4th International Conference on Embedded Systems Design	10.1109/ICVD.2005.28	electronic engineering;real-time computing;genetic algorithm;fault coverage;computer science;automatic test pattern generation;test compression;algorithm	EDA	24.699053667038555	51.57394000539126	191116
7501b721b648f4bb25a75f6a3d7aa1d4812242b4	a coding method of multicolored images using dynamic pattern generator	pattern generation	transmission and s t o r a g e of t h e mult icolored image, s e v e r a l methods have been app l i ed i n The mosaic method and geometric method p r a c t i c e , such as t h e mosaic system [ 21, have been used f o r coding t h e mult icolored t h e geometric method [ 4 , 51, and DRCS [ l ] . image. This paper proposes a dynamic p a t t e r n gene ra t ion method aiming a t a more f a i t h f u l image r ep resen ta t ion . I n t h e proposed t h e image employing s e v e r a l t e n s of p a t t e r n s . method, t h e c o l o r boundary i s e x t r a c t e d from Those methods have d i f f i c u l t i e s i n representt he o r i g i n a l p i c t u r e . The r e s u l t i s divided ing t h e image very a c c u r a t e l y , b u t have a i n t o smaller blocks, and t h e p a t t e r n code is merit t h a t t h e coding, decoding and d i sp lay generated f o r each block, are p o s s i b l e i n a way as s imple a s t h a t i n t h e case of c h a r a c t e r s , with less burden on t h e terminal devices . The mosaic method and DRCS r ep resen t	artificial intelligence;distributed version control	Ikuo Ishii;Yoshimi Kagaya;Junji Yamato;Hideo Makino	1989	Systems and Computers in Japan	10.1002/scj.4690201208	arithmetic;discrete mathematics;color depth;computer science;mathematics	AI	37.89142311502753	47.04147284690729	191121
0b8f5103db7f94b2228b77a0fc164c096dc89a18	reliability analysis for full-2 code	nonmds code;linear codes binary codes combinatorial mathematics decoding fault tolerance;reliability;encoding decoding;storage system;fault tolerant;decoding;parity check codes;erasure code;reliability modeling;binary codes;linear codes;graph representation erasure code full 2 code reliability markov model;arrays;artificial neural networks;storage system reliability analysis full 2 code 2 erasure binary linear code nonmds code optimal encoding decoding fault tolerance recoverable k erasures combinatorial method;markov model;2 erasure binary linear code;linear code hard disks reed solomon codes fault tolerance computational complexity information analysis algorithm design and analysis educational institutions information technology encoding;linear code;full 2 code;fault tolerance;graph representation;combinatorial method;reliability analysis;markov processes;optimal encoding;combinatorial mathematics;reed solomon code;mds code;recoverable k erasures	Recently, with the fast development of storage system, 2-erasure coding schemes were widely used in industrial society. To meet different requirements, many kinds of 2-erasure coding schemes were presented, such as Reed-Solomon codes, binary linear codes, parity array codes, and so on. Full-2 code is a 2-erasure binary linear code. It is a non-MDS code, but achieves optimal encoding, decoding, and updating performance. Moreover, its fault tolerance is beyond 2, i.e. “2-erasure” is the huge undervaluation of its fault tolerance. It is hard to evaluate the precise reliability of full-2 code. The reason is that the reliability model is complex and the proportion of recoverable k-erasures (k ≫ 2) to total k-erasures is difficult to calculate. In this paper, we present a combinatorial method to analyze the precise reliability of full-2 code. The reliability of full-2 based storage systems is also evaluated.	computer data storage;decision problem;erasure code;fault tolerance;graph (abstract data type);linear code;parity bit;peer-to-peer;reed–solomon error correction;reliability engineering;requirement;schedule (computer science);serializability;singleton bound	Sheng Lin;Chi Zhang;Gang Wang;Xiaoguang Liu;Jing Liu	2009	2009 10th International Symposium on Pervasive Systems, Algorithms, and Networks	10.1109/I-SPAN.2009.95	block code;systematic code;multidimensional parity-check code;parallel computing;constant-weight code;low-density parity-check code;universal code;computer science;theoretical computer science;code rate;linear code;locally testable code;algorithm	Arch	36.689722781358554	58.80746163246594	192067
42b00fd3f23817b33ed9329e125c30b0385f4d69	importance sampled circuit learning ensembles for robust analog ic design	device size;circuit assembly;process geometries;robust analog ic design;iscles-specific library;boosting-style importance;target analog behavior;digital-sized circuit;d converter;possible digital-sized circuit block;robust analog design method;simulation;moores law;transistors;importance sampling;analog circuits;topology;boosting;design method;boosting algorithm;integrated circuit design	This paper presents ISCLEs, a novel and robust analog design method that promises to scale with Moore's Law, by doing boosting-style importance sampling on digital-sized circuits to achieve the target analog behavior. ISCLEs consists of: (1) a boosting algorithm developed specifically for circuit assembly; (2) an ISCLEs-specific library of possible digital-sized circuit blocks; and (3) a recently-developed multi-topology sizing technique to automatically determine each block's topology and device sizes. ISCLEs is demonstrated on design of a sinusoidal function generator and a flash A/D converter, showing promise to robustly scale with shrinking process geometries.	algorithm;importance sampling;integrated circuit design;moore's law;sampling (signal processing)	Peng Gao;Trent McConaghy;Georges G. E. Gielen	2008	2008 IEEE/ACM International Conference on Computer-Aided Design	10.1145/1509456.1509547	mixed-signal integrated circuit;control engineering;electronic engineering;computer science;electrical engineering;boosting	EDA	27.69856654282919	50.22513550533341	192232
07f55b006baab1663a80bb010df53b0ada69538e	decoder and pass transistor based digitally controlled linear delay element	transistors asynchronous circuits clocks cmos integrated circuits decoding digital circuits digital control integrated circuit design synchronisation timing circuits;decoder delay element linearity;size 180 nm cmos technology time accuracy delay non linear function asynchronous circuit design pulse timing precision digital input circuits synchronization problem reduction clock circuitry integrated circuit design digitally controlled linear delay element pass transistor decoder;delays capacitors decoding vectors mosfet synchronization	With advancement and scaling in Integrated circuit design, the accuracy of clock circuitry plays an increasingly important role. Timing precision is important to reduce the synchronization problem in digital circuits. The precise timing of pulse is also extremely important for designing asynchronous circuits. Delay element is the basic building block for asynchronous circuits. Many circuits of digitally controlled delay elements have been reported in literature. The reported circuits provide a delay which is a non linear function of digital inputs. Non linearity of delay in reported circuits affects the accuracy of time because of which they cannot be used in real world applications. This paper presents a novel circuit of decoder and pass transistor based delay element. The proposed circuit is designed for 3 bits and it provides a delay which is a linear function of digital inputs. The design is implemented in cadence using 180 nm CMOS technology.	asynchronous circuit;cmos;codel;digital electronics;electronic circuit;image scaling;integrated circuit design;linear function;pass transistor logic	Prachi Sharma;Anil Gupta	2014	2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2014.6968252	mixed-signal integrated circuit;electronic engineering;real-time computing;asynchronous circuit;computer science;control theory	EDA	25.490018993979863	50.389321269047265	192237
460da506713093b17e84225e025f019d886ceb25	critical graphs in index coding	network theory graphs channel capacity directed graphs network coding;capacity region minimal graph distinct rates asymptotic nonlinear index coding union of disjoint strongly connected subgraphs nonuscs critical graph one shot nonlinear index coding problem;indexes vectors channel coding receivers decoding	In this paper we define critical graphs as minimal graphs that support a given set of rates for the index coding problem, and study them for both the one-shot and asymptotic setups. For the case of equal rates, we find the critical graph with minimum number of edges for both one-shot and asymptotic cases. For the general case of possibly distinct rates, we show that for one-shot and asymptotic linear index coding, as well as asymptotic non-linear index coding, each critical graph is a union of disjoint strongly connected subgraphs (USCS). On the other hand, we identify a non-USCS critical graph for a one-shot non-linear index coding problem. In addition, we show that the capacity region of the index coding associated with a given graph can be obtained by time-sharing over valid index codes for its strongly connected components.	code;critical graph;nonlinear system;strongly connected component;time-sharing;unified soil classification system	Mehrdad Tahmasbi;Amirbehshad Shahrasbi;Amin Gohari	2014	2014 IEEE International Symposium on Information Theory	10.1109/ISIT.2014.6874839	1-planar graph;block graph;pathwidth;split graph;tanner graph;combinatorics;discrete mathematics;linear network coding;cograph;shannon–fano coding;graph product;variable-length code;theoretical computer science;pancyclic graph;mathematics;modular decomposition;chordal graph;strongly connected component;indifference graph;line graph	Theory	35.59979929119505	57.31116853336328	192276
0c5c96face12a60ad0a4434cb705c97397d58439	a parametric test method for analog components in integrated mixed-signal circuits	analog component;mixed-signal test;built-in self test;analog circuit;test quality;fault model;integrated mixed-signal circuit;test methodology;analog cmos bandpass filter;test stimulus;test measurement selection;parametric test method;measurement error;vlsi;wavelet transform;fourier transform;band pass filters;fourier transforms;bandpass filter;inductance;analog circuits;test methods;wavelet transforms;device under test;cmos integrated circuits	In this paper, we present a novel approach to use test stimuli generated by digital components of a mixed-signal circuit for testing its analog components. A wavelet transform is applied to the response signal of the device under test (DUT). We will show, that in comparison to Fourier transform or no transform at all, particular properties of this transformation are advantageous for mixed-signal test and especially built-in self test.We introduce a new method for test measurement selection based on a non-deterministic parametric fault model for analog circuits. This approach allows for noise and measurement error in testing. We show, how test quality can be optimized in the presented fault model. Our test methodology is demonstrated on an analog CMOS bandpass filter.	analogue electronics;built-in self-test;built-in test equipment;cmos;device under test;fault coverage;fault model;image noise;mathematical optimization;mixed-signal integrated circuit;test design;wavelet transform	Michael Pronath;Volker Gloeckel;Helmut E. Graeb	2000			analog signal processing;embedded system;fourier transform;electronic engineering;computer science;engineering;electrical engineering;stuck-at fault;automatic test pattern generation;mathematics;band-pass filter;wavelet transform;analog multiplier	EDA	24.87670136563861	51.65766221682732	192626
243b313f48bb16a657e4ef367ef155d7cc3c4513	1-perfect codes over dual-cubes vis-à-vis hamming codes over hypercubes	hypercubes partitioning algorithms topology routing electronic mail context upper bound;hypercube networks graph theory hamming codes;interconnection networks 1 perfect codes hamming codes dual cubes hypercubes exchanged hypercubes latin square domination number resource placement;latin square hamming code dual cube perfect code hypercube spanning subgraph vertex partition	A 1-perfect code of a graph G is a set C ⊆ V(G) such that the 1-balls centered at the vertices in C constitute a partition of V(G). In this paper, we consider the dual-cube D Qm that is a connected (m + 1)-regular spanning subgraph of the hypercube Q2m+1, and show that it admits a 1-perfect code if and only if m = 2k - 2, k ≥ 2. The result closely parallels the existence of Hamming codes over the hypercube. The algorithm for that purpose employs a scheme by Jha and Slutzki for a vertex partition of Qm+1 into Hamming codes using a Latin square, and carefully allocates those codes among various m-cubes in D Qm. The result leads to tight bounds on domination numbers of the dual-cube and the exchanged hypercube.	algorithm;cubes;dominating set;file spanning;hamming code;olap cube;parallels desktop for mac	Pranava K. Jha	2015	IEEE Transactions on Information Theory	10.1109/TIT.2015.2448674	block code;hamming weight;combinatorics;discrete mathematics;hamming distance;hamming bound;theoretical computer science;cyclic code;hamming graph;linear code;hamming code;mathematics;forward error correction;hamming(7,4)	Theory	39.03775993473663	50.687312787180105	192676
cf91c793b52f3fdeab19f6787fad20512ab24f3f	robust finite field arithmetic for fault-tolerant public-key cryptography	public key cryptography;tolerancia falta;detection erreur;deteccion error;cryptographie cle publique;code cyclique;arithmetic coding;fault tolerant;hamming weight;redundancia;codigo ciclico;modulus scaling;securite informatique;anneau;corps fini;effet dimensionnel;arithmetique;arithmetic code;homomorphism;computer security;finite field;codigo aritmetico;fault tolerant system;arithmetic codes;hamming distance;aritmetica;redundancy;idempotency;arithmetics;system design;size effect;seguridad informatica;fault tolerance;finite field arithmetic;distance hamming;homomorphisme;sistema tolerando faltas;campo finito;cyclic codes;cyclic code;code binaire;codigo binario;systeme tolerant les pannes;code arithmetique;ring;homomorfismo;error detection;efecto dimensional;homomorphic embedding;distancia hamming;tolerance faute;binary code;redondance;anillo;modular reduction	We present a new approach to fault tolerant public key cryptography based on redundant arithmetic in finite rings. Redundancy is achieved by embedding non-redundant field or ring elements into larger rings via suitable homomorphisms obtained from modulus scaling. Our approach is closely related to, but not limited by the theory of cyclic binary and arithmetic codes. We present a framework for system-designers that allows flexible trade-offs between circuit area and desired level of fault tolerance. Our method applies to arithmetic in prime fields and extensions of characteristic 2 where it serves two mutually beneficial purposes: The redundancy of the larger ring can be used for error detection, while its modulus has a special low Hamming-weight form, lending itself particularly well to efficient modular reduction.	adversary (cryptography);bitwise operation;code;computation;embedded system;error detection and correction;fault tolerance;hamming weight;image scaling;modulus of continuity;modulus robot;overhead (computing);peer-to-peer lending;public-key cryptography;window function	Gunnar Gaubatz;Berk Sunar	2006		10.1007/11889700_18	arithmetic;finite field arithmetic;fault tolerance;discrete mathematics;mathematics;computer security;algorithm;statistics	Crypto	37.61257631516241	54.600826135065205	192970
5b993546112b71a385b92b5a505755f371ef0f8e	on chip signal generators for low overhead adc bist	adc test;bist;signal generator;inl	Testing of ADCs deeply embedded in SOCs is a significant challenge due to access limitations. ADC Built-in self-test (BIST) is considered a promising alternative to traditional test. This paper investigates implementation issues in adapting the stimulus error identification and removal (SEIR) algorithm, originally developed for production test, into a practical ADC BIST solution. Signal generators with very low transistor count and area consumption are presented. Extremely simple methods for generating small constant voltage level shifts are introduced and evaluated. Simulation results show that the generated signals, together with the level shifts, are able to test a 16-bit ADC to 16 bit accuracy levels. These results demonstrate that accurate BIST of deeply embedded analog and mixed-signal (AMS) blocks may be practically implemented on chip with very low overhead.	built-in self-test	Jingbo Duan;Bharath K. Vasan;Chen Zhao;Degang Chen;Randall L. Geiger	2012	J. Electronic Testing	10.1007/s10836-012-5320-5	embedded system;electronic engineering;real-time computing;engineering	Theory	26.463289561610527	52.64153795253556	193031
36ad17e27080ba59a141be78a6d917ad99b85bf0	correlation-based networks for implementaion in cmos analogue vlsi			cmos;very-large-scale integration	Jey E. E. Ngole	1995				EDA	29.544161009854285	47.37370837966864	193175
d9a0883506f4585a4377aeaf9152be8831e88ca5	subthreshold analog circuit for computing the maximum principal component of 3-d data	analog circuits analog computers vectors principal component analysis equations computer networks circuits and systems artificial neural networks laboratories circuit analysis computing;differential pairs subthreshold analog circuit maximal principal component three dimensional data power consumption mos operation;three dimensional;analog circuits;mos analogue integrated circuits;analogue processing circuits mos analogue integrated circuits;power consumption;analogue processing circuits;principal component	We present the analysis, design, and experimental results of a circuit that computes the maximal principal component of three dimensional data. To reduce power consumption, the implemented model uses circuit elements and differential pairs which operate in the subthreshold regime of MOS operation. >	analogue electronics	Shanti S. Vedula;Fathi M. A. Salam;Gamze Erten	1994		10.1109/ISCAS.1994.409603	mixed-signal integrated circuit;three-dimensional space;electronic engineering;analogue electronics;computer science;electrical engineering;theoretical computer science;machine learning;mathematics;principal component analysis	ML	26.428270547126118	47.194589577872215	193524
aa699a71917490773075fca40db8289dbf38db49	on greedy algorithms in coding theory	graph theory;quantization;decoding;source encoding;derandomization;greedy algorithms;testing;matrix algebra;indexing terms;hypergraphs;rate distortion theory;upper bound;automata;distortion;complexity reduction;coding theory;computational complexity;cryptography;linear code;greedy algorithm;graph theory matrix algebra computational complexity source coding;complexity reduction coding theory greedy algorithms incidence matrices hypergraphs covering codes conflict resolution separating systems source encoding distortion derandomization;separating systems;covering codes;conflict resolution;greedy algorithms rate distortion theory quantization testing automata cryptography linear code information theory upper bound decoding;incidence matrices;information theory;source coding	We study a wide class of problems in coding theory for which we consider two di erent formulations: in terms of incidence matrices and in terms of hypergraphs. These problems are dealt with using a greedy algorithm due to Stein and Lov asz. Some examples, including constructing covering codes, codes for con ict resolution, separating systems, source encoding with distortion, etc., are given a uni ed treatment. Under certain conditions derandomization can be performed, leading to an essential reduction in the complexity of the constructions.	code;coding theory;distortion;greedy algorithm;incidence matrix;naruto shippuden: clash of ninja revolution 3;randomized algorithm	Gérard D. Cohen;Simon Litsyn;Gilles Zémor	1996	IEEE Trans. Information Theory	10.1109/18.556707	combinatorics;greedy algorithm;discrete mathematics;information theory;computer science;graph theory;theoretical computer science;conflict resolution;mathematics;statistics	Theory	38.89391654116233	56.38115281668622	193625
6577d08e8ba02081d9c4ee1ba2eb7a9038006f9b	on infinite and finite lattice constellations for the additive white gaussian noise channel. (constellations finies et infinies de réseaux de points pour le canal awgn)		The problem of transmission of information over the AWGN channel using lattices is addressed. Firstly, infinite constellations are considered. A new family of integer lattices built by means of Construction A with non-binary linear codes is introduced. These lattices are called LDA (Low-Density Construction A) and are characterised by sparse p-ary parity-check matrices, that put them in direct relation with LDPC codes. Two results about the Poltyrev-capacity-achieving qualities of this family are proved, respectively for logarithmic row degree and constant row degree of the associated parity-check matrices. The second result is based on some expansion properties of the Tanner graphs related to these matrices. Another topic of this work concerns finite lattice constellations. A new proof that general random Construction A lattices achieve capacity under lattice decoding is provided, continuing and improving the work of Erez and Zamir (2004), Ordentlich and Erez (2012), and Ling and Belfiore (2013). This proof is based on Voronoi lattice constellations and MMSE scaling of the channel output. Finally, this approach is adapted to the LDA case and it is shown that LDA lattices achieve capacity with the same transmission scheme, too. Once again, it is necessary to exploit the expansion properties of the Tanner graphs. At the end of the dissertation, an iterative message-passing algorithm suitable for decoding LDA lattices in high dimensions is presented. Résumé On étudie le problème de la transmission de l’information à travers le canal AWGN en utilisant des réseaux. On commence par considérer des constellations infinies. Une nouvelle famille de réseaux obtenus par Construction A à partir de codes linéaires non binaires est proposée. Ces réseaux sont appelés LDA (« Low-Density Construction A ») et sont caractérisés par des matrices de parité p-aires creuses, qui les mettent en rélation directe avec les codes LDPC. Deux résultats sur leur possibilité d’atteindre la capacité de Poltyrev sont prouvés ; cela est d’abord démontré pour des poids des lignes logarithmiques des matrices de parité associées, puis pour des poids constants. Le deuxième résultat est basé sur certaines propriétés d’expansion des graphes de Tanner correspondants à ces matrices. Un autre sujet de ce travail concerne les constellations finies de réseaux. Une nouvelle preuve est donnée du fait que des réseaux aléatoires obtenus par Construction A générale atteignent la capacité avec décodage de type « lattice decoding ». Cela prolonge et améliore le travail de Erez et Zamir (2004), Ordentlich et Erez (2012), Ling et Belfiore (2013). Cette preuve est basée sur les constellations de Voronoï et la multiplication par le coefficient de Wiener (« MMSE scaling ») du signal en sortie du canal. Finalement, ce résultat est adapté au cas des réseaux LDA, qui eux aussi atteignent la capacité avec le même procédé de transmission. Encore une fois, il est nécessaire d’exploiter les propriétés d’expansion des graphes de Tanner. À la fin de la dissertation, on présente un algorithme de décodage itératif et de type « message-passing » approprié au décodage des LDA en grandes dimensions.		Nicola di Pietro	2014				Crypto	36.97617711253259	54.611569263207116	193815
1a0c69f67912949416bf2f893c4f586736431362	a framework for the analysis of non-deterministic clock synchronisation algorithms	non-deterministic clock synchronisation algorithms	In recent years, non-deterministic clock synchronisation algorithms (NDCSA) have appeared as an attractive alternative to deter-ministic ones. NDCSA ooer a precision that can be made as small as desired. The price to pay is a probability of success that is less than one. We propose an uniform analysis of NDCSA. Our approach strives at decomposing and identifying the factors that aaect the performance of these algorithms. Our aim is to determine simple local conditions that guarantee that the desired precision will be attained with the desired probability. The results are then used to estimate the communication burden imposed by the synchronisation algorithm and provide us with some guidelines to compare the diierent NDCSA.	algorithm;clock synchronization	Pedro Fonseca;Zoubir Mammeri	1996		10.1007/3-540-61769-8_11	matrix clock;digital clock manager	AI	31.928355970582334	49.697651705884475	194267
9be95ef43d73bea2f9c5cda289acc949cd70ba59	error masking in compact testing based on the hamming code and its modifications	error masking;signature analysis;compact testing;device under test;probability;hamming codes;invalid sequence;code words;masking probability;masking probability error masking compact testing hamming code invalid sequence probability distribution signature analysis code words arbitrary weight;probability hamming codes binary sequences logic testing;probability distribution;binary sequences;logic testing;test methods;hamming code;testing probability distribution cybernetics binary sequences floors;arbitrary weight	Probability that an invalid sequence at an output of a device under test is not detected(error masking) is the measure of the eflectiveness ofcompact testing methods. This paper evaluates and analyses the probability distribution of error masking for compact testing by exploiting the characteristics both of the Hamming code (i.e., the signature analysis) and of some modi$ed Hamming codes. To study the effectiveness of these methods, we derive also the analytical expressions for the number of code words of arbitrary weight. Finally, bounds for error masking probability are obtained.	code word;device under test;hamming code;unsharp masking	Serge N. Demidenko;Alexander Ivanyukovich;Leonid Makhist;Vincenzo Piuri	1995		10.1109/ATS.1995.485352	hamming weight;constant-weight code;hamming distance;hamming bound;theoretical computer science;cyclic code;linear code;hamming code;mathematics;forward error correction;hamming(7,4);algorithm;statistics	ML	38.74261814695358	57.44342911257652	194746
3f8d5fde0013c8b782da96e037c4f87ee87ac5ce	relations between the entropy of a source and the error masking probability for security-oriented codes	circuit faults;wires;random variables;entropy robustness circuit faults random variables security vectors wires;vectors;robustness;entropy;security;cryptographic devices error masking probability security oriented error detecting codes fault injection attacks;error statistics cryptography error detection codes	Security-oriented error-detecting codes are used to detect fault injection attacks on cryptographic devices. These codes are usually designed for uniformly distributed codewords, i.e., for codes that have maximal entropy. In practice, the codewords are not uniformly distributed; thus, their entropy is smaller and their efficiency in detecting attacks degrades. This paper analyzes the relation between the entropy of a code and its worst error masking probability. Based on this relation, a method for determining the rate and structure of a code that provides the required error masking probability is presented.	code (cryptography);code word;cryptography;entropy (information theory);fault injection;maximal set;sensor	Osnat Keren;Mark G. Karpovsky	2015	IEEE Transactions on Communications	10.1109/TCOMM.2014.2377151	random variable;entropy;computer science;entropy encoding;information security;theoretical computer science;mathematics;error floor;computer security;statistics;robustness	Vision	38.589447820785416	57.457193774422485	194860
9b88795b449f6c197ff6a36b6e2cee0de83d4baf	covering and secret sharing with linear codes	partage secret;code lineaire;algorithmique;mathematiques discretes;secret sharing;matematicas discretas;discrete mathematics;code recouvrement;covering code;real world application;algorithmics;coding theory;algoritmica;informatique theorique;linear code;exponential sum;real number;secret sharing scheme;nombre reel;computer theory;codigo lineal;informatica teorica	Secret sharing has been a subject of study for over twenty years, and has had a number of real-world applications. There are several approaches to the construction of secret sharing schemes. One of them is based on coding theory. In principle, every linear code can be used to construct secret sharing schemes. But determining the access structure is very hard as this requires the complete characterisation of the minimal codewords of the underlying linear code, which is a difficult problem. In this paper we present a sufficient condition under which we are able to determine all the minimal codewords of certain linear codes. The condition is derived using exponential sums. We then construct some linear codes whose covering structure can be determined, and use them to construct secret sharing schemes with interesting access structures.	access structure;code word;coding theory;computation;cryptography;discrete mathematics;information security;lecture notes in computer science;linear code;mceliece cryptosystem;pp (complexity);reed–solomon error correction;secret sharing;springer (tank);time complexity	Cunsheng Ding;Jin Yuan	2003		10.1007/3-540-45066-1_2	combinatorics;discrete mathematics;theoretical computer science;covering code;shamir's secret sharing;linear code;mathematics;homomorphic secret sharing;exponential sum;secure multi-party computation;secret sharing;algorithmics;verifiable secret sharing;algorithm;statistics;real number;coding theory;algebra	Crypto	37.217718509848496	53.5806670690139	194863
c0a3b4f7e0aca8ee7e1b6b8a8aecda765bb838e3	integrated hierarchical synthesis of analog/rf circuits with accurate performance mapping	mathematical model space exploration performance evaluation integrated circuit modeling equations optimization;integrated hierarchical rf circuit synthesis;radiofrequency circuit design;radiofrequency amplifiers;fine tuning;performance evaluation;integrated circuit;circuit design;distributed amplifiers;nanometer analog circuit design;trade off aspect identification;space exploration;integrated hierarchical analog circuit synthesis;performance mapping;hierarchical global to local search process;radiofrequency distributed amplifier integrated hierarchical analog circuit synthesis integrated hierarchical rf circuit synthesis performance mapping nanometer analog circuit design mixed signal circuit design radiofrequency circuit design circuit simulator nanometer technology nodes trade off aspect identification hierarchical global to local search process geometry biasing parameter circuit level parameter performance metrics fine tuning large scaled design synthesis;performance metric;radiofrequency distributed amplifier;integrated circuit design;circuit simulation;large scale;radio frequency;large scaled design synthesis;nanoelectronics;integrated circuit modeling;mathematical model;mixed analogue digital integrated circuits;optimal design;circuit simulator;analogue circuits;optimization;circuit level parameter;radiofrequency integrated circuits;nanometer technology nodes;radiofrequency integrated circuits analogue circuits circuit simulation distributed amplifiers integrated circuit design mixed analogue digital integrated circuits nanoelectronics radiofrequency amplifiers;mixed signal circuit design;performance metrics;local search;geometry biasing parameter	This work presents a synthesis framework for nanometer analog, mixed-signal, and radio-frequency circuit design. Our approach has both the advantages of accuracy and efficiency, accomplished by integrating both circuit simulator and analytic formulation. Through performance space exploration, this work facilitates optimal specification setting and trade-off aspect identification from nanometer technology nodes. The hierarchical global-to-local search process consists of device characterization, mapping from geometry-biasing parameters through circuit-level parameters to performance metrics, and reverse identification of optimal design varaibles with fine-tuning. The nature of hierarchy enables the capability of synthesizing large-scaled design with guarantee of accurate and fast convergence to the global optimum. Also this framework can be utilized to identify the constraints that are critically strict to the overall performance, such that the circuit can be designed to operate close to its limit. A radio-frequency distributed amplifier is synthesized as the demonstration showing that the proposed framework is effective and efficient.	biasing;circuit design;distributed amplifier;electronic circuit simulation;global optimization;local search (optimization);mixed-signal integrated circuit;optimal design;radio frequency	Kuo-Hsuan Meng;Po-Cheng Pan;Hung-Ming Chen	2011	2011 12th International Symposium on Quality Electronic Design	10.1109/ISQED.2011.5770817	nanoelectronics;control engineering;mathematical optimization;electronic engineering;computer science;engineering;electrical engineering;local search;optimal design;space exploration;integrated circuit;circuit design;mathematical model;electronic circuit simulation;fine-tuned universe;radio frequency;integrated circuit design	EDA	26.228601729634864	49.536338613215975	194893
1f9ba4f1bea34bf7e09bf70cfb410b424860001f	a novel design methodology for current reference circuits	design methodology us department of energy equations mathematical model solid modeling circuit simulation geometry polynomials temperature dependence voltage;temperature dependency factor;current reference circuits;supply voltage dependency factor current reference circuits design of experiment input electrical simulation polynomial equations temperature dependency factor;reference circuits;automatic generation;polynomials;temperature dependence;input electrical simulation;integrated circuit design;design of experiments;supply voltage dependency factor;computational modeling;integrated circuit modelling;chromium;integrated circuit modeling;mathematical model;polynomial equations;us department of energy;reference circuits design of experiments integrated circuit design integrated circuit modelling polynomials;design methodology;design of experiment	In this paper, design parameters of a current reference circuit are automatically generated according to specific design constraints. These constraints can be minimal consumption current or minimal temperature coefficients. This methodology is based on a current reference mathematical model. This model is generated thanks to a dasiaDesign Of Experimentpsila (DOE) technique. The DOE technique takes as input electrical simulation results of a current reference circuit for different component geometries. DOE generates polynomial equations of the current reference output IREF, the consumption current ISUNK, the temperature dependency factor betaT and the supply voltage dependency factor betaV. Using these equations and according to a given design specification (IREF, ISUNK, betaT or betaV), the most suitable design parameter values are generated.	coefficient;computer simulation;mathematical model;performance;polynomial;rc circuit;reference circuit;speedup	Hassen Aziza;Emmanuel Bergeret;Annie Pérez	2008	2008 15th IEEE International Conference on Electronics, Circuits and Systems	10.1109/ICECS.2008.4674835	control engineering;electronic engineering;computer science;electrical engineering	EDA	27.419722581148996	49.9036392302843	195584
3500fb08988f1b0fc6bdbce3aba93bc7085ccf22	fpga implementation of a re-configurable fft for multi-standard systems in software radio context	software;field programmable gate array;wireless local area network;multi standard systems;re configurable;software radio context fpga implementation field programmable gate array re configurable fast fourier transform multi standard systems;standards;fourier transform;reconfigurable architectures;multistandard system;reconfigurable fft;fpga stratix ii;field programmable gate arrays software radio galois fields fourier transforms arithmetic hardware performance gain frequency domain analysis computer architecture discrete fourier transforms;digital television;fft;fpga;software radio context;software radio;mobile phone;fast fourier transform;finite field;computer architecture;fpga implementation;galois finite field;re configurable fast fourier transform;software radio system;fourier transforms;re configurable fft fpga software radio;mobile communication;transforms;ofdm;fast fourier transforms;handheld device;field programmable gate arrays;software radio fast fourier transforms field programmable gate arrays galois fields reconfigurable architectures;field programmable gate arrays software radio fast fourier transforms fourier transforms galois fields hardware performance gain handheld computers mobile handsets communication standards;software radio fast fourier transforms field programmable gate arrays;multistandard system fpga implementation field programmable gate array reconfigurable fft fast fourier transform galois finite field software radio system fpga stratix ii;galois fields	This study is focused on the Field Programmable Gate Array (FPGA) implementation of a re-configurable Fast Fourier Transform (FFT) operator able to provide Fourier transforms both over complex infinite field X and Galois finite Field GF. This new re-configurable FFT exploits to a great advantage the possibility to share hardware resources when considering multi-standard scenarios for software radio systems. A re-configurable FFT of length N = 256 has been implemented on FPGA. It achieves a performance-to-cost ratio gain from 24% to 9.4% compared to the basic duplicated solution for which no re-configuration is considered. The proposed technology is strongly connected to further consumer handheld devices such as mobile phones intended to support several standards (digital television, mobile communications, wireless local area network etc) where FFT is involved.	carrier-to-noise ratio;code word;computation;etsi satellite digital radio;fast fourier transform;fermat;field-programmable gate array;grammatical framework;mobile device;mobile phone;reed–solomon error correction;strongly connected component;tensor operator	Ali Chamas Al Ghouwayel;Yves Louët	2009	IEEE Transactions on Consumer Electronics	10.1109/TCE.2009.5174479	embedded system;fast fourier transform;electronic engineering;computer hardware;telecommunications;computer science;field-programmable gate array	EDA	31.688642886341256	59.19147446863589	195858
33141c77d7fed33b8238b0ef5b4ffe0761cdfb58	study of regression methodologies on analog circuit design	gaussian processes;training;testing;training data;integrated circuit modeling;mathematical model;predictive models;model predicting error analog circuit design regression technique medium scale circuit modelling problem large scale circuit modelling problem gaussian process models minimum accuracy loss model building time model predicting time;testing integrated circuit modeling training data gaussian processes training predictive models mathematical model;regression analysis analogue integrated circuits gaussian processes integrated circuit modelling	This paper benchmarks accuracy and speed of many regression techniques on medium and large-scale circuit modelling problems, with particular emphasis on Gaussian Process Models. Regression models are widely used in electronics on parasitic modelling, verification, reliability analysis, optimization and layout generators. The main goal of using a regression model is to save simulation time with minimum accuracy loss. However, regression models exhibit different features and their performances can vary depending on the number of variables and training/test data. This paper tests different regression models for different setups and circuits. The test problems were grouped in medium and high dimension problems according with the number of variables. The model building time, model predicting time and model predicting error were compared with the actual data. Some constraints can be found for some regression techniques when are used on different problems and some of them change their performances in terms of time and/or error. The results summary and the performance comparatives can be a decision tool when it is planning to be used one of these techniques.	analogue electronics;circuit design;gaussian process;mathematical optimization;performance;simulation;test data	Ivick Guerra-Gómez;Trent McConaghy;Esteban Tlelo-Cuautle	2015	2015 16th Latin-American Test Symposium (LATS)	10.1109/LATW.2015.7102504	econometrics;computer science;machine learning;statistics	AI	24.61591807343001	58.77809393634819	196147
6df40dca0f0ef805e7e4134c81fcc4f8a125a559	incremental algorithms for digital simulation	digital circuit;simulation;circuit vlsi;simulacion;algorithme;circuit numerique;algorithm;vlsi circuit;circuito numerico;incremental algorithm;circuito vlsi;digital simulation;algoritmo	Incremental simulation implies a simulator that runs in time proportional to the size and implications of design changes instead of the size of the circuit under simulation. The incremental property of the hardware design process is exploited to reduce the number of component evaluations, thus to achieve run-time performance improvement. To accomplish the incremental simulation, the proposed algorithms maintain a history of the circuit state transitions during simulation run and resimulate only portions of the circuit, whose structure/behavior has been modified (incremental-in-space) or whose activities deviate from their history ( incremental-in -time). This paper describes and compares two incremental algorithms for digital simulation: the incremental-in-space and incremental-in-time algorithms.	approximation algorithm;dynamic problem (algorithms);run time (program lifecycle phase);simulation	Sun Young Hwang	1989	Integration	10.1016/0167-9260(89)90057-6	computer science;engineering;electrical engineering;digital electronics;algorithm	EDA	25.8643045410609	55.63504786617597	196212
9427fc78f41c9b273f241198e9657ff917afccd8	a comparative study of continuous sampling plans for functional board testing	sampling methods electronic equipment testing inspection continuous production usability stability assembly statistical analysis system testing product design;functional board testing;csp continuous sampling plans functional board testing multitest environment;continuous sampling plans;multitest environment;csp;printed circuit testing	This paper presents efficient methods that are suitable for sampling in a multi-test environment. The methods are based on established continuous sampling plan (CSP) procedures. The advanced approach presented improves the usability and efficiency of the plans and makes them an excellent alternative for application in functional board testing. The experimental results achieved with the advanced approach are compared to common CSP procedures in terms of efficiency and stability.	computational complexity theory;deployment environment;experiment;functional testing;sampling (signal processing);usability	Jukka Antila;Timo Karhu	2007	2007 IEEE International Test Conference	10.1109/TEST.2007.4437650	non-regression testing;reliability engineering;electronic engineering;computer science;engineering;communicating sequential processes;functional testing	Robotics	25.476067454493094	51.203965631531084	197082
4611e537a0bee7d8639417c55fa5960a63a9c8dc	harvesting design knowledge from the internet: high-dimensional performance tradeoff modeling for large-scale analog circuits	performance tradeoff;measurement;pareto front;small circuit blocks large scale analog circuits harvesting design knowledge internet high dimensional performance tradeoff modeling complex analog systems analog circuit blocks high dimensional pareto front analog to digital converter;pareto optimization;analog circuits;analog circuit;pareto optimisation analogue integrated circuits analogue digital conversion;pareto optimization measurement integrated circuit modeling mathematical model numerical models analog circuits;期刊论文;integrated circuit modeling;mathematical model;numerical models;performance trade off analog circuit pareto front	Efficiently optimizing large-scale, complex analog systems requires to know the performance tradeoffs for various analog circuit blocks. In this paper, we propose a radically new approach for analog performance tradeoff modeling. Our key idea is to broadly search the rich design knowledge from the Internet, and then mathematically encode the knowledge as high-dimensional performance tradeoff curves that are referred to as Pareto fronts in the literature. Toward this goal, several novel numerical algorithms, such as sparse regression and semi-infinite programming, are developed in order to construct the high-dimensional Pareto front model while guaranteeing its monotonicity. Our numerical examples demonstrate that the proposed modeling technique can accurately capture the high-dimensional Pareto fronts for large-scale analog systems (e.g., analog-to-digital converter) while most traditional methods are limited to low-dimensional Pareto front modeling of small circuit blocks without considering layout parasitics and manufacturing nonidealities.	algorithm;algorithmic efficiency;analog-to-digital converter;analogue electronics;anomaly detection;basis function;computation;discretization;encode;electronic component;heuristic (computer science);integrated circuit;internet;level design;mathematical optimization;microcontroller;numerical analysis;optimization problem;outliner;pareto efficiency;power electronics;semi-infinite programming;semiconductor industry;social inequality;sparse matrix;voltage regulator	Jun Tao;Changhai Liao;Xuan Zeng;Xin Li	2016	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2015.2449240	mixed-signal integrated circuit;mathematical optimization;electronic engineering;simulation;analogue electronics;computer science;mathematics;statistics	EDA	25.55899701958538	59.032397078214146	197647
c7af69186a4277b0dad5eefc5fd2c026f10ff46e	understanding complexity in multiphysics systems-on-a-chip: modern approaches for design	electronic systems design multiphysics systems on a chip modern microelectronics complex system;integrated circuit design;computational modeling;radio frequency;micromechanical devices;system on chip;mathematical model solid modeling integrated circuit modeling computational modeling micromechanical devices radio frequency numerical models;conference report;solid modeling;integrated circuit modeling;mathematical model;numerical models;system on chip integrated circuit design	The aim of this paper is to formalise the term “complexity” in the context of modern microelectronics, propose the definitions of key terms and discuss a case study. Our aim is to show that the term “complex system” is implicitly related to the design of electronic systems.	complex system;complexity;microelectromechanical systems;multiphysics;system on a chip	Elena Blokhina;Diarmuid O'Connell;Dennis Andrade-Miceli;Sergi Gorreta;Joan Pons-Nin;Manuel Domínguez Pumar;Orla Feely;Dimitri Galayko	2015	2015 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2015.7168941	system on a chip;control engineering;physical design;electronic engineering;telecommunications;computer science;engineering;electrical engineering;circuit design;mathematical model;solid modeling;computational model;radio frequency;statistics;integrated circuit design	Embedded	27.738663631109567	53.065174019010506	198468
651e58398e0e74cafab1e9854ea17a1fc1e1f3d5	impedance calculation based method for ac fault analysis of mixed-signal circuits	eigenvalues and eigenfunctions;impedance;complexity theory;circuit faults;symmetric matrices impedance circuit faults integrated circuit modeling eigenvalues and eigenfunctions sparse matrices complexity theory;mixed analogue digital integrated circuits fault simulation integrated circuit testing;symmetric matrices;circuit under test impedance calculation based method ac fault analysis mixed signal circuits fault simulation;integrated circuit modeling;sparse matrices	An alternative method of fault simulation is presented in this paper. The proposed method is based on impedance calculations in the circuit under test. Calculation time and other properties of the method are addressed and evaluated. Possible application and results evaluation are demonstrated on an experimental circuit. This method could improve the test development time and quality.	bridging (networking);characteristic impedance;iddq testing;information extraction;iterated conditional modes;mixed-signal integrated circuit;nominal impedance;simulation;speedup;test vector	Juraj Brenkus;Viera Stopjaková;Lukás Nagy;Daniel Arbet	2016	2016 IEEE 19th International Symposium on Design and Diagnostics of Electronic Circuits & Systems (DDECS)	10.1109/DDECS.2016.7482442	equivalent circuit;control engineering;electronic engineering;sparse matrix;computer science;theoretical computer science;automatic test pattern generation;electrical impedance;circuit extraction;discrete circuit;symmetric matrix	EDA	24.698397627659872	49.2119000402247	198588
942159da768a51fbae88cca8c015128337766df3	substrate noise analysis and experimental verification for the efficient noise prediction of a digital pll	phase locked loops circuit noise circuit simulation noise generators integrated circuit noise digital circuits circuit testing phase noise noise level computational modeling;digital phase locked loops;signal integrity;noise measurement;cmos digital integrated circuits;noise measurement digital phase locked loops cmos digital integrated circuits integrated circuit noise circuit analysis computing circuit cad uhf integrated circuits;uhf integrated circuits;90 nm substrate noise analysis noise prediction digital pll mixed signal integration cad tools digital circuits substrate noise measurements cmos process high resistivity substrate 480 mhz;circuit cad;digital circuits;integrated circuit noise;circuit analysis computing;substrate noise	Substrate noise is a major impediment to mixed-signal integration. This paper describes a CAD tool that can be used at any stage of the design cycle to estimate the substrate noise generated by large digital circuits. The results have been verified with substrate noise measurements on a 480 MHz digital PLL implemented in a 90 nm CMOS process on a high resistivity substrate.	cmos;computer-aided design;digital electronics;macromodel;mixed-signal integrated circuit;phase-locked loop;run time (program lifecycle phase);simulation	Nisha Checka;Anantha Chandrakasan;Rafael Reif	2005	Proceedings of the IEEE 2005 Custom Integrated Circuits Conference, 2005.	10.1109/CICC.2005.1568709	flicker noise;mixed-signal integrated circuit;embedded system;effective input noise temperature;substrate coupling;electronic engineering;signal integrity;engineering;noise measurement;electrical engineering;phase noise;digital electronics	EDA	24.712842287463772	54.71408987822456	198978
2822d4bef0ed8027b9c947b4e5b79c38c2a4fcd3	built-in clock skew system for on-line debug and repair	silicon;driver assistance;hardware acceleration;detectors;phase detection;time of flight;measurement error;integrated circuit;clocks;delay lines;clock skew management;bicss;bicss built in clock skew system on line debug on line repair clock skew management integrated circuits two step method central debug circuitry measurement error user adjustable margin variable tolerance phase detector silicon debug on chip clock skews;multiplexing;on line repair;phase detectors;chip;on chip clock skews;taillight detection;silicon debug;integrated circuit interconnections;built in clock skew system;central debug circuitry;delay circuits;logic testing;variable tolerance phase detector;on line debug;integrated circuit testing;clocks delay lines phase detection detectors integrated circuit interconnections integrated circuit measurements circuit testing silicon multiplexing added delay;real time video processing;circuit testing;phase detector;integrated circuit measurements;two step method;phase detectors clocks delay circuits integrated circuit testing logic testing measurement errors;clock skew;user adjustable margin;integrated circuits;added delay;measurement errors	We present a low-cost on-line system for clock skew management in integrated circuits. Our Built-In Clock Skew System (BICSS) uses a centralized approach to identify, quantify and correct skew using a two-step method. The technique assesses the time-of-flight between the central debug circuitry and each region, or tap under test to account for the measurement error due to differences in path length common in existing techniques. The system can be used to detect skew above a user-adjustable margin using a variable tolerance phase detector. The result is a solution which provides silicon debug and repair capability of on-chip clock skews with a very small area overhead.	canonical account;centralized computing;clock skew;debug;electronic circuit;integrated circuit;online and offline;overhead (computing);phase detector;on-line system	Atanu Chattopadhyay;Zeljko Zilic	2008	2008 Design, Automation and Test in Europe	10.1145/1403375.1403435	phase detector;embedded system;electronic engineering;real-time computing;telecommunications;clock domain crossing;clock skew;computer science;engineering;integrated circuit;timing failure;digital clock manager;observational error	EDA	25.418787019822453	53.58088134265626	199544
97c859ae313cbcda83af1dc2691968bfb575dcf5	quantification and round influence on fpga implemented algorithms for an ofdm radio interface	real time;mobile radio channel estimation field programmable gate arrays ofdm modulation;channel estimation fpga implemented algorithm ofdm radio interface radio channels fixed point algorithm;channel estimation;rounding errors;fixed point;mobile radio;ofdm modulation;field programmable gate arrays ofdm digital signal processing signal processing signal processing algorithms web pages application specific integrated circuits ethernet networks web server linux;field programmable gate arrays;algorithm design	OFDM is widely used as the physical radio layer in an increasingly high number of standards due to its ability to manage time and frequency SNR variations on the radio channels. First described by Chang in 1966, the high digital processing resources required for its implementation have delayed its commercialization until the 1990s. In order to speed up the algorithms implementations for real time transmission, the fixed point implementation is needed (based on ASIC, FPGA or DSP). As a drawback, fixed point implementation leads to quantification and round errors that needs to be taken into consideration in the algorithm design.	algorithm design;application-specific integrated circuit;digital data;digital signal processor;field-programmable gate array;fixed point (mathematics);signal-to-noise ratio	Juan Manuel Vazquez;Maria Belen Castan;Luis Miguel Campoy	2004	2004 IEEE 15th International Symposium on Personal, Indoor and Mobile Radio Communications (IEEE Cat. No.04TH8754)	10.1109/PIMRC.2004.1368777	embedded system;algorithm design;real-time computing;orthogonal frequency-division multiplexing;telecommunications;computer science;fixed point;field-programmable gate array	Arch	32.019503196002404	56.758920091096265	199552
