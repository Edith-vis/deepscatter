id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
026d8bc1b87411e2e2a529c627c73e265cee1dbd	maximizing a concave function over the efficient or weakly-efficient set	multiple criteria analysis;multicriteria analysis;multiobjective programming;programmation multiobjectif;non convex programming;nonconvex optimization;maximization;optimum global;programmation non convexe;multiple criteria;global optimum;programacion no convexa;branch and bound method;programacion lineal;metodo branch and bound;linear programming;programmation lineaire;linear program;global optimization;analisis multicriterio;methode separation et evaluation;analyse multicritere;ensemble solution efficace;branch and bound;optimo global;maximizacion;maximisation;programacion multiobjetivo	An important approach in multiple criteria linear programming is the optimization of some function over the ef®cient or weakly-ecient set. This is a very dicult nonconvex optimization problem, even for the case that the function to be optimized is linear. In this article we consider the problem of maximizing a concave function over the ecient or weakly-ecient set. We show that this problem can essentially be formulated as a special global optimization problem in the space of the extreme criteria of the underlying multiple criteria linear program. An algorithm of branch and bound type is proposed for solving the resulting problem. Ó 1999 Elsevier Science B.V. All rights reserved.	algorithm;branch and bound;concave function;convex optimization;global optimization;linear programming;mathematical optimization;optimization problem	Reiner Horst;Nguyen V. Thoai	1999	European Journal of Operational Research	10.1016/S0377-2217(98)00230-6	optimization problem;mathematical optimization;combinatorics;linear-fractional programming;nonlinear programming;linear programming;cutting stock problem;multi-objective optimization;mathematics;active set method;global optimum;mathematical economics;branch and bound;global optimization	ML	25.448221067368372	10.114680018818344	47883
920cd22f598243f13bfe7b80d748bd700b93ac03	comments on a lower bound for convex hull determination	convex hull;lower bound		convex hull	David Avis	1980	Inf. Process. Lett.	10.1016/0020-0190(80)90125-8	mathematical optimization;convex combination;computer science;convex hull;mathematics;upper and lower bounds	DB	27.369519595005258	17.11497254685203	47995
a06419da9e4010129c1af496a3ae6e8235d7dfb3	a learning optimization algorithm in graph theory - versatile search for extremal graphs using a learning algorithm	learning algorithm;extremal graphs;combinatorial optimization	Using a heuristic optimization module based upon Variable Neighborhood Search (VNS), the system AutoGraphiX’s main feature is to find extremal or near extremal graphs, i.e., graphs that minimize or maximize an invariant. From the so obtained graphs, conjectures are found either automatically or interactively. Most of the features of the system relies on the optimization that must be efficient but the variety of problems handled by the system makes the tuning of the optimizer difficult to achieve. We propose a learning algorithm that is trained during the optimization of the problem and provides better results than all the algorithms previously used for that purpose.	aac-ld;algorithm;experiment;graph theory;heuristic;interactivity;mathematical optimization;variable neighborhood search	Gilles Caporossi;Pierre Hansen	2012		10.1007/978-3-642-34413-8_2	extremal optimization;mathematical optimization;combinatorics;combinatorial optimization;computer science;machine learning;mathematics;indifference graph	ML	24.760191457784295	5.956948815834042	48294
277be7169273938a751f6431bfb3ff20b67b454d	a package for exact kinetic data structures and sweepline algorithms	numerical stability;biological patents;biomedical journals;delaunay triangulation;three dimensions;text mining;europe pubmed central;two dimensions;citation search;citation networks;research articles;abstracts;open access;life sciences;clinical guidelines;full text;data structure;kinetic data structure;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	In this paper we present a package for implementing exact kinetic data structures built on objects which move along polynomial trajectories. We discuss how the package design was influenced by various considerations, including extensibility, support for multiple kinetic data structures, access to existing data structures and algorithms in CGAL, as well as debugging. Due to the similarity between the operations involved, the software can also be used to compute arrangements of polynomial objects using a sweepline approach. The package consists of three main parts, the kinetic data structure framework support code, an algebraic kernel which implements the set of algebraic operations required for kinetic data structure processing, and kinetic data structures for Delaunay triangulations in one and two dimensions, and Delaunay and regular triangulations in three dimensions. The models provided for the algebraic kernel support both exact operations and inexact approximations with heuristics to improve numerical stability.	algebraic equation;approximation;cgal;debugging;delaunay triangulation;dimensions;extensibility;hl7publishingsubsection <operations>;heuristics;kernel (operating system);kinetic data structure;kinetics;linear algebra;numerical stability;physical object;point set triangulation;polynomial;sweep line algorithm	Daniel Russel;Menelaos I. Karavelas;Leonidas J. Guibas	2007	Computational geometry : theory and applications	10.1016/j.comgeo.2006.11.006	three-dimensional space;combinatorics;text mining;two-dimensional space;delaunay triangulation;data structure;computer science;data science;theoretical computer science;data mining;mathematics;geometry;numerical stability;algorithm	Theory	34.346855953387646	16.478491985539414	48405
b593419daab2c8997bcca7f0a295ac76a89cf56f	the weight of the shortest path tree	central limit theorem;complete graph;shortest path tree;gaussian distribution	The minimal weight of the shortest path tree in a complete graph with independent and exponential (mean 1) random link weights, is shown to converge to a Gaussian distribution. We prove a conditional central limit theorem and show that the condition holds with probability converging to 1.	automated theorem proving;converge;shortest path problem;time complexity	Remco van der Hofstad;Gerard Hooghiemstra;Piet Van Mieghem	2007	Random Struct. Algorithms	10.1002/rsa.20141	normal distribution;random graph;mathematical optimization;combinatorics;discrete mathematics;widest path problem;constrained shortest path first;central limit theorem;euclidean shortest path;yen's algorithm;mathematics;shortest path problem;distance;complete graph;k shortest path routing;shortest path faster algorithm;statistics;shortest-path tree	Theory	38.934942975480205	15.39361745061461	48541
c56988896d177d13e638462c6ac3dc32748c4717	improving local search in a minimum vertex cover solver for classes of networks		For the minimum vertex cover problem, a wide range of solvers has been proposed over the years. Most classical exact approaches are encountering run time issues on massive graphs that are considered nowadays. A straightforward alternative approach is then to use heuristics, which make assumptions about the structure of the studied graphs. These assumptions are typically hard-coded and are hoped to work well for a wide range of networks—which is in conflict with the nature of broad benchmark sets. With this article, we contribute in two ways. First, we identify a component in an existing solver that influences its performance depending on the class of graphs, and we then customize instances of this solver for different classes of graphs. Second, we create the first algorithm portfolio for the minimum vertex cover to further improve the performance of a single integrated approach to the minimum vertex cover problem.	algorithm;benchmark (computing);boolean satisfiability problem;codec;combinatorial optimization;experiment;feature vector;graph (discrete mathematics);hard coding;heuristic (computer science);local search (optimization);mathematical optimization;optimization problem;run time (program lifecycle phase);solver;travelling salesman problem;vertex cover	Markus Wagner;Tobias Friedrich;Marius Lindauer	2017	2017 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2017.7969507	mathematical optimization;machine learning;artificial intelligence;approximation algorithm;local search (optimization);vertex cover;heuristics;algorithm design;benchmark (computing);solver;portfolio;mathematics	AI	25.367217776216634	5.939508626175954	48786
6f9b01bde827fbe314bab62314f50298144a42ca	computational investigations of the optimality of two- and three-dimensional triangulations under several criteria.	three dimensional			Akira Tajima;Hiroshi Imai	1998			three-dimensional space;mathematics;geometry	Theory	33.54099537703123	17.17766897825871	49086
7e516c513a1646d49dd78655fbb200af353ec374	efficient approximation of labeling problems with applications to immune repertoire analysis	measurement;cost function;approximation algorithms;distortion;linear programming;labeling	Labeling problems are finding increasing applications to optimization problems. They usually get realized into linear or quadratic optimization problems, which are inefficient for large graphs. In this paper we propose an efficient primal-dual solution, MLPD, for a family of labeling problems. We apply this algorithm to the analysis of immune repertoires, and compare it against our baseline approach based on refinement operators. We provide a comparative evaluation both in terms of accuracy and computational efficiency with respect to the baseline model, as well as to quadratic optimization.	algorithm;approximation;approximation algorithm;baseline (configuration management);computation;mathematical optimization;optimization problem;refinement (computing)	Yusuf Osmanlioglu;Santiago Ontañón;Uri Hershberg;Ali Shokoufandeh	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7899997	mathematical optimization;labeling theory;combinatorics;distortion;computer science;linear programming;mathematics;mathematical economics;l-reduction;measurement	Robotics	32.314147541595254	7.2211205812281465	49113
4c5d4d7aa3d1190927782aab73f413e378fae348	fast smallest-enclosing-ball computation in high dimensions	quadratic programming;quadratic program;bola;high dimensionality;espace euclidien;programmation quadratique;combinatorial algorithm;geometrie algorithmique;system with n degrees of freedom;implementation;cyclage;cycling;computational geometry;simplex algorithm;corps flottant;espacio euclidiano;ball;floating body;programacion lineal;ciclaje;cuerpo flotante;dynamic data structure;systeme n degres liberte;estructura datos;linear programming;programmation lineaire;linear program;euclidean space;bille;programacion cuadratica;geometria computacional;structure donnee;floating point;implementacion;sistema n grados libertad;high dimension;off the shelf;data structure	We develop a simple combinatorial algorithm for computing the smallest enclosing ball of a set of points in high dimensional Euclidean space. The resulting code is in most cases faster (sometimes significantly) than recent dedicated methods that only deliver approximate results, and it beats off-the-shelf solutions, based e.g. on quadratic programming solvers. The algorithm resembles the simplex algorithm for linear programming; it comes with a Bland-type rule to avoid cycling in presence of degeneracies and it typically requires very few iterations. We provide a fast and robust floating-point implementation whose efficiency is based on a new dynamic data structure for maintaining intermediate solutions. The code can efficiently handle point sets in dimensions up to 2,000, and it solves instances of dimension 10,000 within hours. In low dimensions, the algorithm can keep up with the fastest computational geometry codes that are available.	approximation algorithm;bounding sphere;code;combinatorial optimization;computation;computational geometry;data structure;degenerate energy levels;dynamic data;fastest;iteration;linear programming;quadratic programming;simplex algorithm;type rule	Kaspar Fischer;Bernd Gärtner;Martin Kutz	2003		10.1007/978-3-540-39658-1_57	mathematical optimization;combinatorics;computer science;floating point;linear programming;euclidean space;mathematics;geometry;ball;cycling;implementation;simplex algorithm;quadratic programming;algorithm	Theory	28.581296350236677	14.675831227052575	49219
5812d8a9355753010dfbbaee32dee9c45e23d5a0	fast algorithms for collision and proximity problems involving moving geometric objects	moving object;fast algorithm	Consider a set of geometric objects, such as points, line segments, or axesparallel hyperrectangles in IR, that move with constant but possibly different velocities along linear trajectories. Efficient algorithms are presented for several problems defined on such objects, such as determining whether any two objects ever collide and computing the minimum inter-point separation or minimum diameter that ever occurs. In particular, two open problems from the literature are solved: Deciding in o(n2) time if there is a collision in a set of n moving points in IR, where the points move at constant but possibly different velocities, and the analogous problem for detecting a red-blue collision between sets of red and blue moving points. The strategy used involves reducing the given problem on moving objects to a different problem on a set of static objects, and then solving the latter problem using techniques based on sweeping, orthogonal range searching, simplex composition, and parametric search.	fast fourier transform;geometric median;parametric search;pokémon red;proximity problems;range searching;range tree;sensor;simplex algorithm	Prosenjit Gupta;Ravi Janardan;Michiel H. M. Smid	1994		10.1007/BFb0049415	computer vision;mathematical optimization;computer science;distributed computing	Theory	30.805086109190768	17.79900996759874	49527
5cd91e81099cece192824f5db325870a04dd85fd	on the average number of steps of the simplex method of linear programming	complexity theory;simplex method;linear program;linear complementarity problem;path following	The goal is to give some theoretical explanation for the efficiency of the simplex method of George Dantzig. Fixing the number of constraints and using Dantzig's self-dual parametric algorithm, we show that the number of pivots required to solve a linear programming problem grows in proportion to the number of variables on the average.	linear programming;simplex algorithm	Stephen Smale	1983	Math. Program.	10.1007/BF02591902	big m method;mathematical optimization;combinatorics;discrete mathematics;criss-cross algorithm;linear-fractional programming;linear programming;dantzig–wolfe decomposition;klee–minty cube;mathematics;revised simplex method;linear complementarity problem;simplex algorithm	Theory	25.239422577496377	13.278349291590905	49552
57b83d65d64cf8e1be6aeb635634a1b7201e17b0	increasing hamiltonian paths in random edge orderings	random graphs;edge labeling;algorithms;monotone subsequences;hamiltonian paths	Let f be an edge ordering of Kn: a bijection . For an edge , we call f(e) the label of e. An increasing path in Kn is a simple path (visiting each vertex at most once) such that the label on each edge is greater than the label on the previous edge. We let S(f) be the number of edges in the longest increasing path. Chvatal and Komlos raised the question of estimating m(n): the minimum value of S(f) over all orderings f of Kn. The best known bounds on m(n) are , due respectively to Graham and Kleitman, and to Calderbank, Chung, and Sturtevant. Although the problem is natural, it has seen essentially no progress for three decades.rnrnrnrnIn this paper, we consider the average case, when the ordering is chosen uniformly at random. We discover the surprising result that in the random setting, S(f) often takes its maximum possible value of n – 1 (visiting all of the vertices with an increasing Hamiltonian path). We prove that this occurs with probability at least about 1/ e. We also prove that with probability 1- o(1), there is an increasing path of length at least 0.85 n, suggesting that this Hamiltonian (or near-Hamiltonian) phenomenon may hold asymptotically almost surely. © 2015 Wiley Periodicals, Inc. Random Struct. Alg., 2015		Mikhail Lavrov;Po-Shen Loh	2016	Random Struct. Algorithms	10.1002/rsa.20592	random graph;mathematical optimization;combinatorics;discrete mathematics;mathematics;algorithm	Theory	37.220624659420224	16.87643969834481	49700
0c6dbe471240d9ed6d30d79ce496d032c8acb1ec	1-segment center problems	analysis of algorithms computational complexity;geometric facility location;computational geometry;facilities equipment planning;discrete facility location;computational geometry geometric location;minimax optimization	We consider a minimax facility location problem for n points such that the facility is a line segment of a given length. The general problem can be solved by case analysis in O(n4 log n) time. When the orientation of the segment is fixed, the problem is shown to be linear-time solvable by using the prune-and-search technique. Other variations of this problem are also discussed. INFORMS Journal on Computing, ISSN 1091-9856, was published as ORSA Journal on Computing from 1989 to 1995 under ISSN 0899-1499.		Hiroshi Imai;D. T. Lee;Chung-Do Yang	1992	INFORMS Journal on Computing	10.1287/ijoc.4.4.426	mathematical optimization;combinatorics;computational geometry;computer science;facility location problem;mathematics;geometry;1-center problem	Theory	28.135622790227032	17.384053034395986	49748
eedf8c3e90f03c4cb87bb7b991b63c26595fb75f	exploiting decomposability using recombination in genetic algorithms: an exploratory discussion	traveling salesman problem;graph coloring;search based software engineering;generalized partition crossover;automatic bug repair	On certain classes of problems, recombination is more effective if the parents that are being recombined share common subsolutions. These common subsolutions can be used to decompose the recombination space into linearly independent subproblems. If a problem can be decomposed into k subproblems, a single greedy recombination can select the best of 2 possible offspring. The idea of exploiting decomposability works well for the Traveling Salesman Problem, and appears to be applicable to other problems such as Graph Coloring. For Search Based Software Engineering, these ideas might be useful, for example, when applying Genetic Programming to fix software bugs in large programs. Another way in which we might achieve decomposability is by exploiting program modularity and reoccurring program patterns.	approximation algorithm;crossover (genetic algorithm);display resolution;exploratory testing;genetic algorithm;genetic programming;graph coloring;greedy algorithm;heuristic;mined;modular programming;search-based software engineering;software bug;travelling salesman problem	L. Darrell Whitley	2011		10.1007/978-3-642-23716-4_2	mathematical optimization;combinatorics;search-based software engineering;artificial intelligence;machine learning;graph coloring;mathematics;travelling salesman problem;algorithm	SE	25.845094208433654	4.649802756009944	49868
47680b362e6f53637ac8a7a428b706d405a86ab4	ultra-fast skeleton based on an isotropic fully parallel algorithm	analisis imagen;algoritmo paralelo;parallel algorithm;image processing;geometrie algorithmique;algoritmo borroso;computational geometry;procesamiento imagen;geometric feature;traitement image;algorithme parallele;boolean operation;fuzzy algorithm;pattern recognition;algorithme flou;image analysis;geometria computacional;reconnaissance forme;reconocimiento patron;analyse image	In this paper we introduce a new thinning algorithm, called MB, which is optimized with respect to the total number of elementary Boolean operators needed to perform it. We rst emphasize the sound foundations of the algorithm, which is built by expressing into the Boolean language the three following constraints: (1) homotopy, (2) median axis and (3) isotropy. The MB algorithm beneets from both novel algorithmic ideas and systematic logic minimization. By hunting down any redundancy in the expressions of topological/geometrical features , we achieve a procedure that is: rstly, dramatically low-cost, as it is completely computed in 18 Boolean binary operators per iteration, and secondly, fully parallel, or one-single-pass, which guarantees that the number of iterations equals half the biggest object thickness.	apache axis;circuit minimization for boolean functions;iteration;logical connective;parallel algorithm;thickness (graph theory);thinning	Antoine Manzanera;Thierry M. Bernard;Françoise J. Prêteux;Bernard Longuet	1999		10.1007/3-540-49126-0_24	computer vision;image analysis;image processing;computational geometry;computer science;artificial intelligence;mathematics;geometry;parallel algorithm;algorithm	EDA	31.74274252076675	15.725580358336048	50197
b4f878bdf803a6f0b5e95659f74477a2279bc576	a projection framework for near-potential games	game theory convex programming;potential game;closed form solution;game theory;nash equilibrium;convex programming;geometry;convex optimization;convex functions;convex optimization projection framework near potential game static analysis dynamic analysis arbitrary strategic form finite game exact potential game weighted potential game;games;optimization;approximation methods;static and dynamic analysis;games approximation methods optimization closed form solution convex functions nash equilibrium geometry	Potential games are a special class of games that admit tractable static and dynamic analysis. Intuitively, games that are “close” to a potential game should enjoy somewhat similar properties. This paper formalizes and develops this idea, by introducing a systematic framework for finding potential games that are close to a given arbitrary strategic-form finite game. We show that the sets of exact and weighted potential games (with fixed weights) are subspaces of the space of games, and that for a given game, the closest potential game in these subspaces (possibly subject to additional constraints) can be found using convex optimization. We provide closed-form solutions for the closest potential game in these subspaces, and extend our framework to more general classes of games. We further investigate and quantify to what extent the static and dynamic features of potential games extend to “near-potential” games. In particular, we show that for a given strategic-form game, we can characterize the approximate equilibria and the sets to which better-response dynamics converges, as a function of the distance of the game to its potential approximation.	approximation algorithm;cobham's thesis;convex optimization;dynamic program analysis;mathematical optimization;ordinal data	Ozan Candogan;Asuman E. Ozdaglar;Pablo A. Parrilo	2010	49th IEEE Conference on Decision and Control (CDC)	10.1109/CDC.2010.5718130	bondareva–shapley theorem;combinatorial game theory;convex function;bayesian game;games;minimax;closed-form expression;mathematical optimization;combinatorics;example of a game without a value;convex optimization;best response;repeated game;mathematics;normal-form game;mathematical economics;sequential game;symmetric game;nash equilibrium	ECom	34.816142023823296	5.818729039776065	50228
a3f4ce5373af0fd43e21c855555edc286b095b39	parallelization of a reservoir simulator	satisfiability;reservoir simulation;parallel computer	Numerical reservoir simulation demands very high computing performance. Vectorization has been widely used as a performance enhancing technique. Parallel computing can also be used to satisfy the increasing requirements of more sophisticated models. This paper presents the implementation of a novel method of parallelizing an already vectorized reservoir simulator, which has the significant advantage of reducing not only the elapsed CPU-time, but also the total CPU-time used. The method relies on the physical independence of isolated geological structures, coupled through the wells, a feature present in many of the hydrocarbon reservoirs found in the North Sea. The vectorized and parallelized code has already been in production a period of time. Results are shown for a realistic case. Introduction There are strong economical incentives for reservoir simulation. Small increases of the recovery of the proven reserves in, for example, the Norwegian sector of the North Sea are worth millions of dollars at the current prices. Grid sizes for full field simulations may be several tens of thousands of cells, and large computer resources are used in pursuing optimal recovery strategies. Statoil has vectorized the reservoir simulator currently in use in the IBM 3090-200 with vector facility ~ (later on called 3090-200/VF) with satisfying speed-up and reduction of CPU-times, However, for some of the largest fields, further reductions are required to decrease turnaround times. Most of the CPU-time used is bound to the solution of the large sparse systems of linear equations arising at each time step. Parallelization of the algorithms used in solving these equations is, at the present time, still an inmature field. Several of the techniques proposed, even at high levels of sophistications 2, actually increase the CPU-time involved, though reducing the total elapsed time. This involves, in any realistic accounting system, higher prices of execution and therefore a deterrence to more detailed simulations. It is our feeling that in the current state of the research efforts, the focus for industrial applications has to be concentrated on the possibilities given by the natural independence of some of the physical processes involved. In our case, an opportunity is given by the fact that in the North Sea, and in several other oil producing areas, some hydrocarbon reservoirs consist of isolated geological structures which are coupled through the offshore platforms or through clusters of wells, and through the production control which has to be provided at this level for complying with sale contracts or optimal exploitation. Many of the reservoirs where Statoil has interests fall on this category. The basic idea behind our method is to solve the set of linear equations arising at each Newton, or non-linear, iteration independently for each isolated structure, and couple the results only at the end of the successful convergence of each of the linear iterations. This approach has several advantages.	algorithm;automatic vectorization;central processing unit;ibm 3090;iteration;linear equation;newton;nonlinear system;parallel computing;requirement;simulation;sparse matrix;system of linear equations;vector processor	Terje Kårstad;Adolfo Henriquez;Knut Korsell	1987		10.1007/3-540-18991-2_50	computational science;parallel computing;computer science;theoretical computer science;satisfiability	HPC	32.12957539855817	9.063389474171569	50342
064dbb897c626235d2ab9d24d81b41ca50be2a32	a decomposition technique in integer linear programming	block diagonalization;integer linear program	In this work, using the group theoretical approach we point out so me conditions on the B−1N matrix, often verified in practice, that make it possible to transform the system of linear congruences (constraints of problem 2) in a block diagonal form. In some cases, using this proce dure, the number of constraints can increase with respect to the number of constraints of problem 2. However, the problem can be solved indipen dently for the variables associated with each block.	integer programming;linear programming	S. Giulianelli;M. Lucertini	1975		10.1007/3-540-07623-9_282	table of gaussian integer factorizations;mathematical optimization;integer programming;linear-fractional programming;unimodular matrix;linear programming relaxation;branch and price;orthogonal diagonalization;branch and cut;cutting-plane method	EDA	25.815702446338268	12.82077991494607	50632
2c74f3a2fac5b6e1af26b0b231c24c15b54f90be	rubber band algorithm for estimating the length of digitized space-curves	image processing;minimum length polygonal curve rubber band algorithm digitized space curves length estimation digital curves 3d orthogonal grid polyhedrally bounded sets digitized arcs three dimensional euclidean space 3d euclidean space;computational geometry;iterative algorithm;three dimensional;computational complexity;image processing computational complexity computational geometry;euclidean space;rubber computer science image analysis data analysis skeleton time measurement joints	We consider simple digital curves in a 3D orthogonal grid as special polyhedrally bounded sets. These digital curves model digitized curves or arcs in three-dimensional euclidean space. The length of such a simple digital curve is defined to be the length of the minimum-length polygonal curve fully contained and complete in the tube of this digita l curve. So far no algorithm was known for the calculation of such a shortest polygonal curve. This paper provides an iterative algorithmic solution, including a presentationf its foundations and of experimental results.	arcs (computing);algorithm;apache axis;approximation;cube 2: sauerbraten;iteration;iterative method;memory-level parallelism;olap cube;provable security;time complexity;xfig	Thomas Bülow;Reinhard Klette	2000		10.1109/ICPR.2000.903604	three-dimensional space;mathematical optimization;combinatorics;image processing;computational geometry;euclidean space;mathematics;geometry;curve;iterative method;computational complexity theory;digital geometry	Theory	32.61162468130871	17.484424174821605	50637
3cf953015777da6fe797525652533961d0b67a49	near-linear algorithms for geometric hitting sets and set covers	approximation algorithms;hitting set;geometric range space;multiplicative weight method;set cover	Given a finite range space Σ = (X, R), with N = |X| + |R|, we present two simple algorithms, based on the multiplicative-weight method, for computing a small-size hitting set or set cover of Σ. The first algorithm is a simpler variant of the Brönnimann-Goodrich algorithm but more efficient to implement, and the second algorithm can be viewed as solving a two-player zero-sum game. These algorithms, in conjunction with some standard geometric data structures, lead to near-linear algorithms for computing a small-size hitting set or set cover for a number of geometric range spaces. For example, they lead to O(N polylog(N)) expected-time randomized O(1)-approximation algorithms for both hitting set and set cover if X is a set of points and ℜ a set of disks in R2.	data structure;michael t. goodrich;randomized algorithm;set cover problem	Pankaj K. Agarwal;Jiangwei Pan	2014		10.1145/2582112.2582152	mathematical optimization;combinatorics;discrete mathematics;solution set;mathematics;set cover problem;index set;k-approximation of k-hitting set;hitting time;approximation algorithm;set function;infinite set	Theory	24.714195216085304	18.20811895521027	51402
71af84eb7ad167775c97cb9c33f74497110e9387	suboptimality bounds for stochastic shortest path problems		We consider how to use the Bellman residual of the dynamic programming operator to compute suboptimality bounds for solutions to stochastic shortest path problems. Such bounds have been previously established only in the special case that “all policies are proper,” in which case the dynamic programming operator is known to be a contraction, and have been shown to be easily computable only in the more limited special case of discounting. Under the condition that transition costs are positive, we show that suboptimality bounds can be easily computed even when not all policies are proper. In the general case when there are no restrictions on transition costs, the analysis is more complex. But we present preliminary results that show such bounds are possible.	bellman equation;bounds checking;computable function;dynamic programming;greedy algorithm;iteration;markov decision process;shortest path problem;stochastic gradient descent	Eric A. Hansen	2011			mathematical optimization;combinatorics;mathematics;mathematical economics;no-arbitrage bounds	ML	36.892960420383915	5.128550154869413	51614
38f7785c4d1843648670cb98fe5fb09d0466da0a	discretized approximations for pomdp with average cost	value iteration;upper bound;discrete approximation;average cost;computational efficiency;lower bound	In this paper, we propose a new lower approximation scheme for POMDP with discounted and average cost criterion. The approximating functions are determined by their values at a finite number of belief points, and can be computed efficiently using value iteration algorithms for finite-state MDP. While for discounted problems several lower approximation schemes have been proposed earlier, ours seems the first of its kind for average cost problems. We focus primarily on the average cost case, and we show that the corresponding approximation can be computed efficiently using multi-chain algorithms for finite-state MDP. We give a preliminary analysis showing that regardless of the existence of the optimal average cost J in the POMDP, the approximation obtained is a lower bound of the liminf optimal average cost function, and can also be used to calculate an upper bound on the limsup optimal average cost function, as well as bounds on the cost of executing the stationary policy associated with the approximation. We show the convergence of the cost approximation, when the optimal average cost is constant and the optimal differential cost is continuous.	approximation algorithm;cost efficiency;discretization;ibm notes;iteration;leslie speaker;loss function;partially observable markov decision process;stationary process	Huizhen Yu;Dimitri P. Bertsekas	2004			mathematical optimization;combinatorics;computer science;mathematics;mathematical economics;upper and lower bounds	Theory	37.13102547832885	5.199059197166868	52010
32797a3062c5997835cdf443a72341bb823ff701	practical methods for approximate geometric pattern matching under rigid motions (preliminary version)	dynamic programming;branching surfaces;surface fitting;geometric hashing;tiling;slice interpolation;surface reconstruction;pattern matching;triangulation;curve matching;branch and bound;polyhedra	We present practical methods for approximate geometric pattern matching in d-dimensions along with experimental data regarding the quality of matches and running times of these methods versus those of a branch-and-bound search. Our methods are faster than previous methods but still produce good matches.	approximation algorithm;branch and bound;pattern matching	Michael T. Goodrich;Joseph S. B. Mitchell;Mark W. Orletsky	1994		10.1145/177424.177572	mathematical optimization;combinatorics;surface reconstruction;triangulation;dynamic programming;pattern matching;mathematics;geometry;branch and bound;polyhedron	Graphics	31.684429508700628	17.501320244638137	52504
617a1609af47651c3fc1dbb7d4d75bb17831fb8b	incorporating sat solvers into hierarchical clustering algorithms: an efficient and flexible approach	hierarchical clustering;constrained clustering;constraint satisfaction;clustering;declarative languages;polynomial time;sat solver	The area of constrained clustering has been actively pursued for the last decade. A more recent extension that will be the focus of this paper is constrained hierarchical clustering which allows building user-constrained dendrograms/trees. Like all forms of constrained clustering, previous work on hierarchical constrained clustering uses simple constraints that are typically implemented in a procedural language. However, there exists mature results and packages in the fields of constraint satisfaction languages and solvers that the constrained clustering field has yet to explore. This work marks the first steps towards introducing constraints satisfaction languages/solvers into hierarchical constrained clustering. We make several significant contributions. We show how many existing and new constraints for hierarchical clustering, can be modeled as a Horn-SAT problem that is easily solvable in polynomial time and which allows their implementation in any number of declarative languages or efficient solvers. We implement our own solver for efficiency reasons. We then show how to formulate constrained hierarchical clustering in a flexible manner so that any number of algorithms, whose output is a dendrogram, can make use of the constraints.	algorithm;boolean satisfiability problem;cluster analysis;constrained clustering;constraint satisfaction;decision problem;dendrogram;hierarchical clustering;horn clause;procedural programming;solver;time complexity	Sean Gilpin;Ian Davidson	2011		10.1145/2020408.2020585	time complexity;correlation clustering;constrained clustering;mathematical optimization;constraint satisfaction;fuzzy clustering;computer science;theoretical computer science;canopy clustering algorithm;machine learning;hierarchical network model;consensus clustering;mathematics;hierarchical clustering;cluster analysis;boolean satisfiability problem;brown clustering;biclustering;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	ML	24.7652502253181	10.615527087349156	53405
c3897bc9723acd9a2b6f709a4614d514a5261b1d	an algorithm for non-linear multi-level integer programming problems	nonlinear programming;satisfactory solutions;fractional programming;integer programming;multi level programming;indefinite quadratic programming	In this paper, an algorithm is proposed to solve a tri-level integer programming problem in which the objective function for the first level is an indefinite quadratic, the second one is linear and the third one is linear fractional. The feasible space of the decision variable is reduced at each level until a satisfactory point is obtained at the last level. The higher level decision-maker reduces the feasible space for the lower level decision maker to search for his optimum. A satisfactory solution of the bilevel decentralised programming problem can also be obtained by the method proposed above. This method is illustrated with the help of examples.	algorithm;integer programming;nonlinear system	Ritu Arora;S. R. Arora	2010	IJCSM	10.1504/IJCSM.2010.037445	fractional programming;mathematical optimization;constraint programming;combinatorics;discrete mathematics;basic solution;integer programming;constraint satisfaction;criss-cross algorithm;linear-fractional programming;nonlinear programming;branch and price;goal programming;mathematics;sequential quadratic programming;active set method;quadratic programming	EDA	25.94573103410123	9.678547395609668	53406
610cb066f46d7a783a0f801a9ce7286fbcce52df	relaxation methods for linear programs	sign pattern;metodo relajacion;painted index algorithm;epsilon complementary slackness;elementary vectors;methode relaxation;tk7855 m41 e3845 no 1553;programacion lineal;mathematical programming;relaxation method;indexation;linear programming;programmation lineaire;linear program;tucker representations;network flow;programmation mathematique;programacion matematica	In this paper we propose a new method for solving linear programs. This method may be viewed as a generalized coordinate descent method whereby the descent directions are chosen from a finite set. The generation of the descent directions are based on results from monotropic programming theory. The method may be alternately viewed as an extension of the relaxation method for network flow problems [1], [2]. Node labeling, cuts, and flow augmentation paths in the network case correspond to respectively tableau pivoting, rows of tableaus, and columns of tableaus possessing special sign patterns in the linear programming case.	column (database);coordinate descent;descent direction;flow network;linear programming relaxation;method of analytic tableaux;relaxation (iterative method)	Paul Tseng;Dimitri P. Bertsekas	1987	Math. Oper. Res.	10.1287/moor.12.4.569	mathematical optimization;flow network;computer science;linear programming;calculus;mathematics;relaxation;algorithm	ML	24.715312991585144	13.038069998965407	53645
77f014853137058c030f7adb2d249e7dbd37e1ac	on the links between probabilistic graphical models and submodular optimisation. (liens entre modèles graphiques probabilistes et optimisation sous-modulaire)		A probabilistic graphical model encodes conditional independences among random variables, which is related to factorisable distributions. Moreover, the entropy of a probability distribution on a set of discrete random variables is always bounded by the entropy of its factorisable counterpart. This is due to the submodularity of entropy on the set of discrete random variables. Submodular functions are also generalisation of matroid rank function; therefore, linear functions may be optimised on the associated polytopes exactly using a greedy algorithm. In this manuscript, we exploit these links between the structures of graphical models and submodular functions: we use greedy algorithms to optimise linear functions on the polytopes related to graphic and hypergraphic matroids for learning the structures of graphical models, while we use inference algorithms on graphs to optimise submodular functions. The irst main contribution of the thesis aims at approximating a probabilistic distribution with a factorisable tractable distribution under the maximum likelihood framework. Since the tractability of exact inference is exponential in the treewidth of the decomposable graph, our goal is to learn bounded treewidth decomposable graphs, which is known to be NP-hard. We pose this as a combinatorial optimisation problem and provide convex relaxations based on graphic and hypergraphic matroids. This leads to an approximate solution with good empirical performance. In the second main contribution, we use the fact that the entropy of a probability distribution is always bounded by the entropy of its factorisable counterpart mainly as a consequence of submodularity. This property of entropy is generalised to all submodular functions and bounds based on graphical models are proposed. We refer to them as graph-based bounds. An algorithm is developped to maximise submodular functions, which is NP-hard, by maximising the graph-based bound using variational inference algorithms on graphs. The third main contribution of the thesis deals with minimising submodular functions that can be written as sum of “simple” submodular functions. It is broadly subdivided into two parts. The irst part deals with reviewing algorithms that minimise sum of “simple” submodular functions using minimisation oracles of the “simple” functions. Here, we speciically deal with cut functions in large scale problems and the minimisation oracles of the “simple” functions are graph inference algorithms. The second part proposes algorithms to minimise sum of general submodular functions using the structure of the polytopes related to individual “simple” submodular functions.		K. S. Sesh Kumar	2016				ML	34.819868358082594	7.61969313689028	53696
ecd5c6a5b6d937456ea1bb0457ebffff15b56dfe	location coding on icosahedral aperture 3 hexagon discrete global grids	metodo analisis;hexagonal shape;representacion espacial;congres international;systeme discret;congreso internacional;spatial index;forme hexagonale;international conference;2 dimensional;algorithme;grid;algorithm;codificacion;methode analyse;analysis method;rejilla;indexation;coding;forma hexagonal;spatial representation;grille;representation spatiale;vector data;sistema discreto;multi resolution;data structure;grid system;discrete system;codage;algoritmo	Discrete global grid systems (DGGSs) represent a relatively new, but increasingly popular, approach to the problem of representing geospatial location on computer systems. Despite growing interest amongst potential users in icosahedral aperture 3 hexagon DGGSs, the practical use of such systems has been hindered by a lack of efficient spatial indexing methods. In this paper we discuss the two primary approaches to developing multi-resolution location coding systems for DGGSs: pyramid addressing and path addressing. We then describe an efficient pyramid addressing system for icosahedral aperture 3 hexagon DGGSs, the quadrilateral 2-dimensional integer system. After reviewing the problems inherent in developing path addressing systems for hexagon-based DGGSs we describe a class of path-based location coding solutions for icosahedral aperture 3 hexagon DGGSs called modified generalized balanced ternary, and show how this system can be used to index vector data. We then discuss a subset of this system, the icosahedral aperture 3 hexagon tree, which can be used to index raster and bucket data structures. Conversion algorithms to/from geodetic coordinates are discussed.	aperture (software);regular icosahedron	Kevin Sahr	2008	Computers, Environment and Urban Systems	10.1016/j.compenvurbsys.2007.11.005	two-dimensional space;data structure;geography;computer science;discrete system;geometry;coding;grid;spatial database;cartography	ML	30.773939087355902	12.544978888867757	54104
75ba9c792cab1a6266a632f77ad02f9d19485753	generating and improving orthogonal designs by using mixed integer programming	design of experiments;orthogonal design creation;statistics;orthogonal design creation design of experiments statistics;article	Analysts faced with conducting experiments involving quantitative factors have a variety of potential designs in their portfolio. However, in many experimental settings involving discrete-valued factors (particularly if the factors do not all have the same number of levels), none of these designs are suitable. In this paper, we present a mixed integer programming (MIP) method that is suitable for constructing orthogonal designs, or improving existing orthogonal arrays, for experiments involving quantitative factors with limited numbers of levels of interest. Our formulation makes use of a novel linearization of the correlation calculation. The orthogonal designs we construct do not satisfy the definition of an orthogonal array, so we do not advocate their use for qualitative factors. However, they do allow analysts to study, without sacrificing balance or orthogonality, a greater number of quantitative factors than it is possible to do with orthogonal arrays which have the same number of runs. 2011 Elsevier B.V. All rights reserved.	detailed balance;experiment;integer programming;linear programming	Hélcio Vieira;Susan M. Sanchez;Karl Heinz Kienitz;Mischel Carmen Neyra Belderrain	2011	European Journal of Operational Research	10.1016/j.ejor.2011.07.005	mathematical optimization;combinatorics;mathematics;design of experiments;statistics	AI	30.961179977086335	9.294163360305776	54958
1fd1e8cf84bf9c048acaf4d71d57d83238f53a41	intersecting a simple mixed integer set with a vertex packing set	mathematics;conflict graph;valid inequalities;inventory routing;mixed integer set	We consider a mixed integer set that results from the intersection of a simple mixed integer set with a vertex packing set from a conflict graph. This set arises as a relaxation of the feasible set of mixed integer problems such as inventory routing problems. We derive families of strong valid inequalities that consider the structures of the simple mixed integer set and the vertex packing set simultaneously.	feasible region;independent set (graph theory);linear programming relaxation;routing;serializability;set packing	Agostinho Agra;Mahdi Doostmohammadi;Cid C. de Souza	2013	Electronic Notes in Discrete Mathematics	10.1016/j.endm.2013.05.109	mathematical optimization;combinatorics;discrete mathematics;set packing;solution set;feedback vertex set;special ordered set;mixed graph;vertex;mathematics;maximal independent set;index set;set function;infinite set	Theory	24.823418956514264	14.007631900936325	55067
d763316a96b3918d1fdd202c238a5b10446ed671	improved estimation of the ranking probabilities in partial orders using random linear extensions by approximation of the mutual ranking probability	linear extension;partial order	The application of partial order theory and Hasse diagram technique in environmental science is getting increasing attention. One of the latest developments in the field of Hasse diagram technique is the use of random linear extensions to estimate ranking probabilities. In the original algorithm for estimating the ranking probability it is assumed that the order between two incomparable pair of objects can be chosen randomly. However, if the total set of linear extensions is considered there is a specific probability that one object will be larger than another, which can be far from 50%. In this study it is investigated if an approximation of the mutual ranking probability can improve the algorithm. Applying an approximation of the mutual ranking probability the estimation of the ranking probabilities are significantly improved. Using a test set of 39 partial orders with randomly chosen values the relative mean root square difference (MRSD) decrease in average from 7.9% to 2.2% and a maximum relative improvement of 90% can be found. In the most successful case the relative MRSD goes as low as 0.77%.	approximation;assumed;ecology;estimated;hasse diagram;large;physical object;plant roots;probability;randomness;system of linear equations;test set;algorithm;orders - hl7publishingdomain	Dorte B. Lerche;Peter B. Sørensen;Rainer Brüggemann	2003	Journal of chemical information and computer sciences	10.1021/ci0300036	partially ordered set;combinatorics;discrete mathematics;chemistry;mathematics;statistics;linear extension	ML	35.041322798591764	14.081343209927995	55136
163d5495349d7a8cc320eaf5972a11e0ffc6eb7a	probabilistic pursuits on graphs		We consider discrete dynamical systems of ”ant-like” agents engaged in a sequence of pursuits on a graph environment. The agents emerge one by one at equal time intervals from a source vertex s and pursue each other by greedily attempting to close the distance to their immediate predecessor, the agent that emerged just before them from s, until they arrive at the destination point t. Such pursuits have been investigated before in the continuous setting and in discrete time when the underlying environment is a regular grid. In both these settings the agents’ walks provably converge to a shortest path from s to t. Furthermore, assuming a certain natural probability distribution over the move choices of the agents on the grid (in case there are multiple shortest paths between an agent and its predecessor), the walks converge to the uniform distribution over all shortest paths from s to t and so the agents’ locations are on average very close to the straight line from s to t. In this work we study the evolution of agent walks over a general finite graph environment G. Our model is a natural generalization of the pursuit rule proposed for the case of the grid. The main results are as follows. We show that ”convergence” to the shortest paths in the sense of previous work extends to all pseudo-modular graphs ∗ammicha3@cs.technion.ac.il †freddy@cs.technion.ac.il 1 ar X iv :1 71 0. 08 10 7v 2 [ cs .D M ] 2 9 O ct 2 01 7 (i.e. graphs in which every three pairwise intersecting disks have a nonempty intersection), and also to environments obtained by taking graph products, generalizing previous results in two different ways. We show that convergence to the shortest paths is also obtained by chordal graphs (i.e. graphs in which all cycles of four or more vertices have a chord), and discuss some further positive and negative results for planar graphs. In the most general case, convergence to the shortest paths is not guaranteed, and the agents may get stuck on sets of recurrent, non-optimal walks from s to t. However, we show that the limiting distributions of the agents’ walks will always be uniform distributions over some set of walks of equal length.	computational complexity theory;converge;directed graph;dynamical system;graph (discrete mathematics);graph product;greedy algorithm;planar graph;regular grid;shortest path problem	Michael Amir;Alfred M. Bruckstein	2017	CoRR		combinatorics;discrete mathematics;chordal graph;mathematics;probability distribution;dynamical systems theory;regular grid;shortest path problem;planar graph;generalization;uniform distribution (continuous)	Theory	37.1551905403546	16.451049900271602	55179
a791097b47b96c943ab02bccc9e747ba3e08fd31	level sets of the value function in differential games with the homicidal chauffeur dynamics		A classical and a modified (acoustic) variants of the differe ntial game “homicidal chauffeur” are considered. An interesting peculiari ty of the letter variant consists, in particular, in the presence of holes located strictly ins ide the victory domain of the pursuit-evasion game. In the paper, an explanation to this p henomenon is given. The explanation is based on an analysis of families of semiperme able curves that are determined from only the dynamics of the system. Results of the com putation of level sets of the value function are presented.	acoustic cryptanalysis;bellman equation;evasion (network security);homicidal chauffeur problem;pursuit-evasion	Valery S. Patsko;Varvara L. Turova	2001	IGTR	10.1142/S021919890100035X	mathematics;mathematical economics;algorithm	AI	31.33477845594667	11.85474399560714	55184
865fbb5bcf390abda58db4116946c15d663a8213	a fast and robust exact algorithm for face embedding	prototype package minsk;robust exact algorithm;face embedding;symmetric solution;candidate code;boolean space;partial solution;unsuitable candidate cube;- face embedding problem;available algorithm;input encoding.;global solution;solution space;efficient search strategy;satisfiability;encoding	We present a new matrix formulation of the face hypercube embedding problem that motivates the design of an efficient search strategy to find an encoding that satisfies all faces of minimum length. Increasing dimensions of the Boolean space are explored; for a given dimension constraints are satisfied one at a time. The following features help to reduce the nodes of the solution space that must be explored: candidate cubes instead of candidate codes are generated, cubes yielding symmetric solutions are not generated, a smaller sufficient set of solutions (producing basic sections) is explored, necessary conditions help discard unsuitable candidate cubes, early detection that a partial solution cannot be extended to be a global solution prunes infeasible portions of the search tree. We have implemented a prototype package minsk based on the previous ideas and run experiments to evaluate it. The experiments show that minsk is faster and solves more problems than any available algorithm. Moreover, minsk is a robust algorithm, while most of the proposed alternatives are not. Besides most problems of the complete MCNC benchmark suite, other solved examples include an important set of decoder PLAs coming from the design of microprocessor instruction sets.	benchmark (computing);code;exact algorithm;experiment;feasible region;genetic algorithm;matrix mechanics;microprocessor;minsk family of computers;olap cube;prototype;search tree;stone's representation theorem for boolean algebras	Eugene Goldberg;Tiziano Villa;Robert K. Brayton;Alberto L. Sangiovanni-Vincentelli	1997	1997 Proceedings of IEEE International Conference on Computer Aided Design (ICCAD)	10.1145/266388.266495	mathematical optimization;discrete mathematics;theoretical computer science;mathematics;algorithm;encoding;satisfiability	EDA	27.03477363383511	8.550676289107196	55394
fc65d70361bb9b88a8be16f0671a804d80360c38	optimizing leader influence in networks through selection of direct followers		The paper considers the problem of a leader that seeks to optimally influence the opinions of agents in a directed network through connecting with a limited number of the agents (“direct followers”), possibly in the presence of a fixed competing leader. The settings involving a single leader and two competing leaders are unified into a general combinatoric optimization problem, for which two heuristic approaches are developed. The first approach is based on a convex relaxation scheme, possibly in combination with the l1-norm regularization technique, and the second is based on a greedy selection strategy. The main technical novelties of this work are in the establishment of supermodularity of the objective function and convexity of its continuous relaxation. The greedy approach is guaranteed to have a lower bound on the approximation ratio sharper than (1−1/e), while the convex approach can benefit from efficient (customized) numerical solvers to have practically comparable solutions possibly with faster computation times. The two approaches can be combined to provide improved results. In numerical examples, the approximation ratio can be made to reach 90% or higher depending on the number of direct followers.	approximation algorithm;computation;convex function;greedy algorithm;heuristic;linear programming relaxation;manifold regularization;mathematical optimization;numerical analysis;optimization problem;optimizing compiler;supermodular function;taxicab geometry	Van Sy Mai;Eyad H. Abed	2018	CoRR		mathematical optimization;linear programming;heuristic;upper and lower bounds;regularization (mathematics);mathematics;greedy algorithm;optimization problem;convergence (routing);convexity	AI	33.06350661711461	5.912486687393762	55447
d2ffcbefe51b431423ff2e5429c6f1ba9401e706	exact clustering via integer programming and maximum satisfiability		We consider the following general graph clustering problem: given a complete undirected graph G = (V,E, c) with an edge weight function c : E → Q, we are asked to find a partition C of V that maximizes the sum of edge weights within the clusters in C. Owing to its high generality, this problem has a wide variety of real-world applications, including correlation clustering, group technology, and community detection. In this study, we investigate the design of mathematical programming formulations and constraint satisfaction formulations for the problem. First, we present a novel integer linear programming (ILP) formulation that has far fewer constraints than the standard ILP formulation by Grötschel and Wakabayashi (1989). Second, we propose an ILP-based exact algorithm that solves an ILP problem obtained by modifying our above ILP formulation and then performs simple post-processing to produce an optimal solution to the original problem. Third, we present maximum satisfiability (MaxSAT) counterparts of both our ILP formulation and ILP-based exact algorithm. Computational experiments using well-known realworld datasets demonstrate that our ILP-based approaches and their MaxSAT counterparts are highly effective in terms of both memory efficiency and computation time.	boolean satisfiability problem;cluster analysis;computation;constraint satisfaction;correlation clustering;exact algorithm;experiment;graph (discrete mathematics);integer programming;linear programming;mathematical optimization;time complexity;video post-processing;weight function	Atsushi Miyauchi;Tomohiro Sonobe;Noriyoshi Sukegawa	2018			mathematical optimization;computer science;satisfiability;cluster analysis;integer programming	AI	24.91174549383837	6.896691496953756	55624
d1a02235c05e6e081fa6e8786dc634c8f4ca1070	a generalized pólya's urn with graph based interactions	gradient like system;reinforcement;unstable equilibria;polya s urn;stochastic approximation algorithms	Given a finite connected graph G, place a bin at each vertex. Two bins are called a pair if they share an edge of G. At discrete times, a ball is added to each pair of bins. In a pair of bins, one of the bins gets the ball with probability proportional to its current number of balls raised by some fixed power α > 0. We characterize the limiting behavior of the proportion of balls in the bins. The proof uses a dynamical approach to relate the proportion of balls to a vector field. Our main result is that the limit set of the proportion of balls is contained in the equilibria set of the vector field. We also prove that if α < 1 then there is a single point v = v(G, α) with non-zero entries such that the proportion converges to v almost surely. A special case is when G is regular and α ≤ 1. We show e.g. that if G is non-bipartite then the proportion of balls in the bins converges to the uniform measure almost surely. © 2013 Wiley Periodicals, Inc. Random Struct. Alg., 46, 614–634, 2015	connectivity (graph theory);interaction;pólya enumeration theorem	Michel Benaïm;Itai Benjamini;Jun Chen;Yuri Lima	2015	Random Struct. Algorithms	10.1002/rsa.20523	reinforcement;mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	37.355746286691335	16.76408204729333	55638
5264aa59c88ca1711a85c7dfd688f71e6f451ee0	guided genetic algorithm for the influence maximization problem		Influence maximization is a hard combinatorial optimization problem. It requires the identification of an optimum set of k network vertices that triggers the activation of a maximum total number of remaining network nodes with respect to a chosen propagation model. The problem is appealing because it is provably hard and has a number of practical applications in domains such as data mining and social network analysis. Although there are many exact and heuristic algorithms for influence maximization, it has been tackled by metaheuristic and evolutionary methods as well. This paper presents and evaluates a new evolutionary method for influence maximization that employs a recent genetic algorithm for fixed–length subset selection. The algorithm is extended by the concept of guiding that prevents selection of infeasible vertices, reduces the search space, and effectively improves the evolutionary procedure.	expectation–maximization algorithm;genetic algorithm	Pavel Krömer;Jana Nowaková	2017		10.1007/978-3-319-62389-4_52	discrete mathematics;genetic algorithm;social network analysis;mathematical optimization;combinatorial optimization;computer science;metaheuristic;social network;heuristic;entropy maximization;maximization	ML	25.05370123878732	5.606750322047699	55828
e4f7a354b2e567d5f887da76cb458d8486ba3fc4	bounds for random binary quadratic programs		In this paper, we consider a binary quadratic program (BQP) with random objective coefficients. Given only information on the marginal distributions of the objective coefficients, we propose a tight bound on the expected optimal value of the random BQP. We show that the complexity of computing this bound does not increase substantially with respect to the complexity of solving the corresponding deterministic BQP. For the quadratic unconstrained binary optimization (QUBO) problem with nonnegative off-diagonal random entries, the bound is shown to be computable in polynomial time. We generalize the asymptotic bound for the random quadratic assignment problem from independent random variables to dependent random variables and propose a new closed-form bound on the expected optimal value of the quadratic k-cluster problem. We also provide polynomial time computable upper bounds on the expected optimal value for the NP-hard instances using the linear and semidefinite programming relaxation of the deterministic...		Karthik Natarajan;Dongjian Shi;Kim-Chuan Toh	2018	SIAM Journal on Optimization	10.1137/16M109778X	mathematical optimization;time complexity;discrete mathematics;quadratic programming;quadratic unconstrained binary optimization;marginal distribution;semidefinite programming;quadratic assignment problem;mathematics;bqp;random variable	Theory	35.30382860540517	6.2976386333392425	55901
fdd01fc1001d242f1af34d78a6b22ed11255e810	efficient computation of a measure of depth between convex objects for graphics applications	efficient algorithms;depth of intersection;efficient algorithm;collision detection;empirical performance;numerical experiment;graphics;convex polygon	Algorithms to compute a measure of depth of intersection between convex polygonal objects in         and convex polyhedral objects in         are presented. The algorithms have linear asymptotic complexity. Numerical experience with the algorithms is encouraging and the algorithms are shown to have linear expected running time. Parallelization of the algorithms is described. We present a new collision detection algorithm that uses the depth measure. Results of implementation are reported.	computation;graphics	K. Sridharan	2002	Computers & Graphics	10.1016/S0097-8493(02)00133-4	mathematical optimization;combinatorics;probabilistic analysis of algorithms;convex combination;computer science;graphics;theoretical computer science;mathematics;collision detection;computer graphics (images)	Graphics	32.06523162348106	17.135636701294896	55909
d8ef38c6d99ce05a711ca5a876bfff7abd7d2967	gathering of asynchronous robots with limited visibility	asynchrony;robot movil;visibilite;protocols;point formation;visibilidad;algorithmique;mobile robot;asynchrone;cooperation;distributed computing;mobile robots;orientation;calculo automatico;controle;cooperacion;computing;rendez vous;calcul automatique;formation point;visibility;robot mobile;algorithmics;protocole;algoritmica;informatique theorique;orientacion;calculo repartido;control;cooperation and control;calcul reparti;moving robot;asincrono;asynchronous;coordinate system;computer theory;check;informatica teorica	In this paper we study the problem of gatheringa collection of identical oblivious mobile robots in the same location of the plane. Previous investigations have focusedmostly on the unlimited visibility setting, where each robot can always see all the others regardless of their distance. In the more difficult and realistic setting where the robots have limited visibility, the existing algorithmic results are only for convergence (towards a common point, without ever reaching it) and only for semi-synchronous environments, where robots’ movements are assumed to be performed instantaneously. In contrast, we study this problem in a totally asynchronousetting, where robots’ actions, computations, and movements require a finite but otherwise unpredictable amount of time. We present a protocol that allows anonymous oblivious robots with limited visibility to gather in the same location in finite time, provided they have orientation (i.e., agreement on a coordinate system). A preliminary version of this paper has been presented at the 18th Symposium on Theoretical Aspects of Computer Science [17]. E-mail addresses: flocchin@site.uottawa.ca (P. Flocchini),prencipe@di.unipi.it (G. Prencipe), santoro@scs.carleton.ca (N. Santoro),widmayer@inf.ethz.ch(P. Widmayer). 0304-3975/$ see front matter © 2005 Elsevier B.V. All rights reserved. doi:10.1016/j.tcs.2005.01.001 148 P. Flocchini et al. / Theoretical Computer Science 337 (2005) 147–168 Our result indicates that,with respect to gathering, orientation is at least aspowerful as instantaneous movements. © 2005 Elsevier B.V. All rights reserved.	algorithm;autonomous robot;blum axioms;complexity;computation;correctness (computer science);graph coloring;map;mobile robot;polyhedral terrain;stacs;semiconductor industry;switzerland;theoretical computer science	Paola Flocchini;Giuseppe Prencipe;Nicola Santoro;Peter Widmayer	2005	Theor. Comput. Sci.	10.1016/j.tcs.2005.01.001	mobile robot;simulation;computer science;artificial intelligence;distributed computing;algorithmics	Theory	32.56175584597551	12.20217027006959	55948
f30222970851947910a6fe6fe90a054a0ed29090	the augmented weighted tchebychev norm for optimizing a linear function over an integer efficient set of a multicriteria linear program	tchebychev metrics;integer programming;multiple objective	Abstract#R##N##R##N#In this paper, we propose a new exact algorithm, using an augmented weighted Tchebychev norm, for optimizing a linear function on the efficient set of a multiple objective integer linear programming problem. This norm is optimized progressively by improving the value of the linear criteria and going through some efficient solutions. The method produced not only the best efficient solution of the linear objective function but also a subset of nondominated solutions that can help decision makers to select the best decision among a large set of Pareto solutions.	linear function;linear programming	Djamal Chaabane;Brahim Brahmi;Z. Ramdani	2012	ITOR	10.1111/j.1475-3995.2012.00851.x	mathematical optimization;combinatorics;discrete mathematics;integer programming;linear-fractional programming;computer science;mathematics	ML	25.71080067885077	9.64323432919708	56007
76334db905c86ceb715ed8de0f9cca80ae56c881	a warm-start approach for large-scale stochastic linear programs	interior point methods;generation;decomposition;interior point;60c06;cutting plane scheme;large scale;90c51;linear program;optimization;stochastic programming;interior point method;strategies	We describe a way of generating a warm-start point for interior point methods in the context of stochastic programming. Our approach exploits the structural information of the stochastic problem so that it can be seen as a structure-exploiting initial point generator. We solve a small-scale version of the problem corresponding to a reduced event tree and use the solution to generate an advanced starting point for the complete problem. The way we produce a reduced tree tries to capture the important information in the scenario space while keeping the dimension of the corresponding (reduced) deterministic equivalent small. We derive conditions which should be satisfied by the reduced tree to guarantee a successful warm-start of the complete problem. The implementation within the HOPDM and OOPS interior point solvers shows remarkable advantages.	algorithmic efficiency;computation;event tree;hoc (programming language);interior point method;iteration;linear programming;linux kernel oops;stochastic process;stochastic programming;time complexity	Marco Colombo;Jacek Gondzio;Andreas Grothey	2011	Math. Program.	10.1007/s10107-009-0290-9	mathematical optimization;combinatorics;discrete mathematics;linear programming;interior point method;mathematics	AI	29.594489041585476	9.280161616410316	56211
88aed5b055b6d4c5dfc421b7b5026cf021543103	rational points near curves and small nonzero |x3-y2| via lattice reduction	algoritmo paralelo;approximation lineaire;plane curve;parallel algorithm;algorithm complexity;algorithm analysis;approximation diophantienne;complejidad algoritmo;rational point;linear approximation;algorithme parallele;number theory;algebraic geometry;diophantine approximation;numerical analysis;complexite algorithme;lattice reduction;informatique theorique;aproximacion lineal;analyse algorithme;aproximacion diofantica;analisis algoritmo;computer theory;informatica teorica	We give a new algorithm using linear approximation and lattice reduction to efficiently calculate all rational points of small height near a given plane curve C. For instance, when C is the Fermat cubic, we find all integer solutions of |x 3  + y 3  - z 3 | < M with 0 < x ≤ y < z < N in heuristic time? (log O(1) N)M provided M? N, using only O(log N) space. Since the number of solutions should be asymptotically proportional to M log N (as long as M < N 3 ), the computational costs are essentially as low as possible. Moreover the algorithm readily parallelizes It not only yields new numerical examples but leads to theoretical results, difficult open questions, and natural generalizations. We also adapt. our algorithm to investigate Hall's conjecture: we find all integer solutions of 0 < |x 3  - y 2 |? x 1/2  with x < X in time O(X 1/2  log O(1) X). By implementing this algorithm with X = 10 18  we shattered the previous record for x 1/2 /|x 3  - y 2 |. The O(X 1/2 log O(1) X) bound is rigorous; its proof also yields new estimates on the distribution mod 1 of (cx) 3/2  for any positive rational c.	lattice reduction	Noam D. Elkies	2000		10.1007/10722028_2	plane curve;combinatorics;number theory;diophantine approximation;lattice reduction;algebraic geometry;numerical analysis;mathematics;geometry;parallel algorithm;rational point;algorithm;linear approximation	Crypto	32.655818439428465	15.458259092952341	56222
70f2eb83b78ddd5df6be05c076e3b77789d1b9c0	solving linear optimization over arithmetic constraint formula	linear programming;disjunctive programming;optimization modulo theories;search algorithm	Since Balas extended the classical linear programming problem to the disjunctive programming (DP) problem where the constraints are combinations of both logic AND and OR, many researchers explored this optimization problem under various theoretical or application scenarios such as generalized disjunctive programming (GDP), optimization modulo theories (OMT), robot path planning, real-time systems, etc. However, the possibility of combining these differently-described but form-equivalent problems into a single expression remains overlooked. The contribution of this paper is two folded. First, we convert the linear DP/GDP model, linear-arithmetic OMT problem and related application problems into an equivalent form, referred to as the linear optimization over arithmetic constraint formula (LOACF). Second, a tree-search-based algorithm named RS-LPT is proposed to solve LOACF. RS-LPT exploits the techniques of interval analysis and nonparametric estimation for reducing the search tree and lowering the number of visited nodes. Also, RS-LPT alleviates bad construction of search tree by backtracking and pruning dynamically. We evaluate RS-LPT against two most common DP/GDP methods, three state-of-the-art OMT solvers and the disjunctive transformation based method on optimization benchmarks with different types and scales. Our results favor RS-LPT as compared to existing competing methods, especially for large scale cases.	linear programming;mathematical optimization	Li Chen;Yinrun Lyu;Chong Wang;Jingzheng Wu;Changyou Zhang;Nasro Min-Allah;Jamal Alhiyafi;Yongji Wang	2017	J. Global Optimization	10.1007/s10898-017-0499-8	mathematical optimization;discrete mathematics;computer science;mathematics;algorithm	Theory	24.889337941826554	8.470197491609557	56251
6bdd5e1a9e3e87071f707107254139e625d25f3c	stable leader election in population protocols requires linear time	population protocols;leader election;time lower bound;chemical reaction network	A population protocol stably elects a leader if, for all n, starting from an initial configuration with n agents each in an identical state, with probability 1 it reaches a configuration y that is correct exactly one agent is in a special leader state $$ell $$ and stable every configuration reachable from y also has a single agent in state $$ell $$. We show that any population protocol that stably elects a leader requires $$Omega n$$ expected parallel -- $$Omega n^2$$ expected total pairwise interactions -- to reach such a stable configuration. Our result also informs the understanding of the time complexity of chemical self-organization by showing an essential difficulty in generating exact quantities of molecular species quickly.		David Doty;David Soloveichik	2015		10.1007/978-3-662-48653-5_40	real-time computing;simulation;distributed computing	Theory	33.175759178292786	11.61862742286465	56513
722f9b38fba3e0ff7b8451103a92d86c5dc8ed25	random regular graphs of non-constant degree: concentration of the chromatic number	distribution;numero cromatico;chromatic number concentration;edge distribution;random graph;prueba;configuracion;chromatic graph;switching;random graph model;probability;grado grafo;loi probabilite;concentracion;ley probabilidad;05bxx;fonction repartition;grafo aleatorio;discrete mathematics;nombre chromatique;chromatic number;graphe aleatoire;funcion distribucion;distribution function;preuve;graphe chromatique;random regular graphs;probability distribution;probabilidad;conmutacion;05c80;probabilite;grafo regular;degre graphe;graphe regulier;configuration;proof;distribucion;concentration;commutation;graph degree;regular graph;grafo cromatico	In this work we show that with high probability the chromatic number of a graph sampled from the random regular graph model Gn,d for d = o(n ) is concentrated in two consecutive values, thus extending a previous result of Achlioptas and Moore. This concentration phenomena is very similar to that of the binomial random graph model G(n, p) with p = d n . Our proof is largely based on ideas of Alon and Krivelevich who proved this two-point concentration result for G(n, p) for p = n where δ > 1/2. The main tool used to derive such a result is a careful analysis of the distribution of edges in Gn,d, relying both on the switching technique and on bounding the probability of exponentially small events in the configuration model.	graph coloring;random graph;random regular graph;with high probability	Sonny Ben-Shimon;Michael Krivelevich	2009	Discrete Mathematics	10.1016/j.disc.2008.12.014	distribution;probability distribution;random regular graph;random graph;combinatorics;discrete mathematics;topology;regular graph;distribution function;probability;proof;mathematics;configuration;concentration	Theory	38.74612811148745	16.132833696218853	56530
92a50c4f84305410c507e0d0262c2a82f13269a0	optimal slope selection via expanders	design algorithm;design of algorithms;algorithm complexity;geometrie algorithmique;complejidad algoritmo;computational geometry;algorithme;algorithm;conception algorithme;complexite algorithme;geometria computacional;algoritmo	Given n points in the plane and an integer k, the slope selection problem is to nd the pair of points whose connecting line has the k-th smallest slope. (In dual setting, given n lines in the plane, we want to nd the vertex of their arrangement with the k-th smallest x-coordinate.) Cole et al. 6] have given an O(n log n) solution (which is optimal), using the parametric searching technique of Megiddo. We obtain another optimal (deterministic) solution that does not depend on parametric searching and uses expander graphs instead. Our solution is somewhat simpler than that of 6] and has a more explicit geometric interpretation.	selection algorithm;theil–sen estimator	Matthew J. Katz;Micha Sharir	1993	Inf. Process. Lett.	10.1016/0020-0190(93)90234-Z	mathematical optimization;combinatorics;computational geometry;computer science;mathematics;algorithm	Theory	28.94749591475918	17.690108648886497	56742
d202aa0d6450e56c91fabdc719d410984a24fa34	the competition for shortest paths on sparse graphs		Optimal paths connecting randomly selected network nodes and fixed routers are studied analytically in the presence of a nonlinear overlap cost that penalizes congestion. Routing becomes more difficult as the number of selected nodes increases and exhibits ergodicity breaking in the case of multiple routers. The ground state of such systems reveals nonmonotonic complex behaviors in average path length and algorithmic convergence, depending on the network topology, and densities of communicating nodes and routers. A distributed linearly scalable routing algorithm is also devised.	anatomy, regional;average path length;behavior;convergence (action);dijkstra's algorithm;ergodicity;exhibits as topic;graph - visual representation;ground state;network congestion;network topology;nonlinear system;randomness;router (computing);routing;scalability;short;shortest path problem;sparse;density	Chi Ho Yeung;David Saad	2012	Physical review letters	10.1103/PhysRevLett.108.208701	convergence;routing protocol	Networks	32.649100015302594	10.802342741155394	56904
f7c694ccb84872bce5381164d447f2261e78b0e8	a neighborhood exploration approach with multi-start for extend generalized block-modeling		Block-modeling is a framework to describe a social network as a small structure. We propose here a Neighborhood Exploration Approach with Multi-start for tackling the Extend Generalized Block-modeling. The Extend Generalized Blockmodeling is the first and most complete model approach: it allows to analyze networks without any a priory knowledge about them. The other models require at least to know the size of the partition (i.e. the number of sub-sets that the partition will contain) and a pre-definition of the ideal models.	experiment;heuristic;social network	Micheli Knechtel;Philippe Michelon;Serigne Gueye;Luis Satoru Ochi	2018	Electronic Notes in Discrete Mathematics	10.1016/j.endm.2018.03.009	combinatorics;discrete mathematics;mathematics;partition (number theory);social network	AI	26.094245665761314	16.280819476627162	57151
90c8881f3c07e096bd3295eeef86c21bf2f87322	on the stability in stochastic programming: the case of individual probability constraints			stochastic programming	Vlasta Kanková	1997	Kybernetika		stochastic programming;mathematical optimization;kolmogorov equations (markov jump process);kolmogorov equations;mathematics;kolmogorov's criterion;chain rule for kolmogorov complexity;kolmogorov structure function;stein's method	Robotics	38.37794727994868	4.963739750722707	57249
cd5467266c13bff05a0b32db8de51a10ae8b9fef	a new knapsack solution approach by integer equivalent aggregation and consistency determination	testing the consistency of a linear equation in integer variables;mathematics;combinatorics;integer;equivalent aggregation of integer valued equations;theory;algorithms;programming	We present a new and highly efficient algorithm for the integer knapsack problem based on a special strategy for aggregating integer-valued equations. Employing a new theorem for creating a single equation with the same nonnegative integer solution set as a system of original equations, we transform the integer knapsack problem into an equivalent problem of determining the consistency of an aggregated equation for a parameterized right hand side. This last problem is solved by a newly developed algorithm with complexity O(min(n α1, n + α12)), where n is the number of variables and α1 is the smallest coefficient in the aggregated equation. Empirical outcomes show our procedure is significantly superior to advanced branch-and-bound methods (previously established to be the most efficient knapsack solution procedures), obtaining solutions several orders of magnitude faster for hard problems.		Djangir A. Babayev;Fred Glover;Jennifer Ryan	1997	INFORMS Journal on Computing	10.1287/ijoc.9.1.43	integer;continuous knapsack problem;trial division;programming;mathematical optimization;combinatorics;discrete mathematics;integer programming;nearest integer function;cutting stock problem;change-making problem;highly cototient number;integer points in convex polyhedra;mathematics;knapsack problem;theory	Vision	24.708292919430342	11.016828417119758	57352
1411da03a2019aa40e4f4b53f0d536d12499137c	optimal multivariate 2-microaggregation for microdata protection: a 2-approximation	minimisation;anonymity;minimization;analyse amas;information loss;temps polynomial;heuristic method;intruso;securite informatique;statistical databases;database;base dato;problema np duro;outlier;metodo heuristico;minimizacion;homogeneidad;grupo puntual;classification;anonymat;computer security;groupe ponctuel;observacion aberrante;base donnee statistique;vida privada;np hard problem;aproximacion polinomial;cluster analysis;private life;point group;data privacy;probleme np difficile;perdida informacion;clustering;seguridad informatica;microdato;statistical disclosure control;microdonnee;approximation polynomiale;polynomial time;sum of squares;base de donnees;observation aberrante;vie privee;intrus;analisis cluster;methode heuristique;homogeneite;microaggregation;intruder;confidentialite donnee;perte information;clasificacion;polynomial approximation;homogeneity;anonimato;microdata;privacy preserving data mining;tiempo polinomial	Microaggregation is a special clustering problem where the goal is to cluster a set of points into groups of at least k points in such a way that groups are as homogeneous as possible. Microaggregation arises in connection with anonymization of statistical databases for privacy protection (k-anonymity), where points are assimilated to database records. A usual group homogeneity criterion is within-groups sum of squares minimization SSE. For multivariate points, optimal microaggregation, i.e. with minimum SSE, has been shown to be NP-hard. Recently, a polynomial-time O(k)-approximation heuristic has been proposed (previous heuristics in the literature offered no approximation bounds). The special case k = 2 (2-microaggregation) is interesting in privacy protection scenarios with neither internal intruders nor outliers, because information loss is lower: smaller groups imply smaller information loss. For 2-microaggregation the existing general approximation can only guarantee a 36-approximation. We give here a new polynomial-time heuristic whose SSE is at most twice the minimum SSE (2-approximation).	algorithm;approximation;cluster analysis;data anonymization;database;heuristic (computer science);microdata (html);np-hardness;numerical analysis;polynomial;streaming simd extensions;time complexity	Josep Domingo-Ferrer;Francesc Sebé	2006		10.1007/11930242_12	information privacy;computer science;data mining;mathematics;cluster analysis;computer security;algorithm;statistics	DB	26.12097346536828	17.16360384168744	57583
7acc20536189d7deef886e2d1e4f8386b44b30fa	an adjusted recursive operator allocation optimization algorithm for line balancing control.	recursion operator;balance control;optimal algorithm	This paper aims to solve the operator allocation optimization problem for line balancing control under two unsatisfied conditions. An approach is proposed for combination condition adjustment. An adjusted recursive operator allocation optimization algorithm is developed for generating the optimal solution under these two conditions adjusted.	algorithm;balanced line;bundle adjustment;mathematical optimization;optimization problem;production leveling;recursion (computer science)	B. L. Song;Wai Keung Wong;J. Fan;S. F. Chan	2006			mathematical optimization;theoretical computer science;control theory;mathematics	AI	26.212426750284354	9.694972825534947	57933
518c1c5cc73bd5c3ba5888a869dae5d37a2e0baf	a novel solution to the att48 benchmark problem	traveling salesman problem;hamiltonian cycle;benchmark problem;computational complexity;data structure;hamiltonian path	A solution to the benchmark ATT48 Traveling Salesman Problem (from the TSPLIB95 library) results from isolating the set of vertices into ten open-ended zones with nine lengthwise boundaries. In each zone, a minimum-length Hamiltonian Path (HP) is found for each combination of boundary vertices, leading to an approximation for the minimum-length Hamiltonian Cycle (HC). Determination of the optimal HPs for subsequent zones has the effect of automatically filtering out non-optimal HPs from earlier zones. Although the optimal HC for ATT48 involves only two crossing edges between all zones (with one exception), adding inter-zone edges can accomodate more complex problems.	approximation;benchmark (computing);hamiltonian path;nonlinear gameplay;travelling salesman problem;vertex (geometry)	Anthony A. Ruffa	2007	CoRR		hamiltonian path;mathematical optimization;combinatorics;data structure;computer science;mathematics;hamiltonian path problem;algorithm;bottleneck traveling salesman problem	AI	28.29753899824184	10.867171103932755	58453
c01499172959126ddb2e44bc66d7df3eeb3f25a9	a system for distance studies and applications of metaheuristics	discrete optimization;bayesian approach;fixed time;knapsack problem;numerical analysis;statistical decision theory;java applet;exact algorithm;heuristic optimization;scheduling problem;global optimization;expert knowledge;monte carlo;local minima;optimality theory;optimal algorithm	The efficiency of metaheuristics depends on parameters. Often this relation is defined by statistical simulation and have many local minima. Therefore, methods of stochastic global optimization are needed to optimize the parameters. The traditional numerical analysis considers optimization algorithms that guarantee some accuracy for all functions to be optimized. This includes the exact algorithms. Limiting the maximal error requires a computational effort that often increases exponentially with the size of the problem [Horst and Pardalos (1995), Handbook of Global Optimization, Kluwer Academic Publisher, Dordrecht/Boston/London]. That limits practical applications. An alternative is the average analysis where the expected error is made as small as possible [Calvin and Zilinskas (2000), JOTA Journal of Optimization Theory and Applications, 106, 297–307]. The average is taken over a set of functions to be optimized. The average analysis is called the Bayesian Approach (BA) [Diaconis (1988), Statistical Decision Theory and Related Topics, Springer-Verlag, Berlin, pp. 163–175, Mockus and Mockus (1987), Theory of Optimal Decision, Vol. 12, Institute of Mathematics and Cybernetics, Akademia Nauk Lithuanian SSR, Vilnius, Lithuania pp. 57–70]. Application of BA to optimization of heuristics is called the Bayesian Heuristic Approach (BHA) [Mockus (2000), A Set of Examples of Global and Discrete Optimization: Application of Bayesian Heuristic Approach, Kluwer Academic Publishers, Dordrecht, ISBN 0-7923-6359-0]. If the global minimum is known then the traditional stopping condition is applied: stop if the distance to the global minimum is within acceptable limits. If the global minimum is not known then the different approach is natural: minimize the average deviation during the fixed time limit because there is no reason to stop before. If the distance from the global minimum is not known the efficiency of method is tested by comparing with average results of some other method. “Pure” Monte Carlo is a good candidate for such comparison because it converges and does not depend on parameters that can be adjusted to a given problem by using some expert knowledge or additional test runs. In this paper a short presentation of the basic ideas of BHA [described in detail in Mockus (2000, A Set of Examples of Global and Discrete Optimization: Application of Bayesian Heuristic Approach, Kluwer Academic Publishers, Dordrecht, ISBN 0-7923-6359-0) and Mockus (1989, Bayesian Approach to Global Optimization, Kluwer Academic Publishers, Dordrec ht-London-Boston)] is given. The simplest knapsack problem is for initial explanation of BHA. The possibilities of application are illustrated by a school scheduling problem and other examples. Designed for distance graduate studies of the theory of games and markets in the Internet environment. All the algorithms are implemented as platform independent Java applets or servlets therefore readers can easily verify and apply the results for studies and for real life heuristic optimization problems. To address this idea, the paper is arranged in a way convenient for the direct reader participation. Therefore, a part of the paper is written as some “user guide”. The rest is a short description of optimization algorithms and models. All the remaining information is on web-sites, for example http://pilis.if.ktu.lt/~mockus.	metaheuristic	Jonas Mockus	2006	J. Global Optimization	10.1007/s10898-005-5369-0	discrete optimization;mathematical optimization;numerical analysis;bayesian probability;computer science;artificial intelligence;maxima and minima;mathematics;knapsack problem;algorithm;java applet;global optimization;monte carlo method	DB	28.909743782967407	7.896440972130831	58639
b39048edb6c7b86b0f6312000e77967fca9a8a14	algorithms for projecting points to give the most uniform distribution with applications to hashing	optimal solution;computational geometry;polynomial time;hash function;linear space;uniform distribution	This paper presents several algorithms for projecting points so as to give the most uniform distribution. Givenn points in the plane and an integerb, the problem is to find an optimal angleθ ofb equally spaced parallel lines such that points are distributed most uniformly over buckets (regions bounded by two consecutive lines). An algorithm is known only in thetight case in which the two extreme lines are the supporting lines of the point set. The algorithm requiresO(bn2 logn) time and On2+bn) space to find an optimal solution. In this paper we improve the algorithm both in time and space, based on duality transformation. Two linear-space algorithms are presented. One runs in On2+K log n+bn) time, whereK is the number of intersections in the transformed plane.K is shown to beO(@#@ n2+bn@#@) based on a new counting scheme. The other algorithm is advantageous ifb < √n. It performs a simplex range search in each slab to enumerate all the lines that intersectbucket lines, and runs in O(b0.610n1.695+K logn) time. It is also shown that the problem can be solved in polynomial time even in therelaxed case. Its one-dimensional analogue is especially related to the design of an optimal hash function for a static set of keys.	algorithm;enumerated type;hash function;range searching;slab allocation;time complexity	Tetsuo Asano;Takeshi Tokuyama	1993	Algorithmica	10.1007/BF01190156	time complexity;mathematical optimization;hash table;combinatorics;discrete mathematics;hash function;linear hashing;perfect hash function;dynamic perfect hashing;computational geometry;universal hashing;mathematics;k-independent hashing;geometry;locality preserving hashing;uniform distribution;linear space	Theory	29.737157584721572	17.96323668992772	58871
10737e8475b0c0b7309d6018d6f8637c5fc69ba6	aspect graphs: an introduction and survey of recent results	aspect graph	Abstract#R##N##R##N#The study of the aspect graph of a three-dimensional object has recently become an active area of research in computer vision. The aspect graph provides a complete enumeration of all possible distinct views of an object, given a model for viewpoint space and a definition for “distinct.” This article presents a tutorial introduction to the aspect graph, surveys the current state of the art in algorithms for automatically constructing aspect graphs, and describes some possible applications of aspect graphs in computer vision and computer graphics.		Kevin W. Bowyer;Charles R. Dyer	1990	Int. J. Imaging Systems and Technology	10.1002/ima.1850020407	combinatorics;computer science;theoretical computer science;graph;graph rewriting	AI	35.37006959902775	17.967590929792458	59015
3b0a900791cc4ceb5e5dcad8d0bad33379bd9d9a	a tighter variant of jensen's lower bound for stochastic programs and separable approximations to recourse functions	metodo separacion;multiplier;separation method;stochastic programming lower bounds recourse function approximation;lower bounds;programmation stochastique;approximation fonction;multiplicateur;function approximation;computer experiment;stochastic approximation;approximation stochastique;borne inferieure;recourse function approximation;aproximacion estocastica;methode separation;stochastic programming;programacion estocastica;lower bound;multiplicador;cota inferior	In this paper, we propose a new method to compute lower bounds on the optimal objective value of a stochastic program and show how this method can be used to construct separable approximations to the recourse functions. We show that our method yields tighter lower bounds than Jensen’s lower bound and it requires a reasonable amount of computational effort even for large problems. The fundamental idea behind our method is to relax certain constraints by associating dual multipliers with them. This yields a smaller stochastic program that is easier to solve. We particularly focus on the special case where we relax all but one of the constraints. In this case, the recourse functions of the smaller stochastic program are one dimensional functions. We use these one dimensional recourse functions to construct separable approximations to the original recourse functions. Computational experiments indicate that our lower bounds can significantly improve Jensen’s lower bound and our recourse function approximations can provide good solutions. Stochastic programs form a powerful tool to model a variety of situations in which the decisions are made over time without completely knowing the realizations of the future random quantities; see Birge and Louveaux (1997) and Ruszczynski and Shapiro (2003). Despite their wide applicability, however, stochastic programs often pose significant optimization challenges. If the problem involves continuous random variables, then it is quite difficult to come up with tractable solution methods. If the random variables take on finitely many possible realizations, then the problem can be formulated as an equivalent deterministic linear program, but the size of this linear program grows exponentially with the number of time periods and the number of possible realizations. Very often, one has to be content with a small number of possible realizations for the random variables, use Monte Carlo techniques or resort to approximation strategies. In this paper, we propose a new method to compute lower bounds on the optimal objective value of a stochastic program and demonstrate how this method can be used to construct separable piecewise linear approximations to the recourse functions. We consider stochastic programs that include random variables only on the right side of their constraints. The fundamental idea behind our method is to relax some of these constraints by associating dual multipliers with them. This yields smaller stochastic programs that are hopefully easier to solve. We particularly focus on the special case where we relax all but one of the constraints. In this case, the recourse functions of the smaller stochastic programs are one dimensional functions, roughly capturing the “curvature” of the original recourse functions along different directions. We use these one dimensional recourse functions to construct separable piecewise linear approximations to the original recourse functions. A traditional method to compute lower bounds on the optimal objective value of a stochastic program is to formulate a deterministic problem by replacing all of the random variables with their expected values and to use Jensen’s inequality. We show that the lower bounds obtained by our method improve the ones that are obtained by using Jensen’s inequality. Also, our method is quite fast and requires a reasonable amount of computational effort even for large problems. In addition, it allows the user to specify for which random variables to use the full distribution information and for which random variables to use only the first moment information. Finally, it turns out that the decisions made under the guidance of the separable piecewise linear recourse function approximations that are obtained by our method can provide significantly better performance when compared with the common engineering practice of ignoring the uncertainty in the problem and assuming that the random variables take on their expected values. There has been a long line of research for computing bounds on the optimal objective values of stochastic programs. Jensen’s lower bound mentioned above is a common approach and one way to visualize this lower bound is to assume that the probability distributions in the problem are replaced with degenerate distributions that put mass only on the expected values of the random variables; see Kall and Wallace (1994). On the other hand, Edmundson Madansky upper bound is obtained by replacing the probability distributions in the problem with two point distributions that put mass only on the extreme points of the supports of the random variables; see Madansky (1959). An important shortcoming of Edmundson Madansky upper bound is that the computational effort for it grows exponentially as the	approximation;cobham's thesis;coefficient;computation;experiment;jensen's inequality;linear programming;long line (telecommunications);mathematical optimization;monte carlo method;piecewise linear continuation;randomness;regret (decision theory);regular language description for xml;social inequality;stochastic programming;wallace tree	Huseyin Topaloglu	2009	European Journal of Operational Research	10.1016/j.ejor.2008.11.020	stochastic programming;stochastic approximation;mathematical optimization;combinatorics;computer experiment;function approximation;computer science;mathematics;multiplier;mathematical economics;upper and lower bounds;statistics	Theory	35.28843457496986	5.15734929361725	59280
d72427e2a42f1d34f8e92d6b45518dc761f0e46d	a compact parallel algorithm for spherical delaunay triangulations		We present a data-parallel algorithm for the construction of Delaunay triangulations on the sphere. Our method combines a variant of the classical Bowyer-Watson point insertion algorithm [2, 14] with the recently published parallelization technique by Jacobsen et al. [7]. It resolves a breakdown situation of the latter approach and is suitable for practical implementation due to its compact formulation. Some complementary aspects are discussed such as the parallel workload, floating-point arithmetics and an application to interpolation of scattered data.	delaunay triangulation;parallel algorithm	Florian Prill;Günther Zängl	2017	Concurrency and Computation: Practice and Experience	10.1002/cpe.3971	delaunay triangulation;ruppert's algorithm;constrained delaunay triangulation;chew's second algorithm;bowyer–watson algorithm	PL	33.478991924070534	15.590430179756405	59398
1ee77c83f9b9a1583a42255d95a371e050246543	lifting/lowering hopfield models ground state energies		In our recent work [9] we looked at a class of random optimization problems that arise in the forms typically known as Hopfield models. We viewed two scenarios which we termed as the positive Hopfield form and the negative Hopfield form. For both of these scenarios we defined the binary optimization problems whose optimal values essentially emulate what would typically be known as the ground state energy of these models. We then presented a simple mechanisms that can be used to create a set of theoretical rigorous bounds for these energies. In this paper we create a way more powerful set of mechanisms that can substantially improve the simple bounds given in [9]. In fact, the mechanisms we create in this paper are the first set of results that show that convexity type of bounds can be substantially improved in this type of combinatorial problems.	ground state;hopfield network;lifting scheme;mathematical optimization;optimization problem;random optimization;statistical model;symmetry breaking	Mihailo Stojnic	2013	CoRR		mathematical optimization;combinatorics;mathematics;algorithm	Theory	30.1484579538052	7.7255964399801895	59543
5727956a0a2e3f18e90d643e1e9ea5412fac2ce5	elongation control in an algorithmic chemistry	model energy;reasonable length bound;narrow energy range;algorithmic chemistry;accurate energy model;computation model;elongation control;energy control;unlimited elongation;autocatalytic molecule;explicit length threshold	Algorithmic chemistries intended as computation models seldom model energy. This could partly explain some undesirable phenomena such as unlimited elongation of strings in these chemistries, in contrast to nature where polymerization tends to be unfavored. In this paper, we show that a simple yet sufficiently accurate energy model can efficiently steer resource usage, in particular for the case of elongation control. A string chemistry is constructed on purpose to make strings grow arbitrarily large. Simulation results show that the addition of energy control alone is able to keep the molecules within reasonable length bounds, even without mass conservation, and without explicit length thresholds. A narrow energy range is detected where the system neither stays inert nor grows unbounded. At this operating point, interesting phenomena often emerge, such as clusters of autocatalytic molecules, which seem to cooperate.	computation;computer;emergence;emergent algorithm;emergentism;operating point;rewriting;semi-thue system;simulation;state space	Thomas Meyer;Lidia Yamamoto;Wolfgang Banzhaf;Christian F. Tschudin	2009		10.1007/978-3-642-21283-3_34	simulation;computer science;artificial intelligence	OS	37.14072424065075	8.52992133114498	59754
ae668ea26bed0a7af6df261960ae921f0758c85e	approximation and contamination bounds for probabilistic programs	stochastic programs with probabilistic constraints;output analysis;contamination technique	Development of applicable robustness results for stochastic programs with probabilistic constraints is a demanding task. In this paper we follow the relatively simple ideas of output analysis based on the contamination technique and focus on construction of computable global bounds for the optimal value function. Dependence of the set of feasible solutions on the probability distribution rules out the straightforward construction of these concavity-based global bounds for the perturbed optimal value function whereas local results can still be obtained. Therefore we explore approximations and reformulations of stochastic programs with probabilistic constraints by stochastic programs with suitably chosen recourse or penalty-type objectives and fixed constraints. Contamination bounds constructed for these substitute problems may be then implemented within the output analysis for the original probabilistic program.	approximation error;bellman equation;computable function;concave function;markov switching multifractal;multi-objective optimization;optimization problem;penalty method;randomized algorithm	Martin Branda;Jitka Dupacová	2012	Annals OR	10.1007/s10479-010-0811-1	mathematical optimization;operations management;mathematics;mathematical economics;statistics	ML	35.576416397767964	4.672921576228755	59937
e0685ab6573e85c6a8096751e0591af1f083c1fc	multisection in the stochastic block model using semidefinite programming		We consider the problem of identifying underlying community-like structures in graphs. Towards this end we study the Stochastic Block Model (SBM) on k-clusters: a random model on n = km vertices, partitioned in k equal sized clusters, with edges sampled independently across clusters with probability q and within clusters with probability p, p > q. The goal is to recover the initial “hidden” partition of [n]. We study semidefinite programming (SDP) based algorithms in this context. In the regime p = α log(m) m and q = β log(m) m we show that a certain natural SDP based algorithm solves the problem of exact recovery in the k-community SBM, with high probability, whenever √ α − √ β > √ 1, as long as k = o(log n). This threshold is known to be the information theoretically optimal. We also study the case when k = θ(log(n)). In this case however we achieve recovery guarantees that no longer match the optimal condition √ α− √ β > √ 1, thus leaving achieving optimality for this range an open question.	algorithm;semidefinite programming;stochastic block model;super bit mapping;with high probability	Naman Agarwal;Afonso S. Bandeira;Konstantinos Koiliaris;Alexandra Kolla	2015	CoRR		mathematical optimization;combinatorics;discrete mathematics;mathematics;algorithm;statistics	Theory	37.52180895824948	16.88101840096516	60173
0da3f33836d5a42ac5cccc0d614ae352f421c8b8	multi-dimensional parametric mincuts for constrained map inference		In this paper, we propose novel algorithms for inferring the Maximum a Posteriori (MAP) solution of discrete pairwise random field models under multiple constraints. We show how this constrained discrete optimization problem can be formulated as a multi-dimensional parametric mincut problem via its Lagrangian dual, and prove that our algorithm isolates all constraint instances for which the problem can be solved exactly. These multiple solutions enable us to even deal with ‘soft constraints’ (higher order penalty functions). Moreover, we propose two practical variants of our algorithm to solve problems with hard constraints. We also show how our method can be applied to solve various constrained discrete optimization problems such as submodular minimization and shortest path computation. Experimental evaluation using the foreground-background image segmentation problem with statistic constraints reveals that our method is faster and its results are closer to the ground truth labellings compared with the popular continuous relaxation based methods.	algorithm;computation;convex optimization;discrete optimization;foreground-background;ground truth;image segmentation;lagrange multiplier;linear programming relaxation;mathematical optimization;max-flow min-cut theorem;minimum cut;optimization problem;shortest path problem;submodular set function	Yongsub Lim;Kyomin Jung;Pushmeet Kohli	2013	CoRR		mathematical optimization;combinatorics;machine learning;mathematics	ML	33.28728264943696	7.064157258773549	60220
04c00e2960dbabfe7bf540089e16604da5a38426	a testbed of simulation-optimization problems	simulation-optimization problem;finite-time performance;constructive comparison;simulation-optimization technique;asymptotic result;simulation;set theory	We propose a testbed of simulation-optimization problems. The purpose of the testbed is to encourage development and constructive comparison of simulation-optimization techniques and algorithms. We are particularly interested in increasing attention to the finite-time performance of algorithms, rather than the asymptotic results that one often finds in the literature.	algorithm;mathematical optimization;program optimization;simulation;testbed	Raghu Pasupathy;Shane G. Henderson	2006	Proceedings of the 2006 Winter Simulation Conference		simulation;computer science;theoretical computer science;mathematics;management science;set theory	EDA	31.477774607656563	5.4415335173532755	60364
312503927677a7db639715f0e2f229ccfd09ff1d	probably approximately correct mdp learning and control with temporal logic constraints		We consider synthesis of controllers that maximize the probability of satisfying given temporal logic specifications in unknown, stochastic environments. We model the interaction between the system and its environment as a Markov decision process (MDP) with initially unknown transition probabilities. The solution we develop builds on the so-called model-based probably approximately correct Markov decision process (PACMDP) method. The algorithm attains an ε-approximately optimal policy with probability 1−δ using samples (i.e. observations), time and space that grow polynomially with the size of the MDP, the size of the automaton expressing the temporal logic specification,	algorithm;automaton;markov chain;markov decision process;probably approximately correct learning;temporal logic	Jie Fu;Ufuk Topcu	2014	CoRR	10.15607/RSS.2014.X.039	mathematical optimization;discrete mathematics;artificial intelligence;control theory;mathematics;algorithm;statistics	Robotics	38.41154146605005	4.914375765225282	60434
32df5cdac499a8a6ba96de916869b3449f8076fb	a maxent-stress model for graph layout	metric embedding;stress;graph theory;stress layout computational modeling entropy force springs approximation methods;maximum entropy methods;statistical multidimensional scaling maxent stress model graph layout graph visualization graph edge graph length all pairs shortest path calculation fast approximation algorithm maximum entropy principle force augmented stress majorization algorithm;maximum entropy methods approximation theory data visualisation graph theory;graph drawing;low dimensional embedding graph drawing metric embedding;low dimensional embedding;layout;force;approximation theory;data visualisation;springs;computational modeling;approximation methods;entropy	In some applications of graph visualization, input edges have associated target lengths. Dealing with these lengths is a challenge, especially for large graphs. Stress models are often employed in this situation. However, the traditional full stress model is not scalable due to its reliance on an initial all-pairs shortest path calculation. A number of fast approximation algorithms have been proposed. While they work well for some graphs, the results are less satisfactory on graphs of intrinsically high dimension, because some nodes may be placed too close together, or even share the same position. We propose a solution, called the maxent-stress model, which applies the principle of maximum entropy to cope with the extra degrees of freedom. We describe a force-augmented stress majorization algorithm that solves the maxent-stress model. Numerical results show that the algorithm scales well, and provides acceptable layouts for large, nonrigid graphs. This also has potential applications to scalable algorithms for statistical multidimensional scaling (MDS) with variable distances.	approximation algorithm;graph (discrete mathematics);graph - visual representation;graph drawing;image scaling;imagery;multidimensional scaling;numerical method;principle of maximum entropy;scalability;short;shortest path problem;stress majorization	Emden R. Gansner;Yifan Hu;Stephen C. North	2012	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2012.299	layout;entropy;mathematical optimization;combinatorics;discrete mathematics;longest path problem;graph theory;hopcroft–karp algorithm;mathematics;stress;graph drawing;computational model;force;data visualization;approximation theory	Visualization	34.694946472118524	11.105857519181066	60676
d32502824469fc0ab52c858787aa404cc00b25ff	the triangle k-club problem		Graph models have long been used in social network analysis and other social and natural sciences to render the analysis of complex systems easier. In applied studies, to understand the behaviour of social networks and the interactions that command that behaviour, it is often necessary to identify sets of elements which form cohesive groups, i.e., groups of actors that are strongly interrelated. The clique concept is a suitable representation for groups of actors that are all directly related pair-wise. However, many social relationships are established not only face-to-face but also through intermediaries, and the clique concept misses all the latter. To deal with these cases, it is necessary to adopt approaches that relax the clique concept. In this paper we introduce a new clique relaxation—the triangle k-club—and its associated maximization problem—the maximum triangle k-club problem. We propose integer programming formulations for the problem, stated in different variable spaces, and derive valid inequalities to strengthen their linear programming relaxations. Computational results on randomly generated and real-world graphs, with k = 2 and k = 3, are reported.	clique (graph theory);complex systems;computation;expectation–maximization algorithm;integer programming;interaction;lagrangian relaxation;linear programming relaxation;procedural generation;relax ng;social network analysis	Filipa Duarte de Carvalho;Maria Teresa Almeida	2017	J. Comb. Optim.	10.1007/s10878-016-0009-9	mathematical optimization;combinatorics;discrete mathematics;mathematics;algorithm	ML	27.011151144203364	13.391949631779655	60736
1fbcfd9bced0286f0c199fcbe00e7a0040eb3b5d	lagrangian relaxation for map estimation in graphical models	optimal solution;convex optimization;dynamic program;lagrange multiplier;satisfiability;dual problem;artificial intelligent;consistent estimator;map estimation;graphical model;spanning tree;gaussian graphical model;lagrangian relaxation	We develop a general framework for MAP estimation in discrete and Gaussian graphical models using Lagrangian relaxation techniques. The key idea is to reformulate an intractable estimation problem as one defined on a more tractable graph, but subject to additional constraints. Relaxing these constraints gives a tractable dual problem, one defined by a thin graph, which is then optimized by an iterative procedure. When this iterative optimization leads to a consistent estimate, one which also satisfies the constraints, then it corresponds to an optimal MAP estimate of the original model. Otherwise there is a “duality gap”, and we obtain a bound on the optimal solution. Thus, our approach combines convex optimization with dynamic programming techniques applicable for thin graphs. The popular tree-reweighted maxproduct (TRMP) method may be seen as solving a particular class of such relaxations, where the intractable graph is relaxed to a set of spanning trees. We also consider relaxations to a set of small induced subgraphs, thin subgraphs (e.g. loops), and a connected tree obtained by “unwinding” cycles. In addition, we propose a new class of multiscale relaxations that introduce “summary” variables. The potential benefits of such generalizations include: reducing or eliminating the “duality gap” in hard problems, reducing the number of Lagrange multipliers in the dual problem, and accelerating convergence of the iterative optimization procedure.	cobham's thesis;convex optimization;duality (optimization);duality gap;dynamic programming;file spanning;graphical model;induced subgraph;iterative method;lagrange multiplier;lagrangian relaxation;linear programming relaxation;loop unrolling;mathematical optimization;optimization problem;relaxation (approximation);spanning tree	Jason K. Johnson;Dmitry M. Malioutov;Alan S. Willsky	2007	CoRR		mathematical optimization;combinatorics;discrete mathematics;convex optimization;duality;lagrangian relaxation;spanning tree;machine learning;mathematics;graphical model;lagrange multiplier;consistent estimator;satisfiability	ML	33.54052623882077	7.2948742611362345	60760
4e871451e6b07396ce5f954b25df264b534cad18	lower bounds on the critical density in the hard disk model via optimized metrics		We prove a new lower bound on the critical density ρc of the hard disk model, i.e., the density below which it is possible to efficiently sample random configurations of n non-overlapping disks in a unit torus. We use a classic Markov chain which moves one disk at a time, but with an improved path coupling analysis. Our main tool is an optimized metric on neighboring pairs of configurations, i.e., configurations that differ in the position of a single disk: we define a metric that depends on the difference in these positions, and which approaches zero continuously as they coincide. This improves the previous lower bound ρc ≥ 1/8 to ρc ≥ 0.154.	hard disk drive;markov chain	Thomas P. Hayes;Cristopher Moore	2014	CoRR		mathematical optimization;combinatorics;mathematics;geometry	Theory	36.09468450594729	17.66532414701188	60824
04ab385c96fae236abdbad57415da3c18ea379a6	concavity of reweighted kikuchi approximation		We analyze a reweighted version of the Kikuchi approximation for estimating the log partition function of a product distribution defined over a region graph. We establish sufficient conditions for the concavity of our reweighted objective function in terms of weight assignments in the Kikuchi expansion, and show that a reweighted version of the sum product algorithm applied to the Kikuchi region graph will produce global optima of the Kikuchi approximation whenever the algorithm converges. When the region graph has two layers, corresponding to a Bethe approximation, we show that our sufficient conditions for concavity are also necessary. Finally, we provide an explicit characterization of the polytope of concavity in terms of the cycle structure of the region graph. We conclude with simulations that demonstrate the advantages of the reweighted Kikuchi approach.	algorithm;approximation;belief propagation;concave function;loss function;optimization problem;partition function (mathematics);simulation	Po-Ling Loh;Andre Wibisono	2014			mathematical optimization;combinatorics;calculus;mathematics	ML	34.14361193867233	7.085848912188213	61071
ca924fcb324fff357376bbe6844a5e9b36f83c6e	memoryless rules for achlioptas processes	giant component;selected works;achlioptas process;qa mathematics;memoryless;05c80;bepress	In an Achlioptas process two random pairs of {1, . . . , n} arrive in each round and the player has to choose one of them. We study the very restrictive version where player’s decisions cannot depend on the previous history and only one vertex from the two random edges is revealed. We prove that the player can create a giant component in (2 √ 5− 4 + o(1))n = (0.4721 . . . + o(1))n rounds and this is best possible. On the other hand, if the player wants to delay the appearence of a giant, then the optimal bound is (1/2 + o(1))n, the same as in the Erdős-Rényi model.	erdős number;erdős–rényi model;giant component	Andrew Beveridge;Tom Bohman;Alan M. Frieze;Oleg Pikhurko	2009	SIAM J. Discrete Math.	10.1137/070684148	combinatorics;discrete mathematics;mathematics;giant component;algorithm	Theory	37.10421853364527	16.78480440827901	61077
2cf6b4cbffd1c935a04a7e3379a83d6225be22d2	proposition matrix search algorithm for satisfiability degree computation	truth value;detectors;proposition matrix search algorithm;proposition matrix;complexity theory;satisfiability degree computation;computability;truth value proposition matrix search algorithm satisfiability degree computation truth detector;search algorithm;prediction algorithms;satisfiability degree;matrix algebra;satisfiability;backtrack search;truth detector;manganese;computational complexity;complexity theory detectors manganese prediction algorithms matrix converters software algorithms probabilistic logic;matrix converters;software algorithms;search problems;probabilistic logic;search problems computability computational complexity matrix algebra;high frequency;satisfiability degree proposition matrix proposition matrix search algorithm	Satisfiability degree is used as a new means of representing uncertainty. It is able to express the extent of a system satisfying some property. How to compute the satisfiability degree is a critically problem. Although this paper has proved the computation for the satisfiability degree has the exponential worst case complexity, the proposition matrix is proposed to construct the proposition matrix search algorithm. It always chooses the high frequency proposition to simplify the old formula to obtain two new formulae, and the satisfiability degree of the old formula is equal to the sum of satisfiability degrees of the two new formulae. If the new formulae can be judged as true or false, the algorithm directly computes their satisfiability degree; else their satisfiability degree is recursively computed as the old formula. The proposition matrix search algorithm uses a truth detector to detect the truth value of a proposition so that computation times can be reduced significantly. Experimental results show it is more effective than the basic enumeration algorithm and the backtracking search algorithm.	backtracking;best, worst and average case;computation;recursion;search algorithm;time complexity;worst-case complexity	Jian Luo;Guiming Luo	2010	9th IEEE International Conference on Cognitive Informatics (ICCI'10)	10.1109/COGINF.2010.5599767	combinatorics;discrete mathematics;maximum satisfiability problem;mathematics;algorithm	AI	33.175280034953786	10.399303378323378	61093
b3ced9e553a7053057360ffa0c0a4d15330281ff	convergence analysis of stationary points in sample average approximation of stochastic programs with second order stochastic dominance constraints	exponential convergence;kkt conditions;second order dominance;stationary points	Sample average approximation (SAA) method has recently been applied to solve stochastic programs with second order stochastic dominance (SSD) constraints. In particular, Hu et al. (Math Program 133:171–201, 2012) presented a detailed convergence analysis of -optimal values and -optimal solutions of sample average approximated stochastic programs with polyhedral SSD constraints. In this paper, we complement the existing research by presenting convergence analysis of stationary points when SAA is applied to a class of stochastic minimization problems with SSD constraints. Specifically, under some moderate conditions we prove that optimal solutions and stationary points obtained from solving sample average approximated problems converge with probability one to their true counterparts. Moreover, by exploiting some recent results on large deviation of random functions and sensitivity analysis of generalized equations, we derive exponential rate of convergence of stationary points. Dedicated to Professor Jon Borwein on the occasion of his 60th birthday. The first author was supported by China Scholarship Council, the National Natural Science Foundation of China No. 11171159 and the Specialized Research Fund of Doctoral Program of Higher Education of China No. 20103207110002. H. Sun Department of Mathematics, Harbin Institute of Technology, Harbin 150001, China e-mail: mathhlsun@gmail.com H. Xu (B) School of Engineering and Mathematical Sciences, City University London, Northampton Square, London EC1V 0HB, UK e-mail: Huifu.Xu.1@city.ac.uk	approximation algorithm;converge;email;holographic principle;polyhedron;rate of convergence;solid-state drive;stationary process;time complexity	Hailin Sun;Huifu Xu	2014	Math. Program.	10.1007/s10107-013-0711-7	mathematical optimization;combinatorics;mathematics;statistics	ML	37.2360243384587	7.8110343983144706	61359
5a8425f09e3fa8417b07181162cc491b390bdeea	on the chvátal-gomory closure of a compact convex set	90c10 integer programming;90c25 convex programming;compact sets;chvatal gomory closure	In this paper, we show that the Chvátal-Gomory closure of any compact convex set is a rational polytope. This resolves an open question of Schrijver [17] for irrational polytopes1, and generalizes the same result for the case of rational polytopes [17], rational ellipsoids [8] and strictly convex bodies [7].	alexander schrijver;convex function;convex set	Daniel Dadush;Santanu S. Dey;Juan Pablo Vielma	2014	Math. Program.	10.1007/s10107-013-0649-9	mathematical optimization;combinatorics;convex polytope;mathematics;compact space	Theory	25.744219137385883	13.598381785798898	61404
40fc8b578a3d3275dd18d230e9fa9adc4507303b	a characterisation of all feasible solutions to an integer program	qa mathematics;integer program	It is shown how the dual of Fourier–Motzkin elimination can be applied to eliminating the constraints of an Integer Linear Program. The result will, in general, be to reduce the Integer Program to a single Diophantine equation together with a series of Linear homogeneous congruences. Extreme continuous solutions to the Diophantine equation give extreme solutions to the Linear Programming relaxation. Integral solutions to the Diophantine equation which also satisfy the congruences give all the solutions to the Integer Program.	integer programming	H. P. Williams	1983	Discrete Applied Mathematics	10.1016/0166-218X(83)90024-0	mathematical optimization;combinatorics;mathematical analysis;discrete mathematics;integer programming;diophantine set;branch and price;highly cototient number;diophantine equation;integer points in convex polyhedra;mathematics	ML	25.31992777830812	13.100580677187622	61572
7c55e1cf7d7c76b3ef3effaced5906f71ebdcace	ant colony optimization for the ship berthing problem	algoritmo paralelo;parallel algorithm;algorithm performance;ant colony optimization;probleme np complet;combinatorial optimization problem;randomised algorithms;cooperative agents;algorithme randomise;algorithme parallele;optimisation combinatoire;resultado algoritmo;informatique theorique;performance algorithme;problema np completo;combinatorial optimization;local search;np complete problem;optimizacion combinatoria;computer theory;dynamic storage allocation;first fit;informatica teorica	Ant Colony Optimization (ACO) is a paradigm that employs a set of cooperating agents to solve functions or obtain good solutions for combinatorial optimization problems. It has previously been applied to the TSP and QAP with encouraging results that demonstrate its potential. In this paper, we present FF-AS-SBP, an algorithm that applies ACO to the ship berthing problem (SBP), a generalization of the dynamic storage allocation problem (DSA), which is NP-complete. FF-AS-SBP is compared against a randomized first-fit algorithm. Experimental results suggest that ACO can be applied effectively to find good solutions for SBPs, with mean costs of solutions obtained in the experiment on difficult (compact) cases ranging from 0% to 17% of optimum. By distributing the agents over multiple processors, applying local search methods, optimizing numerical parameters and varying the basic algorithm, performance could be further improved. 1 Ant Colony Optimization The Ant Colony Optimization (ACO) paradigm was introduced in [1], [2] and [3] by Dorigo, Maniezzo and Colorni. ACO has been applied effectively to the traveling salesman problem (TSP) [4] and the quadratic assignment problem (QAP) [5], among several other problems. The basic idea of ACO is inspired by the way ants explore their environment in search of a food source, wherein the basic action of each ant is: to deposit a trail of pheromone (a kind of chemical) on the ground as it moves, and to probabilistically prefer moving in directions with high concentrations of pheromone deposit. As an ant moves, the pheromone it leaves on the ground marks the path that it takes. Another ant that passes by later can detect the pheromone and decide to follow the trail with high probability. If it does follow the trail, it leaves its own pheromone on it, thus reinforcing the existing pheromone deposit. By this mechanism, the movement of ants along a path between the nest and the food reinforces the pheromone deposit on it, and this in turn encourages further traffic along the path. This behavior characterized by positive feedback is described as autocatalytic. P.S. Thiagarajan, R. Yap (Eds.): ASIAN’99, LNCS 1742, pp. 359–370, 1999. c © Springer-Verlag Berlin Heidelberg 1999 360 Chia Jim Tong, Hoong Chuin Lau, and Andrew Lim On the other hand, ants may take a direction other than the one with the highest pheromone concentration. In this way, an ant does not always have to travel on the path most traveled. If an ant takes a path less traveled that deviates slightly from a popular path, and also happens to be better (shorter) than other popular paths, the pheromone it deposits encourages other ants to also take this new path. Since this path is shorter, the rate of pheromone deposit per ant that travels on it is higher, as an ant traverses a shorter distance in one trip. In this way, positive feedback can occur on this path and it can start to attract ants from other paths. By the interplay of these two mechanisms, better and better paths emerge as the exploration proceeds. For the purpose of designing an algorithm based on this idea drawn from nature, an analogy can be made of: 1) real ants vs. artificial agents, 2) ants’ spatial environment vs. space of feasible solutions, 3) goodness of a given path vs. objective function of a given solution, 4) desirability of taking a particular direction vs. desirability of making a certain decision in constructing the solution, 5) real pheromone at different parts of the environment vs. artificial pheromone for different solution choices. One of the main ideas behind ACO algorithms is how relatively simple agents can, without explicit communication, cooperate to solve a problem by indirect communication through distributed memory implemented as pheromone. In this paper, we study how ACO can be applied effectively to the ship berthing problem (SBP), through the FF-AS-SBP algorithm, an application of ACO to the SBP. The focus of this study is not on the SBP itself or on finetuning our algorithm for maximum performance. Rather, it is on demonstrating that ACO can be applied effectively to the SBP. In Section 2, we formally describe the SBP. In Section 3, we describe a candidate solution representation, from which we adapt an indirect, first-fit (FF), solution approach in Section 4 so that it becomes more suitable for the complex nature of the SBP. In this section, we also describe a randomized FF algorithm and the basis of FF-AS-SBP. FF-AS-SBP is described in Section 5. By naming the algorithm FF-AS-SBP, we acknowledge that there could be many other ACO algorithms for the SBP. In Section 6, we describe the experiment and report and interpret the results, comparing FF-AS-SBP against the randomized FF algorithm. In this section, we also discuss how results could be further improved, how the algorithm lends itself to parallelization, and possible future work. Finally, Section 7 is the conclusion. 2 The Ship Berthing Problem This problem, which has been studied in [6] and [7], can be defined as follows: ships (S = {Si: i = 1, 2, . . . , n}) are specified to have lengths li, arrive at a port at specified times ti and stay at the port for specified durations di. Each ship that arrives is to be berthed along a wharf line of length L, i. e., it is placed at the interval (bi, bi + li) along the wharf line. Once berthed, its location is fixed for the entire duration of its stay. Also, each ship has a minimum interAnt Colony Optimization for the Ship Berthing Problem 361 ship clearance distance ci and a minimum end-berth clearance distance c b i . Four types of constraints apply: – Ships can only be berthed within the extend of the wharf line. No part of any ship can extend beyond the beginning or the end of the wharf line. More strongly, the distance from either end of a ship to either end of the wharf line cannot be less than the minimum end-berth clearance distance. ∀i ∈ {1, 2, . . . , n} ci ≤ bi ≤ L− li − ci – No two ships can share the same space along the wharf line if the time intervals in which they are berthed intersect. More strongly, the end-to-end distance between them cannot be less than the minimum inter-ship clearance of either one of them. ∀i, j ∈ {1, 2, . . . , n} (ti, ti + di) ∩ (tj , tj + dj) 6= ∅ → (bi −max{csi , csj} , bi + li + max {csi , csj}) ∩ (bj , bj + lj) = ∅ – A ship may be given a fixed berthing location (bi is fixed for some values of i). – A ship may be prohibited from berthing in certain intervals of the wharf line. More precisely, the interval bounded by the location of the two ends of a ship after it has been berthed cannot intersect with any of the prohibited interval. (bi, bi + li) ∩ (p, q) = ∅ if constraint applies to Si, where (p, q) is some forbidden interval The minimization version of the problem is to determine Lo the minimum length of the wharf line needed to berth all the ships subject to the given constraints. The decision version of the problem is to determine whether berthing is possible, given a fixed value of L. The density D is defined as the maximum total length of berthed ships at any one time : D = max t∈(−∞,+∞)   ∑ i∈{i:ti≤t<ti+di} li   It is easy to see that D is a tight lower bound on L. In this paper, we also define a measure F , which we call the fragmentation, defined as: F = 1− ∑ dili (max(ti + di)−min(ti))D The berthing scenario can be visualized as a 2-D plane where the x-axis represents time, the y-axis represents space (along the wharf line), and each ship 1 For convienience, this definition ignores minimum end-berth and inter-ship clearance 362 Chia Jim Tong, Hoong Chuin Lau, and Andrew Lim	ant colony optimization algorithms;bin packing problem;central processing unit;combinatorial optimization;decision problem;distributed computing;distributed memory;end-to-end principle;expanded memory;fragmentation (computing);information design;intelligent agent;lecture notes in computer science;local search (optimization);mathematical optimization;memory management;np-completeness;numerical analysis;optic axis of a crystal;optimization problem;parallel computing;positive feedback;program optimization;programming paradigm;quadratic assignment problem;randomized algorithm;sbp;springer (tank);travelling salesman problem;with high probability	Chia Jim Tong;Hoong Chuin Lau;Andrew Lim	1999		10.1007/3-540-46674-6_30	mathematical optimization;ant colony optimization algorithms;np-complete;combinatorial optimization;computer science;artificial intelligence;local search;mathematics;parallel algorithm;algorithm	AI	28.217940530123567	12.01087746784484	61689
dce81a3c4b27eb47ece7015711ba0973b835bbdf	research on solution space of bipartite graph vertex-cover by maximum matchings		Some rigorous results and statistics of the solution space of Vertex-Covers on bipartite graphs are given in this paper. Based on the König’s theorem, an exact solution space expression algorithm is proposed and statistical analysis of the nodes’ states is provided. The statistical results fit well with the algorithmic results until the emergence of the unfrozen core, which makes the fluctuation of statistical quantities and causes the replica symmetric breaking in the solutions. Besides, the entropy of bipartite Vertex-Cover solutions is calculated with the clustering entropy using a cycle simplification technique for the unfrozen core. Furthermore, as generalization of bipartite graphs, bipartite core graph is proposed, the solution space of which can also be easily determined; and based on these results, how to generate a König−Egerváry subgraph is studied by a growth process of adding edges. The investigation of solution space of bipartite graph Vertex-Cover provides intensive understanding and some insights on the solution space complexity, and will produce benefit for finding maximal König−Egerváry subgraphs, solving general graph Vertex-Cover and recognizing the intrinsic hardness of NP-complete problems.	algorithm;cluster analysis;dspace;emergence;feasible region;karp's 21 np-complete problems;könig's lemma;level of detail;matching (graph theory);maximal set;universal conductance fluctuations;vertex cover	Wei Wei;Yunjia Zhang;Ting Wang;Baifeng Li;Baolong Niu;Zhiming Zheng	2015	CoRR		mathematical optimization;combinatorics;discrete mathematics;bipartite graph;mathematics;blossom algorithm	ML	36.348542094265746	13.611974446843305	61830
face2f1f1b5fcf23586241fd7589d800d45df0bd	competitiveness maximization on complex networks	algorithm design and analysis approximation algorithms mathematical model heuristic algorithms complex networks linear programming;submodular algorithm design competitive dynamics np hard optimization	We consider a model of competition on complex networks, in which two competitors are fixed to opposite states while other agents, called normal agents, adjust their states according to a distributed consensus protocol. Suppose that one of the competitors could enhance its influence by creating new links. A natural question is, when the number of new links is limited due to the limited resource, how to add these links so as to maximize the influence of the given competitor over the other one (called competitiveness). We consider two competitiveness maximization problems: Problem 1 tries to maximize the number of supporters of the competitor, while Problem 2 tries to maximize the total supporting degree of normal agents toward the competitor. We prove that Problem 1 is NP-hard. We also show that the objective function of Problem 2 is monotonous and submodular, and hence there exists a polynomial-time greedy algorithm (GA) approximately solving Problem 2. Several centrality-based heuristic algorithms of less computational burden are also designed to provide approximate solutions to these two problems. We carry out extensive simulations to check the performances of these algorithms in six real networks. We find that GA always provides the best approximate solution to Problem 2, while for Problem 1, GA only has the best performance in directed networks. Furthermore, among those heuristic algorithms, an algorithm based on centrality in descending order is better than its counterpart in ascending order in solving Problem 2 in statistical sense. But for Problem 1, the performance of the centrality-based heuristic algorithms is more sensitive to the network structure and the locations of competitors.	approximation algorithm;centrality;competitive analysis (online algorithm);complex network;consensus (computer science);elementary;entropy maximization;expectation–maximization algorithm;game theory;greedy algorithm;heuristic;loss function;np-hardness;network topology;optimization problem;performance;polynomial;regular expression;simulation;software release life cycle;sorting;submodular set function;time complexity	Jiuhua Zhao;Qipeng Liu;Lin Wang;Xiao Fan Wang	2018	IEEE Transactions on Systems, Man, and Cybernetics: Systems	10.1109/TSMC.2016.2636240	competitive analysis;mathematical optimization;combinatorics;criss-cross algorithm;computer science;machine learning;mathematics	AI	25.77069142027491	5.90230571190646	62018
41d0cc6f03ed48368c7379592d50800ca9d487a4	local minimax complexity of stochastic convex optimization		We extend the traditional worst-case, minimax analysis of stochastic convex optimization by introducing a localized form of minimax complexity for individual functions. Our main result gives function-specific lower and upper bounds on the number of stochastic subgradient evaluations needed to optimize either the function or its “hardest local alternative” to a given numerical precision. The bounds are expressed in terms of a localized and computational analogue of the modulus of continuity that is central to statistical minimax analysis. We show how the computational modulus of continuity can be explicitly calculated in concrete cases, and relates to the curvature of the function at the optimum. We also prove a superefficiency result that demonstrates it is a meaningful benchmark, acting as a computational analogue of the Fisher information in statistical estimation. The nature and practical implications of the results are demonstrated in simulations.	benchmark (computing);best, worst and average case;computation;convex optimization;estimation theory;fisher information;mathematical optimization;minimax;modulus of continuity;numerical analysis;scott continuity;simulation;subderivative;subgradient method	Sabyasachi Chatterjee;John C. Duchi;John D. Lafferty;Yuancheng Zhu	2016			mathematical optimization;combinatorics;mathematics;mathematical economics;statistics	ML	34.39226805943547	5.08614596252551	62079
8ca28a4b41c9e642f83ac88fc8ff888ce264bdc1	designing domain-particle dynamics in cellular automata			cellular automaton	Mark Leppard	2013	J. Cellular Automata		combinatorics;cellular automaton;stochastic cellular automaton;mathematics;discrete mathematics;particle;mobile automaton	Theory	38.188300657554414	9.405637956026686	62464
0ed8050ca917d5aeac40337e8add6927e877b0d9	suboptimal measures of predictive complexity for absolute loss function	prediccion;secuencia binaria;binary sequence;ley kolmogorov;fonction perte;funcion perdida;jeu perte absolue;kolmogorov complexity;complexity measure;complexite predictive;loss function;kolmogorov law;mesure complexite;sequence binaire;loi kolmogorov;prediction;medida complexidad	The problem of existence of predictive complexity for the absolute loss game is studied. Predictive complexity is a generalization of Kolmogorov complexity which bounds the ability of any algorithm to predict elements of a sequence of outcomes. For perfectly mixable loss functions (logarithmic and squared difference are among them) predictive complexity is defined like Kolmogorov complexity to within an additive constant. The absolute loss function is not perfectly mixable, and the question of existence of the corresponding predictive complexity, which is defined to within an additive constant, is open. We prove that in the case of the absolute loss game the predictive complexity can be defined to within an additive term O( √ n), where n is the length of a sequence of outcomes. We prove also that in some restricted settings this bound cannot be improved. C © 2002 Elsevier Science (USA)	algorithm;computational complexity theory;kolmogorov complexity;loss function;utility functions on indivisible goods	Vladimir V. V'yugin	2002	Inf. Comput.	10.1006/inco.2001.3066	kolmogorov structure function;combinatorics;average-case complexity;prediction;computer science;calculus;pseudorandom binary sequence;mathematics;statistics;loss function	Theory	36.05049178683985	11.423543890573573	62979
e0f419e616d900c6b48eb38148d3ea706fca4e26	formulation of fuzzy linear programming problems as four-objective constrained optimization problems	optimisation sous contrainte;multi objective linear programming;constrained optimization;optimal solution;multiobjective programming;programmation multiobjectif;optimisation;solution optimale;fuzzy programming;optimizacion;numero difuso;maximization;fuzzy number;nombre flou;optimization method;metodo optimizacion;journal article;optimizacion con restriccion;objective function;programacion lineal;solucion optima;methode optimisation;linear programming;programmation lineaire;linear program;fuzzy linear programming;optimization;programmation floue;constrained optimization problem;maximizacion;multi objective optimization problem;programacion difusa;maximisation;programacion multiobjetivo	This paper concerns the solution of fuzzy linear programming (FLP) problems which involve fuzzy numbers in coefficients of objective functions. Firstly, a number of concepts of optimal solutions to FLP problems are introduced and investigated. Then, a number of theorems are developed so as to convert the FLP to a multi-objective optimization problem with four-objective functions. Finally, two illustrative examples are given to demonstrate the solution procedure. It also shows that our method of solution includes an existing method as a special case.	constrained optimization;linear programming;mathematical optimization	Guangquan Zhang;Yonghong Wu;M. Remias;Jie Lu	2003	Applied Mathematics and Computation	10.1016/S0096-3003(02)00202-3	mathematical optimization;combinatorics;linear programming;fuzzy number;mathematics;algorithm	Theory	25.438942834017737	10.084797820668793	63187
04d0a5e7fda7f56a09f097e2b7daaa73aaa6a939	optimal search with uncertain sweep width		This paper considers file problem of optimal search for a stationary target when the detection capability of the search sensor, as characterized by its sweep width, is fixed but not known in advance. Most of the results are confined to the assumption of a gamma prior sweep-width distribution and to the cases of normal and uniform prior target-location distributions. Explicit formulas are obtained for the optimal search plans, the probability of detection versus time, the expected time to detection, and the posterior marginal distributions for target location and sweep width. In the case of a normal prior target-location distribution, expected time to detection is compared for the optimal plan and search plans based on the assumption of a known value for sweep width; this includes a sensitivity analysis treating actual sweep width as a parameter. For a target of value, the search plan that maximizes expected net return is determined. This plan requires specification of a rule for terminating an unsuccessfu...		Henry R. Richardson;Barry Belkin	1972	Operations Research	10.1287/opre.20.4.764	econometrics;mathematical optimization;mathematics;statistics	DB	38.118704125408634	6.9488888198327	63196
99f735f1c4865bfafcf6eb199d60eeedfc66658a	on strong optimality of interval linear programming		We consider a linear programming problem with interval data. We discuss the problem of checking whether a given solution is optimal for each realization of interval data. This problem was studied for particular forms of linear programming problems. Herein, we extend the results to a general model and simplify the overall approach. Moreover, we inspect computational complexity, too. Eventually, we investigate a related optimality concept of semi-strong optimality, showing its characterization and complexity.	linear programming	Milan Hladík	2017	Optimization Letters	10.1007/s11590-016-1088-3	mathematical optimization;combinatorics;discrete mathematics;mathematics	EDA	25.104897419635073	11.97090199597153	63427
b3efc709f11093db41a5c53d295ae766a09688eb	learning and approximation algorithms for problems motivated by evolutionary trees	approximate algorithm;qa76 electronic computers computer science computer software;markov model;evolutionary trees;qa76 electronic computers computer science computer software computer software	"""In this thesis we consider some computational problems motivated by the biological problem of reconstructing evolutionary trees. In this thesis, we are concerned with the design and analysis of efficient algorithms for clearly defined combinatorial problems motived by this application area. We present results for two different kinds of problem. Our first problem is motivated by models of evolution that describe the evolution of biological species in terms of a stochastic process that alters the DNA of species. The particular stochastic model that we considered is called the Two-State General Markov Model. In this model, an evolutionary tree can be associated with a distribution on the different """"patterns"""" that may appear among the sequences for all the species in the evolutionary tree. Then the data for a collection of species whose evolutionary tree is unknown can be viewed as samples from this (unknown) distribution. An interesting problem asks whether we can use samples from an unknown evolutionary tree M to find another tree M*for those species, so that the distribution of M* is similar to that of M. This is essentially a PAC-learning problem (""""Probably Approximately Correct"""") in the sense of Valiant and Kearns et al.. Our results show that evolutionary trees in the Two-State General Markov can be efficiently PAC-learned in the variation distance metric using a """"reasonable"""" number of samples. The two other problems that we consider are combinatorial problems that are also motivated by evolutionary tree construction. The input to each of these problems consists of a fixed tree topology whose leaves are bijectively labelled by the elements of a species set, as well as data for those species. Both problems involve labelling the internal nodes in the fixed topology in order to minimize some function on that tree (both functions that we consider are assumed to test the quality of the tree topology in some way). The two problems that we consider are known to be NP-hard. Our contribution is to present efficient approximation algorithms for both problems."""	approximation algorithm;phylogenetic tree	Mary Elizabeth Cryan	1999			combinatorics;theoretical computer science;tree rearrangement;k-ary tree;mathematics;tree structure;tree traversal;algorithm	Theory	35.73835684378561	14.904486567488718	63525
83575730451e654e73e35a07ce227c7e5fc5c8be	parallel decomposition of multicommodity network flows using a linear-quadratic penalty algorithm	networks;programming transportation;linear quadratic;network flow	The central theme of this paper is the design and evaluation of a parallel decomposition algorithm for multicommodity network flow problems based on the notion of piecewise linear-quadratic penalty (LQP) functions. The algorithm induces separability of both the constraint set and the objective function by commodity during the subproblem phase. Hence it is suitable for implementations on coarse grain parallel architectures. A master problem, of significantly smaller dimension than the original problem, uses dense linear algebra computations. Hence it exploits a vector architecture, but is also suited for parallel implementation and can be executed using Basic Linear Algebra (BLAS) subroutines. Computational results on a CRAY Y-MPE264 supercomputer with a set of large multicommodity network flow problems drawn from a military application are presented and analyzed. INFORMS Journal on Computing, ISSN 1091-9856, was published as ORSA Journal on Computing from 1989 to 1995 under ISSN 0899-1499.	algorithm	Mustafa Ç. Pinar;Stavros A. Zenios	1992	INFORMS Journal on Computing	10.1287/ijoc.4.3.235	mathematical optimization;flow network;computer science;theoretical computer science;mathematics;algorithm	HPC	27.770881022455512	9.1957754731102	64042
020cd5a9b3cfc991b43867b30b51d316a9486839	submodular point processes with applications to machine learning		We introduce a class of discrete point processes that we call the Submodular Point Processes (SPPs). These processes are characterized via a submodular (or supermodular) function, and naturally model notions of information, coverage and diversity, as well as cooperation. Unlike Log-submodular and Log-supermodular distributions (Log-SPPs) such as determinantal point processes (DPPs), SPPs are themselves submodular (or supermodular). In this paper, we analyze the computational complexity of probabilistic inference in SPPs. We show that computing the partition function for SPPs (and Log-SPPs), requires exponential complexity in the worst case, and also provide algorithms which approximate SPPs up to polynomial factors. Moreover, for several subclasses of interesting submodular functions that occur in applications, we show how we can provide efficient closed form expressions for the partition functions, and thereby marginals and conditional distributions. We also show how SPPs are closed under mixtures, thus enabling maximum likelihood based strategies for learning mixtures of submodular functions. Finally, we argue how SPPs complement existing LogSPP distributions, and are a natural model for several applications.	approximation algorithm;best, worst and average case;computational complexity theory;distribution (mathematics);machine learning;partition function (mathematics);polynomial;submodular set function;supermodular function;time complexity	Rishabh K. Iyer;Jeff A. Bilmes	2015			mathematical optimization;combinatorics;discrete mathematics;submodular set function;mathematics	ML	34.74158369625622	7.475818328931927	64072
742a23e135fd6d8410d55d5f956f3234e616f196	on the smallest possible dimension and the largest possible margin of linear arrangements representing given concept classes	margin bounds;matrix rigidity;learning;espace euclidien;dimension bound;rigidite matricielle;linear arrangement;margin bound;espacio euclidiano;feature space;aprendizaje;dimension bounds;apprentissage;linear arrangements;borne dimension;arrangement lineaire;informatique theorique;classification system;euclidean space;learning theory;theorie complexite;borne marginale;theorie apprentissage;computer theory;informatica teorica	This paper discusses theoretical limitations of classification systems that are based on feature maps and use a separating hyperplane in the feature space. In particular, we study the embeddability of a given concept class into a class of Euclidean half spaces of low dimension, or of arbitrarily large dimension but realizing a large margin. New bounds on the smallest possible dimension or on the largest possible margin are presented. In addition, we present new results on the rigidity of matrices and briefly mention applications in complexity and learning theory.		Jürgen Forster;Hans Ulrich Simon	2006	Theor. Comput. Sci.	10.1016/j.tcs.2005.10.015	inductive dimension;combinatorics;topology;feature vector;effective dimension;computer science;euclidean space;learning theory;mathematics;geometry;minkowski–bouligand dimension;algorithm	Theory	33.76599334661771	13.682382642485791	64297
e47d0e7073d3b7565207c1c0aff77df82e380ced	probability of graphs with large spectral gap by multicanonical monte carlo	random graph;spectral gap;multicanonical monte carlo;large deviation;ramanujan graph;regular graph	Graphs with large spectral gap are important in various fields such as biology, sociology and computer science. In designing such graphs, an important question is how the probability of graphs with large spectral gap behaves. A method based on multicanonical Monte Carlo is introduced to quantify the behavior of this probability, which enables us to calculate extreme tails of the distribution. The proposed method is successfully applied to random 3-regular graphs and large deviation probability is estimated.	monte carlo method	Nen Saito;Yukito Iba	2011	Computer Physics Communications	10.1016/j.cpc.2010.06.039	random graph;combinatorics;discrete mathematics;dynamic monte carlo method;hybrid monte carlo;regular graph;wang and landau algorithm;mathematics;statistics;monte carlo method	Theory	37.973148032372244	14.495090599021793	64382
1b4e9fe167eff7c4adbaa3311b07def2bef02774	on smoothed analysis in dense graphs and formulas	satisfiability;relational model;smoothed analysis;random graph	We study a model of random graphs, where a random instance is obtained by adding random edges to a large graph of a given density. The research on this model has been started by Bohman and colleagues (Random Struct Algor 22 (2003), 33–42; Random Struct Algor 24 (2004), 105–117). Here we obtain a sharp threshold for the appearance of a fixed subgraph and for certain Ramsey properties. We also consider a related model of random k-SAT formulas, where an instance is obtained by adding random k-clauses to a fixed formula with a given number of clauses, and derive tight bounds for the non-satisfiability of the thus-obtained random formula. © 2005 Wiley Periodicals, Inc. Random Struct. Alg., 29, 180–193, 2006	boolean satisfiability problem;john d. wiley;random graph;smoothed analysis;smoothing;struct (c programming language)	Michael Krivelevich;Benny Sudakov;Prasad Tetali	2006	Random Struct. Algorithms	10.1002/rsa.20097	random variate;smoothed analysis;random regular graph;random graph;combinatorics;random permutation;discrete mathematics;random field;relational model;multivariate random variable;random element;random compact set;sum of normally distributed random variables;random function;stochastic simulation;mathematics;convolution random number generator;random walk;algorithm;statistics;satisfiability	Theory	37.97642535017127	15.789238535778052	64784
50406cdf67f61b705294845be399bf33b120aa3a	band search: an efficient alternative to guided depth-first search	best first searches;iterative algorithms;high performance computing;search algorithm;contracts;operations research;iterative methods;guided depth first search;counting circuits;search trees;anomalous behavior;bandwidth;artificial intelligence;depth first search;search problems;anomalous behavior guided depth first search search algorithm band search best first searches;band search;iterative algorithms counting circuits iterative methods operations research high performance computing contracts artificial intelligence algorithm design and analysis bandwidth;algorithm design and analysis	In this paper, we propose a novel search algorithm called band search that generalizes guided depth-first and best-first searches. The search allocates a band of at most W nodes in each level of the search tree for storing active nodes in that level, D priority lists, one for each level, for storing overflow nodes, and D counters, one for each level, for keeping track of and to limit the degree of backtracking allowed. The algorithm has three major features: a) it selects for expansion in a best-first fashion from all nodes in the bands, b) it moves nodes from an overflow list into the corresponding band in a depth-first fashion, and c) it restricts backtracking so that at most W nodes in a level are fully searched before allowing new nodes in this level into the band. The algorithm specializes to be a guided depth-first search (GDFS) when W is one, and a best-first search (BFS) when W is unlimited. By choosing W appropriately, we show empirically that the algorithm can often outperform GDFS and has performance close to that of BFS. Moreover, the algorithm can be used instead of GDFS in iterative searches, such as IDA*, MIDA*, and DFS*. Finally, we identify conditions upon which the algorithm behaves like BFS, and the anomalous behavior when the algorithm performs worse than GDFS.	backtracking;best-first search;depth-first search;emoticon;global data flash storage;iterative method;norm (social);search algorithm;search tree	Lon-Chan Chu;Benjamin W. Wah	1992		10.1109/TAI.1992.246360	linear search;interpolation search;beam search;algorithm design;mathematical optimization;supercomputer;bidirectional search;breadth-first search;beam stack search;computer science;artificial intelligence;local search;theoretical computer science;machine learning;jump search;incremental heuristic search;iterative method;best-first search;combinatorial search;bandwidth;algorithm;guided local search;binary search algorithm;search algorithm	AI	28.677968390780958	5.788785350542505	64792
97a781fd293bb93bcae2966c556bf130dc765aa1	tight upper bounds on the cardinality constrained mean-variance portfolio optimization problem using truncated eigendecomposition	mathematics;matematik	Modern portfolio theory is about determining how to distribute capital among available securities such that, for a given level of risk, the expected return is maximized, or for a given level of return, the associated risk is minimized. In the pioneering work of Markowitz in 1952, variance was used as a measure of risk, which gave rise to the wellknown mean-variance portfolio optimization model. Although other mean-risk models have been proposed in the literature, the mean-variance model continues to be the backbone of modern portfolio theory and it is still commonly applied. The scope of this thesis is a solution technique for the mean-variance model in which eigendecomposition of the covariance matrix is performed.The first part of the thesis is a review of the mean-risk models that have been suggested in the literature. For each of them, the properties of the model are discussed and the solution methods are presented, as well as some insight into possible areas of future research.The second part of the thesis is two research papers. In the first of these, a solution technique for solving the mean-variance problem is proposed. This technique involves making an eigendecomposition of the covariance matrix and solving an approximate problem that includes only relatively few eigenvalues and corresponding eigenvectors. The method gives strong bounds on the exact solution in a reasonable amount of computing time, and can thus be used to solve large-scale mean-variance problems.The second paper studies the mean-variance model with cardinality constraints, that is, with a restricted number of securities included in the portfolio, and the solution technique from the first paper is extended to solve such problems. Near-optimal solutions to large-scale cardinality constrained mean-variance portfolio optimization problems are obtained within a reasonable amount of computing time, compared to the time required by a commercial general-purpose solver.	optimization problem	Fred Mayambala;Elina Rönnberg;Torbjörn Larsson	2014		10.1007/978-3-319-28697-6_54	mathematical optimization;calculus;portfolio optimization;mathematics;mathematical economics	Logic	31.75214532767861	7.61080673514032	65058
03224fc425eb4841598fdd433a4d80001f23ad77	the distribution of patterns in random trees	limit distribution;labeled tree;random tree;discrete mathematics;asymptotic normal;asymptotic equivalence	Let Tn denote the set of unrooted labeled trees of size n and let M be a particular (finite, unlabeled) tree. Assuming that every tree of Tn is equally likely, it is shown that the limiting distribution as n goes to infinity of the number of occurrences of M as an induced subtree is asymptotically normal with mean value and variance asymptotically equivalent to μn and σn, respectively, where the constants μ > 0 and σ ≥ 0 are computable.	computable function;tree (data structure)	Frédéric Chyzak;Michael Drmota;Thomas Klausner;Gerard Kok	2008	Combinatorics, Probability & Computing	10.1017/S0963548307008425	random binary tree;combinatorics;mathematical analysis;discrete mathematics;mathematics	Theory	38.73418875476939	16.676867045526354	65076
0177104b5a0fcde1182100c6ff642ece08644ad7	least squares temporal difference methods: an analysis under general conditions	90c40;65c05;90c39;temporal difference methods;tekninen raportti;approximate dynamic programming;importance sampling;article;markov decision processes;markov chains	We consider approximate policy evaluation for finite state and action Markov decision processes (MDP) with the least squares temporal difference (LSTD) algorithm, LSTD(λ), in an exploration-enhanced learning context, where policy costs are computed from observations of a Markov chain different from the one corresponding to the policy under evaluation. We establish for the discounted cost criterion that LSTD(λ) converges almost surely under mild, minimal conditions. We also analyze other properties of the iterates involved in the algorithm, including convergence in mean and boundedness. Our analysis draws on theories of both finite space Markov chains and weak Feller Markov chains on a topological space. Our results can be applied to other temporal difference algorithms and MDP models. As examples, we give a convergence analysis of a TD(λ) algorithm and extensions to MDP with compact state and action spaces, as well as a convergence proof of a new LSTD algorithm with state-dependent λ-parameters.	approximation algorithm;least squares;markov chain;markov decision process;petri net;temporal difference learning;theory	Huizhen Yu	2012	SIAM J. Control and Optimization	10.1137/100807879	markov decision process;markov chain;mathematical optimization;markov kernel;combinatorics;discrete mathematics;partially observable markov decision process;importance sampling;mathematics;markov model;statistics	ML	38.33177399868276	5.054465722015885	65160
ea0df4099fe201daf846c10cbe2b119fc9939189	sets of approximating functions with finite vapnik-chervonenkis dimension for nearest-neighbors algorithms	k nearest neighbors;structural risk minimization;complexity selection;statistical learning theory;nearest neighbor;k nearest neighbor;cross validation;vapnik chervonenkis;generalization;vapnik chervonenkis dimension;vc dimension	According to a certain misconception sometimes met in the literature: for the nearest-neighbors algorithms there is no fixed hypothesis class of limited Vapnik-Chervonenkis dimension. In the paper a simple reformulation (not a modification) of the nearest-neighbors algorithm is shown where instead of a natural number k, a percentage @a@?(0,1) of nearest neighbors is used. Owing to this reformulation one can construct sets of approximating functions, which we prove to have finite VC dimension. In a special (but practical) case this dimension is equal to @?2/@a@?. It is also then possible to form a sequence of sets of functions with increasing VC dimension, and to perform complexity selection via cross-validation or similarly to the structural risk minimization framework. Results of such experiments are also presented.	alexey chervonenkis;algorithm;vc dimension	Przemyslaw Klesk;Marcin Korzeń	2011	Pattern Recognition Letters	10.1016/j.patrec.2011.07.012	combinatorics;discrete mathematics;structural risk minimization;effective dimension;vc dimension;computer science;machine learning;packing dimension;mathematics;k-nearest neighbors algorithm	Theory	34.239550938608254	13.073638231862963	65426
4e3ec923bf91f9cc3d53c135281a49325a5c6649	optimal adaptive policies for markov decision processes	dynamic programming;theorie indice;optimisation;optimizacion;systeme discret;proceso markov;adaptive control;index theory;decision markov;teoria indice;estimator;estimador;control adaptativo;processus markov;markov process;commande adaptative;markov decision;optimization;sequential estimation;markov decision process;sistema discreto;markov decision processes;discrete system;estimateur	In this paper we consider the problem of adaptive control for Markov Decision Processes. We give the explicit form for a class of adaptive policies that possess optimal increase rate properties for the total expected finite horizon reward, under sufficient assumptions of finite state-action spaces and irreducibility of the transition law. A main feature of the proposed policies is that the choice of actions, at each state and time period, is based on indices that are inflations of the right-hand side of the estimated average reward optimality equations.	markov chain;markov decision process	Apostolos Burnetas;Michael N. Katehakis	1997	Math. Oper. Res.	10.1287/moor.22.1.222	markov decision process;econometrics;mathematical optimization;adaptive control;mathematics;statistics	ML	38.37230338391474	5.739046030633679	66891
176945c3ddd4503976bc44bc56f8695f2848d753	how to recycle your facets	linear ordering polytope;linear order;linear ordering problem;set covering polytope;facet;0 1 polytope;acyclic subgraph polytope;set cover	We show how to transform any inequality defining a facet of some 0/1-polytope into an inequality defining a facet of the acyclic subgraph polytope. While this facet-recycling procedure can potentially be used to construct ‘nasty’ facets, it can also be used to better understand and extend the polyhedral theory of the acyclic subgraph and linear ordering problems.	directed acyclic graph;polyhedron;social inequality	Samuel Fiorini	2006	Discrete Optimization	10.1016/j.disopt.2005.10.007	mathematical optimization;combinatorics;uniform k 21 polytope;discrete mathematics;facet;convex polytope;linear programming;birkhoff polytope;mathematics;set cover problem;vertex enumeration problem;total order;rectification	Theory	25.5467637716097	13.877262814375053	66929
493169c010b818b96dfc2f145f3ff7f72a460bd0	heuristics for a matrix symmetrization problem	unsymmetric sparse matrix;bipartite matching;matrix symmetrization;sparse matrix	We consider the following problem: given a square, nonsymmetric, (0, 1)-matrix, find a permutation of its columns that yields a zero-free diagonal and maximizes the symmetry. The problem is known to be NP-hard. We propose a fast iterative-improvement based heuristic and evaluate the performance of the heuristic on a large set of matrices.	column (database);experiment;heuristic (computer science);iterative method;np-hardness;the matrix	Bora Uçar	2007		10.1007/978-3-540-68111-3_75	mathematical optimization;generalized permutation matrix;hollow matrix;combinatorics;discrete mathematics;bipartite graph;sparse matrix;nonnegative matrix;single-entry matrix;permutation matrix;band matrix;square matrix;mathematics;diagonalizable matrix;state-transition matrix;block matrix;diagonal matrix;matrix;integer matrix;symmetric matrix	ML	26.77273565469504	13.647018319168781	67207
162ca0f8ed78b89ed76d7fc3b081d5b8dd0c3d6c	approximate solution of the multiple watchman routes problem with restricted visibility range	camino mas corto;traveling salesman problem;restricted visibility range;estructura geometrica;graph theory;shortest path;geometrical structure;visibilite;galeria arte;consecutive traveling salesman problem;visibilidad;self organizing maps;complexity theory;structure geometrique;self organizing map based adaptation procedure;multiple watchman route problem;sensors;approximation algorithms;consecutive traveling salesman problem approximate solution multiple watchman route problem restricted visibility range self organizing map based adaptation procedure polygonal domain connected neuron weight art gallery problem;travelling salesman problem;multiple solution;geometry;plus court chemin;watchman route problem self organizing maps;anneau;euclidean distance;travelling salesman problems approximation theory geometry graph theory self organising feature maps;kohonen algorithm;problema viajante comercio;approximation theory;algoritmo kohonen;visibility;polygonal domain;algorithme kohonen;self organising feature maps;connected neuron weight;watchman route problem;probleme commis voyageur;solution multiple;approximate solution;travelling salesman problems;autoorganizacion;cities and towns;self organization;self organized map;approximation methods;algorithms computer simulation maps as topic neural networks computer;neurons;ring;art gallery problem;reseau neuronal;solucion multiple;galerie art;art gallery;red neuronal;geometric structure;autoorganisation;anillo;cities and towns approximation methods neurons complexity theory approximation algorithms euclidean distance sensors;neural network	In this paper, a new self-organizing map (SOM) based adaptation procedure is proposed to address the multiple watchman route problem with the restricted visibility range in the polygonal domain W. A watchman route is represented by a ring of connected neuron weights that evolves in W, while obstacles are considered by approximation of the shortest path. The adaptation procedure considers a coverage of W by the ring in order to attract nodes toward uncovered parts of W. The proposed procedure is experimentally verified in a set of environments and several visibility ranges. Performance of the procedure is compared with the decoupled approach based on solutions of the art gallery problem and the consecutive traveling salesman problem. The experimental results show the suitability of the proposed procedure based on relatively simple supporting geometrical structures, enabling application of the SOM principles to watchman route problems in W.	acclimatization;approximation;art gallery problem;code coverage;computation;computational geometry;experiment;np (complexity);neuron;node - plant part;organizing (structure);self-organization;self-organizing map;short;shortest path problem;solutions;sony watchman;travelling salesman problem;visibility (geometry);watchman route problem;weight	Jan Faigl	2010	IEEE Transactions on Neural Networks	10.1109/TNN.2010.2070518	mathematical optimization;combinatorics;graph theory;machine learning;mathematics;travelling salesman problem;artificial neural network;algorithm	Robotics	28.742397343401525	4.410042716940494	67212
1dd8db60043f51c04eb7200915ebd253d2fabf64	fast random walk with restart and its applications	graph theory;random walk with restart;multiplication operator;matrix inversion;connection subgraphs;image generation;graph partitioning;low rank matrix approximation;community structure;random walk;random processes;on the fly;matrix inversion random walk with restart weighted graph connection subgraphs low rank matrix approximation sherman morrison lemma graph partitioning;matrix approximation;costs sparse matrices delay image storage niobium time factors heart linearity error analysis design optimization;weighted graph;sparse matrix;random processes graph theory;adjacency matrix;low rank approximation;sherman morrison lemma	"""How closely related are two nodes in a graph? How to compute this score quickly, on huge, disk-resident, real graphs? Random walk with restart (RWR) provides a good relevance score between two nodes in a weighted graph, and it has been successfully used in numerous settings, like automatic captioning of images, generalizations to the """"connection subgraphs"""", personalized PageRank, and many more. However, the straightforward implementations of RWR do not scale for large graphs, requiring either quadratic space and cubic pre-computation time, or slow response time on queries. We propose fast solutions to this problem. The heart of our approach is to exploit two important properties shared by many real graphs: (a) linear correlations and (b) block- wise, community-like structure. We exploit the linearity by using low-rank matrix approximation, and the community structure by graph partitioning, followed by the Sherman- Morrison lemma for matrix inversion. Experimental results on the Corel image and the DBLP dabasets demonstrate that our proposed methods achieve significant savings over the straightforward implementations: they can save several orders of magnitude in pre-computation and storage cost, and they achieve up to 150x speed up with 90%+ quality preservation."""	approximation;computation;cubic function;emoticon;graph (discrete mathematics);graph partition;pagerank;personalization;precomputation;relevance;response time (technology);running with rifles;singular value decomposition;time complexity	Hanghang Tong;Christos Faloutsos;Jia-Yu Pan	2006	Sixth International Conference on Data Mining (ICDM'06)	10.1109/ICDM.2006.70	multiplication operator;mathematical optimization;combinatorics;discrete mathematics;sparse matrix;graph partition;graph theory;machine learning;mathematics;random walk;community structure;adjacency matrix;statistics;low-rank approximation	DB	34.67954777490322	11.052848798217688	67319
5a11f0d225b3cc50e42a37bbc9323fedcb5cdc31	a load-balanced algorithm for parallel digital image warping	parallelisme;algoritmo paralelo;image numerique;parallel algorithm;geometric transformation;image processing;communications;equilibrio de carga;gauchissement;equilibrage charge;procesamiento imagen;traitement image;algorithme parallele;parallelism;paralelismo;transformacion geometrica;imagen numerica;torcimiento;transformation geometrique;load balancing;load balance;digital image;communication;comunicacion;warping	This paper introduces and compares three parallel algorithms to compute general geometric image transformations on MIMD machines. We propose three variants of a parallel general scheme. We focus on the load balancing and the data redistributions. Experimental results are reported and compared. The implementation has been done using PPCM library allowing us to run the program over different parallel machines. We compare logical communication schemes for message-passing machines. Since our parallel algorithm needs global communications such as multiscatters, we study the efficiency of two different logical topologies usable with PPCM. These studies allow us to find the best combination of algorithm and virtual topology to use on a given parallel machine.	algorithm;digital image;image warping	Sylvain Contassot-Vivier;Serge Miguet	1999	IJPRAI	10.1142/S0218001499000276	parallel computing;image processing;computer science;load balancing;theoretical computer science;algorithm	HCI	33.98653257210097	11.559947859131103	67629
86ed8c75661e73c02a78f8b373284ba6a8e684af	learning nested differences in the presence of malicious noise	learning nested differences;malicious noise	We investigate the learnability of nested differences of intersection-closed classes in the presence of malicious noise. Examples of intersection-closed classes include axis-parallel rectangles, monomials, linear sub-spaces, and so forth. We present an on-line algorithm whose mistake bound is optimal in the sense that there are concept classes for which each learning algorithm (using nested differences as hypotheses) can be forced to make at least that many mistakes. We also present an algorithm for learning in the PAC model with malicious noise. Surprisingly enough, the noise rate tolerable by these algorithms does not depend on the complexity of the target class but depends only on the complexity of the underlying intersection-closed class.		Peter Auer	1995		10.1007/3-540-60454-5_33	combinatorics;discrete mathematics;weighted majority algorithm;computer science;machine learning;mathematics;algorithm;population-based incremental learning	Crypto	33.770598458851836	13.06685576331905	67712
61c607a49ef803f8595656943e74d2c07dfa22f8	constructing general dual-feasible functions	integer linear programming;generalization;dual feasible functions	Dual-feasible functions have proved to be very effective for generating fast lower bounds and valid inequalities for integer linear programs with knapsack constraints. However, a significant limitation is that they are defined only for positive arguments. Extending the concept of dual-feasible function to the general domain and range R is not straightforward. In this paper, we propose the first construction principles to obtain general functions with domain and range R , and we show that they lead to non-dominated maximal functions.		Jürgen Rietz;Cláudio Alves;José M. Valério de Carvalho;François Clautiaux	2015	Oper. Res. Lett.	10.1016/j.orl.2015.06.002	generalization;mathematical optimization;combinatorics;discrete mathematics;integer programming;mathematics	Crypto	24.635924869759204	11.836813731822238	67726
7f93cfd2bed56f42d6d101baa53dbdadae2fce5d	on-line motion planning: case of a planar rod	motion planning;lower bound	In this paper we develop an algorithm for planning the motion of a planar “rod” (a line segment) amidst obstacles bounded by simple, closed polygons. The exact shape, number and location of the obstacles are assumed unknown to the planning algorithm, which can only obtain information about the obstacles by detecting points of contact with the obstacles. The ability to detect contact with obstacles is formalized by move primitives that we callguarded moves. We call ours theon-line motion planning problem as opposed to the usualoff-line version. This is a significant departure from the usual setting for motion planning problems. What we demonstrate is that the retraction method can be applied, although new issues arise that have no counterparts in the usual setting. We are able to obtain an algorithm with path complexityO(n 2) guarded moves, wheren is the number of obstacle corners. This matches the known lower bound. The computational complexityO(n 2logn) of our algorithm matches the best known algorithm for the off-line version.	algorithm;automated planning and scheduling;computation;motion planning;online and offline;sensor	James Cox;Chee-Keng Yap	1991	Annals of Mathematics and Artificial Intelligence	10.1007/BF01530886	mathematical optimization;combinatorics;simulation;computer science;artificial intelligence;mathematics;motion planning;upper and lower bounds	AI	31.427860549551404	17.722073483833178	67814
8bd365d737ec122be204bf9593febfb2f309c158	network null model based on maximal entropy and the rich-club	cs si;journal article;physics soc ph;stat me	We present a method to construct a network null–model based on the maximum entropy principle and where the restrictions that the rich– club and the degree sequence impose are conserved. We show that the probability that two nodes share a link can be described with a simple probability function. The null–model closely approximates the assortative properties of the network.	approximation algorithm;coefficient;degree (graph theory);lagrange multiplier;maximal set;null model;principle of maximum entropy;ratio club;sorting	Raul J. Mondragón	2014	J. Complex Networks	10.1093/comnet/cnu006	combinatorics;maximum entropy probability distribution;principle of maximum entropy;mathematics;algorithm;statistics	Vision	37.78321634542071	15.522220037735773	68474
ef62b01229a78bc144010f3a9df6aaaaf214e742	box-sphere intersection tests	intervalo;box;interseccion;intervalle;analyse;algorithme;sphere;algorithm;interval;caja;boite;range computations;analysis;esfera;intersection;algoritmo;analisis	Two box-sphere intersection tests based on interval analysis are established. The first test uses the midpoint and radius, and the second test four points of the sphere, as parameters. The tests are simple and robust, and they can be used to recognize configurations such as a box inside a sphere, a box outside a sphere, box-sphere intersection, and a sphere inside a box. Connections with the tests developed by other researchers are discussed.	interval arithmetic;radius	Helmut Ratschek;Jon G. Rokne	1994	Computer-Aided Design	10.1016/0010-4485(94)90089-2	intersection;combinatorics;sphere–cylinder intersection;analysis;mathematics;geometry	EDA	30.027470862819424	16.510959612590394	68594
d473fb988aa9663a56643cefe5a71ccd5accdc4c	using proxies for node immunization identification on large graphs		Given a large graph, like a social network, which  $k$  nodes should be immunized (or removed) to make the network safe from the spread of a virus? This is the node immunization problem. One of the classical methods, inspired by immunology, in analyzing this problem relies on the calculation of the largest eigenvalue before and after immunization in order to create the largest difference in eigenvalue. We propose a method that does not rely on a costly calculation of eigenvalues; instead, we rely on the notion of proxies and deterministic routing areas in order to find such nodes to immunize. We show that our results are consistent with the notion of vulnerability and produces equivalent results when compared with the existing algorithms. Furthermore, experimental results show that when a virus is not allowed to die out (controlled by the strength of the virus), our algorithm ensures that more nodes are safe from infection.	algorithm;deterministic routing;graph (discrete mathematics);proxy server;social network;vulnerability (computing)	Raymond Ahn;Justin Zhijun Zhan	2017	IEEE Access	10.1109/ACCESS.2017.2723838	tree (graph theory);time complexity;deterministic routing;distributed computing;approximation algorithm;eigenvalues and eigenvectors;mathematical optimization;mathematics;graph	ML	36.83794210364746	13.903320789692446	68819
bce3989ea1259ceb077d07f0f94ac8999c341813	markov decision models with weighted discounted criteria	dynamic programming;decision models;programacion dinamica;communication networks;recompense;estrategia optima;markov;performance;decision markov;horizonte infinito;actualizacion;optimal strategy;actualisation;horizon infini;recompensa;reward;sum of discounted rewards with different discount factors;markov optimization;programmation dynamique;markov decision;infinite horizon;discounting;queuing networks;technical report;strategie optimale	We consider a discrete time Markov Decision Process with innnite horizon. The criterion to be maximized is the sum of a number of standard discounted rewards, each with a diierent discount factor. Situations in which such criteria arise include modeling investments, production, modeling projects of diierent durations and systems with multiple criteria, and some axiomatic formulations of multi-attribute preference theory. We show that for this criterion for some positive there need not exist an-optimal (randomized) stationary strategy, even when the state and action sets are nite. However,-optimal Markov (non-randomized) strategies and optimal Markov strategies exist under weak conditions. We exhibit-optimal Markov strategies which are stationary from some time onward. When both state and action spaces are nite, there exists an optimal Markov strategy with this property. We provide an explicit algorithm for the computation of such strategies and give a description of the set of optimal strategies.	computation;markov chain;markov decision process;optimal control;randomized algorithm;stationary process	Eugene A. Feinberg;Adam Shwartz	1994	Math. Oper. Res.	10.1287/moor.19.1.152	markov decision process;markov chain;mathematical optimization;markov kernel;decision model;partially observable markov decision process;markov property;performance;continuous-time markov chain;technical report;dynamic programming;discounting;mathematics;markov process;markov model;mathematical economics;variable-order markov model	AI	37.99857933070068	5.1023256194854705	68896
a36c90014d09acd1c83f49c2cac22b282043d8da	intersection cuts for nonlinear integer programming: convexification techniques for structured sets	90c26;90c30;90c57;90c10	We study the generalization of split, k-branch split, and intersection cuts from Mixed Integer Linear Programming to the realm of Mixed Integer Nonlinear Programming. Constructing such cuts requires calculating the convex hull of the difference between a convex set and an open set with a simple geometric structure. We introduce two techniques to give precise characterizations of such convex hulls and use them to construct split, k-branch split, and intersection cuts for several classes of non-polyhedral sets. In particular, we give simple formulas for split cuts for essentially all convex sets described by a single quadratic inequality. We also give simple formulas for k-branch split cuts and some general intersection cuts for a wide variety of convex quadratic sets.	convex hull;convex set;integer programming;linear programming;mathematical structure;nonlinear programming;nonlinear system;polyhedron;social inequality	Sina Modaresi;Mustafa R. Kilinç;Juan Pablo Vielma	2016	Math. Program.	10.1007/s10107-015-0866-5	mathematical optimization;combinatorics;discrete mathematics;orthogonal convex hull;mathematics	Theory	25.72708148346392	13.634852585265875	69068
628018ef795b7c752bc5b49f50f8a7ff93c9dbaf	a mean string algorithm to compute the average among a set of 2d shapes	experimental tests;piecewise linear;2d shapes;mean shape;mean string;median string;graphics recognition;string edit distance	An algorithm to compute the mean shape, when the shape is represented by a string, is presented as a modi cation of the well-known string edit algorithm. Given N strings of symbols, a string edit sequence de nes a mapping between their corresponding symbols. We transform these sets of mapped symbols (edges) into piecewise linear functions and we compute their mean. To transform them into functions, we use the equation of the line de ning their edges, and the percentage of their length, in order to have a common parameterization. The algorithm has been experimentally tested in the computation of a representative among a class of shapes in a clustering procedure in the domain of a graphics recognition application.	algorithm;cluster analysis;computation;experiment;graphics;linear function;piecewise linear continuation;string (computer science);whole earth 'lectronic link	Gemma Sánchez;Josep Lladós;Karl Tombre	2002	Pattern Recognition Letters	10.1016/S0167-8655(01)00122-2	combinatorics;discrete mathematics;approximate string matching;commentz-walter algorithm;empty string;piecewise linear function;string;wagner–fischer algorithm;machine learning;boyer–moore string search algorithm;mathematics;geometry;string-to-string correction problem;string metric;string searching algorithm	Vision	33.88470339370012	17.25085758579146	69534
1a64e411b9072cdb11f952edef906b9bb80e3458	optimal dispersion and central places	optimisation;optimizacion;model generation;dispersion central place theory linear programming;solution;central place theory;modelo;theory;teoria;linear programming;programmation lineaire;linear program;optimization;modele;dispersion;models;theorie;optimization r12	This paper presents research into optimal dispersion models as applied to central places. The literature regarding location optimization and central places is reviewed and the motivation for employing dispersion models is identified. Models that employ the objective of maximal dispersion in the context of central places are formulated and solved in the context of both singleand multiple-good systems. Two methods for generating multiple-good systems are presented: a multiple-type dispersion model and a K-value constraint set formulation. Sequential solutions to dispersion models demonstrate how a system of central places could develop over time. The solutions to these models generate the patterns of central places expected under the organizing principles of central place theory. The objective of maximal dispersion is posited as both a motivating factor in central place location decisions, and as the optimal outcome of a mature system of central places.	mathematical optimization;maximal set;organizing (structure)	Kevin M. Curtin;Richard L. Church	2007	Journal of Geographical Systems	10.1007/s10109-007-0042-4	mathematical optimization;dispersion;linear programming;calculus;mathematics;theory	HCI	25.859424797930004	10.14298215195541	69572
04d7862785d65a28573ae57e689f91a902c379ee	frequent itemset border approximation by dualization	borders;dualization;approximation;frequent itemsets;hypergraph transversals	The approach FIBAD is introduced with the purpose of computing approximate borders of frequent itemsets by leveraging dualization and computation of approximate minimal transversals of hypergraphs. The distinctiveness of the FIBAD's theoretical foundations is the approximate dualization where a new function $$\widetilde{f}$$ is defined to compute the approximate negative border. From a methodological point of view, the function $$\widetilde{f}$$ is implemented by the method AMTHR that consists of a reduction of the hypergraph and a computation of its minimal transversals. For evaluation purposes, we study the sensibility of FIBAD to AMTHR by replacing this latter by two other algorithms that compute approximate minimal transversals. We also compare our approximate dualization-based method with an existing approach that computes directly, without dualization, the approximate borders. The experimental results show that our method outperforms the other methods as it produces borders that have the highest quality.	approximation;association rule learning	Nicolas Durand;Mohamed Quafafou	2016	T. Large-Scale Data- and Knowledge-Centered Systems	10.1007/978-3-662-49784-5_2	combinatorics;discrete mathematics;mathematics;algorithm	ML	26.525528061677743	15.890772702234463	69976
1848a5a1fd9c7e402ec347fc1ab38c0bdbecb550	line clipping revisited: two efficient algorithms based on simple geometric observations	computer graphics;efficient algorithm;region rectangulaire;computer science automation formerly school of automation;interseccion;segment ligne;algorithme;mechanical engineering;algorithm;methode cohen sutherland;descrestacion;ligne geometrique;algorithme distance perpendiculaire;clipping;algorithme coin oppose;ecretage;intersection;grafico computadora;infographie;line geometry;linea geometrica;algoritmo	Two new line clipping algorithms, the opposite-corner algorithm and the perpendicular-distance algorithm, that are based on simple geometric observations are presented. These algorithms do not require computation of outcodes nor do they depend on the parametric representations of the lines. It is shown that the opposite-corner algorithm perform consistently better than an algorithm due to Nicholl, Lee, and Nicholl which is claimed to be better than the classic algorithm due to Cohen-Sutherland and the more recent Liang-Barsky algorithm. The pseudo-code of the opposite-corner algorithm is provided in the Appendix.	algorithm;line clipping	N. C. Sharma;Swami Manohar	1992	Computers & Graphics	10.1016/0097-8493(92)90071-3	ramer–douglas–peucker algorithm;weighted majority algorithm;hybrid algorithm;computer science;clipping;artificial intelligence;line;intersection;mathematics;geometry;computer graphics;algorithm;computer graphics (images)	Graphics	31.000522809092153	15.900228402172742	70094
c572af579bb0840c8489dc71227e5c650b528766	guarding in a simple polygon	graph theory;teoria grafo;interior point;surveillance;simple polygon;theorie graphe;algorithme;algorithm;vigilancia;extreme value;valeur extreme;graph algorithm;algorithms;art gallery problem;algorithme graphe;graph algorithms;valor extremo;problem solving;algoritmo	Guarding in a simple polygon was motivated by art gallery problems. A guard capable of moving along a line segment in a polygon is called amobile guard. In this paper, we discuss about two different degrees of patrol freedom of mobile guards. First, aguard diagonalis an internal diagonal that a mobile guard moving along the diagonal in a polygon and every interior point of the polygon can be seen by the mobile guard. Second, a guard chordis an internal chord that a mobile guard moving along the chord in a polygon and every interior point of the polygon can be seen by the mobile guard. In this paper, we solve the problem of finding the longest guard diagonal in O (n) time, the shortest guard diagonal in O (nα(n)) and the longest guard chord in O(n) time of a simple polygonP with n vertices, whereα(n) is the inverse of Ackermann’s function.  2000 Elsevier Science B.V. All rights reserved.	ackermann function;algorithm;art gallery problem;guard (information security);time complexity	Bor-Kuan Lu;Fang-Rong Hsu;Chuan Yi Tang	1998	Inf. Process. Lett.	10.1016/S0020-0190(00)00100-9	internal and external angle;combinatorics;diagonal;visibility polygon;simple polygon;rectilinear polygon;graph theory;interior point method;star-shaped polygon;polygon;extreme value theory;mathematics;geometry;art gallery problem;monotone polygon;midpoint polygon;polygon covering;pick's theorem;biggest little polygon;algorithm	Robotics	29.902869975472978	16.455565932356965	70104
735ddb889acc11adbbc7f3d5261259b3a03bb2cd	an efficient fusion move algorithm for the minimum cost lifted multicut problem		The Minimum Lifted Multicut Problem: •An Optimization problem whose feasible solutions are decompositions of a graph •Objective function can penalize or reward all decompositions for which any given pair of nodes are in distinct components •We propose a fusion move algorithm which outperforms existing algorithms •We use this objective function for image segmentation and obtain a new state of the art for a problem in biological image analysis	algorithm;image analysis;image segmentation;optimization problem	Thorsten Beier;Björn Andres;Ullrich Köthe;Fred A. Hamprecht	2016		10.1007/978-3-319-46475-6_44	simulation;machine learning;algorithm	Robotics	25.02024171553839	6.2497313540127655	70297
c406e9132586e014201254f46cfd4b95f2d159a0	motga: a multiobjective tchebycheff based genetic algorithm for the multidimensional knapsack problem	iterative method;multiobjective programming;optimum pareto;programmation multiobjectif;probleme sac a dos;conjunto no dominado;fonction poids;ensemble non domine;pareto front;methode pivotage;problema mochila;algoritmo genetico;metodo iterativo;knapsack problem;multiple objectives;methode iterative;pivoting method;funcion peso;algorithme genetique;reparation;multidimensional knapsack problem;algorithme evolutionniste;genetic algorithm;genetic algorithms;algoritmo evolucionista;weight function;evolutionary algorithm;reparacion;pareto optimum;multiple objective programming;article;optimo pareto;nondominated set;repair;metodo pivotaje;multiobjective genetic algorithm;programacion multiobjetivo	This paper presents a new multiobjective genetic algorithm based on the Tchebycheff scalarizing function, which aims to generate a good approximation of the nondominated solution set of the multiobjective problem. The algorithm performs several stages, each one intended for searching potentially nondominated solutions in a different part of the Pareto front. Pre-defined weight vectors act as pivots to define the weighted-Tchebycheff scalarizing functions used in each stage. Therefore, each stage focuses the search on a specific region, leading to an iterative approximation of the entire nondominated set. This algorithm, called MOTGA (Multiple objective Tchebycheff based GeneticAlgorithm) has been designed to the multiobjective multidimensional 0/1 knapsack problem, for which a dedicated routine to repair infeasible solutions was implemented. Computational results are presented and compared with the outcomes of other evolutionary algorithms. 2006 Elsevier Ltd. All rights reserved.	approximation;computation;display resolution;evolutionary algorithm;experiment;genetic algorithm;iterative method;knapsack problem;loss function;multi-objective optimization;optimization problem;pareto efficiency;r language;test set	Maria João Alves;Marla Almeida	2007	Computers & OR	10.1016/j.cor.2006.02.008	mathematical optimization;genetic algorithm;computer science;evolutionary algorithm;mathematics;mathematical economics;algorithm	AI	25.18313444517329	9.362784392860123	70541
198a8507c7b26f89419430ed51f1c7675e5fa6c7	eigensolver methods for progressive multidimensional scaling of large data	p 720 straight line;m 100 algebraic;multidimensional scaling;inproceedings;large data	We present a novel sampling-based approximation technique for classical multidimensional scaling that yields an extremely fast layout algorithm suitable even for very large graphs. It produces layouts that compare favorably with other methods for drawing large graphs, and it is among the fastest methods available. In addition, our approach allows for progressive computation, i.e. a rough approximation of the layout can be produced even faster, and then be refined until satisfaction.	approximation;computation;eigenvalue algorithm;experiment;fastest;force-directed graph drawing;graph (discrete mathematics);image scaling;java;mathematical optimization;multidimensional scaling;sampling (signal processing);time complexity	Ulrik Brandes;Christian Pich	2006		10.1007/978-3-540-70904-6_6	mathematical optimization;combinatorics;multidimensional scaling;computer science;theoretical computer science;mathematics	DB	34.29331622535606	15.660277200426089	70716
4a7e27bd782903b3f17e2453f1c1bfdfc0e10432	an incremental decomposition method for unconstrained optimization	decomposition;large scale unconstrained optimization;gradient incremental methods	In this work we consider the problem of minimizing a sum of continuously differentiable functions. The vector of variables is partitioned into two blocks, and we assume that the objective function is convex with respect to a block-component. Problems with this structure arise, for instance, in machine learning. In order to advantageously exploit the structure of the objective function and to take into account that the number of terms of the objective function may be huge, we propose a decomposition algorithm combined with a gradient incremental strategy. Global convergence of the proposed algorithm is proved. The results of computational experiments performed on large-scale real problems show the effectiveness of the proposed approach with respect to existing algorithms. 2014 Elsevier Inc. All rights reserved.	algorithm;computation;experiment;gradient;loss function;machine learning;mathematical optimization;optimization problem;singular value decomposition	Luca Bravi;Marco Sciandrone	2014	Applied Mathematics and Computation	10.1016/j.amc.2014.02.088	mathematical optimization;combinatorics;machine learning;mathematics;decomposition	AI	27.585053438413997	7.0215958840919805	70873
07b2a10a0af4dc883e4f9318182f1c956a4c9da1	search in non-homogenous random environments?	empirical distribution;random graphs;social networks;diffusion;random environment	Using a mixed jump and diffusion model, we compute the time and energy needed to find an object placed at a finite distanceD from a searcher’s initial location within an infinite non-homogenous search space, assuming that the searcher has imprecise information about where and how to search, and also that the searcher may be blocked or destroyed during the search. This problem arises in large wired or wireless networks with imprecise routing tables and packet losses [4, 8, 10, 13], in large databases with uncertain or approximately represented data such as the content of images [5, 14], and in the search by robots in hostile environments such as minefields [7]. Introduction An animal’s search for prey was modelled in [9, 12] when the predator renews its energy reserve during the search. Randomly connected finite graphs in [11] represent search in a computer network or a system of roads. In [10] it was shown that the time it takes a data packet to travel from a source to a destination node in an infinitely large and unreliable network is finite on average, if a timeout mechanism destroys the packet after a predetermined time, replacing it with a new one that starts at the source proceeding at random and independently of its predecessor. This was generalised [15] to N searchers which are simultaneously, but independently sent out in the quest for the same object. Most of the literature considers homogenous search spaces, and in this paper we develop a mixed analytical-numerical method for an infinite random nonhomogenous medium that generalises the work in [15] obtaining expressions for the average time and energy that it takes the searcher to eventually find the object it is seeking. An interesting phase transition is exhibited concerning the eventual success of the search depending on the relative speed of approach of the searcher and the intensity of events which block the searcher’s progress. The Model Although traditionally most models in computer systems and networks are discrete [3], here we consider a continuous distance Y (t) of the searcher to the object at time t ≥ 0. The searcher starts at Y (0) = D and the search ends at time T = inf{t : Y (t) = 0}. If the random variable s(t) represents the state of the searcher, s(t) ∈ {S,W,P, ...}, then s(t) ∈ S if the search is proceeding with the search and its distance from the destination is Y (t) > 0. The probability density function of Y (t) is denoted f(z, t)dz = P [z < Y (t) ≤ z + dz, s(t) = S]. s(t) ∈ W if the searcher’s life-span has ended, and so has its search. This can happen because the searcher was destroyed or became lost, and the source was informed via the time-out. After an additional exponentially distributed delay of parameter μ, meant to avoid mistakes in assuming that the searcher is “dead”, a new searcher is placed at the source and a new search immediately begins. We write W (t) = P [s(t) = W]. s(t) ∈ L if the searcher is destroyed or lost, and the search is interrupted until a new searcher can be sent out. The time spent in this state is exponentially distributed with parameter r which is the same parameter as that of the ”time-out” or life-span, since the source realises that the searcher is lost or destroyed via the life-span or time-out effect. At the end of this exponentially distributed time, the searcher is handled just as if it has “died”, and we denote L(t) = P [s(t) = L]. s(t) ∈ P if the searcher has reached its destination, i.e. it has found the object it sought and the search process ends. However, as an artefact to construct an indefinitely repeating recurrent process, after one time unit the search process restarts at the source and a new searcher is sent out. We will use the notation P (t) = P [s(t) = P]. Notice that the process repeats itself indefinitely. If E[T ] is the average time that it takes from any successive start of the search until the first instance when state P is reached again, and P (t) is the probability that the model we have just described is in state P at time t ≥ 0, and P = limt→∞ P (t), then P = 1 1+E[T ] , E[T ] = P − 1. During the searcher’s travel in state S while {Y (t) = z > 0} the following events can occur in the time interval [t, t+Δt[. With probability λ(z)Δt + o(Δt) the searcher is destroyed or lost, and enters state L. From that state it enters state W after an exponentially distributed delay of parameter r. With probability rΔt + o(Δt) the searcher’s life-span runs out and it enters state W. Note that 1/r is the average life-span. As indicated earlier, when it enters state W, after an additional delay of average value 1/μ, the searcher is replaced with a new one at the source. The average rate per unit time at which the searcher approaches the object being sought when it is at distance z is b(z), and the variance of the distance travelled in the interval [t, t+Δt[ is denoted by	c date and time functions;database;interrupt;network packet;numerical method;pollard's p − 1 algorithm;prey;randomness;robot;routing table;timeout (computing)	Omer H. Abdelrahman;Erol Gelenbe	2011	SIGMETRICS Performance Evaluation Review	10.1145/2160803.2160853	empirical distribution function;random graph;combinatorics;random field;machine learning;diffusion;exponential random graph models;complex network;statistics;social network	Theory	38.2685940871108	10.306500325665569	70897
444c1f38ad233c59ab8767226a21c0a3bd16cfcd	first passage optimality and variance minimisation of markov decision processes with varying discount factors		This paper deals with the first passage optimality and variance minimisation problems of discrete-time Markov decision processes (MDPs) with varying discount factors and unbounded rewards/costs. First, under suitable conditions slightly weaker than those in the previous literature on the standard (infinite horizon) discounted MDPs, we establish the existence and characterisation of the first passage expected-optimal stationary policies. Second, to further distinguish the expected-optimal stationary policies, we introduce the variance minimisation problem, prove that it is equivalent to a new first passage optimality problem of MDPs, and, thus, show the existence of a variance-optimal policy that minimises the variance over the set of all first passage expected-optimal stationary policies. Finally, we use a computable example to illustrate our main results and also to show the difference between the first passage optimality here and the standard discount optimality of MDPs in the previous literature.	computable function;markov chain;markov decision process;stationary process	Xiao Wu;Xianping Guo	2015	J. Applied Probability	10.1017/S0021900200012560	mathematical optimization;partially observable markov decision process;mathematical economics	AI	37.62332399985502	5.30019651131983	70966
2c40c34b1a38200f42179278d52be923f6195261	global optimization for the synthesis of integrated water systems in chemical processes	non linear programming;computacion informatica;grupo de excelencia;generalized disjunctive programming;ciencias basicas y experimentales;integrated water networks;quimica;water use;global optimization;convex relaxation;superstructure;branch and bound;lower bound;piecewise estimators;water treatment	ABSTRACT In this paper, we address the problem of optimal synthesis of an integrated water system, where water using processes and water treatment operations are combined into a single network such that the total cost of obtaining freshwater for use in the water using operations, and treating wastewater is minimized. A superstructure, which incorporates all feasible design alternatives for water treatment, reuse and recycle, is proposed. We formulate this structure as a non-convex Non-Linear Programming (NLP) problem, which is solved to global optimality. The problem takes the form of a non-convex Generalized Disjunctive Program (GDP) if there is a flexibility of choosing different treatment technologies for the removal of the various contaminants in the wastewater streams. A new deterministic spatial branch and contract algorithm is proposed for optimizing such systems, in which piecewise underand over-estimators are used to approximate the non-convex terms in the original model to obtain a convex relaxation whose solution gives a lower bound on the global optimum. These lower bounds are made to converge to the solution within a branch and bound procedure. Several examples are presented to illustrate the optimization of these integrated networks using the proposed algorithm.	approximation algorithm;baron;branch and bound;converge;convex hull;disjunctive normal form;global optimization;integrated circuit;linear equation;linear programming relaxation;mathematical optimization;natural language processing;nonlinear programming;nonlinear system;numerical method;piecewise linear continuation;solver	Ramkumar Karuppiah;Ignacio E. Grossmann	2006	Computers & Chemical Engineering	10.1016/j.compchemeng.2005.11.005	water use;mathematical optimization;engineering;water treatment;mathematics;upper and lower bounds;branch and bound;superstructure;algorithm;global optimization	EDA	28.64123812032586	6.887206226539198	71013
3e685fd83628956c3615d32dd7c65a0bde7e5afc	gnccp—graduated nonconvexityand concavity procedure	eigenvalues and eigenfunctions;algorithm design and analysis linear programming pattern matching np hard problem simulated annealing eigenvalues and eigenfunctions;deterministic annealing;structural;graduated optimization;simulated annealing;quadratic assignment problem combinatorial optimization graduated optimization deterministic annealing partial graph matching;np hard problem;pattern matching;subgraph matching;combinatorial algorithms;linear programming;partial graph matching;quadratic assignment problem;qap gnccp graduated nonconvexity and concavity procedure general optimization framework combinatorial optimization problems partial permutation matrices convex concave relaxation procedure ccrp objective function np hard problems quadratic assignment problem partial graph matching;matrix algebra computational complexity concave programming graph theory;combinatorial optimization;graph algorithms;algorithm design and analysis	In this paper we propose the graduated nonconvexity and concavity procedure (GNCCP) as a general optimization framework to approximately solve the combinatorial optimization problems defined on the set of partial permutation matrices. GNCCP comprises two sub-procedures, graduated nonconvexity which realizes a convex relaxation and graduated concavity which realizes a concave relaxation. It is proved that GNCCP realizes exactly a type of convex-concave relaxation procedure (CCRP), but with a much simpler formulation without needing convex or concave relaxation in an explicit way. Actually, GNCCP involves only the gradient of the objective function and is therefore very easy to use in practical applications. Two typical related NP-hard problems, partial graph matching and quadratic assignment problem (QAP), are employed to demonstrate its simplicity and state-of-the-art performance.	combinatorial optimization;concave function;discrete optimization;gradient;graduated optimization;linear programming relaxation;matching (graph theory);mathematical optimization;np-hardness;optimization problem;quadratic assignment problem	Zhiyong Liu;Hong Qiao	2014	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2013.223	algorithm design;structure;mathematical optimization;combinatorics;discrete mathematics;simulated annealing;combinatorial optimization;computer science;linear programming;pattern matching;np-hard;mathematics;quadratic assignment problem	ML	25.9816788663892	7.773411509264968	71126
0ce7fb0bb9f1012711081eef9579b51f65672c98	approximating discrete probability distributions with causal dependence trees	trees mathematics approximation theory probability random processes;probability;minimum weight spanning tree;approximation algorithms;discrete random processes discrete probability distributions causal directed dependence trees dependence tree distributions minimum divergence approximation minimum weight spanning tree algorithm;causal directed dependence trees;dependence tree distributions;bayesian methods;low complexity;random variables;joints;trees mathematics;discrete probability distributions;approximation theory;approximation methods joints mutual information bayesian methods random variables random processes approximation algorithms;random process;probability distribution;random processes;mutual information;minimum weight spanning tree algorithm;approximation methods;spanning tree;discrete random processes;minimum divergence approximation	Chow and Liu considered the problem of approximating discrete joint distributions with dependence tree distributions where the goodness of the approximations were measured in terms of KL distance. They (i) demonstrated that the minimum divergence approximation was the tree with maximum sum of mutual informations, and (ii) specified a low-complexity minimum-weight spanning tree algorithm to find the optimal tree. In this paper, we consider an analogous problem of approximating the joint distribution on discrete random processes with causal, directed, dependence trees, where the approximation is again measured in terms of KL distance. We (i) demonstrate that the minimum divergence approximation is the directed tree with maximum sum of directed informations, and (ii) specify a low-complexity minimum weight directed spanning tree, or arborescence, algorithm to find the optimal tree. We also present an example to demonstrate the algorithm.	algorithm;approximation;causal filter;file spanning;kullback–leibler divergence;list of algorithms;minimum spanning tree;minimum weight;stochastic process	Christopher J. Quinn;Todd P. Coleman;Negar Kiyavash	2010	2010 International Symposium On Information Theory & Its Applications	10.1109/ISITA.2010.5649470	random binary tree;probability distribution;random variable;euclidean minimum spanning tree;stochastic process;mathematical optimization;combinatorics;discrete mathematics;tree rotation;vantage-point tree;exponential tree;spanning tree;bayesian probability;minimum spanning tree;range tree;gomory–hu tree;probability;k-ary tree;interval tree;k-minimum spanning tree;mathematics;mutual information;statistics;approximation theory	Theory	39.05196341585004	15.103571529563554	71208
4afc7c76687207f8346cc73fd0a5507907c851c3	mixed optimization combinatorial method for constructing covering arrays	particular case;mixed optimization combinatorial method;method speed;large number;wide class;application range;new method;optimization method	Covering arrays are used for generating tests for interfaces with a large number of parameters. In this paper, a new method is described for constructing homogeneous and heterogeneous covering arrays that is based on a combination of combinatorial and optimization methods. In a wide class of particular cases, the method speeds up the construction of arrays several times (depending on a particular case) compared with well-known, widely used optimization methods. In most cases, the sizes of the arrays obtained are approximately the same as those of the arrays constructed by other optimization methods; in a number of particular cases, one could obtain arrays that are smaller by 5–15%. The application range of the new method is analyzed.	mathematical optimization	A. A. Petukhov	2014	Programming and Computer Software	10.1134/S036176881401006X	mathematical optimization;combinatorics;discrete mathematics;mathematics	SE	26.392187003098826	5.59156214641592	71287
8b2e7cf5ef96da5dc37b0100e5bad7828f3bc71a	random greedy triangle-packing beyond the 7/4 barrier	steiner triple system;data structure;upper bound;complete graph;stochastic process;greedy algorithm	The random greedy algorithm for constructing a large partial Steiner-Triple-System is defined as follows. Begin with a complete graph on n vertices and proceed to remove the edges of triangles one at a time, where each triangle removed is chosen uniformly at random out of all remaining triangles. This stochastic process terminates once it arrives at a triangle-free graph, and a longstanding open problem is to estimate the final number of edges, or equivalently the time it takes the process to conclude. The intuition that the edge distribution is roughly uniform at all times led to a folklore conjecture that the final number of edges is n with high probability, whereas the best known upper bound is n. It is no coincidence that various methods break precisely at the exponent 7/4 as it corresponds to the inherent barrier where co-degrees become comparable to the variations in their values that arose earlier in the process. In this work we significantly improve upon the previous bounds by establishing that w.h.p. the number of edges in the final graph is at most n 2− 1 2 √ 2 +o(1) . Our approach relies on the differential equation method to dynamically control key graph parameters, where the crucial new idea is to harness the self-correcting nature of the process in order to control these parameters well beyond the point where their early variation matches the order of their expectation.	greedy algorithm;set packing;steiner tree problem;stochastic process;with high probability	Tom Bohman;Alan M. Frieze;Eyal Lubetzky	2011	CoRR		graph power;random regular graph;random graph;stochastic process;mathematical optimization;combinatorics;greedy algorithm;discrete mathematics;topological graph;multiple edges;data structure;null graph;degree;pseudoforest;multigraph;hypercube graph;cycle graph;path graph;graph factorization;mathematics;geometry;path;steiner system;upper and lower bounds;complete graph;complement graph;semi-symmetric graph;strength of a graph	Theory	37.183448319271214	16.90769809652718	71523
251fa0624c9112aa40be7b6355fdfbb0a9471a9a	numerical construction of stackelberg solutions in a linear positional differential game based on the method of polyhedra		We consider the problem of constructing approximate Stackelberg solutions in a linear non-zero-sum positional differential game of two players with terminal payoffs and player controls chosen on convex polyhedra. A formalization of player strategies and motions generated by them is based on the formalization and results of the theory of zero-sum positional differential games developed by N.N. Krasovskii and his scientific school. The problem of finding a Stackelberg solution reduces to solving nonstandard optimal control problems. We propose an approach based on operations with convex polyhedra.	polyhedron	Dmitry R. Kuvshinov;Sergei I. Osipov	2018	Automation and Remote Control	10.1134/S0005117918030074	mathematical optimization;differential game;convex polytope;regular polygon;stackelberg competition;mathematics;polyhedron;optimal control	EDA	29.40576023603422	11.952720851429822	71709
935b684187687c819046e1235b2ce933e7754c23	ray coherence between a sphere and a convex polyhedron	analytical geometry of convex point sets;trazado rayos;capsula convexa;execution time;computer graphics;sistema informatico;ray coherence theorems;trace rayon;computer system;theorem proving;enveloppe convexe;demonstration theoreme;ray tracing;coherence;temps execution;convex hull algorithms;systeme informatique;coherencia;demostracion teorema;tiempo ejecucion;convex hull;grafico computadora;infographie	Abstract#R##N##R##N#Using the two ray coherence theorems of Ohta and Maekawa the computation time of ray tracing algorithms for scenes of spheres and convex polyhedra can be reduced considerably. This paper presents further theorems which, together with the first two, may enable further reduction in computation time.	polyhedron	Tamás Horváth;Gábor Márton;Peter Risztics;László Szirmay-Kalos	1992	Comput. Graph. Forum	10.1111/1467-8659.1120163	ray tracing;mathematical optimization;combinatorics;topology;coherence;computer science;convex hull;mathematics;geometry;automated theorem proving;computer graphics;algorithm	NLP	32.53878732937439	14.446339568674086	71851
809b6de900f316eeaa5c2159ce3de3b971c7edcb	random intersection graph process		We introduce a random intersection graph process aimed at modeling sparse evolving affiliation networks that admit tunable (power law) degree distribution and assortativity and clustering coefficients. We show the asymptotic degree distribution and provide explicit asymptotic formulas for assortativity and clustering coefficients. keywords: random graph process, random intersection graph, degree distribution, power law, clustering, assortativity 2000 Mathematics Subject Classifications: 05C80, 05C07, 05C82		Mindaugas Bloznelis;Michal Karonski	2013		10.1007/978-3-319-03536-9_8	combinatorics;discrete mathematics;graph center;machine learning;mathematics;intersection number;intersection graph	Theory	37.62597957450303	15.524987617550268	72007
615c94c077ccd3a0fd77a70138e99a37cbb231e6	generating constrained random graphs using multiple edge switches	random graph;markov chain mixing;random sampling;complex network;higher order;degree distribution;random graphs;general methods;constrained graphs generation method;graph algorithm;edge switching;markov chain	The generation of random graphs using edge swaps provides a reliable method to draw uniformly random samples of sets of graphs respecting some simple constraints (e.g., degree distributions). However, in general, it is not necessarily possible to access all graphs obeying some given constraints through a classical switching procedure calling on pairs of edges. Therefore, we propose to get around this issue by generalizing this classical approach through the use of higher-order edge switches. This method, which we denote by “k-edge switching,” makes it possible to progressively improve the covered portion of a set of constrained graphs, thereby providing an increasing, asymptotically certain confidence on the statistical representativeness of the obtained sample.	multiple edges;network switch;obedience (human behavior);random graph	Lionel Tabourier;Camille Roth;Jean-Philippe Cointet	2011	ACM Journal of Experimental Algorithmics	10.1145/1963190.2063515	1-planar graph;random regular graph;pathwidth;random graph;mathematical optimization;combinatorics;discrete mathematics;independent set;graph product;dense graph;hopcroft–karp algorithm;graph coloring;trapezoid graph;mathematics;maximal independent set;modular decomposition;partial k-tree;graph operations;chordal graph;indifference graph	Theory	38.094489801016934	15.4877991292562	72071
5d55833a082502a789fab2c83ad37ee276475ff5	smart sort: design and analysis of a fast, efficient and robust comparison based internal sort algorithm		Smart Sort algorithm is a “smart” fusion of heap construction procedures (of Heap sort algorithm) into the conventional “Partition” function (of Quick sort algorithm) resulting in a robust version of Quick sort algorithm. We have also performed empirical analysis of average case behavior of our proposed algorithm along with the necessary theoretical analysis for best and worst cases. Its performance was checked against some standard probability distributions, both uniform and non-uniform, like Binomial, Poisson, Discrete & Continuous Uniform, Exponential, and Standard Normal. The analysis exhibited the desired robustness coupled with excellent performance of our algorithm. Although this paper assumes the static partition ratios, its dynamic version is expected to yield still better results.	best, worst and average case;internal sort;quicksort;sorting algorithm	Niraj Kumar Singh;Soubhik Chakraborty	2012	CoRR		adaptive sort;mathematical optimization;counting sort;combinatorics;discrete mathematics;selection sort;hybrid algorithm;computer science;theoretical computer science;timsort;sorting algorithm;in-place algorithm;comparison sort;algorithm;stooge sort	ML	33.611032156094986	9.363289389328067	72627
862cd759cdb2eba388f1f7a3f018d4ca5218a1a4	bounding the reliability of multistate systems	reliability;721 reliability bounds for multistate systems;inequalities;life tests;computations;mathematical analysis;boundaries;systems analysis	Determining the exact reliability of a complex system can involve extremely large amounts of computation. This paper develops a number of upper and lower bounds for the reliability of multistate systems, that is, systems for which each component may exist in one of a finite number of states. These bounds are based on the notions of minimal paths and minimal cuts and are far more easily computed than the exact system reliability. To further reduce the computations required and to obtain sharper bounds, a modular-decomposition-based bound is developed for multistate systems.		David A. Butler	1982	Operations Research	10.1287/opre.30.3.530	systems analysis;combinatorics;discrete mathematics;computation;inequality;reliability;mathematics;personal boundaries;statistics	EDA	37.1449213316166	11.40148484976222	72686
e873c290c5d5d5ef55469844301b3c823aae46c2	a fast path planning by path graph optimization	graph theory;shortest path;optimization technique;path planning;search algorithm;computational geometry;path planning skeleton optimization methods shape robots application software space technology robustness costs iterative methods;motion planning fast path planning path graph optimization compact mesh representation world space triangulation iterative vertex pushing method dijkstra s shortest path searching algorithm;iterative methods;robots path planning graph theory search problems computational geometry computational complexity iterative methods;optimal path;computational complexity;robots;search problems	In this paper, a fast path planning method by optimization of a path graph for both efficiency and accuracy is proposed. A conventional quadtree-based path planning approach is simple, robust, and efficient. However, it has two limitations. The first limitation is that many small cells are required to represent obstacles because the positions and shapes of the cells are not object-dependent and searching the shortest path in the path graph is slow. The second limitation is generation of nonoptimal paths due to large cells in a free space. The cost of traversing a cell is the same whether a path just clips a corner of the cell or actually passes through the entire width of the cell. The quadtree is very sensitive to obstacle placement and the error factor is proportional to the size of the cell. We propose a path graph optimization technique employing a compact mesh representation. A world space is triangulated into a base mesh and the base mesh is simplified to a compact mesh. The compact mesh representation is object-dependent; the positions of vertexes of the mesh are optimized according to the curvatures of the obstacles. The compact mesh represents the obstacles as accurately as the quadtree even though using much fewer vertexes than the quadtree. The compact mesh distributes vertexes in a free space in a balanced way by ensuring that the lengths of edges are below an edge length threshold. An optimized path graph is extracted from the compact mesh. An iterative vertex pushing method is proposed to include important obstacle boundary edges in the path graph. Dijkstra’s shortest path searching algorithm is used to search the shortest path in the path graph. Experimental results show that the path planning using the optimized path graph is an order of magnitude faster than the quadtree approach while the length of the path generated by the proposed method is almost the same as that of the path generated by the quadtree.	fast path;graphics pipeline;iterative method;mathematical optimization;motion planning;polygon triangulation;quadtree;search algorithm;shortest path problem;vertex (geometry)	Joo Young Hwang;Jun Song Kim;Sang Seok Lim;Kyu Ho Park	2003	IEEE Trans. Systems, Man, and Cybernetics, Part A	10.1109/TSMCA.2003.812599	robot;basis path testing;mathematical optimization;combinatorics;discrete mathematics;bidirectional search;widest path problem;fast path;constrained shortest path first;any-angle path planning;longest path problem;average path length;computational geometry;computer science;pathfinding;artificial intelligence;graph theory;euclidean shortest path;yen's algorithm;mathematics;motion planning;iterative method;path;shortest path problem;computational complexity theory;distance;k shortest path routing;shortest path faster algorithm;search algorithm	Graphics	30.61796353521059	14.74699814841935	73316
9eb1416f3b8051df484c9f794c8999a508d22df5	topological properties of the one dimensional exponential random geometric graph	random geometric graph;interval graph;mathematics;connectivity;degree;exponential distribution;random graph;connected component;nearest neighbor	In this article we study the one-dimensional random geometric (random interval) graph when the location of the nodes are independent and exponentially distributed. We derive exact results and limit theorems for the connectivity and other properties associated with this random graph. We show that the asymptotic properties of a graph with a truncated exponential distribution can be obtained using the exponential random geometric graph. © 2007 Wiley Periodicals, Inc. Random Struct. Alg., 2008	geometric graph theory;random geometric graph;time complexity	Bhupendra Gupta;Srikanth K. Iyer;D. Manjunath	2008	Random Struct. Algorithms	10.1002/rsa.20174	conductance;graph power;random regular graph;random graph;exponential distribution;combinatorics;geometric graph theory;discrete mathematics;connected component;null model;interval graph;topology;null graph;distance-regular graph;connectivity;simplex graph;cubic graph;mathematics;voltage graph;exponential random graph models;butterfly graph;random geometric graph;k-nearest neighbors algorithm;quartic graph;line graph;string graph;strength of a graph;degree;coxeter graph	Theory	38.9459233600692	16.249717162384073	73387
7b0fe1074eccded042e026723b55251b29ee4b05	markov chain design problems	696 system design problem;642 computing an optimal markov chain;markov chain	System design problems using Markov chains and denoted Markov chain design problems are introduced. It is assumed that the chains of the problem have transition matrices and one period conditional expected rewards which are differentiable functions of a vector parameter. A gradient algorithm for maximizing the sum of the discounted expected reward or the limit of the one period expected reward is presented. The algorithm uses approximate objective function values and approximate gradients at each stage. Numerical work on a simple one dimensional queueing model solved the design problem correctly and quickly even though minimal approximation accuracy was used. The convergence of the approximation is proven for an appropriately converging sequence of design parameter values.	markov chain	Richard V. Evans	1981	Operations Research	10.1287/opre.29.5.959	uniformization;markov chain;mathematical optimization;maximum-entropy markov model;markov kernel;combinatorics;discrete phase-type distribution;discrete mathematics;markov chain monte carlo;markov property;continuous-time markov chain;examples of markov chains;balance equation;mathematics;markov renewal process;additive markov chain;markov algorithm;markov process;markov chain mixing time;markov model;absorbing markov chain;hidden markov model;statistics;variable-order markov model	Theory	38.68676849218667	5.585987825958654	73655
2e4d84a6bcc617e28cb657dbcd523f49ea272265	uniform boundedness of critical crossing probabilities implies hyperscaling	critical point;critical exponent;decay rate	We consider bond percolation on the d-dimensional hypercubic lattice. Assuming the existence of a single critical exponent, the exponent ρ describing the decay rate of point-to-plane crossings at the critical point, we prove that hyperscaling holds whenever critical rectangle crossing probabilities are uniformly bounded away from 1.	critical point (network science);lattice gauge theory;percolation theory	Christian Borgs;Jennifer T. Chayes;Harry Kesten;Joel H. Spencer	1999	Random Struct. Algorithms	10.1002/(SICI)1098-2418(199910/12)15:3/4%3C368::AID-RSA9%3E3.0.CO;2-B	combinatorics;radioactive decay;mathematical analysis;discrete mathematics;mathematics;critical point;critical exponent	Theory	37.78061749246614	17.716137440488172	73738
09dbe8f6938473a949737ea3c4a691ee61d67cd5	technical note - a single-commodity transformation for certain multicommodity networks		We derive a sufficient condition by which a multicommodity minimal cost network flow problem can be transformed and solved as an equivalent single-commodity problem. The condition is based on the topological structure of the network and unifies several special results that have been established. The transformation is given, and potential applications are discussed.		James R. Evans	1978	Operations Research	10.1287/opre.26.4.673	mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	25.39179665106282	13.62091428773731	74019
8cade8f343435b5890cc430a72e97c2804a598ef	track drawings of graphs with constant queue number	file attente;complexite;p 720 straight line;image processing;modelo 3 dimensiones;modele 3 dimensions;speech processing;temps lineaire;complejidad;tratamiento palabra;linear time algorithm;procesamiento imagen;traitement parole;three dimensional model;queue;complexity;estiramiento;tiempo lineal;traitement image;etirage;upper bound;line drawings;g 999 others;drawing;p 060 3d;linear time;borne inferieure;grafo linea;pattern recognition;number;reconnaissance forme;reconocimiento patron;line graph;borne superieure;nombre;graphe ligne;fila espera;lower bound;numero;cota superior;cota inferior	A k-track drawing is a crossing-free 3D straight-line drawing of a graph G on a set of k parallel lines called tracks. The minimum value of k for which G admits a k-track drawing is called the track number of G. In [18] it is proved that every graph from a proper minor closed family has constant track number if and only if it has constant queue number. In this paper we study the track number of well-known families of graphs with small queue number. For these families we show upper bounds and lower bounds on the track number that significantly improve previous results in the literature. Linear time algorithms that compute track drawings of these graphs are also presented and their volume complexity is discussed.	graph (discrete mathematics);line drawing algorithm;queue number;time complexity	Emilio Di Giacomo;Henk Meijer	2003		10.1007/978-3-540-24595-7_20	combinatorics;image processing;computer science;speech processing;mathematics;upper and lower bounds;algorithm	Theory	30.57147991960741	16.239648886136457	74791
1b930d826a83004a50cffa9ba8a0e532d46156ad	matrix completion from any given set of observations	conference paper;drntu science mathematics	In the matrix completion problem the aim is to recover an unknown real matrix from a subset of its entries. This problem comes up in many application areas, and has received a great deal of attention in the context of the netflix prize. A central approach to this problem is to output a matrix of lowest possible complexity (e.g. rank or trace norm) that agrees with the partially specified matrix. The performance of this approach under the assumption that the revealed entries are sampled randomly has received considerable attention (e.g. [1, 2, 3, 4, 5, 6, 7, 8]). In practice, often the set of revealed entries is not chosen at random and these results do not apply. We are therefore left with no guarantees on the performance of the algorithm we are using. We present a means to obtain performance guarantees with respect to any set of initial observations. The first step remains the same: find a matrix of lowest possible complexity that agrees with the partially specified matrix. We give a new way to interpret the output of this algorithm by next finding a probability distribution over the non-revealed entries with respect to which a bound on the generalization error can be proven. The more complex the set of revealed entries according to a certain measure, the better the bound on the generalization error.	algorithm;apply;generalization error;netflix prize;randomness;the matrix	Troy Lee;Adi Shraibman	2013			combinatorics;computer science;matrix of ones;machine learning;mathematics;algorithm;statistics	ML	38.40219276253535	12.603422953884348	74920
7a65d26da83c3f5a3ab8c68d0fcc8289842a882a	boundary-labeling algorithms for panorama images	label placement;dynamic programming;dynamic program;polynomial time algorithm;visualization;panorama images;gis;polynomial algorithm;boundary labeling;sliding labels	Boundary labeling deals with placing annotations for objects in an image on the boundary of that image. This problem occurs frequently in situations where placing labels directly in the image is impossible or produces too much visual clutter. Previous algorithmic results for boundary labeling consider a single layer of labels along some or all sides of a rectangular image. If, however, the number of labels is large or labels are too long, multiple layers of labels are needed.  In this paper we study boundary labeling for panorama images, where n points in a rectangle R are to be annotated by disjoint unit-height rectangular labels placed above R in k different rows (or layers). Each point is connected to its label by a vertical leader that does not intersect any other label. We present polynomial-time algorithms based on dynamic programming that either minimize the number of rows to place all n labels, or maximize the number (or total weight) of labels that can be placed in k rows for a given integer k. For weighted labels, the problem is shown to be (weakly) NP-hard, and we give a pseudo-polynomial algorithm to maximize the weight of the selected labels. We have implemented our algorithms; the experimental results show that solutions for realistically-sized instances are computed instantaneously.	algorithm;clutter;dynamic programming;np-hardness;polynomial;time complexity	Andreas Gemsa;Jan-Henrik Haunert;Martin Nöllenburg	2011		10.1145/2093973.2094012	mathematical optimization;combinatorics;automatic label placement;discrete mathematics;geomatics;visualization;computer science;dynamic programming;mathematics	Vision	31.730848004797505	17.603634444551126	75054
21653fc0bc2e52ad649f1ee0412fca9ee888e91f	an approach to zero-one integer programming	integer program	By starting with an all-integer zero-one linear programming problem, it is possible to develop a modified, possibly linear, programming problem that provides a characterization of the basis corresponding to a feasible zero-one solution to the integer problem. This characterization is based on the number of variables equal to one in the feasible solution. This paper develops an approach to zero-one programming based on this characterization. The method uses the criterion function of the original problem as a constraint, and then generates a sequence of feasible zero-one solutions, each with a greater value of the objective function. The solution technique is terminated when no more feasible solutions can be found, indicating that the last feasible solution determined is the optimum.	integer programming	A. Victor Cabot;Arthur P. Hurter	1968	Operations Research	10.1287/opre.16.6.1206	mathematical optimization;combinatorics;discrete mathematics;basic solution;integer programming;constraint satisfaction;linear-fractional programming;branch and price;cutting stock problem;mathematics;active set method;constraint;cutting-plane method	Theory	24.71873201241001	11.53470334798535	75240
5769f1c13e568d10282e4b15f21264b494fc9a27	hybrid distance field computation for volumetric datasets (st)	best approximation;euclidean distance;distance field;collision detection	Distance fields are a widely investigated area within the area of Volume Graphics. Research is divided between applications; such as — skeletonisation, hypertexture, voxelisation, acceleration of rendering techniques, correlation and collision detection; and the fundamental algorithmic calculation of the distance fields. This paper concentrates on the latter by presenting a new method for calculating distance fields and comparing it with the current best approximate method and the true Euclidean distance field. Details are given of the algorithm, and the acceleration methods that are used for calculating the true distance field. Brief descriptions of applications for these accurate distance fields are given at the end of the paper.	computation;distance transform	Richard Satherley;Mark W. Jones	2001		10.2312/VG/VG01/195-209	gilbert–johnson–keerthi distance algorithm;mathematical optimization;radial basis function;combinatorics;weighted voronoi diagram;mathematics;geometry;distance from a point to a line;distance transform;trace distance;distance	Vision	34.31274994813147	16.876904234918232	75359
7c61b6d53c2a6851ee366d1336060b42971067ea	separation algorithms for cutting planes based on mixed integer row relaxations			algorithm	Philipp M. Christophel	2009			discrete mathematics;combinatorics;mathematics;integer	EDA	24.988366399980965	13.597996246533928	75671
4eb96ea041cf18d05c9100c639cbb0c6cd6c8194	abstract representation of object and structural symmetries detection	medial axis transform;spatial reasoning;automorphism group;shape recognition;3d representation;qualitative reasoning;branch and bound	Representation of Object and Structural Symmetries Detection Vincent Dugat (IRIT-UPS, Toulouse, France dugat@cict.fr) Pierre Gambarotto (IRIT-UPS, Toulouse, France gamba@irit.fr) Yannick Larvor (IRIT-UPS, Toulouse, France larvor@irit.fr) Abstract: This paper describes a method for constructing an abstract representation of a shape from a classical polyhedral 3D representation of an object. This framework is suitable for qualitative reasoning. As an application we use this abstract representation to compute the structural symmetries of a 3D polyhedron. The starting point of the computation is a classical polyhedral 3D representation of the object. From the Medial Axis Transform (MAT) of this object we propose a more abstract representation based on a set of spheres extracted from the MAT and structured as one or several graphs. This framework can be used for several purposes. Here we focus on the problem of finding structural symmetries of the object. We use the automorphisms group of the computed graphs. Then we propose a method to compute the automorphisms that have a geometrical sense among the set of all automorphisms. We compare the brute force algorithm with a branch and bound strategy based on the orbits partition of the vertices. This paper describes a method for constructing an abstract representation of a shape from a classical polyhedral 3D representation of an object. This framework is suitable for qualitative reasoning. As an application we use this abstract representation to compute the structural symmetries of a 3D polyhedron. The starting point of the computation is a classical polyhedral 3D representation of the object. From the Medial Axis Transform (MAT) of this object we propose a more abstract representation based on a set of spheres extracted from the MAT and structured as one or several graphs. This framework can be used for several purposes. Here we focus on the problem of finding structural symmetries of the object. We use the automorphisms group of the computed graphs. Then we propose a method to compute the automorphisms that have a geometrical sense among the set of all automorphisms. We compare the brute force algorithm with a branch and bound strategy based on the orbits partition of the vertices.	ap computer science;algorithm;apache axis;branch and bound;brute-force search;classical xy model;compaq evo;complexity;computation;computational geometry;entity framework;graph (discrete mathematics);medial graph;optic axis of a crystal;p (complexity);polyhedron;stemming;ups (debugger)	Vincent Dugat;Pierre Gambarotto;Yannick Larvor	2003	J. UCS	10.3217/jucs-009-09-1008	qualitative reasoning;computer science;artificial intelligence;spatial intelligence;branch and bound;algorithm	Vision	34.75095177349813	17.980980650206384	76180
843a1624b0bae0cd099d51fc815d9dbea47e17f9	simulations between triangular and hexagonal number-conserving cellular automata	conservation law;cellular automata;cellular automaton	A number-conserving cellular automaton is a cellular automaton whose states are integers and whose transition function keeps the sum of all cells constant throughout its evolution. It can be seen as a kind of modelization of the physical conservation laws of mass or energy. In this paper, we first propose a necessary condition for triangular and hexagonal cellular automata to be number-conserving. The local transition function is expressed by the sum of arity two functions which can be regarded as 'flows' of numbers. The sufficiency is obtained through general results on number-conserving cellular automata. Then, using the previous flow functions, we can construct effective number-conserving simulations between hexagonal cellular automata and triangular cellular automata.	automata theory;cellular automaton;computer simulation;mathematical model	Katsunobu Imai;Bruno Martin	2008	CoRR		stochastic cellular automaton;cellular automaton;reversible cellular automaton;block cellular automaton;combinatorics;discrete mathematics;elementary cellular automaton;continuous spatial automaton;quantum finite automata;quantum cellular automaton;asynchronous cellular automaton;continuous automaton;deterministic automaton;ω-automaton;mathematics;rule 184;mobile automaton;timed automaton;conservation law;algorithm;lattice gas automaton	Theory	39.03689132484446	9.152095879737763	76241
a7a843d494162015a0934064a6d5a9bbb7015754	average path length estimation of social networks by random walk		Average path length (APL) is an index of small-world networks. Calculating APL accurately requires measuring all of the shortest path lengths between two arbitrary nodes in a network. However, obtaining an entire social network is difficult because of security and privacy protection restrictions. Therefore, sampling a portion of a network to estimate features can be effective. In this research, we propose a method for estimating APL using random walk, one of the crawling-based sampling methods.	apl;average path length;sampling (signal processing);shortest path problem;social network	Toshiki Matsumura;Kenta Iwasaki;Kazuyuki Shudo	2018	2018 IEEE International Conference on Big Data and Smart Computing (BigComp)	10.1109/BigComp.2018.00107	random walk;mathematical optimization;sampling (statistics);complex network;average path length;social network;shortest path problem;mathematics	DB	36.56932321047397	14.64090680319705	76252
34a988f8adc80223180803657a92db9d16ef4744	automating the construction of patchers that satisfy global constraints	evaluation function;con straint satisfaction problem;global constraint;satisfiability;knowledge compilation	"""Generate-and-test algorithms to solve con­ straint satisfaction problems are often ineffi­ cient , but can be constructed fairly easily by knowledge compilation techniques that con­ vert declarative problem knowledge and do­ main knowledge into a procedural format [Liew and Tong, 1987]. Current research is focus­ ing on methods to improve the efficiency of generate-and-test algorithms by completely in­ corporating local constraints into generators of parts of composite solutions [Braudaway, 1988]. More global constraints on multiple parts can­ not necessarily be incorporated into the part generators. Their satisfaction must be ensured in a different way. We describe an (unimplemented) method for transforming a generate-and-test algorithm into a generate-test-and-patch algorithm that efficiently hillclimbs toward a solution satisfy­ ing a particular global constraint. Our method is based on constructing an evaluation function from the global constraint, that reflects the """"de­ gree"""" to which the constraint has been satisfied. Some of the steps in this method rely on categorizing the global constraint into a generic class. In this paper, the constraint classes on which we focus are quota-meeting and covering constraints. We illustrate the general approach by apply­ ing it to a simple generate-and-test algorithm for house floorplanning. We provide empirical results that corroborate our claim that the ef­ ficiency of the algorithm has been significantly improved. provided The opinions expressed in this paper are those of the authors and do not reflect any policies, either expressed or implied, of any granting agency. Room lengths must be at least minValue. Room widths must be at least minValue. Rooms have to be inside the house. Rooms must be adjacent to the house boundary. Rooms must not overlap. The rooms must completely fill the house space. 1 Introduction In this paper, we address part of the problem of compiling a declarative representation of a class of problems and knowledge relevant to solving it into an efficient problem-solving system. Example domain. We will use the domain of house floorplanning to illustrate several domain-independent ideas for addressing this research problem. Floorplans are arrangements of rectangular rooms in a rectangu­ lar house space. Rooms and houses are aligned with an integer-valued grid of points in a plane. The con­ straints in Figure 1 define a particular class of floorplan­ ning problems. The number of rooms, the dimensions of the house space, and minimum values for room lengths and widths are problem-specific parameters. [Liew and Tong, 1987] h as shown …"""	algorithm;categorization;compiler;evaluation function;floorplan (microelectronics);global serializability;knowledge compilation;problem solving	Kerstin Voigt;Chris Tong	1989			constraint logic programming;mathematical optimization;constraint programming;discrete mathematics;binary constraint;ac-3 algorithm;constraint satisfaction;constraint learning;computer science;constraint graph;artificial intelligence;machine learning;evaluation function;constraint satisfaction dual problem;mathematics;complexity of constraint satisfaction;constraint;constraint satisfaction problem;algorithm;difference-map algorithm;hybrid algorithm;local consistency;backtracking;satisfiability	AI	27.51902475405968	8.110748640441944	76290
62b783953092f6c7cf6d1176c460346b25caeb81	an improved algorithm for the packing of unequal circles within a larger containing circle	optimisation;packing;circle packing;adoptive search;tabu search;physical model;numerical experiment;local minima;modeling	This paper describes an approved algorithm for the problems of unequal circle packing – the quasi-physical quasihuman algorithm. First, the quasi-physical approach for the general packing problems is described in solving the pure problems of unequal circle packing. The method is an analogy to the physical model in which a number of smooth cylinders are packed inside a container. A quasi-human strategy is then proposed to trigger a jump for a stuck object in order to get out of local minima. Our method has been tested in numerical experiments. The computational results are presented, showing the merits of the proposed method. Our algorithm can be thought as an adoptive algorithm of the Tabu search. 2002 Published by Elsevier Science B.V.	algorithm;arjen lenstra;bin packing problem;combinatorial optimization;computation;continuous optimization;experiment;fletcher's checksum;gradient method;mathematical model;mathematical optimization;maxima and minima;metaheuristic;newton;newton's method;numerical analysis;optical fiber cable;polynomial;quasi-newton method;set packing;stacking;tabu search;time complexity;whole earth 'lectronic link	Huaiqing Wang;Wenqi Huang;Quan Zhang;Dongming Xu	2002	European Journal of Operational Research	10.1016/S0377-2217(01)00241-7	mathematical optimization;combinatorics;systems modeling;circle packing;tabu search;physical model;maxima and minima;mathematics;geometry	Robotics	29.171377156654792	5.196091283997629	76291
43ee4b40b2ebb4be1f6921a085e36077f7500bec	increasing and lipschitz continuous minimizers in one-dimensional linear-convex systems without constraints: the continuous and the discrete case	minimisation;sistema lineal;minimization;cost function;minimisation continue lipschitz;decision markov;minimizacion;funcion coste;linear system;teoria decision;stochastic system;convex function;theorie decision;lipschitz continuity;decision theory;linear quadratic;fonction cout;stochastic control;decision process;markov decision;systeme lineaire;sistema estocastico;fonction convexe;systeme stochastique;funcion convexa	We consider a stochastic control model with linear transition law and arbitrary convex cost functions, a far-reaching generalization of the familiar linear quadratic model. Firstly conditions are given under which the continuous state version has minimizersfn at each stagen which are increasing and in addition either right continuous or continuous or Lipschitz continuous with explicitly given Lipschitz constant. For the computationally important discrete version we verify some analogous properties under stronger assumptions.		Karl Hinderer;Michael Stieglitz	1996	Math. Meth. of OR	10.1007/BF01194330	convex function;minimisation;mathematical optimization;discrete mathematics;stochastic control;decision theory;lipschitz domain;calculus;mathematics;lipschitz continuity;linear system;continuous linear operator;statistics	Logic	37.64247220433991	5.283637501249048	76357
eccf43acdbd05bd62580e0baef373ecc83f2c573	“tuning” an asm metric: a case study in metric asm optimization	approximate string matching;edit distance	Wc present an approximate string matching case study. An optimized version of the edit distance algorithm is described which has proven more accurate for a particular commercial application than the existing (benchmark) algorithm. The cvoluhon and nature of the optimization are detailed and test results are presented.	approximate string matching;benchmark (computing);business models for open-source software;edit distance;mathematical optimization;string searching algorithm	Hal Berghel;David Roach;George Balogh;Carroll Hyatt	1992		10.1145/143559.143604	topology;edit distance;approximate string matching;machine learning;mathematics;geometry;bk tree;string metric	EDA	29.79764858520915	6.772461643074276	76889
983393b5e3afd3e595a5611118d79ec457b10306	swarm ant robotics for a dynamic cleaning problem - analytic lower bounds and impossibility results	impossibility swarm ant robotics dynamic cleaning problem multiagent robotics cooperative cleaners problem grid dirty pixels dirty tiles contamination spreading fire spreading cleaning protocol analytic lower bound;swarm ant robotics;protocols;dirty tiles;fire spreading;cleaning tiles contamination robot kinematics robot sensing systems biological system modeling computer science intelligent robots fires animals;multiagent robotics;grid;dynamic environment;impossibility;cleaning protocol;shape;cooperative cleaners problem;analytic lower bound;dynamic cleaning problem;robots;multi robot systems;dirty pixels;fire severity;tiles;contamination;robot dynamics;lower bound;cleaning;contamination spreading;robot kinematics;robot dynamics multi robot systems	Several recent works considered multi a(ge)nt robotics in static environments. In this work we examine ways of operating in dynamic environments, in which changes take place independently of the agents' activity. The work focuses on a dynamic variant of the known Cooperative Cleaners problem (described and analyzed by Wagner and Bruckstein in [1]). This problem assumes a grid, having “dirty” pixels or tiles, that form a connected region of the grid. Several agents move in this dirty region, each having the ability to “clean” the place it is located in. The dynamic variant of the problem involves a deterministic expansion of dirt in the environment, simulating a spreading of contamination, or fire. Several impossibility results for the problem are shown. In addition, a cleaning protocol for the problem is presented, as well as an analytic lower bound on its performance.	ant robotics;pixel;plasma cleaning;region-based memory management;simulation;swarm	Yaniv Altshuler;Vladimir Yanovski;Israel A. Wagner;Alfred M. Bruckstein	2000	2009 4th International Conference on Autonomous Robots and Agents	10.1109/ICARA.2000.4804016	robot;communications protocol;simulation;shape;computer science;artificial intelligence;contamination;upper and lower bounds;grid;robot kinematics	Robotics	31.58919097808544	12.965027906414308	76970
b3bc32c5afc17f87762a5f4f0b0aebb2d1d54b6d	average case analysis of greedy algorithms for optimisation problems on set systems	random graph;approximation asymptotique;equation differentielle;optimisation;graphe biparti;chaine markov;cadena markov;algorithm analysis;optimizacion;algorithme glouton;grafo bipartido;ordinary differential equation;grafo aleatorio;differential equation;graphe aleatoire;average case analysis;asymptotic analysis;ecuacion diferencial;greedy algorithm;ensemble aleatoire;optimization;analyse algorithme;asymptotic approximation;probability model;asymptotic behaviour;bipartite graph;set cover;random set;analisis algoritmo;aproximacion asintotica;markov chain;conjunto aleatorio	Abstract   A general framework is presented for the asymptotic analysis of greedy algorithms for several optimisation problems such as hitting set, set cover, set packing, etc. applied on random set systems. The probability model used is specified by the size  n  of the ground set, the size  m  of the set system and the common distribution of each of its components. The asymptotic behaviour of each algorithm is studied when  n  and  m  tend to ∞, with   m  n   a fixed constant. The main tools used are the generation of random families of sets via random bipartite graphs and the approximation of Markov chains with small steps by solutions of ordinary differential equations.		Joël Blot;Wenceslas Fernandez de la Vega;Vangelis Th. Paschos;Rachid Saad	1995	Theor. Comput. Sci.	10.1016/0304-3975(95)00242-O	random graph;ordinary differential equation;markov chain;mathematical optimization;combinatorics;greedy algorithm;discrete mathematics;asymptotic analysis;set packing;solution set;bipartite graph;random compact set;mathematics;maximal independent set;k-approximation of k-hitting set;differential equation;set function;algorithm;infinite set;statistics	Theory	28.141709152357897	14.141901438084144	77190
1f6e28ce93010d1b55cda43fe4b044b0b95746ee	additive and multiplicative tolerance in multiobjective linear programming	fonction rationnelle;multiobjective programming;programacion fraccionaria;programmation multiobjectif;analisis sensibilidad;optimisation;temps polynomial;optimizacion;multiobjective linear programming;programmation fractionnaire;analyse tolerance;fractional programming;fonction objectif;objective function;programacion lineal;tolerance analysis;generalized fractional programming;sensitivity analysis;tolerancia;polynomial time;linear programming;programmation lineaire;analyse sensibilite;funcion objetivo;optimization;tolerance;funcion racional;efficient point;rational function;tiempo polinomial;programacion multiobjetivo	We consider a multiobjective linear program and the coefficients of the multiobjective function are subject to some uncertainties. Let x be an efficient point. We propose a procedure to compute an additive and multiplicative (percentage) tolerance in which all the objective function coefficients may simultaneously and independently vary while preserving the efficiency of x. Although the tolerances are not maximal in general, they are satisfactorily large. If x is a nondegenerate basic solution, then the procedure runs in a polynomial time.	additive model;coefficient;linear programming;loss function;maximal set;optimization problem;polynomial;time complexity;utility functions on indivisible goods	Milan Hladík	2008	Oper. Res. Lett.	10.1016/j.orl.2007.10.002	fractional programming;time complexity;rational function;mathematical optimization;linear programming;calculus;mathematics;sensitivity analysis;algorithm	Crypto	26.29947102925013	11.4995788072544	77345
ca0382ce02b7b49a3e01d345dbcbb0801da18338	exploiting multi-layer graph factorization for multi-attributed graph matching		Multi-attributed graph matching is a problem of finding correspondences between two sets of data while considering their complex properties described in multiple attributes. However, the information of multiple attributes is likely to be oversimplified during a process that makes an integrated attribute, and this degrades the matching accuracy. For that reason, a multi-layer graph structure-based algorithm has been proposed recently. It can effectively avoid the problem by separating attributes into multiple layers. Nonetheless, there are several remaining issues such as a scalability problem caused by the huge matrix to describe the multi-layer structure and a back-projection problem caused by the continuous relaxation of the quadratic assignment problem. In this work, we propose a novel multiattributed graph matching algorithm based on the multilayer graph factorization. We reformulate the problem to be solved with several small matrices that are obtained by factorizing the multi-layer structure. Then, we solve the problem using a convex-concave relaxation procedure for the multi-layer structure. The proposed algorithm exhibits better performance than state-of-the-art algorithms based on the single-layer structure.	algorithm;attributed graph grammar;concave function;experiment;graph factorization;incidence matrix;layer (electronics);linear programming relaxation;matching (graph theory);performance;processor affinity;quadratic assignment problem;scalability;synthetic intelligence;willow	Han-Mu Park;Kuk-Jin Yoon	2017	CoRR		mathematical optimization;combinatorics;discrete mathematics;graph bandwidth;null graph;graph partition;3-dimensional matching;machine learning;route inspection problem;mathematics;assignment problem;moral graph;complement graph;strength of a graph;closure problem	AI	28.755441847541867	9.850633394003452	77499
6570c9a2e26bbf0af823f6a5d1731497e05b7510	path optimization and near-greedy analysis for graph partitioning: an empirical study	empirical study;simulated annealing algorithm;graph partitioning;computational complexity	This paper presents the results of an experimental study of nranh uartitionina. We describe a new heuristic technioue, path optimizhon, and its application to two varia&& of graph partitioning: the -&ax-cut problem and the min-quotient-cut problem. We present the results of comnutational COmDahOnS between this techniaue and the Kernighan-Lin algorithm, the simulated annealing algorithm, the FLOW-algorithm of [17], the multilevel spectral algorithm of [14], and the recent 0.878-approximation algorithm of [7]. The experiments were conducted on two classes of graphs that have become standard for such tests: random and random geometric. They show that for both classes of inputs and both variations of the problem, the new heuristic is competitive with the other algorithms, and holds a advantage for min-quotient-cut when applied to very large, sparse geometric graphs (10,000 100,000 vertices, average degree 5 10). In the last part of the paper, we describe an approach to analyzing graph partitioning algorithms from the statistical point of view. Every partitioning of a graph is viewed as a result achieved by a “near greedy” partitioning algorithm. The experiments show that for “good” partitionings, the number of non-greedy steps needed to obtain them i: quite small: moreover. it is “statisticallv” smaller for better partitionings. This led us to conjecture that there exists an ‘Loptimal” distribution of the non-greedy steps that 0 the classes of graphs that we studied.	experiment;graph (discrete mathematics);graph partition;greedy algorithm;heuristic;kernighan–lin algorithm;maxima and minima;simulated annealing;sparse matrix	Jonathan W. Berry;Mark K. Goldberg	1995			theta graph;pathwidth;mathematical optimization;maximum cut;combinatorics;discrete mathematics;graph bandwidth;simulated annealing;null graph;computer science;graph partition;mathematics;voltage graph;computational complexity theory;empirical research;algorithm	Theory	25.360755112204977	6.509349587143697	77532
e0b10745bafd8cbf8ae0e4339a86356d655a4047	the window corner algorithm for planning translational paths among polyhedral subassemblies	robot sensing systems;interior point;search space;path planning;path planning robotic assembly robotics and automation orbital robotics testing robot sensing systems systems engineering and theory technology planning collision avoidance algorithm design and analysis;testing;orbital robotics;three dimensional;systems engineering and theory;technology planning;col;space complexity;robotic assembly;collision avoidance;geometric constraints;algorithm design and analysis;robotics and automation	In this paper, we present a complete and exact algorithm for planning a geometrically feasible path for a polyhedral subassembly (robot) translating amongst polyhedral obstacles. Our algorithm can be used to determine whether a pair of non-convex subassemblies can be separated by a sequence of fine-motions and, possibly, contact-motions, when the amount of free space tends to be severly limited. We present the three-dimensional Window Corner algorithm to plan feasible paths. The algorithm solves the FeosiblePath problem, in worst-case time O ( m 5 ) where m is the product of the number of vertices describing the robot and obstacles respectively. A Feasiblepath solution hierarchy, for assembly sequence planning, is described for incremental, straight-line and multi-step paths. The Polyhedral Cone Representation (PCR) is introduced to efficiently represent the geometrical constraints on translation and as a basis for f a s t collision avoidance checks. We introduce the concept of Window Corners in the PCR and this results in a finite search space. The Polyhedral Cone Obstacle Representation (PCUR), which is a constructive representation of all boundary contact configurations, transforms the problem into that of a point moving amongst a collection, O(m), of convex obstacles. In comparison, the worstcase space complexity of the C-space boundary is O(m3). Certain contact-constrained, feasible motions, which lie on surfaces without interior points can be inaccessible in a t y p i d C-space representation.	best, worst and average case;cone;dspace;exact algorithm;polyhedral;robot;window function	S. S. Krishnan;Arthur C. Sanderson	1992		10.1109/IROS.1992.587414	three-dimensional space;algorithm design;mathematical optimization;simulation;computer science;interior point method;mathematics;geometry;motion planning;software testing;dspace;mountain pass	Robotics	30.677945019697844	17.626913149013397	78744
cf1ac58c01a7f2d93b5af008aa0c5ad0e3ee2125	effects of rooting via out-groups on in-group topology in phylogeny	rooting;upga;phylogeny;out group;neighbour joining	Users of phylogenetic methods require rooted trees, because the direction of time depends on the placement of the root. While phylogenetic trees are typically rooted by using an out-group, this mechanism is inappropriate when the addition of an out-group changes the in-group topology. We perform a formal analysis of phylogenetic algorithms under the inclusion of distant out-groups. It turns out that linkage-based algorithms (including UPGMA) and a class of bisecting methods do not modify the topology of the in-group when an out-group is included. By contrast, the popular neighbour joining algorithm fails this property in a strong sense: every data set can have its structure destroyed by some arbitrarily distant outlier. Furthermore, including multiple outliers can lead to an arbitrary topology on the in-group. The standard rooting approach that uses out-groups may be fundamentally unsuited for neighbour joining.		Margareta Ackerman;Daniel G. Brown;David Loker	2014	International journal of bioinformatics research and applications	10.1504/IJBRA.2014.062993	biology;botany;ingroups and outgroups;bioinformatics;phylogenetics	Comp.	31.95683303849301	11.2281338735452	78962
3875f5b3408a43c382a3e7e5b62930fdc515d309	regularly random duality		In this paper we look at a class of random optimization problems. We discuss ways that can help determine typical behavior of their solutions. When the dimensions of the optimization problems are large such an information often can be obtained without actually solving the original problems. Moreover, we also discover that fairly often one can actually determine many quantities of interest (such as, for example, the typical optimal values of the objective functions) completely analytically. We present a few general ideas and emphasize that the range of applications is enormous.	automated theorem proving;constrained optimization;linear programming;mathematical optimization;optimization problem;principle of good enough;random optimization;randomness;theory	Mihailo Stojnic	2013	CoRR		mathematical optimization;combinatorics;mathematics	Theory	30.039307217370094	7.820445745239645	79299
bf4e66015bf28c0aa9bf4599fa68fd73222d6193	multi-label wireless interference identification with convolutional neural networks		The steadily growing use of license-free frequency bands requires reliable coexistence management and therefore proper wireless interference identification (WII). In this work, we propose a WII approach based upon a deep convolutional neural network (CNN) which classifies multiple IEEE 802.15.1, IEEE 802.11 b/g and IEEE 802.15.4 interfering signals in the presence of a utilized signal. The generated multi-label dataset contains frequencyand time-limited sensing snapshots with the bandwidth of 10 MHz and duration of 12.8 μs, respectively. Each snapshot combines one utilized signal with up to multiple interfering signals. The approach shows promising results for same-technology interference with a classification accuracy of approximately 100 % for IEEE 802.15.1 and IEEE 802.15.4 signals. For IEEE 802.11 b/g signals the accuracy increases for cross-technology interference with at least 90 %.	artificial neural network;bandlimiting;bluetooth;coexist (image);convolutional neural network;frequency band;interference (communication);multi-label classification;neural networks;snapshot (computer storage);wii	Sergej Grunau;Dimitri Block;Uwe Meier	2018	CoRR			Embedded	35.04121023546488	9.248750235339436	79321
196ecb038f6507ceb330156ded48d341f5886f61	solving markov random fields with spectral relaxation	quadratic program;markov random field;approximate solution;graph cut;maximum aposteriori	Markov Random Fields (MRFs) are used in a large array of computer vision applications. Finding the Maximum Aposteriori (MAP) solution of an MRF is in general intractable, and one has to resort to approximate solutions, such as Belief Propagation, Graph Cuts, or more recently, approaches based on quadratic programming. We propose a novel type of approximation, Spectral relaxation to Quadratic Programming (SQP). We show our method offers tighter bounds than recently published work, while at the same time being computationally efficient. We compare our method to other algorithms on random MRFs in various settings.	algorithmic efficiency;approximation algorithm;belief propagation;computer vision;cut (graph theory);linear programming relaxation;markov chain;markov random field;relaxation (approximation);sequential quadratic programming;software propagation	Timothée Cour;Jianbo Shi	2007			conductance;random graph;markov chain;mathematical optimization;maximum-entropy markov model;markov kernel;combinatorics;discrete mathematics;random field;markov property;markov process;markov chain mixing time;markov model;variable-order markov model	ML	33.77936883664906	6.903132629541988	79805
b710169eb8588ea1a707f02c798df78673ad28f8	determining an optimal visualization of physically realizable symbol maps	integer linear programming;computational geometry;data association;computational geometry optimal visualization determination physically realizable symbol map visualization proportional symbol maps cartographers geoscience professionals data visualization geopositioned statistical data physically realizable drawing visualization np hard problem integer programming model decomposition technique;data visualisation;visualization;statistical analysis;integer programming;computer experiment;computational complexity;cartography;face stacking measurement polynomials color data visualization integer linear programming;statistical analysis cartography computational complexity computational geometry data visualisation integer programming;integer linear programming visualization cartography computational geometry;integer program;integer linear program	Proportional symbol maps are an often used tool to aid cartographers and geo-science professionals to visualize data associated with events (e.g., earthquakes) or geo-positioned statistical data (e.g., population). At specific locations, symbols are placed and scaled so that their areas become proportional to the magnitudes of the events or data. Recent work approaches the problem of drawing these symbols algorithmically and defines metrics to be optimized to attain different kinds of drawings. We focus specifically on optimizing the visualization of physically realizable drawings of opaque disks by maximizing the sum of the visible borders of such disks. As this problem has been proven to be NP-hard, we provide an integer programming model for its solution along with decomposition techniques designed to decrease the size of input instances. We present computational experiments to assess the performance of our model as well as the effectiveness of our decomposition techniques.	algorithm;cartography;computation;exact algorithm;experiment;floppy disk;integer programming;linear programming formulation;map;mathematical optimization;np-hardness;programming model;real life;stacking	Guilherme Kunigami;Pedro Jussieu de Rezende;Cid C. de Souza;Tallys H. Yunes	2011	2011 24th SIBGRAPI Conference on Graphics, Patterns and Images	10.1109/SIBGRAPI.2011.12	combinatorics;computer science;theoretical computer science;machine learning	Visualization	28.80696126630046	13.875816905952917	80590
8522ea8992655aa0b55fa92f54f5fa5efcfca278	a mixed integer approach for time-dependent gas network optimization	time dependent;sos condition;piece wise linear approximation;simulated annealing algorithm;mixed integer program;transient gas optimization;piece wise linear;network optimization;mixed integer programming;global optimization;branch and cut;nonlinear optimization	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	adaptive grammar;apple sos;approximation algorithm;branch and cut;expectation–maximization algorithm;francis;flow network;heuristic;mathematical optimization;maxima and minima;nl (complexity);nonlinear system;primary source;time complexity	Debora Mahlke;Alexander Martin;Susanne Moritz	2010	Optimization Methods and Software	10.1080/10556780903270886	optimization problem;mathematical optimization;simulated annealing;nonlinear programming;branch and price;control theory;mathematics;branch and cut;global optimization	Robotics	29.562103739151087	6.095612990009497	80597
f4cd39f80e6330b55bc1cc2777fdb6fdf6ab4b63	machine learning for subproblem selection	geometric graph;combinatorial optimization problem;graph coloring;optimization problem;general solution;machine learning;direct search	Subproblem generation, solution, and recombination is a standard approach to combinatorial optimization problems. In many settings identifying suitable subproblems is itself a significant component of the technique. Such subproblems are often identified using a heuristic rule. Here we show how to use machine learning to make this identification. In particular we use a learned objective function to direct search in an appropriate space of subproblem decompositions. We demonstrate the efficacy of our technique for problem decomposition on a particular wellknown combinatorial optimization problem, graph coloring for geometric graphs.	combinatorial optimization;crossover (genetic algorithm);graph coloring;heuristic;machine learning;mathematical optimization;optimization problem	Robert Moll;Theodore J. Perkins;Andrew G. Barto	2000			optimization problem;mathematical optimization;factor-critical graph;combinatorics;graph bandwidth;combinatorial optimization;machine learning;graph coloring;mathematics;assignment problem;graph;closure problem;tree decomposition	ML	24.901681948319887	5.614008266907122	80839
3ec83013017c6c229d6c098ba01acbecf6f982fe	an sdp approach to multi-level crossing minimization	layered graph;sdp approach;optimal solution;sparse graph;aforementioned specialized sdp;minimization problem;small graph;denser graph;new sdp formulation;ilp approach;sdp formulation	We present an approach based on semidefinite programs (SDP) to tackle the multi-level crossing minimization problem. We are given a layered graph (i.e., the graph's vertices are assigned to multiple parallel levels) and are asked for an ordering of the nodes on each level such that, when drawing the graph with straight lines, the resulting number of crossings is minimized. Solving this step is crucial in what is probably the most widely used graph drawing scheme, the Sugiyama framework.  The problem has received a lot of attention in both the fields of heuristics and exact methods. For a long time, integer linear programming (ILP) approaches were the only exact algorithms applicable, at least for small graphs. Recently, SDP formulations for the special case of two levels were proposed and dominated the ILP for dense instances.  In this article, we present a new SDP formulation for the general multi-level version that, for two levels, is even stronger than the aforementioned specialized SDP. As a by-product, we also obtain an SDP-based heuristic, which in practice always gives (near-)optimal solutions.  We conduct a large set of experiments, both on randomized and on real-world instances, and compare our approach to a state-of-the-art ILP-based branch-and-cut implementation. The SDP clearly dominates for denser graphs, while the ILP approach is usually faster for sparse instances. However, even for such sparse graphs, the SDP solves more instances to optimality than the ILP. In fact, there is no single instance that the ILP solved that the SDP did not. Overall, our experiments reveal that, for sparse graphs, one should usually try to find an optimal solution with the ILP first. If this approach does not solve the instance to optimality within reasonable time, the SDP still has a good chance to do so.  Being able to solve larger real-world instances than reported before, we are also able to evaluate heuristics for this problem. In this article, we do so for the traditional barycenter-heuristic (showing that it leaves a large gap to the true optimum) and the state-of-the-art upward-planarization method (showing that it is usually close to the optimum).	branch and cut;crossing number (graph theory);experiment;graph drawing;graph theory;heuristic (computer science);integer programming;linear programming;planarization;randomized algorithm;semidefinite programming;single-instance storage;sparse matrix	Markus Chimani;Philipp Hungerländer;Michael Jünger;Petra Mutzel	2011	ACM Journal of Experimental Algorithmics	10.1145/2133803.2330084	mathematical optimization;combinatorics;mathematics;algorithm	AI	25.095660614763975	7.5805605242718626	81018
28c870d9ebffdd609fc61ad0189a58ff274a8e98	classification by polynomial surfaces	fonction booleenne;sample size;polynomial surface;surface polynomiale;boolean function;classification;fonction seuil;upper bound;expressive power;funcion umbral;funcion booliana;hiperplano;threshold function;reseau neuronal;hyperplane;vapnik chervonenkis;clasificacion;red neuronal;hyperplan;vc dimension;probably approximately correct;artificial neural network;neural network	Linear threshold functions (for real and Boolean inputs) have received much attention, for they are the component parts of many artificial neural networks. Linear threshold functions are exactly those functions such that the positive and negative examples are separated by a hyperplane. One extension of this notion is to allow separators to be surfaces whose equations are polynomials of at most a given degree (linear separation being the degree-1 case). We investigate the representational and expressive power of polynomial separators. Restricting to the Boolean domain, by using an upper bound on the number of functions defined on {0, 1}n by polynomial separators having at most a given degree, we show, as conjectured by Wang and Williams [26], that for almost every Boolean function, one needs a polynomial surface of degree at least bn/2c in order to separate the negative examples from the positive examples. Further, we show that, for odd n, at most half of all Boolean functions are realisable by a separating surface of degree bn/2c. We then compute the Vapnik-Chervonenkis dimension of the class of functions realised by polynomial separating surfaces of at most a given degree, both for the case of Boolean inputs and real inputs. In the case of linear separators, the VC dimensions coincide for these two cases, but for surfaces of higher degree, there is a strict divergence. We then use these results on the VC dimension to quantify the sample size required for valid generalisation in Valiant’s probably approximately correct framework [24, 6].	alexey chervonenkis;artificial neural network;graham scan;linear algebra;michael saks (mathematician);polynomial;probably approximately correct learning;test set;vc dimension	Martin Anthony	1995	Discrete Applied Mathematics	10.1016/0166-218X(94)00008-2	sample size determination;mathematical optimization;combinatorics;discrete mathematics;vc dimension;biological classification;hyperplane;degree of a polynomial;mathematics;boolean function;upper and lower bounds;expressive power;probably approximately correct learning;artificial neural network;algorithm	Theory	33.94322109460283	13.326624659920657	81120
7565aeab8fd9584a94c698974ec3e243e0f73e59	editors' foreword		The 12th Annual International Computing and Combinatorics Conference (COCOON’2006) was held at the Institute of Information Science, Academia Sinica, in Taipei, Taiwan on August 15–18, 2006. 137 extended abstracts were submitted from 27 countries and regions in the world, of which 52 were accepted (including a merged paper from two extended abstracts). Among those accepted extended abstracts, five very good papers in the area of geometric computing and applications were selected and invited to this special issue of IJCGA for COCOON’2006. These five papers all went through the normal review process of IJCGA and were accepted for publication. The papers in this issue represent various aspects of the current geometric computing research. Benkert et al.’s paper presents a theoretical attack on a variant of a geometric packing problem, called geometric dispersion problem, giving a polynomial time algorithm that computes an approximate solution. Aleksandrowicz and Barequet’s paper considers the problems of counting different polycubes in d-D and nonrectangular polyominoes in 2-D, generalizing an algorithm by Redelmeier for counting 2-D rectangular polyominoes. Wu et al.’s paper studies a matrix partition problem (or partitioning a discrete geometric space) that arises in intensitymodulated radiation therapy (IMRT). Poon’s paper is concerned with straightening trees and convexifying polygons in the lattice settings, giving efficient unfolding algorithms for the tasks. Andersson et al.’s paper focuses on solving the problem of simplifying a planar triangle mesh using edge contraction operations.	approximation algorithm;edge contraction;geometric median;information science;p (complexity);partition problem;polynomial;set packing;triangle mesh;unfolding (dsp implementation)	Danny Ziyi Chen;D. T. Lee	2009	Int. J. Comput. Geometry Appl.	10.1142/S0218195909002915		Theory	27.74382902956437	17.428746728540855	81745
4be78298ec6cfa790075f56bbaa6c6805555f0db	finding the minimum-distance schedule for a boundary searcher with a flashlight	minimum distance;space complexity;algorithm design	Consider a dark polygonal region in which intruders move freely, trying to avoid detection. A robot, which is equipped with a flashlight, moves along the polygon boundary to illuminate all intruders. We want to minimize the total distance traveled by the robot until all intruders are detected in the worst case. We present an O(nlogn) time and O(n) space algorithm for optimizing this metric, where n is the number of vertices of the given polygon. This improves upon the best known time and space complexities of O(n2) and O(n2), respectively. The distance graph plays a critical role in our analysis and algorithm design.		Tsunehiko Kameda;Ichiro Suzuki;John Z. Zhang	2010		10.1007/978-3-642-12200-2_9	algorithm design;combinatorics;simulation;computer science;mathematics;geometry;dspace;algorithm	Robotics	30.621281181893668	18.01681717841179	81858
0920991ecfabfc2c9d4ae56e3c02c0ad3e18b01c	monte-carlo algorithms for the improvement of finite-state stochastic controllers: application to bayes-adaptive markov decision processes			algorithm;markov chain;markov decision process	Michael O. Duff	2001			mathematical optimization;machine learning;artificial intelligence;partially observable markov decision process;markov decision process;computer science;monte carlo method;bayes' theorem;markov model	Logic	38.465890459740194	4.478567088200232	82002
6f9214ce5eda70dda19dcbacc77fbcd968ccc34a	weak stochastic ordering for multidimensional markov chains	file attente;chaine markov;cadena markov;funcion utilidad;transition probability;fonction utilite;queueing theory;utility function;satisfiability;multidimensional lattice;multivariate process;processus multivarie;marginal distribution;ordre stochastique faible;processus markov;stochastic order;probabilidad transicion;gestion stock;ley marginal;proceso multivariable;markov processes;reseau multidimensionnel;inventory control;probabilite transition;loi marginale;markov chain	In this paper we study weak stochastic ordering on multidimensional lattices. The main objective of our study is to derive comparison of multidimensional marginal distributions of two Markov chains. We are able to prove this when the matrices of transition probabilities satisfy monotonicity and comparability properties.	marginal model;markov chain	Ad Ridder	1995	Oper. Res. Lett.	10.1016/0167-6377(95)00045-3	econometrics;markov chain;combinatorics;examples of markov chains;mathematics;statistics	Logic	38.4493635652905	6.139562380453847	82033
db3bf6375f4298fa324324466b773966c614b406	an optimal boundary to quadtree conversion algorithm	technical report departmental;historical collection till dec 2001	An algorithm is presented for converting a boundary representation for an image to its region quadtree representation. Our algorithm is designed for operation on the linear quadtree representation, although it can easily be modified for the traditional pointer-based quadtree representation. The algorithm is a two phase process that first creates linear quadtree node records for each of the border pixels. This list of pixels is then sorted by locational code. The second processing phase fills in the nodes interior to the polygons by simulating a traversal of the corresponding pointer-based quadtree. Three previous algorithms have described similar conversion routines requiring time complexity of 0( n I?) for at least one of the two phases, where B is the number of boundary pixels and n is the depth of the final tree for a 2” x 2” image. A fourth algorithm, developed by Webber, can perform the border construction of this conversion in time O(n + B) with the restriction that the polygon must be positioned at constrained locations in the image space. Our algorithm requires time O(n + B) for the second phase, which is optimal. The first phase can be performed using the algorithm of Webber for total conversion time of O(n + B) with constrained location, in time O(I3 log B) using a simple sort to order the border pixels with no restriction in polygon location, or by a Jordan sequence sorting algorithm in time O(B) also with no restriction in polygon location. o EV	boundary representation;extended validation certificate;mod (video gaming);pixel;pointer (computer programming);quadtree;simulation;sorting algorithm;time complexity;tree traversal	Mark R. Lattanzi;Clifford A. Shaffer	1991	CVGIP: Image Understanding	10.1016/1049-9660(91)90018-K	computer vision;discrete mathematics;computer science;theoretical computer science;quadtree;mathematics;algorithm	Graphics	31.753975548002835	18.206674530461786	82095
6c4469417ce5cf628045a1082e1debe7272c6b32	path planning above a polyhedral terrain	shortest path;euclidean space path planning polyhedral terrain approximation algorithm;approximate algorithm;approximation algorithms;path planning;approximation algorithm;path planning approximation algorithms extraterrestrial measurements shortest path problem computer science robot motion motion planning euclidean distance;euclidean distance;approximation theory;computational complexity;robots;robots approximation theory computational complexity path planning;polyhedral terrain;motion planning;euclidean space;robot motion;computer science;extraterrestrial measurements;shortest path problem	We consider the problem of path planning above a polyhedral terrain and present a new algorithm that for any p ges 1, computes a (c + epsi)-approximation to the Lp-shortest path above a polyhedral terrain in O(n/epsi log n log log n) time and O(n log n) space, where n is the number of vertices of the terrain, and c = 2(p-1)p/. This leads to an epsi-approximation algorithm for the problem in L1 metric, and a (radic2 + epsi)-factor approximation algorithm in Euclidean space	approximation algorithm;motion planning;polyhedral terrain;polyhedron;shortest path problem	Hamid Zarrabi-Zadeh	2006	Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.	10.1109/ROBOT.2006.1641819	mathematical optimization;combinatorics;computer science;artificial intelligence;euclidean shortest path;mathematics;geometry;motion planning;shortest path problem;approximation algorithm	Robotics	30.151371268063354	18.17340108335354	82118
12604fa64372d0b32cc1ba8446601fa651ced110	the skeletons you find when you order your ideal's closet		The best-known algorithms to compute a Gröbner basis of an ideal are static, in that they require a term ordering as input, and return a Gröbner basis with respect to that same ordering. The choice of ordering can have a critical effect on the time and memory consumed, and while some orderings have a reputation for efficiency, they are not always a good choice, let alone the best choice. A dynamic algorithm to compute a Gröbner basis does not require a term ordering as input, but tries to compute an efficient term ordering as the computation proceeds, returning both a final ordering, and a Gröbner basis with respect to that ordering. Past implementations have relied on linear programming to do this, but the associated linear programs grow very large, very quickly, putting many ideals of interest out of this technique’s reach. The solution set to these linear programs takes the form of a polyhedral cone, and a dynamic algorithm can use the cone’s skeleton to reduce drastically both the size and number of linear programs, making it possible to compute ideals that were previously beyond the dynamic algorithm’s practical reach. This talk, based on joint work with Massimo Caboara of the University of Pisa, reviews the idea of a dynamic algorithm, describes the new technique, and demonstrates its benefits.	algorithm;computation;convex cone;dynamic problem (algorithms);gröbner basis;linear programming	John Perry	2015	ACM Comm. Computer Algebra	10.1145/2815111.2815159	mathematics;combinatorics;closet	PL	26.59288939457332	14.2397207501859	82412
d7c1a2af71a96afb8af50c4fdd974c87f451762b	markov decision processes with uncertain parameters			markov chain;markov decision process	Dimitri Scheftelowitsch	2018				ML	38.50222047325872	4.572276945271308	83250
63275d187d41b3988459e66f28b5534f7fe5497b	on deletion in delaunay triangulation		This paper present how space of spheres and shelling can be used to delete e ciently a point from d-dimensional triangulation. In 2-dimension, if k is the degree of the deleted vertex, the complexity is O(k log k), but we notice that this number apply only to low cost operations; time consuming computations are done only a linear number of times. This algorithm can be viewed as a variation of Heller algorithm [Hel90, Mid93] which is popular in the geographic information system community. Unfortunalty Heller algorithm is false as explained in this paper. Key-words: computational geometry, geometric computing, Delaunay triangulation, dynamic algorithms. This work was partially supported by ESPRIT LTR 21957 (CGAL) Suppressions dans la triangulation de Delaunay. Résumé : Cet article montre comment l'espace des sphères et l'e euillage (shelling) peuvent être utilisés pour implémenter e cacement la suppression d'un point dans la triangulation de Delaunay. En dimension 2, si k est le degré du sommet supprimé, la complexité est de O(k log k). On peut remarquer que cette complexité ne s'applique qu'à des opérations assez bon marché, les calculs les plus coûteux n'étant qu'en nombre linéaire. Cet algorithme peut être vu comme une variation d'un algorithme proposé par Heller [Hel90, Mid93] populaire dans le domaine des systèmes d'information géographique. Malheureusement l'algorithme original de Heller est faux. Mots-clés : géométrie algorithmique, calcul géométrique, triangulation de Delaunay, algorithmes dynamiques. On deletion in Delaunay triangulation 3	algorithm;bibliothèque de l'école des chartes;cgal;computation;computational geometry;delaunay triangulation;dynamic problem (algorithms);geographic information system;linear algebra;zero suppression	Olivier Devillers	1995	CoRR			Theory	30.854721994029404	16.335678413571735	83525
2ccd2afa07ba1194e962daa653bb17423ef75738	on markov chains for independent sets	processus melangeant;conjunto independiente;graph theory;luby vigoda chain;maximum degree;teoria grafo;chaine markov;cadena markov;temps polynomial;proceso mezclante;independent set;chaine luby vigoda;temps melangeage;processus stationnaire;theorie graphe;upper bound;statistical physics;mixing process;ensemble independant;stationary distribution;state space;mixing time;polynomial time;proceso estacionario;borne superieure;rapidly mixing markov chains;stationary process;tiempo mezcladura;cota superior;markov chain;tiempo polinomial	Random independent sets in graphs arise, for example, in statistical physics, in the hard-core model of a gas. In 1997, Luby and Vigoda described a rapidly mixing Markov chain for independent sets, which we refer to as the Luby–Vigoda chain. A new rapidly mixing Markov chain for independent sets is defined in this paper. Using path coupling, we obtain a polynomial upper bound for the mixing time of the new chain for a certain range of values of the parameter λ. This range is wider than the range for which the mixing time of the Luby–Vigoda chain is known to be polynomially bounded. Moreover, the upper bound on the mixing time of the new chain is always smaller than the best known upper bound on the mixing time of the Luby–Vigoda chain for larger values of λ (unless the maximum degree of the graph is 4). An extension of the chain to independent sets in hypergraphs is described. This chain gives an efficient method for approximately counting the number of independent sets of hypergraphs with maximum degree two, or with maximum degree three and maximum edge size three. Finally, we describe a method which allows one, under certain circumstances, to deduce the rapid mixing of one Markov chain from the rapid mixing of another, with the same state space and stationary distribution. This method is applied to two Markov chains for independent sets, a simple insert/delete chain and the new chain, to show that the insert/delete chain is rapidly mixing for a wider range of values of λ than was previously known.	degree (graph theory);independent set (graph theory);markov chain;michael luby;polynomial;state space;stationary process	Martin E. Dyer;Catherine S. Greenhill	2000	J. Algorithms	10.1006/jagm.1999.1071	time complexity;stationary process;markov chain;combinatorics;discrete phase-type distribution;stationary distribution;discrete mathematics;independent set;state space;continuous-time markov chain;graph theory;balance equation;calculus;mathematics;additive markov chain;mixing;markov chain mixing time;upper and lower bounds;absorbing markov chain	Theory	37.850495014016154	14.771599184274699	83639
560f79127b2ac35bfc855f14dae801428024fcb3	approximation bounds for minimum information loss microaggregation	loss measurement;anonymity;graph theory;k anonymity;centro gravitacional;teoria grafo;euclidean theory;approximation bounds;approximate algorithm;descomposicion grafo;information loss;centre gravite;partition donnee;distance measure;information security;minimum information loss microaggregation;microdata protection;approximation algorithms;approximation algorithm;center of mass;heuristic method;securite informatique;statistical databases;data partition;problema np duro;metodo heuristico;euclidean distances;euclidean distance;grupo puntual;theorie graphe;squared distance measure;polynomials;fonction objectif;disclosure control;anonymat;protection approximation algorithms loss measurement polynomials approximation methods data security information security partitioning algorithms data privacy np hard problem;computer security;objective function;groupe ponctuel;base donnee statistique;protection;np hard problem;graph partitioning;np hard microaggregation problem;point group;data privacy;probleme np difficile;perdida informacion;computational complexity;security of data computational complexity data privacy;seguridad informatica;microdato;microdonnee;algoritmo aproximacion;theorie euclidienne;funcion objetivo;approximation methods;group centroid;methode heuristique;microaggregation;algorithme approximation;information loss data security disclosure control microdata protection microaggregation k anonymity approximation algorithms graph partitioning;confidentialite donnee;perte information;security of data;graph decomposition;particion dato;teoria euclidiana;partitioning algorithms;decomposition graphe;anonimato;squared distance measure approximation bounds minimum information loss microaggregation np hard microaggregation problem euclidean distances group centroid;microdata;data security	The NP-hard microaggregation problem seeks a partition of data points into groups of minimum specified size k, so as to minimize the sum of the squared euclidean distances of every point to its group's centroid. One recent heuristic provides an O(k3) guarantee for this objective function and an O(k2) guarantee for a version of the problem that seeks to minimize the sum of the distances of the points to its group's centroid. This paper establishes approximation bounds for another microaggregation heuristic, providing better approximation guarantees of O(k2) for the squared distance measure and O(k) for the distance measure.	approximation;data point;heuristic;loss function;optimization problem	M. Laszlo;S. Mukherjee	2009	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2009.78	microdata;center of mass;mathematical optimization;combinatorics;discrete mathematics;anonymity;computer science;graph partition;np-hard;data mining;euclidean distance;mathematics;data security;point group;computational complexity theory;approximation algorithm;algorithm;statistics;polynomial	DB	26.103042201622248	17.186494082058122	83705
5b6621a827a4dffe8dcba428342fbfbc0b03adf1	a new approximation algorithm for labeling points with circle pairs	approximate algorithm;procesamiento informacion;algorithm analysis;geometrie algorithmique;approximation algorithm;computational geometry;problema np duro;etiquetage application;space time;circle packing;espacio tiempo;np hard problem;probleme np difficile;informatique theorique;map labeling;garnissage cercle;information processing;algoritmo aproximacion;geometria computacional;analyse algorithme;algorithme approximation;traitement information;analisis algoritmo;espace temps;computer theory;informatica teorica	We study the NP-hard problem of labeling points with maximum-radius circle pairs: given n point sites in the plane, find a placement for 2n interior-disjoint uniform circles, such that each site touches two circles and the circle radius is maximized. We present a new approximation algorithm for this problem that runs in O(n logn + n log ε ) time and O(n) space and achieves an approximation factor of (2 + √3 + 2 √ 4 + √3)/(4 + √3)+ε (≈ 1.486+ε), which improves the previous best bound of 1.491+ε. © 2006 Elsevier B.V. All rights reserved.	approximation algorithm;the circle (file system)	Minghui Jiang	2006	Inf. Process. Lett.	10.1016/j.ipl.2006.04.006	combinatorics;circle packing;information processing;computational geometry;computer science;calculus;space time;np-hard;1 + 2 + 4 + 8 + ⋯;mathematics;approximation algorithm;algorithm	Theory	29.34856072098821	17.311418180218197	83859
3fe8538c623e72724762420778b9d7c863ab22e0	on time-symmetry in cellular automata	time symmetry;universality;reversibility;cellular automata;decidability	The notion of reversibility has been intensively studied in the field of cellular automata (CA), for several reasons. However, a related notion found in physical theories has been so far neglected, not only in CA, but generally in discrete dynamical systems. This is the notion of time-symmetry, which refers to the inability of distinguishing between backward and forward time directions. Here we formalize it in the context of CA, and study some of its basic properties. We also show how some well-known CA fit into the class of time-symmetric CA, and provide a number of results on the relation between this and other classes of CA. The existence of an intrinsically universal time-symmetric CA within the class of reversible CA is proved. Finally, we show the undecidability of time-symmetry for CA of dimension 2 or higher, even within the class of reversible CA. The case of dimension 1 is one of several open questions discussed in the conclusions.	automata theory;cellular automaton	Anahí Gajardo;Jarkko Kari;Andrés Moreira	2012	J. Comput. Syst. Sci.	10.1016/j.jcss.2012.01.006	decidability;cellular automaton;discrete mathematics;computer science;universality;mathematics;t-symmetry;algorithm	Theory	39.02442126581709	8.990436193427662	84414
e32d36d75fa7c660128fad7e8afdc0c22382da2b	cutting resilient networks - complete binary trees		In our previous work [2], we introduced the random k-cut number for rooted graphs. In this paper, we show that the distribution of the k-cut number in complete binary trees of size n, after rescaling, is asymptotically a periodic function of lg n − lg lg n. Thus there are different limit distributions for different subsequences, where these limits are similar to weakly 1-stable distributions. This generalizes the result for the case k = 1, i.e., the traditional cutting model, by Janson [10].	binary tree;minimum k-cut	Xing Shi Cai;Cecilia Holmgren	2018	CoRR			Theory	37.716186446495165	16.920701763884182	84581
3b55cbc190ef25a2879bdd97c08fb62cdc748288	generic cuts: an efficient algorithm for optimal inference in higher order mrf-map	higher order mrf map;submodular function minimization;optimal algorithm	We propose a new algorithm called Generic Cuts for computing optimal solutions to 2 label MRF-MAP problems with higher order clique potentials satisfying submodularity. The algorithm runs in time O(2n) in the worst case (k is clique order and n is the number of pixels). A special gadget is introduced to model flows in a high order clique and a technique for building a flow graph is specified. Based on the primal dual structure of the optimization problem the notions of capacity of an edge and cut are generalized to define a flow problem. We show that in this flow graph max flow is equal to min cut which also is the optimal solution to the problem when potentials are submodular. This is in contrast to all prevalent techniques of optimizing Boolean energy functions involving higher order potentials including those based on reductions to quadratic potential functions which provide only approximate solutions even for submodular functions. We show experimentally that our implementation of the Generic Cuts algorithm is more than an order of magnitude faster than all algorithms including reduction based whose outputs on submodular potentials are near optimal.	analysis of algorithms;approximation algorithm;best, worst and average case;clique (graph theory);coefficient;computable function;experiment;flow network;image resolution;markov random field;mathematical optimization;maxima and minima;maximum flow problem;minimum cut;optimization problem;pixel;polynomial;primitive recursive function;quantum harmonic oscillator;sparse matrix;submodular set function;time complexity	Chetan Arora;Subhashis Banerjee;Prem Kumar Kalra;S. N. Maheshwari	2012		10.1007/978-3-642-33715-4_2	mathematical optimization;combinatorics;discrete mathematics;mathematics	Vision	33.28554167087822	7.3997179021988355	84917
1b954e35b30e47061c30666c9eb113ff746838a6	randomized pursuit-evasion in a polygonal environment	randomized algorithms;robots equations game theory pursuit algorithms path planning collision avoidance air traffic control engineering profession information technology information science;path planning;randomised algorithms;robots randomized pursuit evasion polygonal environment visibility pursuit evasion problem deterministic strategy randomized algorithm;noncooperative game theory;prior knowledge;deterministic algorithms;path planning randomised algorithms deterministic algorithms robots;robots;randomized algorithm;dynamic noncooperative game theory;pursuit evasion games;randomized algorithms dynamic noncooperative game theory path planning pursuit evasion games	"""This paper contains two main results. First, we revisit the well-known visibility-based pursuit-evasion problem, and show that in contrast to deterministic strategies, a single pursuer can locate an unpredictable evader in any simply connected polygonal environment, using a randomized strategy. The evader can be arbitrarily faster than the pursuer, and it may know the position of the pursuer at all times, but it does not have prior knowledge of the random decisions made by the pursuer. Second, using the randomized algorithm, together with the solution to a problem called the """"lion and man problem"""" as subroutines, we present a strategy for two pursuers (one of which is at least as fast as the evader) to quickly capture an evader in a simply connected polygonal environment. We show how this strategy can be extended to obtain a strategy for a polygonal room with a door, two pursuers who have only line-of-sight communication, and a single pursuer (at the expense of increased capture time)."""	apache axis;average-case complexity;data structure;deterministic algorithm;dual graph;evasion (network security);integrated development environment;line-of-sight (missile);medial graph;polynomial;preprocessor;pursuit-evasion;randomized algorithm;subroutine;whole earth 'lectronic link	Volkan Isler;Sampath Kannan;Sanjeev Khanna	2005	IEEE Transactions on Robotics	10.1109/TRO.2005.851373	randomized algorithms as zero-sum games;mathematical optimization;simulation;computer science;artificial intelligence;distributed computing;randomized algorithm	Robotics	31.348411771181915	13.440260985921947	85031
e184e126d4b482767db4f4dbf197b55caa998519	heuristic improvement through triangulation	learning;euclidean distance;multi dimensional;search;multi dimensional heuristics;large classes	Abstract This paper presents a method for improving heuristics using a triangulation technique. Instead of using a heuristic to directly estimate distance (X1, X2) between nodes X1 and X2, the proposed technique selects a reference node Ri applies the heuristic to (X1,Ri) and (X2,Ri), and uses the Euclidean distance formula to calculate a new heuristic value. If two nodes are close to each other, then they should also be approximately equidistant to a third reference node. Utilizing a set of many such reference nodes, node expansions can be reduced for a large class of heuristics. Very early results for this method, referred to as multi-dimensional heuristics, showed that fewer node expansions were needed when using the triangulation technique. New results in this paper include the development of a new learning procedure for selecting reference nodes, experimentation on reusing reference node sets for multiple goal instances, a comparison of multi-dimensional heuristics with weighting and how they dynamic...	heuristic;triangulation (geometry)	Peter C. Nelson;Christopher M. Lain	1995	J. Exp. Theor. Artif. Intell.	10.1080/09528139508953808	mathematical optimization;euclidean distance	AI	30.56885241243794	13.603036021940019	85127
6cc782f60e69039975c60d452c8fbd32bdc3865e	quadtree decomposition, steiner triangulation, and ray shooting	problema arbol steiner;algorithm complexity;algorithm analysis;geometrie algorithmique;complejidad algoritmo;computational geometry;probleme arbre steiner;optimisation combinatoire;complexite algorithme;geometria computacional;analyse algorithme;steiner tree problem;algorithme approximation;combinatorial optimization;ray shooting;analisis algoritmo;optimizacion combinatoria	We present a new quadtree-based decomposition of a polygon possibly with holes. For a polygon of n vertices, a truncated decomposition can be computed in O(nlog n) time which yields a Steiner tri-angulation of the interior of the polygon that has O(n log n) size and approximates the minimum weight Steiner triangulation (MWST) to within a constant factor. An approximate MWST is good for ray shooting in the average case as deened by Aronov and Fortune. The untruncated decomposition also yields an approximate MWST. Moreover, we show that this triangulation supports query-sensitive ray shooting as deened by Mitchell, Mount, and Suri. Hence, there exists a Steiner triangulation that is simultaneously good for ray shooting in the query-sensitive sense and in the average case.	approximation algorithm;best, worst and average case;boris aronov;minimum weight;quadtree;ray casting;steiner tree problem;triangular function	Siu-Wing Cheng;Kam-Hing Lee	1998		10.1007/3-540-49381-6_39	mathematical optimization;minimum-weight triangulation;combinatorics;steiner tree problem;computational geometry;combinatorial optimization;pitteway triangulation;calculus;point set triangulation;mathematics;geometry;polygon triangulation	Theory	29.04659641252401	17.709334700660104	85154
4d46a787633321bc76709d5e3a2008740645431d	fast, near-optimal computation for multi-robot path planning on graphs	integer linear programming;optimality;makespan;divide and conquer;multi robot path planning	We report a new method for computing near optimal makespan solutions to multi-robot path planning problem on graphs. Our focus here is with hard instances those with up to 85% of all graph nodes occupied by robots. Our method yields 100-1000x speedup compared with existing methods. At the same time, our solutions have much smaller and often optimal makespans. Introduction and Problem Formulation In this paper, we study centralized multi-robot path planning problems on graphs, also known as cooperative pathfinding (Silver 2005; Ryan 2008; Standley and Korf 2011; Surynek 2012b). Our focus is on finding plans with optimal or near optimal makespans for problems in which the graph nodes are heavily populated with robots. This problem finds many direct applications in AI and robotics, including microfludics (Ding, Chakrabarty, and Fair 2001) and warehouse automation (Wurman, D’Andrea, and Mountz 2007). Let G = (V,E) be a connected, undirected, simple graph with vertex set V = {vi} and edge set E = {(vi, vj)}. Let R = {r1, . . . , rn} be a set of robots that move with unit speeds along the edges of G, with initial and goal locations on G given by the injective maps xI , xG : R → V , respectively. A path is a map pi : Z → V . A path pi is feasible for a robot ri if it satisfies the following properties: (1) pi(0) = xI(ri), (2) for each i, there exists a smallest k i ∈ Z such that for all k ≥ k i , pi(k) ≡ xG(ri), and (3) for any 0 ≤ k < k i , (pi(k), pi(k+1)) ∈ E or pi(k) = pi(k+1). We say that two paths pi, pj are in collision if there exists k ∈ Z such that pi(k) = pj(k) (collision on a vertex, or meet) or (pi(k), pi(k + 1)) = (pj(k + 1), pj(k)) (collision on an edge, or head-on). If p(k) = p(k + 1), then the robot stays at vertex p(k) between the time steps k and k + 1. Problem (MPPpr). Given (G,R, xI , xG), find a set of paths P = {p1, . . . , pn} such that pi’s are feasible paths for respective robots ri’s and no two paths pi, pj are in collision. ∗This work was supported in part by NSF grant 0904501 (IIS Robotics), NSF grant 1035345 (Cyberphysical Systems), MURI/ONR grant N00014-09-1-1052, and AFOSR grant FA955012-1-0193. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Note that MPPpr (“pr” stands for parallel and rotation, respectively) allows rotations of robots along fully occupied cycles, which differs from the pebble motion problem (PMG for short) studied in (Kornhauser, Miller, and Spirakis 1984) and its parallel extension (MPPp for short) studied in (Silver 2005; Surynek 2012b). Since robots capable of moving in parallel should also be able to move along fully occupied cycles, MPPpr is a more natural model than MPPp. A natural criterion for measuring path set optimality is the number of time steps until the last robot reaches its goal. This is sometimes called the makespan, which can be computed from {k i } for a feasible path set P as TP = max1≤i≤n k min i . Our goal in this paper is to compute solutions to MPPpr and MPPp that minimizes the makespan TP . Our Method In (Yu and LaValle 2013a), a complete algorithm was given for solving MPPpr with minimum makespan using multiflow. Let us denote the integer linear programming (ILP) based algorithm as BASEILP. Since the problem is NP-Hard (Yu and LaValle 2013b), fast computational method for solving the minimum makespan MPPpr must resort to heuristics and approximations. We provide such a heuristic here, augmented by additional heuristics obtained via exploring the structure of the ILP model. The k-splitting heuristic. Our key heuristic uses the simple idea of divide-and-conquer, with the goal of obtaining smaller MPPpr instances that are then fed to the BASEILP algorithm. Due to limited space, we only provide a high level description of the heuristic here, using k = 2. For each robot ri, given its start vertex, xI(ri), and its goal vertex, xG(ri), we compute all vertices of G such that the resulting set of vertices are of equal distance to xI(ri) and xG(ri) or differ in distance by at most one (i.e., such a vertex mi satisfies the property |dist(xI(ri),mi) − dist(xG(ri),mi)| ≤ 1), ignoring the presence of other robots. Let this set of vertices be Mi. A member of Mi is then picked as a waypoint for robot ri. Denote the set of waypoints as {w1, . . . , wn}. This splits the original problem into two subproblems. One of the problem has xI(R) ({wi}) as the start (goal) configuration and the other problem has {wi} (xG(R)) as the start (goal) configuration. It is possible that Mi ⊆ {w1, . . . , wi−1}, i.e., the “best” waypoints for ri are already used by a robot. In this case, Mi is	approximation;artificial intelligence;centralized computing;computation;genetic algorithm;graph (discrete mathematics);heuristic (computer science);high-level programming language;ibm notes;integer programming;internet information services;linear programming;makespan;maxima and minima;motion planning;multiflow;np-hardness;pathfinding;population;robot;robotics;speedup;vertex (graph theory);waypoint	Jingjin Yu;Steven M. LaValle	2013			job shop scheduling;mathematical optimization;divide and conquer algorithms;integer programming;any-angle path planning;longest path problem;computer science;distributed computing	AI	28.769524897721915	11.988577632314568	85305
8b620d4e378151dd35aa00c9f4c00e441e5a670e	11 \times 11 domineering is solved: the first player wins		We have developed a program called MUDoS (Maastricht University Domineering Solver) that solves Domineering positions in a very efficient way. This enables the solution of known positions so far (up to the 10 x 10 board) much quicker (measured in number of investigated nodes). #R##N#More importantly, it enables the solution of the 11 x 11 Domineering board, a board up till now far out of reach of previous Domineering solvers. The solution needed the investigation of 259,689,994,008 nodes, using almost half a year of computation time on a single simple desktop computer. The results show that under optimal play the first player wins the 11 x 11 Domineering game, irrespective if Vertical or Horizontal starts the game. #R##N#In addition, several other boards hitherto unsolved were solved. Using the convention that Vertical starts, the 8 x 15, 11 x 9, 12 x 8, 12 x 15, 14 x 8, and 17 x 6 boards are all won by Vertical, whereas the 6 x 17, 8 x 12, 9 x 11, and 11 x 10 boards are all won by Horizontal.		Jos W. H. M. Uiterwijk	2016		10.1007/978-3-319-50935-8_12	simulation;mathematics;operations research;algorithm	Crypto	29.721773681767036	13.220032814851093	85399
ca45a012a924c14671843b6ca35821263214f577	querying approximate shortest paths in anisotropic regions	camino mas corto;disque;obstaculo;shortest path;distance function;disk;time complexity;approximation algorithms;approximation algorithm;aproximacion;computational geometry;plus court chemin;pregunta documental;numero real;space time;espacio tiempo;disco;approximation;unit;complexite temps;convex function;estructura datos;algoritmo aproximacion;coste;chemin plus court;query;68p05;structure donnee;68u05;real number;nombre reel;algorithme approximation;complejidad tiempo;fonction convexe;68w25;data structure;anisotropic regions;convex distance functions;espace temps;obstacle;requete;unite;funcion convexa;unidad;cout	We present a data structure for answering approximate shortest path queries in a planar subdivision from a fixed source. Let $\rho\geqslant1$ be a real number. Distances in each face of this subdivision are measured by a possibly asymmetric convex distance function whose unit disk is contained in a concentric unit Euclidean disk and contains a concentric Euclidean disk with radius $1/\rho$. Different convex distance functions may be used for different faces, and obstacles are allowed. Let $\varepsilon$ be any number strictly between 0 and 1. Our data structure returns a $(1+\varepsilon)$ approximation of the shortest path cost from the fixed source to a query destination in $O(\log\frac{\rho n}{\varepsilon})$ time. Afterwards, a $(1+\varepsilon)$-approximate shortest path can be reported in $O(\log n)$ time plus the complexity of the path. The data structure uses $O(\frac{\rho^2n^3}{\varepsilon^2}\log\frac{\rho n}{\varepsilon})$ space and can be built in $O(\frac{\rho^2n^3}{\varepsilon^2}(\log\frac{\rho n}{\varepsilon})^2)$ time. Our time and space bounds do not depend on any other parameter; in particular, they do not depend on any geometric parameter of the subdivision such as the minimum angle.		Siu-Wing Cheng;Hyeon-Suk Na;Antoine Vigneron;Yajun Wang	2010	SIAM J. Comput.	10.1137/080742166	convex function;time complexity;mathematical optimization;combinatorics;unit;metric;input/output;approximation;calculus;space time;mathematics;geometry;shortest path problem;approximation algorithm;algorithm;real number	Theory	29.75120834140694	17.847804341140357	86017
9ebc777b368988ccd703c99a7ecb9f26d2652f72	mpl6: enhanced multilevel mixed-size placement	legalization;mixed size placement;force directed placement;helmholtz equation;multilevel optimization	The multilevel placement package mPL6 combines improved implementations of the global placer mPL5 (ISPD05) and the XDP legalizer and detailed placer (ASPDAC06). It consistently produces robust, high-quality solutions to difficult instances of mixed-size placement in fast and scalable run time. Best-choice clustering (ISPD05) is used to construct a hierarchy of problem formulations. Generalized force-directed placement guides global placement at each level of the cluster hierarchy. During the declustering pass from coarsest to finest level, large movable objects are gradually fixed in positions without overlapping with one another. This progressive legalization of large objects during continuous optimization supports determination of a completely overlap-free configuration as close as possible to the continuous solution. Various discrete heuristics are applied to this legalized placement in order to improve the final wirelength.	cluster analysis;continuous optimization;force-directed graph drawing;heuristic (computer science);mathematical optimization;robustness (computer science);run time (program lifecycle phase);scalability;xml data package	Tony F. Chan;Jason Cong;Joseph R. Shinnerl;Kenton Sze;Min Xie	2006		10.1145/1123008.1123055	mathematical optimization;parallel computing;mathematics;helmholtz equation;physics;quantum mechanics;placement	EDA	28.858694132863718	10.493260069064199	86540
1aef8839a1233c80d1f6b86f591802abee0e4f3b	constraint-driven clustering	minimum variance;efficient algorithm;satisfiability;sensor network;objective function;np hardness;clustering;dynamic data structure;clustering method;number of clusters;experimental evaluation;cluster model;market segmentation;constraints	Clustering methods can be either data-driven or need-driven. Data-driven methods intend to discover the true structure of the underlying data while need-driven methods aims at organizing the true structure to meet certain application requirements. Thus, need-driven (e.g. constrained) clustering is able to find more useful and actionable clusters in applications such as energy aware sensor networks, privacy preservation, and market segmentation. However, the existing methods of constrained clustering require users to provide the number of clusters, which is often unknown in advance, but has a crucial impact on the clustering result. In this paper, we argue that a more natural way to generate actionable clusters is to let the application-specific constraints decide the number of clusters. For this purpose, we introduce a novel cluster model, Constraint-Driven Clustering (CDC), which finds an a priori unspecified number of compact clusters that satisfy all user-provided constraints. Two general types of constraints are considered, i.e. minimum significance constraints and minimum variance constraints, as well as combinations of these two types. We prove the NP-hardness of the CDC problem with different constraints. We propose a novel dynamic data structure, the CD-Tree, which organizes data points in leaf nodes such that each leaf node approximately satisfies the CDC constraints and minimizes the objective function. Based on CD-Trees, we develop an efficient algorithm to solve the new clustering problem. Our experimental evaluation on synthetic and real datasets demonstrates the quality of the generated clusters and the scalability of the algorithm.	algorithm;cluster analysis;constrained clustering;data point;data structure;dynamic data;loss function;np-hardness;optimization problem;organizing (structure);requirement;scalability;synthetic intelligence;tree (data structure)	Rong Ge;Martin Ester;Wen Jin;Ian Davidson	2007		10.1145/1281192.1281229	correlation clustering;constrained clustering;mathematical optimization;determining the number of clusters in a data set;minimum-variance unbiased estimator;data stream clustering;wireless sensor network;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;mathematics;cluster analysis;brown clustering;market segmentation;affinity propagation;satisfiability;clustering high-dimensional data	ML	30.44716411629006	10.358304665433264	87399
80dd418286472fa3fab9ea07a4dc191e72b01813	on sums of independent random variables with unbounded variance, and estimating the average degree in a graph	camino mas corto;shortest path;shortest paths;approximate algorithm;grado grafo;loi probabilite;ley probabilidad;variable aleatoire;inequalities;approximation algorithm;variable aleatoria;plus court chemin;inegalite markov;random variable sum;markov inequality;probability distribution;independent random variables;algoritmo aproximacion;random variable;60e15;degre graphe;estimation statistique;algorithme approximation;suma variable aleatoria;estimacion estadistica;68w25;statistical estimation;68w20;somme variable aleatoire;graph degree	We prove the following inequality: for every positive integer n and every collection X1,..., Xn of nonnegative independent random variables that each has expectation 1, the probability that their sum remains below n+1 is at least α > 0. Our proof produces a value of α = 1/13 ≅ 0.077, but we conjecture that the inequality also holds with α = 1/e ≅ 0.368.As an example for the use of the new inequality, we consider the problem of estimating the average degree of a graph by querying the degrees of some of its vertices. We show the following threshold behavior: approximation factors above 2 require far less queries than approximation factors below 2. The new inequality is used in order to get tight (up to multiplicative constant factors) relations between the number of queries and the quality of the approximation. We show how the degree approximation algorithm can be used in order to quickly find those edges in a network that belong to many shortest paths.	approximation algorithm;degree (graph theory);shortest path problem;social inequality	Uriel Feige	2004		10.1145/1007352.1007443	probability distribution;markov's inequality;random variable;mathematical optimization;combinatorics;discrete mathematics;calculus;inequality;mathematics;shortest path problem;approximation algorithm;statistics	Theory	39.05876030933979	15.55015678977762	87480
800f7f8858b090eabe53c686e6e2365d06df46a6	finite horizon approximations of infinite horizon linear programs	linear program;finite horizon;infinite horizon	This paper describes the class of infinite horizon linear programs that have finite optimal values. A sequence of finite horizon (T period) problems is shown to approximate the infinite horizon problems in the following sense: the optimal values of theT period problems converge monotonically to the optimal value of the infinite problem and the limit of any convergent subsequence of initialT period optimal decisions is an optimal decision for the infinite horizon problem.	approximation;linear programming	Richard Grinold	1977	Math. Program.	10.1007/BF01593765	mathematical optimization;combinatorics;discrete mathematics;linear programming;mathematics	Theory	36.69819700892821	5.373422765855494	87529
0cf80304ba0fa8f3eef0dc5d565360d66b5fa9d5	a generalization of the concept of markov decision process to imprecise probabilities.	imprecise probability;markov decision process			David Harmanec	1999			markov decision process;markov kernel;imprecise probability;partially observable markov decision process;markov property;mathematics;markov renewal process;markov process;markov model;statistics;variable-order markov model	Vision	38.481644416822164	4.660238462783163	87852
1f8d52cf0e09d6ec075123f6128be5f90c3b9aa7	evolving exact integer algorithms with genetic programming	benchmark problems exact integer algorithm synthesis genetic programming gp epistasis deceptiveness transactional memory tm program code program behavior frequency fitness assignment method ffa premature convergence loop instructions;program control structures concurrency control genetic algorithms;training benchmark testing genetic programming linear programming sociology statistics educational institutions	The synthesis of exact integer algorithms is a hard task for Genetic Programming (GP), as it exhibits epistasis and deceptiveness. Most existing studies in this domain only target few and simple problems or test a small set of different representations. In this paper, we present the (to the best of our knowledge) largest study on this domain to date. We first propose a novel benchmark suite of 20 non-trivial problems with a variety of different features. We then test two approaches to reduce the impact of the negative features: (a) a new nested form of Transactional Memory (TM) to reduce epistatic effects by allowing instructions in the program code to be permutated with less impact on the program behavior and (b) our recently published Frequency Fitness Assignment method (FFA) to reduce the chance of premature convergence on deceptive problems. In a full-factorial experiment with six different loop instructions, TM, and FFA, we find that GP is able to solve all benchmark problems, although not all of them with a high success rate. Several interesting algorithms are discovered. FFA has a tremendous positive impact while TM turns out not to be useful.	academy;algorithm;benchmark (computing);emoticon;experiment;genetic programming;premature convergence;transactional memory	Thomas Weise;Mingxu Wan;Ke Tang;Xin Yao	2014	2014 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2014.6900292	genetic programming;mathematical optimization;computer science;theoretical computer science;machine learning;genetic representation;inductive programming;algorithm	AI	27.720328747497742	4.6150171013389905	88146
a82707785f45fe6c2bcbc19aea5d2992fb661492	new algorithm for modeling s-box in milp based differential and division trail search		This paper studies an automated differential-trail search against block ciphers in which the problem of finding the optimal trail is converted to one of finding the optimal solution in a mixed-integer-linear programming (MILP). The most difficult part is representing differential properties of an S-box, known as differential distribution table (DDT), with a system of inequalities. Previous work builds the system by using a general-purpose mathematical tool, SAGE Math. However, the generated system for general-purpose contains a lot of redundant inequalities for the purpose of differential-trail search, thus inefficient. Hence, an auxiliary algorithm was introduced to minimize the number of inequalities by hoping that it minimizes the runtime to solve the MILP. This paper proposes a new algorithm to improve this auxiliary algorithm. The main advantage is that while the previous algorithm does not ensure the minimum number of inequalities, the proposed algorithm does ensure it. Moreover it enables the users to choose the number of inequalities in the system. In addition, this paper experimentally shows that the above folklore “minimizing the number of inequalities minimizes the runtime” is not always correct. The proposed algorithm can also be used in the MILP-based division-trail search, which evaluates the bit-based division property for integral attacks.	algorithm;s-box	Yu F Sasaki;Yosuke Todo	2017		10.1007/978-3-319-69284-5_11	inequality;block cipher;s-box;mathematical optimization;algorithm;computer science;greedy algorithm	Vision	26.031993046644967	8.993577392582377	88375
96908c65dfac2b26e122f8ebd8a00602477fc5f6	weber's problem with attraction and repulsion under polyhedral gauges	location theory;location problem;distance function;discretization;objective function;weighted sums;geometrical algorithms;polynomial algorithm;geometric algorithm;global optimization;large classes;structural properties	Given a nite set of points in the plane and a forbidden region R, we want to nd a point X 6 2 int(R), such that the weighted sum to all given points is minimized. This location problem is a variant of the well-known Weber Problem, where we measure the distance by polyhedral gauges and allow each of the weights to be positive or negative. The unit ball of a polyhedral gauge may be any convex polyhedron containing the origin. This large class of distance functions allows very general (practical) settings { such as asymmetry { to be modeled. Each given point is allowed to have its own gauge and the forbidden region R enables us to include negative information in the model. Additionally the use of negative and positive weights allows to include the level of attraction or dislikeness of a new facility. Polynomial algorithms and structural properties for this global optimization problem (d.c. objective function and a non-convex feasible set) based on combinatorial and geometrical methods are presented.	algorithm;computational geometry;discretization;feasible region;global optimization;mathematical optimization;optimization problem;polyhedral;polynomial;quantum information;time complexity;weber problem;weight function;whole earth 'lectronic link	Stefan Nickel;Eva-Maria Dudenhöffer	1997	J. Global Optimization	10.1023/A:1008235107372	mathematical optimization;combinatorics;location theory;metric;discretization;mathematics;geometry;1-center problem;global optimization	Theory	26.14091323677734	15.044751023323876	88455
47d978d6f205942485acdb81c21581180b36f198	finitely additive dynamic programming	dynamic programming;finitely additive probability;markov decision processes	The theory of dynamic programming is formulated using finitely additive probability measures defined on sets of arbitrary cardinality. Many results from the conventional countably additive theory generalize, and the proofs are simpler.	additive model;dynamic programming	William D. Sudderth	2016	Math. Oper. Res.	10.1287/moor.2015.0717	markov decision process;mathematical optimization;combinatorics;discrete mathematics;ba space;dynamic programming;mathematics	Theory	37.596846877669186	5.29669211764874	89466
de8b8c2b6112d6d1a64de5fb9f3f8d95ea00030c	the minimum manhattan distance and minimum jump of permutations		Abstract Let π be a permutation of { 1 , 2 , … , n } . If we identify a permutation with its graph, namely the set of n dots at positions ( i , π ( i ) ) , it is natural to consider the minimum L 1 (Manhattan) distance, d ( π ) , between any pair of dots. The paper computes the expected value (and higher moments) of d ( π ) when n → ∞ and π is chosen uniformly, and settles a conjecture of Bevan, Homberger and Tenner (motivated by permutation patterns), showing that when d is fixed and n → ∞ , the probability that d ( π ) ≥ d + 2 tends to e − d 2 − d . The minimum jump mj ( π ) of π , defined by mj ( π ) = min 1 ≤ i ≤ n − 1 ⁡ | π ( i + 1 ) − π ( i ) | , is another natural measure in this context. The paper computes the asymptotic moments of mj ( π ) , and the asymptotic probability that mj ( π ) ≥ d + 1 for any constant d .	taxicab geometry	Simon R. Blackburn;Cheyne Homberger;Peter Winkler	2019	J. Comb. Theory, Ser. A	10.1016/j.jcta.2018.09.002	euclidean distance;combinatorics;mathematics;permutation;conjecture;expected value;graph;jump	Theory	38.296292997371665	16.831506051650948	89730
14c841fbe60d4b61fd4678277b2184b53b526540	developing a practical projection-based parallel delaunay algorithm	parallel algorithm;delaunay triangulation;visual events;conversative visibility;hierarchical representations;scientific computing;linearized dynamic aspect graphs;temporal coherence;convex hull;octrees;uniform distribution	Guy E. Blelloch Gary L. Miller Dafna Talmor Computer Science Department Carnegie Mellon University {blelloch, glmiller, talmor}@cs. emu. edu In this paper we are concerned with developing a practical parallel algorithm for Delaunay triangulation that works well on general distributions, particularly those that arise in Scientific Computation. Although there have been many theoretical algorithms for the problem, and some implementations based on bucketing that work well for uniform distributions, there has been little work on implementations for general distributions. We use the well known reduction of 2D Delaunay triangulation to 3D convex hull of points on a sphere or paraboloid. A variant of the Edelsbrunner and Shi 3D convex hull is used, but for the special case when the point set lies on either a sphere or a paraboloid. Our variant greatly reduces the constant costs from the 3D convex hull algorithm and seems to be a more promising for a practical implementation than other parallel approaches. We have run experiments on the algorithm using a variety of distributions that are motivated by various problems that use Delaunay triangulations. Our experiments show that for these distributions we are within a factor of approximately two in work from the best sequential algorithm.	computation;computational science;computer science;convex hull;delaunay triangulation;experiment;parallel algorithm;sequential algorithm;whole earth 'lectronic link	Guy E. Blelloch;Gary L. Miller;Dafna Talmor	1996		10.1145/237218.237357	mathematical optimization;minimum-weight triangulation;combinatorics;delaunay triangulation;pitteway triangulation;convex hull;point set triangulation;mathematics;geometry;parallel algorithm;constrained delaunay triangulation;chew's second algorithm;uniform distribution;bowyer–watson algorithm	Theory	32.30732425884073	16.666431868714675	89829
1431833a809884cd90be6952cac93462e7525f37	results on learnability and the vapnik-chervonenkis dimension (extended abstract)	finite domain learnability vapnik chervonenkis dimension distribution free model dynamic sampling finite concept set;distribution free model;learning systems;finite domain;learning from examples;finite concept set;computer science probability distribution distributed computing error analysis sampling methods electronic mail time measurement;dynamic sampling;vapnik chervonenkis;vapnik chervonenkis dimension;vc dimension;learnability	The problem of learning a concept from examples in a distribution-free model is considered. The notion of dynamic sampling, wherein the number of examples examined can increase with the complexity of the target concept, is introduced. This method is used to establish the learnability of various concept classes with an infinite Vapnik-Chervonenkis (VC) dimension. An important variation on the problem of learning from examples, called approximating from examples, is also discussed. The problem of computing the VC dimension of a finite concept set defined on a finite domain is considered. >	alexey chervonenkis;learnability;vc dimension	Nathan Linial;Yishay Mansour;Ronald L. Rivest	1988		10.1109/SFCS.1988.21930	combinatorics;discrete mathematics;structural risk minimization;vc dimension;computer science;machine learning;mathematics	Theory	34.61824096391747	12.603655808985437	89891
63d428751cbffe0a62d32b24a3b1b514970a5254	calculating kolmogorov complexity from the output frequency distributions of small turing machines	engineering;health research;uk clinical guidelines;graph theory;biological patents;public library of science;europe pubmed central;citation search;computer and information sciences;biology;computer applications;physics;uk phd theses thesis;probability distribution;open access;probability theory;chemistry;inclusive;life sciences;ante disciplinary;algorithms;medicine;approximation methods;plos;uk research reports;medical journals;information theory;europe pmc;biomedical research;bioinformatics	Drawing on various notions from theoretical computer science, we present a novel numerical approach, motivated by the notion of algorithmic probability, to the problem of approximating the Kolmogorov-Chaitin complexity of short strings. The method is an alternative to the traditional lossless compression algorithms, which it may complement, the two being serviceable for different string lengths. We provide a thorough analysis for all Σ(n=1)(11) 2(n) binary strings of length n<12 and for most strings of length 12≤n≤16 by running all ~2.5 x 10(13) Turing machines with 5 states and 2 symbols (8 x 22(9) with reduction techniques) using the most standard formalism of Turing machines, used in for example the Busy Beaver problem. We address the question of stability and error estimation, the sensitivity of the continued application of the method for wider coverage and better accuracy, and provide statistical evidence suggesting robustness. As with compression algorithms, this work promises to deliver a range of applications, and to provide insight into the question of complexity calculation of finite (and short) strings. Additional material can be found at the Algorithmic Nature Group website at http://www.algorithmicnature.org. An Online Algorithmic Complexity Calculator implementing this technique and making the data available to the research community is accessible at http://www.complexitycalculator.com.	algorithm;algorithmic probability;busy beaver;complement system proteins;computational complexity theory;data compression;kolmogorov complexity;lossless compression;numerical analysis;semantics (computer science);string (computer science);theoretical computer science;turing machine;web site	Fernando Soler-Toscano;Nicolas Gauvrit	2014		10.1371/journal.pone.0096223	probability distribution;probability theory;complexity;medicine;time hierarchy theorem;information theory;nspace;bioinformatics;graph theory;computer applications;computational complexity theory;super-recursive algorithm	Theory	38.47950780512695	11.776765736482213	90007
47c4c13cd14237b849c34063f1d19d557314eee7	an improved method for solving multiobjective integer linear fractional programming problem		We describe an improvement of Chergui and Moula&#xef;&#x2019;s method (2008) that generates the whole efficient set of a multiobjective integer linear fractional program based on the branch and cut concept. The general step of this method consists in optimizing (maximizing without loss of generality) one of the fractional objective functions over a subset of the original continuous feasible set; then if necessary, a branching process is carried out until obtaining an integer feasible solution. At this stage, an efficient cut is built from the criteria&#x2019;s growth directions in order to discard a part of the feasible domain containing only nonefficient solutions. Our contribution concerns firstly the optimization process where a linear program that we define later will be solved at each step rather than a fractional linear program. Secondly, local ideal and nadir points will be used as bounds to prune some branches leading to nonefficient solutions. The computational experiments show that the new method outperforms the old one in all the treated instances.	fractional programming;linear-fractional programming	Meriem Ait Mehdi;Mohamed El-Amine Chergui;Moncef Abbas	2014	ADS	10.1155/2014/306456	mathematical optimization;combinatorics;mathematics;algorithm;branch and cut	Logic	25.135823529216903	11.025883769473795	90048
5a9e8d1865e2e954289abac4091f321ce56e698f	online successive convex approximation for two-stage stochastic nonconvex optimization		Two-stage stochastic optimization, in which a long-term master problem is coupled with a family of short-term subproblems, plays a critical role in various application areas. However, most existing algorithms for two-stage stochastic optimization only work for special cases, and/or are based on the batch method, which requires huge memory and computational complexity. To the best of our knowledge, there still lack efficient and general two-stage online stochastic optimization algorithms. This paper proposes a two-stage online successive convex approximation (TOSCA) algorithm for general two-stage nonconvex stochastic optimization problems. At each iteration, the TOSCA algorithm first solves one short-term subproblem associated with the current realization of the system state. Then, it constructs a convex surrogate function for the objective of the long-term master problem. Finally, the long-term variables are updated by solving a convex approximation problem obtained by replacing the objective function in the long-term master problem with the convex surrogate function. We establish the almost sure convergence of the TOSCA algorithm and customize the algorithmic framework to solve three important application problems. Simulations show that the TOSCA algorithm can achieve superior performance over existing solutions.	algorithm;approximation;computational complexity theory;computer simulation;iteration;mathematical optimization;optimization problem;program optimization;stochastic optimization	An Liu;Vincent K. N. Lau;Minjian Zhao	2018	IEEE Transactions on Signal Processing	10.1109/TSP.2018.2871389	convergence of random variables;mathematical optimization;stochastic optimization;computational complexity theory;regular polygon;approximation algorithm;mathematics;linear programming;convergence (routing)	ML	32.97929153078066	4.546062161156414	90158
d357d8dfffddf9b6dcbfc1b401137abf36e4873b	adaptive control of average markov decision chains under the lyapunov stability condition	metodo lyapunov;lyapunov stability;politica optima;chaine markov;cadena markov;ams classification 90c40;funcion lyapunov;adaptive optimal policy;recompense;lyapunov function;adaptive control;transition;decision markov;93e20;optimal policy;satisfiability;value iteration;key words schweitzer s transformation;recompensa;reward;transicion;iteraccion;lyapunov method;consistent estimator;fonction lyapunov;non stationary value iteration;state space;iteration;discrepancy function;markov decision;markov decision process;politique adaptee;politique optimale;consistent estimation;methode lyapunov;transformation schweitzer;pause control;markov chain	This note concerns discrete-time Markov decision processes with denumerable state space. A control policy is graded by the long-run expected average reward criterion, and the main feature of the model is that the reward function and the transition law depend on an unknown parameter. Besides standard continuity-compactness restrictions, it is supposed that the controller can use the observed history to generate a consistent estimation scheme, and that the system's transition-reward structure satisfies an adaptive version of the Lyapunov function condition. Within this context, a special implementation of the non stationary value iteration method is studied, and it is shown that this technique produces convergent approximations to the solution of the optimality equation, result that is used to build an optimal adaptive policy.	lyapunov fractal;markov chain	Rolando Cavazos-Cadena	2001	Math. Meth. of OR	10.1007/s001860100138	markov decision process;mathematical optimization;adaptive control;calculus;control theory;mathematics;q-learning;statistics	Theory	38.43300839190043	5.642980404759718	90368
21fa6ad5b8fb578ba5afcbcb800d778e861dadbd	a motion planner for car-like robots based on a mixed global/local approach	topology;shortest path;obstacle avoidance mobile robots path planning car like robots mixed global local approach motion planning shortest paths induced topology sub riemannian geometry;topology computational geometry mobile robots planning artificial intelligence;computational geometry;planning artificial intelligence;mobile robots;configuration space;obstacle avoidance;nonholonomic mobile robot;sub riemannian geometry;motion planning;orbital robotics mobile robots motion planning robotic assembly space stations manipulators jacobian matrices turning topology geometry;lower bound	Deals with the problem of motion planning for a car-like robot (i.e. nonholonomic mobile robot whose turning radius is lower bounded). The main contribution is the introduction of a new metric in the configuration space R/sup 2/*S/sup 1/ of such a system. This metric is defined from the length of the shortest paths in the absence of obstacles. The authors study the relations between the new induced topology and the classical one. This study leads to new theoretical issues about sub-Riemannian geometry and to practical results for motion planning. In particular they prove an inclusion relation of neighbourhoods in both topologies, which is the basis of an efficient obstacle avoidance local method. >	robot	Jean-Paul Laumond;Michel Taïx;Paul E. Jacobs	1990		10.1109/IROS.1990.262494	mobile robot;configuration space;mathematical optimization;topology;computational geometry;computer science;artificial intelligence;mathematics;geometry;motion planning;obstacle avoidance;upper and lower bounds;shortest path problem	Robotics	31.655087304979613	17.307754798397756	90644
80aa1c3c55f87e7cbce9f3e17827fa81b5423169	on the monotonization of the training set	monotonic constraints.;machine learning;supervised learning;convex set;total order;maximal independent set;partial order;convex optimization;convex function	We consider the problem of minimal correction of the training set to make it consistent with monotonic constraints. This problem arises during analysis of data sets via techniques that require monotone data. We show that this problem is NPhard in general and is equivalent to finding a maximal independent set in special orgraphs. Practically important cases of that problem considered in detail. These are the cases when a partial order given on the replies set is a total order or has a dimension 2. We show that the second case can be reduced to maximization of a quadratic convex function on a convex set. For this case we construct an approximate polynomial algorithm based on convex optimization.	approximation algorithm;convex function;convex optimization;convex set;expectation–maximization algorithm;independent set (graph theory);mathematical optimization;maximal independent set;maximal set;polynomial;test set;time complexity;vertex cover;monotone	Rustem Takhanov	2007	CoRR			ML	26.702517432309673	15.04672884095967	90831
237ee5a68d881fdd94e662ba2c67e9a5732c6b3b	study of near consensus complex social networks using eigen theory	eigenvalues and eigenfunctions;social network services;complex networks;the australian standard research classification 210000 science general;initial state value consensus complex social network eigen theory nontrivial topological feature steady state value decision certitude threshold value weight matrix vector set;social networking services;social network services eigenvalues and eigenfunctions steady state vectors proposals indexes complex networks;complex network;matrix algebra;social network;indexes;vectors;indexation;h310 dynamics;proposals;vectors complex networks decision making matrix algebra;steady state	This paper extends the definition of an exact consensus complex social network to that of a near consensus complex social network. A near consensus complex social network is a social network with nontrivial topological features and steady state values of the decision certitudes of the majority of the nodes being either higher or lower than a threshold value. By using eigen theories, the relationships among the vectors representing the steady state values of the decision certitudes of the nodes, the influence weight matrix and the set of vectors representing the initial state values of the decision certitudes of the nodes that satisfies a given near consensus specification are characterized.	eigen (c++ library);social network;steady state;theory	Bingo Wing-Kuen Ling;Paul Stewart;Kok Lay Teo;C. K. Michael Tse	2011	2011 IEEE International Symposium of Circuits and Systems (ISCAS)	10.1109/ISCAS.2011.5938014	combinatorics;discrete mathematics;computer science;artificial intelligence;machine learning;uniform consensus;mathematics;complex network	Theory	37.63130017226858	12.471784524835906	90902
ccdea98f710ff0c2e7d263a7214ec3c1e30abeae	computing the minimal tiling path from a physical map by integer linear programming	theoretical framework;physical map;integer linear program	We study the problem of selecting the minimum tiling path (MTP) from a set of clones arranged in a physical map. We formulate the constraints of the MTP problem in a graph theoretical framework, and we derive an optimization problem that is solved via integer linear programming. Experimental results show that when we compare our algorithm to the commonly used software FPC, the MTP produced by our method covers a higher portion of the genome, even using a smaller number of MTP clones. These results suggest that if one would employ the MTP produced by our method instead of FPC’s in a clone-by-clone sequencing project, one would reduce by about 12% the sequencing cost.	algorithm;integer programming;linear programming;map;mathematical optimization;optimization problem;tessellation (computer graphics);tiling window manager	Serdar Bozdag;Timothy J. Close;Stefano Lonardi	2008		10.1007/978-3-540-87361-7_13	mathematical optimization;combinatorics;discrete mathematics;mathematics	EDA	24.64235315187385	7.280479083564534	91201
03b3d48ad482062b32ef0f6d4bc4aba9e3fd7b0f	large-scale binary quadratic optimization using semidefinite relaxation and applications	biological patents;biomedical journals;image segmentation;standards;semidefinite programming;np hard binary quadratic optimization semidefinite relaxation semidefinite programming binary quadratic programs sdp dual optimization smoothing newton methods dual problem quasinewton solver;text mining;standards optimization linear programming symmetric matrices computer vision image segmentation smoothing methods;europe pubmed central;citation search;markov random fields;citation networks;markov random fields binary quadratic optimization semidefinite programming;computer vision;symmetric matrices;smoothing methods;relaxation computational complexity newton method quadratic programming;research articles;abstracts;open access;binary quadratic optimization;life sciences;clinical guidelines;linear programming;optimization;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	In computer vision, many problems can be formulated as binary quadratic programs (BQPs), which are in general NP hard. Finding a solution when the problem is of large size to be of practical interest typically requires relaxation. Semidefinite relaxation usually yields tight bounds, but its computational complexity is high. In this work, we present a semidefinite programming (SDP) formulation for BQPs, with two desirable properties. First, it produces similar bounds to the standard SDP formulation. Second, compared with the conventional SDP formulation, the proposed SDP formulation leads to a considerably more efficient and scalable dual optimization approach. We then propose two solvers, namely, quasi-Newton and smoothing Newton methods, for the simplified dual problem. Both of them are significantly more efficient than standard interior-point methods. Empirically the smoothing Newton solver is faster than the quasi-Newton solver for dense or medium-sized problems, while the quasi-Newton solver is preferable for large sparse/structured problems.	computational complexity theory;computer vision;dual;duality (optimization);interior point method;lagrangian relaxation;linear programming relaxation;mathematical optimization;np-hardness;newton;newton's method;quadratic programming;relaxation (approximation);scalability;semidefinite programming;smoothing (statistical technique);solver;sparse matrix	Peng Wang;Chunhua Shen;Anton van den Hengel	2017	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2016.2541146	mathematical optimization;combinatorics;text mining;computer science;linear programming;theoretical computer science;machine learning;mathematics;image segmentation;semidefinite programming;symmetric matrix	ML	33.109129731318326	7.36822786064428	91242
4faf501ab295d73b2704b0d9f56d25eb5c3c3fd2	on the definition and computation of rectlinear convex hulls	algorithm analysis;enveloppe convexe;combinatorial geometry;analyse algorithme;geometrie combinatoire;convex hull;algorithme optimal;optimal algorithm	Abstract   Recently the computation of the rectilinear convex hull of a collection of rectilinear polygons has been studied by a number of authors. From these studies three distinct definitions of rectilinear convex hulls have emerged. We examine these three definitions for point sets in general, pointing out some of their consequences, and we give optimal algorithms to compute the corresponding rectilinear convex hulls of a finite set of points in the plane.	computation	Thomas Ottmann;Eljas Soisalon-Soininen;Derick Wood	1984	Inf. Sci.	10.1016/0020-0255(84)90025-2	discrete geometry;convex analysis;subderivative;mathematical optimization;combinatorics;convex polytope;convex combination;orthogonal convex hull;convex hull;mathematics;geometry;convex set;proper convex function	DB	29.788424268754223	17.350296108630932	91251
5a72a14897c39618b7da49f0af6f7f29c317fee5	a global-optimization algorithm for mixed-integer nonlinear programs having separable non-convexity	modeling language;computer experiment;global optimization;mixed integer nonlinear programming	We present a global optimization algorithm for MINLPs (mixed-integer nonlinear programs) where any non-convexity is manifested  as sums of non-convex univariate functions. The algorithm is implemented at the level of a modeling language, and we have  had substantial success in our preliminary computational experiments.  	algorithm;nonlinear programming	Claudia D'Ambrosio;Jon Lee;Andreas Wächter	2009		10.1007/978-3-642-04128-0_10	mathematical optimization;discrete mathematics;computer experiment;branch and price;theoretical computer science;mathematics;modeling language;global optimization	Theory	27.840343190242827	6.824403063131754	91665
caa15f0a57c83979cb27e131c4c8d417c6115f86	adjacency of the best and second best valued solutions in combinatorial optimization problems	combinatorial optimization problem;condition suffisante;optimisation combinatoire;politope;condicion suficiente;sufficient condition;combinatorial optimization;adjacence;optimizacion combinatoria;polytope	We say that a polytope satisfies the strong adjacency property if every best valued extreme point of the polytope is adjacent to some second best valued extreme point for any weight vector. Perfect matching polytopes satisfy this property. In this paper, we give sufficient conditions for a polytope to satisfy the strong adjacency property. From this, binary b-matching polytopes, set partitioning polytopes, set packing polytopes, etc. satisfy the strong adjacency property.	combinatorial optimization;mathematical optimization	Yoshiko Ikebe;Tomomi Matsui;Akihisa Tamura	1993	Discrete Applied Mathematics	10.1016/0166-218X(93)90128-B	polytope;mathematical optimization;combinatorics;discrete mathematics;combinatorial optimization;mathematics;polytope model	Theory	25.105200107979808	16.237306011763703	91900
ec26a42f7f3fdff7fe3e711228ae3319b5d79b7a	a hybrid heuristic for the maximum clique problem	maximum clique problem;greedy heuristic;maximum clique;steady state genetic algorithm;combinatorial optimization	In this paper we present a heuristic based steady-state genetic algorithm for the maximum clique problem. The steady-state genetic algorithm generates cliques, which are then extended into maximal cliques by the heuristic. We compare our algorithm with three best evolutionary approaches and the overall best approach, which is non-evolutionary, for the maximum clique problem and find that our algorithm outperforms all the three evolutionary approaches in terms of best and average clique sizes found on majority of DIMACS benchmark instances. However, the obtained results are much inferior to those obtained with the best approach for the maximum clique problem.	clique (graph theory);clique problem;heuristic	Alok Singh;Ashok Kumar Gupta	2006	J. Heuristics	10.1007/s10732-006-3750-x	mathematical optimization;combinatorics;greedy algorithm;combinatorial optimization;computer science;clique problem;machine learning;mathematics;maximum common subgraph isomorphism problem	AI	24.667212224332747	5.293044205401927	92043
e5d9a496caa0b10733341cb44afec00a131ef2e4	a relax-and-cut framework for large-scale maximum weight connected subgraph problems		Finding maximum weight connected subgraphs within networks is a fundamental combinatorial optimization problem both from theoretical and practical standpoints. One of the most prominent applications of this problem appears in Systems Biology and it corresponds to the detection of active subnetworks within gene interaction networks. Due to its importance, several modeling and algorithmic strategies have been proposed for tackling the maximum weight connected subgraph problem (MWCS) over the last years; the most effective strategies typically depend on the use of integer linear programming (ILP). Nonetheless, this implies that largescale networks (such as those appearing in Systems Biology) can become burdensome; moreover, not all practitioners may have access to an ILP solver. In this paper, a unified modeling and algorithmic scheme is designed to solve the MWCS and some of its application-oriented variants with cardinality-constraints or budget-constraints. The proposed framework is based on a general node-based model which is tackled by a Relax-and-Cut scheme, i.e., Lagrangian relaxation combined with constraint generation; this yields a heuristic procedure capable of providing both dual and primal bounds. The approach is enhanced by additional valid inequalities, lifted valid inequalities, primal heuristics and variable-fixing procedures. Computational results on instances from the literature, as well as on additional large-scale instances, show that the proposed framework is competitive with respect to the existing approaches and it allows to find improved solutions for some unsolved instances from literature. The effect of initializing a Branch-and-Cut approach with information from the Relax-and-Cut is also investigated. The implemented approach is made available online. © 2017 Elsevier Ltd. All rights reserved.	apollonian network;branch and cut;combinatorial optimization;computation;heuristic (computer science);integer programming;lagrangian relaxation;linear programming relaxation;mathematical optimization;optimization problem;solver;systems biology	Eduardo Álvarez-Miranda;Markus Sinnl	2017	Computers & OR	10.1016/j.cor.2017.05.015	mathematical optimization;combinatorics;discrete mathematics;mathematics	AI	25.380142180219146	8.090381989856683	92377
a7c1fe4eaad5b77f789faeb87e58011608a9f23c	the optimal statistical median of a convex set of arrays	robust statistics;statistical median and quantile optimization;median optimization;objective function;optimization problem;polynomial time algorithm;branch bound algorithms;global optimization;convex set;convex combination	We consider the following problem. A set $${r^1, r^2,\ldots , r^K \,{\in} \mathbf{R}^T}$$ of vectors is given. We want to find the convex combination $${z = \sum \lambda_j r^j}$$ such that the statistical median of z is maximum. In the application that we have in mind, $${r^j, j=1,\ldots,K}$$ are the historical return arrays of asset j and $${\lambda_j, j=1,\ldots,K}$$ are the portfolio weights. Maximizing the median on a convex set of arrays is a continuous non-differentiable, non-concave optimization problem and it can be shown that the problem belongs to the APX-hard difficulty class. As a consequence, we are sure that no polynomial time algorithm can ever solve the model, unless P = NP. We propose an implicit enumeration algorithm, in which bounds on the objective function are calculated using continuous geometric properties of the median. Computational results are reported.	convex set;statistical model	Stefano Benati;Romeo Rizzi	2009	J. Global Optimization	10.1007/s10898-008-9308-8	optimization problem;robust statistics;mathematical optimization;combinatorics;discrete mathematics;convex combination;mathematics;convex set;global optimization	Vision	25.79447969535653	12.020153035226835	92807
45d91fc015714e95f8d1c39bcba6e2bff60e3c7e	session analysis of people search within a professional social network	query processing;searching;information processing	We study the problem of learning Bayesian network structures from data. Koivisto and Sood (2004) and Koivisto (2006) presented algorithms that can compute the exact posterior probability of a subnetwork, e.g., a single edge, in O(n2n) time and the posterior probabilities for all n(n − 1) potential edges in O(n2n) total time, assuming that the in-degree, i.e., the number of parents per node, is bounded by a constant. One main drawback of their algorithms is the requirement of a special structure prior that is non uniform and does not respect Markov equivalence. In this paper, we develop an algorithm that can compute the exact posterior probability of a subnetwork in O(3n) time and the posterior probabilities for all n(n− 1) potential edges in O(n3n) total time. Our algorithm also assumes a bounded in-degree but allows general structure priors. We demonstrate the applicability of the algorithm on several data sets with up to 20 variables.	algorithm;bayesian network;directed graph;markov chain;social network;subnetwork;turing completeness;web search engine	Ru He;Jiong Wang;Jin Tian;Cheng-Tao Chu;Bradley Mauney;Igor Perisic	2013	JASIST	10.1002/asi.22814	simulation;information processing;computer science;data mining;world wide web;information retrieval	ML	36.808195440340135	15.524178979318874	92861
006abf7fbeaa5d07726b20537a68fe0aa3a9dd8a	robust optimization with ambiguous stochastic constraints under mean and dispersion information		In this paper we consider ambiguous stochastic constraints under partial information consisting of means and dispersion measures of the underlying random parameters. Whereas the past literature used the variance as the dispersion measure, here we use the mean absolute deviation from the mean (MAD). This makes it possible to use the 1972 result of Ben-Tal and Hochman (BH) in which tight upper and lower bounds on the expectation of a convex function of a random variable are given. First, we use these results to treat ambiguous expected feasibility constraints. This approach requires, however, the independence of the random variables and, moreover, may lead to an exponential number of terms in the resulting robust counterparts. We then show how upper bounds can be constructed that alleviate the independence restriction and require only a linear number of terms, by exploiting models in which random variables are linearly aggregated. Moreover, using the BH bounds we derive new safe tractable approximations of chance constraints. In a numerical study, we demonstrate the efficiency of our methods in solving stochastic optimization problems under mean-MAD ambiguity.	approximation;cobham's thesis;convex function;mad;mathematical optimization;numerical analysis;program optimization;robust optimization;stochastic optimization;time complexity;x86	Krzysztof Postek;Aharon Ben-Tal;Dick den Hertog;Bertrand Melenberg	2018	Operations Research	10.1287/opre.2017.1688	stochastic optimization;mathematics;mathematical optimization;robust optimization;statistics;stochastic programming;probabilistic-based design optimization;random variable;upper and lower bounds;convex function;exponential function	AI	34.99096206545706	4.715836050707289	93286
10268186d1b522801efcd27d8db476f12b9c1958	improved availability bounds for binary and multistate monotone systems with independent component processes		Multistate monotone systems are used to describe technological or biological systems when the system itself and its components can perform at different operationally meaningful levels. This generalizes the binary monotone systems used in standard reliability theory. In this paper we consider the availabilities of the system in an interval, i.e. the probabilities that the system performs above the different levels throughout the whole interval. In complex systems it is often impossible to calculate these availabilities exactly, but if the component performance processes are independent, it is possible to construct lower bounds based on the component availabilities to the different levels over the interval. In this paper we show that by treating the component availabilities over the interval as if they were availabilities at a single time point, we obtain an improved lower bound. Unlike previously given bounds, the new bound does not require the identification of all minimal path or cut vectors.	biological system;bitwise operation;complex systems;cut, copy, and paste;reliability engineering;monotone	Jørund Gåsemyr;Bent Natvig	2017	J. Applied Probability	10.1017/jpr.2017.32	mathematics;monotone polygon;combinatorics;binary number	Theory	37.12992549865881	11.384591530028816	93566
220937239f9bf7305a4a2a924ea551e64867377d	efficient parallelization of the method of moments for queueing networks using multi-modular algebra	enhanced performance;exact solution;large model;parallel solver;chinese remainder theorem;queueing network;mom algorithm;service performance model;multi-modular algebra;large linear system;efficient parallelization;multiple service class;mom linear system	The solution of service performance models based on closed queueing networks often relies on the Mean Value Analysis (MVA) algorithm, which is unable to solve exactly large models with multiple service classes and hundreds or thousands of users. The Method of Moments (MoM) algorithm has been introduced and addressed this problem, by relying on the exact solution of large linear systems with rational coefficients. In this paper, we focus on the design, analysis and implementation of a parallel solver for MoM linear systems. Parallelization is introduced using residue number systems and recombining the results by the Chinese Remainder Theorem. A comprehensive test set representative of modern applications is used for experimental evaluation. The overall result proves the enhanced performance of both the MoM algorithm over established ones, namely Convolution and RECAL, and of the parallel solver over the serial one.	algorithm;coefficient;convolution;linear system;model–view–adapter;parallel computing;solver;test set	Michail Makaronidis;Giuliano Casale	2011			mathematical optimization;discrete mathematics;computer science;theoretical computer science	Metrics	34.23657336301391	9.305321795407009	93676
8f7d045df8c37db75823f5d2d7a4e05a2b6a5847	heuristic approaches to large-scale periodic packing of irregular shapes on a rectangular sheet	lattice packing;periodic packing;packing periodic packing lattice packing heuristics;mobilier;heuristic method;packing;metodo heuristico;geometria variable;geometrie variable;cutting stock problem;furniture;large scale;enrejado;probleme decoupe;treillis;variable geometry;mobiliario;problema troquelado;heuristics;methode heuristique;escala grande;garnissage;relleno;lattice;echelle grande	The nesting problem is a two-dimensional cutting and packing problem where the small pieces to cut have irregular shapes. A particular case of the nesting problem occurs when congruent copies of one single shape have to fill, as much as possible, a limited sheet. Traditional approaches to the nesting problem have difficulty to tackle with high number of pieces to place. Additionally, if the orientation of the given shape is not a constraint, the general nesting approaches are not particularly successful. This problem arises in practice in several industrial contexts such as footwear, metalware and furniture. A possible approach is the periodic placement of the shapes, in a lattice way. In this paper, we propose three heuristic approaches to solve this particular case of nesting problems. Experimental results are compared with published results in literature and additional results obtained from new instances are also provided. 2007 Elsevier B.V. All rights reserved.	cutting stock problem;heuristic (computer science);set packing	M. Teresa Costa;A. Miguel Gomes;José Fernando Oliveira	2009	European Journal of Operational Research	10.1016/j.ejor.2007.09.012	mathematical optimization;combinatorics;cutting stock problem;heuristics;lattice;mathematics	AI	29.239883566695497	15.659569244074586	93698
4b2f977022879f853fff6fcc5737ddbf84cac576	algorithms and hardware for efficient image smoothing	efficient image smoothing	This work develops some algorithms for efficient implementation of mean and separable median filters for image noise smoothing. The procedures suggested use one-dimensional algorithms repeatedly to obtain two-dimensional mean and separable median in ≤4 and 4[log 2 ( n +1)]-2 operations, respectively, for an  n  ×  n  square neighborhood (nbhd). A simple generalization of the algorithms to  k  dimensions shows that at most 2 k  and  k (2[log 2  ( n +1)]−1) operations are required for obtaining the mean and separable median of an  n k   nbhd. However, parallel processing can be easily applied to these algorithms. given only five parallel processors, less than one operation is required to obtain the sum of an  n  ×  n  nbhd in two dimensions, and for  n ≤31 the maximum number of comparisons required to obtain the two-dimensional separable median filter of an  n  ×  n  nbhd is reduced to less than four. Experimentally we demonstrate that the separable median filter performs better than the median filter for certain classes of images. A simple hardware implementation of the above algorithm is also outlined. It is seen that the complexity of this implementation increases linearly depending on the number of inputs, unlike sort-based implementations whose complexity increases exponentially.	algorithm;image editing;smoothing	Anup Basu;Christopher M. Brown	1987	Computer Vision, Graphics, and Image Processing	10.1016/S0734-189X(87)80111-1	mathematical optimization;combinatorics;discrete mathematics;mathematics	Vision	32.67683734450893	15.046038842607084	93924
3e65429ae418b22a9d08189c76fbbfb6a23623a5	crystal structure determination by global optimisation in configuration space: a case study for distributed computing on the nrw-grid	condor;distributed computing;global optimisation;crystal structure determination;grid	The problem of ab-initio crystal structure determination from (incomplete) powder X-ray diffraction data is formulated as a global optimisation problem. The problem is of complexity class P, which suggests that the problem itself is practically infeasible for any but trivial structures. Yet, we show in this report that the inherent symmetry of the problem along with suitable heuristics and with massive computational power, allows the determination of structures with up to 20 atoms in the asymmetric unit from scratch. We give an overview of the methods employed and show numerical examples obtained by solving a model problem on the NRW-grid, which harnesses the computational power of several hundred computers at several universities in the state of Northrhine-Westfalia.	complexity class;computation;computer;crystal structure;distributed computing;global optimization;heuristic (computer science);mathematical optimization;numerical analysis;p (complexity)	Georg Roth;Christian H. Bischof;Thomas Eifert	2011	IJCSE	10.1504/IJCSE.2011.042020	computational problem;mathematical optimization;simulation;computer science;mathematics;distributed computing;grid	Logic	31.95268091004835	9.012052835565157	93990
63e70ef490a687ae837d438e41691bdc22f56813	filtered search for submodular maximization with controllable approximation bounds		Most existing submodular maximization algorithms provide theoretical guarantees with approximation bounds. However, in many cases, users may be interested in an anytime algorithm that can offer a flexible tradeoff between computation time and optimality guarantees. In this paper, we propose a filtered search (FS) framework that allows the user to set an arbitrary approximation bound guarantee with a “tunable knob”, from 0 (arbitrarily bad) to 1 (globally optimal). FS naturally handles monotone and nonmonotone functions as well as unconstrained problems and problems with cardinality, matroid, and knapsack constraints. Further, it can also be applied to (non-negative) nonsubmodular functions and still gives controllable approximation bounds based on their submodularity ratio. Finally, FS encompasses the greedy algorithm as a special case. Our framework is based on theory in A∗ search, but is substantially more efficient because it only requires heuristics that are critically admissible (CA) rather than admissible—a condition that gives more effective pruning and is substantially easier to implement.	admissible heuristic;anytime algorithm;approximation;computation;control knob;entity–relationship model;expectation–maximization algorithm;graph traversal;greedy algorithm;heuristic (computer science);hsinchun chen;ibm notes;mathematical optimization;matroid;maxima and minima;optimization problem;parallels desktop for mac;partial order reduction;submodular set function;time complexity;unified framework;yao graph;monotone	Wenlin Chen;Yixin Chen;Kilian Q. Weinberger	2015			mathematical optimization;mathematical economics;welfare economics	ML	32.92891184102184	5.8157532322545205	94482
0ac8b0e4e211b5641713751ab73002aa4d845bf7	deterministic algorithms for 2-d convex programming and 3-d online linear programming	on line processing;convex programming;programmation convexe;algorithme deterministe;three dimensional;tratamiento en linea;objective function;deterministic algorithms;mathematical programming;linear programming;programmation lineaire;linear program;traitement en ligne;programmation mathematique;programacion convexa	We present a deterministic algorithm for solving twodimensional convex programs with a linear objective function. The algorithm requires O(k log k) primitive operations for k constraints; if a feasible point is given, the bound reduces to O(k log k/ log log 6). As a consequence, we can decide whether t convex n-gons in the plane have a common intersection in O(lc log n min{log k, log log ~8)) worst-case time, Furthermore, we can solve the three-dimensional online linear programming problem in o(log3 n) worst-case time per operation.	best, worst and average case;convex optimization;convex set;deterministic algorithm;linear programming;loss function;optimization problem	Timothy M. Chan	1997		10.1006/jagm.1997.0914	three-dimensional space;mathematical optimization;combinatorics;linear-fractional programming;linear programming;mathematics;algorithm	Theory	28.23895972259138	17.1764606670652	94792
0df86a40c4788bc9bd563c959e4eac464e09feb5	a probabilistic result for the max-cut problem on random graphs	maximum cut;random graph;eigenvalues of random matrices;probabilistic analysis;random variable	We consider the max-cut problem on a random graph G with n vertices and weights wij being independent bounded random variables with the same xed positive expectation and variance . It is well known that the max-cut number mc(G) always exceeds 2 ∑ i¡j wij . We prove that with probability greater than pn the max-cut number satis es	maximum cut;random graph	Amir Beck;Marc Teboulle	2000	Oper. Res. Lett.	10.1016/S0167-6377(00)00055-9	random variate;conductance;random regular graph;random graph;random variable;mathematical optimization;maximum cut;combinatorics;discrete mathematics;random field;probabilistic analysis of algorithms;multivariate random variable;random element;random compact set;random function;mathematics;exchangeable random variables;statistics	Theory	39.04005288541051	15.29800038006693	94815
692f3d49de86e322070759f808b4791af1011b10	sweep-line algorithm for constrained delaunay triangulation	geographic information system;algorithm;sweep line approach;region of interest;constrained delaunay triangulation	This paper introduces a new algorithm for constrained Delaunay triangulation, which is built upon sets of points and constraining edges. It has various applications in geographical information system (GIS), for example, iso-lines triangulation or the triangulation of polygons in land cadastre. The presented algorithm uses a sweep-line paradigm combined with Lawson's legalisation. An advancing front moves by following the sweep-line. It separates the triangulated and non-triangulated regions of interest. Our algorithm simultaneously triangulates points and constraining edges and thus avoids consuming location of those triangles containing constraining edges, as used by other approaches. The implementation of the algorithm is also considerably simplified by introducing two additional artificial points. Experiments show that the presented algorithm is among the fastest constrained Delaunay triangulation algorithms available at the moment.	constrained delaunay triangulation;sweep line algorithm	Vid Domiter;Borut Zalik	2008	International Journal of Geographical Information Science	10.1080/13658810701492241	triangulation;mathematical optimization;minimum-weight triangulation;combinatorics;delaunay triangulation;ruppert's algorithm;computer science;pitteway triangulation;point set triangulation;mathematics;geometry;geographic information system;constrained delaunay triangulation;chew's second algorithm;triangulation;surface triangulation;polygon triangulation;bowyer–watson algorithm;region of interest	Theory	32.75384508769953	16.975771155629833	94837
222686528326aba7f318cf29bd74328697932c09	how many bits can a flock of birds compute?		We derive a tight bound on the time it takes for a flock of birds to reach equilibrium in a standard model. Birds navigate by constantly averaging their velocities with those of their neighbors within a fixed distance. It is known that the system converges after a number of steps no greater than a tower-of-twos of height logarithmic in the number of birds. We show that this astronomical bound is actually tight in the worst case. We do so by viewing the bird flock as a distributed computing device and deriving a sharp estimate on the growth of its busy-beaver function. The proof highlights the use of spectral techniques in natural algorithms. ACM Classification: F.2.0 AMS Classification: 68W25	acm computing classification system;algorithm;best, worst and average case;busy beaver;computer;distributed computing;flock;memory bound function	Bernard Chazelle	2014	Theory of Computing	10.4086/toc.2014.v010a016		Theory	36.94873170064093	18.09050430400761	95356
98789d9060ec7ace490e9d6d90ee415bdc700574	asymptotic properties of keys and functional dependencies in random databases	qa75 electronic computers computer science szamitastechnika;discrete distribution;poisson approximation;szamitogeptudomany;bonferroni inequality;functional dependency;probabilistic model;minimal keys;decision theoretic;random database;asymptotic properties;keys	Practical database applications give the impression that sets of constraints are rather small and that large sets are unusual and are caused by bad design decisions. Theoretical investigations, however, show that minimal constraint sets are potentially very large. Their size can be estimated to be exponential in terms of the number of attributes. The gap between observation in practice and theory results in the rejection of theoretical results. However, practice is related to average cases and is not related to worst cases. The theory used until now considered the worst-case complexity. This paper aims to develop a theory for the average-case complexity. Several probabilistic models and asymptotics of corresponding probabilities are investigated for random databases formed by independent random tuples with a common discrete distribution. Poisson approximations are studied for the distributions of some characteristics for such databases where the number of tuples is sufficiently large. We intend to prove that the exponential complexity of key sets and sets of functional dependencies is rather unusual and almost all minimal keys in a relation have a length which depends mainly on the size of the relation.	approximation;average-case complexity;best, worst and average case;database model;estimation theory;fractional poisson process;functional dependency;key (cryptography);key size;random graph;rejection sampling;relational database;relational model;statistical model;table (database);time complexity;worst-case complexity	János Demetrovics;Gyula O. H. Katona;Dezsö Miklós;Oleg Seleznjev;Bernhard Thalheim	1998	Theor. Comput. Sci.	10.1016/S0304-3975(97)00089-3	probability distribution;statistical model;combinatorics;discrete mathematics;dependency theory;computer science;mathematics;functional dependency;key;algorithm;statistics	DB	37.03451571323218	12.328261550118333	95514
36ad07a9bb5477af5d8c1c416da1737de10cc5e9	the convex-hull-and-line traveling salesman problem: a solvable case	traveling salesman problem;shortest path;well solvable case;euclidean traveling salesman problem;algorithme;polynomial time algorithm;algorithm;euclidean metric;shorthest path;convex hull;linearity property;algoritmo	We solve the special case of the Euclidean Traveling Salesman Problem where n m cities lie on the boundary of the convex hull of all n cities, and the other m cities lie on a line segment inside this convex hull by an algorithm which needs O(mn) time and O(n) space.	algorithm;convex hull;decision problem;travelling salesman problem	Vladimir G. Deineko;René van Dal;Günter Rote	1994	Inf. Process. Lett.	10.1016/0020-0190(94)00071-9	2-opt;mathematical optimization;combinatorics;discrete mathematics;christofides algorithm;euclidean shortest path;convex hull;euclidean distance;mathematics;convex set;shortest path problem;travelling salesman problem;algorithm;3-opt;bottleneck traveling salesman problem	Theory	28.887084931644015	17.791714666164022	95538
9b5f58f62dfca9432ee1b26a30af15af4b3be741	bounds for sparse planar and volume arrays	search problems planar antenna arrays array signal processing direction of arrival estimation linear antenna arrays;linear array;doa estimation sparse volume planar efficiency bounds sensors redundancies antenna sensors sparse linear arrays regular planar array hole square arrays computer search direction of arrival estimation;array signal processing;linear antenna arrays;search problems;direction of arrival estimation;planar antenna arrays	This correspondence improves and extends bounds on the numbers of sensors, redundancies, and holes for sparse linear arrays to sparse planar and volume arrays. As an application, the efficiency of regular planar and volume arrays with redundancies but no holes is deduced. Also, examples of new redundancy and hole square arrays, found by exhaustive computer search, are given.	search algorithm;sensor;sparse matrix	Yann Meurisse;Jean Pierre Delmas	2001	IEEE Trans. Information Theory	10.1109/18.904563	mathematical optimization;theoretical computer science;smart antenna;mathematics	Vision	33.01292558867805	17.57837677733487	95872
2fff40cc3397e9288f0352110e4b9a7d9e067759	a combinatorial bound for beacon-based routing in orthogonal polygons		Beacon attraction is a movement system whereby a robot (modeled as a point in 2D) moves in a free space so as to always locally minimize its Euclidean distance to an activated beacon (which is also a point). This results in the robot moving directly towards the beacon when it can, and otherwise sliding along the edge of an obstacle. When a robot can reach the activated beacon by this method, we say that the beacon attracts the robot. A beacon routing from p to q is a sequence b1, b2, . . . , bk of beacons such that activating the beacons in order will attract a robot from p to b1 to b2 . . . to bk to q, where q is considered to be a beacon. A routing set of beacons is a set B of beacons such that any two points p, q in the free space have a beacon routing with the intermediate beacons b1, b2, . . . bk all chosen from B. Here we address the question of “how large must such a B be?” in orthogonal polygons, and show that the answer is “sometimes as large as ⌊ n−4 3 ⌋ , but never larger.”	euclidean distance;robot;routing	Thomas C. Shermer	2015	CoRR		simulation;telecommunications	Robotics	32.406973929911494	12.901896547513104	95885
e9878638adaec23649455cfc6f64d7e011cc87ca	torpid mixing of simulated tempering on the potts model	complexity;low temperature;potts model;mean field;phase transition;first order;iterated rounding;simulated tempering;matching;state space;linear programming relaxation;ising model;stabbing number;triangulation;spanning tree;crossing number;complete graph;markov chain	Simulated tempering and swapping are two families of sampling algorithms in which a parameter representing temperature varies during the simulation. The hope is that this will overcome bottlenecks that cause sampling algorithms to be slow at low temperatures. Madras and Zheng demonstrate that the swapping and tempering algorithms allow efficient sampling from the low-temperature mean-field Ising model, a model of magnetism, and a class of symmetric bimodal distributions [10]. Local Markov chains fail on these distributions due to the existence of bad cuts in the state space.Bad cuts also arise in the q-state Potts model, another fundamental model for magnetism that generalizes the Ising model. Glauber (local) dynamics and the Swendsen-Wang algorithm have been shown to be prohibitively slow for sampling from the Potts model at some temperatures [1, 2, 6]. It is reasonable to ask whether tempering or swapping can overcome the bottlenecks that cause these algorithms to converge slowly on the Potts model.We answer this in the negative, and give the first example demonstrating that tempering can mix slowly. We show this for the 3-state ferromagnetic Potts model on the complete graph, known as the mean-field model. The slow convergence is caused by a first-order (discontinuous) phase transition in the underlying system. Using this insight, we define a variant of the swapping algorithm that samples efficiently from a class of bimodal distributions, including the mean-field Potts model.	converge;cryptocurrency tumbler;first-order logic;first-order predicate;glauber;ising model;markov chain;paging;potts model;sampling (signal processing);simulation;state space;swendsen–wang algorithm;three-state logic	Nayantara Bhatnagar;Dana Randall	2004			phase transition;matching;ising model;markov chain;mathematical optimization;combinatorics;potts model;discrete mathematics;complexity;spanning tree;triangulation;state space;linear programming relaxation;mean field theory;first-order logic;mathematics;crossing number;complete graph	Theory	37.56478201072724	14.956636186899887	96282
916007102fd8f63017bcf1164b3bcee7de9d3d3c	an effective refinement algorithm based on swarm intelligence for graph bipartitioning	swarm intelligence;vlsi design;data mining;graph partitioning;experimental evaluation;task scheduling;parallel processing	Partitioning is a fundamental problem in diverse fields of study such as VLSI design, parallel processing, data mining and task scheduling. The min-cut bipartitioning problem is a fundamental graph partitioning problem and is NP-Complete. In this paper, we present an effective multi-level refinement algorithm based on swarm intelligence for bisecting graph. The success of our algorithm relies on exploiting both the swarm intelligence theory with a boundary refinement policy. Our experimental evaluations on 18 different benchmark graphs show that our algorithm produces high quality solutions compared with those produced by MeTiS that is a state-of-the-art partitioner in the literature.	approximation algorithm;benchmark (computing);data mining;display resolution;graph partition;iso 10303;karp's 21 np-complete problems;metis;max-flow min-cut theorem;maxima and minima;minimum cut;parallel computing;partition problem;preprocessor;refinement (computing);scheduling (computing);swarm intelligence	Lingyu Sun;Ming Leng	2007		10.1007/978-3-540-74450-4_6	mathematical optimization;computer science;theoretical computer science;machine learning	AI	26.035115253599724	6.596849893533247	96392
78763f0d49c2b702df39a7e61fcb482eec38ee0e	energy saving and collision-free motion planning for oblivious robots		In distributed computing, many tasks have been studied involving mobile entities - also called robots - with weak capabilities. A well-known scenario is that in which robots operate in Look-Compute-Move (LCM) cycles. During each cycle, a robot acquires a snapshot of the surrounding environment (Look phase), then executes an appropriate algorithm by using the obtained snapshot as input (Compute phase), and finally moves toward a desired destination, if any (Move phase). In this context, we consider robots that have to visit a partially ordered set of locations. A solution to the problem is the assignment to each robot of a trajectory to follow in order to visit the required locations. The resolution of the task is subject to two main constraints. Robots have to minimize the energy spent to accomplish an assigned trajectory, and they have to avoid collisions among each other. The minimization of the energy is expressed in terms of the number of turns a robot has to perform in between two different locations. This equals the number of bends the assigned trajectory contains in between such locations. In general, the problem is known to require Ω(n) bends per connection, with n being the number of locations, even if considering just two robots involved. We study the case where the locations that a single robot has to visit are represented as colored points in the Euclidean plane, and only two colors are provided. This means the partial order among the locations is just based on two colors per robot. In this case, we provide a constructive solution for two robots with five bends per connection.	algorithm;bend minimization;color;computer science;distributed computing;entity;graph drawing;latent class model;loop (graph theory);motion planning;robot;snapshot (computer storage)	Alfredo Navarra;Diletta Cacciagrano	2018	2018 32nd International Conference on Advanced Information Networking and Applications Workshops (WAINA)	10.1109/WAINA.2018.00150	computer science;snapshot (computer storage);collision;partially ordered set;distributed computing;motion planning;robot;trajectory;euclidean geometry;robot kinematics	Robotics	32.33425798456734	12.869115037175728	96479
0a7a1889c5cd26b9b0131de18b746e075d9d00c4	from non-negative to general operator cost partitioning	abstraction heuristics;potential heuristics;operator counting constraints;cost optimal planning;classical planning;cost partitioning	Operator cost partitioning is a well-known technique to make admissible heuristics additive by distributing the operator costs among individual heuristics. Planning tasks are usually defined with non-negative operator costs and therefore it appears natural to demand the same for the distributed costs. We argue that this requirement is not necessary and demonstrate the benefit of using general cost partitioning. We show that LP heuristics for operator-counting constraints are cost-partitioned heuristics and that the state equation heuristic computes a cost partitioning over atomic projections. We also introduce a new family of potential heuristics and show their relationship to general cost partitioning.	admissible heuristic;heuristic (computer science);utility functions on indivisible goods	Florian Pommerening;Malte Helmert;Gabriele Röger;Jendrik Seipp	2015			mathematical optimization;heuristics	AI	26.467025120552037	11.113513534123799	96517
2bb674b0fcb5c5fc7d4cef8bf828629ebbde2db7	ib4e: a software framework for parametrizing specialized lp problems	inner product;program library;funcion matematica;programacion lineal;linear programming;bibliotheque programme;software framework;programmation lineaire;linear program;mathematical function;fonction mathematique;biblioteca programa	Given a polytope P, the classical linear programming (LP) problem asks us to find a point in P which attains maximal inner product with a given real objective vector c. When the objective is a vector of unknown parameters, the LP problem amounts to computing certain information about the polytope P, such as its vertices and normal fan.	lp-type problem;software framework	Peter Huggins	2006		10.1007/11832225_24	mathematical optimization;combinatorics;dot product;computer science;linear programming;software framework;mathematics;function;algorithm	ML	25.97836235631854	14.366864395370692	96607
a81aa793f34570916eb43a1ff0cd1af07d4551c1	an adaptive, multivariate partitioning algorithm for global optimization of nonconvex programs		In this work, we develop an adaptive, multivariate partitioning algorithm for solving mixed-integer nonlinear programs (MINLP) with multilinear terms to global optimality. The algorithm combines two key ideas that exploit the structure of convex relaxations to MINLPs. First, we apply sequential bound tightening techniques to obtain the tightest possible bounds, based on both continuous and discrete relaxations with partitioned variable domains. Second, we leverage relaxations to adaptively partition variable domains via piecewise convex relaxations of multi-linear terms and develop an iterative algorithm for globally solving MINLPs and provide proofs on convergence guarantees. We demonstrate the effectiveness of our disjunctive formulations and the algorithm on well-known benchmark problems (including Pooling and Blending instances) from MINLPLib and compare with a state-ofthe-art global optimization solver. With this novel approach, we solve several large-scale instances which are, in some cases, intractable by the global optimization solver. We also shrink the best known optimality gap for one of the hard, generalized pooling problem instance.	alpha compositing;approximation;apriori algorithm;benchmark (computing);branch and bound;character encoding;computation;convex function;convex hull;disjunctive normal form;experiment;global optimization;iteration;iterative method;local consistency;mathematical optimization;nonlinear programming;nonlinear system;numerical analysis;polyhedron;relevance;search tree;software propagation;solver;space partitioning;whole earth 'lectronic link	Harsha Nagarajan;Mowen Lu;Site Wang;Russell Bent;Kaarthik Sundar	2017	CoRR		global optimization;iterative method;mathematical optimization;piecewise;mathematics;regular polygon;nonlinear system;polynomial;multivariate statistics;pooling;algorithm	AI	32.90056186275379	4.649606115089403	97187
f5c5b7908aa28004688ad949e2c6d4cf641bd11c	graph entropy rate minimization and the compressibility of undirected binary graphs	graph theory;complex networks;lattices;network analysis;real world networks graph entropy rate minimization undirected binary graphs compressibility complex network analysis graph entropy random walks degree distribution node centrality entropy rate scale free networks lattice networks star networks random networks;symmetric matrices;computational modeling;entropy codes;network analysis complex networks entropy codes graph theory markov processes;bandwidth;entropy;markov processes;entropy lattices bandwidth encoding markov processes computational modeling symmetric matrices;encoding	With the increasing popularity of complex network analysis through the use of graphs, a method for computing graph entropy has become important for a better understanding of a network's structure and for compressing large complex networks. There have been many different definitions of graph entropy in the literature which incorporate random walks, degree distribution, and node centrality. However, these definitions are either computationally complex or seemingly ad hoc. In this paper we propose a new approach for computing graph entropy with the intention of quantifying the compressibility of a graph. We demonstrate the effectiveness of our measure by identifying the lower bound of the entropy rate for scale-free, lattice, star, random, and real-world networks.	centrality;complex network;degree distribution;entropy rate;graph (discrete mathematics);hoc (programming language);network theory	Marcos E. Bolanos;Selin Aviyente;Hayder Radha	2012	2012 IEEE Statistical Signal Processing Workshop (SSP)	10.1109/SSP.2012.6319634	network science;random graph;mathematical optimization;combinatorics;discrete mathematics;power graph analysis;binary entropy function;transfer entropy;null graph;simplex graph;clustering coefficient;mathematics;voltage graph;graph;joint quantum entropy;exponential random graph models;random geometric graph;complex network;conditional entropy	ML	35.69265816042674	12.24983758343289	97400
2a29db371e7ed139a66aa3b3537ccddd097db398	embedding pyramids into 3d meshes	embedding;algoritmo paralelo;vision ordenador;architecture systeme;parallel algorithm;multiprocessor;reseau interconnecte;algorithme parallele;computer vision;pyramidal architecture;plongement;arquitectura sistema;vision ordinateur;network architecture;systeme parallele;inmersion;parallel system;multiprocesador;system architecture;red interconectada;interconnected power system;sistema paralelo;multiprocesseur	cessing and image understanding [11, 13]. The most efficient applications of the pyramid are in the areas of scalespace (or multiresolution) and coarse-to-fine operations. Multiscale or multiresolution image representation is a very powerful tool for analyzing numerous image features at multiple scales [6, 11]. Moreover, pyramid machines are not limited just to image processing tasks. By exploiting the hierarchy inherent in the tree structure of a pyramid, and the parallelism inherent at each level, pyramids can handle various problems in graph theory [9], digital geometry [9], and recursive parallel tasks [5]. The 2D mesh architecture, on the other hand, has had wide availability in the research and commercial community. A natural extension of the 2D mesh is the 3D mesh, which has recently gained marked popularity due to a number of inherent architectural features. These include simple VLSI layout, good scalability, higher bandwidth, and smaller diameter compared to 2D mesh (when they have same link width). In addition, since it has three dimensions, it is capable of modeling many physical world problems more naturally, such as 3D image processing and finite element methods. The advantages and rich topological characteristics of 3D mesh have led to the development of the massively parallel MIT J-Machine [3] and, more recently, the CRAY T3D. With the increasing popularity of the 3D mesh and its potential availability coupled with the suitability of many image processing and computer vision applications on a pyramid, we consider the problem of embedding pyramids into 3D meshes. The rest of the paper is organized as follows. Section 2 presents basic terminology and notations and discusses the measures used for our embedding of the pyramid into 3D mesh. Section 3 presents a simple embedding scheme denoted as natural embedding. Section 4 gives an improved embedding scheme denoted as multiple embedding. Section 5 provides some conclusions.	2.5d;3d computer graphics;computer vision;cray t3d;digital geometry;dilation (morphology);experiment;finite element method;graph theory;image processing;j–machine;map;network congestion;parallel computing;polygon mesh;pyramid (geometry);recursion;scalability;tree structure;very-large-scale integration	Cindy K. Y. Ng;Lawrence K. L. Pun;Dixon Man-Ching Ip;Mounir Hamdi;Ishfaq Ahmad	1996	J. Parallel Distrib. Comput.	10.1006/jpdc.1996.0097	parallel computing;multiprocessing;network architecture;computer science;embedding;mathematics;geometry;parallel algorithm;systems architecture;computer graphics (images)	Vision	33.51636542553666	11.246044044083188	97604
bd5076069669e1f59d6b87691a917c70c20675d3	a negative dual rectangle cancellation algorithm for the linear assignment problem	assignment problem;partial zero cover;integer programming;hungarian method;negative dual rectangle	In this paper, we consider three alternative primal models and their corresponding alternative dual models for the linear assignment problem. We then define the concept of Negative Dual Rectangle (NDR) and suggest an algorithm that solves two of these dual problems by repeatedly finding and cancelling NDRs until it yields an optimal solution to the assignment problem. The algorithm is simple, flexible, efficient, and unified. We also introduce the notion of partial zero cover as an interpretation of an NDR. We then introduce some heuristic methods for finding NDRs. We also state and prove a lemma to establish the optimal use of an NDR. Furthermore, we show that on a new class of benchmark instances that is introduced in this paper the running time of our algorithm is highly superior to a well-known pure shortest path algorithm.	algorithm;assignment problem;benchmark (computing);bounce message;heuristic;time complexity	Mohammad S. Sabbagh;Sayyed Rasoul Mousavi;Yasin Zamani	2013	Computers & Industrial Engineering	10.1016/j.cie.2013.05.013	mathematical optimization;combinatorics;integer programming;computer science;hungarian algorithm;mathematics;assignment problem;algorithm	EDA	24.613096221818314	11.788893453616955	97620
225266229f392a227ceed3141b04623ccb44c7a6	a generalized upper bounding algorithm for multicommodity network flow problems	iterations;simplex method;upper bound;numerical analysis;matrices mathematics;linear programming;algorithms;optimization;network flow	Abstract : An algorithm for solving min cost or max flow multicommodity flow problems is described. It is a specialization of the simplex method, which takes advantage of the special structure of the multicommodity problem. The only non- graph or non-additive operations in a cycle involve the inverse of a working basis, whose dimension is the number of currently saturated arcs. Efficient relations for updating this inverse are derived.	algorithm;flow network	James K. Hartman;Leon S. Lasdon	1971	Networks	10.1002/net.3230010404	mathematical optimization;combinatorics;discrete mathematics;flow network;iteration;minimum-cost flow problem;numerical analysis;linear programming;mathematics;upper and lower bounds;simplex algorithm;algorithm	Theory	25.38177854004876	13.334079195751576	97784
2edb1048ebb27432792eec261ea6b2d9728d383a	fast algorithms for approximate semidefinite programming using the multiplicative weights update method	eigenvalues and eigenfunctions;approximate algorithm;approximation algorithms;random sampling;approximation algorithms algorithm design and analysis frequency estimation computer science polynomials lagrangian functions eigenvalues and eigenfunctions sampling methods ellipsoids np hard problem;frequency estimation;ellipsoids;eigenvalues;polynomials;np hard problem;eigenvalues and eigenfunctions computational complexity;computational complexity;fast algorithm;random sampling fast algorithms approximate semidefinite programming multiplicative weights update approximation algorithms interior point method lagrangian relaxation based technique eigenvalue computations;computer science;sampling methods;algorithm design and analysis;lagrangian functions;lagrangian relaxation;semidefinite program	Semidefinite programming (SDP) relaxations appear in many recent approximation algorithms but the only general technique for solving such SDP relaxations is via interior point methods. We use a Lagrangian-relaxation based technique (modified from the papers of Plotkin, Shmoys, and Tardos (PST), and Klein and Lu) to derive faster algorithms for approximately solving several families of SDP relaxations. The algorithms are based upon some improvements to the PST ideas - which lead to new results even for their framework - as well as improvements in approximate eigenvalue computations by using random sampling.	approximation algorithm;best, worst and average case;cholesky decomposition;computation;convex optimization;cut (graph theory);interior point method;iteration;lu decomposition;lagrangian relaxation;linear programming relaxation;mathematical optimization;microwave;monte carlo method;planar separator theorem;plotkin bound;polynomial;program structure tree;rounding;sampling (signal processing);semidefinite programming	Sanjeev Arora;Elad Hazan;Satyen Kale	2005	46th Annual IEEE Symposium on Foundations of Computer Science (FOCS'05)	10.1109/SFCS.2005.35	sampling;mathematical optimization;combinatorics;discrete mathematics;computer science;mathematics;approximation algorithm;algorithm	Theory	25.369420136729662	15.432970080192504	97821
dc4aa13ade5aeddbce0ae7202e7b093a1cee8a30	an efficient alternative to ollivier-ricci curvature based on the jaccard metric		We study Ollivier-Ricci curvature, a discrete version of Ricci curvature, which has gained popularity over the past several years and has found applications in diverse fields. However, the Ollivier-Ricci curvature requires an optimal mass transport problem to be solved, which can be computationally expensive for large networks. In view of this, we propose two alternative measures of curvature to Ollivier-Ricci which are motivated by the Jaccard coefficient and are demonstrably less computationally intensive, a cheaper Jaccard (JC) and a more expensive generalized Jaccard (gJC) curvature metric. We show theoretically that the gJC closely matches the Ollivier-Ricci curvature for Erdös-Rényi graphs in the asymptotic regime of large networks. Furthermore, we study the goodness of approximation between the proposed curvature metrics and Ollivier-Ricci curvature for several network models and real networks. Our results suggest that in comparison to an alternative curvature metric for graphs, the Forman-Ricci curvature, the gJC exhibits a reasonably good fit to the Ollivier-Ricci curvature for a wide range of networks, while the JC is shown to be a good proxy only for certain scenarios.	analysis of algorithms;approximation;coefficient;jaccard index;proxy server;transportation theory (mathematics)	Siddharth Pal;Feng Yu;Terrence J. Moore;Ram Ramanathan;Amotz Bar-Noy;Ananthram Swami	2017	CoRR		jaccard index;mathematical optimization;network model;curvature;ricci curvature;mathematics;graph	ML	32.31176138234429	4.938303042561393	97912
5f3ce93afa8aed522110eacef96cde5cabc5409a	intolerance does not necessarily lead to segregation: a computer-aided analysis of the schelling segregation model		We assume each individual to have a neighbourhood range r ∈ N \ 0, which is made by those individuals that live at most r positions to the right or r positions to the left. Individuals have preferences over their neighbours. In particular they have a preference ratio p ∈ [0, 1], i.e., the least proportion of individuals in their neighbourhood that need to share their characteristics, in our case their same type. In Schelling’s model, unhappy individuals are allowed to move to the closest neighbourhood that would make them happy. Specifically, individuals are moved according to the following protocol, which we refer to as the Schelling turn function: (1) Record the set of currently unhappy individuals; (2) Select the leftmost individual in the set, who is still unhappy in the current configuration and hasn’t been selected yet; (3) Move him or her to the closest neighbourhood that would make the individual happy, jumping over all individuals in between.1 If he or she cannot be made happy, then he or she does not move;	neighbourhood (graph theory)	Alex Carver;Paolo Turrini	2018			computer-aided;machine learning;artificial intelligence;computer science;convergence (routing);pecking order	Theory	36.864312043271376	16.72862344227808	98132
d678193492913a4d4db67beff53910cee94b6d4d	shannon information and self-similarity in whole genomes	89 70 c;complete genome;87 10 e;self similarity;genome size;shannon information;universality class;word length;complete genomes;random sequence;87 14 gg;growth model;02 50 r;02 50 r complete genomes;87 23 kg	The Shannon information (SI) in distributions of occurrence frequency of short words in whole genomes is shown to exhibit universality. For given word length, the SI in genomes of all lengths is the same as that in random sequences of a universal lengthsLr . For the shorter words Lr is far shorter than the genome. For example, Lr ∼ 1000 bases for three-letter words. We further show that whole genomes are highly self-similar in the sense that any segment of the genome down to a length of Λsim, about twiceLr , also shares the universal property. We devise a simple genome growth model in which genome-size sequences grown by maximally stochastic segmental duplication and random mutation possess the universal and self-similar properties of genomes.  2005 Elsevier B.V. All rights reserved. PACS: 87.10.+e; 89.70.+c; 87.14.Gg; 87.23.Kg; 02.50.-r	information;lr parser;picture archiving and communication system;population dynamics;self-similarity;shannon (unit);universality probability	Ta-Yuan Chen;Li-Ching Hsieh;Hoong-Chien Lee	2005	Computer Physics Communications	10.1016/j.cpc.2005.03.050	combinatorics;self-similarity;bioinformatics;renormalization group;random sequence;genome size;mathematics;statistics;entropy	Theory	37.97256699901712	9.989886113823497	98165
4149f4237d3a59895717e2bc7c9db200f0de0f22	conformational ensembles and sampled energy landscapes: analysis and comparison	optimal transport;sampling;molecular conformations;optimization;morse theory;energy landscapes	We present novel algorithms and software addressing four core problems in computational structural biology, namely analyzing a conformational ensemble, comparing two conformational ensembles, analyzing a sampled energy landscape, and comparing two sampled energy landscapes. Using recent developments in computational topology, graph theory, and combinatorial optimization, we make two notable contributions. First, we present a generic algorithm analyzing height fields. We then use this algorithm to perform density-based clustering of conformations, and to analyze a sampled energy landscape in terms of basins and transitions between them. In both cases, topological persistence is used to manage (geometric) frustration. Second, we introduce two algorithms to compare transition graphs. The first is the classical earth mover distance metric which depends only on local minimum energy configurations along with their statistical weights, while the second incorporates topological constraints inherent to conformational transitions. Illustrations are provided on a simplified protein model (BLN69), whose frustrated potential energy landscape has been thoroughly studied. The software implementing our tools is also made available, and should prove valuable wherever conformational ensembles and energy landscapes are used.		Frédéric Cazals;Tom Dreyfus;Dorian Mazauric;Christine-Andrea Roth;Charles H. Robert	2015	Journal of computational chemistry	10.1002/jcc.23913	sampling;mathematical optimization;combinatorics;computational chemistry;mathematics;morse theory	Comp.	36.30439420438215	13.455412340993323	98435
670c13e56dd6634a05b49f12f142711008b8953e	a study of chaos in cellular automata		This paper presents a study of chaos in one-dimensional cellular automata (CAs). The communication of information from one part of the system to another has been taken into consideration in this study. This communication is formalized as a binary relation over the set of cells. It is shown that this relation is an equivalence relation and all the cells form a single equivalence class when the cellular automaton (CA) is chaotic. However, the communication between two cells is sometimes blocked in some CAs by a subconfiguration which appears in between the cells during evolution. This blocking of communication by a subconfiguration has been analyzed in this paper with the help of de Bruijn graph. We identify two types of blocking — full and partial. Finally a parameter has been developed for the CAs. We show that the proposed parameter performs better than the existing parameters.	cellular automaton	Supreeti Kamilya;Sukanta Das	2018	I. J. Bifurcation and Chaos	10.1142/S0218127418300082	mathematics;discrete mathematics;mathematical analysis;cellular automaton;binary relation;parametrization;equivalence relation;de bruijn graph;equivalence class	Theory	39.12867210324955	9.015979536748853	98483
fb31c09dcc5cdde446c1c1a0a9aa911f9c1dafe1	generating covering arrays with pseudo-boolean constraint solving and balancing heuristic		Covering arrays (CAs) are interesting objects in combinatorics and they also play an important role in software testing. It is a challenging task to generate small CAs automatically and efficiently. In this paper, we propose a new approach which generates a CA column by column. A kind of balancing heuristic is adopted to guide the searching procedure. At each step (column extension), some pseudo Boolean constraints are generated and solved by a PBO solver. A prototype tool is implemented, which turns out to be able to find smaller CAs than other tools, for some cases.	constraint satisfaction problem;heuristic	Feifei Ma;Jian Zhang	2016		10.1007/978-3-319-42911-3_22	machine learning;software;artificial intelligence;computer science;theoretical computer science;heuristic;mathematical optimization;solver	AI	25.832439158567325	4.5888149561735165	98538
9d9821a0460c7e1e75ad3343b88d900e18119db2	finding a bounded mixed-integer solution to a system of dual network inequalities	dual network inequalities;algebre max;max algebra;eigenvector;flujo red;programacion mixta entera;programmation partiellement en nombres entiers;mixed integer programming;network flow;flot reseau;eigenvectors;algebra max	"""We show that using max-algebraic techniques it is possible to generate the set of all solutions to a system of inequalities x""""i-x""""j>=b""""i""""j, i,j=1,...,n using n generators. This efficient description enables us to develop a pseudopolynomial algorithm which either finds a bounded mixed-integer solution, or decides that no such solution exists."""		Peter Butkovic	2008	Oper. Res. Lett.	10.1016/j.orl.2008.04.004	mathematical optimization;combinatorics;discrete mathematics;integer programming;eigenvalues and eigenvectors;mathematics;algorithm	Theory	24.91664396138104	13.192191819117703	98672
04492607b53267a272aabdfd6af2408e946b0a74	nonlinear integer programming for optimal allocation in stratified sampling	nonlinear integer programming;dynamic programming;sample size;programacion dinamica;non linear programming;programacion entera;cost function;nonlinear programming;programacion no lineal;echantillonnage;branch and bound algorithm;programmation non lineaire;dynamic program;lagrange multiplier;linear constraint;funcion coste;programmation en nombres entiers;sampling;objective function;branch and bound method;integer programming;metodo branch and bound;stratified random sampling;echantillon stratifie;programmation dynamique;fonction cout;methode separation et evaluation;muestreo;branch and bound;simple random sampling;stratified sample;stratified sampling;muestra estratificada	A stratified random sampling plan is one in which the elements of the population are first divided into nonoverlapping groups, and then a simple random sample is selected from each group. In this paper, we focus on determining the optimal sample size of each group. We show that various versions of this problem can be transformed into a particular nonlinear program with a convex objective function, a single linear constraint, and bounded variables. Two branch and bound algorithms are presented for solving the problem. The first algorithm solves the transformed subproblems in the branch and bound tree using a variable pegging procedure. The second algorithm solves the subproblems by performing a search to identify the optimal Lagrange multiplier of the single constraint. We also present linearization and dynamic programming methods that can be used for solving the stratified sampling problem. Computational testing indicates that the pegging branch and bound algorithm is fastest for some classes of problems, and the linearization method is fastest for other classes of problems.	integer programming;mathematical optimization;sampling (signal processing);stratified sampling	Kurt M. Bretthauer;Anthony D. Ross;Bala Shetty	1999	European Journal of Operational Research	10.1016/S0377-2217(98)00180-5	mathematical optimization;combinatorics;nonlinear programming;branch and price;mathematics;stratified sampling;algorithm;branch and cut	Theory	24.80833512356089	10.119233486389518	98765
54b2502cbbb810f77901fca169766db0e6ce0f47	on the average altitude of heap-ordered trees	heap ordered trees;probability;data structures;algorithms;average altitude;generating functions	We consider heap-ordered trees in this paper. The main theorems in this paper show that the average and the variance of the altitude (or level) of the nodes in a random n-node heap-ordered tree are and , respectively where γ=0.5772156649… is the Euler’s constant.		Wen-Chin Chen;Wen-Chun Ni	1994	Int. J. Found. Comput. Sci.	10.1142/S0129054194000062	generating function;combinatorics;discrete mathematics;data structure;computer science;probability;mathematics;algorithm	ECom	38.826557076293774	17.033032507719938	98833
21b57d1d4c4d606fe5896b74e04dbc0a40b0cbc4	component evolution in general random intersection graphs	graph theory;random graph;giant component;stochastic process;probabilistic method;random generation;diagrams;randomness;network analysis;wireless sensor network;branching process;social network;intersection graphs;existence and uniqueness	Random intersection graphs (RIGs) are an important random structure with algorithmic applications in social networks, epidemic networks, blog readership, and wireless sensor networks. RIGs can be interpreted as a model for large randomly formed non-metric data sets. We analyze the component evolution in general RIGs, giving conditions on the existence and uniqueness of the giant component. Our techniques generalize existing methods for analysis of component evolution: we analyze survival and extinction properties of a dependent, inhomogeneous Galton-Watson branching process on general RIGs. Our analysis relies on bounding the branching processes and inherits the fundamental concepts of the study of component evolution in Erdős-Rényi graphs. The major challenge comes from the underlying structure of RIGs, which involves both a set of nodes and a set of attributes, with different probabilities associated with each attribute.	algorithm;blog;erdős number;erdős–rényi model;evolution;giant component;randomness;social network	Milan Bradonjic;Aric A. Hagberg;Nicolas W. Hengartner;Allon G. Percus	2010		10.1007/978-3-642-18009-5_5	random graph;stochastic process;branching process;combinatorics;discrete mathematics;wireless sensor network;network analysis;graph theory;diagram;probabilistic method;mathematics;randomness;giant component;algorithm;statistics;social network	ML	37.04817630075906	14.68309081969123	99087
356b388f0f3fede6e5be625bd331fc9a9687e6c5	maximal nonlinearity in balanced boolean functions with even number of inputs, revisited	evolutionary computation;boolean functions;ciphers;genetic algorithms;search problems;probabilistic logic	The problem of obtaining maximal nonlinearity in Boolean functions is well researched, both from the cryptographic and the evolutionary computation side. However, the results are still not conclusive enough to be able to show how good a heuristic approach is when tackling this problem. In this paper, we investigate how to obtain the maximal possible nonlinearity in balanced Boolean functions, but we also analyze how difficult is the problem itself. In order to do so, we conduct experiments with Estimation of distribution algorithms as well as the fitness landscape analysis and the deception analysis. Our results indicate that the first difficulties arise from the inappropriate fitness function and representation of solutions coupled with a huge search space. The fitness landscape analysis does not reveal any significant differences that could justify the assumed jump in problem difficulty when going from Boolean functions with 6 inputs to those with 8 inputs. Finally, we show that this problem is not order-1 deceptive.	balanced boolean function;bit array;cryptography;estimation of distribution algorithm;evolutionary computation;experiment;fitness function;hadamard transform;heuristic;high-level programming language;maximal set;nonlinear system;search algorithm	Stjepan Picek;Roberto Santana;Domagoj Jakobovic	2016	2016 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2016.7744197	mathematical optimization;combinatorics;circuit minimization for boolean functions;discrete mathematics;boolean network;genetic algorithm;boolean expression;computer science;artificial intelligence;maximum satisfiability problem;machine learning;mathematics;probabilistic logic;boolean function;statistics;evolutionary computation	Theory	27.76976902926815	4.5635913117379	99193
19204d112a296789bf0816ecde991c21dc3e2d33	estimating the selectivity of spatial queries using the 'correlation' fractal dimension		We examine the estimation of selectivities for range and spatial join queries in real spatial databases. As we have shown earlier [FK94], real point sets: (a) violate consistently the “uniformity” and “independence” assumptions, (b) can often be described as “fractals”, with non-integer (fractal) dimension. In this paper we show that, among the infinite family of fractal dimensions, the so called “Correlation Dimension” Dz is the one that we need to predict the selectivity of spatial join. The main contribution is that, for all the real and synthetic point-sets we tried, the average number of neighbors for a given point of the point-set follows a power law, with LI& as the exponent. This immediately solves the selectivity estimation for spatial joins, as well as for “biased” range queries (i.e., queries whose centers prefer areas of high point density). We present the formulas to estimate the selectivity for the biased queries, in&ding an integration constant (KLshape,) for each query shape. Finally, we show results on real and synthetic point sets, where our formulas achieve very low relative errors (typically about lo%, versus 40%-100% of the uniformity assumption).	algorithm;circuit complexity;correlation dimension;database;emoticon;fractal dimension;range query (data structures);selectivity (electronic);spatial query;synthetic data;synthetic intelligence	Alberto Belussi;Christos Faloutsos	1995				DB	31.08286146245772	14.483107063804617	99359
8af8af63df536d2c85a6490ffd73f65430879df9	a generalized method of moments for closed queueing networks	method of moments;computational algorithms;queueing network;queue length;exact solution;journal article;closed queueing network;higher order;mean value analysis;queueing network model;exact algorithm;method of moment;queueing network models;evaluation model;generalized method of moment	We introduce a new solution technique for closed product-fo rm queueing networks that generalizes the Method of Moments (MoM), a recently propose d exact algorithm that is several orders of magnitude faster and memory efficient than t e established Mean Value Analysis (MVA) algorithm. Compared to MVA, MoM recursively computes higher-order moments of queue lengths instead of mean values, an approach that remarkably reduces the computational costs of exact solutions, especially on mode ls with large number of jobs. In this paper, we show that the MoM recursion can be generaliz d to include multiple recursive branches that evaluate models with different numbe r of queues, a solution approach inspired by the Convolution algorithm. Combining the appro aches of MoM and Convolution simplifies the evaluation of normalizing constants a nd leads to large computational savings with respect to the recursive structure originally proposed for MoM.	computation;convolution;exact algorithm;google compute engine;job stream;linear system;model–view–adapter;population;queueing theory;recursion;recursion (computer science)	Giuliano Casale	2011	Perform. Eval.	10.1016/j.peva.2010.08.026	mean value analysis;mathematical optimization;combinatorics;discrete mathematics;higher-order logic;method of moments;gordon–newell theorem;bulk queue;computer science;layered queueing network;mathematics;statistics	ML	34.228207870045786	9.260191494098299	99963
8a1783670b874b53a64e95a8e84648726395cced	using weighted max-sat engines to solve mpe	learning;latent class models;local dependence;local search;model based clustering;probabilistic reasoning;bayesian networks	Logical and probabilistic reasoning are Closely related. Many examples in each group have natural analogs in the other. One example is the strong relationship between weighted MAXSAT and MPE. This paper presents a simple reduction of MPE to weighted MAX-SAT. It also investigates approximating MPE by converting it to a weighted MAX-SAT problem, then using the incomplete methods for solving weighted MAX-SAT to generate a solution. We show that converting MPE problems to MAX-SAT problems and using a method designed for MAX-SAT to solve them often produces solutions that are vastly superior to the previous local search methods designed directly for the MPE problem.	approximation algorithm;generalized least squares;hp multi-programming executive;local search (optimization);max;maximum satisfiability problem	James D. Park	2002			mathematical optimization;computer science;artificial intelligence;local search;machine learning;bayesian network;probabilistic logic	AI	27.147764691328486	5.90446159168693	100365
2adee8407aa917844041930f1a4e77537a7dab6d	"""correction to """"recognition of noisy subsequences using constrained edit distances"""""""	constrained edit distances;edit distance			B. John Oommen	1988	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/TPAMI.1988.10005		Vision	33.51850605805388	16.583788462592192	100904
9ff295b624067c025f626d5fdd6e36729f72c45e	"""a geometric characterization of """"optimality-equivalent"""" relaxations"""	optimal solution;objective function;optimization problem;convex geometry;mathematical programming;sensitivity analysis;linear program;convex relaxation	"""An optimization problem is defined by an objective function to be maximized with respect to a set of constraints. To overcome some theoretical and practical difficulties, the constraint-set is sometimes relaxed and """"easier"""" problems are solved. This led us to study relaxations providing exactly the same set of optimal solutions. We give a complete characterization of these relaxations and present several examples. While the relaxations introduced in this paper are not always easy to solve, they may help to prove that some mathematical programs are equivalent in terms of optimal solutions. An example is given where some of the constraints of a linear program can be relaxed within a certain limit."""		Walid Ben-Ameur;José Neto	2008	J. Global Optimization	10.1007/s10898-007-9275-5	convex geometry;optimization problem;mathematical optimization;combinatorics;discrete mathematics;nonlinear programming;linear programming;mathematics;sensitivity analysis	Theory	25.42961258738551	11.85719118237294	100939
8215ae8c92d7f939fbaecc9362136d27f7723a81	strategic multi-layer network formation	arbitrary utility functions strategic multilayer network formation network nodes edge set distance minimization distance based utility functions generalization global utility function pairwise stable networks incident edges pairwise stability network formation path;vegetation nickel resource management games optimization cities and towns complex networks;network theory graphs	We study the problem of strategic network formation among a set of nodes where each node forms links with other nodes in the network to maximize some utility. While previous work in this area has considered the formation of a single edge set between the nodes, we consider the problem of the strategic formation of multiple edge sets between the nodes, corresponding to different types of relationships. We start by considering the case where one edge set is chosen to minimize distances between nodes that are neighbors in another edge set. This corresponds to a generalization of distance-based utility functions studied in the literature. In this setting, we characterize efficient networks (that are optimal with respect to a global utility function), and pairwise stable networks (where individual nodes cannot benefit from the addition or removal of incident edges).We then generalize existing concepts of pairwise stability and improving paths for network formation to the multi-layer setting with arbitrary utility functions.	layer (electronics);multiple edges;strategic network formation;utility	Ebrahim Moradi Shahrivar;Shreyas Sundaram	2013	52nd IEEE Conference on Decision and Control	10.1109/CDC.2013.6759944	mathematical optimization;combinatorics;network formation;machine learning;mathematics	ECom	35.95021732026795	12.77570244721271	101193
310d9cec4721faa6208cedf3999c7d32b85b7d7a	a framework for pursuit evasion games in rn	procesamiento informacion;algorithm analysis;geometrie algorithmique;computational geometry;juego persecucion;probleme homme lion;juego persecucion evasion;algorithme;algorithm;capture;informatique theorique;information processing;initial condition;lion man problem;jeu poursuite evasion;captura;geometria computacional;analyse algorithme;hiperplano;traitement information;hyperplane;pursuit game;jeu poursuite;analisis algoritmo;hyperplan;pursuit evasion game;computer theory;algoritmo;informatica teorica	We present a framework for solving pursuit evasion games in Rn for the case of N pursuers and a single evader. We give two algorithms that capture the evader in a number of steps linear in the original pursuer-evader distances. We also show how to generalize our results to a convex playing field with finitely many hyperplane boundaries that serve as obstacles.	algorithm;average-case complexity;evasion (network security);pursuit-evasion	Swastik Kopparty;Chinya V. Ravishankar	2005	Inf. Process. Lett.	10.1016/j.ipl.2005.04.012	information processing;computational geometry;artificial intelligence;hyperplane;mathematics;initial value problem;algorithm	AI	30.003776385881054	15.932732547125072	101374
5d5dcba870d06c9c98e457d0b43f719c2ea35f65	general drift analysis with tail bounds		Drift analysis is one of the state-of-the-art techniques for the runtime analysis of randomized search heuristics. In recent years, many different drift theorems, including additive, multiplicative and variable drift, have been developed, applied and partly generalized or adapted to particular processes. A comprehensive overview article was missing. We provide not only such an overview but also present a universal drift theorem that generalizes virtually all existing drift theorems found in the literature. On the one hand, the new theorem bounds the expected first hitting time of optimal states in the underlying stochastic process. On the other hand, it also allows for general upper and lower tail bounds on the hitting time, which were not known before except for the special case of upper bounds in multiplicative drift scenarios. As a proof of concept, the new tail bounds are applied to prove very precise sharp-concentration results on the running time of the (1+1) EA on OneMax, general linear functions and LeadingOnes. Moreover, user-friendly specializations of the general drift theorem are given. 1 ar X iv :1 30 7. 25 59 v1 [ cs .N E ] 9 J ul 2 01 3	analysis of algorithms;average-case complexity;expect;heuristic (computer science);like button;linear function;mathematical optimization;nyquist–shannon sampling theorem;randomized algorithm;requirements analysis;rice's theorem;stochastic process;tail call;time complexity;usability;utility functions on indivisible goods	Per Kristian Lehre;Carsten Witt	2013	CoRR		mathematical optimization	Theory	34.46501470338446	8.35178184279411	101759
a363e70ed1cec6df666d019d5251bdbe7fcca33f	probabilistic analysis of two euclidean location problems			probabilistic analysis of algorithms	Alberto Marchetti-Spaccamela;Maurizio Mastropasqua Talamo	1983	ITA		computer science;euclidean shortest path;mathematics;1-center problem	Theory	28.48002973400248	17.92017708808033	102057
611e6397b2dd44cc2cc803ee69edfa39e4006e70	sufficient conditions for the ergodicity of fuzzy markov chains	max product composition;max min composition;fuzzy markov chain;powers of a fuzzy matrix;ergodicity;article	Analogous to the traditional Markov chains which have been studied extensively and have many successful applications, fuzzy Markov chains have been proposed for decision-making in an environment of uncertainty and imprecision for decades. It is known that results of fuzzy Markov chains depend on the transition matrix as well as the algebraic composition involved. In their study of max–min fuzzy Markov chains, Avrachenkov and Sanchez raised an open question for finding conditions to ensure the ergodicity of max–min fuzzy Markov chains. In this paper, we provide sufficient conditions for the ergodicity of both max–min and max-product fuzzy Markov chains. It is not surprising that such sufficient conditions are very different because of the max–min and max-product compositions.	ergodicity;markov chain	Dong-Mei Zhu;Wai-Ki Ching;Sy-Ming Guu	2016	Fuzzy Sets and Systems	10.1016/j.fss.2016.01.005	matrix analytic method;markov chain;mathematical optimization;markov kernel;combinatorics;discrete mathematics;ergodicity;examples of markov chains;balance equation;fuzzy number;mathematics;markov renewal process;markov process;markov chain mixing time;markov model;variable-order markov model	Logic	37.267048842848034	6.364845364171638	102710
16949d0a2a04097e9b206df7f846f5dca88bc1d4	reconstructing hv-convex polyominoes from orthogonal projections	proyeccion ortogonal;tomograpjie discrete;discrete tomography;convexite;combinatorial problems;maillage;convexidad;algorithme;algorithm;combinatorial problem;probleme combinatoire;problema combinatorio;celdarada;polynomial algorithm;linear time;tomographie;orthogonal projection;grid pattern;convexity;tomografia;data structure;tomography;projection orthogonale;polyominoes;algoritmo	Tomography is the area of reconstructing objects from projections. In discrete tomography an object T we wish to reconstruct may be a set of cells of a multidimensional grid. We perform measurements of T , each one involving a projection that determines the number of cells in T on all lines parallel to the projection’s direction. Given a finite number of such measurements, we wish to reconstruct T or, if unique reconstruction is not possible, to compute any object consistent with these projections. Gardner et al. [7] proved that deciding if there is an object consistent with given measurements is NP-complete, even for three non-parallel projections in the 2D grid.	discrete tomography	Marek Chrobak;Christoph Dürr	1999	Inf. Process. Lett.	10.1016/S0020-0190(99)00025-3	time complexity;combinatorics;discrete mathematics;convexity;computer science;mathematics;geometry;tomography;orthographic projection;polyomino;algorithm	ML	29.552954728739625	16.317240354337663	102750
2c25023dab8513ee6538114a8b2a78f8e0802de5	a fast and stable hybrid genetic algorithm for the ratio-cut partitioning problem on hypergraphs	design automation;distributed computing;permission;genetic algorithms permission design automation distributed computing machinery;genetic algorithm;genetic algorithms;machinery;hybrid genetic algorithm	A genetic algorithm (GA) for partitioning a hypergraph into two disjoint graphs of least ratio-cut is presented. Two notable features of this algorithm are: (i) a fast local optimizer, and (ii) a preprocessing step. Some supporting combinatorial arguments for the preprocessing heuristic are also provided. Experimental results on industrial benchmarks circuits are favorable when compared with recently published algorithms [25], [26], [19].	genetic algorithm;mathematical optimization;partition problem;preprocessor	Thang Nguyen Bui;Byung Ro Moon	1994	31st Design Automation Conference	10.1145/196244.196607	mathematical optimization;genetic algorithm;electronic design automation;cultural algorithm;computer science;electrical engineering;theoretical computer science;machine learning;genetic representation;distributed computing;algorithm;population-based incremental learning	EDA	24.97612903561205	4.521335868774338	102876
f820f54dcddc789b90a7e17b8606a0f2e545b88c	algorithms for deciding membership in polytopes of general dimension		We study the fundamental problem of polytope membership aiming at large convex polytopes, i.e. in high dimension and with many facets, given as an intersection of halfspaces. Standard data-structures as well as brute force methods cannot scale, due to the curse of dimensionality. We design an efficient algorithm, by reduction to the approximate Nearest Neighbor (ANN) problem based on the construction of a Voronoi diagram with the polytope being one bounded cell. We thus trade exactness for efficiency so as to obtain complexity bounds polynomial in the dimension, by exploiting recent progress in the complexity of ANN search. We employ this algorithm to present a novel boundary data structure based on a Newton-like iterative intersection procedure. We implement our algorithms and compare with brute-force approaches to show that they scale very well as the dimension and number of facets grow larger.	approximation algorithm;brute-force search;curse of dimensionality;data structure;iterative method;nearest-neighbor interpolation;newton;polynomial;voronoi diagram	Evangelos Anagnostopoulos;Ioannis Z. Emiris;Vissarion Fisikopoulos	2018	CoRR		polytope;combinatorics;mathematics;voronoi diagram;regular polygon;algorithm;data structure;k-nearest neighbors algorithm;bounded function	Theory	31.620164146637357	15.360657335176933	102921
df7a6b81e66ddbee6d6a69d085c7f8bfe2383c81	optimization over nonnegative and convex polynomials with and without semidefinite programming		The problem of optimizing over the cone of nonnegative polynomials is a fundamental problem in computational mathematics, with applications to polynomial optimization, control, machine learning, game theory, and combinatorics, among others. A number of breakthrough papers in the early 2000s showed that this problem, long thought to be out of reach, could be tackled by using sum of squares programming. This technique however has proved to be expensive for large-scale problems, as it involves solving large semidefinite programs (SDPs). In the first part of this thesis, we present two methods for approximately solving largescale sum of squares programs that dispense altogether with semidefinite programming and only involve solving a sequence of linear or second order cone programs generated in an adaptive fashion. We then focus on the problem of finding tight lower bounds on polynomial optimization problems (POPs), a fundamental task in this area that is most commonly handled through the use of SDP-based sum of squares hierarchies (e.g., due to Lasserre and Parrilo). In contrast to previous approaches, we provide the first theoretical framework for constructing converging hierarchies of lower bounds on POPs whose computation simply requires the ability to multiply certain fixed polynomials together and to check nonnegativity of the coefficients of their product. In the second part of this thesis, we focus on the theory and applications of the problem of optimizing over convex polynomials, a subcase of the problem of optimizing over nonnegative polynomials. On the theoretical side, we show that the problem of testing whether a cubic polynomial is convex over a box is NP-hard. This result is minimal in the degree of the polynomial and complements previously-known results on checking convexity of a polynomial globally. We also study norms generated by convex forms and provide an SDP hierarchy for optimizing over them. This requires an extension of a result of Reznick on sum of squares representation of positive definite forms to positive definite biforms. On the application side, we study a problem of interest to robotics and motion planning, which iii involves modeling complex environments with simpler representations. In this setup, we are interested in containing 3D-point clouds within polynomial sublevel sets of minimum volume. We also study two applications in machine learning: the first is multivariate monotone regression, which is motivated by some applications in pricing; the second concerns a specific subclass of optimization problems called difference of convex (DC) programs, which appear naturally in machine learning problems. We show how our techniques can be used to optimally reformulate DC programs in order to speed up some of the best-known algorithms used for solving them.	algorithm;coefficient;computation;computational mathematics;cubic function;game theory;machine learning;mathematical optimization;motion planning;np-hardness;optimization problem;point cloud;polynomial ring;robotics;semidefinite programming;monotone	Georgina Hall	2018	CoRR		discrete mathematics;computational mathematics;game theory;regular polygon;mathematics;explained sum of squares;semidefinite programming;polynomial;optimization problem;sum-of-squares optimization	Theory	26.161599266947835	14.315900666960534	102946
e022362c4e31ca233b8fa8d0db3b171fbf73d2ec	capacity and expressiveness of genomic tandem duplication		The majority of the human genome consists of repeated sequences. An important type of repeated sequences common in the human genome are tandem repeats, where identical copies appear next to each other. For example, in the sequence $AGTCunderline {TGTG}C$ , $TGTG$ is a tandem repeat, that may be generated from $AGTCTGC$ by a tandem duplication of length 2. In this paper, we investigate the possibility of generating a large number of sequences from a seed , i.e. a small initial string, by tandem duplications of bounded length. We study the capacity of such a system, a notion that quantifies the system’s generating power. Our results include exact capacity values for certain tandem duplication string systems. In addition, motivated by the role of DNA sequences in expressing proteins via RNA and the genetic code, we define the notion of the expressiveness of a tandem duplication system as the capability of expressing arbitrary substrings. We then completely characterize the expressiveness of tandem duplication systems for general alphabet sizes and duplication lengths. In particular, based on a celebrated result by Axel Thue from 1906, presenting a construction for ternary squarefree sequences, we show that for alphabets of size 4 or larger, bounded tandem duplication systems, regardless of the seed and the bound on duplication length, are not fully expressive, i.e. they cannot generate all strings even as substrings of other strings. Note that the alphabet of size 4 is of particular interest as it pertains to the genomic alphabet. Building on this result, we also show that these systems do not have full capacity. In general, our results illustrate that duplication lengths play a more significant role than the seed in generating a large number of sequences for these systems.		Siddharth Jain;Farzad Farnoud;Jehoshua Bruck	2017	IEEE Trans. Information Theory	10.1109/TIT.2017.2728079	tandem repeat;human genome;combinatorics;genomics;tandem;computer science;substring;gene duplication;genetic code;tandem exon duplication	Visualization	37.0964717482377	10.536587165263493	103155
09a38fc20e315338c8f3e7c232d35dc70878134a	epidemic spreading driven by biased random walks	complex networks	Random walk is one of the basic mechanisms found in many network applications. We study the epidemic spreading dynamics driven by biased random walks on complex networks. In our epidemic model, each time infected nodes constantly spread some infected packets by biased random walks to their neighbor nodes causing the infection of the susceptible nodes that receive the packets. An infected node get recovered from infection with a fixed probability. Simulation and analytical results on model and real-world networks show that the epidemic spreading becomes intense and wide with the increase of delivery capacity of infected nodes, average node degree, homogeneity of node degree distribution. Furthermore, there are corresponding optimal parameters such that the infected nodes have instantaneously the largest population, and the epidemic spreading process covers the largest part of a network.	complex network;degree distribution;random access;simulation	Cunlai Pu;Siyuan Li;Jian Yang	2014	CoRR		combinatorics;mathematics;complex network;statistics	Theory	36.60178306323072	14.690502427061457	103241
90e5d331f0e217fca71f45835203c37bc6882437	sink insertion for mesh improvement	mesh quality;parallel meshing;dynamic data structures;delaunay triangulations;experimentation;mesh generation	We propose sink insertion as a new technique to improve the mesh quality of Delaunay triangulations. We compare it with the conventional circumcenter insertion technique under three scheduling regimes: incremental, in blocks, and in parallel. Justification for sink insertion is given in terms of mesh quality, numerical robustness, running time, and ease of parallelization.		Herbert Edelsbrunner;Damrong Guoy	2002	Int. J. Found. Comput. Sci.	10.1142/S0129054102001060	mesh generation;ruppert's algorithm;computer science;theoretical computer science;mathematics;distributed computing;chew's second algorithm;t-vertices	Logic	33.42832484840022	15.595251323046828	103555
36e03d793f05ff3aa2775e4f7272c3b7dcf817ac	wireless autonomous robot evacuation from equilateral triangles and squares	mobile robots;square;wireless communication;triangle;evacuation	"""Consider an equilateral triangle or square with sides of lengthi¾?$$1$$. A number of robots starting at the same location on the perimeter or in the interior of the triangle or square are required to evacuate from an exit which is located at an unknown location on its perimeter. At any time the robots can move at identical speed equal to $$1$$, and they can cooperate by communicating with each other wirelessly. Thus, if a robot finds the exit it can broadcast """"exit found"""" to the remaining robots which then move in a straight line segment towards the exit to evacuate. Our task is to design robot trajectories that minimize the evacuation time of the robots, i.e., the time the last robot evacuates from the exit. Designing such optimal algorithms turns out to be a very demanding problem and even the case of equilateral triangles turns out to be challenging.#R##N##R##N#We design optimal evacuation trajectories algorithms for two robots in the case of equilateral triangles for any starting position and for squares for starting positions on the perimeter. It is shown that for an equilateral triangle, three or more robots starting on the perimeter cannot achieve better evacuation time than two robots, while there exist interior starting points from which three robots evacuate faster than two robots. For the square, three or more robots starting at one of the corners cannot achieve better evacuation time than two robots, but there exist points on the perimeter of the square such that three robots starting from such a point evacuate faster than two robots starting from this same point. In addition, in either the equilateral triangle or the square it can be shown that a simple algorithm is asymptotically optimal in the number $$k$$ of robots, as $$k \rightarrow \infty $$, provided that the robots start at the centre of the corresponding domain."""	autonomous robot	Jurek Czyzowicz;Evangelos Kranakis;Danny Krizanc;Lata Narayanan;Jaroslav Opatrny;Sunil M. Shende	2015		10.1007/978-3-319-19662-6_13	mobile robot;combinatorics;simulation;computer science;square;wireless	Robotics	32.35982699635963	12.864973784982405	103610
b67b5f62ceaba163b9331dd1d150fbaf83d8b192	the entropic penalty approach to stochastic programming	penalty methods;nonlinear programming;functions mathematics;relative entropy functional;penalties;stochastic optimization;stochastic processes;risk;decision theory;entropy;stochastic programming;determinants mathematics;lagrangian functions;information theory;approximation mathematics	A penalty-type decision-theoretic approach to Nonlinear Programming Problems with stochastic constraints is introduced. The Stochastic Program SP is replaced by a Deterministic Program DP by adding a term to the objective function to penalize solutions which are not “feasible in the mean.” The special feature of our approach is the choice of the penalty function PE, which is given in terms of the relative entropy functional, and is accordingly called entropic penalty. It is shown that PE has properties which make it suitable to treat stochastic programs. Some of these properties are derived via a dual representation of the entropic penalty which also enable one to compute PE more easily, in particular if the constraints in SP are stochastically independent. The dual representation is also used to express the Deterministic Problem DP as a saddle function problem. For problems in which the randomness occurs in the rhs of the constraints, it is shown that the dual problem of DP is equivalent to Expected Utility Maximization of the classical Lagrangian dual function of SP, with the utility being of the constant-risk-aversion type. Finally, mean-variance approximations of PE and the induced Approximating Deterministic Program are considered.	stochastic programming	Aharon Ben-Tal	1985	Math. Oper. Res.	10.1287/moor.10.2.263	stochastic programming;entropy;mathematical optimization;combinatorics;discrete mathematics;decision theory;information theory;nonlinear programming;stochastic optimization;penalty method;risk;mathematics;algorithm;statistics	AI	35.88719026394149	4.755071666314807	103778
7b1d5d25a3271b82af300ec4a9381e9379d434c4	maximizing induced cardinality under a determinantal point process		Determinantal point processes (DPPs) are well-suited to recommender systems where the goal is to generate collections of diverse, high-quality items. In the existing literature this is usually formulated as finding the mode of the DPP (the so-called MAP set). However, the MAP objective inherently assumes that the DPP models “optimal” recommendation sets, and yet obtaining such a DPP is nontrivial when there is no ready source of example optimal sets. In this paper we advocate an alternative framework for applying DPPs to recommender systems. Our approach assumes that the DPP simply models user engagements with recommended items, which is more consistent with how DPPs for recommender systems are typically trained. With this assumption, we are able to formulate a metric that measures the expected number of items that a user will engage with. We formalize this optimization of this metric as the Maximum Induced Cardinality (MIC) problem. Although the MIC objective is not submodular, we show that it can be approximated by a submodular function, and that empirically it is well-optimized by a greedy algorithm.	approximation algorithm;cardinality;collections (publication);digital photo professional (dpp);greedy algorithm;map;mathematical optimization;optimization problem;point process;preparation;recommender system;submodular set function	Jennifer Gillenwater;Alex Kulesza;Zelda Mariet	2018			mathematical optimization;recommender system;computer science;submodular set function;machine learning;cardinality;expected value;artificial intelligence;mode (statistics);point process;determinantal point process;greedy algorithm	ML	34.05484417232632	6.098804858756846	103920
3a80d48a3c87f99d635131dc80a156e9b2b205d2	a stochastic heuristic for visualising graph clusters in a bi-dimensional space prior to partitioning	random graph;graph clustering;combinatorial problems;a priori knowledge;graph partitioning;structural properties	This paper presents a new stochastic heuristic to reveal some structures inherent in large graphs, by displaying spatially separate clusters of highly connected vertex subsets on a two-dimensional grid. The algorithm employed is inspired by a biological model of ant behavior; it proceeds by local optimisations, and requires neither global criteria, nor any a priori knowledge of the graph. It is presented here as a preliminary phase in a recent approach to graph partitioning problems: transforming the combinatorial problem (minimising edge cuts) into one of clustering by constructing some bijective mapping between the graph vertices and points in some geometric space. After reviewing different embeddings proposed in the literature, we define a dissimilarity coefficient on the vertex set which translates the graph’s interesting structural properties into distances on the grid, and incorporate it into the clustering heuristic. The heuristic’s performance on a well-known class of pseudo-random graphs is assessed according to several metric and combinatorial criteria.	algorithm;cluster analysis;coefficient;graph partition;heuristic;pseudorandomness;random graph	Pascale Kuntz;Dominique Snyers;Paul J. Layzell	1999	J. Heuristics	10.1023/A:1009665701840	theta graph;lattice graph;random graph;mathematical optimization;combinatorics;geometric graph theory;discrete mathematics;a priori and a posteriori;graph bandwidth;null graph;graph property;regular graph;graph partition;clique-width;simplex graph;cubic graph;clustering coefficient;mathematics;voltage graph;distance-hereditary graph;graph;butterfly graph;complement graph;line graph;strength of a graph;circulant graph;coxeter graph	Theory	36.00072091692433	13.542614709481814	104078
19c0e64232bee798cf67b876a273cb20ee885535	touring convex bodies - a conic programming solution	convex body;computer experiment	We study the problem of finding a shortest tour visiting a given sequence of convex bodies in R. To our knowledge, this is the first attempt to attack the problem in its full generality: we investigate high-dimensional cases (d ≥ 2); we consider convex bodies bounded by (hyper)planes and/or (hyper)spheres; we do not restrict the start and the goal positions of the tour to be single points, we measure the length of the tour according to either Euclidean or L1 metric. Formulating the problem as a second order cone program (SOCP) makes it possible to incorporate distance constraints, which cannot be handled by a purely geometric algorithm. We implemented the SOCP in MATLAB and obtained its solution with the SeDuMi package. We ran computational experiments, which suggest that the proposed solution is practical. Finally, we present NP-hardness results, showing that the assumptions we make in the statement of our problems are crucial for the problems to be tractable.	algorithm;cobham's thesis;conic optimization;experiment;matlab;np-hardness;second-order cone programming	Valentin Polishchuk;Joseph S. B. Mitchell	2005			discrete mathematics;convex analysis;combinatorics;convex body;second-order cone programming;conic optimization;conic section;mathematical optimization;bounded function;mathematics;restrict;convex combination	ML	26.357398447847388	15.006380322958496	104295
a09bcefa88e7a7336ff381c25a338044d5afb3f5	structure and reversibility of 2d hexagonal cellular automata	rule matrix;rule based;reversible cellular automata;hexagonal cellular automata;matrix algebra;cellular automata;cellular automaton;geometric structure	Cellular automata are used to model dynamical phenomena by focusing on their local behavior which depends on the neighboring cells in order to express their global behavior. The geometrical structure of the models suggests the algebraic structure of cellular automata. After modeling the dynamical phenomena, it is sometimes an important problem to be able tomove backwards in order to understand it better. This is only possible if cellular automata is reversible. In this paper, 2D finite cellular automata defined by local rules based on hexagonal cell structure are studied. Rule matrix of the hexagonal finite cellular automaton is obtained. The rank of rule matrices representing the 2D hexagonal finite cellular automata via an algorithm is computed. It is a well known fact that determining the reversibility of a 2D cellular automata is a very difficult problem in general. Here, the reversibility problem of this family of 2D hexagonal cellular automata is also resolved completely. © 2011 Elsevier Ltd. All rights reserved.	algorithm;automata theory;cellular automaton;dynamical system;linear algebra;reversible computing	Irfan Siap;Hasan Akin;Selman Uguz	2011	Computers & Mathematics with Applications	10.1016/j.camwa.2011.09.066	stochastic cellular automaton;cellular automaton;reversible cellular automaton;block cellular automaton;combinatorics;discrete mathematics;continuous spatial automaton;quantum finite automata;quantum cellular automaton;asynchronous cellular automaton;continuous automaton;ω-automaton;mathematics;rule 184;mobile automaton;timed automaton;algorithm;lattice gas automaton	Theory	39.1490069689739	9.095723046314964	104433
ebd1beeb55f91a613d3fbe7ff92615e160678316	solver scheduling via answer set programming		Although Boolean Constraint Technology has made tremendous progress over the last decade, the efficacy of state-of-the-art solvers is known to vary considerably across different types of problem instances and is known to depend strongly on algorithm parameters. This problem was addressed by means of a simple, yet effective approach using handmade, uniform and unordered schedules of multiple solvers in ppfolio, which showed very impressive performance in the 2011 SAT Competition. Inspired by this, we take advantage of the modeling and solving capacities of Answer Set Programming (ASP) to automatically determine more refined, that is, non-uniform and ordered solver schedules from existing benchmarking data. We begin by formulating the determination of such schedules as multi-criteria optimization problems and provide corresponding ASP encodings. The resulting encodings are easily customizable for different settings and the computation of optimum schedules can mostly be done in the blink of an eye, even when dealing with large runtime data sets stemming from many solvers on hundreds to thousands of instances. Also, the fact that our approach can be customized easily enabled us to swiftly adapt it to generate parallel schedules for multi-processor machines.	answer set programming;blink;complementarity theory;computation;hydra (chess);mathematical optimization;multi-core processor;multi-objective optimization;multiprocessing;open-source software;randomized algorithm;reference implementation;schedule (computer science);sid meier's alpha centauri;solver;stable model semantics;stemming;timeout (computing);true quantified boolean formula;word lists by frequency	Holger H. Hoos;Roland Kaminski;Marius Thomas Lindauer;Torsten Schaub	2014	CoRR		mathematical optimization;computer science;artificial intelligence;theoretical computer science;machine learning;algorithm	AI	26.098100574047283	7.114449771975943	104467
186a9f94fef899e87955aab802aa99d5e706ecac	finding the 3d shortest path with visibility graph and minimum potential energy	potential energy path planning orbital robotics robotics and automation mobile robots intelligent robots manipulators application software mechanical engineering educational institutions;shortest path;manipulators;3d shortest path;visible edge identification;application software;intelligent robots;mobile robot;minimum potential energy;path planning;mobile robots;orbital robotics;three dimensional;collineation;robot manipulator;mechanical engineering;recursive process;convex polyhedra;polynomial time;automatic path planning;potential energy;robot manipulators;computer simulation;robotics and automation;visibility graph;computer simulation visible edge identification recursive process 3d shortest path visibility graph minimum potential energy automatic path planning mobile robots robot manipulators convex polyhedra collineation edge sequences polynomial time;edge sequences	Finding a three dimensional shortest path is of importance in the development of automatic path planning for mobile robots and robot manipulators, and for practical implementation, the algorithms need to be efficient. Presented is a method for shortest path planning in three-dimensional space in the presence of convex polyhedra. It is based on the visibility graph approach, extended from two to three-dimensional space. A collineation is introduced for the identification of visible edges in the three-dimensional visibility graph. The principle of minimum potential energy is adopted for finding a set of sub-shortest paths via different edge sequences, and from them the global shortest path is selected. The three dimensional visibility graph is constructed in O(n/sup 3/v/sup k/) time, where n is the number of vertices of the polyhedra, k is the number of obstacles and v is the largest number of vertices on any one obstacle. The process to determine the shortest path runs recursively in polynomial time. Results of a computer simulation are given, showing the versatility and efficiency of the approach.	shortest path problem;visibility graph	Kaichun Jiang;Lakmal D. Seneviratne;S. W. E. Earles	1993		10.1109/IROS.1993.583190	computer simulation;gallai–hasse–roy–vitaver theorem;mobile robot;visibility graph;mathematical optimization;combinatorics;canadian traveller problem;widest path problem;constrained shortest path first;any-angle path planning;longest path problem;average path length;computer science;pathfinding;artificial intelligence;euclidean shortest path;yen's algorithm;path graph;mathematics;geometry;path;shortest path problem;induced path;distance;k shortest path routing;shortest path faster algorithm;shortest-path tree	Vision	31.473940081525118	16.593085673511347	104717
de61be01f78b60814b75b6e7e624b2b62e963cf4	fast projection onto the simplex and the l1 ball	l1 norm ball;65c60;65k05;49m30;90c25;simplex;l_1 l1 norm ball;large scale optimization	A new algorithm is proposed to project, exactly and in finite time, a vector of arbitrary size onto a simplex or an l1-norm ball. It can be viewed as a GaussSeidel-like variant of Michelot’s variable fixing algorithm; that is, the threshold used to fix the variables is updated after each element is read, instead of waiting for a full reading pass over the list of non-fixed elements. This algorithm is empirically demonstrated to be faster than existing methods.	algorithm;taxicab geometry	Laurent Condat	2016	Math. Program.	10.1007/s10107-015-0946-6	mathematical optimization;combinatorics;mathematics;geometry;simplex	ML	25.862675584019815	17.497642299984086	105197
3c9ca1a95a1abf7075d745d92170c5bd8f2410f9	drawing undirected graphs with genetic algorithms	poligono convexo;graphe non oriente;non directed graph;graph drawing;polygone convexe;bibliografia;bibliography;algoritmo genetico;simple genetic algorithm;grafo no orientado;bibliographie;data visualization;improved genetic algorithm;algorithme genetique;genetic algorithm;visualisation donnee;convex polygon;fitness function	This paper proposes an improved genetic algorithm for producing aesthetically pleasing drawings of general undirected graphs. Previous undirected graph drawing algorithms draw large cycles with no chords as concave polygons. In order to overcome such disadvantage, the genetic algorithm in this paper designs a new mutation operator single-vertexneighborhood mutation and adds a component aiming at symmetric drawings to the fitness function, and it can draw such type graphs as convex polygons. The improved algorithm is of following advantages: The method is simple and it is easy to be implemented, and the drawings produced by the algorithm are beautiful, and also it is flexible in that the relative weights of the criteria can be altered. The experiment results show that the drawings of graphs produced by our algorithm are more beautiful than those produced by simple genetic algorithms, the original spring algorithm and the algorithm in bibliography .		Qingguo Zhang;Huayong Liu;Wei Zhang;Yajun Guo	2005		10.1007/11539902_4	mathematical optimization;combinatorics;genetic algorithm;computer science;artificial intelligence;machine learning;mathematics;bibliography;graph drawing;fitness function;data visualization;algorithm	Theory	30.019878012124686	10.7284140274109	105425
e70919dfabb130209fbaac2d0141a74cd7f87451	topological analysis and synthesis of structures related to certain classes of k-geodetic computer networks		A fundamental characteristic of computer networks is their topological structure. The question of the description of the structural characteristics of computer networks represents a problem that is not completely solved. Search methods for structures of computer networks, for which the values of the selected parameters of their operation quality are extreme, have not been completely developed. The construction of computer networks with optimum indices of their operation quality is reduced to the solution of discrete optimization problems over graphs. This paper describes in detail the advantages of the practical use of k-geodetic graphs [2, 3] in the topological design of computer networks as an alternative for the solution of the fundamental problems mentioned above which, we believe, are still open. Also, the topological analysis and synthesis of some classes of these networks have been performed.	cluster analysis;discrete optimization;geodetic datum;information processing;mathematical optimization	Carlos E. Frasser	2017	CoRR		geodetic datum;topology;mathematics	Theory	31.428149400874553	10.981235967156342	105457
7bebbc17edc80210fdbe2a76c737c1d201198912	a combinatorial approach to the classification problem	heuristic;efficiency;fonction discriminante;classification;discriminant function;algorithme;algorithm;eficacia;branch and bound method;combinatorial approach;metodo branch and bound;classification rules;funcion discriminante;efficacite;classification problem;hiperplano;heuristics;methode separation et evaluation;branch and bound;hyperplane;local search;clasificacion;recherche locale;hyperplan;heuristic algorithm;algoritmo	We study the two-group classi®cation problem which involves classifying an observation into one of two groups based on its attributes. The classi®cation rule is a hyperplane which misclassi®es the fewest number of observations in the training sample. Exact and heuristic algorithms for solving the problem are presented. Computational results con®rm the eciency of this approach. Ó 1999 Elsevier Science B.V. All rights reserved.	algorithm;branch and bound;computation;greedy algorithm;heuristic;heuristic (computer science);local search (optimization);naruto shippuden: clash of ninja revolution 3;open road tolling;polyhedron;search algorithm;tabu search	Nicola Yanev;Stephan Balev	1999	European Journal of Operational Research	10.1016/S0377-2217(98)00229-X	heuristic;mathematical optimization;combinatorics;heuristic;biological classification;computer science;local search;hyperplane;heuristics;multiclass classification;discriminant function analysis;mathematics;efficiency;branch and bound;algorithm	AI	25.084743434452562	9.445155147507963	105468
4b46c7cc302ee4ea070cc6145542721a77d26e8e	optimal pebble motion on a tree	mouvement arborescent caillou;dynamic programming;graph theory;optimal solution;complexite;solution optimale;programacion dinamica;teoria grafo;teorema existencia;motion control;probleme np complet;complejidad;pebble motion in tree;gmp1r;existence theorem;robotics;complexity;leftmost canonical plan;theorie graphe;algorithme;commande mouvement;algorithm;control movimiento;planificacion;mathematical programming;solucion optima;programmation dynamique;graph motion planning of 1 robot;motion planning;robotica;planning mouvement;lcp;planning;problema np completo;robotique;planification;programmation mathematique;theoreme existence;programacion matematica;np complete problem;algoritmo	In this paper we consider the following pebble coordination problem. Consider a tree with n vertices andk pebbles located at distinct vertices of the tree. Each pebbl e can be moved from its current position to an adjacent unoccupied vertex. Among th e k pebbles, one distinguished pebbles has been assigned a destination. We give anO(n5) algorithm for the problem of designing the shortest sequenc e of moves that takes the distinguished pebble from its original position t its destination. Our algorithm improves the running time of the best previously presented algorithm t at needed to solve O(n6) min-cost flow problems on graphs of size O(n). Our algorithm does not resort to reduction to flow but is inst ead based on a novel dynamic programming approach.	algorithm;dynamic programming;maxima and minima;minimum-cost flow problem;time complexity;vertex (geometry)	Vincenzo Auletta;Giuseppe Persiano	2001	Inf. Comput.	10.1006/inco.2000.3005	planning;motion control;combinatorics;complexity;np-complete;computer science;graph theory;dynamic programming;calculus;mathematics;motion planning;robotics;algorithm	Theory	30.711512230125617	16.969399491267158	105652
4177d535626022f8568d70eb983dba629d42c5cf	properties and an approximation algorithm of round-tour voronoi diagrams	round tour;generalized voronoi diagram;restaurants and bookstores;approximate algorithm;shortest round tour;efficient algorithm;facility location analysis;voronoi diagram;facility location	"""This paper proposes a new generalization of the Voronoi diagram. Consider two kinds of facilities located in a city, for example, restaurants and bookstores. We want to visit both and return to our house. To each pair of a restaurant and a bookstore is assigned a region such that a resident in this region can visit them in a shorter round tour than visiting any other pair. The city is partitioned into these regions according to which pair of a restaurant and bookstore permits the shortest round tour. We call this partitioning a """"round-tour Voronoi Diagram"""" for the restaurants and bookstores. We study the basic properties of this Voronoi diagram and consider an efficient algorithm for its approximate construction."""	approximation algorithm;round-off error;voronoi diagram	Hidenori Fujii;Kokichi Sugihara	2010	Trans. Computational Science	10.1007/978-3-642-16007-3_5	mathematical optimization;combinatorics;voronoi diagram;operations management;mathematics	Theory	27.449595675213555	16.342970580046096	105797
079db145eb93fa62d91c5ad345276b3407a0bbe4	downsampling of signals on graphs via maximum spanning trees	max cut;interpolation;graph multiresolution;maximum spanning tree;graph wavelet filter banks;bipartite approximation signal downsampling scheme maximum spanning trees general weighted graph mst based method graph multiresolution graph cuts critical sampling graph wavelet transforms graph signal compression;vegetation;wavelet transforms approximation theory signal sampling trees mathematics;laplace equations;bipartite graph interpolation signal resolution laplace equations joining processes transforms vegetation;bipartite approximation;downsampling on graphs;transforms;joining processes;signal processing on graphs bipartite approximation downsampling on graphs graph multiresolution graph wavelet filter banks max cut maximum spanning tree;signal resolution;signal processing on graphs;bipartite graph	Downsampling of signals living on a general weighted graph is not as trivial as of regular signals where we can simply keep every other samples. In this paper we propose a simple, yet effective downsampling scheme in which the underlying graph is approximated by a maximum spanning tree (MST) that naturally defines a graph multiresolution. This MST-based method significantly outperforms the two previous downsampling schemes, coloring-based and SVD-based, on both random and specific graphs in terms of computations and partition efficiency quantified by the graph cuts. The benefit of using MST-based downsampling for recently developed critical-sampling graph wavelet transforms in compression of graph signals is demonstrated.	approximation algorithm;computation;connectivity (graph theory);cut (graph theory);decimation (signal processing);directed graph;experiment;file spanning;filter bank;graph coloring;maximum cut;minimum spanning tree;sampling (signal processing);singular value decomposition;tree structure;wavelet transform	Ha Q. Nguyen;Minh N. Do	2015	IEEE Transactions on Signal Processing	10.1109/TSP.2014.2369013	block graph;lattice graph;graph power;mathematical optimization;maximum cut;factor-critical graph;combinatorics;geometric graph theory;discrete mathematics;graph bandwidth;bipartite graph;null graph;interpolation;regular graph;clique-width;simplex graph;comparability graph;graph factorization;mathematics;voltage graph;distance-hereditary graph;complement graph;vegetation;strength of a graph;statistics	Vision	35.51650064618177	11.94102298588572	106019
0a1f17e3360f28150533ccf346a69c8ff7b6a51b	multi-index transportation problems with 1-nested structure		Consideration was given to the solution of the multi-index transportation problems of linear and integer-linear programming. It was proposed to use the approach based on studying the reducibility of the multi-index transportation problems to the problem of the minimal cost in the treelike network. It was proved that within the framework of the reduction scheme the condition for 1-nesting of the multi-index problems is necessary and sufficient for reducibility to the problem of the minimal-cost flow problem on a treelike network. An algorithm was proposed to solve the 1-nested multi-index problems requiring as many computer operations as the square of variables in the original problem.		Lev G. Afraimovich;Aleksey S. Katerov;Michail Kh. Prilutskii	2016	Automation and Remote Control	10.1134/S0005117916110023	mathematical optimization;combinatorics;discrete mathematics;covering problems;fuzzy transportation;mathematics	Robotics	24.69565352508958	12.646892652971342	106078
c07ac76bf4a2113a3c393e048ef9726063899602	finite-state approximation of markov decision processes with unbounded costs and borel spaces	finite state approximation optimal stationary policy stochastic control problem borel action spaces borel state unbounded one stage cost discrete time markov decision process;kernel;quantization markov decision processes stochastic control finite state approximation;cost function;optimal control approximation theory discrete time systems markov processes;aerospace electronics;markov processes;cost function markov processes kernel aerospace electronics extraterrestrial measurements;extraterrestrial measurements	Computing optimal policies for stochastic control problems with general state and action spaces is often intractable. This paper studies finite-state approximations of discrete time Markov decision processes (MDPs) with Borel state and action spaces and unbounded one-stage cost function, for both discounted and average cost criteria. Under mild technical assumptions, it is shown that stationary optimal policies obtained from the solutions to finite-state models can approximate an optimal stationary policy with arbitrary precision. A simulation example is provided.	approximation algorithm;arbitrary-precision arithmetic;discretization;loss function;markov chain;markov decision process;simulation;state space;stationary process;stochastic control	Naci Saldi;Serdar Yüksel;Tamás Linder	2015	2015 54th IEEE Conference on Decision and Control (CDC)	10.1109/CDC.2015.7403015	markov decision process;time reversibility;markov chain;mathematical optimization;markov kernel;combinatorics;discrete mathematics;kernel;partially observable markov decision process;markov property;mathematics;markov renewal process;markov process;markov model;statistics;variable-order markov model	ML	38.55687837687917	5.143779895710406	106141
159fe7d7e12787f4eea327711aba797da6719443	computational aspects of the colorful carathéodory theorem		Let P1, . . . , Pd+1 ⊂ R be d-dimensional point sets such that the convex hull of each Pi contains the origin. We call the sets Pi color classes, and we think of the points in Pi as having color i. A colorful choice is a set with at most one point of each color. The colorful Carathéodory theorem guarantees the existence of a colorful choice whose convex hull contains the origin. So far, the computational complexity of finding such a colorful choice is unknown. We approach this problem from two directions. First, we consider approximation algorithms: an m-colorful choice is a set that contains at most m points from each color class. We show that for any constant ε > 0, an dε(d + 1)e-colorful choice containing the origin in its convex hull can be found in polynomial time. This notion of approximation has not been studied before, and it is motivated through the applications of the colorful Carathéodory theorem in the literature. In the second part, we present a natural generalization of the colorful Carathéodory problem: in the Nearest Colorful Polytope problem (NCP), we are given sets P1, . . . , Pn ⊂ R that do not necessarily contain the origin in their convex hulls. The goal is to find a colorful choice whose convex hull minimizes the distance to the origin. We show that computing local optima for the NCP problem is PLS-complete, while computing a global optimum is NP-hard.	approximation algorithm;complexity class;computation;computational complexity theory;convex hull;global optimization;local optimum;local search (optimization);np-hardness;polynomial;search problem;stumbleupon;time complexity	Wolfgang Mulzer;Yannik Stein	2018	Discrete & Computational Geometry	10.1007/s00454-018-9979-y	mathematics;combinatorics;polytope;local optimum;time complexity;convex hull;topology;regular polygon;computational complexity theory;global optimum	Theory	27.401585227389027	17.35410540297459	106248
21134957b34c43ec1b85039136feb504c62691ce	minimizing unsatisfaction in colorful neighborhoods		Abstract. Coloring sparse graphs under various restrictions is a theoretical problem of significant practical relevance. Here we consider the problem of maximizing the number of different colors available at the nodes and their neighborhoods, given a predetermined number of colors. In the analytical framework of a tree approximation, solutions obtained by population dynamics show that existing algorithms starting from random initial conditions may have suboptimal performance, due to the existence of metastable states.	algorithm;approximation;color;initial condition;population dynamics;relevance;sparse matrix	K. Y. Michael Wong;David Saad	2007	CoRR		polymer;alkyl;phenyl group;methine group;magenta;monomer;silver halide;polymer chemistry;phenylene;materials science	ML	38.89775047244178	13.79040521316294	106434
5e983ecae5b806cee1180d0b293c4d63f57aa7bb	optimal competitive online ray search with an error-prone robot	online algorithm;errors;competitive strategy;algorithmique;estrategia optima;computational geometry;blind;robotics;optimal strategy;ray search;algorithmics;algoritmica;motion planning;robotica;robotique;online algorithms;strategie optimale;ciego;aveugle	We consider the problem of finding a door along a wall with a blind robot that neither knows the distance to the door nor the direction towards of the door. This problem can be solved with the well-known doubling strategy yielding an optimal competitive factor of 9 with the assumption that the robot does not make any errors during its movements. We study the case that the robot’s movement is erroneous. In this case the doubling strategy is no longer optimal. We present optimal competitive strategies that take the error assumption into account. The analysis technique can be applied to different error models.	best, worst and average case;cognitive dimensions of notations;competitive analysis (online algorithm);emoticon;iteration;like button;mathematical model;maximal set;period-doubling bifurcation;robot;strategic management;whole earth 'lectronic link;monotone	Tom Kamphans;Elmar Langetepe	2005		10.1007/11427186_51	online algorithm;simulation;computational geometry;computer science;artificial intelligence;machine learning;robotics;algorithmics	Theory	31.008858153989713	13.536680799193547	106480
7c42e05e37c955af8a23e219482d21cfc0d2c6d0	statistical and computational limits for sparse matrix detection		This paper investigates the fundamental limits for detecting a high-dimensional sparse matrix contaminated by white Gaussian noise from both the statistical and computational perspectives. We consider p×pmatrices whose rows and columns are individually k-sparse. We provide a tight characterization of the statistical and computational limits for sparse matrix detection, which precisely describe when achieving optimal detection is easy, hard, or impossible, respectively. Although the sparse matrices considered in this paper have no apparent submatrix structure and the corresponding estimation problem has no computational issue at all, the detection problem has a surprising computational barrier when the sparsity level k exceeds the cubic root of the matrix size p: attaining the optimal detection boundary is computationally at least as hard as solving the planted clique problem. The same statistical and computational limits also hold in the sparse covariance matrix model, where each variable is correlated with at most k others. A key step in the construction of the statistically optimal test is a structural property for sparse matrices, which can be of independent interest.	clique problem;column (database);computation;cubic function;planted clique;sensor;sparse matrix;the matrix	T. Tony Cai;Yihong Wu	2018	CoRR		statistics;mathematical optimization;covariance matrix;sparse matrix;row and column spaces;additive white gaussian noise;matrix (mathematics);clique problem;mathematics	ML	38.94083826803895	12.805641726436829	106550
7330a43d5d407740875ed42603ecc2505e6b52f4	efficient approximate representations for computationally expensive features			analysis of algorithms;approximation algorithm	Raúl Santos-Rodríguez;Niall Twomey	2018				AI	31.206246493479146	6.494997795365078	106635
ca1c2ad10314baaf03e0873f23e5a50b1f8286b6	approximation guarantees of stochastic greedy algorithms for subset selection		Subset selection is a fundamental problem in many areas, which aims to select the best subset of size at most k from a universe. Greedy algorithms are widely used for subset selection, and have shown good approximation performances in deterministic situations. However, their behaviors are stochastic in many realistic situations (e.g., large-scale and noisy). For general stochastic greedy algorithms, bounded approximation guarantees were obtained only for subset selection with monotone submodular objective functions, while real-world applications often involve non-monotone or nonsubmodular objective functions and can be subject to a more general constraint than a size constraint. This work proves their approximation guarantees in these cases, and thus largely extends the applicability of stochastic greedy algorithms.		Chao Qian;Yang Yu;Ke Tang	2018		10.24963/ijcai.2018/205	machine learning;artificial intelligence;computer science;greedy algorithm	AI	32.090159075215375	5.509691888810737	107115
daaf93404da05f605d55c33f8f3aa95a5e186e3d	the query complexity of estimating weighted averages		The query complexity of estimating the mean of some [0, 1] variables is understood. Inspired by some work by Carterette et al. on evaluating retrieval systems, and by Moffat and Zobel’s new proposal for such evaluation, we examine the query complexity of weighted average calculation. In general, determining an answer within accuracy $${\varepsilon}$$ , with high probability, requires $${\Omega(\varepsilon^{-2})}$$ queries, as the mean is a special case. There is a matching upper bound for the weighted mean. If the weights are a normalized prefix of a divergent series, the same result holds. However, if the weights follow a geometric sequence, a sample of size $${\Omega(\log (1/\varepsilon))}$$ suffices. Our principal contribution is the investigation of power-law sequences of weights. We show that if the ith largest weight is proportional to i −p , for p > 1, then the query complexity is in $${\Omega(\varepsilon^{2/(1-2p)})}$$ .	decision tree model;with high probability;zobel network	Amit Chakrabarti;Venkatesan Guruswami;Andrew Wirth;Anthony Wirth	2011	Acta Informatica	10.1007/s00236-011-0145-8	combinatorics;discrete mathematics;mathematics;statistics	ML	34.9408815393653	13.868354852837086	107213
951d686ed20b00c10faf9307bfcef0b7c6d632df	allocation strategies for high fidelity models in the multifidelity regime		We propose a novel approach to allocating resources for expensive simulations of high fidelity models when used in a multifidelity framework. Allocation decisions that distribute computational resources across several simulation models become extremely important in situations where only a small number of expensive high fidelity simulations can be run. We identify this allocation decision as a problem in optimal subset selection, and subsequently regularize this problem so that solutions can be computed. Our regularized formulation yields a type of group lasso problem that has been studied in the literature to accomplish subset selection. Our numerical results compare performance of algorithms that solve the group lasso problem for algorithmic allocation against a variety of other strategies, including those based on classical linear algebraic pivoting routines and those derived from more modern machine learning-based methods. We demonstrate on well known synthetic problems and more difficult real-world simulations that this group lasso solution to the relaxed optimal subset selection problem performs better than the alternatives.		Daniel J. Perry;Robert Michael Kirby;Akil C. Narayan;Ross T. Whitaker	2018	CoRR			AI	31.423778333411743	6.438027717813788	107550
93e2aae1f32d65cc6e2cb90ad3ae44d4bb3aeea7	upper bound performance of semi-definite programming for localisation in inhomogeneous media		In this paper, we regarded an absorbing inhomogeneous medium as an assembly of thin layers having different propagation properties. We derived a stochastic model for the refractive index and formulated the localisation problem given noisy distance measurements using graph realisation problem. We relaxed the problem using semi-definite programming (SDP) approach in lp realisation domain and derived upper bounds that follow Edmundson-Madansky bound of order 6p (EM6p) on the SDP objective function to provide an estimation of the techniques' localisation accuracy. Our results showed that the inhomogeneity of the media and the choice of lp norm have significant impact on the ratio of the expected value of the localisation error to the upper bound for the expected optimal SDP objective value. The tightest ratio was derived when l∞ norm was used.	absorbing markov chain;loss function;optimization problem;semiconductor industry;semidefinite programming;software propagation	Esmaeil S. Nadimi;Victoria Blanes-Vidal	2017	2017 IEEE 27th International Workshop on Machine Learning for Signal Processing (MLSP)	10.1109/MLSP.2017.8168145	artificial intelligence;noise measurement;stochastic modelling;norm (mathematics);expected value;pattern recognition;semidefinite programming;stochastic process;computer science;mathematical optimization;upper and lower bounds;graph	Vision	25.53757795848024	16.141073203708547	107589
b507df21e31f3d99a8e2a3cdf2f202f87d094b8b	the degree sequence of a random graph. i. the models	degree sequence;random graph	We show that the joint distribution of the degrees of a random graph can be accurately approximated by several simpler models derived from a set of independent binomial distributions. On the one hand we consider the distribution of degree sequences of random graphs with n vertices and 1 2 m edges. For a wide range of values of m, this distribution is almost everywhere in close correspondence with the conditional distribution {(X1, . . . ,Xn) | ∑ Xi = m} where X1, . . . ,Xn are independent random variables, each having the same binomial distribution as the degree of one vertex. We also consider random graphs with n vertices and edge probability p. For a wide range of functions p = p(n), the distribution of the degree sequence can be approximated by {(X1, . . . ,Xn) | ∑ Xi is even}, where X1, . . . ,Xn are independent random variables each having the distribution Binom(n − 1, p′), where p′ is itself a random variable with a particular truncated normal distribution. To facilitate computations, we demonstrate techniques by which statistics in this model can be inferred from those in a simple model of independent binomial random variables. Where they apply, the accuracy of our method is sufficient to determine asymptotically all probabilities greater than n−k for any fixed k. In this first paper, we use the geometric mean of the degrees as a tutorial example. In the second paper, we will determine the asymptotic distribution of the t-th largest degree for all functions t = t(n) as n →∞.	approximation algorithm;computation;degree (graph theory);random graph;vertex (geometry)	Brendan D. McKay;Nicholas C. Wormald	1997	Random Struct. Algorithms	10.1002/(SICI)1098-2418(199709)11:2%3C97::AID-RSA1%3E3.0.CO;2-O	independent and identically distributed random variables;marginal distribution;random regular graph;random graph;probability density function;combinatorics;discrete mathematics;multivariate random variable;beta negative binomial distribution;stability;random element;product distribution;convergence of random variables;mixture distribution;central limit theorem;mathematics;compound probability distribution;joint probability distribution;infinite divisibility;statistics;probability integral transform	Theory	38.85555220834527	15.463020477592671	107609
415501c7eb7aa804523d3522a646d1846ef91666	insert and delete algorithms for maintaining dynamic delaunay triangulations	nearest neighbor condensing;delaunay triangulation;computational geometry;algorithmic complexity;insertion deletion	We recall that optimal condensing of nearest neighbor data requires the construction of the Delaunay triangulation of the training set. We argue that, from the viewpoint of computational complexity, an iterative approach using a dynamic triangulation is most desirable. We describe two algorithms, Insert and Delete, which permit to maintain a dynamic Delaunay triangulation.	algorithm;delaunay triangulation	Pierre A. Devijver;Michel Dekesel	1982	Pattern Recognition Letters	10.1016/0167-8655(82)90015-0	minimum-weight triangulation;combinatorics;discrete mathematics;delaunay triangulation;computational geometry;theoretical computer science;pitteway triangulation;point set triangulation;mathematics;geometry;constrained delaunay triangulation;bowyer–watson algorithm	Vision	32.64631642757268	16.814029998681068	107870
923c332ffa5d0e18c1e210d99dc68a1ffe11c1f0	digital straightness - a review	11xx;optimisation;straightness;62h35;articulo sintesis;optimizacion;article synthese;algorithme combinatoire;65d18;digital geometry;sturmian words;number theory;sturmian word;digital straight lines;informatique theorique;rectitude;52c99;rectitud;segment droite;segmento recta;theorie nombre;optimization;line segment;68u05;teoria numeros;review;computer theory;informatica teorica	A digital arc is called ‘straight’ if it is the digitization of a straight line segment. Since the concept of digital straightness was introduced in the mid-1970s, dozens of papers on the subject have appeared; many characterizations of digital straight lines have been formulated, and many algorithms for determining whether a digital arc is straight have been de1ned. This paper reviews the literature on digital straightness and discusses its relationship to other concepts of geometry, the theory of words, and number theory. ? 2003 Published by Elsevier B.V. MSC: 52C99; 62H35; 65D18; 68U05	adjacency matrix;algorithm;digital image;grayscale;online and offline;performance evaluation;random number generation;tree traversal	Reinhard Klette;Azriel Rosenfeld	2004	Discrete Applied Mathematics	10.1016/j.dam.2002.12.001	sturmian word;computer vision;number theory;line segment;mathematics;geometry;algorithm;algebra;digital geometry	Graphics	30.263905148471483	16.382463797432894	107891
1f2f31c598a34753d0d0f516b0a09aee594b6688	large deviation analysis for layered percolation problems on the complete graph	exponential rate function;percolation;erdos renyi random graph	We analyze the large deviation properties for the (multitype) version of percolation on the complete graph – the simplest substitutive generalization of the Erdős-Rènyi random graph that was treated in article by Bollobás et al. (Random Structures Algorithms 31 (2007), 3–122). Here the vertices of the graph are divided into a fixed finite number of sets (called layers) the probability of {u, v} being in our edge set depends on the respective layers of u and v. We determine the exponential rate function for the probability that a giant component occupies a fixed fraction of the graph, while all other components are small. We also determine the exponential rate function for the probability that a particular exploration process on the random graph will discover a certain fraction of vertices in each layer, without encountering a giant component. © 2011 Wiley Periodicals, Inc. Random Struct. Alg., 40, 460–492, 2012	algorithm;bollobás–riordan polynomial;erdős number;giant component;john d. wiley;norm (social);percolation theory;rado graph;random graph;struct (c programming language);time complexity;vertex (geometry)	Lincoln Chayes;S. Alex Smith	2012	Random Struct. Algorithms	10.1002/rsa.20387	combinatorics;discrete mathematics;percolation;mathematics;erdős–rényi model;statistics	Theory	37.82163001821738	15.841146002914712	107961
8ccd2047c1f9f8ada7d5b5590d1ecca1855a1036	vlsi placement problem based on ant colony optimization algorithm		The paper discusses a modified algorithm based on the ants’ behavior in nature. We suggest to apply this algorithm for solving the element placement problem—one of the most difficult problem in the VLSI design. This problem belongs to the NP-class problem that is there are no precise methods to solve this problem. Also we formulate the placement problem and choose an optimization criterion. The developed ant colony optimization (ACO) algorithm obtains optimal and quasi-optimal solutions during polynomial time. The distinguish feature of the algorithm is that alternative solution are represented as an undirected graph with weighted edges. Besides, at each generation the algorithm creates a taboo-list to eliminate the quantity of agent (ant) which is wrong from the point of view the using of Reverse Polish notation. To compare obtained results with known analogous algorithms we developed software which allows to carry out experiments on the basis of IBM benchmarks. Conducted experiments shown that the ACO algorithm is better than the other algorithms an average of 9 %.	algorithm;ant colony optimization algorithms;optimization problem;very-large-scale integration	Daria V. Zaruba;Dmitry Yu. Zaporozhets;Vladimir V. Kureichik	2016		10.1007/978-3-319-33625-1_12	ant colony optimization algorithms;metaheuristic	EDA	24.61823434284993	4.781251410573706	108275
274210a272b90e9707afe54e6838917ab0a6cd6a	subgraph detection using eigenvector l1 norms		When working with network datasets, the theoretical framework of detection theory for Euclidean vector spaces no longer applies. Nevertheless, it is desirable to determine the detectability of small, anomalous graphs embedded into background networks with known statistical properties. Casting the problem of subgraph detection in a signal processing context, this article provides a framework and empirical results that elucidate a “detection theory” for graph-valued data. Its focus is the detection of anomalies in unweighted, undirected graphs through L1 properties of the eigenvectors of the graph’s so-called modularity matrix. This metric is observed to have relatively low variance for certain categories of randomly-generated graphs, and to reveal the presence of an anomalous subgraph with reasonable reliability when the anomaly is not well-correlated with stronger portions of the background graph. An analysis of subgraphs in real network datasets confirms the efficacy of this approach.	algorithm;analysis of algorithms;anomaly detection;dense subgraph;detection theory;embedded system;graph (discrete mathematics);modularity (networks);randomness;signal processing;simulation;t-norm;taxicab geometry;the matrix	Benjamin A. Miller;Nadya T. Bliss;Patrick J. Wolfe	2010			combinatorics;discrete mathematics;universal graph;graph theory;forbidden graph characterization;machine learning;subgraph isomorphism problem;mathematics;induced subgraph isomorphism problem;statistics	ML	38.70055672013856	12.736033014195646	108578
3a93b0d547d22d9da88dbafac47d4451eaa401d3	space defragmentation heuristic for 2d and 3d bin packing problems	defragmentation technique;free space;additional item;bin packing problem;fragmented space;main difficulty;space defragmentation heuristic;bin shuffling strategy;multi-dimensional packing problem;leading meta-heuristic approach;continuous usable space;incremental improvement	One of main difficulties of multi-dimensional packing problems is the fragmentation of free space into several unusable small parts after a few items are packed. This study proposes a defragmentation technique to combine the fragmented space into a continuous usable space, which potentially allows the packing of additional items. We illustrate the effectiveness of this technique on the twoand three-dimensional Bin Packing Problems. In conjunction with a bin shuffling strategy for incremental improvement, our resultant algorithm outperforms all leading meta-heuristic approaches.	algorithm;apache axis;bpp (complexity);bin packing problem;computation;constructive heuristic;experiment;fragmentation (computing);optic axis of a crystal;resultant;set packing;usability;visibility graph	Zhaoyi Zhang;Songshan Guo;Wenbin Zhu;Wee-Chong Oon;Andrew Lim	2011		10.5591/978-1-57735-516-8/IJCAI11-123	mathematical optimization;bin packing problem;mathematics	AI	29.445974213702097	14.639762101469318	108671
787332fc9e1845671dab8b4df1b6d45ee28917a7	a polynomial arc-search interior-point algorithm for convex quadratic programming	arc search convex quadratic programming interior point method polynomial algorithm;convex quadratic programming;polynomial algorithm;arc search;interior point method	Arc-search is developed for linear programming in [24] and [25]. The algorithms search for optimizers along an ellipse that is an approximation of the central path. In this paper, the arc-search method is applied to primal-dual path-following interior-point method for convex quadratic programming. A simple algorithm with iteration complexity is devised. Several improvements on the simple algorithm, which improve computational efficiency, increase step length, and further reduce duality gap in every iteration, are then proposed and implemented. It is intuitively clear that the iteration with these improvements will reduce the duality gap more than the iteration of the simple algorithm without the improvements, though it is hard to show how much these improvements reduce the complexity bound. The proposed algorithm is implemented in MATLAB and tested on quadratic programming problems originating from [13]. The result is compared to the one obtained by LOQO in [22]. The proposed algorithm uses fewer iterations in all these problems and the number of total iterations is 27% fewer than the one obtained by LOQO. This preliminary result shows that the proposed algorithm is promising.	approximation algorithm;demon seed;matlab;polynomial;quadratic programming	Yaguang Yang	2011	European Journal of Operational Research	10.1016/j.ejor.2011.06.020	mathematical optimization;combinatorics;discrete mathematics;criss-cross algorithm;second-order cone programming;interior point method;jenkins–traub algorithm;mathematics;quadratic programming;output-sensitive algorithm	Theory	26.729518591258532	8.330360789713296	108733
72f60533e1650bc66745fc2df7f800a92c941546	quantum algorithms for some hidden shift problems	symbole legendre;computadora;68w40;efficient algorithms;hidden subgroup problem;43a38;11y16;decalaje;subgrupo;subgroup;fourier transform;analyse fourier;ordinateur;periodicite;sous groupe;computer;decalage;68wxx;algorithme;periodicity;algorithm;quantum computation;periodicidad;quantum fourier transform;quantum computer;fourier transformation;quantum algorithm;transformation fourier;fourier analysis;analisis fourier;cryptosysteme;shift;calcul quantique;quantum computing;calculo cuantico;81p68;legendre symbol;transformacion fourier;algoritmo	"""Almost all of the most successful quantum algorithms discovered to date exploit the ability of the Fourier transform to recover subgroup structure of functions, especially periodicity. The fact that Fourier transforms can also be used to capture shift structure has received far less attention in the context of quantum computation.In this paper, we present three examples of """"unknown shift"""" problems that can be solved efficiently on a quantum computer using the quantum Fourier transform. We also define the hidden coset problem, which generalizes the hidden shift problem and the hidden subgroup problem. This framework provides a unified way of viewing the ability of the Fourier transform to capture subgroup and shift structure."""	computation;hidden subgroup problem;quantum fourier transform;quantum algorithm;quantum computing;quasiperiodicity	Wim van Dam;Sean Hallgren;Lawrence Ip	2003		10.1137/S009753970343141X	fourier transform;quantum fourier transform;mathematical optimization;combinatorics;mathematical analysis;discrete mathematics;hidden subgroup problem;fourier inversion theorem;mathematics;quantum computer;quantum algorithm;algorithm;algebra	Theory	36.63399328704099	9.88308344852614	108769
ba17ad1cc1afefa45cb899e85f077ba83a68e206	efficient bitmap resemblance under translations	minimum hausdorff distance;rasterized approximation;bitmap;geometric pattern matching	We present three efficient algorithms for resemblance between two bitmaps. Two of the algorithms are rasterized approximations, based on existing algorithms which compute the exact minimum Hausdorff distance between point sets under translation. The minimum Hausdorff distance is a min – max – min distance. We convert this operation into an and – or – and operation. This conversion, together with the fact that we encode distances into bits in the words of the pixel plane, of standard graphics hardware, contribute to the speed-up of our rasterized algorithms. The third algorithm is faster than the first two rasterized algorithms, and combines speed-up ideas from both. The performance of our rasterized algorithm is compared to an existing rasterized approximation algorithm for bitmap resemblance. We compare runtimes of these algorithms, parametrized by the size of the bitmap, the density of black bits in the bitmap and other parameters. Our results are summarized in tables and show that our algorithm is faster.	bitmap	Klara Kedem;Daniel Cohen-Or	1997	Int. J. Comput. Geometry Appl.	10.1142/S0218195997000053	combinatorics;theoretical computer science;mathematics;geometry;bitmap;algorithm	Theory	34.094503423066406	16.96466119200186	108801
03a15b497f90a285f46f91e69bcc4cb5c464f244	sampling and reconstruction of graph signals via weak submodularity and semidefinite relaxation		We study the problem of sampling a bandlimited graph signal in the presence of noise, where the objective is to select a node subset of prescribed cardinality that minimizes the signal reconstruction mean squared error (MSE). To that end, we formulate the task at hand as the minimization of MSE subject to binary constraints, and approximate the resulting NP-hard problem via semidefinite programming (SDP) relaxation. Moreover, we provide an alternative formulation based on maximizing a monotone weak submodular function and propose a randomized-greedy algorithm to find a sub-optimal subset. We then derive a worst-case performance guarantee on the MSE returned by the randomized greedy algorithm for general non-stationary graph signals. The efficacy of the proposed methods is illustrated through numerical simulations on synthetic and realworld graphs. Notably, the randomized greedy algorithm yields an order-of-magnitude speedup over state-of-the-art greedy sampling schemes, while incurring only a marginal MSE performance loss.		Abolfazl Hashemi;Rasoul Shafipour;Haris Vikalo;Gonzalo Mateos	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461925	submodular set function;mathematical optimization;discrete mathematics;mathematics;sampling (statistics);speedup;semidefinite programming;signal processing;signal reconstruction;greedy algorithm;mean squared error	ML	34.4431951549833	6.266651412278536	108874
143cc58a332346141c9983f6bc2fd6215d3cb4e2	stochastic complexity for tree models	kth order markov chains stochastic complexity tree models data compression problem string gambling string prediction minimax bayes strategy curved exponential families minimax regret;trees mathematics bayes methods computational complexity data compression formal languages markov processes minimax techniques;context markov processes encoding data models maximum likelihood estimation complexity theory	We study the problem of data compression, gambling and prediction of strings xn = x1x2...xn in terms of coding regret, where the tree model is assumed as a target class. We apply the minimax Bayes strategy for curved exponential families to this problem and show that it achieves the minimax regret without restriction on the data strings. This is an extension of the minimax result by (Takeuchi et al. 2013) for models of kth order Markov chains and determines the constant term of the Stochastic Complexity for the tree model.	constant term;data compression;kolmogorov complexity;markov chain;minimax;naive bayes classifier;regret (decision theory);time complexity	Jun'ichi Takeuchi;Andrew R. Barron	2014	2014 IEEE Information Theory Workshop (ITW 2014)	10.1109/ITW.2014.6970825	mathematical optimization;combinatorics;machine learning;mathematics;game complexity;variable-order markov model	ML	36.349216719037734	7.829340756018602	108946
2d41d996f541450f9cf756236fcc0283f9876017	mean-variance criteria for finite continuous-time markov decision processes	minimisation;transition rate matrix;politica optima;continuous time;control optimo;minimization;matriz transicion;mathematics;equivalent mean average criterion;finite continuous time markov decision process;ergodicite;martingale;proceso markov;canonical form;variance minimization policy;forme canonique;ergodicity condition;temps continu;minimizacion;tiempo continuo;93e20;portfolios;minimization methods;optimal policy;satisfiability;mean variance;transition matrix;martingale technique;average variance minimization problem;policy iteration algorithm;optimal control;iterative methods;continuous time systems;variance minimization policy ams 2000 90c40 93e20 finite continuous time markov decision process ctmdp g condition mean variance policy iteration algorithm;commande stochastique;processus markov;commande optimale;markov process;g condition;statistics;mean variance criteria;forma canonica;ergodicidad;councils;stochastic control;control estocastico;terminology;policy iteration;markov processes;markov decision process;ergodicity;ams 2000 90c40;finite continuous time markov decision process ctmdp;markov processes continuous time systems iterative methods;politique optimale;unichain condition;policy iteration algorithm mean variance criteria finite continuous time markov decision process average variance minimization problem g condition unichain condition ergodicity condition equivalent mean average criterion martingale technique canonical form transition rate matrix;matrice transition;minimization methods portfolios terminology councils mathematics statistics	This technical note deals with the mean variance problem (known as the average variance (AV) minimization problem) for finite continuous time Markov decision processes. We first introduce a so called G-condition which is weaker than the well known ergodicity and unichain conditions and sufficient for the finiteness of the AV of a policy. Also, we present an example of a policy having infinite AV when the G-condition is not satisfied. Under the G-condition we prove that the AV criterion can be transformed into an equivalent mean (or expected) average criterion by using a martingale technique and an observation from the canonical form of a transition rate matrix, and thus the existence and calculation of an AV minimal policy over a class of mean optimal policies are obtained by a policy iteration algorithm in an finite number of iterations. As byproduct, we obtain some interesting new results about the mean average optimality.	algorithm;ergodicity;iteration;iterative method;markov chain;markov decision process;mean squared error;whole earth 'lectronic link	Xianping Guo;XinYuan Song	2009	IEEE Transactions on Automatic Control	10.1109/TAC.2009.2023833	mathematical optimization;discrete mathematics;mathematics;markov process;statistics	ML	38.302429134757745	5.530419921656014	109548
580afb733a0aa5183a69323ba7535e33a67676c1	learning mixtures of structured distributions over discrete domains	algorithms;design;general;theory;computations on discrete structures	Let C be a class of probability distributions over the discrete domain [n] = {1, . . . , n}. We show that if C satisfies a rather general condition – essentially, that each distribution in C can be well-approximated by a variable-width histogram with few bins – then there is a highly efficient (both in terms of running time and sample complexity) algorithm that can learn any mixture of k unknown distributions from C. We analyze several natural types of distributions over [n], including log-concave, monotone hazard rate and unimodal distributions, and show that they have the required structural property of being wellapproximated by a histogram with few bins. Applying our general algorithm, we obtain near-optimally efficient algorithms for all these mixture learning problems as described below. More precisely, • Log-concave distributions: We learn any mixture of k log-concave distributions over [n] using k · Õ(1/ε) samples (independent of n) and running in time Õ(k log(n)/ε) bit-operations (note that reading a single sample from [n] takes Θ(log n) bit operations). For the special case k = 1 we give an efficient algorithm using Õ(1/ε) samples; this generalizes the main result of [DDS12b] from the class of Poisson Binomial Distributions to the much broader class of all log-concave distributions. Our upper bounds are not far from optimal since any algorithm for this learning problem requires Ω(k/ε) samples. • Monotone hazard rate (MHR) distributions: We learn any mixture of k MHR distributions over [n] using O(k log(n/ε)/ε) samples and running in time Õ(k log(n)/ε4) bit-operations. Any algorithm for this learning problem must use Ω(k log(n)/ε) samples. • Unimodal distributions: We give an algorithm that learns any mixture of k unimodal distributions over [n] using O(k log(n)/ε) samples and running in time Õ(k log(n)/ε4) bit-operations. Any algorithm for this problem must use Ω(k log(n)/ε) samples. ∗siuon@cs.berkeley.edu. †ilias@cs.berkeley.edu. Research supported by a Simons Postdoctoral Fellowship. ‡rocco@cs.columbia.edu. Supported by NSF grants CCF-0915929 and CCF-1115703. §xiaoruisun@cs.columbia.edu.	approximation algorithm;concave function;ibm notes;keneth alden simons;sample complexity;time complexity;monotone	Siu-on Chan;Ilias Diakonikolas;Rocco A. Servedio;Xiaorui Sun	2013		10.1137/1.9781611973105.100	combinatorics;discrete mathematics;mathematics;statistics	Theory	34.80302412673392	8.066518011456823	110139
3453940ad1dc372b67a83a14cce3e2ffb58b3d4f	optimal combination of nested clusters by a greedy approximation algorithm	optimal solution;approximation algorithms clustering algorithms greedy algorithms machine learning algorithms information retrieval information analysis pattern analysis pattern recognition data mining machine learning;machine learning algorithms;optimisation approximation theory computational complexity greedy algorithms;complejidad espacio;evaluation performance;optimisation;global solution;solution optimale;analyse amas;approximate algorithm;performance evaluation;optimizacion;time complexity;algorithme glouton;greedy approximation algorithm;approximation algorithms;information retrieval;approximation algorithm;evaluacion prestacion;recubrimiento;overlay;problema np duro;greedy algorithms;data mining;classification;recouvrement;theorem proving;optimization problem;approximation theory;demonstration theoreme;np hard problem;induccion;complexite temps;cluster analysis;induction;machine learning;evaluation measure;probleme np difficile;mathematical programming;computational complexity;clustering;solucion optima;space complexity nested clusters greedy approximation algorithm optimization problem np hard problem global optimal solution;global optimal solution;algoritmo aproximacion;space complexity;pattern recognition;clustering algorithms;greedy algorithm;algoritmo gloton;global optimization;analisis cluster;optimization;optimization clustering classification performance evaluation;pattern analysis;solution globale;complexite espace;demostracion teorema;algorithme approximation;complejidad tiempo;programmation mathematique;information analysis;algorithms artificial intelligence computer simulation decision support techniques models theoretical pattern recognition automated;solucion global;programacion matematica;clasificacion;nested clusters	Given a set of clusters, we consider an optimization problem which seeks a subset of clusters that maximizes the microaverage F-measure. This optimal value can be used as an evaluation measure of the goodness of clustering. For arbitrarily overlapping clusters, finding the optimal value is NP-hard. We claim that a greedy approximation algorithm yields the global optimal solution for clusters that overlap only by nesting. We present a mathematical proof of this claim by induction. For a family of n clusters containing a total of N objects, this algorithm has an O(n2) time complexity and O(N) space complexity.	approximation algorithm;best, worst and average case;cluster analysis;cost effectiveness;dspace;greedy algorithm;heuristic (computer science);heuristics;mathematical induction;mathematical optimization;mathematics;np-hardness;optimization problem;physical object;subgroup;time complexity;statistical cluster;subclass	Edward K. F. Dang;Robert Wing Pong Luk;Dik Lun Lee;Edward Kei Shiu Ho;Stephen Chi-fai Chan	2009	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2009.75	mathematical optimization;greedy algorithm;computer science;machine learning;mathematics;cluster analysis;approximation algorithm;algorithm	Theory	26.984082726017697	14.937519092573392	110203
f23b932e7d43e396b3fb827ccf47355c8d520be1	approximate map labeling is in omega (n log n)	complexite calcul;geometrie algorithmique;probleme np complet;computational geometry;cartographie;algorithme;algorithm;temps calcul;complejidad computacion;cartografia;computational complexity;interpolation method;map labeling;borne inferieure;cartography;geometria computacional;problema np completo;tiempo computacion;computation time;lower bound;np complete problem;cota inferior;algoritmo	Given n real numbers, the e-CLOSENESS problem consists in deciding whether any two of them are within E of each other, where E is a fixed parameter of the problem. This is a basic problems with an R (n log n) lower bound on the running time in the algebraic computation tree model. In this paper we show that for a natural approximation version thereof the same lower bound holds. The main tool used is a lower bound theorem of Ben-Or. We introduce a new interpolation method relating the approximation version of the problem to two corresponding exact versions. Using this result, we are able to prove the optimality of the running time of a cartographic algorithm, that determines an approximate solution to the so-called MAP LABELING problem. MAP LABELING was shown to be NP-complete. The approximation algorithm discussed here is of provably optimal approximation quality. Our result is the first n log n lower bound for an approximate problem. The proof method is general enough to be potentially helpful for further results of this type.	approximation algorithm;cartography;computation tree;interpolation;linear algebra;np-completeness;omega;symbolic computation;time complexity	Frank Wagner	1994	Inf. Process. Lett.	10.1016/0020-0190(94)90001-9	combinatorics;np-complete;computational geometry;computer science;calculus;mathematics;upper and lower bounds;computational complexity theory;algorithm	Theory	29.209479099931315	17.10903332092138	110360
11d1324316731fbac9b34d0d5fdf0c0a41086530	engineering the divide-and-conquer closest pair algorithm	time complexity;computational geometry;euclidean distance;algorithmic engineering;circle packing;analysis of algorithms;closest pair;comparative study;divide and conquer	We improve the famous divide-and-conquer algorithm by Bentley and Shamos for the planar closest-pair problem. For n points on the plane, our algorithm keeps the optimal O(n log n) time complexity and, using a circle-packing property, computes at most 7n/2 Euclidean distances, which improves Ge et al.’s bound of (3n log n)/2 Euclidean distances. We present experimental results of our comparative studies on four different versions of the divide-and-conquer closest pair algorithm and propose two effective heuristics.	algorithm;closest pair of points problem;euclidean distance;heuristic (computer science);set packing;time complexity	Minghui Jiang;Joel Gillespie	2007	Journal of Computer Science and Technology	10.1007/s11390-007-9066-y	time complexity;mathematical optimization;divide and conquer algorithms;circle packing;closest pair of points problem;computational geometry;computer science;analysis of algorithms;comparative research;euclidean distance;euclidean distance matrix;algorithm	Theory	29.677558806348173	17.431887087840092	110409
beda2a672650c314fd3ff97e3512477a7e56b805	polyhedral analysis and branch-and-cut for the structural analysis problem	facet;communication conference;matching;branch and cut;structural analysis;bipartite graph;conditional differential algebraic system;polytope	In this paper we consider the structural analysis problem for differential-algebraic systems with conditional equations. This consists, given a conditional differential-algebraic system, in verifying if the system is structurally solvable for every state, and if not in finding a state in which the system is structurally singular. We give an integer linear programming formulation for the problem. We also identify some classes of valid inequalities and characterize when these inequalities define facets for the associated polytope. Moreover, we devise separation routines for these inequalities. Based on this, we develop a Branch-and-Cut algorithm along with experimental results are presented.	algorithm;branch and cut;decision problem;integer programming;linear algebra;linear programming formulation;polyhedral;structural analysis;verification and validation	Mathieu Lacroix;Ali Ridha Mahjoub;Sébastien Martin	2012		10.1007/978-3-642-32147-4_12	mathematical optimization;combinatorics;discrete mathematics;mathematics;vertex enumeration problem	Logic	25.300636694501797	13.150471703978454	110513
1fcf0768229c54dce08453b06e2586b0c94965af	on simulated annealing in eda	computer aided design;stochastic algorithm;cost function;cad;simulated annealing algorithm;eda;vlsi design;simulated annealing;optimization problem;polynomial time;vlsi;electronic design automation	"""Simulated annealing was first introduced in 1983 as a generic stochastic algorithmic approach to solve optimization problems. Prof. C. L. Liu and his students H. W. Leong and D. F. Wong were among the earliest EDA researchers who applied simulated annealing to EDA. They solved a wide range of EDA problems with successes and reported their results in a series of papers at premier EDA conferences such DAC and ICCAD. Liu, Leong, and Wong later summarized their works in a research monograph entitled Simulated Annealing for VLSI Design, published in 1988 by Kluwer Academic Publishers. In the Preface of their book, the authors wrote """"We hope that our experiences with the techniques we employed, some of which indeed bear certain similarities for different problems, could be useful as hints and guides for other researchers in applying the method to the solutions of other problems"""". Indeed, there were """"similarities in techniques"""" among Liu's works that had influenced the design of simulated annealing EDA algorithms in the past 20 some years. To better understand Liu's contributions in simulated annealing, one should note that a typical simulated annealing algorithm uses a solution space and a cost function that come directly with the problem. Although computation time may be high, the algorithm is straightforward to design, making simulated annealing an attractive option for difficult problems where clever algorithms are hard to come by. On the contrary, Liu's simulated annealing algorithms have a completely different style: They resemble clever algorithms that attempt to solve difficult problems in polynomial time, and they often have a strong algorithmic flavor with a significant effort spent on optimizing the solution space and cost function. In this talk, we will discuss Prof. Liu's pioneering contributions in simulated annealing for EDA."""	algorithm;computation;digital-to-analog converter;electronic design automation;experience;feasible region;international conference on computer-aided design;loss function;mathematical optimization;simulated annealing;stochastic gradient descent;time complexity	Martin D. F. Wong	2012		10.1145/2160916.2160930	mathematical optimization;simulated annealing;electronic design automation;computer science;artificial intelligence;mathematics;very-large-scale integration;algorithm	Theory	28.639397529091788	7.866297467777556	110831
9c4d87dbf1b67c7bb0c07c4483ad9e580b4e010b	spanning trees from the commute times of random walks on graphs	analisis imagen;fonction green;random graph;acoplamiento grafo;image recognition;reconocimiento imagen;funcion discreta;random walks on graphs;image processing;funcion green;laplacian spectrum;grafo aleatorio;arbre maximal;laplacian;procesamiento imagen;graphe aleatoire;spectrum;graph matching;traitement image;laplacien;couplage graphe;discrete function;laplaciano;fonction discrete;arbol maximo;random walk;reconnaissance image;minimum spanning tree;heat kernel;image analysis;spanning tree;marcha aleatoria;graph laplacian;analyse image;marche aleatoire;green function;matching method	This paper exploits the properties of the commute time for the purposes of graph matching. Our starting point is the lazy random walk on the graph, which is determined by the heat-kernel of the graph and can be computed from the spectrum of the graph Laplacian. We characterise the random walk using the commute time between nodes, and show how this quantity may be computed from the Laplacian spectrum using the discrete Green’s function. We use the commute-time to locate the minimum spanning tree of the graph. The spanning trees located using commute time prove to be stable to structural variations. We match the graphs by applying a tree-matching method to the spanning trees. We experiment with the method on synthetic and real-world image data, where it proves to to be effective.	file spanning	Huaijun Qiu;Edwin R. Hancock	2006		10.1007/11867661_34	random regular graph;random graph;spectrum;computer vision;combinatorics;geometric graph theory;laplace operator;discrete mathematics;image analysis;topology;graph bandwidth;laplacian matrix;minimum degree spanning tree;spanning tree;image processing;minimum spanning tree;comparability graph;loop-erased random walk;graph factorization;connected dominating set;mathematics;voltage graph;distance-hereditary graph;trémaux tree;heat kernel;tutte polynomial;green's function;reverse-delete algorithm;random geometric graph;random walk;tree;line graph;matching	Theory	38.66240802262228	17.63312287388321	110892
14906081db259b5bd993cfa0a4f2254e99076a4b	the general steiner problem in rectangular crisscross space	crisscross space;probleme extremum;espacio 2 dimensiones;probleme steiner;espace entrecroisement;two dimensional space;extremum problem;distancia;espace 2 dimensions;point minimum;structure rectangulaire;distance;problema extremo	Abstract   The crisscross space is rectangularly structured. Start from a Cartesian coordinate system with distance  D  of points defined as  D ( P  1 , P  2 )=| x  2 − x  1 |+| y  2 − y  1 |, where  P   1 =( x  1 , y  1 ) and  P  2 ( x  2 , y  2 ) are points in the real plane. The general Steiner problem is to find the minimum point  P  of   Φ(P)=  Σ  i=1  n  c     i   D(P     i   ,p)  , where  p   i  ,  i =1,…, n , are given points and  c    i   is the weight of  P   i  . The paper concludes that (1) the 2-dimensional (as well as the  n -dimensional) problem can be reduced to a pair of 1-dimensional problems: (2) the solution of the 1-dimensional problem is determined by the weights only, viz. if there is an integer  k  (1⩽ k ⩽ n −1) such that   Σ  i=1  k   c     i   =  Σ  i=k+1  n   c     i    all points on the interval [ x   k  , x   k +1 ] are minimum points. In degenerate cases, i.e. when   Σ  i=1  k   c     i   =  Σ  i=k  n   c     i    or when neither of the above kinds of  k  exist, the solution reduces to a single point. In the 2-dimensional case the solution space is a rectangular region with [ x   k  , x   k +1 ] and [ y   j  , y   j +1 ] as sides. In a degenerate case the solution space reduces to a segment or a point.	steiner tree problem	Shou-Tian Ting;Shu-Yu Zhao	1993	Discrete Mathematics	10.1016/0012-365X(93)90011-H	combinatorics;two-dimensional space;topology;mathematics;geometry;distance	Theory	30.06241358729358	17.081954801238567	110969
04a56dc2d5a5013390d3c3963fe9382f99003022	mdl summarization with holes	quadratic program;dynamic programming algorithm;dynamic program;greedy algorithm;minimum description length principle;experimental evaluation	Summarization of query results is an important problem for many OLAP applications. The Minimum Description Length principle has been applied in various studies to provide summaries. In this paper, we consider a new approach of applying the MDL principle. We study the problem of finding summaries of the form S H for k-d cubes with tree hierarchies. The S part generalizes the query results, while the H part describes all the exceptions to the generalizations. The optimization problem is to minimize the combined cardinalities of S and H . We first characterize the problem by showing that solving the 1-d problem can be done in time linear to the size of hierarchy, but solving the 2-d problem is NPhard. We then develop three different heuristics, based on a greedy approach, a dynamic programming approach and a quadratic programming approach. We conduct a comprehensive experimental evaluation. Both the dynamic programming algorithm and the greedy algorithm can be used for different circumstances. Both produce summaries that are significantly shorter than those generated by state-of-the-art alternatives.	automatic summarization;cardinality (data modeling);computation;display resolution;dynamic programming;electron hole;greedy algorithm;heuristic (computer science);mdl (programming language);mathematical optimization;minimum description length;np-hardness;olap cube;online analytical processing;optimization problem;quadratic programming;scalability	Shaofeng Bu;Laks V. S. Lakshmanan;Raymond T. Ng	2005			mathematical optimization;greedy algorithm;computer science;machine learning;dynamic programming;database;quadratic programming	DB	25.339810241707408	8.93801843652149	110987
678f3f556b1cca5ec0aa362d558968bae44198c5	computing efficient exact designs of experiments using integer quadratic programming	d optimal design;d;d q optimal design;cost constraints;dq optimal design;marginal constraints;exact design;dq;integer quadratic programming	We propose a method of computing exact experimental designs by integer quadratic programming. The key idea is a suitable quadratic approximation of the criterion of D-optimality in the neighbourhood of the approximate D-optimal information matrix, which we call the criterion of Q-optimality. We demonstrate on several examples that the D-efficiency of the exact Q-optimal designs is usually very high. An important advantage of the method is that it can be applied to situations with marginal and cost constraints on the design.	approximation algorithm;design of experiments;experiment;marginal model;optimality criterion;quadratic programming	Radoslav Harman;Lenka Filová	2014	Computational Statistics & Data Analysis	10.1016/j.csda.2013.02.021	mathematical optimization;combinatorics;discrete mathematics;optimal design;mathematics;statistics	ML	35.464774638954765	6.347111789727705	111520
679703bb217b5edef12017354ed1dc68476a7ce6	maximum likelihood trajectories for continuous-time markov chains		Continuous-time Markov chains are used to model systems in which transitions between states as well as the time the system spends in each state are random. Many computational problems related to such chains have been solved, including determining state distributions as a function of time, parameter estimation, and control. However, the problem of inferring most likely trajectories, where a trajectory is a sequence of states as well as the amount of time spent in each state, appears unsolved. We study three versions of this problem: (i) an initial value problem, in which an initial state is given and we seek the most likely trajectory until a given final time, (ii) a boundary value problem, in which initial and final states and times are given, and we seek the most likely trajectory connecting them, and (iii) trajectory inference under partial observability, analogous to finding maximum likelihood trajectories for hidden Markov models. We show that maximum likelihood trajectories are not always well-defined, and describe a polynomial time test for well-definedness. When well-definedness holds, we show that each of the three problems can be solved in polynomial time, and we develop efficient dynamic programming algorithms for doing so.	algorithm;computation;computational problem;dynamic programming;estimation theory;hidden markov model;markov chain;polynomial;time complexity	Theodore J. Perkins	2009			mathematical optimization;combinatorics;mathematics;statistics	ML	38.110767696416666	5.940567678943984	111644
b39a4d158ecbebc3d5db7d9e5165e329f2282add	on the behavior of tile assembly system at high temperatures	optimizing temperature;specific form;minimum size tas;tile assembly system;integer programming;threshold programming;important result;boolean satisfiability problem;high temperature	"""Behaviors of Winfree's tile assembly systems (TASs) at high temperatures are investigated in combination with integer programming of a specific form called threshold programming. First, we propose a way to build bridges from the Boolean satisfiability problem ($\ensuremath{\mbox{\rm S{\scriptsize AT}}}$) to threshold programming, and further to TAS's behavior, in order to prove the NP-hardness of optimizing temperatures of TASs that behave in a way given as input. These bridges will take us further to two important results on the behavior of TASs at high temperatures. The first says that arbitrarily high temperatures are required to assemble some shape by a TAS of """"reasonable"""" size. The second is that for any temperature τ≥4 given as a parameter, it is NP-hard to find the minimum size TAS that self-assembles a given shape and works at a temperature below τ."""		Shinnosuke Seki;Yasushi Okuno	2012		10.1007/978-3-642-30870-3_55	real-time computing;algorithm	Robotics	35.50705888722914	17.295579486216813	111657
654993ec23d1069a843e2dfd2c72892e1a2521a1	the multilinear polytope for acyclic hypergraphs		We consider the Multilinear polytope defined as the convex hull of the set of binary points z satisfying a collection of equations of the form ze = ∏ v∈e zv, e ∈ E, where E denotes a family of subsets of {1, . . . , n} of cardinality at least two. Such sets are of fundamental importance in many types of mixedinteger nonlinear optimization problems, such as 0−1 polynomial optimization. Utilizing an equivalent hypergraph representation, we study the facial structure of the Multilinear polytope in conjunction with the acyclicity degree of the underlying hypergraph. We provide explicit characterizations of the Multilinear polytopes corresponding to Berge-acylic and γ-acyclic hypergraphs. As the Multilinear polytope for γ-acyclic hypergraphs may contain exponentially many facets in general, we present a strongly polynomial-time algorithm to solve the separation problem, implying polynomial solvability of the corresponding class of 0−1 polynomial optimization problems. As an important byproduct, we present a new class of cutting planes for constructing tighter polyhedral relaxations of mixed-integer nonlinear optimization problems with multilinear sub-expressions.	algorithm;berge's lemma;convex hull;directed acyclic graph;mathematical optimization;nonlinear programming;nonlinear system;polyhedron;polynomial;regular expression;time complexity	Alberto Del Pia;Aida Khajavirad	2018	SIAM Journal on Optimization	10.1137/16M1095998	combinatorics;cardinality;hypergraph;multilinear map;discrete mathematics;convex hull;polytope;mathematics;nonlinear programming;family of sets;polynomial	Theory	25.244735334399127	13.773938101081509	111952
d2577c616014c23ccae814e941d18c3796db30de	pivoting in an outcome polyhedron	neighborhood problem;nonlinear programming;nonconvex optimization;pivoting;simplex method;mathematical programming;extreme point;linear programming;linear program;outcome polyhedron;global optimization;extreme point mathematical programming	In many types of linear, convex and nonconvex optimization problems over polyhedra, a global optimal solution can be found by searching the extreme points of the outcome polyhedron Y instead of the extreme points of the decision set polyhedron Z. Since the dimension of Y is often significantly smaller than the dimension of Z, and since the structure of Y is often much simpler than the structure of Z, such an approach has the potential to often yield significant computational savings. This article seeks to motivate these potential savings through both general theory and concrete examples. The article then develops two new procedures. The first procedure is linear-programming based and finds an initial extreme point of an outcome polyhedron Y. The second procedure provides a mechanism for moving from a given extreme point y of Y along any chosen edge of Y emanating from y until a neighboring extreme point to y is reached. As a by-product of the second procedure, as in the pivoting process of the simplex method, a complete algebraic description of the chosen edge can also be easily obtained.	polyhedron	Harold P. Benson;Erjiang Sun	2000	J. Global Optimization	10.1023/A:1008364005245	reduced cost;extreme point;mathematical optimization;combinatorics;linear programming;mathematics;simplex algorithm;algorithm	Theory	27.79755566838601	10.919055163636724	112289
6133ae8e7388de4242134f2145e037b1e30aeaff	q-learning for risk-sensitive control	dynamic programming;reinforcement learning;q learning;stochastic approximation;risk sensitive control;markov decision processes	We propose for risk-sensitive control of finite Markov chains a counterpart of the popular Q-learning algorithm for classical Markov decision processes. The algorithm is shown to converge with probability one to the desired solution. The proof technique is an adaptation of the o.d.e. approach for the analysis of stochastic approximation algorithms, with most of the work involved used for the analysis of the specific o.d.e.s that arise.	q-learning	Vivek S. Borkar	2002	Math. Oper. Res.	10.1287/moor.27.2.294.324	markov decision process;stochastic approximation;time reversibility;markov chain;mathematical optimization;markov kernel;partially observable markov decision process;markov property;computer science;continuous-time markov chain;artificial intelligence;machine learning;dynamic programming;markov blanket;markov renewal process;additive markov chain;markov algorithm;markov process;markov model;reinforcement learning;q-learning;hidden markov model;variable-order markov model	Theory	38.48884421274228	4.719109259632718	112696
f8e673805f9ddbb68d48d965d6fdd994f07099e2	volume decomposition and feature recognition: part 1 - polyhedral objects	calcul matriciel;object recognition;computer aided design;intersecting features;polyedre;intersections;image processing;feature recognition;eliminacion;geometrie algorithmique;poliedro;metodo descomposicion;methode decomposition;computational geometry;polyhedron;reconnaissance objet;traitement image;decomposition method;pattern recognition;conception assistee;matrix calculus;reconnaissance forme;elimination;process planning;intersection;calculo de matrices;cell decomposition	A method has been developed that decomposes a polyhedron into maximal cells by intersecting it with half spaces of its faces having concave edges. Every set of such half spaces that would result in maximal convex cells is found very efficiently without actual intersection operations by examining the relationships among the half spaces and the neighbourhoods of concave edges. One application of this decomposition method is recognition of intersecting features for process planning. Recognition of intersecting features has been a major difficulty in automating process planning. With this decomposition method, a delta volume is decomposed into maximal convex cells. By subtracting maximal convex cells from each other in different orders, multiple interpretations of features are generated. Since the number of interpretations, which may reach N! for N maximal convex cells, can be very large and thus selecting one of them for machining may be difficult, we tried to generate a machining sequence directly from maximal convex cells of a delta volume using a small number of heuristics on machining. The result was the same machining sequence as the one suggested by a machinist.	feature recognition;polyhedron	Hiroshi Sakurai	1995	Computer-Aided Design	10.1016/0010-4485(95)00007-0	feature recognition;decomposition method;image processing;matrix calculus;computational geometry;computer aided design;cognitive neuroscience of visual object recognition;intersection;mathematics;geometry;engineering drawing;algorithm;elimination;polyhedron	EDA	31.918030142595022	16.132172678963894	112768
c53f219f1f3bb38c2308e68892f69807fef6c178	combinatorial optimization and hierarchical classifications	cluster algorithm;optimization problem;hierarchical classification;polynomial time;combinatorial optimization;problem solving	This paper is devoted to some selected topics relating Combinatorial Optimization and Hierarchical Classification. It is oriented toward extensions of the standard classification schemes (the hierarchies): pyramids, quasi-hierarchies, circular clustering, rigid clustering and others. Bijection theorems between these models and dissimilarity models allow to state some clustering problems as optimization problems. Within the galaxy of optimization we have especially discussed the following: NP-completeness results and search for polynomial instances; problems solved in a polynomial time (e.g. subdominant theory); design, analysis and applications of algorithms. In contrast with the orientation to “new” clustering problems, the last part discusses some standard algorithmic approaches.	algorithm;cluster analysis;combinatorial optimization;computational complexity theory;mathematical optimization;np-completeness;optimization problem;p (complexity);polynomial;pyramid (geometry);time complexity	Jean-Pierre Barthélemy;François Brucker;Christophe Osswald	2004	4OR	10.1007/s10288-004-0051-9	time complexity;discrete optimization;optimization problem;extremal optimization;mathematical optimization;multi-swarm optimization;combinatorics;cross-entropy method;combinatorial optimization;computer science;generalized assignment problem;combinatorial explosion;machine learning;mathematics;assignment problem;continuous optimization;weapon target assignment problem;vector optimization;3-opt;metaheuristic;global optimization;quadratic assignment problem;minimum k-cut	Theory	24.940404670923854	5.306864557840813	112924
5e3ea0529cf5739908683ab96483e7ec91d5eeb7	traveling salesman problem: the human case	traveling salesman problem;human cognition;path planning;cognitive process;traveling salesman	In the field of human cognition, performance and optimization behavior in the TSP has mainly been investigated by means of visual versions in which humans are confronted with a number of dots on a computer monitor. Their task is to connect these dots by a straight line such that the resulting path is optimal with respect to overall length. Path planning tasks similar to the TSP are quite common also in everyday navigation, for example, in shopping routes. In this paper, we systematically disentangle the cognitive processes and the range of external factors that influence problem solving in tasks that resemble the classical TSP, covering the area so as to include everyday human navigation tasks. We identify those areas for which human heuristics and strategies are already known and work out hypotheses concerning the generalizability of results gained within particular subfields of the area.	cognition;computer monitor;heuristic (computer science);mathematical optimization;motion planning;problem solving;travelling salesman problem	Jan Malte Wiener;Thora Tenbrink	2008	KI		traveling purchaser problem;motion planning;combinatorial optimization;2-opt;travelling salesman problem;bottleneck traveling salesman problem;heuristics;mathematical optimization;computer science;cognition	AI	28.57439050109948	4.802437830377246	113002
0ed17be065f1fadcec11610c336d812756f2382b	self-improving algorithms for convex hulls	algorithm analysis;shortest vector problem;sieving algorithms;cryptography;probability distribution;software implementations;convex hull	We describe an algorithm for computing planar convex hulls in the self-improving model: given a sequence <i>I</i><sub>1</sub>, <i>I</i><sub>2</sub>,... of planar <i>n</i>-point sets, the upper convex hull conv (<i>I</i>) of each set <i>I</i> is desired. We assume that there exists a probability distribution <i>D</i> on <i>n</i>-point sets, such that the inputs <i>I</i><sub><i>j</i></sub> are drawn independently according to <i>D</i>. Furthermore, <i>D</i> is such that the individual points are distributed independently of each other. In other words, the <i>i</i>'th point is distributed according to <i>D</i><sub><i>i</i></sub>. The <i>D</i><sub><i>i</i></sub>'s can be arbitrary but are independent of each other. The distribution <i>D</i> is not known to the algorithm in advance. After a learning phase of <i>n</i>ε rounds, the expected time to compute conv(<i>I</i>) is <i>O</i>(<i>n</i> + <i>H</i>(conv(<i>I</i>))). Here, <i>H</i>(conv(<i>I</i>)) is the entropy of the output, which is a lower bound for the expected running time of <i>any</i> algebraic computation tree that computes the convex hull. (More precisely, <i>H</i>(conv(<i>I</i>)) is the minimum entropy of any random variable that maps <i>I</i> to a description of conv(<i>I</i>) and to a labeling scheme that proves nonextremality for every point in <i>I</i> not on the hull.) Our algorithm is thus asymptotically optimal for <i>D</i>.	asymptotically optimal algorithm;average-case complexity;computation tree;convex hull;linear algebra;planar (computer graphics);symbolic computation;time complexity	Kenneth L. Clarkson;Wolfgang Mulzer;Seshadhri Comandur	2010		10.1137/1.9781611973075.126	probability distribution;mathematical optimization;combinatorics;discrete mathematics;cryptography;convex hull;mathematics;geometry;statistics	Theory	28.55748564505896	17.774634305575894	113371
121651249eef1e46f21120917bffbc4dc420be80	brief announcement: pan and scan	directional sensors;targets;approximate algorithm;dynamic programming algorithm;field of view;coverage	We introduce the pan and scan problem, in which cameras are configured to observe multiple target locations. A camera's configuration consists of its orientation and its zoom factor or field or view (its position is given); the quality of a target's reading by a camera depends (inversely) on both the distance and field of view.  After briefly discussing an easy setting in which a target accumulates measurement quality from all cameras observing it, we move on to a more challenging setting in which for each target only the best measurement of it is counted, for which we give various results. Although both variants admit continuous solutions, we observe that we may restrict our attention to solutions based on pinned cones.  For a geometrically constrained setting, we give an optimal dynamic programming algorithm. For the unconstrained setting of this problem, we prove NP-hardness, present efficient centralized and distributed 2-approximation algorithms, and observe that a PTAS exists under certain assumptions.  For a synchronized distributed setting, we give a 2-approximation protocol and a (2β)/(1-α)-approximation protocol (for all 0 ≤ α ≤ 1 and β ≥ 1) with the stability feature that no target's camera assignment changes more than logβ(m/α) times. We also discuss the running times of the algorithms and study the speed-ups that are possible in certain situations.	algorithm;centralized computing;dynamic programming;np-hardness;ptas reduction	Matthew P. Johnson;Amotz Bar-Noy	2010		10.1145/1835698.1835729	computer vision;mathematical optimization;simulation;field of view;dynamic programming;mathematics;distributed computing	ECom	31.044463232755202	16.834308631586687	113423
9126bac02c283ec15cdceae8ea0630a08bc799b3	local limit theorems for the giant component of random hypergraphs	giant component;local limit theorem;probabilistic approach	Let  H   d  ( n , p ) signify a random  d -uniform hypergraph with  n vertices in which each of the ${n}\choose{d}$ possible edges is present with probability  p =  p ( n ) independently, and let  H   d  ( n , m ) denote a uniformly distributed  d -uniform hypergraph with  n vertices and  m edges. We establish a  local limit theorem for the number of vertices and edges in the largest component of  H   d  ( n , p ) in the regime      , thereby determining the joint distribution of these parameters precisely. As an application, we derive an asymptotic formula for the probability that  H   d  ( n , m ) is connected, thus obtaining a formula for the asymptotic number of connected hypergraphs with a given number of vertices and edges. While most prior work on this subject relies on techniques from enumerative combinatorics, we present a new, purely probabilistic approach.	giant component	Michael Behrisch;Amin Coja-Oghlan;Mihyun Kang	2007		10.1007/978-3-540-74208-1_25	combinatorics;discrete mathematics;topology;mathematics	Theory	38.987711668534246	15.866119978676364	113441
2c9a2135b852c124ea201b737915c122abe5fbc1	on mathematical programming with indicator constraints	primary 90c11;bigm method;on off constraints;disjunctive programming;perspective reformulation;90c57;secondary 90c25	In this paper we review the relevant literature on mathematical optimization with logical implications, i.e., where constraints can be either active or disabled depending on logical conditions to hold. In the case of convex functions, the theory of disjunctive programming allows one to formulate these logical implications as convex nonlinear programming problems in a space of variables lifted with respect to its original dimension.Weconcentrate on the attempt of avoiding the issue of dealingwith large NLPs. In particular, we review some existing results that allow to work in the original space of variables for two relevant special cases where the disjunctions corresponding to the logical implications have two terms. Then, we significantly extend these special cases in two different directions, one involving more general convex sets and the other with disjunctions involving three terms. Computational experiments comparing disjunctive programming formulations in the original space of variables with straightforward bigM ones show that the former are computationally viable and promising.	algorithm;branch and bound;computation;convex function;convex set;disjunctive normal form;experiment;mathematical optimization;nonlinear programming;nonlinear system;routing;scheduling (computing)	Pierre Bonami;Andrea Lodi;Andrea Tramontani;Sven Wiese	2015	Math. Program.	10.1007/s10107-015-0891-4	mathematical optimization;nonlinear programming;mathematics;algorithm	AI	25.70279486524282	10.950373282072174	113660
0dc3f0280a55a064105407049fdc61938b3e376b	the transformation technique for spatial objects revisited	allgemeine werke;000 informatik;data structure;informationswissenschaft	The transformation technique is one of the earliest approaches for storing a set of bounding boxes of arbitrary geometric objects such that insertion, deletion and proximity queries can be carried out with reasonable performance. _The basic idea is to transform the bounding boxes into points in higher dimensional space in order to apply data structures for points which are better understood and easier to handle. Even though the basic concept of the transformation idea at first glance seems fascinatingly simple and elegant, the majority of the data structure community regard this technique as less appropriate because some of its properties are considered harmful. Main contribution of this paper is to shed some new light on the transformation technique in a sense that some of these properties can be proven to be harmless while the harmful ones can be overcome by new methods. Furthermore, we demonstrate that new kinds of transformations which take other than pure location parameters into account, provide the transformation technique with new quality. *This work has been supported by the European Community, ESPRIT Project No. 6881 (AMUSING).		Bernd-Uwe Pagel;Hans-Werner Six;Heinrich Toben	1993		10.1007/3-540-56869-7_5	theoretical computer science;mathematics;algorithm	DB	34.363038875035485	14.657345836339982	113716
2b22adf8c29e4e1d39b55ef35ed80d4412f7445d	a message passing graph match algorithm based on a generative graphical model	markov random fields;approximate graph matching;loopy belief propagation	In This paper, we present a generative model to measure the graph similarity, assuming that an observed graph is generated from a template by a Markov random field. The potentials of this random process are characterized by two sets of parameters: the attribute expectations specified by the the template graph, and the variances that can be learned by a maximum likelihood estimator from a collection of samples. Once a sample graph is observed, a max-product loopy belief propagation algorithm is applied to approximate the most probable explanation of the template's vertices, mapped to the sample's vertices. As demonstrated by the experiments, compared with other algorithms, the proposed approach performed better for near isomorphic graphs in the typical graph alignment and information retrieval applications.	algorithm;graphical model;message passing	Gang Shen;Wei Li	2012		10.1007/978-3-642-35236-2_27	graph power;random graph;factor-critical graph;geometric graph theory;graph bandwidth;null graph;graph labeling;computer science;simplex graph;machine learning;pattern recognition;voltage graph;moral graph;butterfly graph;quartic graph;complement graph;line graph;string graph;strength of a graph;belief propagation	ML	38.774072191509234	15.077493925133084	113881
04069824473dce553c3ec803ad651f8b952c05a2	asymptotics of bernoulli random walks, bridges, excursions and meanders with a given number of peaks	bridge;weak convergence	A Bernoulli random walk is a random trajectory starting from 0 and having i.i.d. increments, each of them being +1 or −1, equally likely. The other families quoted in the title are Bernoulli random walks under various conditions. A peak in a trajectory is a local maximum. In this paper, we condition the families of trajectories to have a given number of peaks. We show that, asymptotically, the main effect of setting the number of peaks is to change the order of magnitude of the trajectories. The counting process of the peaks, that encodes the repartition of the peaks in the trajectories, is also studied. It is shown that suitably normalized, it converges to a Brownian bridge which is independent of the limiting trajectory. Applications in terms of plane trees and parallelogram polyominoes are provided, as well as an application to the “comparison” between runs and Kolmogorov-Smirnov statistics.	bernoulli polynomials;brownian motion;like button;maxima and minima	Jean-Maxime Labarbe;Jean-François Marckert	2006	CoRR		brownian bridge;bernoulli process;combinatorics;mathematical analysis;discrete mathematics;mathematics;weak convergence;random walk;bridge;statistics	Theory	38.56291290497485	16.819903496524763	114198
1aa9b8e5c2773be854cdb95845eca568838509d3	a quasi-newton approach to nonsmooth convex optimization problems in machine learning	line search;statistical machine learning;time complexity;quasi newton;bundle method;global convergence;convex optimization;objective function;machine learning;quasi newton method;risk minimization;direction finding;open source	We extend the well-known BFGS quasi-Newton method and its memory-limited variant LBFGS to the optimization of nonsmooth convex objectives. This is done in a rigorous fashion by generalizing three components of BFGS to subdifferentials: the local quadratic model, the identification of a descent direction, and the Wolfe line search conditions. We prove that under some technical conditions, the resulting subBFGS algorithm is globally convergent in objective function value. We apply its memory-limited variant (subLBFGS) to L2-regularized risk minimization with the binary hinge loss. To extend our algorithm to the multiclass and multilabel settings, we develop a new, efficient, exact line search algorithm. We prove its worst-case time complexity bounds, and show that our line search can also be used to extend a recently developed bundle method to the multiclass and multilabel settings. We also apply the direction-finding component of our algorithm to L1-regularized risk minimization with logistic loss. In all these contexts our methods perform comparable to or better than specialized state-of-the-art solvers on a number of publicly available data sets. An open source implementation of our algorithms is freely available.	best, worst and average case;convex optimization;descent direction;hinge loss;limited-memory bfgs;line search;loss functions for classification;machine learning;mathematical optimization;newton;newton's method;open-source software;optimization problem;quadratic equation;quasi-newton method;search algorithm;subgradient method;time complexity;whole earth 'lectronic link	Jin Yu;S. V. N. Vishwanathan;Simon Günter;Nicol N. Schraudolph	2010	Journal of Machine Learning Research	10.1145/1756006.1756045	time complexity;mathematical optimization;combinatorics;convex optimization;quasi-newton method;computer science;machine learning;mathematics;line search	ML	32.233971555194515	6.325395791292218	114255
230298404c395ff91264541a6b18de271c7014c2	algorithms on minimizing the maximum sensor movement for barrier coverage of a linear domain	barrier coverage;open question;specified segment;n sensor;polynomial time;time algorithm;linear domain;maximum sensor movement;previous best o;time solution	In this paper, we study the problem of moving n sensors on a line to form a barrier coverage of a specified segment of the line such that the maximum moving distance of the sensors is minimized. Previously, it was an open question whether this problem on sensors with arbitrary sensing ranges is solvable in polynomial time. We settle this open question positively by giving an O(n logn) time algorithm. For the special case when all sensors have the same-size sensing range, the previously best solution takes O(n) time. We present an O(n logn) time algorithm for this case; further, if all sensors are initially located on the coverage segment, our algorithm takes O(n) time. Also, we extend our techniques to the cycle version of the problem where the barrier coverage is for a simple cycle and the sensors are allowed to move only along the cycle. For sensors with the same-size sensing range, we solve the cycle version in O(n) time, improving the previously best O(n) time solution.	algorithm;cycle (graph theory);decision problem;polynomial;sensor;time complexity	Danny Ziyi Chen;Yan Gu;Jian Li;Haitao Wang	2012		10.1007/978-3-642-31155-0_16	mathematical optimization;simulation;telecommunications;computer science	Theory	30.893437752362587	17.20098853673481	115170
b1bda6ba77c034881b7484d54fc83e7b458ac56b	on slowly percolating sets of minimal size in bootstrap percolation	bootstrap percolation;maximum time;grid	Bootstrap percolation, one of the simplest cellular automata, can be seen as a model of the spread of infection. In r-neighbour bootstrap percolation on a graph G we assign a state, infected or healthy, to every vertex of G and then update these states in successive rounds, according to the following simple local update rule: infected vertices of G remain infected forever and a healthy vertex becomes infected if it has at least r already infected neighbours. We say that percolation occurs if eventually every vertex of G becomes infected. A well known and celebrated fact about the classical model of 2-neighbour bootstrap percolation on the n × n square grid is that the smallest size of an initially infected set which percolates in this process is n. In this paper we consider the problem of finding the maximum time a 2-neighbour bootstrap process on [n]2 with n initially infected vertices can take to eventually infect the entire vertex set. Answering a question posed by Bollobás we compute the exact value for this maximum showing that, for n > 4, it is equal to the integer nearest to (5n2 − 2n)/8. ∗Supported by CNPq and FUNCAP. the electronic journal of combinatorics 20(2) (2013), #P46 1	automata theory;bollobás–riordan polynomial;bootstrap percolation;cellular automaton;percolation theory;square tiling;vertex (geometry)	Fabrício Siqueira Benevides;Michal Przykucki	2013	Electr. J. Comb.		combinatorics;discrete mathematics;mathematics;grid;statistics	Theory	38.09023023476262	16.506677287055588	115362
5e0ed2ed60790c0aef987253d12874f07eb96491	maximum entropy summary trees	i 4 10 image representation hierarchical;i 2 8 problem solving control methods and search dynamic programming;g 2 2 graph theory trees;g 2 1 combinatorics combinatorial algorithms	Given a very large, node-weighted, rooted tree on, say, n nodes, if one has only enough space to display a knode summary of the tree, what is the most informative way to draw the tree? We define a type of weighted tree that we call a summary tree of the original tree that results from aggregating nodes of the original tree subject to certain constraints. We suggest that the best choice of which summary tree to use (among those with a fixed number of nodes) is the one that maximizes the information-theoretic entropy of a natural probability distribution associated with the summary tree, and we provide a (pseudopolynomial-time) dynamic-programming algorithm to compute this maximum entropy summary tree, when the weights are integral. The result is an automated way to summarize large trees and retain as much information about them as possible, while using (and displaying) only a fraction of the original node set. We illustrate the computation and use of maximum entropy summary trees on five real data sets whose weighted tree representations vary widely in structure. We also provide an additive approximation algorithm and a greedy heuristic that are faster than the optimal algorithm, and generalize to trees with real-valued weights.	akregator;approximation algorithm;computation;dynamic programming;greedy algorithm;heuristic;information theory;principle of maximum entropy;pseudo-polynomial time;utility functions on indivisible goods	Howard J. Karloff;Kenneth E. Shirley	2013	Comput. Graph. Forum	10.1111/cgf.12094	left-child right-sibling binary tree;segment tree;red–black tree;mathematical optimization;combinatorics;discrete mathematics;tree rotation;vantage-point tree;exponential tree;binary tree;range tree;2–3 tree;tree rearrangement;gomory–hu tree;incremental decision tree;k-ary tree;interval tree;mathematics;fractal tree index;tree structure;search tree;tree;tree traversal;algorithm	ML	27.596557523614884	15.965610274408109	115465
374b38ecbd0ee3f51a2b02145d68ee00a779a400	neighbor selection and hitting probability in small-world graphs	probability theory and statistics;navigable;mathematics;evolutionary model;network evolution;freenet;statistik;small world;natural sciences;navigation;ciencias basicas y experimentales;social networks;matematicas;statistics;model;grupo a;hitting probability	Small-world graphs, which combine randomized and structured elements, are seen as prevalent in nature. Jon Kleinberg showed that in some graphs of this type it is possible to route, or navigate, between vertices in few steps even with very little knowledge of the graph itself. In an attempt to understand how such graphs arrise we introduce a different criterion for graphs being navigable in this sense, relating the neighbor selection of a vertex with the hitting probability of routed walks. In several models starting from both discrete and continuous settings, this can be shown to lead to graphs with the desired properties. It also leads directly to an evolutionary model for the creation of similar graphs by the stepwise rewiring of the edges, and we conjecture, supported by simulations, that these too are navigable.	graph (discrete mathematics);internet;models of dna evolution;randomized algorithm;relevance;routing;simulation;stepwise regression	Oskar Sandberg	2007	CoRR	10.1214/07-AAP499	navigation;mathematics;operations research;statistics;social network	Theory	37.2708198246602	16.2178986550353	115608
1f0ffe9ca799b9e65774ea1828d74477ecdf2465	sensor network localization with imprecise distances	optimal solution;reseau information;reseau capteur;optimisation;anchor nodes;modele geometrique;sensors distance geometry;local algorithm;optimizacion;constrenimiento igualdad;computation theory;position transducteur;localization;localization distance geometry;localizacion;distance estimation;information network;capteur distance;journal article;sensor network;sensor network localization;optimization problem;equality constraint;captador medida;measurement sensor;red sensores;capteur mesure;localisation;sensor networks;distance geometry;sensor array;sensor nodes;constraint theory;sensor distancia;optimization;distance sensor;keywords algorithms;posicion transductor;transducer position;problem solving;red informacion;geometrical model;contrainte egalite;modelo geometrico	An approach to formulate geometric relations among distances between nodes as equality constraints is introduced in this paper to study the localization problem with imprecise distance information in sensor networks. These constraints can be further used to formulate optimization problems for distance estimation. The optimization solutions correspond to a set of distances that are consistent with the fact that sensor nodes live in the same plane or 3D space as the anchor nodes. These techniques serve as the foundation for most of the existing localization algorithms that depend on the sensors’ distances to anchors to compute each sensor’s location. © 2006 Elsevier B.V. All rights reserved.		Ming Cao;Brian D. O. Anderson;A. Stephen Morse	2006	Systems & Control Letters	10.1016/j.sysconle.2006.05.004	mathematical optimization;wireless sensor network;topology;theory of computation;mathematics;geometry	AI	33.84312704036101	10.204341206268062	115831
06421c0209d7de0838b6ab40749321ffa0768b5d	roads, codes and spatiotemporal queries	road network;hamming distance;planar graph	We present a novel coding-based technique for answering spatial and spatiotemporal queries on objects moving along a system of curves on the plane such as many road networks. We handle join, range, intercept, and other spatial and spatiotemporal queries under these assumptions, with distances being measured along the trajectories. Most work to date has studied the significantly simpler case of objects moving in straight lines on the plane. Our work is an advance toward solving the problem in its more general form.Central to our approach is an efficient coding technique, based on hypercube embedding, for assigning labels to nodes in the network. The Hamming distance between codes corresponds to the physical distance between nodes, so that we can determine shortest distances in the network extremely quickly. The coding method also efficiently captures many properties of the network relevant to spatial and spatiotemporal queries. Our approach also yields a very effective spatial hashing method for this domain. Our analytical results demonstrate that our methods are space- and time-efficient.We have studied the performance of our method for large planar graphs designed to represent road networks. Experiments show that our methods are efficient and practical.	code;hamming distance;planar graph;visual intercept	Sandeep Gupta;Swastik Kopparty;Chinya V. Ravishankar	2004		10.1145/1055558.1055576	combinatorics;discrete mathematics;hamming distance;computer science;theoretical computer science;mathematics;planar graph	DB	31.237268795537215	18.176293640555475	116503
79f669607ddcbe37367af023b8de74c5215b2030	an extension of dead end elimination for protein side-chain conformation using merge-decoupling	optimal solution;protein side chain conformation;algorithm;dead end elimination;reduction method;exhaustive search	A two-phase strategy is widely adopted to solve the side-chain conformation prediction (SCCP) problem. Phase one is a fast reduction phase removing large numbers of rotamers not existing in the GMEC. Phase two (optimization phase) uses heuristics or exhaustive search to find a good/optimal solution. Presently, DEE (Dead End Elimination) is the only deterministic reduction method for phase one. However, to achieve convergence in phase two using DEE, the strategy of forming super-residues is used. This quickly leads to a combinatorial explosion, and becomes inefficient In this paper, an improvement of the DEE process by forming super-residues efficiently is proposed for phase one. The method basically merges residues into pairs based on some merging criteria. Simple Goldstein is then applied until no more elimination is possible. A decoupling process then reforms the original residues sans removed rotamers and rotamer pairs. The process of merging and elimination is repeated until no more elimination is possible. Initial experiments have shown the method, called Merge-Decoupling DEE, can fix up to 25% of the unfixed residues coming out of Simple Goldstein DEE.	brute-force search;coupling (computer programming);dead-end elimination;experiment;heuristic (computer science);in-phase and quadrature components;mathematical optimization;sans institute;sparse conditional constant propagation;two-phase locking	Ket Fah Chong;Hon Wai Leong	2006		10.1145/1141277.1141320	mathematical optimization;computer science;brute-force search;algorithm	AI	28.6936071380748	8.483458071562843	116682
3312f526e782b97ae2db76c3309b61b9b351ad50	using weighted-sum functions to compute nonsupported efficient solutions in multiobjective combinatorial-{0, 1} problems	efficient solutions;approximate methods;multiobjective;exact methods;1 problems;combinatorial 0 1 problems;combinatorial 0	In multiobjective linear programming, the weighted-sum functions can be used to characterize the entire set of efficient solutions, but in multiobjective combinatorial-{0,1} problems these functions can only determine a small subset of efficient solutions, called supported efficient solutions. In this paper, we show how the entire set of efficient solutions can be found with the same technique by modifying the original problem. An algorithm is proposed. Some results are presented and the effect of some parameters of the proposed algorithm is illustrated with the multiobjective {0,1}-knapsack problem.		Carlos Gomes da Silva;João C. N. Clímaco	2013	International Journal of Information Technology and Decision Making	10.1142/S0219622013500028	mathematical optimization;combinatorics;discrete mathematics;mathematics	EDA	25.812278436123407	5.297057734355715	116718
71ee3a57edce34de0e5a916d2128ec4580491fff	computing the volume is difficult	operations research;technical report;industrial engineering	"""For every polynomial time algorithm which gives an upper bound vol(K) and a lower bound vol(K) for the volume of a convex set K C R d, t h e r a t i o vo-'-~(K)/vo_.._~(K) i s a t l e a s t (d/log d) d for some convex set K C R d"""""""	algorithm;convex set;p (complexity);polynomial	Imre Bárány;Zoltán Füredi	1986		10.1145/12130.12176	computer science;technical report	Theory	27.11079819960424	17.020438871908983	117191
6f78c345fcfece14f2bc59aa01b604c1f57ed2c5	relaxation-based coarsening for multilevel hypergraph partitioning		Multilevel partitioning methods that are inspired by principles of multiscaling are the most powerful practical hypergraph partitioning solvers. Hypergraph partitioning has many applications in disciplines ranging from scientific computing to data science. In this paper we introduce the concept of algebraic distance on hypergraphs and demonstrate its use as an algorithmic component in the coarsening stage of multilevel hypergraph partitioning solvers. The algebraic distance is a vertex distance measure that extends hyperedge weights for capturing the local connectivity of vertices which is critical for hypergraph coarsening schemes. The practical effectiveness of the proposed measure and corresponding coarsening scheme is demonstrated through extensive computational experiments on a diverse set of problems. Finally, we propose a benchmark of hypergraph partitioning problems to compare the quality of other solvers.	algorithm;benchmark (computing);binary space partitioning;computation;computational science;data science;degree distribution;experiment;graph partition;iterative method;linear algebra;linear programming relaxation;similarity measure;social network;vertex (geometry);vertex (graph theory)	Ruslan Shaydulin;Jie Chen;Ilya Safro	2017	CoRR		mathematical optimization;mathematics;theoretical computer science;hypergraph;algebraic number;vertex distance;vertex (geometry);constraint graph;ranging	HPC	26.063991128302202	6.702264325053186	117261
35a3a628c2b795f8c73ed2053c5948cd1edc2cae	inserting points uniformly at every instance	tecnologia electronica telecomunicaciones;procesamiento informacion;geometrie algorithmique;heuristic method;computational geometry;linear time algorithm;metodo heuristico;1 dimensional;circle packing;algorithme;algorithm;discrepancy;information processing;uniformity;geometria computacional;methode heuristique;tecnologias;grupo a;traitement information;local search;algoritmo	Arranging n points as uniformly as possible is a frequently occurring problem. It is equivalent to packing n equal and non-overlapping circles in a unit square. In this paper we generalize this problem in such a way that points are inserted one by one with uniformity preserved at every instance. Our criterion for uniformity is to minimize the gap ratio (which is the maximum gap over the minimum gap) at every point insertion. We present a linear time algorithm for finding an optimal n-point sequence with the maximum gap ratio bounded by 2 n/2 /( n/2 +1) in the 1-dimensional case. We describe how hard the same problem is for a point set in the plane and propose a local search heuristics for finding a good solution. key words: algorithm, circle packing, computational geometry, discrepancy, local search, uniformity	algorithm;circuit complexity;discrepancy function;polyhedron;time complexity	Sachio Teramoto;Tetsuo Asano;Naoki Katoh;Benjamin Doerr	2006	IEICE Transactions	10.1093/ietisy/e89-d.8.2348	combinatorics;circle packing;information processing;computational geometry;computer science;local search;one-dimensional space;mathematics;geometry;algorithm	Theory	29.21865379391769	17.314658861633244	117872
aa8c408477e4500fa2a79117aed828a1f0276f05	preferential attachment and vertex arrival times		We study preferential attachment mechanisms in random graphs that are parameterized by (i) a constant bias affecting the degreebiased distribution on the vertex set and (ii) the distribution of times at which new vertices are created by the model. The class of random graphs so defined admits a representation theorem reminiscent of residual allocation, or “stick-breaking” schemes. We characterize how the vertex arrival times affect the asymptotic degree distribution, and relate the latter to neutral-to-the-left processes. Our random graphs generate edges “one end at a time”, which sets up a one-toone correspondence between random graphs and random partitions of natural numbers; via this map, our representation induces a result on (not necessarily exchangeable) random partitions that generalizes a theorem of Griffiths and Spanó. A number of examples clarify how the class intersects with several known random graph models.	attachments;degree distribution;random graph	Benjamin Bloem-Reddy;Peter Orbanz	2017	CoRR		combinatorics;residual;mathematics;natural number;preferential attachment;vertex (geometry);representation theorem;parameterized complexity;degree distribution;random graph	Theory	37.9621435466867	16.78931069302789	118155
b8c0b7bcc292c0caedfa8f94152ef0e85f52db23	self-assembly of any shape with constant tile types using high temperature		Abstract Inspired by nature and motivated by a lack of top-down tools for precise nanoscale manufacture, self-assembly is a bottom-up process where simple, unorganized components autonomously combine to form larger more complex structures. Such systems hide rich algorithmic properties – notably, Turing universality – and a self-assembly system can be seen as both the object to be manufactured as well as the machine controlling the manufacturing process. Thus, a benchmark problem in self-assembly is the unique assembly of shapes: to design a set of simple agents which, based on aggregation rules and random movement, self-assemble into a particular shape and nothing else. We use a popular model of self-assembly, the 2-handed or hierarchical tile assembly model, and allow the existence of repulsive forces, which is a well-studied variant. The technique utilizes a finely-tuned temperature (the minimum required affinity required for aggregation of separate complexes). We show that calibrating the temperature and the strength of the aggregation between the tiles, one can encode the shape to be assembled without increasing the number of distinct tile types. Precisely, we show one tile set for which the following holds: for any finite connected shape S, there exists a setting of binding strengths between tiles and a temperature under which the system uniquely assembles S at some scale factor. Our tile system only uses one repulsive glue type and the system is growth-only (it produces no unstable assemblies). The best previous unique shape assembly results in tile assembly models use O( K(S) logK(S) ) distinct tile types, where K(S) is the Kolmogorov (descriptional) complexity of the shape S.	benchmark (computing);bottom-up proteomics;control theory;descriptive complexity theory;encode;processor affinity;self-assembly;tile-based video game;top-down and bottom-up design;turing;universality probability	Cameron T. Chalk;Austin Luchsinger;Robert T. Schweller;Tim Wylie	2018		10.4230/LIPIcs.ESA.2018.14	combinatorics;discrete mathematics;tile;self-assembly;computer science	Theory	35.628739857398756	17.334808036775456	118247
ebea61fdc72b5d520904968f2f0c05ab5a3c3e7f	an approximation approach to ergodic semi-markov control processes	rate of convergence;espace borel;proceso markov;transition probability;relacion convergencia;espace etat;taux convergence;convergence rate;satisfiability;optimality principle;principio optimalidad;key words semi markov control models;cout moyen;average cost;processus markov;estabilidad estocastica;state space;processus semi markovien;coste medio;stabilite stochastique;borel state space;markov process;borel space;proceso semi markoviano;geometric ergodicity;espacio estado;stochastic stability;principe optimalite;average cost optimality equation;semimarkovian process;processus controle semi markovien;espacio borel	We consider semi-Markov control models (SMCMs) with a Borel state space satisfying certain stochastic stability assumptions on the transition structure which imply the so-called V-uniform geometric ergodicity of the state process. We deal with a class of e-perturbations of transition probability functions of the original model. First, we determine the rate of convergence of the optimal expected costs in in perturbed models to the optimal expected cost in the orginal SMCM. Next, we present a new algorithm for finding the solution to the average cost optimality equation (ACOE). The algorithm makes use of a sequence of solutions to the ACOE for the perturbed models, which can be found by a simple iterative procedure.	approximation;control theory;ergodicity;markov chain;semiconductor industry	Anna Jaskiewicz	2001	Math. Meth. of OR	10.1007/s001860000079	mathematical optimization;combinatorics;calculus;mathematics;rate of convergence;statistics	ML	38.27352497758219	5.614133028567875	118333
6080f7f7dbde982d90535c3fc87fbcff28eae235	scrip: successive convex optimization methods for risk parity portfolio design	quadratic programming convex programming investment;standards;general risk parity portfolio problem formulation scrip successive convex optimization methods traditional markowitz portfolio optimization sequential quadratic programming interior point methods nonconvex risk parity formulations;portfolios;indexes;covariance matrices;portfolios reactive power approximation methods indexes standards signal processing algorithms covariance matrices;approximation methods;successive convex optimization efficient sequential algorithms risk budgeting risk parity;signal processing algorithms;reactive power	The traditional Markowitz portfolio optimization proposed in the 1950s has not been embraced by practitioners despite its theoretical elegance. Recently, an alternative risk parity portfolio design has been receiving significant attention from both the theoretical and practical sides due to its advantage in diversification of (ex-ante) risk contributions among assets. Such risk contributions can be deemed good predictors for the (ex-post) loss contributions, especially when there exist huge losses. Most of the existing specific problem formulations on risk parity portfolios are highly nonconvex and are solved via standard off-the-shelf numerical optimization methods, e.g., sequential quadratic programming and interior point methods. However, for nonconvex risk parity formulations, such standard numerical approaches may be highly inefficient and may not provide satisfactory solutions. In this paper, we first propose a general risk parity portfolio problem formulation that can fit most of the existing specific risk parity formulations, and then propose a family of simple and efficient successive convex optimization methods for the general formulation. The numerical results show that our proposed methods significantly outperform the existing ones.	approximation algorithm;central processing unit;computation;convex optimization;diversification (finance);existential quantification;experiment;hoc (programming language);interior point method;local convergence;mathematical optimization;numerical analysis;sequential quadratic programming;stationary process;synthetic intelligence	Yiyong Feng;Daniel Pérez Palomar	2015	IEEE Transactions on Signal Processing	10.1109/TSP.2015.2452219	database index;mathematical optimization;computer science;portfolio optimization;mathematics;ac power;mathematical economics	ML	34.02508518874654	4.525847681232678	118687
b502f77c1c3cb6ba188a883cae85750a67a8af37	the asymptotic value of randic index for trees	double-star;tree;asymptotic value;generating function;normal distribution;average distance.;general randic index;indexation;connected graph	Let Tn denote the set of all unrooted and unlabeled trees with n vertices, and (i, j) a double-star. By assuming that every tree of Tn is equally likely, we show that the limiting distribution of the number of occurrences of the double-star (i, j) in Tn is normal. Based on this result, we obtain the asymptotic value of Randić index for trees. Fajtlowicz conjectured that for any connected graph the Randić index is at least the average distance. Using this asymptotic value, we show that this conjecture is true not only for almost all connected graphs but also for almost all trees.	connectivity (graph theory);randić's molecular connectivity index	Xueliang Li;Yiyang Li	2010	CoRR			Theory	38.61320334953106	16.858486083696462	118702
9ddeb28cf5fdeeb4a9cf71aeb20cba86bafa2b95	on minimal trajectories for mobile sampling of bandlimited fields		We study the design of sampling trajectories for stable sampling and the reconstruction of bandlimited spatial fields using mobile sensors. The spectrum is assumed to be a symmetric convex set. As a performance metric we use the path density of the set of sampling trajectories that is defined as the total distance traveled by the moving sensors per unit spatial volume of the spatial region being monitored. Focusing first on parallel lines, we identify the set of parallel lines with minimal path density that contains a set of stable sampling for fields bandlimited to a known set. We then show that the problem becomes ill posed when the optimization is performed over all trajectories by demonstrating a feasible trajectory set with arbitrarily low path density. However, the problem becomes well-posed if we explicitly specify the stability margins. We demonstrate this by obtaining a non-trivial lower bound on the path density of an arbitrary set of trajectories that contain a sampling set with explicitly specified stability bounds.	bandlimiting	Karlheinz Gröchenig;José Luis Romero;Jayakrishnan Unnikrishnan;Martin Vetterli	2013	CoRR		mathematical optimization;discrete mathematics;mathematics;geometry	ML	32.199041431898976	17.793265390639185	118812
0d1444ce50c0ae6fb2dffbf6d1744c9f0d51e6fe	on the continuous fermat-weber problem	k median problem;location theory;location problem;shortest path;optimisation;probleme k median;probleme localisation;localisation installation;optimizacion;geometric optimization;continuous location fermat weber problem;computational geometry;problema np duro;average distance;polynomial time algorithm;requirement analysis;np hard problem;planificacion;probleme np difficile;computational complexity;exact algorithm;facility location problem;planning;optimization;facilities equipment planning;problema localizacion;planification;continuous demand;problema k medio;probleme fermat weber;data structure;facility location	We give the first exact algorithmic study of facility location problems that deal with finding a median for a continuum of demand points. In particular, we consider versions of the “continuous k-median (Fermat-Weber) problem” where the goal is to select one or more center points that minimize the average distance to a set of points in a demand region. In such problems, the average is computed as an integral over the relevant region, versus the usual discrete sum of distances. The resulting facility location problems are inherently geometric, requiring analysis techniques of computational geometry. We provide polynomial-time algorithms for various versions of the L1 1-median (Fermat-Weber) problem. We also consider the multiple-center version of the L1 k-median problem, which we prove is NP-hard for large k. MSC Classification: 90B85, 68U05 ACM Classification: F.2.2	acm computing classification system;algorithm;approximation algorithm;best, worst and average case;cpu cache;computational anatomy;computational geometry;cubic function;euclidean distance;extensibility;fermat;geometric median;k-medians clustering;local optimum;loss function;map;maxima and minima;np-hardness;optic axis of a crystal;optimization problem;polyhedron;polynomial;shortest path problem;subdivision surface;taxicab geometry;time complexity;triune continuum paradigm;voronoi diagram;weber problem;worst-case complexity	Sándor P. Fekete;Joseph S. B. Mitchell;Karin Beurer	2005	Operations Research	10.1287/opre.1040.0137	mathematical optimization;location theory;data structure;computational geometry;computer science;facility location problem;calculus;mathematics;1-center problem;algorithm	Theory	28.195441456594935	17.257247674139084	119172
55a07b318c00f2b0618d202b656d56d3e32906e7	new strategies for finding multiplicative decompositions of probability trees	probability trees;approximation;multiplicative factorisation	Probability trees are a powerful data structure for representing probabilistic potentials. However, their complexity can become intractable if they represent a probability distribution over a large set of variables. In this paper, we study the problem of decomposing a probability tree as a product of smaller trees, with the aim of being able to handle bigger probabilistic potentials. We propose exact and approximate approaches and evaluate their behaviour through an extensive set of experiments. 2013 Elsevier Inc. All rights reserved.	approximation algorithm;data structure;decision tree;experiment	Irene Martínez;Serafín Moral;Carmelo Rodríguez;Antonio Salmerón	2013	Applied Mathematics and Computation	10.1016/j.amc.2013.10.023	probability distribution;random variable;mathematical optimization;combinatorics;probability mass function;discrete mathematics;probability measure;convolution of probability distributions;machine learning;approximation;tree diagram;mathematics	AI	35.62548279433228	7.6984321438982555	119541
fe88523b3c9f3579a1589e65ee60c4ef8061807e	balancing domain decomposition applied to structural analysis problems	balancing domain decomposition;structure analysis	Publisher Summary This chapter discusses the present status of a library of iterative sub-structuring solvers for the parallel solution of large sparse linear systems of equations that arise from large scale industrial finite-element applications. The iterative solution schemes have been applied to a series of model problems but also to real-life test cases that include structural analysis problems from the automobile industry and ship structure analysis. The library provides a single framework that includes Schur complement techniques and Schwarz procedures. The pre-conditioners include well-known incomplete factorization variants, as well as more advanced techniques, (such as for example balancing domain decomposition). The chapter shows some of the performance of the software on a real-life industrial problem. The SALSA software package features state-of-the-art domain decomposition techniques. These include iterative sub-structuring techniques with l-level and 2-level Neumann-Neumann pre-conditioners. The 2-level method features a variety of coarse spaces. Some of these coarse spaces are designed for specific classes of problems, but others are computed algebraically and no additional problem information is required. The software contains other domain decomposition techniques as well, but this chapter restricts the discussion to iterative sub-structuring and the Neumann-Neumann pre-conditioners.	balancing domain decomposition method;domain decomposition methods;structural analysis	Petter E. Bjørstad;Jacko Koster	2003			computer science;structural analysis;balancing domain decomposition method	Vision	32.146622002065335	9.32510203031079	119587
0594e57506a2c0045dd08030396b3c071bfda34a	a computationally motivated definition of parametric estimation and its applications to the gaussian distribution	approximate algorithm;parametric estimation;random variable;polynomial time;gaussian distribution;penalty function	We introduce a treatment of parametric estimation in which optimality of an estimator is measured in probability rather than in variance (the measure for which the strongest general results are known in statistics). Our motivation is that the quality of an approximation algorithm is measured by the probability that it fails to approximate the desired quantity within a set tolerance. We concentrate on the Gaussian distribution and show that the sample mean is the unique “best” estimator, in probability, for the mean of a Gaussian distribution. We also extend this method to general penalty functions and to multidimensional spherically symmetric Gaussians. The algorithmic significance of studying the Gaussian distribution is established by showing that determining the average matching size in a graph is #P-hard, and moreover approximating it reduces to estimating the mean of a random variable that (under some mild conditions) has a distribution closely approximating a Gaussian. This random variable is (essentially) polynomial time samplable, thereby yielding an FPRAS for the problem.	approximation algorithm;p (complexity);polynomial-time approximation scheme;sharp-p;time complexity	Leonard J. Schulman;Vijay V. Vazirani	2005	Combinatorica	10.1007/s00493-005-0028-4	normal distribution;gaussian random field;time complexity;random variable;econometrics;mathematical optimization;normal-inverse gaussian distribution;penalty method;gaussian process;mathematics;gaussian function;statistics	Theory	39.181483926223734	12.872251020305377	119639
e303eefd48faf872c6275dff5021d6930d90997c	a new effective dynamic program for an investment optimization problem	dynamic programming;fptas;investment problem;programming;multi choice knapsack problem;dynamic	After a series of publications of T.E. O’Neil et al. (e.g. in 2010), dynamic programming seems to be the most promising way to solve knapsack problems. Some techniques are known to make dynamic programming algorithms (DPA) faster. One of them is the graphical method that deals with piecewise linear Bellman functions. For some problems, it was previously shown that the graphical algorithm has a smaller running time in comparison with the classical DPA and also some other advantages. In this paper, an exact graphical algorithm (GrA) and a fully polynomial-time approximation scheme based on it are presented for an investment optimization problem having the best known running time. The algorithms are based on new Bellman functional equations and a new way of implementing the GrA.	algorithm;bellman equation;dynamic programming;graphical user interface;knapsack problem;list of graphical methods;mathematical optimization;optimization problem;piecewise linear continuation;polynomial;polynomial-time approximation scheme;time complexity	Evgeny R. Gafarov;Alexandre Dolgui;Alexander A. Lazarev;Frank Werner	2016	Automation and Remote Control	10.1134/S0005117916090101	programming;mathematical optimization;combinatorics;cutting stock problem;change-making problem;dynamic programming;mathematics;knapsack problem;algorithm	Theory	26.66287707807627	5.54723984788094	119644
227902ae0fae007c99958f58803d1103a702b65f	algorithms for analysis and control of boolean networks		Boolean network is a discrete mathematical model of gene regulatory networks. In this short article, we briefly review algorithmic results on finding attractors in Boolean networks. Since it is known that the problem of finding a singleton attractor is NP-hard and the problem can be trivially solved in (O^{*}(2^n)) time (under a reasonable assumption), we focus on special cases in which the problem can be solved in (O((2-delta )^n)) time for some constant (delta u003e0). We also briefly review algorithmic results on control of Boolean networks.	algorithm;boolean network	Tatsuya Akutsu	2018		10.1007/978-3-319-91938-6_1	discrete mathematics;boolean network;controllability;attractor;gene regulatory network;singleton;mathematics	Logic	37.7618792447616	11.233360660859699	119749
8495b67fd2479cde1250bfcf1dde34922042f931	on soliton collisions between localizations in complex elementary cellular automata: rules 54 and 110 and beyond		In this paper, a single-soliton two-component cellular automaton (CA) model of waves is presented as mobile self-localizations, also known as particles, waves, or gliders, in addition to its version with memory. The model is based on coding sets of strings where each chain represents a unique mobile self-localization. The original soliton models in CAs proposed with filter automata are briefly discussed, followed by solutions in elementary CAs (ECAs) domain with the famous universal ECA rule!110, and reporting a number of new solitonic collisions in ECA rule 54. A mobile self-localization in this study is equivalent to a single soliton because the collisions of the mobile self-localizations studied in this paper satisfy the property of solitonic collisions. A specific ECA with memory (ECAM), the ECAM rule fR9maj:4, is also presented; it displays single-soliton solutions from any initial codification (including random initial conditions) for a kind of mobile self-localization because such an automaton is able to adjust any initial condition to soliton structures.	electronic centralised aircraft monitor;elementary cellular automaton;initial condition;rule 110;soliton	Genaro Juárez Martínez;Andrew Adamatzky;Fangyue Chen;Leon O. Chua	2012	Complex Systems		calculus;pure mathematics;mathematics;algorithm;quantum mechanics	AI	38.761763629595414	9.306989961034564	119867
5600b056f2d96c51882423ae9f43b1bd8bc9bd65	a branch-and-cut algorithm for the undirected selective traveling salesman problem	traveling salesman problem;scheduling;trucks;branch and cut;orienteering	The Selective Traveling Salesman Problem (STSP) is defined on a graph in which profits are associated with vertices and costs are associated with edges. Some vertices are compulsory. The aim is to construct a tour of maximal profit including all compulsory vertices and whose cost does not exceed a preset constant. We developed several classes of valid inequalities for the symmetric STSP and used them in a branch-and-cut algorithm. Depending on problem parameters, the proposed algorithm can solve instances involving up to 300 vertices. q 1998 John Wiley & Sons, Inc. Networks 32: 263–273, 1998	algorithm;branch and cut;graph (discrete mathematics);john d. wiley;maximal set;travelling salesman problem;vertex (geometry);vertex (graph theory)	Michel Gendreau;Gilbert Laporte;Frédéric Semet	1998	Networks	10.1002/(SICI)1097-0037(199812)32:4%3C263::AID-NET3%3E3.0.CO;2-Q	mathematical optimization;combinatorics;integer programming;level structure;graph center;combinatorial optimization;graph theory;mathematics;travelling salesman problem;algorithm	Theory	24.90681401124872	16.94733127608654	119938
84134fb8acc00d0bc2c42af8dd881b2aa24c442e	decomposition in global optimization	mathematical programming;global optimization	The purpose of this article is to propose a simple framework for the various decomposition schemes in mathematical programming. Special instances are discussed. Particular attention is devoted to the general mathematical programming problem with two sets of variables. An economic interpretation in the context of hierarchical planning is done for the suggested decomposition procedure. The framework is based on general duality theory in mathematical programming and thus focussing on approaches leading to global optimality.	global optimization;mathematical optimization	Jørgen Tind	1991	J. Global Optimization	10.1007/BF00119987	stochastic programming;fractional programming;mathematical optimization;robust optimization;nonlinear programming;computer science;theoretical computer science;multi-objective optimization;machine learning;mathematics;active set method;global optimization	Vision	25.738315604018684	10.152212339997885	119989
70397a7bbdc169a81413a205c8fdd1de3dc92277	a small go board study of metric and dimensional evaluation functions	arbre recherche;obstaculo;complexite;evaluation function;evaluation fonction;image processing;speech processing;complejidad;tratamiento palabra;procesamiento imagen;traitement parole;base connaissance;metric;complexity;traitement image;funcion matematica;arbol investigacion;function evaluation;modelo 2 dimensiones;combinatorial complexity;modele 2 dimensions;pattern recognition;mathematical function;base conocimiento;metrico;fonction mathematique;reconnaissance forme;reconocimiento patron;search tree;metrique;two dimensional model;obstacle;knowledge base	The difficulty to write successful 19x19 go programs lies not only in the combinatorial complexity of go but also in the complexity of designing a good evaluation function containing a lot of knowledge. Leaving these obstacles aside, this paper defines very-little-knowledge evaluation functions used by programs playing on very small boards. The evaluation functions are based on two mathematical tools, distance and dimension, and not on domaindependent knowledge. After a qualitative assessment of each evaluation function, we built several programs playing on 4x4 boards by using tree search associated with these evaluation functions. We set up an experiment to select the best programs and identify the relevant features of these evaluation functions. Thanks to the results obtained by these very-little-knowledge-based programs, we can foresee the usefulness of each evaluation function.	computer go;evaluation function	Bruno Bouzy	2002		10.1007/978-3-540-40031-8_25	knowledge base;complexity;metric;image processing;computer science;artificial intelligence;evaluation function;speech processing;mathematics;search tree;function;algorithm	AI	30.708412262757935	15.051657466782252	120300
3bc44c7a617f4fae8c554e6fbc2152fd2c396738	one-round discrete voronoi game in ℝ2 in presence of existing facilities		In this paper we consider a simplified variant of the discrete Voronoi Game in R, which is also of independent interest in competitive facility location. The game consists of two players P1 and P2, and a finite set U of users in the plane. The players have already placed two sets of facilities F and S, respectively in the plane. The game begins by P1 placing a new facility followed by P2 placing another facility, and the objective of both the players is to maximize their own total payoffs. When |F | = |S| = m, this corresponds to the last round of the (m + 1)-round discrete Voronoi Game in R. In this paper we propose polynomial time algorithms for obtaining optimal strategies of both the players under arbitrary locations of the existing facilities F and S. We show that the optimal strategy of P2, given any placement of P1, can be found in O(n) time, and the optimal strategy of P1 can be found in O(n) time.	algorithm;polynomial;time complexity;universal quantification;voronoi diagram	Aritra Banik;Bhaswar B. Bhattacharya;Sandip Das;Satyaki Mukherjee	2013				ECom	25.77727494289377	17.653911758619035	120439
3f5452fbf49d9561c8d0332484cc8850172e49cb	approximating the bethe partition function	computer science	When belief propagation (BP) converges, it does so to a stationary point of the Bethe free energyF , and is often strikingly accurate. However, it may converge only to a local optimum or may not converge at all. An algorithm was recently introduced by Weller and Jebara for attractive binary pairwise MRFs which is guaranteed to return anǫ-approximation to the global minimum ofF in polynomial time provided the maximum degree∆ = O(log n), wheren is the number of variables. Here we extend their approach and derive a new method based on analyzing first derivatives ofF , which leads to much better performance and, for attractive models, yields a fully polynomial-time approximation scheme (FPTAS) without any degree restriction. Further, our methods apply to general (nonattractive) models, though with no polynomial time guarantee in this case, demonstrating that approximatinglog of the Bethe partition function, logZB = −minF , for a general model to additiveǫ-accuracy may be reduced to a discrete MAP inference problem. This allows the merits of the global Bethe optimum to be tested.	algorithm;belief propagation;bethe–salpeter equation;converge;local optimum;maxima and minima;partition function (mathematics);partition type;polynomial;polynomial-time approximation scheme;software propagation;stationary process;time complexity	Adrian Weller;Tony Jebara	2014			mathematical optimization;combinatorics;discrete mathematics;computer science;machine learning;mathematics;statistics	ML	33.96370519347958	8.255855223120122	120475
2c0c121ad11dbd31cc23e790610ff7347f9d0f1c	new error measures and methods for realizing protein graphs from distance data	distance geometry;protein conformation;mathematical programming	Abstract The interval Distance Geometry Problem (i DGP) consists in finding a realization in R of a simple undirected graph G = (V,E) with nonnegative intervals assigned to the edges in such a way that, for each edge, the Euclidean distance between the realization of the adjacent vertices is within the edge interval bounds. In this paper, we focus on the application to the conformation of proteins in space, which is a basic step in determining protein function: given interval estimations of some of the inter-atomic distances, find their shape. Among different families of methods for accomplishing this task, we look at mathematical programming based methods, which are well suited for dealing with intervals. The basic question we want to answer is: what is the best such method for the problem? The most meaningful error measure for evaluating solution quality is the coordinate root mean square deviation. We first introduce a new error measure which addresses a particular feature of protein backbones, i.e. many partial reflections also yield acceptable backbones. We then present a set of new and existing quadratic and semidefinite programming formulations of this problem, and a set of new and existing methods for solving these formulations. Finally, we perform a computational evaluation of all the feasible solver+formulation combinations according to new and existing error measures, finding that the best methodology is a new heuristic method based on multiplicative weights updates.	algorithm;atom;benchmark (computing);computation;dynamic graphics project;emoticon;euclidean distance;general-purpose modeling;graph (discrete mathematics);heuristic;integer factorization;linear programming relaxation;mathematical optimization;mean squared error;metaheuristic;modulo operation;neighbourhood (graph theory);quadratic function;r language;reflection (computer graphics);semidefinite programming;whole earth 'lectronic link	Claudia D'Ambrosio;Ky Khac Vu;Carlile Lavor;Leo Liberti;Nelson Maculan	2017	Discrete & Computational Geometry	10.1007/s00454-016-9846-7	mathematical optimization;combinatorics;discrete mathematics;topology;mathematics;geometry	ML	25.44645453682445	15.002513912603524	120594
ed1b98d26d91a77b30eddd0b5f6492acd0a00abb	on the distribution of betweenness centrality in random trees	betweenness centrality;centroid;random tree;simply generated tree;increasing tree;subcritical graph class	Betweenness centrality is a quantity that is frequently used to measure how ‘central’ a vertex v is. It is defined as the sum, over pairs of vertices other than v, of the proportions of shortest paths that pass through v. In this paper, we study the distribution of the betweenness centrality in random trees and related, subcritical graph families. Specifically, we prove that the betweenness centrality of the root vertex in a simply generated tree is usually of linear order, but has a mean of order n. We also show that a randomly chosen vertex typically also has linear-order betweenness centrality, and that the maximum betweenness centrality in a simply generated tree is of order n. We obtain limiting distributions for the betweenness centrality of the root vertex and of a randomly chosen vertex, as well as for the maximum betweenness centrality, and we also show that the centroid has positive probability in the limit to be the vertex of maximum betweenness centrality. Some similar results also hold for subcritical graph classes, which will be briefly discussed. Finally, we study random recursive trees and other families of increasing trees, where the situation is quite different: here, the root betweenness centrality is of quadratic order, as is the maximum betweenness centrality. The betweenness centrality of a random vertex, on the other hand, is again of linear order. Again, we also have limiting distributions upon suitable normalisation.	betweenness centrality;randomness;recursion;shortest path problem	Kevin Durant;Stephan Wagner	2017	Theor. Comput. Sci.	10.1016/j.tcs.2016.12.023	random walk closeness centrality;combinatorics;discrete mathematics;topology;centroid;katz centrality;alpha centrality;mathematics;centrality;betweenness centrality	ECom	38.73881251569887	16.73690624408266	120634
d8dc8e0cc86fcada6a7b326ba80e13ee2b99387d	cellular automaton labyrinths and solution finding	pattern generation;optimal path;parallel computer;cellular automata;localized state;cellular automaton	Abstract   In the paper we combine pattern generation with parallel computation in nonlinear active media using simulations in two-dimensional cellular automata. The labyrinth is constructed as a cellularautomata model of living populations where predators compete for prey and prey for the resources. We show how to find an optimal path from an arbitrary site inside the labyrinth toward the exit outside. The computation resembles the spreading of auto-waves that compete for space and modify local states in order to fix the path.	cellular automaton	Andrew Adamatzky	1997	Computers & Graphics	10.1016/S0097-8493(97)00027-7	stochastic cellular automaton;cellular automaton;reversible cellular automaton;block cellular automaton;combinatorics;discrete mathematics;elementary cellular automaton;continuous spatial automaton;computer science;theoretical computer science;asynchronous cellular automaton;continuous automaton;mathematics;mobile automaton;algorithm	Logic	38.66855322706596	9.266294218098547	120698
d6f3355eccef23eb96f7c84b4fde7eb21e15687c	optimization problems with algebraic solutions: quadratic fractional programs and ratio games	optimal solution;quadratic programming;optimisation;game theory;programmation quadratique;algebraic numbers;theorie jeu;operations research;programmation fractionnaire;quadratic fractional programming;fractional programming;jeu 2 personnes;objective function;optimization problem;optimal strategy;solution algebrique;mathematical programming;recherche operationnelle;two person game;polynomial time;optimization;ratio games;jeu somme nulle;programmation mathematique;zero sum game;strategie optimale;algebraic optimization problems	A mathematical program with a rational objective function may have irrational algebraic solutions even when the data are integral. We suggest that for such problems the optimal solution will be represented as follows: If λ* denotes the optimal value there will be given an intervalI and a polynomialP(λ) such thatI contains λ* and λ* is the unique root ofP(λ) inI. It is shown that with this representation the solutions to convex quadratic fractional programs and ratio games can be obtained in polynomial time.		Ramaswamy Chandrasekaran;Arie Tamir	1984	Math. Program.	10.1007/BF02591937	fractional programming;time complexity;optimization problem;game theory;mathematical optimization;combinatorics;algebraic number;mathematics;zero-sum game;mathematical economics;quadratic programming	Theory	26.71816325796518	12.536864243505542	121226
56bfdecfd21491cc0f09ccbb537b230639f17098	improving the orthogonal range search k -windows algorithm	databases;search problem;machine learning algorithms;cluster algorithm;pattern clustering;analyse amas;mathematics;algoritmo busqueda;high dimensionality;iterative algorithms;time complexity;geometrie algorithmique;homogeneous groups;algorithme recherche;search algorithm;computational geometry;range free;tree data structures;problema investigacion;spatial data structure;data mining;classification;apprentissage machine;range tree;complexite temps;cluster analysis;machine learning;computational complexity;clustering;superlinear space requirements;orthogonal range search k windows algorithm;pattern clustering orthogonal range search k windows algorithm pattern set partitioning disjoint groups homogeneous groups disjoint clusters homogeneous clusters spatial data structures range free low time complexity superlinear space requirements range tree;low time complexity;clustering algorithms partitioning algorithms iterative algorithms machine learning algorithms mathematics data mining artificial intelligence tree data structures databases pattern analysis;clustering algorithms;artificial intelligence;spatial data structures;homogeneous clusters;geometria computacional;analisis cluster;disjoint groups;pattern analysis;search problems;pattern set partitioning;complejidad tiempo;probleme recherche;disjoint clusters;clasificacion;partitioning algorithms;computational complexity pattern clustering search problems spatial data structures;structure donnee spatiale	Clustering, that is the partitioning of a set of patterns into disjoint and homogeneous meaningful groups (clusters), is a fundamental process in the practice of science. k-windows is an efficient clustering algorithm that reduces the number of patterns that need to be examined for similarity, using a windowing technique. It exploits well known spatial data structures, namely the range tree, that allows fast range searches. From a theoretical standpoint, the kwindows algorithm is characterized by lower time complexity compared to other well-known clustering algorithms. Moreover, it achieves high quality clustering results. However, it appears that it cannot be directly applicable in highdimensional settings due to the superlinear space requirements for the range tree. In this paper, an improvement of the k-windows algorithm, aiming at resolving this def iciency, is presented. The improvement is based on an alternative solution to the orthogonal range search problem.	algorithm;cluster analysis;computer cluster;data structure;display resolution;microsoft windows;range searching;range tree;requirement;search problem;time complexity	Panagiotis D. Alevizos;Basilis Boutsinas;Dimitris K. Tasoulis;Michael N. Vrahatis	2002		10.1109/TAI.2002.1180810	correlation clustering;constrained clustering;data stream clustering;fuzzy clustering;computational geometry;computer science;theoretical computer science;canopy clustering algorithm;machine learning;cure data clustering algorithm;cluster analysis;algorithm	DB	27.245361948567474	14.912200132124966	121269
1708e005be628570c31adc02f56ed42f82ffc028	on hierarchical diameter-clustering and the supplier problem	hierarchical clustering;online algorithm;metric space;approximate algorithm;approximation algorithm;lower bound	Given a data set in a metric space, we study the problem of hierarchical clustering to minimize the maximum cluster diameter, and the hierarchical k-supplier problem with customers arriving online. We prove that two previously known algorithms for hierarchical clustering, one (offline) due to Dasgupta and Long and the other (online) due to Charikar, Chekuri, Feder and Motwani, output essentially the same result when points are considered in the same order. We show that the analyses of both algorithms are tight and exhibit a new lower bound for hierarchical clustering. Finally we present the first constant factor approximation algorithm for the online hierarchical k-supplier problem.	apx;approximation algorithm;cluster analysis;hierarchical clustering;olami–feder–christensen model;online and offline	Aparna Das;Claire Mathieu	2009	Theory of Computing Systems	10.1007/s00224-009-9186-6	correlation clustering;online algorithm;mathematical optimization;combinatorics;data stream clustering;metric space;computer science;canopy clustering algorithm;machine learning;hierarchical network model;cure data clustering algorithm;mathematics;hierarchical clustering;upper and lower bounds;single-linkage clustering;brown clustering;approximation algorithm;hierarchical clustering of networks	Theory	25.87477831640977	18.22335122638678	121346
507d2331d4f3cbbb2524e022f00788a08478b721	anisotropic diagrams: labelle shewchuk approach revisited.	chevauchement;diagramme voronoi;hypothese;anisotropic voronoi diagram;definicion;overlap;potencia;imbricacion;68wxx;algorithme;algorithm;hipotesis;definition;construccion;triangle;triangulacion;anisotropic meshing;anisotropic;informatique theorique;meshing;puissance;3 dimensional;triangulation;hypothesis;diagrama voronoi;construction;power;voronoi diagram;computer theory;algoritmo;informatica teorica	F. Labelle and J. Shewchuk have proposed a discrete definition of anisotropic Voronoi diagrams. These diagrams are parametrized by a metric field. Under mild hypotheses on the metric field, such Voronoi diagrams can be refined so that their dual is a triangulation, with elements shaped according to the specified anisotropic metric field. We propose an alternative view of the construction of these diagrams and a variant of Labelle and Shewchuk’s meshing algorithm. This variant computes the Voronoi vertices using a higher dimensional power diagram and refines the diagram as long as dual triangles overlap. We see this variant as a first step toward a 3-dimensional anisotropic meshing algorithm.	anisotropic diffusion;computation;computational complexity theory;distortion;greedy algorithm;power diagram;sliver polygon;voronoi diagram	Jean-Daniel Boissonnat;Camille Wormser;Mariette Yvinec	2005	Theor. Comput. Sci.	10.1016/j.tcs.2008.08.006	three-dimensional space;combinatorics;hypothesis;topology;construction;voronoi diagram;definition;triangulation;power;mathematics;geometry;anisotropy	Graphics	33.31513399467509	17.331038522041055	121969
411114f1c44418e01c7f3d775fb3eade75ca93e9	bounds on optimal merge performance, and a strategy for optimality	limit distribution;large scale;storage capacity;probability distribution;random variable;rational number;upper and lower bounds;random access;lower bound;information theory	The length of a sorted sequence produced by the internal sort phase of a large scale general purpose sort routine is a random variable. In a random access environment, any given set of such sequences can be merged in an optimal way, and in practice this is often done. The expected work per item required by an optimal merge depends upon the probability distribution for sequence length, and it is this dependence which is studied in this paper. Reasonably sharp upper and lower bounds are derived. The distribution which is optimal in the sense of minimizing the lower bound on any bounded interval is determined, and it is shown that this is the strongest result of its kind.	approximation algorithm;internal sort;maximal set;random access	W. Donald Frazer;B. T. Bennett	1972	J. ACM	10.1145/321724.321729	vysochanskij–petunin inequality;probability distribution;random variable;mathematical optimization;combinatorics;discrete mathematics;convergence of random variables;information theory;mathematics;upper and lower bounds;moment-generating function;statistics	Theory	35.26483429048675	11.491390886713429	122014
9c6ad4069da2ef83eda091ea905d15c8dfa7407b	n points and one line: analysis of randomized games	algorithm complexity;algorithm analysis;complejidad algoritmo;randomised algorithms;algorithme randomise;programacion lineal;complexite algorithme;informatique theorique;linear programming;programmation lineaire;analyse algorithme;analisis algoritmo;computer theory;informatica teorica	We are given n points in the plane and a vertical line l (directed upwards) that intersects the convex hull, convS, of S. We want to find 'ways' to the first edge (facet) of convS met by line l. It has been known since the early eighties that the problem of computing the first edge of convS met by l can be solved optimally in linear time by linear programming. Our motivation for considering the above processes is actually motivated by the higher-dimensional counterparts.	randomized algorithm	Emo Welzl	2000		10.1007/3-540-40064-8_2	linear programming;artificial intelligence;mathematics;algorithm	Theory	28.01290177936705	17.779026416746344	122610
a0a3ef004fb3368f4b670d6e7e65899c0b342352	on the computational utility of posynomial geometric programming solution methods	nonlinear programming;geometric program	Ten codes or code variants were used to solve the five equivalent posynomial GP problem formulations. Four of these codes were general NLP codes; six were specialized GP codes. A total of forty-two test problems was solved with up to twenty randomly generated starting points per problem. The convex primal formulation is shown to be intrinsically easiest to solve. The general purpose GRG code called OPT appears to be the most efficient code for GP problem solution. The reputed superiority of the specialized GP codes GGP and GPKTC appears to be largely due to the fact that these codes solve the convex primal formulation. The dual approaches are only likely to be competitive for small degree of difficulty, tightly constrained problems.		J. E. Fattler;Gintaras V. Reklaitis;Y. T. Sin;R. R. Root;K. M. Ragsdell	1982	Math. Program.	10.1007/BF01581036	mathematical optimization;combinatorics;nonlinear programming;mathematics;algorithm	Theory	25.9938443015696	7.848643592038349	122835
96f89451ba4cb0287526721f6e7a2f85b960dbe6	computing the rectilinear center of uncertain points in the plane		In this paper, we consider the rectilinear one-center problem on uncertain points in the plane. In this problem, we are given a set P of n (weighted) uncertain points in the plane and each uncertain point has m possible locations each associated with a probability for the point appearing at that location. The goal is to find a point q∗ in the plane which minimizes the maximum expected rectilinear distance from q∗ to all uncertain points of P , and q∗ is called a rectilinear center. We present an algorithm that solves the problem in O(mn) time. Since the input size of the problem is Θ(mn), our algorithm is optimal.	algorithm;information;regular grid;taxicab geometry	Haitao Wang;Jingru Zhang	2018	Int. J. Comput. Geometry Appl.	10.1142/S0218195918500073	mathematical optimization;combinatorics;mathematics;geometry	Theory	28.673909832546624	18.137425686365034	122993
9e648220e36b8bbbc0a33515814e8194abce02fd	point labeling with sliding labels in interactive maps		We consider the problem of labeling point objects in interactive maps where the user can pan and zoom continuously. We allow labels to slide along the point they label. We assume that each point comes with a priority; the higher the priority the more important it is to label the point. Given a dynamic scenario with user interactions, our objective is to maintain an occlusion-free labeling such that, on average over time, the sum of the priorities of the labeled points is maximized. Even the static version of the problem is known to be NP-hard. We present an efficient and effective heuristic that labels points with sliding labels in real time. Our heuristic proceeds incrementally; it tries to insert one label at a time, possibly pushing away labels that have already been placed. To quickly predict which labels have to be pushed away, we use a geometric data structure that partitions screen space. With this data structure we were able to double the frame rate when rendering maps with many labels.	algorithm;data structure;glossary of computer graphics;heuristic;interaction;loss function;map;np-hardness;optimization problem	Nadine Schwartges;Jan-Henrik Haunert;Alexander Wolff;Dennis Zwiebler	2014		10.1007/978-3-319-03611-3_17	combinatorics;simulation;mathematics;engineering drawing	Theory	31.829208094308274	17.306525209134797	123036
170d9a6409fe4d7aa41787075946406c21199af0	the width and integer optimization on simplices with bounded minors of the constraint matrices	unimodular decomposition;efficient algorithm;flatness theorem;matrix minors;integer programming;width;polytope	In this paper, we will show that the width of simplices defined by systems of linear inequalities can be computed in polynomial time if some minors of their constraint matrices are bounded. Additionally, we present some quasi-polynomial-time and polynomial-time algorithms to solve the integer linear optimization problem defined on simplices minus all their integer vertices assuming that some minors of the constraint matrices of the simplices are bounded.	algorithm;graph minor;integer programming;linear inequality;linear programming;mathematical optimization;optimization problem;polynomial;quasi-polynomial;time complexity	Dmitry V. Gribanov;Aleksandr Yu. Chirkov	2016	Optimization Letters	10.1007/s11590-016-1048-y	polytope;mathematical optimization;combinatorics;discrete mathematics;integer programming;length;mathematics	Theory	25.02860429499344	14.398314742572927	123217
a704dc74e461aef35f28f1eaadb8544e60f52b85	flexible time and the evolution of one-dimensional cellular automata	cellular automata	"""Here I describe a view of the evolution of cellular automata that allows to operate on larger structures. Instead of calculating the next state of all cells in one step, the method here developed uses a time slice that can proceed at different places differently. This allows to """"jump"""" over the evolution of known structures in a single step."""	cellular automaton	Markus Redeker	2010	J. Cellular Automata		stochastic cellular automaton;cellular automaton;simulation;continuous spatial automaton;growcut algorithm;computer science;quantum cellular automaton;theoretical computer science;mathematics;mobile automaton;algorithm	Theory	38.88537565342731	9.202119823958963	123320
fe4b96179f87eb9815742bad6bb7b632525a4bf7	on a level-set characterization of the value function of an integer program and its application to stochastic programming	level set;grupo de excelencia;integer programming;ciencias basicas y experimentales;matematicas;characterization;value function;grupo a;stochastic programming	We propose a level-set approach to characterize the value function of a pure linear integer program with inequality constraints. We study theoretical properties of our characterization and show how they can be exploited to optimize a class of stochastic integer programs through a value function reformulation. Specifically, we develop algorithmic approaches that solve two-stage multidimensional knapsack problems with random budgets, yielding encouraging computational results.	bellman equation;computation;integer programming;knapsack problem;linear programming;social inequality;stochastic programming	Andrew C. Trapp;Oleg A. Prokopyev;Andrew J. Schaefer	2013	Operations Research	10.1287/opre.1120.1156	stochastic programming;mathematical optimization;combinatorics;discrete mathematics;integer programming;branch and price;level set;mathematics;bellman equation	AI	25.536388264192986	11.40372704831973	123344
49f770d928038a198757d4f3b23bcbaa52f74971	discrimination of singleton and periodic attractors in boolean networks	boolean networks;boolean logic;attractors;observability;discrimination;biomarkers	Determining the minimum number of sensor nodes to observe the internal state of the whole system is important in analysis of complex networks. However, existing studies suggest that a large number of sensor nodes are needed to know the whole internal state. In this paper, we focus on identification of a small set of sensor nodes to discriminate statically and periodically steady states using the Boolean network model where steady states are often considered to correspond to cell types. In other words, we seek a minimum set of nodes to discriminate singleton and periodic attractors. We prove that one node is not necessarily enough but two nodes are always enough to discriminate two periodic attractors by using the Chinese remainder theorem. Based on this, we present an algorithm to determine theminimum number of nodes to discriminate all given attractors. We also present a much more efficient algorithm to discriminate singleton attractors. The results of computational experiments suggest that attractors in realistic Boolean networks can be discriminated by observing the states of only a small number of nodes. © 2017 Elsevier Ltd. All rights reserved.	algorithm;boolean network;complex network;computation;experiment;network model;polynomial remainder theorem;steady state	Xiaoqing Cheng;Takeyuki Tamura;Wai-Ki Ching;Tatsuya Akutsu	2017	Automatica	10.1016/j.automatica.2017.07.012	discrete mathematics;boolean algebra;periodic graph (geometry);mathematical optimization;boolean network;chinese remainder theorem;attractor;mathematics;topology;complex network;small set;singleton	AI	38.06004541529752	11.27880803948929	123434
0a829c2c5f89267a23132506bc76f4ad399b9f0f	asynchronous distributed searchlight scheduling	graph theory;concave programming;multiagent system;position measurement art distributed algorithms usa councils scheduling algorithm robot sensing systems image sensors laser modes mechanical engineering optical imaging;distributed scheduling;scheduling concave programming graph theory multi robot systems;scheduling;multi robot systems;connected visibility graph asynchronous distributed searchlight scheduling asynchronous distributed scheduling multiple controlled searchlights nonconvex polygonal environment ray emission source location controlled slewing distributed one way sweep strategy parallel tree sweep strategy robotic agents	"""This paper develops and compares two asynchronous distributed scheduling algorithms for multiple controlled searchlights in nonconvex polygonal environments. A searchlight is a ray emitted by source location that (i) cannot penetrate the boundary of the environment and (ii) undergoes controlled slewing about its source location. Evaders move inside the environment along continuous trajectories and are detected precisely when they are on the searchlight ray at some time instant. The objective is for the searchlights to detect any evader in finite time and to do so using only local sensing and limited communication among them. The first algorithm we develop, called the distributed one way sweep strategy (DOWSS), is a distributed version of an algorithm described originally in 1990 by Sugihara et al [Sugihara, K., et al., 1990]; this algorithm may be slow in """"sweeping"""" the environment because only one searchlight slews at a time. Second we develop an algorithm, called the parallel tree sweep strategy (PTSS), in which searchlights sweep concurrently under the assumption that they are placed in appropriate locations; for this algorithm we establish linear completion time."""	algorithm;george sugihara;scheduling (computing);searchlight bbs	Karl J. Obermeyer;Anurag Ganguli;Francesco Bullo	2007	2007 46th IEEE Conference on Decision and Control	10.1109/CDC.2007.4434009	fair-share scheduling;real-time computing;simulation;computer science;graph theory;mathematics;distributed computing;scheduling	Robotics	31.69122730185341	12.914478077036463	123451
6a4061b10dff82159bbab5bd2350ade514dea7a9	cumulative step-size adaptation on linear functions	evolution path;csa;evolution strategies;cumulative path;step size adaptation	The CSA-ES is an Evolution Strategy with Cumulative Step size Adaptation, where the step size is adapted measuring the length of a so-called cumulative path. The cumulative path is a combination of the previous steps realized by the algorithm, where the importance of each step decreases with time. This article studies the CSA-ES on composites of strictly increasing functions with affine linear functions through the investigation of its underlying Markov chains. Rigorous results on the change and the variation of the step size are derived with and without cumulation. The step-size diverges geometrically fast in most cases. Furthermore, the influence of the cumulation parameter is studied.	algorithm;cma-es;evolution strategy;linear function;markov chain;signal-to-noise ratio	Alexandre Adrien Chotard;Anne Auger;Nikolaus Hansen	2012		10.1007/978-3-642-32937-1_8	mathematical optimization;calculus;cartan subalgebra;mathematics;statistics	ML	35.28754347045491	13.560043130700596	124118
6f2c454eb310488ad3dca066c212edc7c6cb68f7	a linear approximation method for probabilistic inference	probabilistic inference;linear approximation method;linear approximation	There have been a number of techniques developed in recent years for the efficient analysis of probabilistic inference problems, represented as Bayes' networks or influence diagrams (Lauritzen and Spiegelhalter [9], Pearl [12], Shachter [14]). To varying degrees these methods exploit the conditional independence assumed and revealed in the problem structure to analyze problems in polynomial time, essentially polynomial in the number of variables and the size of the largest state space encountered during the evaluation. Unfortunately, there are many problems of interest for which the variables of interest are continuous rather than discrete, so the relevant state spaces become infinite and the polynomial complexity is of little help.	approximation algorithm;bayesian network;coefficient;influence diagram;linear approximation;mos technology vic-ii;odds algorithm;polynomial;robert;state space;time complexity	Ross D. Shachter	1988			variable elimination;mathematical optimization;randomized rounding;statistical inference;discrete mathematics;fiducial inference;influence diagram;frequentist inference;computer science;mathematics;statistics;linear approximation	AI	35.5708261411945	7.668397715258011	124306
87f2adf8e98af573959386a87d25056a73a92900	distributions of points and large convex hulls of k points	metodo polinomial;algorithmique;temps polynomial;capsula convexa;cube;geometrie algorithmique;cubo;computational geometry;algorithme deterministe;enveloppe convexe;polynomial time algorithm;deterministic algorithms;algorithmics;algoritmica;polynomial method;polynomial time;borne inferieure;geometria computacional;convex hull;methode polynomiale;lower bound;cota inferior;tiempo polinomial	We consider a variant of Heilbronn’s triangle problem by asking for fixed integers d, k ≥ 2 and any integer n ≥ k for a distribution of n points in the d-dimensional unit cube [0, 1] such that the minimum volume of the convex hull of k points among these n points is as large as possible. We show that there exists a configuration of n points in [0, 1], such that, simultaneously for j = 2, . . . , k, the volume of the convex hull of any j points among these n points is Ω(1/n(j−1)/(1+|d−j+1|)). Moreover, for fixed k ≥ d+1 we provide a deterministic polynomial time algorithm, which finds for any integer n ≥ k a configuration of n points in [0, 1], which achieves, simultaneously for j = d + 1, . . . , k, the lower bound Ω(1/n(j−1)/(1+|d−j+1|)) on the minimum volume of the convex hull of any j among the n points.	algorithm;convex hull;p (complexity);polynomial	Hanno Lefmann	2006		10.1007/11775096_17	time complexity;mathematical optimization;combinatorics;convex combination;computational geometry;convex hull;cube;mathematics;geometry;upper and lower bounds;algorithmics	Theory	28.94524920375143	17.895106000080936	124379
7c07e4b676c39959f5eb9bf1a7afb8838092c7ea	bounds in multistage linear stochastic programming	secs s 06 metodi mat dell economia e scienze attuariali e finanziarie;multistage stochastic programming;skeleton solution;mat 09 ricerca operativa;expected value problem;value of stochastic solution	Multistage stochastic programs, which involve sequences of decisions over time, are usually hard to solve in realistically sized problems. Providing bounds for optimal solution may help in evaluating whether it is worth the additional computations for the stochastic program vs. simplified approaches. In this paper we generalize measures from the two-stage case, based on different levels of available information, to the multistage stochastic programming problems. A set of theorems providing chains of inequalities among the new quantities are proved. Numerical results on a case study related to a simple transportation problem illustrate the described relationships.	approximation algorithm;cobham's thesis;computation;linear programming;multistage amplifier;nonlinear system;problem solving;stochastic process;stochastic programming;transportation theory (mathematics)	Francesca Maggioni;Elisabetta Allevi;Marida Bertocchi	2014	J. Optimization Theory and Applications	10.1007/s10957-013-0450-1	stochastic programming;mathematical optimization;mathematics;mathematical economics;algorithm	AI	35.4377774021141	4.718107629357942	124420
a46699723314b4cfdd4741a73298ecdf5a575e8a	column enumeration based decomposition techniques for a class of non-convex minlp problems	decomposition;nonlinear programming;packing;objective function;optimization problem;column enumeration;decomposition algorithm;minlp;mixed integer nonlinear programming	We propose a decomposition algorithm for a special class of nonconvex mixed integer nonlinear programming problems which have an assignment constraint. If the assignment decisions are decoupled from the remaining constraints of the optimization problem, we propose to use a column enumeration approach. The master problem is a partitioning problem whose objective function coefficients are computed via subproblems. These problems can be linear, mixed integer linear, (non-)convex nonlinear or mixed integer nonlinear. However, the important property of the subproblems is that we can compute their exact global optimum quickly. The proposed technique will be illustrated solving a cutting problem with nonconvex nonlinear programming subproblems.	algorithm;assignment problem;coefficient;global optimization;integer programming;linear programming;mathematical optimization;nonlinear programming;nonlinear system;optimization problem;partition problem	Steffen Rebennack;Josef Kallrath;Panos M. Pardalos	2009	J. Global Optimization	10.1007/s10898-007-9271-9	optimization problem;mathematical optimization;combinatorics;discrete mathematics;integer programming;overlapping subproblems;nonlinear programming;branch and price;cutting stock problem;mathematics;decomposition	EDA	25.94248921855188	12.180444348227244	124480
905ba09d4db4f5e150457599553610fc2cb7e105	efficient pose and cell segmentation using column generation		We study the problems of multi-person pose segmentation in natural images and instance segmentation in biological images with crowded cells. We formulate these distinct tasks as integer programs where variables correspond to poses/cells. To optimize, we propose a generic relaxation scheme for solving these combinatorial problems using a column generation formulation where the program for generating a column is solved via exact optimization of very small scale integer programs. This results in efficient exploration of the spaces of poses and cells.	column generation;linear programming relaxation;mathematical optimization	Shaofei Wang;Chong Zhang;Miguel Ángel González Ballester;Julian Yarkony	2016	CoRR		computer vision;mathematical optimization;machine learning;mathematics	ML	25.17749029310488	6.613510239958716	125208
c96191b13042142f2078b7b93709892b575e786e	polyhedral studies for minimum-span graph labelling with integer distance constraints	polyhedral analysis;integer programming;graph labelling	This paper studies the polytope of the minimum-span graph labelling problems with integer distance constraints (DC-MSGL). We first introduce a few classes of new valid inequalities for the DC-MSGL defined on general graphs and briefly discuss the separation problems of some of these inequalities. These are the initial steps of a branch-and-cut algorithm for solving the DC-MSGL. Following that, we present our polyhedral results on the dimension of the DC-MSGL polytope, and that some of the inequalities are facet defining, under reasonable conditions, for the polytope of the DC-MSGL on triangular graphs.	algorithm;branch and cut;exact algorithm;polyhedral	Vicky H. Mak-Hau	2007	ITOR	10.1111/j.1475-3995.2007.00577.x	mathematical optimization;combinatorics;geometric graph theory;discrete mathematics;polyhedral graph;integer programming;polyhedral combinatorics;computer science;birkhoff polytope;mathematics;vertex enumeration problem	Theory	24.88846856622535	14.907154241520049	125240
29a72ba28e1fb8e83dd1bd2ee00916a9133d945f	martingales and large deviations for binary search trees	binary search trees;binary search tree;large deviations;martingales;large deviation;random trees	Abstract#R##N##R##N#We establish an almost sure large deviations theorem for the depth of the external nodes of binary search trees (BSTs). To achieve this, a parametric family of martingales is introduced. This family also allows us to get asymptotic results on the number of external nodes at deepest level. © 2001 John Wiley & Sons, Inc. Random Struct. Alg., 19: 112–127, 2001		Jean Jabbour-Hattab	2001	Random Struct. Algorithms	10.1002/rsa.1023	random binary tree;optimal binary search tree;combinatorics;discrete mathematics;binary search tree;binary expression tree;geometry of binary search trees;self-balancing binary search tree;mathematics;weight-balanced tree;statistics	ECom	38.93590886922597	16.39158099503683	125265
f4976102211e17dc776d91235dbe338a263cd816	an algorithm to find polygon similarity	transformation affine;measurement;geometrie algorithmique;computational geometry;polygone;similitude;polygon;medida;pattern matching;affine transformation;geometria algoritmica;poligono;mesure;concordance forme;similitud;transformacion afin	Abstract We present an algorithm for testing polygon similarity with respect to affine transformation by locating the matching pair of the polygons.	algorithm	Jayaramaiah Boreddy;R. N. Mukherjee	1989	Inf. Process. Lett.	10.1016/0020-0190(89)90142-7	combinatorics;point in polygon;complex polygon;topology;visibility polygon;shoelace formula;simple polygon;computational geometry;rectilinear polygon;star-shaped polygon;polygon;mathematics;geometry;affine-regular polygon;monotone polygon;polygon covering;polygon triangulation;pick's theorem	DB	30.085349115492033	16.959890163679294	125374
ff7799f49cb966d71c74fa54f2584c627ee158e5	partitioning polyhedral objects into nonintersecting parts	concepcion asistida;computer aided design;modele geometrique;polyedre;modelo 3 dimensiones;polyedron;computer graphics;poliedro;modele 3 dimensions;computational geometry;three dimensional model;face detection partitioning algorithms solids;interseccion;computer graphics computational geometry;information gathering;algorithme;algorithm;algorritmo;particion;partition;conception assistee;face detection;intersection;topological decisions nonintersecting parts planar polygons face pairs set theoretic operations numerical inaccuracy;solids;geometrical model;partitioning algorithms;modelo geometrico	An algorithm is described for partitioning intersecting polyhedrons into disjoint pieces and, more generally, removing intersections from sets of planar polygons embedded in three space. Polygons, or faces, need not be convex and may contain multiple holes. Intersections are removed by considering pairs of faces and slicing the faces apart along their regions of intersection. To reduce the number of face pairs examined, bounding boxes around groups of faces are checked for overlap. The intersection algorithm also computes set-theoretic operations on polyhedrons. Information gathered during face cutting is used to determine which portions of the original boundaries may be present in the result of an intersection, a union, or a difference of solids. The method includes provisions to detect and in some cases overcome, the effects of numerical inaccuracy on the topological decisions that the algorithm must make. The regions in which ambiguous results are possible are flagged so that the user can take appropriate action.<<ETX>>	convex function;embedded system;intersection algorithm;numerical analysis;numerical method;polyhedron;set theory	Mark Segal;Carlo H. Séquin	1988	IEEE Computer Graphics and Applications	10.1109/38.490	partition;computer vision;face detection;combinatorics;computational geometry;computer science;artificial intelligence;intersection;solid;mathematics;geometry;computer graphics;algorithm	Visualization	31.87418135552383	16.08740041287213	125610
05194ec1d06ca768bd7224c74896869f25c3d5ac	disjoint, partition and intersection constraints for set and multiset variables	metodo polinomial;cardinal number;multiensemble;multiplicite;problema np duro;global constraint;constraint satisfaction;satisfaction contrainte;multiset;np hard problem;set constraint;finite domain;nombre cardinal;numero cardinal;probleme np difficile;polynomial method;polynomial algorithm;constrenimiento conjunto;multiplicidad;contrainte ensembliste;multiconjunto;satisfaccion restriccion;methode polynomiale;multiplicity	We have started a systematic study of global constraints on s et and multiset variables. We consider here disjoint, partition, and intersection constraints in conjunction with cardinality constraints. These global constraints fall into one of three classes. In the first class, we show that we can decomp ose the constraint without hindering bound consistency. No new algorithms the refore need be developed for such constraints. In the second class, we show th at decomposition hinders bound consistency but we can present efficient polyn omial algorithms for enforcing bound consistency. Many of these algorithms e xploit a dual viewpoint, and call upon existing global constraints for finitedomain variables like the global cardinality constraint. In the third class, we sh ow that enforcing bound consistency is NP-hard. We have little choice therefore but to enforce a lesser level of local consistency when the size of such constraints grows.	algorithm;belief propagation;british undergraduate degree classification;cardinality (data modeling);enea ose;first-class function;local consistency;mike lesser;np-hardness;polynomial;software propagation	Christian Bessiere;Emmanuel Hebrard;Brahim Hnich;Toby Walsh	2004		10.1007/978-3-540-30201-8_13	cardinal number;mathematical optimization;combinatorics;discrete mathematics;constraint satisfaction;np-hard;mathematics;multiplicity;constraint;algorithm;local consistency	AI	24.764119341033915	13.731032496750851	125725
f6eef999290ed80ac8aa211091ba2ee03bbad299	optimal control of a class of stochastic hybrid systems with probabilistic constraints	quadratic programming;quadratic program;stochastic process;cost function;computer model;discrete time systems;stochastic hybrid system;discrete time;model predictive control;optimal control;computational modeling;vectors;stochastic processes;integer programming;stochastic systems discrete time systems integer programming optimal control quadratic programming reachability analysis;model predictive control optimal control probabilistic constraints discrete time stochastic hybrid systems continuous state regions backward reachability graph mixed integer quadratic programming problems;mixed integer quadratic programming;probabilistic logic;stochastic systems;reachability analysis;optimal control problem;probabilistic logic optimal control stochastic processes computational modeling cost function quadratic programming vectors	In this paper, a class of discrete-time stochastic hybrid systems, in which only discrete dynamics are stochastic, is considered. For this system, a solution method for the optimal control problem with probabilistic constraints is proposed. Probabilistic constraints guarantee that the probability that the continuous state reaches a given unsafe region is less than a given constant. In the propose method, first, continuous state regions, from which the state reaches a given unsafe region, are computed by a backward-reachability graph. Next, mixed integer quadratic programming problems with constraints derived from the backward-reachability graph are solved. The proposed method can be applied to model predictive control.	control theory;hybrid system;optimal control;quadratic programming;reachability	Koichi Kobayashi;Koichiro Matou;Kunihiko Hiraishi	2011	IEEE Conference on Decision and Control and European Control Conference	10.1109/CDC.2011.6160240	stochastic process;mathematical optimization;discrete time and continuous time;combinatorics;discrete mathematics;optimal control;mathematics;probabilistic logic;computational model;quadratic programming;model predictive control	Robotics	39.02075941590295	5.189431097421645	125870
1a69055329e740efe7bed0ef0303d0b294032d1a	epidemics on small worlds of tree-based wireless sensor networks	期刊论文;li qiao zhang baihai cui lingguo fan zhun athanasios v vasilakos 无线传感器网络 小世界现象 疫情 流行病学 渗滤阈值 小世界网络 免疫程序 感染 epidemics on small worlds of tree based wireless sensor networks	Due to link additions, small world phenomena exist in tree-based wireless sensor networks. Epidemics on small worlds of tree-based networks are studied, and the epidemic threshold at which the outbreak of the epidemic occurs is calculated. Epidemiological processes are analyzed when the infection probability is larger than the percolation threshold. Although different epidemiological processes occur on the underlying tree topology, the number of infected nodes increases exponentially as the infection spreads. The uniform immunization procedure is conducted in the homogeneous small-world network. The infection still extends exponentially although the immunization effectively reduces the prevalence speed.		Qiao Li;Baihai Zhang;Lingguo Cui;Zhun Fan;Athanasios V. Vasilakos	2014	J. Systems Science & Complexity	10.1007/s11424-014-1178-1	simulation;telecommunications;mathematics;distributed computing	Embedded	36.60120563193858	14.662490740879754	126300
8869941d09c6075e20225c0f793c15ea0e677479	introduction to special xqx issue	special xqx issue	The Unconstrained Binary Quadratic Programming (UBQP) problem is notable for encompassing a remarkable range of applications in combinatorial optimization. As observed in Kochenberger et al. (2013), classes of problems that can be formally expressed using UBQP formulations include: Quadratic Assignment Problems, Capital Budgeting Problems, Multiple Knapsack Problems, Task Allocation Problems (distributed computer systems), Maximum Diversity Problems, P-Median Problems, Asymmetric Assignment Problems, Symmetric Assignment Problems, Side Constrained Assignment Problems, Quadratic Knapsack Problems, Constraint Satisfaction Problems (CSPs), Set Partitioning Problems, Fixed Charge Warehouse Location Problems, Maximum Clique Problems, Maximum Independent Set Problems, Maximum Cut Problems, Graph Coloring Problems, Graph Partitioning Problems, Number Partitioning Problems, and Linear Ordering Problems. Even more remarkable is the fact that, once given a UBQP formulation, these problems can be solved by a UBQP method which is not specialized to exploit the problem domain of any individual class of problems, to yield solutions whose quality in many cases rivals or even surpasses the quality of the solutions produced by the best specialized methods, while achieving this outcome with an efficiency that likewise rivals or surpasses the efficiency of leading specialized methods. Moreover, these outcomes typically dominate those produced by current state-of-the-art commercial solvers for mixed integer linear and quadratic optimization, sometimes consuming two orders of magnitude less solution time to yield solutions superior to those of competing approaches. In other cases, where specialized methods edge out general UBQP methods for certain classes of problems, the UBQP formulation often still		Gary A. Kochenberger;Fred Glover	2013	J. Heuristics	10.1007/s10732-013-9227-9	mathematics;mathematical optimization;independent set;combinatorial optimization;knapsack problem;quadratic programming;maximum cut;constraint satisfaction problem;graph partition;graph coloring	AI	26.109373204782276	7.77800114901641	126386
2b0222f103be0464511468eb45cf19604a961b90	emergence as a computability-theoretic phenomenon	analisis numerico;computability theory;matematicas aplicadas;algorithmique;mathematiques appliquees;complexite calcul;emergence;computability;hombre;definability;analyse numerique;03dxx;complejidad computacion;numerical analysis;algorithmics;algoritmica;computational complexity;calculabilite;human;turing invariance;model of computation;58a25;applied mathematics;atomic processes;calculabilidad;homme;58j70	In dealing with emergent phenomena, a common task is to identify useful descriptions of them in terms of the underlying atomic processes, and to extract enough computational content from these descriptions to enable predictions to be made. Generally, the underlying atomic processes are quite well understood, and (with important exceptions) captured by mathematics from which it is relatively easy to extract algorithmic content. A widespread view is that the difficulty in describing transitions from algorithmic activity to the emergence associated with chaotic situations is a simple case of complexity outstripping computational resources and human ingenuity. Or, on the other hand, that phenomena transcending the standard Turing model of computation, if they exist, must necessarily lie outside the domain of classical computability theory. In this talk we suggest that much of the current confusion arises from conceptual gaps and the lack of a suitably fundamental model within which to situate emergence. We examine the potential for placing emergent relations in a familiar context based on Turing’s 1939 model for interactive computation over structures described in terms of reals. The explanatory power of this model is explored, formalising informal descriptions in terms of mathematical definability and invariance, and relating a range of basic scientific puzzles to results and intractable problems in computability theory.	algorithm;computability theory;computational resource;emergence;interactive computation;model of computation;turing	S. Barry Cooper	2009	Applied Mathematics and Computation	10.1016/j.amc.2009.04.050	model of computation;mathematical optimization;mathematical analysis;computability theory;applied mathematics;numerical analysis;computer science;artificial intelligence;mathematics;computability;computational complexity theory;algorithmics;physics;algorithm;emergence;algebra	NLP	36.64925670104863	9.841451554715205	126605
07d524a0791ac99c0cbde6e8a378ee7870487252	derman's book as inspiration: some results on lp for mdps	optimal policy;average reward;dual program;discount reward;deterministic policy	In 1976 I was looking for a suitable subject for my PhD thesis. My thesis advisor Arie Hordijk and I found a lot of inspiration in Derman’s book (Finite state Markovian decision processes, Academic Press, New York, 1970). Since that time I was interested in linear programming methods for Markov decision processes. In this article I will describe some results in this area on the following topics: (1) MDPs with the average reward criterion; (2) additional constraints; (3) applications. These topics are the main elements of Derman’s book.	linear programming;markov chain;markov decision process;nico habermann	Lodewijk C. M. Kallenberg	2013	Annals OR	10.1007/s10479-011-1047-4	computer science;artificial intelligence;mathematics;operations research	ML	37.33237993605974	4.708986474824554	126754
2a36cc76eef7784670ac6a6fe27a42ede9422a04	a theoretical extension on the operational law for monotone functions of uncertain variables	uncertain variable;monotone function;generalized operational law;expected value	The classical operational law of uncertain variables proposed by Liu makes an important contribution to the development of the uncertainty theory in both theories and applications. It provides a powerful and practical approach for calculating the uncertainty distribution of strictly monotone function of uncertain variables. However, the restriction on strictly monotone functions of the operational law limits its applications since many practical problems cannot be modeled by strictly monotone functions but general monotone functions. Therefore, an extension of the original operational law is needed. For this purpose, some properties concerning the uncertainty distributions of monotone functions of uncertain variables as well as the generalized inverse uncertainty distributions are presented first in this paper. On the basis of these discussions, a generalized operational law is proposed as a natural extension of the original operational law. Then the uncertainty distribution of a general monotone function of independent regular uncertain variables can be derived, which is analogous to the way that suggested by the original operational law for dealing with strictly monotone functions. Furthermore, as an application of the generalized operational law, a theorem for calculating the expected values of general monotone functions of uncertain variables is presented as well.	monotone	Yuanyuan Liu;Jing Liu;Ke Wang;Hui Zhang	2016	Soft Comput.	10.1007/s00500-015-1992-y	mathematical optimization;mathematical analysis;discrete mathematics;mathematics	Logic	35.218304258592816	4.8081111599592585	126860
4ff1ebba4469f9f653d4f80408c685de5cf7b6af	capacity and data complexity in multidimensional linear attack	linear hull effect;linear probability;cryptanalysis;multidimensional linear attack;data complexity;capacity	Multidimensional linear attacks are one of the most powerful variants of linear cryptanalytic techniques now. However, there is no knowledge on the key-dependent capacity and data complexity so far. Their values were assumed to be close to the average value for a vast majority of keys. This assumption is not accurate. In this paper, under a reasonable condition, we explicitly formulate the capacity as a Gamma distribution and the data complexity as an Inverse Gamma distribution, in terms of the average linear probability and the dimension. The capacity distribution is experimentally verified on the 5-round PRESENT. Regarding to complexity, we solve the problem of estimating the average data complexity, which was difficult to estimate because of the existence of zero correlations. We solve the problem of using the median complexity in multidimensional linear attacks, which is an open problem since proposed in Eurocrypt 2011. We also evaluate the difference among the median complexity, the average complexity and a lower bound of the average complexity – the reciprocal of average capacity. In addition, we estimate more accurately the key equivalent hypothesis, and reveal the fact that the average complexity only provides an accurate estimate for less than half of the keys no matter how many linear approximations are involved. Finally, we revisit the so far best attack on PRESENT based on our theoretical result.	approximation;computational complexity theory;cryptanalysis;eurocrypt;experiment	Jialin Huang;Serge Vaudenay;Xuejia Lai;Kaisa Nyberg	2015	IACR Cryptology ePrint Archive	10.1007/978-3-662-47989-6_7	best, worst and average case;cryptanalysis;mathematical optimization;combinatorics;discrete mathematics;average-case complexity;mathematics;algorithm;statistics	Crypto	37.069102623144964	12.389425458791964	126872
8ddde3cf1d09c1878c4746f9fbcf349ab3edce3e	culling a set of points for roundness or cylindricity evaluations	minimum zone circle;surface measurement;cylindricity;metrology;input culling;minimum zone cylinder;linear program;roundness	Roundness and cylindricity evaluations are among the most important problems in computational metrology, and are based on sets of surface measurements (input data points). A recent approach to such evaluations is based on a linear-programming approach yielding a rapidly converging solution. Such a solution is determined by a fixed-size subset of a large input set. With the intent to simplify the main computational task, it appears desirable to cull from the input any point that cannot provably define the solution. In this note we present an analysis and an efficient solution to the problem of culling the input set. For input data points arranged in cross-sections under mild conditions of uniformity, this algorithm runs in linear time. keywords: metrology, minimum zone circle, minimum zone cylinder, roundness, cylindricity, input culling	algorithm;back-face culling;circuit complexity;cylinder seal;data point;linear programming;time complexity	Olivier Devillers;Franco P. Preparata	2003	Int. J. Comput. Geometry Appl.	10.1142/S021819590300113X	linear programming;roundness;mathematics;geometry;metrology	Theory	32.01991723537049	17.270629129639627	127039
3629435103130c8bfac4f163c82abedb230c3840	a branch-and-bound based heuristic algorithm for convex multi-objective minlps		We study convex multi-objective Mixed Integer Non-Linear Programming problems (MINLPs), which are characterized by multiple objective functions and non linearities, features that appear in real-world applications. To derive a good approximated set of non-dominated points for convex multi-objective MINLPs, we propose a heuristic based on a branch-and-bound algorithm. It starts with a set of feasible points, obtained, at the root node of the enumeration tree, by iteratively solving, with an e-constraint method, a single objective model that incorporates the other objective functions as constraints. Lower bounds are derived by optimally solving Non-Linear Programming problems (NLPs). Each leaf node of the enumeration tree corresponds to a convex multi-objective NLP, which is solved heuristically by varying the weights in a weighted sum approach. In order to improve the obtained points and remove dominated ones, a tailored refinement procedure is designed. Although the proposed method makes no assumptions on the number of objective functions nor on the type of the variables, we test it on bi-objective mixed binary problems. In particular, as a proof-of-concept, we tested the proposed heuristic algorithm on instances of a real-world application concerning power generation, and instances of the convex biobjective Non-Linear Knapsack Problem. We compared the obtained results with those derived by well-known scalarization methods, showing the effectiveness of the proposed method.	algorithm;branch and bound;heuristic (computer science)	Valentina Cacchiani;Claudia D'Ambrosio	2017	European Journal of Operational Research	10.1016/j.ejor.2016.10.015	convex analysis;subderivative;mathematical optimization;combinatorics;discrete mathematics;convex combination;mathematics	Robotics	25.27984022514697	11.436898689679785	127103
d808015146dbe1f42eb8e4cf23ed61f561617c71	sir epidemics on random graphs with a fixed degree sequence	random graph;dynamic concentration;epidemic	Let ∆ > 1 be a fixed positive integer. For z ∈ R∆+ let Gz be chosen uniformly at random from the collection of graphs on ‖z‖1n vertices that have zin vertices of degree i for i = 1, . . . ,∆. We determine the likely evolution in continuous time of the SIR model for the spread of an infectious disease on Gz, starting from a single infected node. Either the disease halts after infecting only a small number of nodes, or an epidemic spreads to infect a linear number of nodes. Conditioning on the event that more than a small number of nodes are infected, the epidemic is likely to follow a trajectory given by the solution of an associated system of ordinary differential equations. These results also give the likely number of nodes infected during the course of the epidemic and the likely length in time of the epidemic.	degree (graph theory);random graph;vertex (geometry)	Tom Bohman;Michael Picollelli	2012	Random Struct. Algorithms	10.1002/rsa.20401	random graph;combinatorics;calculus;mathematics;algorithm	Theory	37.796241426343386	16.14969864180762	127114
c666ba3f0ae05522f2d51ae831873aef2b958bf7	accelerating parallel multicriterial optimization methods based on intensive using of search information		Abstract In the present paper, an efficient parallel method for solving complex multicriterial optimization problems, which the optimality criteria can be multiextremal, and the computing of the criteria values can require a large amount of computations in, is proposed. The proposed approach is based on the reduction of the multicriterial problems to the global optimization ones using the minimax convolution of the partial criteria, the dimensionality reduction with the use of the Peano space-filling curves, and the application of the efficient parallel information-statistical global optimization methods. The intensive use of the search information obtained in the course of computations is provided when conducting the computations. The results of the computational experiments demonstrated such an approach to allow reducing the computation costs of solving the multicriterial optimization problems essentially – tens and hundreds times.	program optimization	Victor P. Gergel;Evgeny Kozinov	2017		10.1016/j.procs.2017.05.051	global optimization;mathematical optimization;peano axioms;theoretical computer science;computational complexity theory;computer science;dimensionality reduction;computation;convolution;minimax;optimization problem	HPC	31.024762701880398	6.52847602003734	127435
d2ffaebb5025ddf0c0f3f2dd1f378c44d45a1ede	finding minimal enclosing boxes	geometrie calculatoire;computational geometry;volume;rectangular shape;three dimensional;algorithme;minimo;algorithm;algorritmo;volumen;minimum;pattern recognition;reconnaissance forme;reconocimiento patron;convex hull;forma rectangular;forme rectangulaire	The problem of finding minimal volume boxes circumscribing a given set of three-dimensional points is investigated. It is shown that it is not necessary for a minimum volume box to have any sides flush with a face of the convex hull of the set of points, which makes a naive search problematic. Nevertheless, it is proven that at least two adjacent box sides are flush with edges of the hull, and this characterization enables anO(n 3) algorithm to find all minimal boxes for a set ofn points.	algorithm;allocate-on-flush;convex hull	Joseph O'Rourke	1985	International Journal of Computer & Information Sciences	10.1007/BF00991005	three-dimensional space;mathematical optimization;computational geometry;convex hull;volume	Theory	31.678844782342157	16.346744771955304	127683
9ae22a7fd9b76360eca42f4d2e817a36a12b03ff	an efficient algorithm for minimizing a sum of euclidean norms with applications	optimal solution;location problem;convex programming;efficient algorithm;steiner minimum trees;68q20;polynomial time algorithm;minimizing a sum of euclidean norms;90c25;linear time;polynomial time;shortest networks;90c35;gaussian elimination;facility location problem;68q25;euclidean facilities location;nonlinear optimization;interior point algorithm	In recent years rich theories on polynomial-time interior-point algorithms have been developed. These theories and algorithms can be applied to many nonlinear optimization problems to yield better complexity results for various applications. In this paper, the problem of minimizing a sum of Euclidean norms is studied. This problem is convex but not everywhere differentiable. By transforming the problem into a standard convex programming problem in conic form, we show that an -optimal solution can be computed efficiently using interior-point algorithms. As applications to this problem, polynomial-time algorithms are derived for the Euclidean single facility location problem, the Euclidean multifacility location problem, and the shortest network under a given tree topology. In particular, by solving the Newton equation in linear time using Gaussian elimination on leaves of a tree, we present an algorithm which computes an -optimal solution to the shortest network under a given full Steiner topology interconnecting N regular points, in O(N √ N(log(c̄/ )+ logN)) arithmetic operations where c̄ is the largest pairwise distance among the given points. The previous best-known result on this problem is a graphical algorithm which requires O(N2) arithmetic operations under certain conditions.	algorithm;computation;convex function;convex optimization;degenerate energy levels;facility location problem;gaussian elimination;interior point method;mathematical optimization;newton;nonlinear programming;nonlinear system;polynomial;steiner tree problem;theory;time complexity;tree network	Guoliang Xue;Yinyu Ye	1997	SIAM Journal on Optimization	10.1137/S1052623495288362	time complexity;mathematical optimization;combinatorics;discrete mathematics;convex optimization;nonlinear programming;euclidean shortest path;mathematics	Theory	26.216723757976144	12.763935657481948	127771
83b65b0e65935f3de13d6185377a451e67d30971	bypassing correlation decay for matchings with an application to xorsat	1rsb bypassing correlation decay combinatorial optimization problems sparse graphs correlation decay property cavity method monotonicity properties cavity equations finite graphs unimodular networks theory random xorsat problem one step replica symmetry breaking;optimisation combinatorial mathematics graph theory;equations correlation bipartite graph cavity resonators convergence vectors algorithm design and analysis	Many combinatorial optimization problems on sparse graphs do not exhibit the correlation decay property. In such cases, the cavity method remains a sophisticated heuristic with no rigorous proof. In this paper, we consider the maximum matching problem which is one of the simplest such example. We show that monotonicity properties of the problem allows us to define solutions for the cavity equations. More importantly, we are able to identify the `right' solution of these equations and then to compute the asymptotics for the size of a maximum matching. The results for finite graphs are self-contained. We give references to recent extensions making use of the notion of local weak convergence for graphs and the theory of unimodular networks. As an application, we consider the random XORSAT problem which according to the physics literature has a `one-step replica symmetry breaking' (1RSB) glass phase. We derive new bounds on the satisfiability threshold valid for general graphs (and conjectured to be tight).	combinatorial optimization;heuristic;matching (graph theory);mathematical optimization;sparse matrix;symmetry breaking;unimodular polynomial matrix	Marc Lelarge	2013	2013 IEEE Information Theory Workshop (ITW)	10.1109/ITW.2013.6691322	mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	30.57139125900485	11.50433655632444	127861
0843841efc499df804daafb4eadcfdeda838f7e2	on the exact solution to a smart grid cyber-security analysis problem	2736 bus system exact solution smart grid cyber security analysis problem electric power network false data attack constrained cardinality minimization problem polyhedral combinatoric ieee 118 bus system ieee 300 bus system polish 2383 bus system;operation research;atmospheric measurements;elektroteknik och elektronik;publikationer;polyhedral combinatoric;particle measurements;electrical engineering electronic engineering information engineering;false data attack;exact solution;security operation research optimization methods power network state estimation;konferensbidrag;transmission line measurements vectors indexes security power measurement atmospheric measurements particle measurements;indexes;smart power grids computerised instrumentation power engineering computing scada systems security of data;power engineering computing;vectors;smart power grids;power network state estimation;transmission line measurements;ieee 300 bus system;artiklar;rapporter;electric power network;2736 bus system;ieee 118 bus system;constrained cardinality minimization problem;computerised instrumentation;scada systems;polish 2383 bus system;security;security of data;smart grid cyber security analysis problem;power measurement;optimization methods	This paper considers a smart grid cyber-security problem analyzing the vulnerabilities of electric power networks to false data attacks. The analysis problem is related to a constrained cardinality minimization problem. The main result shows that an relaxation technique provides an exact optimal solution to this cardinality minimization problem. The proposed result is based on a polyhedral combinatorics argument. It is different from well-known results based on mutual coherence and restricted isometry property. The results are illustrated on benchmarks including the IEEE 118-bus, IEEE 300-bus, and the Polish 2383-bus and 2736-bus systems.	benchmark (computing);computer security;lagrangian relaxation;linear programming relaxation;mutual coherence (linear algebra);polyhedron;restricted isometry property	Kin Cheong Sou;Henrik Sandberg;Karl Henrik Johansson	2013	IEEE Transactions on Smart Grid	10.1109/TSG.2012.2230199	database index;embedded system;mathematical optimization;telecommunications;computer science;engineering;electrical engineering;information security;theoretical computer science;computer security	Embedded	27.674374757435494	12.579269008603621	127962
e311b860292d2e5182ef3b1adf2d18d89000947b	a new framework for connected components labeling of binary images	limited work space;computational complexity;connected component	Given a binary image of n pixels, assign integral labels to all pixels so that any background pixel has label 0 and any two foreground pixels have the same positive integral labels, if and only if they belong to the same connected components. This problem is referred to as ’Connected Components Labeling’ and it is one of the most fundamental problems in image processing and analysis. This paper presents a new algorithmic framework for the problem. From an algorithmic point of view, the problem can be solved in O(n) time and O(n) space. We propose new algorithms which use smaller work space without much sacrifice of the running time. More specifically, assuming that an input binary image is given by a read-only array, our algorithm outputs correct labels in the raster order in O(n log n) time using only O( √ n) work space. Some applications of the algorithms are also given.	algorithm;binary image;connected component (graph theory);connected-component labeling;image processing;pixel;read-only memory;time complexity	Tetsuo Asano;Sergey Bereg	2012		10.1007/978-3-642-34732-0_7	combinatorics;discrete mathematics;connected component;computer science;machine learning;mathematics;geometry;computational complexity theory;algorithm	Theory	32.441084098369544	15.44613179274776	128580
33847751a4b8c8324e8f1073b0cc4a834cc3ffbf	characterization of cutoff for reversible markov chains	finite reversible markov chains;maximal inequality;cutoff;trees;mixing time;hitting times	A sequence of Markov chains is said to exhibit (total variation) cutoff if the convergence to stationarity in total variation distance is abrupt. We consider reversible lazy chains. We prove a necessary and sufficient condition for the occurrence of the cutoff phenomena in terms of concentration of hitting time of “worst” (in some sense) sets of stationary measure at least α, for some α ∈ (0, 1). We also give general bounds on the total variation distance of a reversible chain at time t in terms of the probability that some “worst” set of stationary measure at least α was not hit by time t. As an application of our techniques we show that a sequence of lazy Markov chains on finite trees exhibits a cutoff iff the ratio of their relaxation-times and their (lazy) mixing-times tends to 0.	c date and time functions;lazy evaluation;linear programming relaxation;markov chain;stationary process	Riddhipratim Basu;Jonathan Hermon;Yuval Peres	2015		10.1137/1.9781611973730.119	mathematical optimization;combinatorics;discrete mathematics;mathematics;markov chain mixing time;statistics	Theory	38.664209394054794	16.240251229566326	128826
4ba29f4f7e6aee9c40414ff82a0fef99b08259ed	ergodicity, decisions, and partial information		In the simplest sequential decision problem for an ergodic s to hastic processX, at each timen a decisionun is made as a function of past observations X0, . . . ,Xn−1, and a lossl(un,Xn) is incurred. In this setting, it is known that one may choose (under a mild integrability assumption) a decisi on trategy whose pathwise time-average loss is asymptotically smaller than that of ny other strategy. The corresponding problem in the case of partial information pr oves to be much more delicate, however: if the process X is not observable, but decisions must be based on the observation of a different process Y, the existence of pathwise optimal strategies is not guaranteed. The aim of this paper is to exhibit connect ions between pathwise optimal strategies and notions from ergodic theory. The seq uential decision problem is developed in the general setting of an ergodic dynamical s ystem(Ω ,B,P,T) with partial informationY⊆B. The existence of pathwise optimal strategies grounded in two basic properties: the conditional ergodic theory of the dynamical system, and the complexity of the loss function. When the loss function is no t too complex, a general sufficient condition for the existence of pathwise opti mal strategies is that the dynamical system is a conditional K-automorphism relative to the past observations ∨ n≥0T nY. If the conditional ergodicity assumption is strengthened , the complexity assumption can be weakened. Several examples demonstrate t he interplay between complexity and ergodicity, which does not arise in the case o f full information. Our results also yield a decision-theoretic characterization of weak mixing in ergodic theory, and establish pathwise optimality of ergodic nonli near filters.	complexity;decision problem;dynamical system;ergodic theory;ergodicity;kolmogorov automorphism;loss function;mixing (mathematics);observable	Ramon van Handel	2012	CoRR	10.1007/978-3-319-11970-0_18	mathematical optimization;combinatorics;discrete mathematics;ergodicity;stationary ergodic process;mathematics	ML	37.22510143464709	5.188896329213971	129190
0937bbffcfae0118d18502adf66fc610b333fb24	gradient-based algorithms for finding nash equilibria in extensive form games	gradient method;convex optimization;nash equilibria;imperfect information;first order;saddle point	We present a computational approach to the saddle-point formulation for the Nash equilibria of two-person, zerosum sequential games of imperfect information. The algorithm is a first-order gradient method based on modern smoothing techniques for non-smooth convex optimization. The algorithm requires O(1/ ) iterations to compute an -equilibrium, and the work per iteration is extremely low. These features enable us to find approximate Nash equilibria for sequential games with a tree representation of about 10 nodes. This is three orders of magnitude larger than what previous algorithms can handle. We present two heuristic improvements to the basic algorithm and demonstrate their efficacy on a range of real-world games. Furthermore, we demonstrate how the algorithm can be customized to a specific class of problems with enormous memory savings.	approximation algorithm;convex optimization;first-order predicate;general-purpose modeling;gradient method;heuristic;heuristic (computer science);interior point method;iteration;linear programming;mathematical optimization;matrix representation;nash equilibrium;parallel computing;rate of convergence;requirement;smoothing;world games	Andrew Gilpin;Samid Hoda;Javier Peña;Tuomas Sandholm	2007		10.1007/978-3-540-77105-0_9	epsilon-equilibrium;mathematical optimization;combinatorics;convex optimization;economics;computer science;gradient method;perfect information;first-order logic;mathematics;saddle point;mathematical economics;nash equilibrium	ECom	31.8884869915999	6.822529668179731	129673
2b64c66bcb1b9733ed15312c3b4b02ba3227b3c3	using underapproximations for sparse nonnegative matrix factorization	explotacion texto;metodo relajacion;institutional repositories;methode recursive;filtering;evaluation performance;traitement texte;filtrage;matrix factorization;graphe biparti;fedora;performance evaluation;image processing;analisis datos;text mining;matriz no negativa;grafo bipartido;producto matriz;evaluacion prestacion;filtrado;metodo recursivo;procesamiento imagen;recursive method;nonnegative matrix;fouille texte;non negative matrix;problema np duro;lagrange multiplier;methode relaxation;traitement image;vital;nonnegative matrices;factorisation matricielle;upper bound;methode matricielle;np hard problem;data analysis;microarray data analysis;matrice creuse;sparsity;underapproximation;collaborative filtering;probleme np difficile;nonnegative matrix factorization;methode lagrange;relaxation method;metodo lagrange;multiplicateur lagrange;matrix method;metodo matriz;tratamiento textos;multiplicador lagrange;representacion parsimoniosa;lagrangian method;analyse donnee;matrice non negative;maximum edge biclique problem;sparse matrix;data analysis techniques;vtls;produit matrice;sparse representation;bipartite graph;factorizacion matricial;matrix product;ils;word processing;lagrangian relaxation;matriz dispersa;representation parcimonieuse	Nonnegative Matrix Factorization consists in (approximately) factorizing a nonnegative data matrix by the product of two low-rank nonnegative matrices. It has been successfully applied as a data analysis technique in numerous domains, e.g., text mining, image processing, microarray data analysis, collaborative filtering, etc. We introduce a novel approach to solve NMF problems, based on the use of an underapproximation technique, and show its effectiveness to obtain sparse solutions. This approach, based on Lagrangian relaxation, allows the resolution of NMF problems in a recursive fashion. We also prove that the underapproximation problem is NP-hard for any fixed factorization rank, using a reduction of the maximum edge biclique problem in bipartite graphs. We test two variants of our underapproximation approach on several standard image datasets and show that they provide sparse part-based representations with low reconstruction error. Our results are comparable and sometimes superior to those obtained by two standard Sparse Nonnegative Matrix Factorization techniques.	approximation algorithm;approximation error;collaborative filtering;greedy algorithm;image processing;lagrangian relaxation;linear programming relaxation;microarray;np-hardness;non-negative matrix factorization;recursion;sparse matrix;text mining;turing completeness;video post-processing	Nicolas Gillis;François Glineur	2010	Pattern Recognition	10.1016/j.patcog.2009.11.013	mathematical optimization;combinatorics;text mining;image processing;computer science;machine learning;calculus;mathematics;data analysis	ML	35.12503679975696	10.198009590905627	129701
ae56a694e9f567a29715940adbfa2c684e6df086	fast fully parallel thinning algorithms		Three new fast fully parallel 2-D thinning algorithms using reduction operators with 11-pixel supports are presented and evaluated. These are compared to earlier fully parallel thinning algorithms in tests on artificial and natural images; the new algorithms produce either superior parallel computation time (number of parallel iterations) or thinner medial curve results with comparable parallel computation time. Further, estimates of the best possible parallel computation time are developed which are applied to the specific test sets used. The parallel computation times of the new algorithms and one earlier algorithm are shown to approach closely or surpass these estimates and are in this sense near optimally fast. B 1992 Academic Press, Inc.	algorithm;computation;h2 database engine;iteration;medial graph;parallel computing;performance;pixel;test case;thinning;time complexity;year 10,000 problem	Zicheng Guo;Richard W. Hall	1992	CVGIP: Image Understanding	10.1016/1049-9660(92)90029-3	mathematical optimization;parallel computing;computer science;theoretical computer science;analysis of parallel algorithms;cost efficiency	HPC	32.78673016293816	15.645978106976079	129948
ed9f42a9672f4b4aec635010dbaf6ae861bf723a	the symbolic obdd algorithm for finding optimal semi-matching in bipartite graphs	semi matching;bipartite graphs;ordered binary decision diagram;load balancing	The optimal semi-matching problem is one relaxing form of the maximum cardinality matching problems in bipartite graphs, and finds its applications in load balancing. Ordered binary decision diagram (OBDD) is a canonical form to represent and manipulate Boolean functions efficiently. OBDD-based symbolic algorithms appear to give improved results for large-scale combinatorial optimization problems by searching nodes and edges implicitly. We present novel symbolic OBDD formulation and algorithm for the optimal semimatching problem in bipartite graphs. The symbolic algorithm is initialized by heuristic searching initial matching and then iterates through generating residual network, building layered network, backward traversing node-disjoint augmenting paths, and updating semi-matching. It does not require explicit enumeration of the nodes and edges, and therefore can handle many complex executions in each step. Our simulations show that symbolic algorithm has better performance, especially on dense and large graphs.	algorithm;binary decision diagram;combinatorial optimization;flow network;graph (discrete mathematics);heuristic;influence diagram;load balancing (computing);matching (graph theory);mathematical optimization;semiconductor industry;simulation	Tianlong Gu;Liang Chang;Zhoubo Xu	2011	Communications and Network	10.4236/cn.2011.32009	bipartite graph;computer science;3-dimensional matching;load balancing;theoretical computer science;hopcroft–karp algorithm;computer security;matching	Theory	28.03658640433592	9.463383315933385	130003
ec2855f565090e448f38a5a04b95e835f315a80e	inverse center location problems	location problem;time complexity;combinatorial algorithm;cost model;tree height	Abstract   We investigate the inverse 1-center location problem on trees and outline combinatorial algorithms with time complexity  O ( n  2 ) in case that the topology of the tree does not change. In the uniform cost model an improved running time of  O ( n  log  n ) can be obtained. If topology changes occur, the complexity increases by a factor bounded by  n . This improves earlier results of Yang and Zhang.		Rainer E. Burkard;Behrooz Alizadeh	2010	Electronic Notes in Discrete Mathematics	10.1016/j.endm.2010.05.014	time complexity;mathematical optimization;combinatorics;discrete mathematics;mathematics;algorithm	Theory	27.950089510187798	17.45090183037749	130008
564bd7d86621ba639e0f94761cf45f1c29e068f1	some remarks on greedy algorithms	greedy algorithm	There has recently been much interest in approximation by linear combinations of functions taken from a redundant set 79. That is, the elements of 79 are not linearly independent. Perhaps the first example of this type was considered by Schmidt in 1907 [6] who considered the approximation of functions f ( x , y ) of two variables by bilinear forms ~-~tfl=lUi(X)Vi(y ) in L2([0,112). This problem is closely connected with properties of the integral operator with ke rne l f (x , y). We mention two other prominent examples of this type of approximation. In neural networks, one approximates functions of d-variables by linear combinations of functions from the set	artificial neural network;bilinear filtering;greedy algorithm;linear approximation;schmidt decomposition	Ronald A. DeVore;Vladimir N. Temlyakov	1996	Adv. Comput. Math.	10.1007/BF02124742	greedy randomized adaptive search procedure;mathematical optimization;combinatorics;greedy algorithm;machine learning;mathematics;weighted matroid;greedy coloring	Theory	27.86378464070133	13.060852848034171	130038
e54ef293fd2507d571b98b127d381983ae1dc350	primal simplex network codes: state-of-the-art implementation technology	network coding	Abstract#R##N##R##N#In recent years there have been several extremely successful specialization of the primal simplex method for solving network flow problems. Much of this success is due to the development of highly efficient computational techniques for implementing the primal simplex algorithm. We view these efficient techniques as a new body of knowledge which we call implementation technology. This exposition presents the state-of-the-art of implementation technology.	code	Agha Iqbal Ali;Richard V. Helgason;Jeffery L. Kennington;H. S. Lall	1978	Networks	10.1002/net.3230080405	mathematical optimization;linear network coding;computer science;theoretical computer science;mathematics;algorithm;computer network	Networks	30.899386637101212	7.7793833568724855	130209
84af8ec237fbe21787e04a59e76697c8100ecc94	notes on learning probabilistic automata	probabilistic automata	"""Alberto Apostolico y Probabilistic models of various classes of sources are developed in the context of coding and compression as well as in machine learning and classi cation. In the rst domain, the repetitive structures of substrings are regarded as redundancies and sought to be removed. In the second, repeated subpatterns are unveiled as carriers of information and structure. In both contexts, one rather pervasive problem is that of learning or estimating probabilities from the observed strings. For most probabilistic models, such a task poses interesting algorithmic questions (cf., e.g., the references). A popular approach to the statistical modeling of sequences relies on the structure of uniform, xed-memory Markov models. For sequences in important families, the autocorrelation or \memory"""" exhibited decays exponentially fast with length. In other words, there is a maximum length L of the recent history of a sequence, above which the empirical probability distribution of next symbol given the the last L > L symbols does not change appreciably. It is possible and customary to model these sources by Markov chains of order L, this maximum useful memory length. Even so, such automata tend to be in practice unnecessarily bulky and computationally imposing both during their synthesis and use. In [6], much more compact, tree-shaped variants of probabilistic automata are built which assume an underlying Markov process of variable memory length not exceeding some maximum L. The probability distributions generated by these automata is equivalent to that of a Markov chain of order L, but the description of the automaton itself is much more succinct. The process of learning the automaton from a given training set S of sequences requires (Ln) worst-case time, where n is the total length of the sequences in S and L is the length of a longest substring of S to be considered for a candidate state in the automaton. Once the automaton is built, predicting the likelihood of a query sequence of m characters may cost time (m) in the worst case. This work introduces automata equivalent to PSTs that can be learned in O(n) time, and also discusses notions of empirical probability and their e cient computation. Details of the learning procedure and of a linear time classi er or parser may be found in [2, 3]."""	autocorrelation;best, worst and average case;computation;integrated circuit layout design protection;machine learning;markov chain;markov model;pervasive informatics;probabilistic automaton;statistical model;substring;test set;time complexity	Alberto Apostolico	2000		10.1109/DCC.2000.10002	probabilistic ctl;growcut algorithm;computer science;nested word;probabilistic automaton;machine learning;automata theory;mathematics	Theory	37.31791128005056	7.544403551480225	130321
b4611aadfb3d08a85a220e2b061a2c22c03b6bbb	algorithm 872: parallel 2d constrained delaunay mesh generation	automatic mesh generation;algoritmo paralelo;triangulacion delaunay;parallel algorithm;delaunay triangulation;theory delaunay triangulation;generation automatique maille;heterogeneous cluster;two dimensions;triangulation delaunay;performance;generation maille;tetrahedral shape;algorithme parallele;refinement method;generacion automatica red;forme tetraedrique;parallel refinement;communication cost;algorithms;design;methode raffinement;mesh generation;metodo afinamiento;tetrahedral mesh;forma tetraedrica	Delaunay refinement is a widely used method for the construction of guaranteed quality triangular and tetrahedral meshes. We present an algorithm and a software for the parallel constrained Delaunay mesh generation in two dimensions. Our approach is based on the decomposition of the original mesh generation problem into N smaller subproblems which are meshed in parallel. The parallel algorithm is asynchronous with small messages which can be aggregated and exhibits low communication costs. On a heterogeneous cluster of more than 100 processors our implementation can generate over one billion triangles in less than 3 minutes, while the single-node performance is comparable to that of the fastest to our knowledge sequential guaranteed quality Delaunay meshing library (the Triangle).	asynchronous i/o;central processing unit;delaunay triangulation;fastest;mesh generation;parallel algorithm;refinement (computing);ruppert's algorithm	Andrey N. Chernikov;Nikos Chrisochoides	2008	ACM Trans. Math. Softw.	10.1145/1322436.1322442	mesh generation;design;delaunay triangulation;performance;ruppert's algorithm;mathematics;geometry;parallel algorithm;constrained delaunay triangulation;chew's second algorithm;bowyer–watson algorithm;algorithm	HPC	33.37799362255792	15.667675874376666	130456
046212b320189165b11a9ab3923119175f8ff80f	a random graph model for power law graphs	random graph;random graph model;satisfiability;degree sequence;growth rate;power law	We propose a random graph model which is a special case of sparse random graphs with given degree sequences which satisfy a power law. This model involves only a small number of parameters, called logsize and log-log growth rate. These parameters capture some universal characteristics of massive graphs. Furthermore, from these parameters, various properties of the graph can be derived. For example, for certain ranges of the parameters, we will compute the expected distribution of the sizes of the connected components which almost surely occur with high probability. We will illustrate the consistency of our model with the behavior of some massive graphs derived from data in telecommunications. We will also discuss the threshold function, the giant component, and the evolution of random graphs in this model.	connected component (graph theory);degree (graph theory);giant component;graph (discrete mathematics);random graph;sparse matrix;with high probability	William Aiello;Fan Chung Graham;Linyuan Lu	2001	Experimental Mathematics	10.1080/10586458.2001.10504428	1-planar graph;block graph;random regular graph;pathwidth;random graph;power law;split graph;combinatorics;discrete mathematics;cograph;universal graph;null model;graph product;pancyclic graph;forbidden graph characterization;comparability graph;mathematics;voltage graph;modular decomposition;partial k-tree;chordal graph;indifference graph;line graph;satisfiability	Theory	37.79450939018495	15.28967792525992	131195
429ce727d269108ecb0bf5225ffb5a4fa7eed47d	the effectiveness of lloyd-type methods for the k-means problem	randomized algorithms;approximation algorithms	We investigate variants of Lloyd's heuristic for clustering high-dimensional data in an attempt to explain its popularity (a half century after its introduction) among practitioners, and in order to suggest improvements in its application. We propose and justify a clusterability criterion for data sets. We present variants of Lloyd's heuristic that quickly lead to provably near-optimal clustering solutions when applied to well-clusterable instances. This is the first performance guarantee for a variant of Lloyd's heuristic. The provision of a guarantee on output quality does not come at the expense of speed: some of our algorithms are candidates for being faster in practice than currently used variants of Lloyd's method. In addition, our other algorithms are faster on well-clusterable instances than recently proposed approximation algorithms, while maintaining similar guarantees on clustering quality. Our main algorithmic contribution is a novel probabilistic seeding process for the starting configuration of a Lloyd-type iteration.	approximation algorithm;cluster analysis;clustering high-dimensional data;computer cluster;heuristic;iteration;k-means clustering;lloyd's algorithm	Rafail Ostrovsky;Yuval Rabani;Leonard J. Schulman;Chaitanya Swamy	2006	2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS'06)	10.1145/2395116.2395117	mathematical optimization;combinatorics;computer science;data mining;mathematics;randomized algorithm;approximation algorithm;algorithm;statistics	Theory	25.087615196518676	4.395680138926294	131325
765c70b3ffe9efc13ddae2e220f4882dac8b0fe7	a note on the equivalence of the set covering and process network synthesis problems	qa75 electronic computers computer science szamitastechnika;szamitogeptudomany;process network synthesis;set cover		integrated telecom technology;network synthesis filters;turing completeness	Balázs Imreh;János Fülöp;Ferenc Friedler	2000	Acta Cybern.		combinatorics;discrete mathematics;covering problems;computer science;mathematics;set cover problem;algorithm	Theory	27.1437301628536	11.606771805974361	131462
2b8c64e9967e9a6eda67922e491def748815a10d	local optimization-based untangling algorithms for quadrilateral meshes		The generation of a valid computational mesh is an essential step in the solution of many complex scientiic and engineering applications. In this paper we present a new, robust algorithm, and several variants, for untangling invalid quadrilateral meshes. The primary computational aspect of the algorithm is the solution of a sequence of local linear programs, one for each interior mesh vertex. We show that the optimal solution to these local subproblems can be guaranteed and found eeciently. We present experimental results showing the eeectiveness of this approach for problems where invalid, or negative area, elements can arise near highly concave domain boundaries.	algorithm;computation;concave function;linear programming;local search (optimization)	Lori A. Diachin;Paul E. Plassmann	2001			local search (optimization);mathematical optimization;linear programming;quadrilateral;polygon mesh;mathematics	Theory	28.743833353624492	10.648352653465084	131637
f35624a50bad5122834724f53d17a52200f89874	a method for the solution of transportation problems with tall matrices	transportation problem	_J the activity and node tables will be constructed as shown in Figure 8. The forward scan begins wil,h node 1 placed in the Z-list; however, when the successor of node 1 is processed, and nod(, ] is deleted from the Z-list, the Z-list becomes empty. A~ ~his point the loop identification routine, described above, is invoked. A search of the node table places node 2 in the L-list. During succeeding cycles, the L-list grows as follows:		Paul S. Dwyer;Bernard A. Galler	1967	Commun. ACM	10.1145/363242.363266	transportation theory;computer science	AI	26.78081358508723	17.217054135513532	132059
5afec085bfd42dbd57d372b31742f0e60f0030ca	cell packing structures	sphere packing;fabrication aware design;architectural geometry;polyhedral packing;cell packing;offset mesh;polyhedral mesh;spatial tiling;torsion free support structure;shading system	This paper is an overview of architectural structures which are composed of polyhedral cells or closely related to them. We introduce the concept of a support structure of such a polyhedral cell packing. It is formed by planar quads and obtained by connecting corresponding vertices in two combinatorially equivalent meshes whose corresponding edges are coplanar and thus determine planar quads. Since corresponding triangle meshes only yield trivial structures, we focus on support structures associated with quad or hexdominant meshes. For the quadrilateral case, we provide a short survey of recent research which reveals the beautiful relations to discrete differential geometry. Those are essential for successfully initializing numerical optimization schemes for their computation. Hex-dominant structures may be designed via Voronoi tesselations, power diagrams, sphere packings and various extensions of these concepts. Apart from the obvious application as load bearing structures, we illustrate here a new application to systems for shading and indirect lighting. On a higher level, our work emphasizes the interplay between geometry, optimization, statics, and manufacturing, with the overall aim of combining form, function and fabrication into novel integrated design tools.	cell (microprocessor);computation;energy minimization;mathematical optimization;polygon mesh;polyhedron;set packing;shading;triangle mesh;voronoi diagram	Helmut Pottmann;Caigui Jiang;Mathias Höbinger;Philippe Bompas;Johannes Wallner	2015	Computer-Aided Design	10.1016/j.cad.2014.02.009	combinatorics;architectural geometry;mathematics;geometry;sphere packing;engineering drawing	EDA	35.05065971630481	16.1896705109601	132390
4e95aacf5deab1ec6a3403ee50ef9f60f314b012	on lower bounds for optimal jacobian accumulation		The task of finding a way to compute by using the minimal number of multiplications is generally referred to as the Optimal Jacobian Accumulation () problem. Often a special case of the problem is examined, where the accumulation of the Jacobian is considered as a transformation of the linearized directed acyclic graph (l-DAG) G into , such that is a subgraph of the complete directed bipartite graph . The transformation of the l-DAG is performed by elimination methods. The most commonly used elimination methods are vertex elimination and edge elimination. The problem of transforming an l-DAG into a bipartite graph by vertex or edge eliminations with minimal costs is generally referred as the Vertex Elimination () or Edge Elimination () problem. Both the and problems are conjectured to be NP-hard. Thus Branch and bound based algorithms in addition to greedy heuristics are used to solve these problems. For efficient application of such algorithms, good lower bounds are essential. In this paper, we develop a...	jacobian matrix and determinant;tree accumulation	Viktor Mosenkis;Uwe Naumann	2018	Optimization Methods and Software	10.1080/10556788.2017.1397145	jacobian matrix and determinant;discrete mathematics;mathematical optimization;directed acyclic graph;vertex (geometry);mathematics;heuristics;special case;bipartite graph;branch and bound	ML	28.7392789449418	13.359576324917034	133105
3a859de23453bca9b524c40c42a1ce624cc92af4	on the distribution of random walk hitting times in random trees		The hitting time Hxy between two vertices x and y of a graph is the average time that the standard simple random walk takes to get from x to y. In this paper, we study the distribution of the hitting time between two randomly chosen vertices of a random tree. We consider both uniformly random labelled trees and a more general model with vertex weights akin to simply generated trees. We show that the r-th moment of the hitting time is of asymptotic order n3r/2 in trees of order n, and we describe the limiting distribution upon normalisation by means of its moments. Moreover, we also obtain joint moments with the distance between the two selected vertices. Finally, we discuss a somewhat different model of randomness, namely random recursive trees. In this setup, the root is of special importance, and so we study the hitting time from the root to a random vertex or from a random vertex to the root. Interestingly, the hitting time from the root is of order n log n, with a normal limit law, while the hitting time to the root is only of linear order and has a non-Gaussian limit law.	fitts's law;random graph;random tree;randomness;recursion;time complexity;vertex (geometry)	Joubert Oosthuizen;Stephan N Wagner	2017		10.1137/1.9781611974775.7	statistics;random walk;mathematics	Theory	38.82280149268367	16.405732232197195	133214
9f29c4df914478e6b14a07e2f83860b7ba01be82	the information geometry of bregman divergences and some applications in multi-expert reasoning	discrete probability function;multi expert reasoning;bregman divergence;information geometry;probabilistic merging;pooling operator	The aim of this paper is to develop a comprehensive study of the geometry involved in combining Bregman divergences with pooling operators over closed convex sets in a discrete probabilistic space. A particular connection we develop leads to an iterative procedure, which is similar to the alternating projection procedure by Csiszár and Tusnády. Although such iterative procedures are well studied over much more general spaces than the one we consider, only a few authors have investigated combining projections with pooling operators. We aspire to achieve here a comprehensive study of such a combination. Besides, pooling operators combining the opinions of several rational experts allows us to discuss possible applications in multi-expert reasoning.	apevia;bregman divergence;information geometry;iterative method	Martin Adamcík	2014	Entropy	10.3390/e16126338	mathematical optimization;combinatorics;machine learning;mathematics;information geometry;statistics;bregman divergence	AI	36.33404733794012	9.204446106598697	133438
e1420dfbb6c236dbc53d1973624d24c03556d10e	algorithm for approximate solution of the generalized weber problem with an arbitrary metric	parallel computing;optimisation;discrete optimization;probability;parallel computing discrete optimization weber problem;boolean algebra;weber problem;continuous coordinate values approximate solution generalized weber problem arbitrary metric multifacility weber location problem pseudo boolean optimization problem heuristic random search anytime algorithm fermat weber problem unconstrained problem single facility problem euclidean metric problem planar multifacility weber problem noneuclidean distances probability changing method;search problems boolean algebra facility location optimisation probability;search problems;europe;facility location	The multi-facility Weber location problem is transformed into a pseudo-Boolean optimization problem and solved with use of a heuristic random search anytime-algorithm. Fermat-Weber problem in its simplest form (unconstrained, single facility, Euclidean metric) is well investigated. A lot of algorithms are developed for more complex cases. However, the generalized multi-facility problem with barriers, restricted zones and arbitrary metric has no well-known algorithm for its solving. In this report, we consider the planar multi-facility Weber problem with restricted zones and non-Euclidean distances, propose an algorithm based on the probability changing method and prove its efficiency for approximate solving this problem by replacing the continuous coordinate values by discrete ones. Version of the algorithm for multiprocessor systems is proposed. An example of a problem solution is given.	anytime algorithm;approximation algorithm;euclidean distance;fermat;heuristic;mathematical optimization;multiprocessing;optimization problem;random search;weber problem	Lev Kazakovtsev	2012	2012 Sixth UKSim/AMSS European Symposium on Computer Modeling and Simulation	10.1109/EMS.2012.52	computational problem;optimization problem;mathematical optimization;combinatorics;discrete mathematics;counting problem;generalized assignment problem;cutting stock problem;facility location problem;p versus np problem;mathematics;1-center problem	AI	26.39926422703451	15.245421752106143	133982
96d7a650df9d9732a43754dcb1d8e2d84b52a259	analyzing probabilistic models in hierarchical boa	bayes estimation;spin glass;modelizacion;belief networks;hierarchical system;optimisation;2d ising spin glasses;condiciones limites;probability;complex probability distributions hierarchical bayesian optimization algorithm concatenated traps random additively decomposable problems hierarchical traps 2d ising spin glasses;modelo ising;optimizacion;loi probabilite;condition aux limites;ley probabilidad;boundary conditions;probabilistic model estimation of distribution algorithms hierarchical boa model structure model complexity;echantillonnage;model structure;systeme hierarchise;government;modele ising;complex probability distributions;bayesian methods;useful information;informacion util;testing;probabilistic approach;periodic boundary condition;random additively decomposable problems;glass;model complexity;sampling;optimization problem;modelisation;probabilistic model;concatenated codes government bayesian methods robustness testing glass boundary conditions information analysis computer science laboratories;sistema jerarquizado;estimacion bayes;estimation of distribution algorithm;mathematical programming;boundary condition;enfoque probabilista;approche probabiliste;probability distribution;ising model;algorithme evolutionniste;robustness;algoritmo evolucionista;optimization;concatenated codes;computer science;evolutionary algorithm;verre spin;probability belief networks optimisation;muestreo;hierarchical bayesian optimization algorithm;modeling;programmation mathematique;information analysis;estimation of distribution algorithms;programacion matematica;hierarchical boa;concatenated traps;information utile;estimation bayes;hierarchical traps;vidrio spin	The hierarchical Bayesian optimization algorithm (hBOA) can solve nearly decomposable and hierarchical problems of bounded difficulty in a robust and scalable manner by building and sampling probabilistic models of promising solutions. This paper analyzes probabilistic models in hBOA on four important classes of test problems: concatenated traps, random additively decomposable problems, hierarchical traps and two-dimensional Ising spin glasses with periodic boundary conditions. We argue that although the probabilistic models in hBOA can encode complex probability distributions, analyzing these models is relatively straightforward and the results of such analyses may provide practitioners with useful information about their problems. The results show that the probabilistic models in hBOA closely correspond to the structure of the underlying optimization problem, the models do not change significantly in consequent iterations of BOA, and creating adequate probabilistic models by hand is not straightforward even with complete knowledge of the optimization problem.	algorithm;bayesian optimization;concatenation;encode;ising model;iteration;mathematical optimization;optimization problem;periodic boundary conditions;sampling (signal processing);scalability	Mark Hauschild;Martin Pelikan;Kumara Sastry;Cláudio F. Lima	2009	IEEE Transactions on Evolutionary Computation	10.1109/TEVC.2008.2004423	mathematical optimization;combinatorics;estimation of distribution algorithm;boundary value problem;computer science;artificial intelligence;machine learning;evolutionary algorithm;mathematics;algorithm;statistics	ML	36.319455915375876	7.11640623949388	134655
4c9c2b41fc5288f60e107cf345f33e548221c699	combinatorial optimization in vlsi design		VLSI design is probably the most fascinating application area of combinatorial optimization. Virtually all classical combinatorial optimization problems, and many new ones, occur naturally as subtasks. Due to the rapid technological development and major theoretical advances the mathematics of VLSI design has changed significantly over the last ten to twenty years. This survey paper gives an up-to-date account on the key problems in layout and timing closure. It also presents the main mathematical ideas used in a set of algorithms called BonnTools, which are used to design many of the most complex integrated circuits in industry.	algorithm;combinatorial optimization;integrated circuit;mathematical optimization;timing closure;very-large-scale integration	Stephan Held;Bernhard Korte;Dieter Rautenbach;Jens Vygen	2011		10.3233/978-1-60750-718-5-33	theoretical computer science;very-large-scale integration;combinatorial optimization;mathematics	EDA	28.112360411326534	7.787654322925322	134977
bacd320c699977406f5ecad6c2559ba0f89269c4	on the gap between the quadratic integer programming problem and its semidefinite relaxation	desigualdad matricial lineal;quadratic programming;programacion entera;cost function;programmation quadratique;matrice diagonale;programmation en nombres entiers;upper bound;relaxation semidefinie;linear matrix inequality;matriz diagonal;integer programming;mathematical programming;funcion matricial;matrix function;polynomial time;arreglo;fonction matricielle;programacion cuadratica;inegalite matricielle lineaire;hyperplane arrangements;hiperplano;arrangement;relajacion semidefinida;integer program;quadratic integer programming;hyperplane;programmation mathematique;programacion matematica;semidefinite relaxation;hyperplane arrangement;linear matrix inequalities;hyperplan;zonotopes;diagonal matrix	Consider the semidefinite relaxation (SDR) of the quadratic integer program (QIP): γ := max {xT Qx : x ∈ {−1, 1}n} ≤ min {trace(D) : D − Q 0} =: γ̄ where Q is a given symmetric matrix and D is diagonal. We consider the SDR gap γ̄ − γ . We establish the uniqueness of the SDR solution and prove that γ = γ̄ if and only if γr := n−1 max {xT V V T x : x ∈ {−1, 1}n} = 1 where V is an orthogonal matrix whose columns span the (r–dimensional) null space of D − Q and where D is the unique SDR solution. We also give a test for establishing whether γ = γ̄ that involves 2r−1 function evaluations. In the case that γr < 1 we derive an upper bound on γ which is tighter than γ̄ . Thus we show that ‘breaching’ the SDR gap for the QIP problem is as difficult as the solution of a QIP with the rank of the cost function matrix equal to the dimension of the null space of D − Q. This reduced rank QIP problem has been recently shown to be solvable in polynomial time for fixed r .	column (database);decision problem;etsi satellite digital radio;gap theorem;integer programming;kernel (linear algebra);linear programming relaxation;loss function;maxima and minima;time complexity	U. Malik;Imad M. Jaimoukha;George D. Halikias;S. K. Gungah	2006	Math. Program.	10.1007/s10107-005-0692-2	time complexity;matrix function;mathematical optimization;combinatorics;discrete mathematics;integer programming;linear matrix inequality;hyperplane;mathematics;upper and lower bounds;quadratic programming;diagonal matrix	ML	25.492858842762185	14.47280356902287	135257
324893a59aed1f375127f58591a235570b57088f	multiresolution banded refinement to accelerate surface reconstruction from polygons	surface reconstruction;full resolution	We propose a m&hod for construct.ing a tiling between :J. pair of planar polygons. Our technique uses multiresolut’ion: tilings of lower resolut,ion polygons are used to consbrucb a Cling for the full resolut.ion polygons. The t,ilings are constructed using banded refinement, by restricbed dynamic programming, in roughly linear time and space. By conbrast, t’he opt,imal dynamic programming mebhod requires quadratic time and space. In our empirical study of surface reconstruction of brain contours our algorithm exhibited significant speedup over t.hc opbimal dynamic program, yet nearly always found an opbimal reconst,ruct,ion. Our approach appears t,o be gcneralizable t,o ot’her geometric problems solvable by dynamic programming, and flesible enough to be tuned for varying data set. characteristics.	algorithm;decision problem;dynamic programming;refinement (computing);speedup;tiling window manager;time complexity	James D. Fix;Richard E. Ladner	1998		10.1145/276884.276912	surface reconstruction;mathematics	PL	32.46084920744166	16.261456768514453	135495
a1305a728dffd0a6e1ddbc928dd2a4ba7571a3a5	simultaneous containment of several polygons	simultaneous containment	IINTRODUCTION We investigate the problem of whether one or several Polygons I, . ..lk can be translated to fit inside another polygon E without overlapping each other. The containment problem for one polygon, say I, has been studied Previously by Chazelle (C), Baker, Fortune and Mahaney (BFM) and Fortune (F). Chazelle derived an algorithm that runs in time O(n+m) to solve lhis problem, in the case that both polygons E and I are convex. Here n is the number of edges of E and m the number of edges of I. The case that I is non-convex but E is convex, can be easily reduced to the previous one. The case that E is non-convex but I is convex is considered in @FM) and (F). The best algorithm is obtained by Fortune who presents an O(nm lognm) algorithm. This algorithm is surely close to optimal (in the worst-case sense) since the feasible region may have O(mn) vertices. This paper presents an algorithm that solve the general case where I and E may be non-convex and may have (non-convex) holes. The algorithm provides the set of all the possible placements. Its complexity is O(CiCeNlOgN) where N=cin+cem, Ci=l +number of concave vertices of I, c,=number of convex vertices of E which are not vertices of the convex hull of E plus number of edges of the convex hull of E which are not edges of E This algorithm is close to optimal. in the worst-case where c,=O(n) and ci=O(m), since the feasible region may have O(n2m2) vertices. In the case of rectilinear polygons the algorithm runs in O(n2m2) time and, thus, is optimal. The problem of coordinating the movements of several disks has been studied by Schwartz and Sharir (SS) and by Yap(Y). Yap has obtained an O(nk) algorithm for the coordination of k disks	algorithm;best, worst and average case;bus functional model;concave function;convex function;convex hull;feasible region;regular grid;vertex (geometry)	Francis Avnaim;Jean-Daniel Boissonnat	1987		10.1145/41958.41984	combinatorics;discrete mathematics;polygon;computer science	Theory	28.151923313479656	18.207020094722512	135756
adf67e5aac5b689c57212562934964236cbc2a32	improved analysis of greedy mapping	graph theory;shortest path;path planning mobile robots graph theory;terrain mapping robot sensing systems mobile robots upper bound robustness educational institutions search methods;graph theory greedy mapping mobile robots travel distance informative location unscanned location unvisited location terrain mapping edge traversals vertices sensor range;mobile robot;path planning;mobile robots;upper bound;upper and lower bounds	We analyze Greedy Mapping, a simple mapping method that has successfully been used on mobile robots. Greedy Mapping moves the robot from its current location on a shortest path towards a closest unvisited, unscanned or informative location, until the terrain is mapped. Previous work has resulted in upper and lower bounds on its worst-case travel distance but there was a large gap between the bounds. In this paper, we reduce the gap substantially by decreasing the upper bound from O(|V |) to O(|V | ln |V |) edge traversals, where |V | is the number of vertices of the graph. This upper bound demonstrates that the travel distance of Greedy Mapping is guaranteed to be small and thus suggests that Greedy Mapping is indeed a reasonable mapping method. The guaranteed good performance of Greedy Mapping is robust in that it holds for different versions of Greedy Mapping, regardless of sensor type	best, worst and average case;depth-first search;greedy algorithm;information;mobile robot;shortest path problem;vertex (geometry)	Craig A. Tovey;Sven Koenig	2003		10.1109/IROS.2003.1249657	mobile robot;mathematical optimization;combinatorics;computer science;artificial intelligence;graph theory;machine learning;mathematics;upper and lower bounds	Robotics	26.94846868241746	17.894757758460955	135785
24bbf993f38f0a9bf3806489d73a656903a77213	rapidly mixing markov chains: a comparison of techniques (a survey)		For many fundamental sampling problems, the best, and often the only known, approach to solving them is to take a long enough random walk on a certain Markov chain and then return the current state of the chain. Techniques to prove how long “long enough” is, i.e., the number of steps in the chain one needs to take in order to be sufficiently close to the stationary distribution of the chain, are of great importance in obtaining estimates of running times of such sampling algorithms. In this report, we survey existing techniques to bound the mixing time of Markov chains. The mixing time of a Markov chain is exactly captured by the “spectral gap” of its underlying transition matrix. The spectral gap is closely related to a geometric parameter called “conductance” which is a measure of the “edge-expansion” of the Markov chain. Conductance also captures the mixing time up to square factors. Lower bounds on conductance, which give upper bounds on the mixing time, are typically obtained by a technique called “canonical paths” where the idea is to find a set of paths, one between every unequal source-destination pair, such that no edge is very heavily congested. Unlike conductance, the canonical paths approach cannot always show rapid mixing of a rapidly mixing chain. It is known that this “drawback” disappears if we allow the flow between a pair of states to be spread along multiple paths. We prove that for a large class of Markov chains, including all the ones that we use in the sampling applications we will be interested in, canonical paths does capture rapid mixing, i.e., we show that small mixing time implies the existence of some collection of paths with low edge congestion. Allowing multiple paths to route the flow still does help a great deal in the design of such flows, and this is best illustrated by a recent result of Morris and Sinclair [34] on the rapid mixing of a natural Markov chain for sampling knapsack solutions; this result seems to rely critically on fractional flows. An entirely different approach to prove rapid mixing, which in fact historically preceded the conductance/canonical paths based approach, is “Coupling”. Coupling is a very elegant technique and has been used to prove rapid mixing of several chains where designing good canonical paths seems to be a hideous task. “Path Coupling” is a related technique discovered by Bubley and Dyer [5] that often tremendously reduces the complexity of designing good Couplings. We present several applications of Path Coupling in proofs of rapid mixing, and these invariably lead to much better bounds on mixing time than known using conductance, and moreover Coupling based proofs usually turn out to be much simpler. These applications motivate the question of whether Coupling indeed can be made to work whenever the chain is rapidly mixing. This question was answered in the negative in very recent work by Kumar and Ramesh [27], who showed that no Coupling strategy can prove the rapid mixing of the famous Jerrum-Sinclair chain for sampling perfect and near-perfect matchings (the chain is known to be rapidly mixing via a canonical paths argument).	algorithm;conductance (graph);coupling (computer programming);markov chain;network congestion;sampling (signal processing);stationary process;stochastic matrix	Venkatesan Guruswami	2000	CoRR		mathematical optimization;combinatorics;discrete mathematics;mathematics;algorithm;statistics	Theory	37.736601179204015	16.18971147398142	135909
6cac9b9b0d79e162258cfb34d2bba057d8b1f1b2	subgraph extraction and metaheuristics for the maximum clique problem	metaheuristics;maximum clique problem;circular arc graph;iterated local search;genetic algorithm;triangulated graph	The maximum clique problem involves finding the largest set of pairwise adjacent vertices in a graph. The problem is classic but still attracts much attention because of its hardness and its prominent applications. Our work is based on the existence of an order of all the vertices whereby those belonging to a maximum clique stay close enough to each other. Such an order can be identified via the extraction of a particular subgraph from the original graph. The problem can consequently be seen as a permutation problem that can be addressed efficiently by metaheuristics. We first design a memetic algorithm (MA) for this purpose. Computational experiments conducted on the DIMACS benchmark instances clearly show that our MA not only outperforms existing genetic approaches, but it also compares very well to state-of-the-art algorithms regarding the maximal clique size found after different runs. Furthermore, we show that a hybridization of MA with an iterated local search (ILS) improves the stability of the algorithm. This hybridization (MA-ILS) permits to find two distinct maximal cliques of size 79 and one of size 80 for the C2000.9 instance of the DIMACS benchmark.	clique (graph theory);clique problem;metaheuristic	Duc-Cuong Dang;Aziz Moukrim	2012	J. Heuristics	10.1007/s10732-012-9207-5	clique;block graph;mathematical optimization;split graph;combinatorics;clique graph;genetic algorithm;independent set;perfect graph;k-tree;computer science;clique problem;artificial intelligence;simplex graph;machine learning;subgraph isomorphism problem;iterated local search;mathematics;induced subgraph isomorphism problem;maximum common subgraph isomorphism problem;metaheuristic	AI	24.934034197649652	5.922350952191248	136045
64d1382ac32fd3b8232155b6e6c45e8bac9702a8	sufficient conditions for deceptive and easy binary functions	satisfiability	This paper finds sufficient conditions for fully or partially deceptive binary functions by calculating schema average fitness values. Deception conditions are first derived for functions of unitation (functions that depend only on the number of 1s in the string) and then extended for any binary function. The analysis is also extended to find a set of sufficient conditions for fully easy binary functions. It is found that the computational effort required to investigate full or partial deception in a problem of sizel using these sufficient conditions isO(2 l ) and using all necessary conditions of deception isO(4 l ). This calculation suggests that these sufficient conditions can be used to quickly test deception in a function. Furthermore, it is found that these conditions may also be systematically used to design a fully deceptive function by performing onlyO(l 2) comparisons and to design a partially deceptive function to orderk by performing onlyO(kl) comparisons. The analysis shows that in the class of functions of unitation satisfying these conditions of deception, an order-k partially deceptive function is also partially deceptive to any lower order. Finally, these sufficient conditions are used to investigate deception in a number of currently-used deceptive problems.	computation	Kalyanmoy Deb;David E. Goldberg	1993	Annals of Mathematics and Artificial Intelligence	10.1007/BF01531277	computer science;artificial intelligence;mathematics;satisfiability	AI	28.10532337147388	4.839207295583151	136421
a7624f559a048c0e2a9ee2c6c3cbbe8e70b34f02	variance balanced circular designs involving sequences of treatments with first and second residuals		Designs involving sequences of treatments (also called changeover, crossover, or repeated measurements designs) balanced for first and second residuals available in literature are usually large even for a moderate number of treatments. Besides, most of these designs require number of periods at least equal to the number of treatments. In this paper a class of variance balanced circular designs for prime or prime power number of treatments, v (= mp + 1; m, a positive integer) using p ( 4, u003c v) periods and only mv experimental units has been proposed. A simple method of analysis of these designs is given and the efficiency factors relative to the orthogonal designs have been tabulated for v 31 and p 4, 12.		V. K. Sharma;Seema Jaggi	2011	MASA	10.3233/MAS-2011-0190	econometrics;operations management;statistics	EDA	31.098603567520485	9.34433910510896	136548
0beb2e2f74675763e9fbca1760ed6fdcd9922daa	what makes some pomdp problems easy to approximate?	polynomial time;state transition	Point-based algorithms have been surprisingly successful in computing approximately optimal solutions for partially observable Markov decision processes (POMDPs) in high dimensional belief spaces. In this work, we seek to understand the belief-space properties that allow some POMDP problems to be approximated efficiently and thus help to explain the point-based algorithms’ success often observed in the experiments. We show that an approximately optimal POMDP solution can be computed in time polynomial in the covering number of a reachable belief space, which is the subset of the belief space reachable from a given belief point. We also show that under the weaker condition of having a small covering number for an optimal reachable space, which is the subset of the belief space reachable under an optimal policy, computing an approximately optimal solution is NP-hard. However, given a suitable set of points that “cover” an optimal reachable space well, an approximate solution can be computed in polynomial time. The covering number highlights several interesting properties that reduce the complexity of POMDP planning in practice, e.g., fully observed state variables, beliefs with sparse support, smooth beliefs, and circulant state-transition matrices.	acoustic radiation force;approximation algorithm;circulant matrix;experiment;fastest;leslie speaker;markov chain;np-hardness;partially observable markov decision process;partially observable system;polynomial;sampling (signal processing);sparse matrix;time complexity	David Hsu;Wee Sun Lee;Nan Rong	2007				ML	36.90038552321806	5.496799967491222	136599
d124589c4dc6e6e2c1b4efcba56bdbea503ea579	circuit generation for efficient projection onto polyhedral sets in first-order methods	complexity theory;design optimization;decision trees complexity theory design optimization computer architecture economic indicators indexes;computer architecture;indexes;model predictive control first order numerical optimization method low cost embedded mpc implementations euclidean projection nontrivial polyhedral constraints polyhedral sets piecewise affine function iteratively traversing binary trees recursive circuit implementations recursion free approach mixed integer linear programming design stage design optimization problem automatic circuit generation problem specific implementations;trees mathematics control system synthesis integer programming iterative methods linear programming predictive control set theory;decision trees;economic indicators	First-order numerical optimization methods are a common choice for low-cost embedded MPC implementations. Their applicability is typically restricted to problems with simple constraints due to the difficulty of Euclidean projection onto more complex feasible sets. However, many practical problems have non-trivial polyhedral constraints. For such polyhedral sets, the projection can be explicitly written as an evaluation of a piecewise affine function. Existing methods evaluate such functions by iteratively traversing binary trees, which leads to small recursive circuit implementations that have a large computational latency. In this paper, we present a recursion-free approach that uses mixed-integer linear programming in the design stage to optimize result reuse. A heuristic is presented to approximately solve the design optimization problem in a practical amount of time. Automatic circuit generation is used to obtain problem-specific implementations that can significantly outperform current reference implementations with only modest increases in circuit size. The resulting projection circuits enable the application of first-order methods to problems with polyhedral constraints while retaining high performance, increasing their range of application.	binary tree;computation;embedded system;experiment;first-order predicate;heuristic;iw engine;integer programming;linear programming;mathematical optimization;multidisciplinary design optimization;numerical analysis;optimization problem;picasa web albums;polyhedron;recursion;reference implementation;solver;speedup;switzerland	Sandro Merkli;Juan Luis Jerez;Alexander Domahidi;Roy S. Smith;Manfred Morari	2015	2015 European Control Conference (ECC)	10.1109/ECC.2015.7331066	mathematical optimization;combinatorics;discrete mathematics;mathematics	EDA	32.586272032775604	8.450046249070772	136681
369dcc4b5f8561ba2d475bacf6b259ee1583bc26	tree search and quantum computation	heuristic;search algorithm;spectrum;tree search;quantum computation;quantum computer;search model;problem solving	Traditional tree search algorithms supply a blueprint for modeling problem solving behaviour. A diverse spectrum of problems can be formulated in terms of tree search. Quantum computation, in particular Grover’s algorithm, has aroused a great deal of interest since it allows for a quadratic speedup to be obtained in search procedures. In this work we consider the impact of incorporating classical search concepts alongside Grover’s algorithm into a hybrid quantum search system. Some of the crucial points examined include: (1) the reverberations of contemplating the use of non-constant branching factors; (2) determining the consequences of incorporating an heuristic perspective into a quantum tree search model.	blueprint;branching factor;heuristic;international symposium on fundamentals of computation theory;point of view (computer hardware company);problem solving;quantum computing;quantum state;search algorithm;speedup;tree traversal	Luís Tarrataca;Andreas Wichert	2011	Quantum Information Processing	10.1007/s11128-010-0212-z	optimal binary search tree;beam search;mathematical optimization;combinatorics;bidirectional search;heuristic;theoretical computer science;mathematics;incremental heuristic search;iterative deepening depth-first search;best-first search;monte carlo tree search;combinatorial search;quantum computer;tree traversal;physics;quantum mechanics;fringe search;depth-first search;guided local search;binary search algorithm;dichotomic search;search algorithm	AI	29.796767141169234	5.927070766349664	136815
b65127adf72e4ee9a07857d7ce1c6be8bf344518	warm-start routines for solving augmented weighted tchebycheff network programs in multiple-objective network programming	initial solution;network flow problems;multiple criteria decision making;interactive procedures;multiple objectives;flow algorithms;network programming;multiple objective programming	Three warm-start routines are developed to find initial basic feasible solutions for augmented weighted Tchebycheff network programs, subproblems derived from multiple-objective network-programming problems. In an interactive solution procedure, a series of augmented weighted Tchebycheff network programs need to be solved sequentially to find representative nondominated solutions. To speed up the solution process using the network structure of the problem, these warm-start routines start the solution process of one augmented weighted Tchebycheff network program from the optimal solution of the previous one. All three warm-start routines use the same strategy but different ways of reducing the number of basic flow variables, or equivalently increasing the number of basic nonflow variables to construct a basic solution. These warm-start routines can be used by any interactive procedures to facilitate the solution process of multiple-objective network-programming problems. A detailed example is presented. A computational experiment is conducted to compare the performance of these warm-start routines. A cold-start routine and NETSIDE, specialized software for solving network problems with side constraints, are also used as references in the experiment. These warm-start routines can save substantial computation time.		Minghe Sun	2005	INFORMS Journal on Computing	10.1287/ijoc.1050.0140	mathematical optimization;computer science;machine learning;computer network programming;algorithm	HPC	26.764101584337737	8.568575927487098	136837
8d53a47ec9b6a2cc8e7baa59661e682ee041e78a	hierarchical representations of collections of small rectangles	tratamiento datos;representation;hierarchical data structure;geographic information system;hierarchized structure;estudio comparativo;multidimensional data structure;circuit vlsi;data processing;structure hierarchisee;traitement donnee;rectangular shape;cartographie;coleccion;very large scale integrated;vlsi design;etude comparative;collection;hierarchical representation;search trees;cartografia;vlsi circuit;estructura datos;comparative study;cartography;structure donnee;circuito vlsi;data structure;estructura jerarquizada;forma rectangular;representacion;forme rectangulaire	A tutorial survey is presented of hierarchical data structures for representing collections of small rectangles. Rectangles are often used as an approximation of shapes for which they serve as the minimum rectilinear enclosing object. They arise in applications in cartography as well as very large-scale integration (VLSI) design rule checking. The different data structures are discussed in terms of how they support the execution of queries involving proximity relations. The focus is on intersection and subset queries. Several types of representations are described. Some are designed for use with the plane-sweep paradigm, which works well for static collections of rectangles. Others are oriented toward dynamic collections. In this case, one representation reduces each rectangle to a point in a higher multidimensional space and treats the problem as one involving point data. The other representation is area based—that is, it depends on the physical extent of each rectangle.	approximation;cartography;data structure;feature learning;hierarchical database model;integrated circuit;logical volume management;programming paradigm;regular grid;sweep line algorithm;very-large-scale integration	Hanan Samet	1988	ACM Comput. Surv.	10.1145/50020.50021	collection;data structure;data processing;computer science;comparative research;geographic information system;very-large-scale integration;programming language;representation;algorithm	PL	30.7577500415171	12.574910861791013	136999
6b61ddfca85a4631ac9cbbc2cba84e928ae7ec8e	performance d'une heuristique d'agrégation optimale bidimensionnelle	serveur institutionnel;archive institutionnelle;open access;archive ouverte unige;cybertheses;institutional repository	This paper discusses the efficiency of Ritschard et al.’s heuristic [RIT 01] for simultaneously determining the optimal aggregation of two categorical variables. The heuristic is a bottom-up algorithm that proceeds successively to the grouping of two categories that maximizes the increase in the selected criteria. The heuristic is compared to the exhaustive scanning of all possible groupings. The simulations reported show that the dramatic reduction in required computation time has a relatively high cost in terms of the percentage of missed optima. They show, nevertheless, that the missed solutions are very close to the global optimum. MOTS-CLÉS :table de contingence, agrégation, association, discrétisation	algorithm;bottom-up parsing;computation;global optimization;heuristic;simulation;time complexity;xslt/muenchian grouping	Gilbert Ritschard	2002			art;art history;cartography	AI	25.071308721976298	7.256822448494802	137914
6477af1b59a4056b946e15a0efe611b290a5b83e	a local search approximation algorithm for k-means clustering	asymptotically efficient approximation algorithm;k-means clustering;approximation factor;data point;practical value;local improvement heuristic;exact polynomial-time algorithm;approximation algorithm;local search approximation algorithm;fixed number;high constant factor;practical approximation algorithm;empirical study;clustering;local search;computational geometry;k means;k means clustering;cluster computing;approximation algorithms	In k-means clustering we are given a set of n data points in d-dimensional space Rd and an integer k, and the problem is to determine a set of k points in Rd, called centers, to minimize the mean squared distance from each data point to its nearest center. No exact polynomial-time algorithms are known for this problem. Although asymptotically efficient approximation algorithms exist, these algorithms are not practical due to the very high constant factors involved. There are many heuristics that are used in practice, but we know of no bounds on their performance.We consider the question of whether there exists a simple and practical approximation algorithm for k-means clustering. We present a local improvement heuristic based on swapping centers in and out. We prove that this yields a (9 + ε)-approximation algorithm. We present an example showing that any approach based on performing a fixed number of swaps achieves an approximation factor of at least (9 - ε) in all sufficiently high dimensions. Thus, our approximation factor is almost tight for algorithms based on performing a fixed number of swaps. To establish the practical value of the heuristic, we present an empirical study that shows that, when combined with Lloyd's algorithm, this heuristic performs quite well in practice.	approximation algorithm;cluster analysis;k-means clustering;local search (optimization)	Tapas Kanungo;David M. Mount;Nathan S. Netanyahu;Christine D. Piatko;Ruth Silverman;Angela Y. Wu	2004	Comput. Geom.	10.1016/j.comgeo.2004.03.003	correlation clustering;mathematical optimization;combinatorics;data stream clustering;k-medians clustering;computational geometry;canopy clustering algorithm;machine learning;cure data clustering algorithm;mathematics;cluster analysis;k-medoids;approximation algorithm;k-means clustering	Theory	26.06689332413637	18.23591706695723	138028
08ca5f38901f06cb9219c91b67c0493253be1bec	shape fitting with outliers	approximate algorithm;approximation;shape fitting;outliers;linear time	Given a set <i>H</i> of <i>n</i> hyperplanes in <i>R<sup>d</sup></i>, we present an algorithm that <i>e</i>-approximates the extent between the top and bottom <i>k</i> levels of the arrangement of <i>H</i> in time <i>O(n + (k/e)<sup>c</sup>)</i>, where <i>c</i> is a constant depending on <i>d</i>. The algorithm relies on computing a subset of <i>H</i> of size.<i>O(k/e<sup>d-1</sup>)</i>, in near linear time, such that the <i>k</i>-level of the arrangement of the subset approximates that of the original arrangement. Using this algorithm, we propose efficient approximation algorithms for shape fitting with outliers for various shapes. This is the first algorithms to handle outliers efficiently for the shape fitting problems considered.	approximation algorithm;time complexity	Sariel Har-Peled;Yusu Wang	2003		10.1145/777792.777798	time complexity;mathematical optimization;outlier;machine learning;approximation;mathematics;algorithm;statistics	Theory	29.066681451234142	18.055488049650734	138153
a47af9ab472a2a49034b5b8bbd15f9eaa62123f0	pareto optimal solutions for smoothed analysts	multi objective optimization;smoothed analysis;objective function;optimization problem;speculative execution;pareto optimal solution;data structure;pareto optimality	"""Consider an optimization problem with n binary variables and d+1 linear objective functions. Each valid solution x ∈{0,1}n gives rise to an objective vector in Rd+1, and one often wants to enumerate the Pareto optima among them. In the worst case there may be exponentially many Pareto optima; however, it was recently shown that in (a generalization of) the smoothed analysis framework, the expected number is polynomial in~n. Unfortunately, the bound obtained had a rather bad dependence on d; roughly ndd. In this paper we show a significantly improved bound of n2d.  Our proof is based on analyzing two algorithms. The first algorithm, on input a Pareto optimal x, outputs a """"testimony"""" containing clues about x's objective vector, x's coordinates, and the region of space B in which x's objective vector lies. The second algorithm can be regarded as a speculative execution of the first --- it can uniquely reconstruct x from the testimony's clues and just some of the probability space's outcomes. The remainder of the probability space's outcomes are just enough to bound the probability that x's objective vector falls into the region B."""	algorithm;best, worst and average case;enumerated type;mathematical optimization;optimization problem;pareto efficiency;polynomial;smoothed analysis;smoothing;speculative execution	Ankur Moitra;Ryan M O'Donnell	2011		10.1145/1993636.1993667	smoothed analysis;optimization problem;mathematical optimization;combinatorics;data structure;computer science;multi-objective optimization;mathematics;lomax distribution;vector optimization;pareto interpolation;statistics;speculative execution	Theory	30.202327557339462	8.59691932803588	139018
6c8869eab627800bc285c919e3db31dfe5fe1397	a second-order cone programming approximation to joint chance-constrained linear programs	optimal value;stochastic linear program;random data;socps problem;stochastic program;joint chance-constrained linear program;probabilistic lot-sizing problem;latter stochastic program;random matrix;original problem;second-order cone programming approximation;approximated stochastic program	We study stochastic linear programs with joint chance constraints, where the random matrix is a special triangular matrix and the random data are assumed to be normally distributed. The problem can be approximated by another stochastic program, whose optimal value is an upper bound of the original problem. The latter stochastic program can be approximated by two second-order cone programming (SOCP) problems [5]. Furthermore, in some cases, the optimal values of the two SOCPs problems provide a lower bound and an upper bound of the approximated stochastic program respectively. Finally, numerical examples with probabilistic lot-sizing problems are given to illustrate the effectiveness of the two approximations.		Jianqiang Cheng;Céline Gicquel;Abdel Lisser	2012		10.1007/978-3-642-32147-4_8	stochastic programming;mathematical optimization;combinatorics;discrete mathematics;second-order cone programming;mathematics	ML	36.24686829756277	5.1943609498929915	139138
304014ae51faa657d7594fbc13068e7d11895845	tighter relaxations for map-mrf inference: a local primal-dual gap based separation algorithm		We propose an efficient and adaptive method for MAP-MRF inference that provides increasingly tighter upper and lower bounds on the optimal objective. Our method starts by solving the first-order LOCAL(G) linear programming relaxation. This is followed by an adaptive tightening of the relaxation where we incrementally add higher-order interactions to enforce proper marginalization over groups of variables. Computing the best interaction to add is an NP-hard problem. We show good solutions to this problem can be readily obtained from “local primal-dual gaps” given the current primal solution and a dual reparameterization vector. This is not only extremely efficient, but in contrast to previous approaches, also allows us to search over prohibitively large sets of candidate interactions to add. We demonstrate the superiority of our approach on MAP-MRF inference problems encountered in computer vision.	algorithm;computer vision;first-order reduction;heuristic;interaction;lagrange multiplier;linear programming relaxation;markov random field;np-hardness;scoring functions for docking	Dhruv Batra;Sebastian Nowozin;Pushmeet Kohli	2011			mathematical optimization;machine learning;mathematics;algorithm	ML	33.077361319162776	6.384845642441968	139244
478d044f9507200658963c4fa4c5e780a4f2b905	a unified approach to box-mengerian hypergraphs	covering;matroid;conference_paper;packing;box total dual integrality;hypergraph;article	LetH = (V, E) be a hypergraph and let A be the E−V incidence matrix. We callH box-Mengerian if the linear system Ax ≥ 1, x ≥ 0 is box-totally dual integral (box-TDI). As it is NP -hard in general to recognize box-Mengerian hypergraphs, a basic theme in combinatorial optimization is to identify such objects associated with various problems. In this paper we show that the so-called ESP (equitable subpartion) property, first introduced by Ding and Zang in their characterization of all graphs with the min-max relation on packing and covering cycles, turns out to be even sufficient for box-Mengerian hypergraphs. We also establish several new classes of box-Mengerian hypergraphs based on ESP property. This approach is of transparent combinatorial nature and hence is fairly easy to work with. MSC 2000 subject classification. Primary: 90C10, 90C27, 90C57. OR/MS subject classification. Primary: Programming/graphs.	combinatorial optimization;esp game;incidence matrix;linear system;mathematical optimization;maxima and minima;set packing	Xujin Chen;Zhibin Chen;Wenan Zang	2010	Math. Oper. Res.	10.1287/moor.1100.0458	matroid;mathematical optimization;combinatorics;discrete mathematics;mathematics;algorithm	Theory	25.2117542894822	14.104649495162668	139567
1c52a63c97cf5d134f48be87ac15cb6591e1fa61	on the matrix-cut rank of polyhedra	projection operator;cutting planes;cutting plane;semidefinite programming;integer hull;valid inequalities;convex hull;rank of polytopes;projection operators;semidefinite program	Lovász and Schrijver (1991) described a semi-definite operator for generating strong valid inequalities for the 0-1 vectors in a prescribed polyhedron. Among their results, they showed that n iterations of the operator are sufficient to generate the convex hull of 0-1 vectors contained in a polyhedron in n-space. We give a simple example, having Chvátal rank 1, that meets this worst case bound of n. We describe another example requiring n iterations even when combining the semi-definite and Gomory-Chvátal operators. This second example is used to show that the standard linear programming relaxation of a k-city traveling salesman problem requires at least bk/8c iterations of the combined operator; this bound is best possible, up to a constant factor, as k + 1 iterations suffice.	alexander schrijver;best, worst and average case;convex hull;gomory–hu tree;iteration;linear programming relaxation;polyhedron;semiconductor industry;the matrix;travelling salesman problem	William J. Cook;Sanjeeb Dash	2001	Math. Oper. Res.	10.1287/moor.26.1.19.10593	mathematical optimization;combinatorics;discrete mathematics;projection;convex hull;mathematics;semidefinite programming;cutting-plane method	Theory	25.233232716007336	14.657336363443655	139622
c45f6fca56f71917f9533685b51c3432923f3d4f	intrinsic complexity of learning geometrical concepts from positive data	learning algorithm;concept geometrique;lower and upper bound;bepress selected works;estrategia;algorithme apprentissage;broken straight lines open semi hulls strategy learning primitive basic strategies;strategy;upper bound;primitive basic strategies;complexity measure;mesure complexite;strategy learning;broken straight lines;upper and lower bounds;learning artificial intelligence;algoritmo aprendizaje;strategie;learning strategies;open semi hulls;medida complexidad;apprentissage intelligence artificielle	Intrinsic complexity is used to measure complexity of learning areas limited by broken-straight lines (called open semi-hulls) and intersections of such areas. Any strategy learning such geometrical concept can be viewed as a sequence of primitive basic strategies. Thus, the length of such a sequence together with complexities of primitive strategies used can be regarded as complexity of learning the concept in question. We obtained best possible lower and upper bounds on learning open semi-hulls, as well as matching upper and lower bounds on complexity of learning intersections of such areas. Surprisingly, upper bounds in both cases turn out to be much lower than those provided by natural learning strategies. Another surprising result is that learning intersections of open semi-hulls (and their complements) turns out to be easier than learning open semi-hulls themselves.	semiconductor industry	Sanjay Jain;Efim B. Kinber	2003	J. Comput. Syst. Sci.	10.1016/S0022-0000(03)00067-9	combinatorics;algorithmic learning theory;artificial intelligence;machine learning;mathematics;upper and lower bounds;algorithm	Theory	33.65675750681547	13.782626048541783	139722
e27fb84fccd3d9724df8ce35fe14149da5de3251	policy gradients with variance related risk criteria		Managing risk in dynamic decision problems is of cardinal importance in many fields such as finance and process control. The most common approach to defining risk is through various variance related criteria such as the Sharpe Ratio or the standard deviation adjusted reward. It is known that optimizing many of the variance related risk criteria is NP-hard. In this paper we devise a framework for local policy gradient style algorithms for reinforcement learning for variance related criteria. Our starting point is a new formula for the variance of the cost-togo in episodic tasks. Using this formula we develop policy gradient algorithms for criteria that involve both the expected cost and the variance of the cost. We prove the convergence of these algorithms to local minima and demonstrate their applicability in a portfolio planning problem.	algorithm;color gradient;control variates;decision problem;experiment;ideographic rapporteur group;local optimum;mathematical optimization;maxima and minima;policy-based design;reinforcement learning;risk aversion;risk measure	Dotan Di Castro;Aviv Tamar;Shie Mannor	2012			mathematical optimization;mathematics;algebraic formula for the variance	ML	35.86696839051633	4.2504128927241975	139959
0df4dd194c4f27f68cd2419359fa0897cb8cfbf4	optimal factorization of three-way binary data using triadic concepts	three-way binary data;factorization;triadic concept analysis;3rd order tensor	We present a new approach to factor analysis of three-way binary data, i.e. data described by a 3-dimensional binary matrix I, describing a relationship between objects, attributes, and conditions. The problem consists in finding a decomposition of I into three binary matrices, an object-factor matrix A, an attribute-factor matrix B, and a condition-factor matrix C, with the number of factors as small as possible. The scenario is similar to that of decomposition-based methods of analysis of three-way data but the difference consists in the composition operator and the constraint on A, B, and C to be binary. We show that triadic concepts of I, developed within formal concept analysis, provide us with optimal decompositions. We present an example demonstrating the usefulness of the decompositions. Since finding optimal decompositions is NP-hard, we propose a greedy algorithm for computing suboptimal decompositions and evaluate its performance.	approximation algorithm;binary data;cartesian closed category;cuboid;data pre-processing;factor analysis;formal concept analysis;greedy algorithm;heuristic (computer science);np-hardness;point of view (computer hardware company);preprocessor	Radim Belohlávek;Cynthia Vera Glodeanu;Vilém Vychodil	2013	Order	10.1007/s11083-012-9254-4	mathematical optimization;combinatorics;discrete mathematics;mathematics;matrix decomposition	AI	26.923979762300604	13.561844813719288	140355
83ea223f7f6c667936bc14065b02dcc49dace3bf	some combinatorial problems on binary rooted trees occurring in population genetics		Models in evolutionary biology are intimately linked to the tree paradigm. Given a direction by time, ancestry relationship between species, individuals, alleles or cells can be depicted as a rooted tree. Of particular interest are binary rooted unordered trees. These can be further classified into shape trees, phylogenetic trees, ranked trees and labelled ranked trees. In this work we want to focus on several combinatorial aspects concerning these classes of trees. We consider numerations and probabilistic properties of these trees when generated under the random coalescent process. We derive several summary statistics which serve to characterize 'typical' trees.		Filippo Disanto;Thomas Wiehe	2011	CoRR		random binary tree;combinatorics;discrete mathematics;tree rearrangement;mathematics;algorithm	Theory	35.74225326209443	14.943424211267153	140361
e163a7fd021dd28fff50b58e3e7fb410b4510c15	shortest paths of bounded curvature in the plane	shortest path;optimal control;motion planning	Given two oriented points in the plane, we determine and compute the shortest paths of bounded curvature joining them. This problem has been solved recently by Dubins in the no-cusp case, and by Reeds and Shepp otherwise. We propose a new solution based on the minimum principle of Pontryagin. Our approach simplifies the proofs and makes clear the global or local nature of the results.		Jean-Daniel Boissonnat;André Cérézo;Juliette Leblond	1994	Journal of Intelligent and Robotic Systems	10.1007/BF01258291	mathematical optimization;topology;optimal control;constrained shortest path first;any-angle path planning;average path length;computer science;pathfinding;artificial intelligence;euclidean shortest path;yen's algorithm;geometry;motion planning;shortest path problem;distance;k shortest path routing;shortest path faster algorithm	Robotics	31.41674752547072	17.604260190281586	140395
cdabb7582397d88bf14f59876a51e25473992c34	on the freezing of variables in random constraint satisfaction problems	spin glass;critical behavior;random graph;structural phase transition;random tree;satisfiability;mean field;message passing;constraint satisfaction problem;information theory	The set of solutions of random constraint satisfaction problems (zero energy groundstates of mean-field diluted spin glasses) undergoes several structural phase transitions as the amount of constraints is increased. This set first breaks down into a large number of well separated clusters. At the freezing transition, which is in general distinct from the clustering one, some variables (spins) take the same value in all solutions of a given cluster. In this paper we study the critical behavior around the freezing transition, which appears in the unfrozen phase as the divergence of the sizes of the rearrangements induced in response to the modification of a variable. The formalism is developed on generic constraint satisfaction problems and applied in particular to the random satisfiability of boolean formulas and to the coloring of random graphs. The computation is first performed in random tree ensembles, for which we underline a connection with percolation models and with the reconstruction problem of information theory. The validity of these results for the original random ensembles is then discussed in the framework of the cavity method.	bethe–salpeter equation;cluster analysis;col (game);computation;constraint satisfaction problem;cryptographic service provider;expanded memory;factor graph;feasible region;graph coloring;hamming distance;heuristic;image scaling;information theory;local search (optimization);message passing;percolation;percolation theory;random graph;random tree;reconstruction conjecture;recursion;scaling limit;search algorithm;semantics (computer science);social inequality;solver;stationary process;universality probability;window function	Guilhem Semerjian	2007	CoRR	10.1007/s10955-007-9417-7	random graph;combinatorics;discrete mathematics;random field;message passing;multivariate random variable;random element;spin glass;information theory;random compact set;theoretical computer science;mean field theory;random function;constraint satisfaction dual problem;mathematics;complexity of constraint satisfaction;constraint satisfaction problem;physics;quantum mechanics;statistics;satisfiability	AI	37.791521466090686	14.34555118078487	140785
a5b85e1be6894d0915242aa451cd2d80e8c431f8	bounds for the symmetric difference of generalized marcum q-functions	lower and upper bounds;approximations symmetric difference of marcum q functions lower and upper bounds;computational intelligence;closed form solutions;qa mathematics matematika;symmetric difference generalized marcum q function;upper bound;statistical distributions;function approximation;symmetric difference of marcum q functions;informatics;qa74 analysis analizis;upper bound function approximation closed form solutions computational intelligence informatics;approximations	Recently, an approximation for large values of a and b for the symmetric difference of Marcum Q-functions Qv(a, b) was given in [1] in the case of integer order, i.e. when v = n ϵ N. Motivated by this result, in this note we study the symmetric difference of Marcum Q-functions Qv(a, b) of real order v ≥ 1 for the parameters a > b > 0. Our aim is to use some of the lower and upper bounds of the Marcum Q-function that appear in the literature to obtain some tight bounds for the symmetric difference. Another approach, presented in this note, is to investigate the difference via closed forms of the Marcum Q-function.	approximation;emoticon	Árpád Baricz;Timea Meszaros	2015	2015 IEEE 10th Jubilee International Symposium on Applied Computational Intelligence and Informatics	10.1109/SACI.2015.7208171	probability distribution;mathematical optimization;combinatorics;mathematical analysis;function approximation;computer science;machine learning;computational intelligence;mathematics;upper and lower bounds;informatics	Theory	38.162423512294204	13.045247092999233	141080
3114baab5ab6f9c675e5bc8ace1766c759b4b9aa	a unified approach to approximate proximity searching	nearest neighbor queries;range query;data structure	The inability to answer proximity queries efficiently for spaces of dimension d > 2 has led to the study of approximation to proximity problems. Several techniques have been proposed to address different approximate proximity problems. In this paper, we present a new and unified approach to proximity searching, which provides efficient solutions for several problems: spherical range queries, idempotent spherical range queries, spherical emptiness queries, and nearest neighbor queries. In contrast to previous data structures, our approach is simple and easy to analyze, providing a clear picture of how to exploit the particular characteristics of each of these problems. Despite the simplicity and generality of our data structures, our complexities essentially match the most efficient data structures known and often offer small improvements. As applications of our approach, we provide simple and practical data structures that match the best previous results up to logarithmic factors, as well as advanced data structures that improve over the best previous results for all aforementioned proximity problems.	approximation algorithm;data structure;idempotence;proximity problems;proximity search (text);range query (data structures)	Sunil Arya;Guilherme Dias da Fonseca;David M. Mount	2010		10.1007/978-3-642-15775-2_32	range query;proximity problems;data structure;computer science;theoretical computer science;data mining;database;mathematics;programming language	Theory	29.778127366840074	15.111975159589981	141223
ec8efb3ddbfa003b55a7490eef4987b24f08821e	greedy random walk	discrete time;random process;random walk;data structure	We study a discrete time self interacting random process on graphs, which we call Greedy Random Walk. The walker is located initially at some vertex. As time evolves, each vertex maintains the set of adjacent edges touching it that have not been crossed yet by the walker. At each step, the walker being at some vertex, picks an adjacent edge among the edges that have not traversed thus far according to some (deterministic or randomized) rule. If all the adjacent edges have already been traversed, then an adjacent edge is chosen uniformly at random. After picking an edge the walk jumps along it to the neighboring vertex. We show that the expected edge cover time of the greedy random walk is linear in the number of edges for certain natural families of graphs. Examples of such graphs include the complete graph, even degree expanders of logarithmic girth, and the hypercube graph. We also show that GRW is transient in Z for all d ≥ 3.	amiga walker;degree (graph theory);edge cover;ghirardi–rimini–weber theory;girth (graph theory);greedy algorithm;interaction;randomized algorithm;stochastic process;vertex (graph theory)	Tal Orenshtein;Igor Shinkar	2014	Combinatorics, Probability & Computing	10.1017/S0963548313000552	random graph;discrete time and continuous time;combinatorics;discrete mathematics;topology;data structure;edge cover;vertex;mathematics;random walker algorithm;random walk;neighbourhood;statistics	Theory	37.30073841825695	16.403056728542445	141246
557e3ec085d5afcbbcc90336d96d33c1f60d897c	layout metrics for euler diagrams	aesthetic layout metric;graph theory;optimisation;aesthetics based method;graph drawing;contour;engineering drawings;qa 76 software;diagrams;set theory;operations research;software engineering;hill climbing variant aesthetics based method euler diagram aesthetic layout metric graph drawing algorithm aesthetic principle hill climbing multicriteria optimiser contour java implementation;computer programming;data visualisation;smoothing methods;aesthetic principle;unified modeling language;data visualization;data visualization engineering drawings java software engineering set theory smoothing methods concrete councils software tools unified modeling language;councils;diagrams graph theory optimisation java data visualisation operations research set theory;software tools;hill climbing;layout metrics;hill climbing variant;euler diagram;java implementation;hill climbing multicriteria optimiser;graph drawing algorithm;structural properties;concrete;java;euler diagrams;g000 computing and mathematical sciences	An alternative term for these diagrams is Euler-Venn diagrams but they are often inaccurately called Venn diagrams. Venn diagrams often look similar, but must contain all possible intersections of contours. In contrast, Euler diagrams contain any desired combination of intersections between the contours. Visualizations of Venn diagrams are often created by taking advantage of the symmetries present in a Venn structure [12]. We present an aesthetics based method for drawing Euler diagrams. Aesthetic layout metrics have been found to be useful in graph drawing algorithms, which use metrics motivated by aesthetic principles that aid user understanding of diagrams. We have taken a similar approach to Euler diagram drawing, and have defined a set of suitable metrics to be used within a hill climbing multicriteria optimiser to produce good drawings. There are added difficulties when drawing Euler diagrams as they are made up of contours whose structural properties of intersection and containment must be preserved under any layout improvements. In this paper we describe our Java implementation of a pair of hill climbing variants to find good drawings, a set of metrics that measure aesthetics for good diagram layout, and issues concerning the choice of weightings for a useful combination of the metrics.	algorithm;diagram;euler;euler–lagrange equation;graph drawing;hill climbing;java	Jean Flower;Peter Rodgers;Paul Mutton	2003		10.1109/IV.2003.1217990	computer science;theoretical computer science;engineering drawing;algorithm	HCI	30.063168790923744	10.702780633844942	141564
001154a4b084d48b6338ef470b790fc0d8b14068	optimization results for a generalized coupon collector problem	coupon collector problem;schur convex functions;schur convex functions 2010 mathematics subject classification primary 60c05 secondary 60j05 postal address;optimization	We study in this paper a generalized coupon collector problem, which consists in analyzing the time needed to collect a given number of distinct coupons that are drawn from a set of coupons with an arbitrary probability distribution. We suppose that a special coupon called the null coupon can be drawn but never belongs to any collection. In this context, we prove that the almost uniform distribution, for which all the non-null coupons have the same drawing probability, is the distribution which stochastically minimizes the time needed to collect a fixed number of distinct coupons. Moreover, we show that in a given closed subset of probability distributions, the distribution with all its entries, but one, equal to the smallest possible value is the one, which stochastically maximizes the time needed to collect a fixed number of distinct coupons.	null (sql)	Emmanuelle Anceaume;Yann Busnel;Ernst Schulte-Geers;Bruno Sericola	2016	J. Applied Probability	10.1017/jpr.2016.27	mathematical optimization;combinatorics;coupon collector's problem;discrete mathematics;mathematics;statistics	Theory	27.446214921203104	16.206152867654037	141673
141a7879c760c5eac9fc5d03dbff9694aed4f65c	a recursive algorithm for the generation of space-filling curves	computers;curva;computer graphics;algoritmo recursivo;courbe;filling;curve;shape algorithm design and analysis filling arrays encoding cryptography computers;arrays;algorithme recursif;shape;cryptography;recursive estimation computational geometry;hilbert generation recursive algorithm space filling curves generation aesthetic forms mathematical geometry peano curve;recursive algorithm;space filling curve;encoding;grafico computadora;infographie;algorithm design and analysis	Space-filling curves have intrigued both artists and mathematicians for a long time. They bridge the gap between aesthetic forms and mathematical geometry. To enable construction by computer, an efficient recursive algorithm for the generation of space-filling curves is given. The algorithm is elegant, short and considerably easier to implement than previous recursive and non-recursive algorithms, and can be efficiently coded in all programming languages that have integer operations. The algorithmic technique is shown applied to the generation of the Hilbert and a form of the meandering Peano curve. This coding technique could be successfully applied to the generation of other regular space-filling curves.	algorithm;peano curve;programming language;recursion (computer science);space-filling curve	Greg Breinholt;Christoph Schierz	1998	9th European Signal Processing Conference (EUSIPCO 1998)		discrete mathematics;mathematics;geometry;algorithm	Graphics	33.47711400654229	16.634456482530055	142146
c6760d77061d3d8f614571799a204af4eb187182	probabilistic 2d cellular automata rules for binary classification	probability;training;two dimensional displays;automata;computer science;probabilistic logic;algorithm design and analysis	In this paper are presented classification methods with use of two-dimensional three-state cellular automata. This methods are probabilistic forms of cellular automata rule modified from wide known almost deterministic rule designed by Fawcett. Fawcetts rule is modified into two proposed forms partially and fully probabilistic. The effectiveness of classifications of these three methods is analysed and compared. The classification methods are used as the rules in the two-dimensional three-state cellular automaton with the von Neumann and Moore neighbourhood. Preliminary experiments show that probabilistic modification of Fawcett's method can give better results in the process of reconstruction (classification) than the original algorithm.	algorithm;automata theory;binary classification;cellular automaton;concave function;disjunctive normal form;experiment;mod (video gaming);moore neighborhood;parabolic antenna;probabilistic turing machine;rule 184;three-state logic;von neumann neighborhood	Miroslaw Szaban	2016	2016 Federated Conference on Computer Science and Information Systems (FedCSIS)	10.15439/2016F409	algorithm design;discrete mathematics;quantum finite automata;probabilistic relevance model;computer science;artificial intelligence;machine learning;probability;mathematics;automaton;linguistics;probabilistic logic;algorithm;statistics	Vision	37.924553486107484	9.432536707724998	142281
1955013b1982b1b08cfbf5aaaed2d2307cd94610	elementary algorithms for multiresolution geometric tomography with strip model of projections	geometry;greedy algorithms;image resolution;linear programming;simulated annealing;game;cell decomposition;elementary algorithms;greedy algorithm;multiresolution geometric tomography;multiresolution heuristic;parallel simulated annealing;strip-based model of projections	We consider a problem of geometric tomography whith a strip-based model of projections: the projections are the areas of the intersection between the shape and some strips. We investigate some elementary algorithms that allow to construct a shape with prescribed projections. We start with an exact algorithm based on Linear Programming which leads to practical difficulties. Then we investigate three practical heuristics based on a cell decomposition of the domain of interest: GA is a greedy algorithm, GAME its multiresolution derivative and MPH a multiresolution heuristic close to a parallel Simulated Annealing.	exact algorithm;greedy algorithm;heuristic (computer science);linear programming;strips;simulated annealing;software release life cycle;tomographic reconstruction;tomography	Yan Gérard	2013	2013 8th International Symposium on Image and Signal Processing and Analysis (ISPA)		mathematical optimization;combinatorics;machine learning;mathematics	Theory	30.260842991705076	14.088320679808401	142405
8e0703dff5f5cc66e45c443b774d6830da6edc0a	convergence of autonomous mobile robots with inaccurate sensors and movements	theoretical model;convergence;measurement;autonomous system;68w15;commande repartie;autonomous mobile robot;68wxx;input;sistema autonomo;autonomous robots;modele theorique;convergencia;estimation erreur;68q22;medida;error estimation;fault tolerance;algorithme reparti;estimacion error;systeme autonome;entree ordinateur;coordinacion;algoritmo repartido;mesure;control repartido;inaccurate sensors;entrada ordenador;58a25;robot;distributed algorithm;distributed control;modelo teorico;coordination;70b15	A number of recent studies concern algorithms for distributed control and coordination in systems of autonomous mobile robots. The common theoretical model adopted in these studies assumes that the positional input of the robots is obtained by perfectly accurate visual sensors, that robot movements are accurate, and that internal calculations performed by the robots on (real) coordinates are perfectly accurate as well. The current paper concentrates on the effect of weakening this rather strong set of assumptions and replacing it with the more realistic assumption that the robot sensors, movement, and internal calculations may have slight inaccuracies. Specifically, the paper concentrates on the ability of robot systems with inaccurate sensors, movements, and calculations to carry out the task of convergence. The paper presents several impossibility theorems, limiting the inaccuracy levels that still allow convergence, and prohibiting a general algorithm for gathering, namely, meeting at a point, in a finite number of steps. The main positive result is an algorithm for convergence under bounded measurement, movement, and calculation errors.	autonomous robot;sensor	Reuven Cohen;David Peleg	2008	SIAM J. Comput.	10.1137/060665257	robot;distributed algorithm;fault tolerance;simulation;convergence;computer science;autonomous system;control theory;measurement	Robotics	32.652415355796535	12.047449906826259	142506
f86033a21ab0548dba34f64c846bb9c648f9736a	tight bounds for stochastic convex programs	limite superieure;probability;convex programming;limite inferior;programmation stochastique;programmation convexe;upper bound;stochastic bounds for stochastic convex programs;mathematical programming;limite superior;stochastic programming;programmation mathematique;programming;limite inferieure;programacion estocastica;programacion matematica;stochastic model approximations in stochastic programming;lower bound;programacion convexa	Variable and row aggregation as a technique of simplifying a mathematical program is utilized to develop bounds for two-stage stochastic convex programs with random right-hand sides. If one is able to utilize the problem structure along with only first moment information, a tighter bound than the usual mean model bound based on Jensen's inequality may be obtained. Moreover, it is possible to construct examples for which the mean model bound will be arbitrarily poor. Consequently, one can tighten Jensen's bound for stochastic programs when the distribution has a compact support. This bound may be improved further by partitioning the support using conditional first moments. With regard to first moment upper bounds, the Gassmann-Ziemba inequality is used for the stochastic convex program to seek a model which can be solved using standard convex programming techniques. Moreover, it allows one to easily construct upper bounds using the solution of the lower bounding problem. Finally, the results are extended to multistage stochastic convex programming problems.	convex optimization	N. C. P. Edirisinghe;William T. Ziemba	1992	Operations Research	10.1287/opre.40.4.660	convex analysis;mathematical optimization;combinatorics;discrete mathematics;convex optimization;convex combination;jensen's inequality;linear matrix inequality;mathematics;upper and lower bounds;algorithm;proper convex function	ML	35.53342515505256	5.195923513948671	142628
7cf4314e2255ae42e49ef2604350207b2063532f	on complexity of easy predictable sequences	loss function;lower bound	Predictive complexity is a generalization of Kolmogorov complexity. It corresponds to an “optimal” prediction strategy and gives a natural lower bound to ability of any algorithm to predict elements of a sequence of outcomes. A natural question is studied: how complex can easy-to-predict sequences be? The standard measure of complexity, used in the paper, is Kolmogorov complexity K (which is close to predictive complexity for logarithmic loss function). The difficulty of prediction is measured by the notion of predictive complexity KG for bounded loss function (of nonlogarithmic type). We present an asymptotic relation supx :l(x)=n K (x | n) KG(x) ∼ 1 a log n, when n → ∞, where a is a constant and l(x) is the length of a sequence x . An analogous asymptotic relation holds for relative complexities K (x | n)/n and KG(x)/n, where n = l(x). To obtain these results we present lower and upper bounds of the cardinality of all sequences of given predictive complexity. C © 2002 Elsevier Science (USA)	algorithm;blum axioms;kasparov's gambit;kolmogorov complexity;loss function	Michael V. Vyugin;Vladimir V. V'yugin	2002	Inf. Comput.	10.1006/inco.2002.3164	kolmogorov structure function;generalization;calculus;mathematics;upper and lower bounds;algorithm	Theory	36.03516046985253	11.437613947485856	142676
ff39c8ce3af05d310511f03418fe205889cd2ad4	branch and bound algorithm for multidimensional scaling with city-block metric	quadratic program;branch and bound algorithm;objective function;branch and bound method;multidimensional scaling;global optimization;branch and bound;city block metric	A two level global optimization algorithm for multidimensional scaling (MDS) with city-block metric is proposed. The piecewise quadratic structure of the objective function is employed. At the upper level a combinatorial global optimization problem is solved by means of branch and bound method, where an objective function is defined as the minimum of a quadratic programming problem. The later is solved at the lower level by a standard quadratic programming algorithm. The proposed algorithm has been applied for auxiliary and practical problems whose global optimization counterpart was of dimensionality up to 24.		Antanas Zilinskas;Julius Zilinskas	2009	J. Global Optimization	10.1007/s10898-008-9306-x	mathematical optimization;combinatorics;mathematical analysis;branch and price;mathematics;quadratic programming;branch and bound;branch and cut;global optimization;quadratic assignment problem	ML	25.891675333101013	11.614761143480617	143437
66ecb25aa426cffad3f25ffd403f173fd8bbc1dc	using computational learning strategies as a tool for combinatorial optimization	learning algorithm;deterministic finite automaton;combinatorial optimization problem;combinatorial problems;graph coloring;ordered binary decision diagram;optimization problem;combinatorial optimization;computational learning theory;learning strategies	In this paper, we describe how a basic strategy from computational learning theory can be used to attack a class of NP‐hard combinatorial optimization problems. It turns out that the learning strategy can be used as an iterative booster: given a solution to the combinatorial problem, we will start an efficient simulation of a learning algorithm which has a “good chance” to output an improved solution. This boosting technique is a new and surprisingly simple application of an existing learning strategy. It yields a novel heuristic approach to attack NP‐hard optimization problems. It does not apply to each combinatorial problem, but we are able to exactly formalize some sufficient conditions. The new technique applies, for instance, to the problems of minimizing a deterministic finite automaton relative to a given domain, the analogous problem for ordered binary decision diagrams, and to graph coloring.	algorithm;binary decision diagram;booster (electric power);combinatorial optimization;computational learning theory;deterministic finite automaton;finite-state machine;graph coloring;heuristic;iterative method;mathematical optimization;simulation	Andreas Birkendorf;Hans Ulrich Simon	1998	Annals of Mathematics and Artificial Intelligence	10.1023/A:1018991519323	computational problem;optimization problem;mathematical optimization;combinatorics;cross-entropy method;combinatorial optimization;computer science;deterministic finite automaton;combinatorial explosion;machine learning;graph coloring;mathematics;stability;computational learning theory;quadratic assignment problem	AI	27.24723464759791	4.444494313112225	143956
d8ce3d9ebe9a46a60be5582069a52d72ba9aa736	gathering asynchronous mobile robots with inaccurate compasses	tolerancia falta;robot movil;distributed system;pragmatics;multiagent system;systeme reparti;mobile robot;autonomous system;localization;pragmatica linguistca;multiplicite;punto fijo;localizacion;robotics;autonomous mobile robot;sistema autonomo;transmision asincronica;sistema coordenadas;sistema repartido;localisation;pragmatique;robot mobile;point fixe;fault tolerance;multiplicidad;systeme autonome;robotica;asynchronous transmission;transmission asynchrone;robotique;systeme coordonnee;sistema multiagente;multiplicity;fix point;tolerance faute;moving robot;systeme multiagent;coordinate system	This paper considers a system of asynchronous autonomous mobile robots that can move freely in a two-dimensional plane with no agreement on a common coordinate system. Starting from any initial configuration, the robots are required to eventually gather at a single point, not fixed in advance (gathering problem).#R##N##R##N#Prior work has shown that gathering oblivious (i.e., stateless) robots cannot be achieved deterministically without additional assumptions. In particular, if robots can detect multiplicity (i.e., count robots that share the same location) gathering is possible for three or more robots. Similarly, gathering of any number of robots is possible if they share a common direction, as given by compasses, with no errors.#R##N##R##N#Our work is motivated by the pragmatic standpoint that (1) compasses are error-prone devices in reality, and (2) multiplicity detection, while being easy to achieve, allows for gathering in situations with more than two robots. Consequently, this paper focusses on gathering two asynchronous mobile robots equipped with inaccurate compasses. In particular, we provide a self-stabilizing algorithm to gather, in a finite time, two oblivious robots equipped with compasses that can differ by as much as π/4.		Samia Souissi;Xavier Défago;Masafumi Yamashita	2006		10.1007/11945529_24	mobile robot;embedded system;fault tolerance;simulation;internationalization and localization;computer science;autonomous system;artificial intelligence;coordinate system;asynchronous communication;distributed computing;multiplicity;robotics;pragmatics	Robotics	32.61602149767482	12.102571567163674	144132
f4b5f8f880543c3fcd200ffce52deaac5ad3eff6	growing network models having part edges removed/added randomly		Abstract: Since network motifs are an important property of networks and some networks have the behaviors of rewiring or reducing or adding edges between old vertices before new vertices entering the networks, we construct our non-randomized model N(t) and randomized model N ′(t) that have the predicated fixed subgraphs like motifs and satisfy both properties of growth and preferential attachment by means of the recursive algorithm from the lower levels of the so-called bound growing network models. To show the scale-free property of the randomized model N ′(t), we design a new method, called edge-cumulative distribution, and democrat two edge-cumulative distributions of N(t) and N ′(t) are equivalent to each other.	attachments;randomized algorithm;randomness;recursion (computer science);vertex (geometry)	Bing Yao;Xiaomin Wang;Xia Liu;Jin Xu	2015	CoRR		combinatorics;discrete mathematics;mathematics;algorithm	Theory	37.52323152235172	16.322372777711813	144356
5458fe64052ad8a1a30522e7fec3b8646789713c	convex optimization for non-convex problems via column generation		We apply column generation to approximating complex structured objects via a set of primitive structured objects under either the cross entropy or L2 loss. We use L1 regularization to encourage the use of few structured primitive objects. We attack approximation using convex optimization over an infinite number of variables each corresponding to a primitive structured object that are generated on demand by easy inference in the Lagrangian dual. We apply our approach to producing low rank approximations to large 3-way tensors.	approximation;column generation;convex optimization;cross entropy;elastic net regularization;lagrange multiplier;mathematical optimization	Julian Yarkony;Kamalika Chaudhuri	2016	CoRR		mathematical optimization;combinatorics;discrete mathematics;mathematics	ML	33.03433138829645	6.959859086590826	144595
743332b0a45eaa6a4740607ee9eb0ae2c69a307f	a comparative study of modern inference techniques for discrete energy minimization problems	minimisation;discrete optimization;computational modeling runtime message passing computer vision optimization benchmark testing minimization;benchmark;markov random fields;benchmark graphical models discrete optimization markov random fields;inference mechanisms;computer vision;graphical models;integer programming;random processes;random processes computer vision inference mechanisms integer programming markov processes minimisation;integer programming solvers modern inference techniques discrete energy minimization problems markov random field mrf optimization technique higher order interactions flexible connectivity structures learned energy tables computer vision applications opengm2 framework polyhedral methods;markov processes	Even years ago, Szeliski et al. published an influential study on energy minimization methods for Markov random fields (MRF). This study provided valuable insights in choosing the best optimization technique for certain classes of problems. While these insights remain generally useful today, the phenominal success of random field models means that the kinds of inference problems we solve have changed significantly. Specifically, the models today often include higher order interactions, flexible connectivity structures, large label-spaces of different cardinalities, or learned energy tables. To reflect these changes, we provide a modernized and enlarged study. We present an empirical comparison of 24 state-of-art techniques on a corpus of 2,300 energy minimization instances from 20 diverse computer vision applications. To ensure reproducibility, we evaluate all methods in the OpenGM2 framework and report extensive results regarding runtime and solution quality. Key insights from our study agree with the results of Szeliski et al. for the types of models they studied. However, on new and challenging types of models our findings disagree and suggest that polyhedral methods and integer programming solvers are competitive in terms of runtime and solution quality over a large range of model types.	algorithm;computer vision;cutting-plane method;energy minimization;integer programming;interaction;lp-type problem;linear programming relaxation;markov chain;markov random field;mathematical optimization;polyhedron;software framework;text corpus	Jörg H. Kappes;Björn Andres;Fred A. Hamprecht;Christoph Schnörr;Sebastian Nowozin;Dhruv Batra;Sungwoong Kim;Bernhard X. Kausler;Jan Lellmann;Nikos Komodakis;Carsten Rother	2013	2013 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2013.175	discrete optimization;minimisation;mathematical optimization;integer programming;benchmark;computer science;theoretical computer science;machine learning;mathematics;graphical model;markov process;statistics	Vision	32.03051039407967	4.8322791943359835	145513
6637fb570a3b8780abc4ae7b5a96050e314e8216	lost in self-stabilization	game theory	One of the questions addressed here is ”How can a twisted thread correct itself?”. We consider a theoretical model where the studied mathematical object represents a 2D twisted discrete thread linking two points. This thread is made of a chain of agents which are “lost”, i.e. they have no knowledge of the global setting and no sense of direction. Thus, the modifications made by the agents are local and all the decisions use only minimal information about the local neighborhood. We introduce a random process such that the thread reorganizes itself efficiently to become a discrete line between these two points. The second question addressed here is to reorder a word by local flips in order to scatter the letters to avoid long successions of the same letter. These two questions are equivalent. The work presented here is at the crossroad of many different domains such as modeling cooling process in crystallography [2, 3, 8], stochastic cellular automata [6, 7], organizing a line of robots in distributed algorithms (the robot chain problem [5, 11]), and Christoffel words in language theory [1]. ∗This work is partially supported by Programs ANR Dynamite, Quasicool and IXXI (Complex System Institute, Lyon). †Corresponding author. 1 ar X iv :1 41 0. 76 69 v2 [ cs .D M ] 2 9 O ct 2 01 4	ambiguous name resolution;automata theory;complex system;computer cooling;distributed algorithm;organizing (structure);robot;self-stabilization;stochastic cellular automaton;stochastic process;twisted	Damien Regnault;Eric Rémila	2015		10.1007/978-3-662-48057-1_34	combinatorics;discrete mathematics;computer science;artificial intelligence;theoretical computer science;mathematics;algorithm	Theory	38.21634920718553	10.129184880817618	145555
b50ecd4169f4fb9952220448e5a79b032e8d23a5	sparse solutions of sparse linear systems: fixed-parameter tractability and an application of complex group testing	enumeration;sparse vector;problem kernel;linear system;parameterized algorithm;hitting set;group testing	A vector with at most k nonzeros is called k-sparse. We show that enumerating the support vectors of k-sparse solutions to a system Ax = b of r-sparse linear equations (i.e., where the rows of A are rsparse) is fixed-parameter tractable (FPT) in the combined parameter r, k. For r = 2 the problem is simple. For 0, 1-matrices A we can also compute an O(rk) kernel. For systems of linear inequalities we get an FPT result in the combined parameter d, k, where d is the total number of minimal solutions. This is achieved by interpeting the problem as a case of group testing in the complex model. The problems stem from the reconstruction of chemical mixtures by observable reaction products.	algorithm;cobham's thesis;experiment;kernel (operating system);linear equation;linear inequality;linear programming;linear system;numerical analysis;observable;parameterized complexity;polyhedron;polynomial;set cover problem;sparse matrix;vertex cover	Peter Damaschke	2011		10.1007/978-3-642-28050-4_8	mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	26.390982337288147	13.97020954371753	146040
282f8cc4b239b66e3110b245a3aae9f199072e05	comparative study between kleinberg algorithm and biased selection algorithm for construction of small world networks		Actually Small-World Networks is a very important topic, it is present in a lot of applications in our environment. A target of many algorithms is to establish methods to get that any node in a graph can establish a direct connection with a randomly “long-range neighbor”. This work is comparative study between two algorithms that get this target (Kleinberg and Biased Selection), I demonstrate by my experiments that both get the Kleinberg’s distribution. I conclude that the Kleinberg’s algorithm distribution maintains a probability directly proportional to Euclidian distance, and Biased Selection, although also maintains a probability directly proportional to Euclidian distance, allows that a node can get a farther node as “long-range neighbor” more frequently.	selection algorithm	Miguel Arcos Argudo	2017	Computación y Sistemas		mathematical optimization;selection algorithm;euclidean distance;small-world network;computer science;algorithm;graph	Theory	32.40592038007524	10.828194438055698	146098
02cc28cfd0e38f3d664578337616a305897b63b3	parity problem with a cellular automaton solution	science and technology;computational techniques;boundary conditions;theorem proving;self organizing system;complex system;binary sequences;cellular automata;article;cellular automaton;problem solving	The parity of a bit string of length N is a global quantity that can be efficiently computed using a global counter in O(N) time. But is it possible to find the parity using cellular automata with a set of local rule tables without using any global counter? Here, we report a way to solve this problem using a number of r=1 binary, uniform, parallel, and deterministic cellular automata applied in succession for a total of O(N2) time.	automata theory;bit array;cellular automaton;data table;parity bit;succession;newton	K. M. Lee;Hao Xu;H. F. Chau	2001	Physical review. E, Statistical, nonlinear, and soft matter physics	10.1103/PhysRevE.64.026702	stochastic cellular automaton;cellular automaton;reversible cellular automaton;block cellular automaton;combinatorics;discrete mathematics;continuous spatial automaton;boundary value problem;continuous automaton;mathematics;automated theorem proving;rule 184;mobile automaton;algorithm;science, technology and society	Theory	38.18782771542944	9.506148113105697	146310
13e3ec069015f0112dd2c80a7c114fbee93c61f0	bicriteria optimization of a queue with a controlled input stream	multicriteria optimization;liverpool;communication networks;queuing system;queue length;pareto set;repository;community networks;constrained optimization problems;markov process;threshold control;university;controlled jump markov processes;lagrange function;constrained optimization problem;control strategy	Motivated by a certain situation in communication networks, we will investigate the M/M/1/∞ queuing system, where one can change the input stream. Performance criteria coincide with the long-run average throughput of the system and the queue length. We will present a rigorous mathematical study of the constrained version of the multicriteria optimization problem for jump Markov processes. Subsequently, it will be shown that the stationary control strategies of the threshold type form a sufficient class in the initial bicriteria problem considered.	mathematical optimization;optimization problem;stationary process;stream (computing);telecommunications network;throughput	Alexei B. Piunovskiy	2004	Queueing Syst.	10.1023/B:QUES.0000039892.03630.24	mathematical optimization;simulation;m/m/c queue;computer science;mathematics;queue management system;m/g/1 queue;markov process;statistics	Metrics	37.604815269164355	4.476831538488685	146660
340d6468544ecefa7e017d820257b0953f97eeda	a multilevel memetic approach for improving graph k-partitions	silicon;graph theory;optimisation;perturbation techniques;memetics;graph partitioning;moon;time limit ranging multilevel memetic approach graph k partitioning np complete problems multiparent crossover operator powerful perturbation based tabu search algorithm crossover operator;tabu search;optimization;multiparent crossover;search problems;landscape analysis;backbone;optimal algorithm;search problems graph theory optimisation perturbation techniques;algorithm design and analysis;tabu search backbone graph partitioning landscape analysis multiparent crossover;benchmark testing;partitioning algorithms;partitioning algorithms memetics silicon optimization algorithm design and analysis moon benchmark testing	Graph partitioning is one of the most studied NP-complete problems. Given a graph G=(V, E) , the task is to partition the vertex set V into k disjoint subsets of about the same size, such that the number of edges with endpoints in different subsets is minimized. In this paper, we present a highly effective multilevel memetic algorithm, which integrates a new multiparent crossover operator and a powerful perturbation-based tabu search algorithm. The proposed crossover operator tends to preserve the backbone with respect to a certain number of parent individuals, i.e., the grouping of vertices which is common to all parent individuals. Extensive experimental studies on numerous benchmark instances from the graph partitioning archive show that the proposed approach, within a time limit ranging from several minutes to several hours, performs far better than any of the existing graph partitioning algorithms in terms of solution quality.	archive;benchmark (computing);graph partition;internet backbone;karp's 21 np-complete problems;memetic algorithm;memetics;search algorithm;tabu search;vertex (graph theory)	Una Benlic;Jin-Kao Hao	2011	IEEE Transactions on Evolutionary Computation	10.1109/TEVC.2011.2136346	theta graph;graph power;algorithm design;benchmark;memetics;mathematical optimization;combinatorics;cut;graph bandwidth;level structure;turán graph;null graph;tabu search;computer science;regular graph;natural satellite;graph partition;graph theory;simplex graph;mixed graph;machine learning;hypercube graph;mathematics;voltage graph;path;silicon;complement graph;strength of a graph	DB	24.908633686917415	5.6204753908087755	146857
6635639236934975cc8e52c4d41a2dae4a0cefe6	gradient methods for submodular maximization		In this paper, we study the problem of maximizing continuous submodular functions that naturally arise in many learning applications such as those involving utility functions in active learning and sensing, matrix approximations and network inference. Despite the apparent lack of convexity in such functions, we prove that stochastic projected gradient methods can provide strong approximation guarantees for maximizing continuous submodular functions with convex constraints. More specifically, we prove that for monotone continuous DR-submodular functions, all fixed points of projected gradient ascent provide a factor 1/2 approximation to the global maxima. We also study stochastic gradient methods and show that after O(1/ ) iterations these methods reach solutions which achieve in expectation objective values exceeding (OPT 2 − ). An immediate application of our results is to maximize submodular functions that are defined stochastically, i.e. the submodular function is defined as an expectation over a family of submodular functions with an unknown distribution. We will show how stochastic gradient methods are naturally well-suited for this setting, leading to a factor 1/2 approximation when the function is monotone. In particular, it allows us to approximately maximize discrete, monotone submodular optimization problems via projected gradient ascent on a continuous relaxation, directly connecting the discrete and continuous domains. Finally, experiments on real data demonstrate that our projected gradient methods consistently achieve the best utility compared to other continuous baselines while remaining competitive in terms of computational effort.	approximation algorithm;computation;discrete mathematics;expectation–maximization algorithm;experiment;fixed point (mathematics);heuristic (computer science);iteration;linear programming relaxation;local search (optimization);mathematical optimization;maxima;monte carlo method;optimization problem;stochastic gradient descent;submodular set function;times ascent;monotone	S. Hamed Hassani;Mahdi Soltanolkotabi;Amin Karbasi	2017			discrete mathematics;mathematical optimization;submodular set function;monotone polygon;gradient descent;matrix (mathematics);maximization;fixed point;mathematics;convexity;optimization problem	ML	33.079332761656204	5.807506808293475	146973
c3adde17830060b54dca530bd43f9adb6eecd83a	a polynomial-time algorithm for computing the resilience of arrangements of ray sensors	ray sensors;barrier coverage;sensor networks;barrier resilience	Given an arrangement A of n sensors and two points s and t in the plane, the barrier resilience of A with respect to s and t is the minimum number of sensors whose removal permits a path from s to t such that the path does not intersect the coverage region of any sensor in A. When the surveillance domain is the entire plane and sensor coverage regions are unit line segments, even with restricted orientations, the problem of determining the barrier resilience is known to be NP-hard. On the other hand, if sensor coverage regions are arbitrary lines, the problem has a trivial linear time solution. In this paper, we study the case where each sensor coverage region is an arbitrary ray, and give an O(n2m) time algorithm for computing the barrier resilience when there are m ⩾ 1 sensor intersections.	algorithm;polynomial;sensor	David G. Kirkpatrick;Boting Yang;Sandra Zilles	2014	Int. J. Comput. Geometry Appl.	10.1142/S0218195914600048	wireless sensor network;telecommunications;computer security	Theory	30.91563178425913	17.464560920290747	147028
f40fba0df9d4d0b1562a010a233acb938672fe17	computationally tractable counterparts of distributionally robust constraints on risk measures		In optimization problems appearing in fields such as economics, finance, or engineering, it is often important that a risk measure of a decision-dependent random variable stays below a prescribed level. At the same time, the underlying probability distribution determining the risk measure’s value is typically known only up to a certain degree and the constraint should hold for a reasonably wide class of probability distributions. In addition to that, the constraint should be computationally tractable. In this paper we review and generalize results on the derivation of tractable counterparts of such constraints for discrete probability distributions. Using established techniques in robust optimization, we show that the derivation of a tractable robust counterpart can be split into two parts: one corresponding to the risk measure and the other to the uncertainty set. This holds for a wide range of risk measures and uncertainty sets for probability distributions defined using statistical goodness-of-fit tests or probability metrics. In this way, we provide a unified framework of reformulating this class of constraints, extending the number of solvable risk measure-uncertainty set combinations considerably, including also risk measures that are nonlinear in the probabilities. To provide a clear overview for the user, we give the computational tractability status for each of the uncertainty set-risk measure pairs of which some have been solved in the literature. Examples, including portfolio optimization and antenna array design, illustrate the proposed approach in a theoretical and numerical setting.	best, worst and average case;bram moolenaar;closed convex function;cobham's thesis;concave function;convex set;decision problem;duality (optimization);effective domain;emoticon;fenchel's duality theorem;kullback–leibler divergence;login;mathematical optimization;maximal set;nonlinear system;numerical analysis;optimization problem;p (complexity);risk measure;robust optimization;robustness (computer science);unified framework	Krzysztof Postek;Dick den Hertog;Bertrand Melenberg	2016	SIAM Review	10.1137/151005221	mathematical optimization;discrete mathematics;robust optimization;probability measure;entropic value at risk;mathematics;dynamic risk measure;coherent risk measure;statistics	ML	35.24601577650806	5.241816853152767	147300
aff6461eb84799107b680d162d020eca42a82875	improved discrete reformulations for the quadratic assignment problem		This paper presents an improved as well as a completely new version of a mixed integer linear programming (MILP) formulation for solving the quadratic assignment problem (QAP) to global optimum. Both formulations work especially well on instances where at least one of the matrices is sparse. Modification schemes, to decrease the number of unique elements per row in symmetric instances, are presented as well. The modifications will tighten the presented formulations and considerably shorten the computational times. We solved, for the first time ever to proven optimality, the instance esc32b from the quadratic assignment problem library, QAPLIB.	computation;global optimization;integer programming;linear programming relaxation;quadratic assignment problem;sparse matrix	Axel Nyberg;Tapio Westerlund;Andreas Lundell	2013		10.1007/978-3-642-38171-3_13	linear bottleneck assignment problem;generalized assignment problem;quadratic assignment problem	Theory	25.889514966689433	8.038026872912923	147969
25693fca71c1b8f7ccc7efb92819005f0310223b	exact delaunay graph of smooth convex pseudo-circles: general predicates, and implementation for ellipses	polynomial system;silhouette curves;gpu;nurbs;closest point;minimum distance;exact computation;clearance analysis;algebraic complexity;hybrid cpu gpu algorithms;voronoi diagram	We examine the problem of computing exactly the Delaunay graph (and the dual Voronoi diagram) of a set of, possibly intersecting, smooth convex pseudo-circles in the Euclidean plane, given in parametric form. Pseudo-circles are (convex) sites, every pair of which has at most two intersecting points. The Delaunay graph is constructed incrementally. Our first contribution is to propose robust end efficient algorithms for all required predicates, thus generalizing our earlier algorithms for ellipses, and we analyze their algebraic complexity, under the exact computation paradigm. Second, we focus on InCircle, which is the hardest predicate, and express it by a simple sparse 5 X 5 polynomial system, which allows for an efficient implementation by means of successive Sylvester resultants and a new factorization lemma. The third contribution is our cgal-based c++ software for the case of ellipses, which is the first exact implementation for the problem. Our code spends about 98 sec to construct the Delaunay graph of 128 non-intersecting ellipses, when few degeneracies occur. It is faster than the cgal segment Delaunay graph, when ellipses are approximated by k-gons for k > 15.	approximation algorithm;cgal;computation;convex function;degenerate energy levels;delaunay triangulation;linear algebra;programming paradigm;resultant;sparse matrix;system of polynomial equations;voronoi diagram	Ioannis Z. Emiris;Elias P. Tsigaridas;George M. Tzoumas	2009		10.1145/1629255.1629282	mathematical optimization;combinatorics;discrete mathematics;non-uniform rational b-spline;voronoi diagram;mathematics;geometry;constrained delaunay triangulation;bowyer–watson algorithm	Theory	32.95154859437215	16.551824363496376	148570
81b9aa31fa5c0809eaf36a4f127327c25213ed06	the deltaup constraint solver: minimizing the number of method selections in deltablue	deltablue;local propagation;constraint solving	Abstract#R##N##R##N#We present a modification of the DeltaBlue constraint solver called DeltaUp. DeltaBlue is an incremental constraint solver based on local propagation, which is widely used for constructing graphical user interfaces and algorithm animations. DeltaUp minimizes the number of time-consuming tasks of DeltaBlue, namely method selections, in each planning phase. To compute the exact number of needed method selections, we introduced a cost function up-cost to DeltaBlue. Our benchmarks show that DeltaUp is approximately two times faster than DeltaBlue in the best case. Even in the worst case, it is only slightly slower than DeltaBlue. Copyright © 2001 John Wiley & Sons, Ltd.	solver	Tetsuya Suzuki;Takehiro Tokuda	2001	Softw., Pract. Exper.	10.1002/spe.421	constraint logic programming;mathematical optimization;computer science;algorithm	SE	27.760146877554067	8.577882226149848	148684
6d1840e4d6a43ce46d80fba5327eb5c20b4c2e34	neural networks and rational functions		Neural networks and rational functions efficiently approximate each other. In more detail, it is shown here that for any ReLU network, there exists a rational function of degreeO(poly log(1/ )) which is -close, and similarly for any rational function there exists a ReLU network of size O(poly log(1/ )) which is -close. By contrast, polynomials need degree Ω(poly(1/ )) to approximate even a single ReLU. When converting a ReLU network to a rational function as above, the hidden constants depend exponentially on the number of layers, which is shown to be tight; in other words, a compositional representation can be beneficial even for rational functions. 1. Overview Significant effort has been invested in characterizing the functions that can be efficiently approximated by neural networks. The goal of the present work is to characterize neural networks more finely by finding a class of functions which is not only well-approximated by neural networks, but also well-approximates neural networks. The function class investigated here is the class of rational functions: functions represented as the ratio of two polynomials, where the denominator is a strictly positive polynomial. For simplicity, the neural networks are taken to always use ReLU activation σr(x) := max{0, x}; for a review of neural networks and their terminology, the reader is directed to Section 1.4. For the sake of brevity, a network with ReLU activations is simply called a ReLU network. 1.1. Main results The main theorem here states that ReLU networks and rational functions approximate each other well in the sense University of Illinois, Urbana-Champaign; work completed while visiting the Simons Institute. Correspondence to: your friend <mjt@illinois.edu>. Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s). −1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00 0 1 2 3 4 spike rat poly net Figure 1. Rational, polynomial, and ReLU network fit to “spike”, a function which is 1/x along [1/4, 1] and 0 elsewhere. that -approximating one class with the other requires a representation whose size is polynomial in ln(1 / ), rather than being polynomial in 1/ . Theorem 1.1. 1. Let ∈ (0, 1] and nonnegative integer k be given. Let p : [0, 1] → [−1,+1] and q : [0, 1] → [2−k, 1] be polynomials of degree ≤ r, each with≤ smonomials. Then there exists a function f : [0, 1] → R, representable as a ReLU network of size (number of nodes) O ( k ln(1 / ) + min { srk ln(sr / ), sdk ln(dsr / ) }) ,	approximation algorithm;first-class function;international conference on machine learning;keneth alden simons;maxima and minima;neural networks;neural network software;polynomial;rectifier (neural networks)	Matus Telgarsky	2017			mathematics;combinatorics;discrete mathematics;existential quantification;artificial neural network;exponential growth;polynomial;rational function	ML	31.06924181198347	10.48177211033069	148733
44a449c1ed7a5e757431c604f5974cfac3d03343	performance optimization of semi-markov decision processes with discounted-cost criteria	modelizacion;politica optima;poisson equation;optimisation;convergence;optimizacion;proceso markov;decision markov;condicion estacionaria;equation poisson;condition stationnaire;ecuacion poisson;optimal policy;discounted poisson equation;value iteration;modelisation;convergencia;semi markov decision process;processus markov;semi markov decision processes;rebajas;markov process;stationary condition;markov decision;optimization;policy iteration;α potential;discount;politique optimale;rabais;discounted cost criteria;modeling;performance optimization	We discuss the problems of discounted-cost performance optimization for a class of semi-Markov decision processes (SMDPs). We define a matrix which can be used as the infinitesimal generator of a Markov process. The discounted Poisson equation is proposed for an SMDP by using this matrix, from which the -potential is defined. The optimality equation satisfied by the optimal stationary policy is given and the relation between discounted model and average model is discussed. Two iteration algorithms to find -optimal policies are proposed and the proofs of convergence of these two algorithms are given. A numerical example is provided to illustrate the application of the algorithms.	algorithm;iteration;markov chain;markov decision process;mathematical optimization;numerical analysis;performance tuning;semiconductor industry;stationary process	Baoqun Yin;Yanjie Li;Yaping Zhou;Hongsheng Xi	2008	Eur. J. Control	10.3166/ejc.14.213-222	markov decision process;mathematical optimization;combinatorics;systems modeling;convergence;poisson's equation;mathematics;markov process;mathematical economics;statistics	ML	38.05628142117553	5.499731946830207	148968
724643a39bc40c1c3bbc7c94ed7a74300382cc63	exploring the disjunctive rank of some facet-inducing inequalities of the acyclic coloring polytope	acyclic coloring;disjunctive rank	In a previous work we presented six facet-inducing families of valid inequalities for the polytope associated to an integer programming formulation of the acyclic coloring problem. In this work we study their disjunctive rank, as defined by [E. Balas, S. Ceria and G. Cornuejols, Math. Program. 58 (1993) 295–324]. We also propose to study a dual concept, which we call the disjunctive anti-rank of a valid inequality.	acyclic coloring;directed acyclic graph;disjunctive normal form;graph coloring	Mónica Braga;Javier Marenco	2016	RAIRO - Operations Research	10.1051/ro/2015053	complete coloring;mathematics	Theory	24.756668795234194	15.008403673171964	149395
099852b611e634b90ad7b06822f93e98a2f96f1e	adaptive spatial decomposition in fast multipole method	cluster algorithm;multipoles;algoritmo adaptativo;fast multipole method;calculation;three dimensional;methode calcul;multipole;adaptive algorithm;algorithme adaptatif;technique calcul;tree data structure;calculation methods;hybrid boundary node method;adaptive node cluster;data structure	This work presents a new adaptive node-cluster algorithm for fast multipole method. In the algorithm, we use rectangular boxes instead of cubes, subdivide a box based on its shape, and tighten the child boxes at each subdivision step. More importantly, we determine the number of expansion terms in multipole to local translations according to the distance between the two interaction boxes. Our method is tested using benchmark examples for three-dimensional potential problems. The results obtained show that the new algorithm can solve a problem with 100 thousands nodes in about 20 min, and runs nearly three times faster than the standard algorithm. The proposed algorithm is especially suitable for treating slender and shell-like structures. 2007 Elsevier Inc. All rights reserved.	adaptive algorithm;benchmark (computing);binary tree;computation;data structure;fast multipole method;maxima and minima;numerical analysis;olap cube;octal;octree;s-box;subdivision surface;tree (data structure)	Jianming Zhang;Masataka Tanaka	2007	J. Comput. Physics	10.1016/j.jcp.2007.03.032	three-dimensional space;mathematical optimization;combinatorics;calculation;data structure;fast multipole method;multipole expansion;mathematics;tree;algorithm	EDA	32.88483525254062	14.681605877178447	149500
cba37f9fa9c4bb089871ed446aa5ce48da65b3e7	exact bounding spheres by iterative octant scan	computational geometry;geometric algorithms;bounding spheres	We propose an exact minimum bounding sphere algorithm for large point sets in low dimensions. It aims to reduce the number of required passes by retrieving a well-balanced set of outliers in each linear search through the input. The behaviour of the algorithm is mainly studied in the important three-dimensional case. The experimental evidence indicates that the convergence rate is superior compared to previous exact methods, which effectively results in up to three times as fast execution times. Furthermore, the run times are not far behind simple 2-pass constant approximation heuristics.	approximation algorithm;bounding sphere;heuristic (computer science);iterative method;linear search;rate of convergence	Thomas Larsson	2015			bounding volume;classical mechanics;bounding interval hierarchy;minimum bounding box;theoretical computer science;geometry;minimum bounding box algorithms;bounding volume hierarchy;bounding sphere	AI	32.56239214528353	15.413386671166014	149511
c5a00e9e8bc273bf590916ca90cfb15cc0872022	an algorithm for mixed-integer optimal control of solar thermal climate systems with mpc-capable runtime		This work presents an algorithm for solution of Mixed-Integer Optimal Control Problems (MIOCPs) for Solar Thermal Climate Systems (STCSs) with MPC-capable runtime. We implement the so-called Combinatorial Integral Approximation (CIA) algorithm for a model of an STCS of a building that incorporates an adsorption cooling machine and apply the algorithm within a numerical case study to solve a Mixed-Integer Non-Linear Program (MINLP) resulting from an MIOCP for the system. We compare the results of the CIA algorithm to those of a general MINLP solver and show that our algorithm achieves comparable solution quality at a runtime that is up to 1000 times smaller.	approximation;computer cooling;cool - action;integer (number);linear iga bullous dermatosis;linear programming;mesenchymal stem cells;numerical analysis;one thousand;optimal control;small;solver;algorithm;non-t, non-b, calla negative childhood acute lymphoblastic leukemia	Adrian B&#xFC;rger;Clemens Zeile;Angelika Altmann-Dieses;Sebastian Sager;Moritz Diehl	2018	2018 European Control Conference (ECC)	10.23919/ECC.2018.8550424	heat transfer;algorithm;thermal;optimal control;atmospheric model;integer;computer science;solver	EDA	28.901034145301157	6.724025644112785	149630
c0fea1849a4ad98882a6e561a04bfd5951022a58	a bit-array representation ga for structural topology optimization	topology optimized production technology optimization methods numerical simulation production engineering testing computational efficiency design optimization power generation economics solids;topology;minimum weight design problem bit array representation ga structural topology optimization hierarchical violation penalty constraint function genetic algorithm performance testing computational cost representation degeneracy design connectivity problem connectivity analysis population initialization method;topology optimization;penalty method;numerical analysis;structural engineering computing;genetic algorithms;numerical analysis genetic algorithms topology structural engineering computing	A bit-array representation method for structural topology optimization using the GA is proposed. The importance of design connectivity is further emphasized and a hierarchical violation penalty method is proposed to penalize the violated constraint functions so that the problem of representation degeneracy can be overcome and the GA search can be driven towards the combination of better structural performance, less unusable material and fewer connected objects in the design domain. An identical initialization method is also proposed to test the performance of the GA operators. With the appropriately selected GA operators, the bit-array representation GA is applied to the structural topology optimization problems of minimum weight. Numerical results demonstrate that the present GA can achieve better accuracy with less computational cost and suggest that the GA performance can be significantly improved by handling the design connectivity properly.	bit array;mathematical optimization;software release life cycle;topology optimization	Sheng Yin Wang;Kang Tai	2003		10.1109/CEC.2003.1299640	mathematical optimization;topology optimization;computational topology;genetic algorithm;numerical analysis;computer science;theoretical computer science;machine learning;penalty method;mathematics	Vision	29.379845087557566	4.559387221687771	150036
8d2367212dca83a898e9d820f35f06016dc0ca6a	new values in domineering	new value	We find previously unknown values in Domineering games by constructing a repeating Domineering position which has corners and kinks. By varying the construction, we find values which include arbitrarily small numbers, some hot switches and some infinitesimals.	network switch;value (computer science)	Yonghoan Kim	1996	Theor. Comput. Sci.	10.1016/0304-3975(95)00150-6	mathematical optimization;combinatorics;discrete mathematics;mathematics	ECom	36.31830821794342	17.92912290660236	150247
5b77c62f07dce6fa83d61a91fd73c8759b7db29c	robotic surveillance and markov chains with minimal first passage time	robotic surveillance sdp semidefinite programs convex optimization problems reversible markov chains weighted kemeny constant heterogeneous service time heterogeneous travel time mean first passage time optimization discrete network environments anomaly detection stochastic surveillance strategies minimal first passage time;markov processes surveillance symmetric matrices vectors minimization algorithm design and analysis robots;surveillance convex programming graph theory markov processes mobile robots	We propose stochastic surveillance strategies for quickest detection of anomalies in discrete network environments. Our surveillance strategy is determined by optimizing the mean first passage time also known as the Kemeny constant of a Markov chain. We generalize the notion of the Kemeny constant to environments with heterogeneous travel and service times, denote this generalization as the weighted Kemeny constant, and characterize its properties. For reversible Markov chains, we show that both the Kemeny constant and its heterogeneous counterpart can be formulated as convex optimization problems and, moreover, can be expressed as semidefinite programs (SDPs). We numerically illustrate the proposed design: compared with other well-known Markov chains, the performance of our Kemeny-based strategies are always better and in many cases substantially so.	convex function;convex optimization;first-hitting-time model;markov chain;mathematical optimization;numerical analysis;semidefinite programming;stochastic matrix	Pushkarini Agharkar;Rushabh Patel;Francesco Bullo	2014	53rd IEEE Conference on Decision and Control	10.1109/CDC.2014.7040425	mathematical optimization;simulation;machine learning;mathematics	ML	38.869996403505745	5.24827753312629	150278
cbe53bda2f7e6d3666fd1f2b9ff852b996625c19	broadcasting colourings on trees. a combinatorial view		The broadcasting models on a d-ary tree T arise in many contexts such as discrete mathematics, biology, information theory, statistical physics and computer science. We consider the k-colouring model, i.e. the root of T is assigned an arbitrary colour and, conditional on this assignment, we take a random colouring of T . A basic question here is whether the information of the assignment at the root affects the distribution of the colourings at the leaves. This is the so-called reconstruction/nonreconstruction problem. It is well known that d/ ln d is a threshold function for this problem, i.e. • if k ≥ (1 + )d/ ln d, then the colouring of the root has a vanishing effect on the distribution of the colourings at the leaves, as the height of the tree grows • if k ≤ (1− )d/ ln d, then the colouring of the root biases the distribution of the colouring of the leaves regardless of the height of the tree. However, there is no apparent combinatorial reason why such a result should be true. When k ≥ (1 + )d/ ln d, the threshold implies the following: We can couple two broadcasting processes that assign the root different colours such that the probability of having disagreement at the leaves reduces with their distance from the root. It is natural to perceive such coupling as a mapping from the colouring of the first broadcasting process to the colouring of the second one. In that terms, here, we study how can we have such a mapping “combinatorially”. Devising a mapping where the disagreements vanish as we move away from the root turns out to be a non-trivial task to accomplish for any k ≤ d. In this work we obtain a coupling which has the aforementioned property for any k > 3d/ ln d, i.e. much smaller than d. Interestingly enough, the decisions that we make in the coupling are somehow local. It is not clear clear whether such a coupling should be local for any k down to d/ ln d. Finally, we relate our result to sampling k-colourings of sparse random graphs, with expected degree d and k ≤ d.	color;computer science;coupling (computer programming);discrete mathematics;graph coloring;information theory;random graph;sampling (signal processing);sparse matrix;vanish (computer science)	Charilaos Efthymiou	2012	CoRR		combinatorics;discrete mathematics;mathematics;algorithm	Theory	37.87101964650757	16.893047189105307	150465
9ada5b9984bc861172f32269309f53c3d26847f8	line-of-sight rendezvous	street;search problem;agent mobile;mobile agents;qa mathematics;agente movil;red aritmetica;propagation visibilite directe;problema investigacion;problema cita;resolucion problema;search;probleme rendez vous;integer lattice;fork join problem;rendezvous;mobile agent;calle;reseau arithmetique;probleme recherche;propagacion visibilidad directa;rue;line of sight;problem solving;resolution probleme;line of sight propagation	We consider the rendezvous problem faced by two mobile agents, initially placed according to a known distribution on intersections in Manhattan (nodes of the integer lattice Z). We assume they can distinguish streets from avenues (the two axes) and move along a common axis in each period (both to an adjacent street or both to an adjacent avenue). However they have no common notion of North or East (positive directions along axes). How should they move, from node to adjacent node, so as to minimize the expected time required to ‘see’ each other, to be on a common street or avenue. This is called ‘line-of-sight’ rendezvous. It is equivalent to a rendezvous problem where two rendezvousers attempt to find each other via two means of communication. We show how this problem can be reduced to a double alternating search (DAS) problem in which a single searcher minimizes the time required to find one of two objects hidden according to known distributions in distinct regions (e.g. a datum held on multiple disks), and we develop a theory for solving the latter problem. The DAS problem generalizes a related search problem introduced earlier by the author and J.V. Howard. We solve the original rendezvous problem in the case that the searchers are initially no more than four streets or avenues apart. 2007 Elsevier B.V. All rights reserved.	algorithm;apache axis;average-case complexity;dynamic programming;email;geodetic datum;line-of-sight (missile);mobile agent;school of the legends;search problem;stationary process;telephone number	Steve Alpern	2008	European Journal of Operational Research	10.1016/j.ejor.2007.03.043	simulation;integer lattice;search problem;computer science;artificial intelligence;mobile agent;mathematics	AI	28.185435990432268	16.371517488042148	150749
92460afe4f87b8466f3943455652211c31a87372	symmetry breaking constraints for the problem of packing equal circles in a square		The Packing Equal Circles in a Square (PECS) problem is a nonconvex nonlinear optimization problem which involves a high degree of symmetry. The Branch-and-Bound algorithms work bad due to the presence of symmetric optima, because the Branch-and-Bound tree becomes large, and the time to reach the leaves (i.e., the optimal solutions) increases. In this paper, we introduce some inequalities which reduce the symmetry of the problem, and we present some numerical results.	algorithm;branch and bound;mathematical optimization;nonlinear programming;nonlinear system;numerical analysis;optimization problem;set packing;symmetry breaking	Alberto Costa;Ider Tseveendorj	2012			combinatorics;circle packing;square packing in a square	AI	25.81479397120142	15.669201244543126	150862
c7cf3ac31ceabe1606a5c92a5e1d15904370bd37	improving performance in combinatorial optimisation using averaging and clustering	vertex cover;search algorithm;search strategy;np hard problem;large scale;landscape structure;combinatorial optimisation	In a recent paper an algorithm for solving MAX-SAT was proposed which worked by clustering good solutions and restarting the search from the closest feasible solutions. This was shown to be an extremely effective search strategy, substantially out-performing traditional optimisation techniques. In this paper we extend those ideas to a second classic NP-Hard problem, namely Vertex Cover. Again the algorithm appears to provide an advantage over more established search algorithms, although it shows different characteristics to MAX-SAT. We argue this is due to the different large-scale landscape structure of the two problems.	cluster analysis;combinatorial optimization;computer cluster;global illumination;mathematical optimization;max;maximum satisfiability problem;np-hardness;regular expression;search algorithm;vertex cover	Mohamed Qasem;Adam Prügel-Bennett	2009		10.1007/978-3-642-01009-5_16	beam search;mathematical optimization;combinatorics;machine learning;mathematics;best-first search;combinatorial search;search algorithm	AI	24.713699923053973	4.411945637032738	150877
169f4433080ac32bc16bcee1d90630fd69f1d6c9	sequential convex approximations to joint chance constrained programs: a monte carlo approach	grupo de excelencia;chance constrained program;stochastic;ciencias basicas y experimentales;matematicas;grupo a;programming	When there is parameter uncertainty in the constraints of a convex optimization problem, it is natural to formulate the problem as a joint chance constrained program (JCCP) which requires that all constraints be satisfied simultaneously with a given large probability. In this paper, we propose to solve the JCCP by a sequence of convex approximations. We show that the solutions of the sequence of approximations converge to a Karush-Kuhn-Tuck (KKT) point of the JCCP under a certain asymptotic regime. Furthermore, we propose to use a gradient-based Monte Carlo method to solve the sequence of convex approximations.	approximation;converge;convex optimization;gradient;karush–kuhn–tucker conditions;mathematical optimization;monte carlo method;optimization problem	L. Jeff Hong;Yi Yang;Liwei Zhang	2011	Operations Research	10.1287/opre.1100.0910	programming;mathematical optimization;calculus;mathematics;mathematical economics;stochastic;management;statistics	ML	36.30009359266172	4.298343616549987	151104
6fd12521e88f7d6caab9ef6ddfaf8b9d43f3bd74	walking streets faster	streets faster	A fundamental problem in robotics is to compute a path for a robot from its current location to a given goal In this paper we consider the problem of a robot equipped with an on board vision system searching for a goal g in an unknown environment We assume that the robot is originally located at a point s on the boundary of a street polygon A street is a simple polygon with two distinguished points s and g which are located on the polygon boundary and the part of the polygon boundary from s to g is weakly visible to the part from g to s and vice versa Our aim is to minimise the ratio of the length of the path traveled by the robot to the length of the shortest path from s to g In analogy to on line algorithms this value is called the competitive ratio We present a series of strategies all of which follow the same general high level strategy In the rst part we present a class of strategies any of which can be shown to have a competitive ratio of These strategies are robust under small navigational errors and their analysis is very simple In the second part we present the strategy continuous lad which is based on the heuristic optimality criterion of minimising the local absolute detour We show an upper bound on the competitive ratio of continuous lad of Finally we also present a hybrid strategy consisting of continuous lad and the strategy Move in Quadrant We show that this combination of strategies achieves a competitive ratio of This about halves the gap between the known p lower bound for this problem and the previously best known competitive ratio of This research is supported by the DFG Project Diskrete Probleme No Ot Department of Computer Science University of Waterloo Waterloo Ontario CANADA N L G e mail alopez o neumann UWaterloo ca Department of Computer Science University of Western Ontario London Ont Canada N A B and Institut f ur Informatik Universit at Freiburg Am Flughafen D Freiburg FRG e mail schuiere informatik uni freiburg de n s d possible placements of g Figure An example of a lower bound for searching in simple polygons	algorithm;aqua;competitive analysis (online algorithm);computer science;email;functional renormalization group;heuristic;high-level programming language;optimality criterion;robot;robotics;shortest path problem	Alejandro López-Ortiz;Sven Schuierer	1996		10.1007/3-540-61422-2_144	discrete mathematics;simulation;robot;computer science;machine vision;robotics;artificial intelligence	Theory	30.634518165242145	17.130835790622182	151206
741c5acd487d3502f39b7577344c3daa643977c3	counting circular arc intersections	efficient algorithm	In this paper efficient algorithms for counting intersections in a collection of circles or circular arcs are presented. An algorithm for counting intersections in a collection of n circles is presented whose running time is O (n3/2+), for any e > 0 is presented. Using this algorithm as a subroutine, it is shown that the intersections in a set of n circular arcs can also be counted in time O(n3/2+). If all arcs have the same radius, the running time can be improved to 0(n4/3+), for any e > 0. Key words, arrangements, point location, random sampling, partition tree AMS subject classifications. 68P05, 68Q25, 68Q40, 68U05	arcs (computing);algorithm;point location;powerset construction;sampling (signal processing);subroutine;time complexity;vhdl-ams	Pankaj K. Agarwal;Micha Sharir	1991		10.1145/109648.109650	combinatorics;mathematics;geometry;algorithm	Theory	30.3632311591656	18.144215203009946	151296
306013f7fc8e36a9ca424852e23fce2ed7ee75d2	dependence between path-length and size in random digital trees		We study the size and the external path length of random tries and show that they are asymptotically independent in the asymmetric case but strongly dependent with small periodic fluctuations in the symmetric case. Such an unexpected behavior is in sharp contrast to the previously known results on random tries that the size is totally positively correlated to the internal path length and that both tend to the same normal limit law. These two dependence examples provide concrete instances of bivariate normal distributions (as limit laws) whose correlation is $0$, $1$ and periodically oscillating. Moreover, the same type of behaviors is also clarified for other classes of digital trees such as bucket digital trees and Patricia tries.		Michael Fuchs;Hsien-Kuei Hwang	2017	J. Applied Probability	10.1017/jpr.2017.56	combinatorics;discrete mathematics;mathematics;statistics	Theory	38.01871400469802	16.7862181422844	151399
9f2493063cc869a7ea385d3ddc398c7e64a61746	evolving bent quaternary functions		Boolean functions have a prominent role in many real-world applications, which makes them a very active research domain. Throughout the years, various heuristic techniques proved to be an attractive choice for the construction of Boolean functions with different properties. One of the most important properties is nonlinearity, and in particular maximally nonlinear Boolean functions are also called bent functions. In this paper, instead of considering Boolean functions, we experiment with quaternary functions. The corresponding problem is much more difficult and presents an interesting benchmark as well as realworld applications. The results we obtain show that evolutionary metaheuristics, especially genetic programming, succeed in finding quaternary functions with the desired properties. The obtained results in the quaternary domain can also be translated into the binary domain, in which case this approach compares favorably with the state-of-the-art in Boolean optimization. Our techniques are able to find quaternary bent functions for up to 8 inputs, which corresponds to obtaining Boolean bent functions of 16 inputs.	benchmark (computing);bent function;boolean algebra;genetic programming;heuristic;mathematical optimization;metaheuristic;nonlinear system	Stjepan Picek;Karlo Knezevic;Luca Mariot;Domagoj Jakobovic;Alberto Leporati	2018	2018 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2018.8477677	computer science;mathematical optimization;evolutionary computation;boolean function;genetic programming;metaheuristic;nonlinear system;binary number;heuristic;bent molecular geometry	DB	27.540673773420355	4.261894985100904	151732
a68a6e54a40d56eb1563e933632badba548d29f5	a duality theorem for linear congruences	qa mathematics	An analogous duality theorem to that for Linear Programming is presented for systems of linear congruences. It is pointed out that such a system of linear congruences is a relaxation of an Integer Programming model (for which the duality theorem does not hold). Algorithms are presented for both the resulting primal and dual problems. These algorithms serve to give a constructive proof of the duality theorem.	algorithm;integer programming;linear programming relaxation;programming model	H. P. Williams	1984	Discrete Applied Mathematics	10.1016/0166-218X(84)90116-1	perturbation function;mathematical optimization;combinatorics;discrete mathematics;duality;duality;fenchel's duality theorem;duality gap;max-flow min-cut theorem;weak duality;fundamental theorem;mathematics;poincaré duality;strong duality;algebra	Theory	25.457094598709922	13.579096032118846	151748
b38c64baf2783e2c57797982895bb35b5bcddbb6	computational study of the gdpo dual phase-1 algorithm	phase 1;piecewise linear;dual simplex;simplex method;linear programming;linear program;computer analysis;dual phase	The GDPO algorithm for phase-1 of the dual simplex method developed by Maros possesses some interesting theoretical features that have potentially huge computational advantages. This paper gives account of a computational analysis of GDPO that has investigated how these features work in practice by exploring the internal operation of the algorithm. Experience of a systematic study involving 48 problems gives an insight how the predicted performance advantages materialize that ultimately make GDPO an indispensable tool for dual phase-1.	algorithm;computation;dilution of precision (navigation)	István Maros	2010	Comput. Manag. Science	10.1007/s10287-009-0094-7	mathematical optimization;simulation;piecewise linear function;linear programming;mathematics;simplex algorithm;algorithm	Logic	30.839299583227405	8.215585503822606	151903
585e937f6220f37d697bbc7c1eb2a8063c389d82	influence of the null-model on motif detection	graph theory;key user identification;probability;approximation algorithms;events in twitter;motif count approximation motif detection motif analysis graph theoretic null model influence probability function fdsm time complexity kolmogorov smirnow two sample test;real time feature computation;computational modeling;mathematical model approximation algorithms computational modeling switches silicon compounds graph theory probability;on and off topic features;silicon compounds;mathematical model;probability computational complexity graph theory;switches;svm classification and ranking	This paper focuses on the suitability of three different null-models to motif analysis that all get as an input a desired degree sequence. A graph theoretic null-model is defined as a set of graphs together with a probability function. Here we discuss the configuration model, as the simplest model; a variant of the configuration model where multi-edges are deleted; and the set of all graphs with a given degree sequence (FDSM), that most scientists would recommend to use but that has the disadvantage of a high time-complexity to sample from it. Furthermore, we develop equations for the expected number of motifs in the FDSM, based on the degree sequence and the assumption of simple independence. We present the motif count for several real-world graphs and compare them with the sampled average number of these motif counts in the different null-models. We check with a Kolmogorov-Smirnow two-sample test whether the samples originated from the same distribution. It can then be shown that the motif counts in the configuration model do not coincide with those of the FDSM. The equations are a good enough approximation of the motif count in generated graphs based on a prescribed degree sequence.	approximation;degree (graph theory);graph (discrete mathematics);graph theory;kolmogorov complexity;principle of good enough;sequence motif;time complexity	Wolfgang Eugen Schlauch;Katharina Anna Zweig	2015	2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)	10.1145/2808797.2809400	combinatorics;discrete mathematics;network switch;graph theory;machine learning;probability;mathematical model;data mining;mathematics;computational model;statistics	DB	37.86374840901242	14.461440309564422	152080
687d820c62f0f8b4dc30aa295d2219dce5165d0a	computing quantiles in markov reward models	bahnsysteme	Probabilistic model checking mainly concentrates on techniques for reasoning about the probabilities of certain path properties or expected values of certain random variables. For the quantitative system analysis, however, there is also another type of interesting performance measure, namely quantiles. A typical quantile query takes as input a lower probability bound p ∈ ]0, 1] and a reachability property. The task is then to compute the minimal reward bound r such that with probability at least p the target set will be reached before the accumulated reward exceeds r. Quantiles are well-known from mathematical statistics, but to the best of our knowledge they have not been addressed by the model checking community so far. In this paper, we study the complexity of quantile queries for until properties in discrete-time finite-state Markov decision processes with nonnegative rewards on states. We show that qualitative quantile queries can be evaluated in polynomial time and present an exponential algorithm for the evaluation of quantitative quantile queries. For the special case of Markov chains, we show that quantitative quantile queries can be evaluated in pseudo-polynomial time.	emoticon;graph coloring;markov chain;markov decision process;model checking;np-hardness;polynomial;probabilistic ctl;pseudo-polynomial time;reachability;search algorithm;system analysis;time complexity	Michael Ummels;Christel Baier	2013		10.1007/978-3-642-37075-5_23	discrete mathematics;computer science;machine learning;mathematics;statistics	Logic	37.644366113684555	5.682752082785977	152131
3257d12e404ee19eae620935e5a41cbad616f8e3	an outer approximation algorithm for generating all efficient extreme points in the outcome set of a multiple objective linear programming problem	approximate algorithm;multiple objective linear programming;efficient set;outer approximation;extreme point;global optimization;vector maximization	Various difficulties have been encountered in using decision set-based vector maximization methods to solve a multiple objective linear programming problem (MOLP). Motivated by these difficulties, some researchers in recent years have suggested that outcome set-based approaches should instead be developed and used to solve problem (MOLP). In this article, we present a finite algorithm, called the Outer Approximation Algorithm, for generating the set of all efficient extreme points in the outcome set of problem (MOLP). To our knowledge, the Outer Approximation Algorithm is the first algorithm capable of generating this set. As a by-product, the algorithm also generates the weakly efficient outcome set of problem (MOLP). Because it works in the outcome set rather than in the decision set of problem (MOLP), the Outer Approximation Algorithm has several advantages over decision set-based algorithms. It is also relatively easy to implement. Preliminary computational results for a set of randomly-generated problems are reported. These results tangibly demonstrate the usefulness of using the outcome set approach of the Outer Approximation Algorithm instead of a decision set-based approach.	approximation algorithm	Harold P. Benson	1998	J. Global Optimization	10.1023/A:1008215702611	extreme point;mathematical optimization;combinatorics;machine learning;mathematics;set function;global optimization	Theory	31.780167948486305	5.718317297701122	152159
03eee4f271dff86e7e0691cb3e6a0eb3f86ec2fd	sweep as a generic pruning technique applied to the non-overlapping rectangles constraint	chevauchement;forbidden line;geometrie algorithmique;computational geometry;rectangular shape;global constraint;overlap;constraint satisfaction;imbricacion;satisfaction contrainte;eficacia barrido;sweep efficiency;geometria computacional;satisfaccion restriccion;efficacite balayage;raie interdite;raya prohibida;forma rectangular;forme rectangulaire	We firstpresen ta generic pruning technique which aggregates several constraints sharing some variables. The method is derived from an idea called sweep which is extensively used in computational geometry. A first benefit of this technique comes from the fact that it can be applied to several families of global constraints. A second advantage is that it does not lead to any memory consumption problem since it only requires temporary memory which can be reclaimed after each invocation of the method.We then specialize this technique to the non-overlapping rectangles constraint, describe several optimizations, and give an empirical evaluation based on six sets of test instances with different characteristics.		Nicolas Beldiceanu;Mats Carlsson	2001		10.1007/3-540-45578-7_26	mathematical optimization;constraint satisfaction;computational geometry;computer science;mathematics;algorithm	AI	29.037008911079088	15.624739531761936	152451
34700b31922a6af5f1532b75fba954a9a3998673	the convex subclass method: combinatorial classifier based on a family of convex sets	metodo polinomial;algoritmo aleatorizado;analyse exploratoire;temps polynomial;analisis datos;metodo combinatorio;exploratory analysis;methode combinatoire;plan randomise;intelligence artificielle;algorithme randomise;data mining;algorithme numerique;sphere;data analysis;plan aleatorizado;fouille donnee;polynomial method;randomized design;exact algorithm;numerical algorithm;polynomial time;recouvrement ensemble;pattern classification;ensemble convexe;randomized algorithm;pattern recognition;algoritmo numerico;artificial intelligence;analyse donnee;combinatorial method;esfera;inteligencia artificial;reconnaissance forme;convex set;set covering;numerical experiment;cubierta conjunto;reconocimiento patron;methode polynomiale;busca dato;conjunto convexo;analisis exploratorio;exploratory data analysis;tiempo polinomial;classification forme	We propose a new nonparametric classification framework for numerical patterns, which can also be exploitable for exploratory data analysis. The key idea is approximating each class region by a family of convex geometric sets which can cover samples of the target class without containing any samples of other classes. According to this framework, we consider a combinatorial classifier based on a family of spheres, each of which is the minimum covering sphere for a subset of positive samples and does not contain any negative samples. We also present a polynomial-time exact algorithm and an incremental randomized algorithm to compute it. In addition, we discuss the soft-classification version and evaluate these algorithms by some numerical experiments.	computational complexity theory;convex function;convex set;exact algorithm;experiment;numerical analysis;parallel computing;randomized algorithm;randomness;time complexity	Ichigaku Takigawa;Mineichi Kudo;Atsuyoshi Nakamura	2005		10.1007/11510888_10	time complexity;combinatorics;computer science;artificial intelligence;data mining;mathematics;convex set;randomized algorithm;data analysis;completely randomized design;exploratory data analysis;algorithm;sphere;statistics	ML	25.260042710358228	9.580638376941401	152468
77a91a8e4b4aea6019215003813e2f9dc7fe6d15	an algorithm for the three-dimensional packing problem with asymptotic performance analysis	three dimensional packing;optimisation;approximate algorithm;three dimensions;optimizacion;approximation numerique;complexite calcul;geometrie algorithmique;approximation algorithm;computational geometry;probleme np dur;problema np duro;asymptotic analysis;asymptotic behavior;comportement asymptotique;three dimensional;aproximacion numerica;theorem proving;algorithme;algorithm;comportamiento asintotico;demonstration theoreme;np hard problem;complejidad computacion;computational complexity;performance analysis;performance bounds;geometria computacional;optimization;numerical approximation;demostracion teorema;algorithme approximation;algoritmo	The three-dimensional packing problem can be stated as follows. Given a list of boxes, each with a given length, width, and height, the problem is to pack these boxes into a rectangular box of fixed-size bottom and unbounded height, so that the height of this packing is minimized. The boxes have to be packed orthogonally and oriented in all three dimensions. We present an approximation algorithm for this problem and show that its asymptotic performance bound is between 2.5 and 2.67. This result answers a question raised by Li and Cheng [5] about the existence of an algorithm for this problem with an asymptotic performance bound less than 2.89.	approximation algorithm;cuboid;set packing	Flávio Keidi Miyazawa;Yoshiko Wakabayashi	1997	Algorithmica	10.1007/BF02523692	three-dimensional space;mathematical optimization;combinatorics;asymptotic analysis;calculus;mathematics;geometry;algorithm;square packing in a square	Theory	29.359133656997745	17.504966124962387	152585
6903f0726090b659c083b786bea284a32acaa425	reconstructing strings from random traces	generalization error;qa mathematics;mutation rate;co connectivity;antihole;evolutionary process;weakly chordal graph;reconstruction algorithm;majority voting;hole	We are given a collection of m random subsequences (traces) of a string t of length n where each trace is obtained by deleting each bit in the string with probability q. Our goal is to exactly reconstruct the string t from these observed traces. We initiate here a study of deletion rates for which we can successfully reconstruct the original string using a small number of samples. We investigate a simple reconstruction algorithm called Bitwise Majority Alignment that uses majority voting (with suitable shifts) to determine each bit of the original string. We show that for random strings t, we can reconstruct the original string (w.h.p.) for q = O(1/ log n) using only O(log n) samples. For arbitrary strings t, we show that a simple modification of Bitwise Majority Alignment reconstructs a string that has identical structure to the original string (w.h.p.) for q = O(1/n1/2+ε) using O(1) samples. In this case, using O(n log n) samples, we can reconstruct the original string exactly. Our setting can be viewed as the study of an idealized biological evolutionary process where the only possible mutations are random deletions. Our goal is to understand at what mutation rates, a small number of observed samples can be correctly aligned to reconstruct the parent string.In the process of establishing these results, we show that Bitwise Majority Alignment has an interesting self-correcting property whereby local distortions in the traces do not generate errors in the reconstruction and eventually get corrected.	algorithm;bitwise operation;distortion;mutation (genetic algorithm);randomness;tracing (software)	Tugkan Batu;Sampath Kannan;Sanjeev Khanna;Andrew McGregor	2004			mutation rate;majority rule;mathematical optimization;combinatorics;commentz-walter algorithm;theoretical computer science;machine learning;mathematics;geometry;algorithm;statistics;string searching algorithm;generalization error	Theory	35.81382642146852	15.038069942961926	152667
c5131209e9aa0519cb794c1b612e314baf0bc777	exponentially slow mixing in the mean-field swendsen-wang dynamics		Swendsen–Wang dynamics for the Potts model was proposed in the late 1980’s as an alternative to single-site heat-bath dynamics, in which global updates allow this MCMC sampler to switch between metastable states and ideally mix faster. Gore and Jerrum (1999) found that this dynamics may in fact exhibit slow mixing: they showed that, for the Potts model with q ≥ 3 colors on the complete graph on n vertices at the critical point βc(q), Swendsen–Wang dynamics has tmix ≥ exp(c √ n). The same lower bound was extended to the critical window (βs, βS) around βc by Galanis et al. (2015), as well as to the corresponding mean-field FK model by Blanca and Sinclair (2015). In both cases, an upper bound of tmix ≤ exp(c′n) was known. Here we show that the mixing time is truly exponential in n: namely, tmix ≥ exp(cn) for Swendsen–Wang dynamics when q ≥ 3 and β ∈ (βs, βS), and the same bound holds for the related MCMC samplers for the mean-field FK model when q > 2.	color;critical point (network science);forward kinematics;gore (segment);markov chain monte carlo;potts model;sampling (signal processing);time complexity;window of opportunity	Reza Gheissari;Eyal Lubetzky;Yuval Peres	2018		10.1137/1.9781611975031.129	discrete mathematics;potts model;combinatorics;vertex (geometry);mean field theory;critical point (thermodynamics);mathematics;exponential growth;exponential function;upper and lower bounds;complete graph	Theory	37.79096728737313	16.246184646570022	152698
0af7b9a71c53d42327b2eed97072bde2cf34f9c6	approximation bounds for sparse principal component analysis	90c27;62h25;90c22	We produce approximation bounds on a semidefinite programming relaxation for sparse principal component analysis. These bounds control approximation ratios for tractable statistics in hypothesis testing problems where data points are sampled from Gaussian models with a single sparse leading component. We study approximation bounds for a semidefinite relaxation of the sparse eigenvalue problem, written here in penalized form max ‖x‖2=1 xΣx− ρCard(x) in the variable x ∈ Rn, where Σ ∈ Sn and ρ ≥ 0. Sparse eigenvalues appear in many applications in statistics and machine learning. Sparse eigenvectors are often used, for example, to improve the interpretability of principal component analysis, while sparse eigenvalues control recovery thresholds in compressed sensing [Candes and Tao, 2007]. Several convex relaxations and greedy algorithms have been developed to find approximate solutions (see d’Aspremont et al. [2007, 2008], Journée et al. [2008], Journée et al. [2008] among others), but except in simple scenarios where ρ is small and the two leading eigenvalues of Σ are separated, very little is known about the tightness of these approximation methods. Here, using randomization techniques based on [Ben-Tal and Nemirovski, 2002], we derive simple approximation bounds for the semidefinite relaxation derived in [d’Aspremont, Bach, and El Ghaoui, 2008]. We do not produce a constant approximation ratio and our bounds depend on the optimum value of the semidefinite relaxation: the higher this value, the better the approximation. A similar behavior was observed by Zwick [1999] for the semidefinite relaxation to MAXCUT, who showed that the classical approximation ratio of Goemans and Williamson [1995] can be improved when the value of the cut is high enough. We then show that, in some applications, it is possible to bound a priori the optimum value of the semidefinite relaxation, hence produce a lower bound on the approximation ratio. In particular, following recent works by [Amini and Wainwright, 2009, Berthet and Rigollet, 2012], we focus on the problem of detecting the presence of a (significant) sparse principal component in a Gaussian model, hence test the significance of eigenvalues isolated by sparse principal component analysis. More precisely, we apply our approximation results to the problem of discriminating between the two Gaussian models N (0, In) and N ( 0, In + θvv T ) where v ∈ Rn is a sparse vector with unit Euclidean norm and cardinality k. We use a convex relaxation for the sparse eigenvalue problem to produce a tractable statistic for this hypothesis testing problem and show that in a high-dimensional setting where the dimension n, the number of samples m and the cardinality k grow towards infinity proportionally, the detection threshold on θ remains finite. More broadly speaking, in the spirit of smoothed analysis [Spielman and Teng, 2001], this shows that analyzing the performance of semidefinite relaxations on random problem instances is sometimes easier and provides a somewhat more realistic description of typical approximation ratios. Another classical example of this phenomenon is a MAXCUT-like problem arising in statistical physics, for which explicit (asymptotic) formulas can be derived for certain random instances, e.g. the Parisi formula [Mezard et al., 1987, Mezard and Montanari, 2009, Talagrand, 2010] for computing the ground state of spin glasses in the SherringtonKirkpatrick model. It thus seems that comparing the performance of convex relaxations on random problem Date: June 18, 2012. 2010 Mathematics Subject Classification. 62H25, 90C22, 90C27.	approximation algorithm;cobham's thesis;compressed sensing;data point;greedy algorithm;ground state;karloff–zwick algorithm;linear programming relaxation;machine learning;mathematics subject classification;maximum cut;principal component analysis;relaxation (approximation);semidefinite programming;sensor;smoothed analysis;smoothing;sparse matrix	Alexandre d'Aspremont;Francis R. Bach;Laurent El Ghaoui	2014	Math. Program.	10.1007/s10107-014-0751-7	mathematical optimization;sparse pca;combinatorics;sparse approximation;mathematics;approximation algorithm;statistics	ML	39.08658002926091	12.947214109562708	152807
291e150dd6f6e0ff46985fb7324e8828f6e87afe	designing digital circuits for the knapsack problem	subset sum problem;genetic pro gramming;multiple solution;knapsack problem;multi expression programming;numerical experiment;digital circuits;np complete problem	Multi Expression Programming (MEP) is a Genetic Programming variant that uses linear chromosomes for solution encoding. A unique feature of MEP is its ability of encoding multiple solutions of a problem in a single chromosome. In this paper we use Multi Expression Programming for evolving digital circuits for a well-known NP-Complete problem: the knapsack (subset sum) problem. Numerical experiments show that Multi Expression Programming performs well on the considered test problems.	complete (complexity);digital electronics;experiment;genetic programming;knapsack problem;media-embedded processor;multi expression programming;np-completeness;numerical method;subset sum problem	Mihai Oltean;Crina Grosan;Mihaela Oltean	2004		10.1007/978-3-540-24688-6_162	continuous knapsack problem;mathematical optimization;combinatorics;discrete mathematics;polynomial-time approximation scheme;np-complete;computer science;generalized assignment problem;cutting stock problem;change-making problem;mathematics;knapsack problem;subset sum problem;digital electronics;algorithm	AI	26.283888377501643	4.640548327539513	152941
6190aa3c247b94bedd830f78cea4ce1af587108f	on the confluence of the graphic calculus with penrose diagrams (i)	necessary background;general model;graphic calculus;complex calculus;practical tool;interchange law;solid theoretical foundation;different kind;penrose diagram;graphical stuff	In paper Molinelli et al, 1998 a general model allowing the integration of different kinds of calculus with diagrams appearing in several fields of Science and Engineering was introduced. And also, a computer aided system enabling some manipulation of this graphical stuff was presented.#R##N##R##N#Traditionally most of these diagrams have been used as an aid in the development of complex calculus, although the lack of a solid theoretical foundation has prevent the existence of practical tools.#R##N##R##N#As a contribution to that necessary background, we present here an implementation of the diagrams using Coq and a first discussion on the confluence of the rewriting based on the interchange law.	confluence;diagram	José Luis Freire-Nistal;Antonio Blanco Ferro;J. M. Molinelli Barba;Enrique Freire Brañas	2011		10.1007/978-3-642-27549-4_22	computer science;artificial intelligence;pure mathematics;mathematics;story-driven modeling;algorithm	Logic	34.50085822385109	14.580958648063389	152986
7adb043655135376ae470f2ff95be6631a381db9	une contrainte globale pour le problème de l'isomorphisme de graphes	global constraint;contrainte globale keywords: graph isomorphism;mots-clés : isomorphisme de graphes;graph isomorphism;constraint satisfaction problem;constraint programming	The graph isomorphism problem consists in deciding if two given graphs have an identical structure. This problem can be modeled as a constraint satisfaction problem in a very straightforward way, so that one can use constraint programming to solve it. However, constraint programming is a generic tool that may be less efficient than dedicated algorithms which can take advantage of the global semantic of the original problem. Hence, we introduce in this paper a new global constraint dedicated to graph isomorphism problems, and we define an associated filtering algorithm that exploits all edges of the graphs in a global way to narrow variable domains. We then show how this global constraint can be decomposed into a set of “distance” constraints which propagate more domain reductions than “edge” constraints that are usually generated for this problem. MOTS-CLÉS : Isomorphisme de graphes, contrainte globale	algorithm;constraint programming;constraint satisfaction problem;graph isomorphism problem	Sébastien Sorlin;Christine Solnon	2004			combinatorics;mathematical economics;mathematics	AI	24.684573046480384	15.567468257272983	153200
76ed23108515620d2977b7ac3d47879ab781372b	a multiplicative weights update algorithm for packing and covering semi-infinite linear programs		We consider the following semi-infinite linear programming problems: (max ) (resp., (min )) (c^Tx) s.t. (y^TA_ix+(d^i)^Tx le b_i) (resp., (y^TA_ix+(d^i)^Tx ge b_i)), for all (y mathcal Y_i), for (i=1,ldots ,N), where (mathcal Y_isubseteq mathbb {R}^{m_i}_+) are given compact convex sets and (A_iin mathbb {R}^{m_itimes n}_+), (b=(b_1,ldots ,b_N)in mathbb {R}_+^N), (d_iin mathbb {R}_+^n), and (cin mathbb {R}_+^n) are given non-negative matrices and vectors. This general framework is useful in modeling many interesting problems. For example, it can be used to represent a sub-class of Robust optimization in which the coefficients of the constraints are drawn from convex uncertainty sets (mathcal Y_i), and the goal is to optimize the objective function for the worst-case choice in each (mathcal Y_i). When the uncertainty sets (mathcal Y_i) are ellipsoids, we obtain a sub-class of Second-Order Cone Programming. We show how to extend the multiplicative weights update method to derive approximation schemes for the above packing and covering problems. When the sets (mathcal Y_i) are simple, such as ellipsoids or boxes, this yields substantial improvements in the running time over general convex programming solvers.	algorithm;linear programming;set packing;system of linear equations	Khaled M. Elbassioni;Kazuhisa Makino;Waleed Najy	2016		10.1007/978-3-319-51741-4_7	mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	25.408865547037305	14.469059060369574	153490
0480dc0f99fdbf6c94edca7993ef3f87c51cefe9	finding integer efficient solutions for multiple objective network programming problems	network flow algorithms;multiple criteria decision making;integer programming;multiple objective programming	For many practical multiple objective network programming (MONP) problems, only integer solutions are meaningful and acceptable. Representative efficient solutions are usually generated by solving augmented weighted Tchebycheff network programs (AWTNPs), sub-problems derived from MONP problems. However, efficient solutions generated this way are usually not integer valued. In this study, two algorithms are developed to construct integer efficient solutions starting from fractional efficient solutions. One algorithm finds a single integer efficient solution in the neighborhood of the fractional efficient solution. The other enumerates all integer efficient solutions in the same neighborhood. Theory supporting the proposed algorithms is developed. Two detailed examples are presented to demonstrate the algorithms. Computational results are reported. The best integer efficient solution is very close, if not equal, to the integer optimal solution. The CPU time taken to find integer efficient solutions is negligible, when compared with that taken to solve AWTNPs. © 2010 Wiley Periodicals, Inc. NETWORKS, Vol. 57(4), 362–375 2011	algorithm;central processing unit;computation;computer network programming;john d. wiley	Minghe Sun	2011	Networks	10.1002/net.20407	mathematical optimization;combinatorics;discrete mathematics;integer programming;nearest integer function;branch and price;mathematics	EDA	26.52637373918603	8.5306890769986	153869
e4ceed69fbfe82152042d8433d33922f14a5aa8f	fixed-dimensional stochastic dynamic programs: an approximation scheme and an inventory application	discrete convexity;text;approximation algorithms;lovasz extension;control models;grupo de excelencia;convex functions;ciencias basicas y experimentales;matematicas;discrete functions;grupo a;demand;multidimensional stochastic dynamic programs;multi dimensional stochastic dynamic programs	We study fixed-dimensional stochastic dynamic programs in a discrete setting over a finite horizon. Under the primary assumption that the cost-to-go functions are discrete L -convex, we propose a pseudo-polynomial time approximation scheme that solves this problem to within an arbitrary prespecified additive error of e > 0. The proposed approximation algorithm is a generalization of the explicit-enumeration algorithm and offers us full control in the trade-off between accuracy and running time.#R##N##R##N#The main technique we develop for obtaining our scheme is approximation of a fixed-dimensional L -convex function on a bounded rectangular set, using only a selected number of points in its domain. Furthermore, we prove that the approximation function preserves L -convexity. Finally, to apply the approximate functions in a dynamic program, we bound the error propagation of the approximation. Our approximation scheme is illustrated on a well-known problem in inventory theory, the single-product problem with lost sales and lead times. We demonstrate the practical value of our scheme by implementing our approximation scheme and the explicit-enumeration algorithm on instances of this inventory problem.	approximation	Wei Chen;Milind Dawande;Ganesh Janakiraman	2014	Operations Research	10.1287/opre.2013.1239	convex function;mathematical optimization;combinatorics;discrete mathematics;polynomial-time approximation scheme;mathematics;minimax approximation algorithm;approximation algorithm;demand;algorithm;statistics	Robotics	35.904315735672526	5.37990551344127	154682
b100752fbffcf50cb6b37279a35ca3b341523862	optimal traffic networks	random graph;networks;supply and information networks;cost function;generic model;information network;random graphs;variational approach;minimum spanning tree;shortest path tree;spanning tree;communication	Inspired by studies on the airports’ network and the physical Internet, we propose a general model of weighted networks via an optimization principle. The topology of the optimal network turns out to be a spanning tree that minimizes a combination of topological and metric quantities. It is characterized by strongly heterogeneous traffic, non-trivial correlations between distance and traffic and a broadly distributed centrality. A clear spatial hierarchical organization, with local hubs distributing traffic in smaller regions, emerges as a result of the optimization. Varying the parameters of the cost function, different classes of trees are recovered, including in particular the minimum spanning tree and the shortest path tree. These results suggest that a variational approach represents an alternative and possibly very meaningful path to the study of the structure of complex weighted networks.	centrality;file spanning;loss function;mathematical optimization;minimum spanning tree;physical internet;shortest path problem;variational principle;weighted network	Marc Barthelemy;Alessandro Flammini	2006	CoRR	10.1088/1742-5468/2006/07/L07002	segment tree;random graph;euclidean minimum spanning tree;mathematical optimization;combinatorics;kruskal's algorithm;vantage-point tree;minimum degree spanning tree;spanning tree;prim's algorithm;minimum spanning tree;gomory–hu tree;loop-erased random walk;k-ary tree;spatial network;interval tree;connected dominating set;mathematics;distributed minimum spanning tree;tree traversal	ML	36.01485002569025	12.866976934592065	154713
05cf2622305b2cf7bf0d2be332acc0e93a1ac0d0	minimizing movement: fixed-parameter tractability	fixed parameter tractability;pebbles;movement problems;treewidth	We study an extensive class of movement minimization problems that arise from many practical scenarios but so far have little theoretical study. In general, these problems involve planning the coordinated motion of a collection of agents (representing robots, people, map labels, network messages, etc.) to achieve a global property in the network while minimizing the maximum or average movement (expended energy). The only previous theoretical results about this class of problems are about approximation and are mainly negative: many movement problems of interest have polynomial inapproximability. Given that the number of mobile agents is typically much smaller than the complexity of the environment, we turn to fixed-parameter tractability. We characterize the boundary between tractable and intractable movement problems in a very general setup: it turns out the complexity of the problem fundamentally depends on the treewidth of the minimal configurations. Thus, the complexity of a particular problem can be determined by answering a purely combinatorial question. Using our general tools, we determine the complexity of several concrete problems and fortunately show that many movement problems of interest can be solved efficiently.	cobham's thesis;hardness of approximation;mobile agent;parameterized complexity;polynomial;robot;treewidth	Erik D. Demaine;Mohammad Taghi Hajiaghayi;Dániel Marx	2014	ACM Trans. Algorithms	10.1145/2650247	mathematical optimization;combinatorics;discrete mathematics;mathematics;treewidth;algorithm	Theory	31.435533084106922	17.117383258185615	154974
f67c5090f7f1aae7211a44eb7f21abe84081c051	objective function bounds for the inexact linear programming problem with generalized cost coefficients	objective function;mathematical programming;linear program;programmation mathematique;programmation lineaire inexacte	This pepcr addrcsscs the inexact linear programming problem in which the objective function coefficients are not fixed but lie in some predetermined set, C. Under certain convexity assumptions, standard mathematical programming techniques arc employed to determine worst case and best case solutions. Simulation is then used to explore the properties of the general problem: max (cx : Ax d b) for some c E C. A wide range of configurations is examined and it is statistically demonstrated that the variance of objective function values is proportional to the size and shape of C. A number of examples are given to highlight the results.	best, worst and average case;coefficient;emoticon;linear programming;loss function;mathematical optimization;optimization problem;simulation	Jonathan F. Bard;Sangit Chatterjee	1985	Computers & OR	10.1016/0305-0548(85)90020-6	mathematical optimization;combinatorics;discrete mathematics;linear-fractional programming;linear programming;mathematics	AI	25.719929616309855	11.95800715002878	155020
73817ee7bb4f0e249d28da1c013e508252696888	sampling with arbitrary precision		We study the problem of the generation of a continuous random variable when a source of independent fair coins is available. We first motivate the choice of a natural criterion for measuring accuracy, the Wasserstein L∞ metric, and then show a universal lower bound for the expected number of required fair coins as a function of the accuracy. In the case of an absolutely continuous random variable with finite differential entropy, several algorithms are presented that match the lower bound up to a constant, which can be eliminated by generating random variables in batches.	algorithm;arbitrary-precision arithmetic;differential entropy	Luc Devroye;Claude Gravel	2015	CoRR		mathematical optimization;combinatorics;mathematics;statistics	Theory	35.31805156500807	11.264700021536672	155136
ef873899a8e3c1111070ea476e82972bb94d59eb	iterative solution methods for obtaining the steady-state probability distributions of markovian multi-echelon repairable item inventory systems	iterative method;steady state probability;queueing network;gauss seidel method;methode gauss seidel;loi probabilite;probabilite stationnaire;ley probabilidad;methode gradient biconjugue;proceso markov;sistema n niveles;conjugate gradient method;red cola espera;administracion deposito;metodo iterativo;probability law;metodo gauss seidel;systeme n niveaux;reseau file attente;metodo gradiente conjugado;methode iterative;methode jacobi;processus markov;probability distribution;multilevel system;markov process;metodo jacobi;gestion stock;reparation;methode gradient conjugue;iterative solution;reparacion;inventory control;repair;jacobi method;steady state	Abstract   The Jacobi, Gauss-Seidel and bi-conjugate gradient methods are used to compute the steady-state probability distributions for finite state-space continuous time Markov processes (closed queueing networks) that arise in the modeling of two- and three-echelon repairable item inventory systems. Alternative systems of linear equations that express the steady-state conditions are examined, generation and storage of the transition rate matrix are discussed briefly and various initializations and stopping criteria are tested. Numerical results are given for problems with up to one million states. Good results are obtained with a two-phase algorithm that uses the Gauss-Seidel method first and the bi-conjugate gradient method subsequently.	iterative method;row echelon form;steady state	Donald Gross;Bingchang Gu;Richard M. Soland	1993	Computers & OR	10.1016/0305-0548(93)90103-P	inventory control;probability distribution;mathematical optimization;jacobi method;calculus;mathematics;iterative method;conjugate gradient method;markov process;steady state;algorithm;statistics;gauss–seidel method	Robotics	37.05102070704856	6.368116385144004	155499
346b04958c5fb48e731f38c319309c67a60ee294	regret in online combinatorial optimization	online optimization;mirror descent;multi armed bandits;minimax regret;combinatorial optimization	We address online linear optimization problems when the possible actions of the decision maker are represented by binary vectors. The regret of the decision maker is the difference between her realized loss and the best loss she would have achieved by picking, in hindsight, the best possible action. Our goal is to understand the magnitude of the best possible (minimax) regret. We study the problem under three different assumptions for the feedback the decision maker receives: full information, and the partial information models of the so-called “semi-bandit” and “bandit” problems. Combining the Mirror Descent algorithm and the INF (Implicitely Normalized Forecaster) strategy, we are able to prove optimal bounds for the semi-bandit case. We also recover the optimal bounds for the full information setting. In the bandit case we discuss existing results in light of a new lower bound, and suggest a conjecture on the optimal regret in that case. Finally we also prove that the standard exponentially weighted average forecaster is provably suboptimal in the setting of online combinatorial optimization.	algorithm;combinatorial optimization;descent;information model;linear programming;mathematical optimization;minimax;program optimization;regret (decision theory);semiconductor industry	Jean-Yves Audibert;Sébastien Bubeck;Gábor Lugosi	2014	Math. Oper. Res.	10.1287/moor.2013.0598	mathematical optimization;combinatorial optimization;machine learning;mathematics;mathematical economics;regret	ML	33.331250840557836	5.261356666447876	155844
bcddc3258b334b376bcadae2bf420147cefebafd	on the power of threshold measurements as oracles		We consider the measurement of physical quantities that are thresholds. We use hybrid computing systems modelled by Turing machines having as an oracle physical equipment that measures thresholds. The Turing machines compute with the help of qualitative information provided by the oracle. The queries are governed by timing protocols and provide the equipment with numerical data with (a) infinite precision, (b) unbounded precision, or (c) finite precision. We classify the computational power in polynomial time of a canonical example of a threshold oracle using non-uniform complexity classes.	binary logarithm;brewster's angle;church–turing thesis;complexity class;emoticon;experiment;hybrid system;incidence matrix;international symposium on fundamentals of computation theory;level of measurement;monochrome;numerical analysis;oe-cake!;oracle database;oracle machine;p/poly;photoelectric effect;polynomial;ray (optics);solar cell;stack machine;time complexity;turing machine;yet another	Edwin J. Beggs;José Félix Costa;Diogo Poças;J. V. Tucker	2013		10.1007/978-3-642-39074-6_3	time complexity;discrete mathematics;oracle;mathematics;physical quantity;turing machine;complexity class	Theory	36.04068008679163	10.024825520821524	155900
e041fe57e37385a833c8578d1a6cce3ef9f41984	a simple method for solving 2-dimensional static range searching	2 dimensional		range searching	Mark H. Overmars;Emo Welzl	1985	Bulletin of the EATCS		discrete mathematics;mathematics;theoretical computer science;range searching	Logic	29.436150051152755	17.790810475903655	155908
fbbc9ef927ba83e7ec25e34f18b7eea156da4b97	a linear bound on the number of scalarizations needed to solve discrete tricriteria optimization problems	discrete tricriteria optimization;box algorithm;90c31;scalarization;90c29;90c27	General multi-objective optimization problems are often solved by a sequence of parametric single objective problems, so-called scalarizations. If the set of nondominated points is finite, and if an appropriate scalarization is employed, the entire nondominated set can be generated in this way. In the bicriteria case it is well known that this can be realized by an adaptive approach which, given an appropriate initial search space, requires the solution of at most 2|N | − 1 subproblems, where N denotes the nondominated set of the underlying problem. For higher dimensional problems, the best known bound is O(|N |m−1) with m being the number of objectives. We present a new procedure for finding the entire nondominated set of tricriteria optimization problems for which the number of scalarized subproblems to be solved is bounded by 3|N | − 2. The approach includes an iterative update of the search space that, given a (sub-)set of nondominated points, describes the area in which additional nondominated points may be located. In particular, we show that the number of boxes, into which the search space is decomposed, depends linearly on the number of nondominated points.	algorithm;brute-force search;feasible region;iteration;iterative method;mathematical optimization;multi-objective optimization;neighbourhood (graph theory);thinking outside the box	Kerstin Dächert;Kathrin Klamroth	2015	J. Global Optimization	10.1007/s10898-014-0205-z	mathematical optimization;combinatorics;mathematics;algorithm	Theory	26.063575702841934	5.512094241491887	156066
68adfab3cbdd8a43636415ef3f059f0f249802ec	mathematical formulations for the balanced vertex k-separator problem	graph theory;linear programming graph theory integer programming;communication conference;computational modeling particle separators electronic mail integer linear programming mathematical model jacobian matrices benchmark testing;branching scheme mathematical formulations balanced vertex k separator problem indirected graph minimum cardinality separator disconnected subsets compact integer linear programming formulation polyhedral study polytope exponential size formulation column generation	Given an indirected graph G = (V;E), a Vertex k-Separator is a subset of the vertex set V such that, when the separator is removed from the graph, the remaining vertices can be partitioned into k subsets that are pairwise edge-disconnected. In this paper we focus on the Balanced Vertex k-Separator Problem, i.e., the problem of finding a minimum cardinality separator such that the sizes of the resulting disconnected subsets are balanced. We present a compact Integer Linear Programming formulation for the problem, and present a polyhedral study of the associated polytope. We also present an Exponential-Size formulation, for which we derive a column generation and a branching scheme. Preliminary computational results are reported comparing the performance of the two formulations on a set of benchmark instances.	benchmark (computing);column generation;geometric separator;indirection;integer programming;linear programming formulation;polyhedron;vertex (graph theory)	Denis Cornaz;Fabio Furini;Mathieu Lacroix;Enrico Malaguti;Ali Ridha Mahjoub;Sébastien Martin	2014	2014 International Conference on Control, Decision and Information Technologies (CoDIT)	10.1109/CoDIT.2014.6996889	loop;vertex separator;mathematical optimization;combinatorics;discrete mathematics;feedback vertex set;level structure;vertex cover;degree;edge space;cycle graph;vertex;mathematics	Robotics	25.27842185871349	14.908247581963085	156332
0a89c6f23d337cb11e9d00bcd2302c96b3f7a7e1	multiobjective shape optimization with constraints based on estimation distribution algorithms and correlated information	moea;finite element method;correlated information;objective function;shape optimization;edas;probability distribution;constraint handling;distributed algorithm	A new approach based on Estimation Distribution Algorithms for constrained multiobjective shape optimization is proposed in this article. Pareto dominance and feasibility rules are used to handle constraints. The algorithm uses feasible and infeasible individuals to estimate the probability distribution of evolving designs. Additionally, correlation among problem design variables is used to improve exploration. The design objectives are: minimum weight and minimum nodal displacement. Also, the resulting structures must fulfill three design constraints: a) maximum permissible Von Misses stress, b)connectedness of the structure elements, and c) small holes are not allowed in the structure. The finite element method is used to evaluate the objective functions and stress constraint.	algorithm;displacement mapping;finite element method;mathematical optimization;minimum weight;pareto efficiency;shape optimization;von neumann architecture	Sergio Ivvan Valdez Peña;Salvador Botello Rionda;Arturo Hernández Aguirre	2005		10.1145/1068009.1068135	probability distribution;distributed algorithm;mathematical optimization;combinatorics;computer science;shape optimization;machine learning;finite element method;edas;mathematics	EDA	27.818501954123096	5.601910767852678	156333
7b222a5a0197e16c2d12bffe64f9e4c5cefaa64c	new approaches for understanding the asymptotic complexity of a* tree searching	exponential function;polynomial complexity;random variable	Previous studies of A* tree-searching have modeled heuristics as random variables. The average number of nodes expanded is expressed asymptotically in terms of distance to goal. The conclusion reached is that A* complexity is an exponential function of heuristic error: Polynomial error implies exponential complexity and logarithmic accuracy is required for polynomial complexity. This paper eliminates simplifying assumptions of earlier studies. “Error” is replaced by a concept called “discrepancy”, a measure of the relative attractiveness to A* of a node for expansion when that node is compared with competing nodes on the solution path. According to our model, in order to have polynomial A* complexity, it is not necessary to have the “logarithmic accuracy” described in previous studies. Another way is for a heuristic's values to vary, or cluster, near a “central function” which grows at least as fast as distance to goal. Generally, logarithmic variation or less is adequate. For one class of heuristics considered, the faster this central function grows, the more is variation from it tolerated.	a* search algorithm;computational complexity theory;discrepancy function;heuristic (computer science);polynomial;time complexity	Stephen V. Chenoweth;Henry W. Davis	1992	Annals of Mathematics and Artificial Intelligence	10.1007/BF01543474	time complexity;complexity class;random variable;exponential error;mathematical optimization;combinatorics;discrete mathematics;average-case complexity;ph;double exponential function;structural complexity theory;exponential function;sparse language;worst-case complexity;mathematics;up;pseudo-polynomial time;square-free polynomial;game complexity	AI	35.000724048324585	13.830461353678828	156370
6950b28d2bf8eb30eecb36849c9827b3db765450	bias and overtaking equilibria for zero-sum continuous-time markov games	continuous time;continuous time zero sum markov games;overtaking optimality;91a15;91a25;bias optimality;state space;optimality criteria;60j27	This paper deals with continuous-time zero-sum two-person Markov games with denumerable state space, general (Borel) action spaces and possibly unbounded transition and reward/cost rates. We analyze the bias optimality and the weakly overtaking optimality criteria. An example shows that, in contrast to control (or one-player) problems, these criteria are not equivalent for games.	markov chain	Tomás Prieto-Rumeau;Onésimo Hernández-Lerma	2005	Math. Meth. of OR	10.1007/s001860400392	mathematical optimization;simulation;state space;mathematics;mathematical economics	Theory	37.73742276294676	5.354685311965197	156459
6430ccb327463454e694c9953e0556e3ab27aacf	fast randomized point location without preprocessing in two- and three-dimensional delaunay triangulations	geometric computing;computer aided design;randomized algorithms;delaunay triangulation;information systems;uses;three dimensions;random sampling;visual events;point location;geometry;computational geometry;three dimensional;conversative visibility;hierarchical representations;three dimensional calculations;randomized algorithm;algorithms;99 mathematics computers information science management law miscellaneous;delaunay triangulations;linearized dynamic aspect graphs;temporal coherence;octrees;mathematics computers information science management law miscellaneous;two dimensional calculations	This paper studies the point location problem in Delaunay triangulations without preprocessing and additional storage. The proposed procedure finds the query point by simply “walking through” the triangulation, after selecting a “good starting point” by random sampling. The analysis generalizes and extends a recent result for d = 2 dimensions by proving this procedure takes expected time close to O (n1/(d+1)) for point location in Delaunay triangulations ofn random points ind = 3 dimensions. Empirical results in both two and three dimensions show that this procedure is efficient in practice.  1999 Elsevier Science B.V. All rights reserved.	average-case complexity;delaunay triangulation;monte carlo method;point location;preprocessor;randomized algorithm;sampling (signal processing)	Ernst P. Mücke;Isaac Saias;Binhai Zhu	1996		10.1145/237218.237396	jump-and-walk algorithm;three-dimensional space;combinatorics;delaunay triangulation;computational geometry;theoretical computer science;pitteway triangulation;mathematics;geometry;constrained delaunay triangulation;randomized algorithm;bowyer–watson algorithm;algorithm	Theory	30.708851299875022	17.64476391781102	156920
8076b8d4b2d5c03e9bdc6f0cb17bcee5616febe7	generating a contact state graph of polyhedral objects for robotic application	graph theory;robot sensing systems;topology;concavity;real time;assembly contacts robot sensing systems topology computational complexity collision avoidance;contacts;evolutionary transitions;assembly;position control;computational complexity;vertex triangle contact;robotic assembly graph theory position control;robotic assembly;collision avoidance;contact state graph;convexity;polyhedral objects;robotic application;evolutionary transitions contact state graph polyhedral objects robotic application vertex triangle contact edge edge contact convexity concavity;edge edge contact	Traditional methods require large computation to model a contact state graph of polyhedral objects for robotic application, and moreover they are heuristic. In this paper, we propose a framework to generate the contact state graph automatically. All faces of the polyhedral objects are triangulated. A sub-contact is defined as single contact between two polyhedral objects and a contact state is presented with a set of the sub-contacts. There are two sub-contacts, a vertex-triangle contact and an edge-edge contact. According to convexity or concavity of the edges composing the triangle, the vertex-triangle contact and the edge-edge contact are classified into 10 types and 7 types, respectively. A contact state graph is made by evolutionary transitions of the sub-contacts. This procedure is accomplished only using the topology of the sub-contacts, which is possible in real-time. The proposed framework is evaluated by an example of square peg-in-hole assembly.	computation;concave function;convex function;expect;heuristic;polygon triangulation;polyhedron;real-time clock;robot	Sung Jo Kwak;Seong Youb Chung;Tsutomu Hasegawa	2010	2010 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2010.5650000	combinatorics;topology;frameworks supporting the polyhedral model;convexity;computer science;graph theory;mathematics;assembly;geometry;computational complexity theory	Robotics	33.6143346714541	17.94681088437134	157112
80f28dc4ad44402381d1f13cd60ee4925a6aee58	chaos and gliders in periodic cellular automaton rule 62	chaos;期刊论文;symbolic dynamics;collision;cellular automata;glider		cellular automaton	Fangyue Chen;Lun Shi;Guanrong Chen;Weifeng Jin	2012	J. Cellular Automata		stochastic cellular automaton;cellular automaton;symbolic dynamics;block cellular automaton;simulation;computer science;theoretical computer science;asynchronous cellular automaton;mathematics;glider;algorithm;collision	Logic	38.60715062509699	9.290100955286498	157300
3414ad08b34bb934c54fe32f0bcb581df6c007ee	a dynamic combined flow algorithm for the two-commodity max-flow problem over delay-tolerant networks		The multi-commodity flow problem plays an important role in network optimization, routing, and service scheduling. With the network partitioning and the intermittent connectivity, the commodity flows in delay tolerant networks (DTNs) are time-dependent, which is very different from that over the static networks. As an NP-hard problem, existing works can only obtain sub-optimal results on maximizing the multi-commodity flow of dynamic networks. To overcome these bottlenecks, in this paper, we propose a graph-based algorithm to solve the maximum two-commodity flow problem over the DTNs. Through analyzing the relationship between the two commodities, we propose a maximum two-commodity flow theorem to simplify the coupling two-commodity flow problem as the two single-commodity flow ones. Then, with the help of the storage time aggregated graph (STAG) (a DTN model with less memory), we construct a pair of flow graphs to describe the reduced two single-commodity flows (addition flow and subtraction flow), and design the corresponding flow calculation methods. Moreover, we design a STAG-based dynamic combined flow algorithm to maximize the two-commodity flow. Finally, the computational complexity of the proposed algorithm is analyzed, and its efficacy has also been demonstrated through an illustrative example and numerical simulations.		Tao Zhang;Hongyan Li;Jiandong Li;Shun Zhang;Haiying Shen	2018	IEEE Transactions on Wireless Communications	10.1109/TWC.2018.2872551		Metrics	28.13411658318321	9.530884431467276	157323
4c6047813f5a54f792f7a1f3b752cf586db1d646	futures for partitioning in physical design (tutorial)	ucla;top down;physical design;divide and conquer	The context for partitioning in physical design is dominated by two concerns: top-down design and the focus on spatial embedding. The role of partitioning is exactly that of a facilitator of divide-and-conquer metaheuristics for floorplanning, timing and placement optimization. Formulations or optimization objectives for partitioning follow from its context and role. Finally, the available algorithm technology determines how effectively we can address a given partitioning formulation and optimize a given objective. This invited paper considers the future of partitioning for physical design in light of these factors, and proposes a list of technology needs. A living version of this paper can be found at vlsicad.cs.ucla.edu.	algorithm;floorplan (microelectronics);futures studies;mathematical optimization;metaheuristic;physical design (electronics);top-down and bottom-up design	Andrew B. Kahng	1998		10.1145/274535.274563	physical design;divide and conquer algorithms;simulation;computer science;artificial intelligence;theoretical computer science;top-down and bottom-up design	EDA	26.22938317102398	6.523060579159765	157802
d116d4dcdca1646036ca10a632b05da54b978321	polynomial time solvable algorithms to a class of unconstrained and linearly constrained binary quadratic programming problems	dynamic programming;polynomial time solvable algorithm;binary quadratic programming	Binary quadratic programming (BQP) is a typical integer programming problem widely applied in the field of signal processing, economy, management and engineering. However, it is NP-hard and lacks efficient algorithms. Due to this reason, in this paper, some novel polynomial algorithms are proposed to solve a class of unconstrained and linearly constrained binary quadratic programming problems. We first deduce the polynomial time solvable algorithms to the unconstrained binary quadratic programming problems with Q being a seven-diagonal matrix ðUBQP7Þ and a five-diagonal matrix ðUBQP5Þ respectively with two different approaches. Then, the algorithm to unconstrained problem is combined with the dynamic programming method to solve the linearly constrained binary quadratic programming problem with Q being a five-diagonal matrix ðLCBQP5Þ. In addition, the polynomial solvable feature of these algorithms is analyzed and some specific examples are presented to illustrate these new algorithms. Lastly, we demonstrate their polynomial feature as well as their high efficiency. & 2016 Elsevier B.V. All rights reserved.	algorithm;bqp;decision problem;dynamic programming;integer programming;np-hardness;p (complexity);polynomial;quadratic programming;signal processing;time complexity	Shenshen Gu;Rui Cui;Jiao Peng	2016	Neurocomputing	10.1016/j.neucom.2015.09.130	quadratic unconstrained binary optimization;mathematical optimization;combinatorics;discrete mathematics;computer science;dynamic programming;binary quadratic form;mathematics;matrix polynomial;quadratic programming	AI	25.265725877368997	13.444900286864293	157905
03c93c088609cd1e64b92768163631059b558cb7	characterization and computation of correlated equilibria in infinite games	game theory;discretization algorithms infinite games nash equilibria two player zero sum games polynomial payoffs arbitrary polynomial like games discretization techniques continuous games correlated equilibria approximation polynomial games semidefinite programming relaxation algorithms;nash equilibria;polynomials;approximation theory;relaxation theory approximation theory game theory mathematical programming polynomials;mathematical programming;polynomials nash equilibrium extraterrestrial measurements approximation algorithms usa councils distributed computing algorithm design and analysis;standard definition;semidefinite programming relaxation;relaxation theory;zero sum game;semidefinite program	Motivated by work on computing Nash equilibria in two-player zero-sum games with polynomial payoffs by semidefinite programming and in arbitrary polynomial-like games by discretization techniques, we consider the problems of characterizing and computing correlated equilibria in games with infinite strategy sets. We prove several characterizations of correlated equilibria in continuous games which are more analytically tractable than the standard definition and may be of independent interest. Then we use these to construct algorithms for approximating correlated equilibria of polynomial games with arbitrary accuracy, including a sequence of semidefinite programming relaxation algorithms and discretization algorithms.	approximation algorithm;cobham's thesis;computation;discretization;linear programming relaxation;nash equilibrium;polynomial;semidefinite programming;standard-definition television	Noah D. Stein;Pablo A. Parrilo;Asuman E. Ozdaglar	2007	2007 46th IEEE Conference on Decision and Control	10.1109/CDC.2007.4434890	standard-definition television;game theory;mathematical optimization;combinatorics;mathematics;zero-sum game;mathematical economics;nash equilibrium;polynomial;approximation theory	Theory	31.93745719003965	7.960151295779947	158825
18da64084ae0db9608a58a671e102b7fc1feb7a9	approximating shortest paths on weighted polyhedral surfaces	camino mas corto;geometria euclidiana;metodo caso peor;shortest path;polyedre;euclidean shortest path;time complexity;geometrie algorithmique;edge detection;poliedro;geometrie euclidienne;computational geometry;plus court chemin;polyhedron;deteccion contorno;detection contour;complexite temps;methode cas pire;analyse non convexe;euclidean geometry;geometria computacional;information system;non convex analysis;complejidad tiempo;worst case method;systeme information;reseau triangulaire irregulier;shortest path problem;sistema informacion;analisis no convexo	One common problem in computational geometry is that of computing shortest paths between two points in a constrained domain. In the context of Geographical Information Systems (GIS), terrains are often modeled as Triangular Irregular Networks (TIN) which are a special class on non-convex polyhedra. It is often necessary to compute shortest paths on the TIN surface which takes into account various weights according to the terrain features. We have developed algorithms to compute approximations of shortest paths on non-convex polyhedra in both the unweighted and weighted domain. The algorithms are based on placing Steiner points along the TIN edges and then creating a graph in which we apply Dijkstra's shortest path algorithm. For two points s and t on a non-convex polyhedral surface P , our analysis bounds the approximate weighted shortest path cost as || Π'(s,t)|| ≤ ||Π(s,t)|| + W |L| , where L denotes the longest edge length of \cal P and W denotes the largest weight of any face of P . The worst case time complexity is bounded by O(n 5 ) . An alternate algorithm, based on geometric spanners, is also provided and it ensures that ||Π' (s,t)|| ≤β(||Π(s,t)|| + W|L|) for some fixed constant β >1 , and it runs in O(n 3 log  n) worst case time. We also present detailed experimental results which show that the algorithms perform much better in practice and the accuracy is near-optimal.	approximation algorithm;best, worst and average case;computational geometry;dijkstra's algorithm;geographic information system;geometric spanner;polyhedral;shortest path problem;steiner tree problem;time complexity	Mark Lanthier;Anil Maheshwari;Jörg-Rüdiger Sack	2001	Algorithmica	10.1007/s00453-001-0027-5	euclidean geometry;mathematical optimization;combinatorics;floyd–warshall algorithm;computational geometry;pathfinding;euclidean shortest path;yen's algorithm;mathematics;geometry;shortest path problem;k shortest path routing;shortest path faster algorithm;algorithm	Theory	28.961360921708387	17.598069007911064	159029
f1c61675eb59fed2caa51ebad09f6d6320b88945	an optimization problem with a surprisingly simple solution	optimization problem	Suppose you and n of your friends play the following game. A random number from the uniform distribution on [0, 1] will be generated. This number is called the target. Each of you will independently guess what the target number will be and the person whose guess is closest will be declared the winner. In order to investigate an optimal strategy for this game, we need to assume something about your friends’ guesses. Consider first the case where you have complete knowledge of all your friends’ guesses before you make yours. In that case the optimal strategy is trivial: simply order their guesses and then find the largest gap between successive guesses. If that gap is at least twice as large as that between 0 and the smallest guess and also that between the largest guess and 1, then position yourself halfway between those two guesses. If not, then position yourself as close as the rules allow to the left of the smallest guess or the right of the largest guess, as appropriate. Your friends would likely not find this game to be worth playing and even you would probably not find it interesting. Let us now assume that you do not have exact knowledge of your friends’ guesses. Most of the papers on similar games (see, for example, [1], [7], [2], and [3]) make the assumption that your friends will behave game-theoretically. That is, they will make guesses based on what they think your guess will be, knowing that your guess also depends on what they are likely to guess. With this approach it seems necessary also to assume the number of players is small, for if n is large such strategical thinking is probably not as feasible and the computations may become much more difficult. Here we are primarily interested in large n, so we take a different viewpoint: we assume that you have probabilistic knowledge about your friends’ guesses. That is, you do not know what the guesses will be, but you know that they will all come from a particular probability distribution. We will consider several distributions in the sections ahead, but for now let us focus on the mathematics of a relatively simple case. If your friends are all computers programmed to guess a uniform random number between 0 and 1, then the optimal strategy can be found. Before we do it, though, take a moment to guess what the answer is. If you	computation;computer;optimization problem;program optimization;random number generation;video game developer	Douglas Drinen;Kathleen Grace Kennedy;William M. Priestley	2009	The American Mathematical Monthly		computational problem;optimization problem;mathematical optimization;multi-swarm optimization;constrained optimization;lagrangian relaxation;combinatorial optimization;nonlinear programming;generalized assignment problem;derivative-free optimization;cutting stock problem;mathematics;vector optimization;random optimization;3-opt;metaheuristic;global optimization;quadratic assignment problem	Theory	29.75241708876094	7.989480716614791	159211
303f8d99759e1f1fc922b0cffdd9fe3e75017b74	algorithms for the satisfiability (sat) problem: a survey	mathematical logic;computer architecture;parallel processing;computer vision;computer aided manufacturing;optimization;satisfiability;computer aided design;algorithms;experimental design;robotics;mathematical programming;computer networks	The satisfiability (SAT) problem is a core, problem in mathematical logic and computing theory. In practice, SAT is fundamental in solving many problems in automated reasoning, computer-aided design, computeraided manufacturing, machine vision, database, robotics, integrated circuit design, computer architecture design, and computer network design. Traditional methods treat SAT as a discrete, constrained decision problem. In recent years, many optimization methods, parallel algorithms, and practical techniques have been developed for solving SAT. In this survey, we present a general framework (an algorithm space) that integrates existing SAT algorithms into a unified perspective. We describe sequential and parallel SAT algorithms including variable splitting, resolution, local search, global optimization, mathematical programming, and practical SAT algorithms. We give performance evaluation of some existing SAT algorithms. Finally, we provide a set of practical applications of the satisfiability problems.	automated reasoning;boolean satisfiability problem;computer architecture;computer-aided design;database;decision problem;global optimization;integrated circuit design;local search (optimization);machine vision;mathematical optimization;network planning and design;parallel algorithm;performance evaluation;robotics;variable splitting	Jun Gu;Paul W. Purdom;John Franco;Benjamin W. Wah	1996				AI	27.049562893604676	9.499240952926755	159307
de2129fdb5792fe77230ace5f81b73a4b7202823	optimal volume subintervals with k points and star discrepancy via integer programming	criterio optimalidad;empirical study;programacion entera;geometrie algorithmique;computational geometry;programmation en nombres entiers;programacion lineal;integer programming;volume optimal;key words integer programming;linear programming;programmation lineaire;geometria computacional;optimality criterion;critere optimalite;integer program;star discrepancy	Given n points in the s-dimensional unit cube, we consider the problem of finding a subinterval of minimum or maximum volume that contains exactly k of the n points. We give integer programming formulations of these problems and techniques to tackle their resolution. These optimal volume problems are used in an algorithm to compute the star discrepancy of n points in the s-dimensional unit cube. We propose an ultimately convergent strategy that gradually reduces the size of an interval containing this value. Results of some star discrepancy experiments and an empirical study of the computation time of the method are presented.	discrepancy function;integer programming;low-discrepancy sequence	Eric Thiémard	2001	Math. Meth. of OR	10.1007/s001860100141	mathematical optimization;combinatorics;discrete mathematics;integer programming;computational geometry;linear programming;mathematics;empirical research	Theory	28.109737835083948	17.001562027192158	159586
69fd56d4ebb4a3a615e6942ee00a28c37596530a	using tree-decomposable structures to approximate belief networks	logarithm scoring rule;optimal approximation;approximate belief network;actual belief network;tree-decomposable structure;tree structure;optimal approximating tree;appropriate optimality criterion;logarithm rule;binary variable;different tree-decomposable structure;belief network	Tree structures have been shown to provide an efficient framework for propagating beliefs [Pearl,l986]. This paper studies the problem of finding an optimal approximating tree. The star­ decomposition scheme for sets of three binary variables [Lazarsfeld,l966; Pearl,1986] is shown to enhance the class of probability distributions that can support tree structures; such structures are called tree-decomposable structures. The logarithm scoring rule is found to be an appropriate optimality criterion to evaluate different tree-decomposable structures. Characteristics of such structures closest to the actual belief network are identified using the logarithm rule, and greedy and exact techniques are developed to find the optimal approximation.	approximation algorithm;bayesian network;belief propagation;greedy algorithm;optimality criterion;polynomial;qr decomposition;software propagation;tree structure	Sumit Sarkar	1993			mathematical optimization;combinatorics;discrete mathematics;computer science;machine learning;bayesian network;mathematics;statistics	AI	36.029988519444046	7.635711709526807	159888
50e81d23d4ae1ab5f850d46de516db2401f1a600	iterative self-assembly with dynamic strength transformation and temperature control	temperature control;hexagonal tiles;strength transformation;algorithmic self assembly	We propose an iterative approach to constructing regular shapes by self-assembly. Unlike previous approaches, which construct a shape in one go, our approach constructs a final shape by alternating the steps of assembling and disassembling, increasing the size of the shape iteratively. This approach is embedded into an extended hexagonal tile assembly system, with dynamic strength transformation and temperature control. We present the construction of equilateral triangles as an example and prove the uniqueness of the final shape. The tile complexity of this approach is O(1).	blueprint;embedded system;iteration;iterative method;self-assembly	Dandan Mo;Darko Stefanovic	2013		10.1007/978-3-319-01928-4_11	combinatorics;mathematics;geometry;engineering drawing	Theory	35.14740276831097	16.342903650354394	160282
1876a8bf8a4a91729def937729d6d9fa665935fc	zero-discount partially observable markov decision processes for computational improvising agents			computation;markov chain;partially observable markov decision process	Aengus Martin;Craig T. Jin;André van Schaik;William L. Martens	2010			mathematical optimization;markov decision process;partially observable markov decision process;mathematics;markov model;observable	AI	38.509997788544695	4.316879265723526	160293
96bdbfdb293e9ea99af598b6104aac4bbb60fe55	drawing a figure in a two-dimensional plane for a qualitative representation	circle packing;spatial representation;genetic algorithm;convex polygon;symbolic representation	This paper describes an algorithm for generating a figure in a two-dimensional plane from a qualitative spatial representation of PLCA. In general, it is difficult to generate a figure from qualitative spatial representations, since they contain positional relationships but do not hold quantitative information such as position and size. Therefore, an algorithm is required to determine the coordinates of the objects while preserving the positional relationships. Moreover, it is more desirable that the resulting figure meets a user’s requirement. PLCA is a simple symbolic representation consisting of points, lines, circuits and areas. We have already proposed one algorithm for drawing, but the resulting figures are far from a “good” one. In that algorithm, we generate the graph corresponding to a given PLCA expression, decompose it into connected subgraphs, determine the coordinates in a unit circle for each subgraph independently, and finally determine the position and size of each subgraph by locating the circles in appropriate positions. This paper aims at generating a “good” figure for a PLCA expression. We use a genetic algorithm to determine the locations and the sizes of circles in the last step of the algorithm. We have succeeded in producing a figure in which objects are drawn as large as possible, with complex parts larger than others. This problem is considered to be a type of “circle packing,” and the method proposed here is applicable to the other problems in which locating objects in a non-convex polygon.	genetic algorithm;requirement;set packing;software release life cycle	Shou Kumokawa;Kazuko Takahashi	2007		10.1007/978-3-540-74788-8_21	combinatorics;genetic algorithm;topology;circle packing;computer science;mathematics;geometry	AI	33.15644536048201	17.772465574148026	160725
8bc4f45782dfdcd392f0a6eff416870480af10f5	bounded truncation error for long-run averages in infinite markov chains		We consider long run averages of additive functionals on infinite discrete-state Markov chains, either continuous or discrete in time. Special cases include long run average costs or rewards, stationary moments of the components of ergodic multi-dimensional Markov chains, queueing network performance measures, and many others. By exploiting Foster-Lyapunov-type criteria involving drift conditions for the finiteness of long run averages we determine suitable finite subsets of the state space such that the truncation error is bounded. Illustrative examples demonstrate the application of this method.	ergodicity;lyapunov fractal;markov chain;network performance;queueing theory;state space;stationary process;truncation error;utility functions on indivisible goods	Hendrik Baumann;Werner Sandmann	2015	J. Applied Probability	10.1017/S0021900200113324	truncation error;markov chain;mathematical optimization;discrete mathematics;mathematics;additive markov chain;truncation error;statistics	Metrics	39.09546673636983	5.952816534268424	160829
6b7ae92a950dbf7a3607bfdcb454df6b76248ff1	identification of departure from controlled flight using neighbor class ratio cumulative scoring				Daniel Everson;Eric Silberg	2012		10.2514/6.2012-2606	statistics;computer science	Robotics	38.1577381644625	7.34770721390455	160950
61900284c648e4eb67a07bb892df0b66662920c5	challenging the evolutionary strategy for synthesis of analogue computational circuits	computational circuits;analog circuit synthesis;spice;article;evolutionary electronics	There are very few reports in the past on applications of Evolutionary Strategy (ES) towards the synthesis of analogue circuits. Moreover, even fewer reports are on the synthesis of computational circuits. Last fact is mainly due to the difficulty in designing of the complex nonlinear functions that these circuits perform. In this paper, the evolving power of the ES is challenged to design four computational circuits: cube root, cubing, square root and squaring functions. The synthesis succeeded due to the usage of oscillating length genotype strategy and the substructure reuse. The approach is characterized by its simplicity and represents one of the first attempts of application of ES towards the synthesis of “QR” circuits. The obtained experimental results significantly exceed the results published before in terms of the circuit quality, economy in components and computing resources utilized, revealing the great potential of the technique proposed to design large scale analog circuits.	analogue electronics;computation;nonlinear system;numerical integration	Yerbol Sapargaliyev;Tatiana Kalganova	2010	JSEA	10.4236/jsea.2010.311121	computer science;engineering;artificial intelligence;machine learning;algorithm	EDA	27.979929215465248	4.5158819412311635	160965
9283efea52495146c82fa8a50bad23c38cce4893	entropic-genetic clustering	genetics	This paper addresses the clustering problem given the simil ar ty matrix of a dataset. By representing this matrix as a weighted g raph we transform this problem to a graph clustering/partitioning problem wh ich aims at identifying groups of strongly inter-connected vertices. We define t wo distinct criteria with the aim of simultaneously minimizing the cut size and ob taining balanced clusters. The first criterion minimizes the similarity betw en objects belonging to different clusters and is an objective generally met in cl ustering. The second criterion is formulated with the aid of generalized entr opy. The trade-off between these two objectives is explored using a multi-obje ctiv genetic algorithm with enhanced operators. As the experimental results how, the Pareto front offers a visualization of the trade-off between the tw o objectives.	cluster analysis;genetic algorithm;i/o controller hub;pareto efficiency;partition problem	Mihaela Breaban;Henri Luchian;Dan A. Simovici	2011			mathematical optimization;genetic algorithm;operator (computer programming);vertex (geometry);matrix (mathematics);cluster analysis;clustering coefficient;multi-objective optimization;graph;mathematics	ML	25.738342932494874	5.317894186247655	161398
9365edf9d0e875df7c10c4ea5ff8af74a20335a8	theory and algorithms for linear multiple objective programs with zero-one variables	multi criteria optimization;multiple objective optimization;vector optimization;integer program;multiple objective programming	A new algorithm and theoretical results are presented for linear multiple objective programs with zero–one variables. A procedure to identify strong and weak efficient points as well as an extension of the main problem are analyzed. Extensive computational results are given and several topics for further research are discussed.	algorithm	Gabriel R. Bitran	1979	Math. Program.	10.1007/BF01588256	mathematical optimization;combinatorics;discrete mathematics;mathematics;vector optimization	Theory	24.741621502171384	11.02096767643693	161436
f19ecefcfdd8c1d1978575593374ae6cc30e3899	glider collisions in hybrid cellular automaton with memory rule (43, 74)		In the case of one-dimensional cellular automaton (CA), a hybrid CA (HCA) is the member whose evolution of the cells is dependent on nonunique global functions. The HCAs exhibit a wide range of traveling and stationary localizations in their evolution. We focus on HCA with memory (HCAM) because they produce a host of gliders and complicated glider collisions by introducing the hybrid mechanism. In particular, we undertake an exhaustive search of gliders and describe their collisions using quantitative approach in HCAM(43, 74). By introducing the symbol vector space and exploiting the mathematical definition of HCAM, we present an analytical method of complex asymptotic dynamics of the gliders.	brute-force search;cellular automaton;glider (conway's life);hierarchical clustering;stationary process	Bo Chen;Fangyue Chen;Genaro Juárez Martínez	2017	I. J. Bifurcation and Chaos	10.1142/S0218127417500821		Theory	38.533679652945004	9.304953781046466	161507
6ba984f0f27849ed29d77095a549234a447dc533	delta dlp 3-d printing of large models		"""This paper presents a 3-D printing system that uses a low-cost off-the-shelf consumer projector to fabricate large models. Compared with traditional digital light processing (DLP) 3-D printers using a single vertical carriage, the platform of our DLP 3-D printer using delta mechanism can also move horizontally in the plane. We show that this system can print 3-D models much larger than traditional DLP 3-D printers. The major challenge to realize 3-D printing of large models in our system comes from how to cover a planar polygonal domain by a minimum number of rectangles with fixed size, which is NP-hard. We propose a simple yet efficient approximation algorithm to solve this problem. The key idea is to segment a polygonal domain using its medial axis and afterward merge small parts in the segmentation. Given an arbitrary polygon <inline-formula> <tex-math notation=""""LaTeX"""">$\Omega $ </tex-math></inline-formula> with <inline-formula> <tex-math notation=""""LaTeX"""">$n$ </tex-math></inline-formula> generators (i.e., line segments and reflex vertices in <inline-formula> <tex-math notation=""""LaTeX"""">$\Omega $ </tex-math></inline-formula>), we show that the time complexity of our algorithm is <inline-formula> <tex-math notation=""""LaTeX"""">$O(n^{2}\log ^{2}~n)$ </tex-math></inline-formula> and the number of output rectangles covering <inline-formula> <tex-math notation=""""LaTeX"""">$\Omega $ </tex-math></inline-formula> is <inline-formula> <tex-math notation=""""LaTeX"""">$O(Kn)$ </tex-math></inline-formula>, where <inline-formula> <tex-math notation=""""LaTeX"""">$K$ </tex-math></inline-formula> is an input-polygon-dependent constant. A physical prototype system is built and several large 3-D models with complex geometric structures have been printed as examples to demonstrate the effectiveness of our approach. <italic>Note to Practitioners</italic>—Low-cost 3-D printers and 3-D printing of large models are two important but often conflicting goals in manufacturing industry. Usually low-cost devices such as DLP 3-D printer using an off-the-shelf consumer projector cannot print large models, due to the small projection area of the projector. In this paper, we propose to horizontally move the platform such that a large area can be printed by the composition of multiple small projection areas. Based on this new mechanism, we propose a simple yet efficient algorithm to cover an arbitrary polygonal shape (possibly with holes or multiple disjoint polygons) by a small set of rectangles with fixed size. Our algorithm is theoretically sound and can be easily implemented. We built a physical prototype system of the proposed low-cost Delta DLP 3-D printer, which successfully prints several large models with satisfied mechanical properties."""	3d printing;apache axis;approximation algorithm;delta robot;digital light processing;maximal set;medial graph;np-hardness;printer (computing);prototype;scalability;time complexity;video projector	Ran Yi;Chenming Wu;Yong-Jin Liu;Ying He;Charlie C. L. Wang	2018	IEEE Transactions on Automation Science and Engineering	10.1109/TASE.2017.2751664	computer science;mathematical optimization;time complexity;medial axis;polygon;vertex (geometry);digital light processing;approximation algorithm;disjoint sets;solid modeling	Graphics	34.01620367915003	17.72850985858838	161571
4262032a2f7d4d2354b14d05c4d177f923190400	depth properties of scaled attachment random recursive trees	power of choice;second moment method;height;random trees;renewal process	We study depth properties of a general class of random recursive trees where each node i attaches to the random node biXic and X0, . . . , Xn is a sequence of i.i.d. random variables taking values in [0, 1). We call such trees scaled attachment random recursive trees (sarrt). We prove that the typical depth Dn, the maximum depth (or height) Hn and the minimum depth Mn of a sarrt are asymptotically given by Dn ∼ μ−1 logn, Hn ∼ αmax logn andMn ∼ αmin logn where μ, αmax and αmin are constants depending only on the distribution of X0 whenever X0 has a density. In particular, this gives a new elementary proof for the height of uniform random recursive trees Hn ∼ e logn that does not use branching random walks.	attachments;random graph;recursion	Luc Devroye;Omar Fawzi;Nicolas Fraiman	2012	Random Struct. Algorithms	10.1002/rsa.20391	renewal theory;combinatorics;height;discrete mathematics;mathematics;second moment method;algorithm;statistics	Theory	38.918619126356575	16.61998532937171	161687
42c99e59e177f7c19f3367e284a17ac2235e2c0a	a symbolic dynamics perspective of elementary cellular automaton rule 12 with minority memory	期刊论文		elementary cellular automaton	Bo Chen;Fangyue Chen;Weifeng Jin;Junbiao Guan	2015	J. Cellular Automata		stochastic cellular automaton;discrete mathematics;computer science;theoretical computer science;deterministic automaton;mathematics;algorithm	ECom	38.13358650649971	9.43997147967983	161880
3440c2c6f854b39ed5906b50874d642795f0fc5e	orbits of the bernoulli measure in single-transition asynchronous cellular automata	info info dm computer science cs discrete mathematics cs dm;cellular automata asynchronous rules;math math ds mathematics math dynamical systems math ds;math math co mathematics math combinatorics math co;measure dynamics;nlin nlin cg nonlinear sciences physics cellular automata and lattice gases nlin cg	HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d’enseignement et de recherche français ou étrangers, des laboratoires publics ou privés. Orbits of the Bernoulli measure in single-transition asynchronous cellular automata Henryk Fukś, Andrew Skelton	approximation algorithm;automata theory;benchmark (computing);bernoulli scheme;cellular automaton;computer experiment;cylinder seal;euler–bernoulli beam theory;iteration;production (computer science);recursion (computer science);simulation	Henryk Fuks;Andrew Skelton	2011			combinatorics;discrete mathematics;pure mathematics;mathematics;algorithm;algebra	Logic	38.05329933891277	9.463831271047065	162010
fa00130006be374828bacdc769c17d6ddf142ffc	high-quality ultra-compact grid layout of grouped networks	biological patents;power graph;journal_article;biomedical journals;standards;graph drawing;text mining;europe pubmed central;routing;network visualization;citation search;layout;citation networks;research articles;abstracts;pipelines;open access;layout encoding optimization containers standards routing pipelines;life sciences;large neighborhood search;clinical guidelines;optimization;search problems complex networks computability graph theory integer programming large scale systems;full text;large neighborhood search network visualization graph drawing power graph optimization;encoding;rest apis;orcids;europe pmc;biomedical research;containers;large neighborhood search meta heuristic approach high quality ultra compact grid layout grouped networks fast heuristic techniques complex multistage pipelines small graphs pipeline techniques orthogonal style layout small networks grouping constraints ultra compact grid like network layout aesthetic grid arrangements typographical layout heuristic based graph layout methods pipeline based graph layout methods mip cp sat combinatorial problems mixed integer optimization problems;bioinformatics;literature search	Prior research into network layout has focused on fast heuristic techniques for layout of large networks, or complex multi-stage pipelines for higher quality layout of small graphs. Improvements to these pipeline techniques, especially for orthogonal-style layout, are difficult and practical results have been slight in recent years. Yet, as discussed in this paper, there remain significant issues in the quality of the layouts produced by these techniques, even for quite small networks. This is especially true when layout with additional grouping constraints is required. The first contribution of this paper is to investigate an ultra-compact, grid-like network layout aesthetic that is motivated by the grid arrangements that are used almost universally by designers in typographical layout. Since the time when these heuristic and pipeline-based graph-layout methods were conceived, generic technologies (MIP, CP and SAT) for solving combinatorial and mixed-integer optimization problems have improved massively. The second contribution of this paper is to reassess whether these techniques can be used for high-quality layout of small graphs. While they are fast enough for graphs of up to 50 nodes we found these methods do not scale up. Our third contribution is a large-neighborhood search meta-heuristic approach that is scalable to larger networks.	approximation algorithm;baseline (configuration management);cerebral palsy;decision problem;declarative programming;diagram;drawings (art);experiment;generic drugs;graph (discrete mathematics);graph - visual representation;graph drawing;heuristic (computer science);integer (number);large;layout manager;lesch-nyhan syndrome;logarithmic number system;mathematical optimization;metaheuristic;node - plant part;numerous;online and offline;optimization problem;pipeline (computing);routing;scalability;research study	Vahan Yoghourdjian;Tim Dwyer;Graeme Gange;Steve Kieffer;Karsten Klein;Kim Marriott	2016	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2015.2467251	text mining;computer science;theoretical computer science;data mining;graph drawing	Visualization	25.165186788885784	6.299588840416599	162246
8ec6338cb3fd387357f2fb9df52bbe8e88261642	strong duality for the maximum borel flow problem	continuous model;dual problem;borel measure;independent approach;strong duality result;continuous representation;strong duality;maxflow-mincut theorem;maximum borel flow problem;borel flow	Research on flows over time has been conducted mainly in two separate and independent approaches, namely discrete and continuous models, depending on whether a discrete or continuous representation of time is used. Recently, Borel flows have been introduced to build a bridge between these two models. In this paper, we consider the maximum Borel flow problem formulated in a network where capacities on arcs are given as Borel measures and storage might be allowed at the nodes of the network. This problem is formulated as a linear program in a space of measures. We define a dual problem and prove a strong duality result.We show that strong duality is closely related to a MaxFlow-MinCut Theorem.	strong duality	Ronald Koch;Ebrahim Nasrabadi	2011		10.1007/978-3-642-21527-8_30	riesz–markov–kakutani representation theorem;borel set;combinatorics;mathematical analysis;discrete mathematics;duality;borel measure;weak duality;borel hierarchy;mathematics	Logic	25.732449934098007	13.48171343200692	163563
7ce9646676c65cb8f40171f3ac32e6e68014599c	a recipe for finding good solutions to minlps		Finding good (or even just feasible) solutions for Mixed-Integer Nonlinear Programming problems independently of the specific problem structure is a very hard but practically important task, especially when the objective and/or the constraints are nonconvex. With this goal in mind, we present a general-purpose heuristic based on Variable Neighborhood Search, Local Branching, a local Nonlinear Programming algorithm and Branch-and-Bound. We test the proposed approach on MINLPLib, comparing with several existing heuristic and exact methods. An implementation of the proposed heuristic is freely available and can employ all NLP/MINLP solvers with an AMPL interface as the main search tools.	ampl;algorithm;branch and bound;claudia neuhauser;computation;experiment;general-purpose modeling;heuristic;natural language processing;nonlinear programming;solver;variable neighborhood search	Leo Liberti;Nenad Mladenovic;Giacomo Nannicini	2011	Math. Program. Comput.	10.1007/s12532-011-0031-y	mathematical optimization;combinatorics;mathematics;algorithm	AI	25.962432334364575	7.46147323093643	163585
84300f47ce7ca53911d0357cf9c226f5db88bc39	markov strategies in dynamic programming	dynamic programming;dynamic program;markov strategies	It is shown that the supremum of the expected total return over the Markov strategies equals the supremum over all strategies. The model assumptions are: the state space is countable, the action space is measurable and the supremum of the expected total of the positive rewards over the Markov strategies is finite.	dynamic programming;markov chain	Kees M. van Hee	1978	Math. Oper. Res.	10.1287/moor.3.1.37	markov decision process;markov chain;mathematical optimization;markov kernel;combinatorics;discrete mathematics;markov property;dynamic programming;mathematics;markov algorithm;markov process;markov model;variable-order markov model	Theory	37.9077621494075	5.245800222066905	164035
eb93089518bbf6ffbe748201257cdaa0232f5dd0	resolving the pursuit evasion problem in known environment using graph theory	graph theory;robotics;pursuit evasion;bio inspired computation	In this article, we study a form of pursuit-evasion problem, in which one or more pursuers must find a strategy of moves in a given closed environment (such as building) to guarantee the detection of an unpredictable evader. The main idea is to use a graph of visibility between vertices and some techniques to bring this graph into a graph of building areas. Because of the increasing interest on the pursuit-evasion applications, such as: searching buildings for intruders, traffic control, military strategy and surgical operation, a lot of research (Basar and Olsder, 1999; Guibas et al., 1999; Hespanha et al., 1999, 2000; Suzuki and Yamashita, 1992) has been done on this problem. The main interest of this work is the simplification and test of the visibility graph. A technique to solve the pursuit-evasion problem has been proposed, this technique is the worst-case search. Finally, the used tool in the experimental work has been presented.	evasion (network security);graph theory;pursuit-evasion	Rabah Lakel;Adel Djellal	2010	IJBIC	10.1504/IJBIC.2010.037023	mathematical optimization;combinatorics;simulation;computer science;artificial intelligence;graph theory;machine learning;mathematics;robotics	Theory	31.632111497160075	13.880914605435368	164275
aca9cd9671514f1c01afe2458a66336f6d76721a	towards a taxonomy of techniques for designing parameterized algorithms	iterative method;formal specification;methode noyau;localization;branching;systematique;localizacion;classification;methode calcul;specification formelle;metodo iterativo;metodo calculo;especificacion formal;induccion;localisation;sistematica;induction;methode iterative;parameterized algorithm;ramificacion;metodo nucleo;taxonomy;ramification;kernel method;clasificacion;computing method	A survey is given of the main techniques in parameterized algorithm design, with a focus on formal descriptions of the less familiar techniques. A taxonomy of techniques is proposed, under the four main headings of Branching, Kernelization, Induction and Win/Win. In this classification the Extremal Method is viewed as the natural maximization counterpart of Iterative Compression, under the heading of Induction. The formal description given of Greedy Localization generalizes the application of this technique to a larger class of problems.	algorithm	Christian Sloper;Jan Arne Telle	2006		10.1007/11847250_23	kernel method;combinatorics;internationalization and localization;branching;biological classification;computer science;operating system;machine learning;calculus;formal specification;mathematics;iterative method;ramification;algorithm;taxonomy	Theory	32.078257717032805	10.158178552048547	164302
149ac14b33d283d1068cbffb1ce97d519ba1f6fa	accurate computation of the medial axis of a polyhedron	medial axis of a polyhedron;plane curve;voronoi diagram of a polyhedron;efficient algorithm;lazy evaluation;voronoi graph;linear program;floating point;medial axis;geometric structure;voronoi diagram	We present an accurate and e cient algorithm to compute the internal Voronoi region and medial axis of a 3-D polyhedron. It uses exact arithmetic and representations for accurate computation of the medial axis. The sheets, seams, and junctions of the medial axis are represented as trimmed quadric surfaces, algebraic space curves, and algebraic numbers, respectively. The algorithm works by recursively nding neighboring junctions along the axis. It utilizes discretization of space and linear programming to speed up the search step. We also present a new algorithm for analysis of the topology of an algebraic plane curve, which is the core of our medial axis algorithm. To speed up the computation, we have designed specialized algorithms for fast computation on implicit geometric structures. These include lazy evaluation based on multivariate St urm sequences, fast resultant computation, curve topology analysis, and oating-point lters. The algorithm has been implemented and we highlight its performance on a number of examples.	algebraic equation;algorithm;apache axis;computation;discretization;lazy evaluation;linear programming;medial graph;optic axis of a crystal;polyhedron;recursion;resultant;voronoi diagram	Tim Culver;John Keyser;Dinesh Manocha	1999		10.1145/304012.304030	plane curve;combinatorics;weighted voronoi diagram;power diagram;topology;voronoi diagram;medial axis;centroidal voronoi tessellation;floating point;linear programming;lazy evaluation;mathematics;geometry	Robotics	34.19041113611089	16.495979179852746	164389
3c101d115ae16afae4e5ea7a7c7f16af5af4b207	on an optimum principle for partially observed controlled jump markovian processes	partially observed controlled jump markovian processes;cost function;optimum principle;local dynamic programming condition;transition intensities;a posteriori probabilities of the states	Controlled jump processes have been intensively investigated by many authors, e.g. S. R. Pliska [7], R. Boel, P. Varaiya [ l ] , M. H. A. Davis, R. Elliott [2], C. B. Wan, M. H. A. Davis [9], and R. W. Rishel [8]. Pliska considered completely observed controlled jump Markovian processes. Boel and Varaiya investigated general jump process, gave a local optimality condition for value increasing controls and derived a differential equation to define the value function in the case of complete information. The paper by Davis and Elliott dealt with optimality condition for completely observed controlled jump processes the probability distributions of which is defined by the so-called Levy system (A, A) depending on control u and showed that a control is optimal iff it maximizes some Hamiltonian. Wan and Davis derived conditions for existence of an optimal control for the case of complete information. The paper by Rishel dealt with the minimum principle and dynamic programming conditions for two dimensional jump Markovian processes when only one component is observable. This paper concerns partially observable controlled jump Markovian processes in which at each time moment some function of the state is observable. A representation of the value function and a local dynamic programming condition involving transition intensities and aposteriori probabilities of the states of the process are obtained.		Van Huu Nguyen	1981	Kybernetika		mathematical optimization;control theory;mathematics;statistics;loss function	Logic	37.6831617373241	4.24258272930955	164514
a06608bf337da8b8866774f335342e40d84a1f65	polygonal path approximation: a query based approach	euclidean theory;approximate algorithm;geometrie algorithmique;approximation algorithm;heuristic method;computational geometry;polygone;metric;metodo heuristico;euclidean distance;greedy heuristic;polygon;exact algorithm;algoritmo aproximacion;theorie euclidienne;time factor;poligono;metrico;geometria computacional;methode heuristique;algorithme approximation;metrique;teoria euclidiana	In this paper we present a new, query based approach for approximating polygonal chains in the plane. We give a few results related with this approach, some of more general interest, and propose a greedy heuristic to speed up the computation. We also give an O(n log n) time, factor 2 approximation algorithm with infinite beam criterion. Finally, we show that the query based approach can be used to obtain a subquadratic time exact algorithm with infinite beam criterion and Euclidean distance metric if some condition on the input path holds.	approximation	Ovidiu Daescu;Ningfang Mi	2003		10.1007/978-3-540-24587-2_6	mathematical optimization;combinatorics;computational geometry;polygon;mathematics;geometry;approximation algorithm	Robotics	29.484125972744774	17.561023004720038	164694
bc5a264d0c23fdf5d2635e49bb9b1fb18acf80ff	deterministic symmetry breaking in ring networks	mobile robots;location discovery;bouncing	We study a distributed coordination mechanism for uniform agents located on a circle. The agents perform their actions in synchronised rounds. At the beginning of each round an agent chooses the direction of its movement from clockwise, anticlockwise, or idle, and moves at unit speed during this round. Agents are not allowed to overpass, i.e., When an agent collides with another it instantly starts moving with the same speed in the opposite direction (without exchanging any information with the other agent). However, at the end of each round each agent has access to limited information regarding its trajectory of movement during this round. We assume that n mobile agents are initially located on a circle unit circumference at arbitrary but distinct positions unknown to other agents. The agents are equipped with unique identifiers from a fixed range. The location discovery task to be performed by each agent is to determine the initial position of every other agent. Our main result states that, if the only available information about movement in a round is limited to distance between the initial and the final position, then there is a superlinear lower bound on time needed to solve the location discovery problem. Interestingly, this result corresponds to a combinatorial symmetry breaking problem, which might be of independent interest. If, on the other hand, an agent has access to the distance to its first collision with another agent in a round, we design an asymptotically efficient and close to optimal solution for the location discovery problem.	mobile agent;symmetry breaking;unique identifier	Leszek Gasieniec;Tomasz Jurdzinski;Russell Martin;Grzegorz Stachowiak	2015	2015 IEEE 35th International Conference on Distributed Computing Systems	10.1109/ICDCS.2015.59	mobile robot;simulation;computer science;artificial intelligence;distributed computing	Theory	32.474463796280524	12.638881284165386	164739
4123dbdf7ec290ccdff9edd4c3d3bb3cfae31bac	fast winner-takes-all networks for the maximum clique problem	graph theory;grafo maximo;teoria grafo;maximum clique problem;combinatorial optimization problem;graph clique;theorie graphe;mathematical analysis;graphe maximal;optimisation combinatoire;clique graphe;reseau neuronal;combinatorial optimization;winner take all;maximal graph;red neuronal;neural network;optimizacion combinatoria	We present an in-depth mathematical analysis of a winner-takes-all Network tailoredto the maximum clique problem, a well-known intractable combinatorial optimization problem which has practical applications in several real world domains. The analysis yields tight bounds for the parameter settings to ensure energy descent to feasible solutions. To verify the theoretical results we employ a fast annealing schedule to the WTA algorithm and show the effectiveness of the proposed approach for large scaled problems in extensive computer simulations.	clique problem	Brijnesh J. Jain;Fritz Wysotzki	2002		10.1007/3-540-45751-8_11	winner-take-all;clique;mathematical optimization;combinatorics;discrete mathematics;combinatorial optimization;graph theory;machine learning;mathematics	Theory	24.831845978311318	16.33957060083063	164780
a798e751ac3b6db139d22ba225abd8e5e611c86f	solving stochastic structural optimization problems by rsm-based stochastic approximation methods - gradient estimation in case of intermediate variables	reliability based structural optimization;response surface method;optimisation;reliability;response surface methodology;structure optimization;analisis estructural;numerical method;design criteria;programmation stochastique;probabilistic approach;surface reponse;stochastic optimization;metodo numerico;mathematical programming;enfoque probabilista;approche probabiliste;fiabilite;probability distribution;stochastic approximation;rupture;random variable;superficie respuesta;gradient estimate;ruptures;optimization;analyse structurale;stochastic programming;structural analysis;response surface;programmation mathematique;methode numerique;intermediate variables	""""""" Reliability-based structural optimization methods use mostly the following basic design criteria: I) Minimum weight (volume or costs) and II) high strength of the structure. Since several parameters of the structure, e.g. material parameters, loads, manufacturing errors, are not given, fixed quantities, but random variables having a certain probability distribution P, stochastic optimization problems result from criteria (I), (II), which can be represented by minF(x) with F(x) := Ef(co, x) . (1) x E D Here, f =f (e) , x) is a function on ~ r depending on a random element co, """"E"""" denotes the expectation operator and D is a given closed, convex subset of ~T. Stochastic approximation methods are considered for solving (1), where gradient estimators are obtained by means of the response surface methodology (RSM). Moreover, improvements of the RSM-gradient estimator by using """"intermediate"""" or """"intervening"""" variables are examined."""	approximation algorithm;convex set;emoticon;gradient;mathematical optimization;minimum weight;response surface methodology;shape optimization;stochastic approximation;stochastic optimization	Kurt Marti	1997	Math. Meth. of OR	10.1007/BF01194863	stochastic approximation;econometrics;mathematical optimization;response surface methodology;stochastic optimization;mathematics;statistics	ML	36.691331217881306	6.635094427140746	165025
c8392725dc56504b0b01f5ad2a270473a253cff9	a dual-based algorithm for solving lexicographic multiple objective programs	multiobjective programming;programmation multiobjectif;duality;fonction objectif;objective function;dualite;lexicografia;programacion lineal;lexicography;linear programming;lexicographie;programmation lineaire;linear program;funcion objetivo;dualidad;molp;lexicographic;multiple objective programming;programacion multiobjetivo	In this paper, we propose an algorithm for solving lexicographic multiple objective programs based upon duality theorem. In the existing algorithm, we should solve several linear programming problems (LPPs); therefore if, in particular, there are several objective functions, this method is not worthwhile from the viewpoint of computation. But in our new algorithm we just solve one LPP. 2006 Elsevier B.V. All rights reserved.	computation;computer memory;duality (optimization);first draft of a report on the edvac;lexicographical order;linear programming;simplex algorithm	Latif Pourkarimi;M. Zarepisheh	2007	European Journal of Operational Research	10.1016/j.ejor.2005.10.046	mathematical optimization;combinatorics;duality;linear programming;lexicographical order;lexicography;mathematics;algorithm	AI	25.21744877123232	10.271177133649838	165298
8edf50b66becf4de6af65f4e5a7d9873360886dd	computation based on signal random fluctuation in asynchronous cellular automata	brownian circuits;universal computation;cellular automaton;brownian cellular automaton	A Brownian cellular automaton (BCA) is an asynchronous cellular automaton (ACA) in which the local configurations representing signals may fluctuate randomly in the cell space. The random fluctuation of signals enables effective stochastic search to conduct computation, which can actually result in the decreased complexity of BCAs. This paper proposes a novel BCA based on a conventional ACA, which employs a smaller number of rules as compared to the previous model. This BCA is capable of universal computing which might demonstrate the crucial role of signal random fluctuation for reducing the complexity of universal ACAs.	american cryptogram association;asynchronous cellular automaton;automata theory;computation;elementary cellular automaton;quantum fluctuation;randomness;stochastic optimization;universal turing machine	Wen-Li Xu;Jia Lee	2016	2016 Fourth International Symposium on Computing and Networking (CANDAR)	10.1109/CANDAR.2016.0051	stochastic cellular automaton;reversible cellular automaton;block cellular automaton;combinatorics;discrete mathematics;elementary cellular automaton;quantum cellular automaton;theoretical computer science;asynchronous cellular automaton;continuous automaton;deterministic automaton;mathematics;mobile automaton;timed automaton	Arch	38.295778901849765	9.533253023102949	165351
96c36e1cd51ff94e65ad221ea19695b68cade0d5	large-scale integer linear programming for orientation preserving 3d shape matching	i 3 5 computer graphics;i 3 5 computer graphics computational geometry and object modeling;computational geometry and object modeling	We study an algorithmic framework for computing an elastic orientation-preserving matching of non-rigid 3D shapes. We outline an Integer Linear Programming formulation whose relaxed version can be minimized globally in polynomial time. Because of the high number of optimization variables, the key algorithmic challenge lies in efficiently solving the linear program. We present a performance analysis of several Linear Programming algorithms on our problem. Furthermore, we introduce a multiresolution strategy which allows the matching of higher resolution models.	algorithm;experiment;integer programming;lagrangian relaxation;linear programming formulation;matching (graph theory);mathematical optimization;multiresolution analysis;polynomial;time complexity	Thomas Windheuser;Ulrich Schlickewei;Frank R. Schmidt;Daniel Cremers	2011	Comput. Graph. Forum	10.1111/j.1467-8659.2011.02021.x	mathematical optimization;combinatorics;computer science;theoretical computer science	AI	28.91164345809546	10.786581735112982	165725
0310434bad987ccb1106691a110327b780484a1d	predicting {0,1}-functions on randomly drawn points (extended abstract)	randomly drawn points;learnability model;many valued logics;probability;geometric range queries;learning systems;prediction strategies;axis parallel rectangles;many valued logics learning systems;axis parallel rectangles randomly drawn points learnability model target functions prediction strategies 1 inclusion graph structure geometric range queries probability indicator functions;1 inclusion graph structure;indicator functions;target functions;predictive models feedback computational complexity probability distribution approximation algorithms	"""We consider the problem of predicting (0 , l ) valued functions on R"""" and smaller domains, based on their values on randomly drawn points. Our model is related to Valiant's learnability model, but does not require the hypotheses used for prediction to be represented in any specified form. First we disregard computational complexity and show how to construct prediction strategies that are optimal to within a constant factor for any reasonable class F of target functions. These prediction strategies use the 1-inclusion graph structure bom Non. Haussler and Welzl's work on geometric range queries to minimize the probability of incorrect prediction. We then turn to computationally efficient algorithms. For indicator functions of axis-parallel rectangles and halfspaces in R"""" , we demonstrate how our techniques can be applied to construct computationally efficient prediction stxategies that are optimal to within a constant factor. Finally. we compare the general performance of prediction strategies derived by OUT method to those derived"""	algorithm;algorithmic efficiency;apache axis;computational complexity theory;emo welzl;learnability;randomness;range query (data structures)	David Haussler;Nick Littlestone;Manfred K. Warmuth	1988		10.1109/SFCS.1988.21928	combinatorics;discrete mathematics;machine learning;probability;mathematics;algorithm;statistics	Theory	34.58886484152815	13.172448461384409	166115
689c8d452be8f6698d4b5b7e7e3e61bbf3a35413	minimum-length polygons of first-class simple cube-curves	analisis imagen;image processing;rubber;generic algorithm;cube;analisis forma;cubo;polygone;procesamiento imagen;traitement image;polygon;grid;red multinivel;rejilla;grille;caoutchouc;poligono;image analysis;pattern analysis;technical report;multilayer network;caucho;reseau multicouche;reseau neuronal;analyse image;red neuronal;analyse forme;neural network	We consider simple cube-curves in the orthogonal 3D grid. The union of all cells contained in such a curve (also called the tube of this curve) is a polyhedrally bounded set. The curve’s length is defined to be that of the minimum-length polygonal curve (MLP) fully contained and complete in the tube of the curve. So far only one general algorithm called rubber-band algorithm was known for the approximative calculation of such an MLP. A proof that this algorithm always converges to the correct curve, is still an open problem. This paper proves that the rubber-band algorithm is correct for the family of first-class simple cube-curves.	algorithm;approximation algorithm;canny edge detector;computation;computational complexity theory;computational geometry;cube;euclidean shortest path;interactivity;ipke wachsmuth;jonas;john reif;lecture notes in computer science;louis rosenfeld;memory-level parallelism;motion planning;planar (computer graphics);quad flat no-leads package;shortest path problem;springer (tank);vertex (graph theory)	Fajie Li;Reinhard Klette	2005		10.1007/11556121_40	natural rubber;computer vision;vertical line test;image analysis;ramer–douglas–peucker algorithm;jacobian curve;image processing;tripling-oriented doche–icart–kohel curve;computer science;calculus;polygon;mathematics;geometry;parallel curve;artificial neural network;algorithm;moore curve;stable curve;curve orientation;curve fitting	Theory	32.73440951223862	17.562657301305208	166380
e91e36054d67cb6c47d7e10c09edc9cfe53009f6	on the refinement of bounds of heuristic algorithms for the traveling salesman problem	heuristics;approximation.;traveling salesman problem;heuristic algorithm;traveling salesman;assignment problem;satisfiability;triangle inequality	A number of heuristics for the traveling salesman problem (TSP) rely on the assumption that the triangle inequality (TI) is satisfied. When TI does not hold, the paper proposes a transformation such that for the transformed problem the TI holds. Consequently, the bounds obtained for heuristics are valid with appropriate modification. Moreover, for a TSP satisfying TI the same transformation strengthens such bounds. The transformation essentially maps the problem into one that is minimal with respect to the property that TI holds. For the symmetric TSP the transformation is particularly simple. For an application of the transformation in the asymmetric case we need the dual solution of an assignment problem.	algorithm;assignment problem;heuristic (computer science);map;refinement (computing);social inequality;travelling salesman problem	Bernd Jeromin;Frank Körner	1985	Math. Program.	10.1007/BF01585662		Theory	25.67230711495732	15.451147308406824	166408
a1cd242f70505fa3a848f55dc2176dd523d50a2a	computing best transition pathways in high-dimensional dynamical systems: application to the alphal \leftrightharpoons beta \leftrightharpoons alphar transitions in octaalanine	high dimensionality;lower and upper bound;aqueous solution;performance;transition network;aqueous solutions;dynamic system;pathway;alanines;sampling;numerical integration;polyalanine;peptide;benchmarks;algorithms;weighted graph;basic biological sciences;monte carlo;70;protein	The direct computation of rare transitions in high-dimensional dynamical systems such as biomolecules via numerical integration or Monte Carlo is limited by the sampling problem. Alternatively, the dynamics of these systems can be modeled by transition networks (TNs) which are weighted graphs whose edges represent transitions between stable states of the system. The computation of the globally best transition paths connecting two selected stable states is straightforward with available graph-theoretical methods. However, these methods require that the energy barriers of all TN edges be determined, which is often computationally infeasible for large systems. Here, we introduce energy-bounded TNs, in which the transition barriers are specified in terms of lower and upper bounds. We present algorithms permitting the determination of the globally best paths on these TNs while requiring the computation of only a small subset of the true transition barriers. Several variants of the algorithm are given which ach...	dynamical system	Frank Noé;Marcus Oswald;Gerhard Reinelt;Stefan Fischer;Jeremy C. Smith	2006	Multiscale Modeling & Simulation	10.1137/050641922	aqueous solution;mathematical optimization;combinatorics;theoretical computer science;mathematics;statistics	ML	33.2480031146508	11.521235751340893	166475
f64b7c3216d77f44463de29a5b9f332e029806e9	new insights into witsenhausen's counterexample	qa75 electronic computers computer science	The accuracies of certain suboptimal solutions to the famous and still unsolved optimization problem known as “Witsenhausen’s counterexample” are investigated. The differences between the corresponding suboptimal values of the Witsenhausen functional and its optimum are estimated, too. The results give insights into the effectiveness of certain approaches proposed in the literature to face this hard optimization problem and into numerical results obtained by some researchers.	functional testing;mathematical optimization;numerical analysis;optimization problem;witsenhausen's counterexample	Giorgio Gnecco;Marcello Sanguineti	2012	Optimization Letters	10.1007/s11590-011-0339-6	mathematical optimization;witsenhausen's counterexample;computer science;mathematics;algorithm	PL	29.87865079352351	7.589797197903589	166588
29c667599b116af7fe501f45bc8b2eacff4dde16	complexity indices for the traveling salesman problem based on short edge subgraphs		We present the extension of our previous work on complexity indices for the traveling salesman problem (TSP). Since we study the symmetric traveling salesman problem, the instances are represented by complete graphs G with distances between cities as the edge weights. A complexity index is an invariant of an instance I by which the execution time of an exact TSP algorithm for I can be predicted. We consider some subgraphs of G consisting of short edges and define several new invariants related to their connected components. For computational experiments we have used the well-known TSP Solver Concorde. Experiments with instances on 50 vertices with the uniform distribution of integer edge weights in the interval [1, 100] show that there exists a notable correlation between the sequences of selected invariants and the sequence of execution times of the TSP Solver Concorde. We provide logical explanations of these phenomena.	complexity index;travelling salesman problem	Dragos Cvetkovic;Mirjana Cangalovic;Zorica Drazic;Vera Kovacevic-Vujcic	2018	CEJOR	10.1007/s10100-017-0513-8	mathematical optimization;economics;vertex (geometry);complexity index;invariant (mathematics);travelling salesman problem;connected component;integer;uniform distribution (continuous);solver	Theory	25.992862978802076	16.429541376796475	167019
309d008f05bae03fb96cccf652a52b42526fc2b4	on time-optimal trajectories in non-uniform mediums	dynamic programming;49k30 optimal solutions belonging to restricted classes;algebraic elimination theory;49 xx calculus of variations and optimal control;time optimal trajectory;90c39 dynamic programming;optimization	This paper addresses the problem of finding time-optimal trajectories in two-dimensional space, where the mover's speed monotonically decreases or increases in one of the space's coordinates. We address such problems in different settings for the velocity function and in the presence of obstacles. First, we consider the problem without any obstacles. We show that the problem with linear speed decrease is reducible to the de l'Hopital's problem, and that, for this case, the time-optimal trajectory is a circular segment. Next, we show that the problem with linear speed decreases and rectilinear obstacles can be solved in polynomial time by a dynamic program. Finally, we consider the case without obstacles, where the medium is non-uniform and the mover's velocity is a piecewise linear function. We reduce this problem to that of solving a system of polynomial equations of fixed degree, for which algebraic elimination theory allows us to solve the problem to optimality.		André Berger;Alexander Grigoriev;Ralf Peeters;Natalya Usotskaya	2015	J. Optimization Theory and Applications	10.1007/s10957-014-0590-y	mathematical optimization;combinatorics;cutting stock problem;dynamic programming;mathematics;geometry	Theory	31.08607919400641	17.213067487179586	167314
7d9852f66e4d2d4ffd796c57ce6869246800d77f	some continuous functions related to corner polyhedra	cutting plane;continuous variable;valid inequalities;mixed integer program;integer program	0 .1° Inequalities based on the integer nature of some or all of the variables are useful in almost any algorithm for integer programming. They can furnish cut offs for branch and bound or truncated enumeration methods, or cutting planes for cutting plane methods. In this paper we describe methods for producing such inequalities and develop some underlying theory. We will a t tempt to outline our general approach, taking the pure integer case first and then the general mixed integer problem. Consider a pure integer problem		Ralph E. Gomory;Ellis L. Johnson	1972	Math. Program.	10.1007/BF01584976	mathematical optimization;combinatorics;discrete mathematics;integer points in convex polyhedra;mathematics;cutting-plane method	Theory	25.087751855897718	11.85857474588283	167885
1ab1d3398c467cd0fd8b779d9e1ff4ed64daeb0c	large deviations for the weighted height of an extended class of trees	modelizacion;approximation asymptotique;arbre recherche binaire;random tree;recherche aleatoire;random recursive tree;lopsided trees;grande deviation;probabilistic approach;optimization problem;modelisation;law of large numbers;multi dimensional;branching process;search trees;gran desviacion;arbol investigacion binaria;binary search tree;probabilistic analysis;enfoque probabilista;approche probabiliste;independent random variables;large deviations;random variable;random binary search trees;investigacion aleatoria;plane oriented trees;asymptotic approximation;random recursive trees;modeling;large deviation;random search;large classes;branching random walk;aproximacion asintotica	We use large deviations to prove a general theorem on the asymptotic edge-weighted height Hn* of a large class of random trees for which Hn* ∼ c log n for some positive constant c. A graphical interpretation is also given for the limit constant c. This unifies what was already known for binary search trees, random recursive trees and plane oriented trees for instance. New applications include the heights of some random lopsided trees and of the intersection of random trees.	graphical user interface;random graph;recursion	Nicolas Broutin;Luc Devroye	2006	Algorithmica	10.1007/s00453-006-0112-x	random binary tree;random variable;optimization problem;branching process;combinatorics;discrete mathematics;large deviations theory;probabilistic analysis of algorithms;binary search tree;random search;systems modeling;law of large numbers;geometry of binary search trees;mathematics;weight-balanced tree;statistics	Theory	38.0927310877471	17.886521216020693	167991
ae049b3f76488031a7f156e86eabe44732edd537	analysis of ground state in random bipartite matching	stability;information	In human society, a lot of social phenomena can be concluded into a mathematical problem called the bipartite matching, one of the most well known model is the marriage problem proposed by Gale and Shapley. In this article, we try to find out some intrinsic properties of the ground state of this model and thus gain more insights and ideas about the matching problem. We apply Kuhn-Munkres Algorithm to find out the numerical ground state solution of the system. The simulation result proves the previous theoretical analysis using replica method. In the result, we also find out the amount of blocking pairs which can be regarded as a representative of the system stability. Furthermore, we discover that the connectivity in the bipartite matching problem has a great impact on the stability of the ground state, and the system will become more unstable if there were more connections between men and women.	blocking (computing);computer simulation;control theory;frequency-hopping spread spectrum;ground state;hungarian algorithm;linear programming relaxation;matching (graph theory);ming library;non-blocking algorithm;numerical analysis;simulation;stable marriage problem;the unsolved	Gui-Yuan Shi;Yi-Xiu Kong;Hao Liao;Yi-Cheng Zhang	2015	CoRR		combinatorics;discrete mathematics;topology;bipartite graph;3-dimensional matching;mathematics;blossom algorithm;ground state;physics;quantum mechanics	ECom	31.414726892160978	11.366754661621952	168601
f7ff4633fdecc859ebdf804b6c8e2d2df3372ff5	automatic design of noncryptographic hash functions using genetic programming	evolutionary computation;genetic programming;hash functions	Noncryptographic hash functions have an immense number of important practical applications owing to their powerful search properties. However, those properties critically depend on good designs: Inappropriately chosen hash functions are a very common source of performance losses. On the other hand, hash functions are difficult to design: They are extremely nonlinear and counterintuitive, and relationships between the variables are often intricate and obscure. In this work, we demonstrate the utility of genetic programming (GP) and avalanche effect to automatically generate noncryptographic hashes that can compete with state-of-the-art hash functions. We describe the design and implementation of our system, called GP-hash, and its fitness function, based on avalanche properties. Also, we experimentally identify good terminal and function sets and parameters for this task, providing interesting information for future research in this topic. Using GP-hash, we were able to generate two different families of noncryptographic hashes. These hashes are able to compete with a selection of the most important functions of the hashing literature, most of them widely used in the industry and created by world-class hashing experts with years of experience.	avalanche effect;collision resistance;cryptographic hash function;evolutionary algorithm;evolutionary computation;exclusive or;experiment;fitness function;flaw hypothesis methodology;general-purpose markup language;general-purpose modeling;genetic programming;nonlinear system;software engineer;software industry	César Estébanez;Yago Sáez;Gustavo Recio;Pedro Isasi Viñuela	2014	Computational Intelligence	10.1111/coin.12033	feature hashing;genetic programming;security of cryptographic hash functions;double hashing;hash function;dynamic perfect hashing;computer science;artificial intelligence;theoretical computer science;machine learning;universal hashing;k-independent hashing;algorithm;swifft;hash tree;evolutionary computation	Security	27.825499396052077	4.466684496116874	168676
2611084ebd0205179f723a2710dc3a84dcaf740e	improved algorithms for exact and approximate boolean matrix decomposition	approximation algorithms;data mining;matrix decomposition;exact bmd heuristics exact boolean matrix decomposition approximate boolean matrix decomposition boolean matrix multiplication operator np hard heuristic algorithms mathematical formula approximation data mining column use condition factor matrix educational databases ideal item response matrix knowledge state matrix q matrix;heuristic algorithms;matrix decomposition approximation algorithms heuristic algorithms approximation methods data mining algorithm design and analysis;approximation methods;matrix decomposition approximation theory computational complexity data mining;algorithm design and analysis	An arbitrary m×n Boolean matrix M can be decomposed exactly as M = UοV, where U (resp. V) is an m×k (resp. k ×n) Boolean matrix and ο denotes the Boolean matrix multiplication operator. We first prove an exact formula for the Boolean matrix J such that M = MοJT holds, where J is maximal in the sense that if any 0 element in J is changed to a 1 then this equality no longer holds. Since minimizing k is NP-hard, we propose two heuristic algorithms for finding suboptimal but good decomposition. We measure the performance (in minimizing k) of our algorithms on several real datasets in comparison with other representative heuristic algorithms for Boolean matrix decomposition (BMD). The results on some popular benchmark datasets demonstrate that one of our proposed algorithms performs as well or better on most of them. Our algorithms have a number of other advantages: They are based on exact mathematical formula, which can be interpreted intuitively. They can be used for approximation as well with competitive “coverage.” Last but not least, they also run very fast. Due to interpretability issues in data mining, we impose the condition, called the “column use condition,” that the columns of the factor matrix U must form a subset of the columns of M. In educational databases, the “ideal item response matrix” R, the “knowledge state matrix” A and the “Q-matrix” Q play important roles. We show that they are related exactly by R̅ = A ̅ο QT. Thus, given R, we can find A and Q with a small number (k) of “knowledge states,” using our exact BMD heuristics.	approximation algorithm;benchmark (computing);boolean algebra;column (database);computation;data mining;database;heuristic (computer science);intellect;item response theory;jt (visualization format);matlab;mapreduce;matrix multiplication;maximal set;np-hardness;parallel computing;ply (game theory);testbed;universal quantification	Yuan Sun;Shiwei Ye;Yi Sun;Tsunehiko Kameda	2015	2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)	10.1109/DSAA.2015.7344813	matrix function;algorithm design;mathematical optimization;combinatorics;discrete mathematics;eigendecomposition of a matrix;sparse matrix;lu decomposition;nonnegative matrix;maximum satisfiability problem;matrix of ones;invertible matrix;mathematics;logical matrix;pascal matrix;state-transition matrix;matrix decomposition;block matrix;approximation algorithm;eigenvalue algorithm;algorithm;matrix	DB	27.559658508572213	13.340951603108573	168821
d5ab6238ce6d4f8d506c77ba1ed59babcd7341ba	a linear-time combinatorial algorithm to find the orthogonal hull of an object on the digital plane	image processing;combinatorial algorithm;shape analysis;digital geometry;convex hull;multiresolution analysis	A combinatorial algorithm to compute the orthogonal hull of a digital object imposed on a background grid is presented in this paper. The resolution and complexity of the orthogonal hull can be controlled by varying the grid size, which may be used for a multiresolution analysis of a given object. Existing algorithms on finding the convex hull are based on divide and conquer strategy, sweepline approach, etc., whereas the proposed algorithm is combinatorial in nature whose time complexity is linear on the object perimeter instead of the object area. For a larger grid size, the perimeter of an object decreases in length in terms of grid units, and hence the runtime of the algorithm reduces significantly. The algorithm uses only comparison and addition in the integer domain, thereby making it amenable to usage in real-world applications where speed is a prime factor. Experimental results including the CPU time demonstrate the elegance and efficacy of the proposed algorithm.	algorithm;combinatorial optimization;time complexity	Arindam Biswas;Partha Bhowmick;Moumita Sarkar;Bhargab B. Bhattacharya	2012	Inf. Sci.	10.1016/j.ins.2012.05.029	multiresolution analysis;mathematical optimization;combinatorics;discrete mathematics;image processing;computer science;gift wrapping algorithm;convex hull;machine learning;shape analysis;mathematics;algorithm;output-sensitive algorithm;digital geometry	Theory	31.679331128328425	15.730799785539977	168893
1488ca250a8081c5c28e8695a8f83c0d06543f5f	cut-generating functions	s free sets;separation;generalized gauges;integer programming;convex analysis	In optimization problems such as integer programs or their relaxations, one encounters feasible regions of the form {x ∈ R+ : Rx ∈ S} where R is a general real matrix and S ⊂ R is a specific closed set with 0 / ∈ S. For example, in a relaxation of integer programs introduced in [ALWW2007], S is of the form Z − b where b 6∈ Z . One would like to generate valid inequalities that cut off the infeasible solution x = 0. Formulas for such inequalities can be obtained through cut-generating functions. This paper presents a formal theory of minimal cut-generating functions and maximal S-free sets which is valid independently of the particular S. This theory relies on tools of convex analysis.	convex analysis;linear programming relaxation;mathematical optimization;max-flow min-cut theorem;maximal set;optimization problem	Michele Conforti;Gérard Cornuéjols;Aris Daniilidis;Claude Lemaréchal;Jérôme Malick	2013		10.1007/978-3-642-36694-9_11	convex analysis;mathematical optimization;combinatorics;discrete mathematics;integer programming;mathematics	Theory	25.205768525054136	12.690038620938436	168982
0a6f11953b88b879723ac659ed55fc95dc26b01d	ant colony optimization and the minimum spanning tree problem	ant colony optimization;construction procedure;random walk;runtime behavior;construction graph;simple aco algorithm;aco algorithm;incremental construction procedure;minimum spanning tree problem;so-called construction graph;heuristic information;broder-based algorithm	Ant Colony Optimization (ACO) is a kind of randomized search heuristic that has become very popular for solving problems from combinatorial optimization. Solutions for a given problem are constructed by a random walk on a so-called construction graph. This random walk can be influenced by heuristic information about the problem. In contrast to many successful applications, the theoretical foundation of this kind of randomized search heuristic is rather weak. Theoretical investigations with respect to the runtime behavior of ACO algorithms have been started only recently for the optimization of pseudo-boolean functions. We present the first comprehensive rigorous analysis of a simple ACO algorithms for a combinatorial optimization problem. In our investigations we consider the minimum spanning tree problem and examine the effect of two construction graphs with respect to the runtime behavior. The choice of the construction graph in an ACO algorithm seems to be crucial for the success of such an algorithm. First, we take the input graph itself as the construction graph and analyze the use of a construction procedure that is similar to Broder’s algorithm [1] for choosing a spanning tree uniformly at random. After that, a more incremental construction procedure is analyzed. It turns out that this procedure is superior to the Broder-based algorithm and produces additionally in a constant number of iterations a minimum spanning tree if the influence of the heuristic information is large enough. Financial support by the Deutsche Forschungsgemeinschaft (SFB) in terms of the Collaborative Research Center “Computational Intelligence” (SFB 531) is gratefully acknowledged. 1 Electronic Colloquium on Computational Complexity, Report No. 143 (2006)	andrei broder;ant colony optimization algorithms;blue (queue management algorithm);combinatorial optimization;computation;computational intelligence;dijkstra's algorithm;electronic colloquium on computational complexity;evolutionary algorithm;file spanning;formal proof;http 404;heuristic;iteration;kruskal's algorithm;mathematical optimization;minimum spanning tree;optimization problem;polynomial;program optimization;pseudo-boolean function;randomized algorithm;with high probability	Frank Neumann;Carsten Witt	2006		10.1007/978-3-540-92695-5_12	random graph;euclidean minimum spanning tree;optimization problem;mathematical optimization;combinatorics;kruskal's algorithm;combinatorial optimization;minimum spanning tree;machine learning;gomory–hu tree;mathematics;reverse-delete algorithm;distributed minimum spanning tree;metaheuristic	Theory	27.076246747203818	5.661056135080642	169159
accc6ba8c0b6451c01997c1cef8a59d44245c4b1	where and how chew's second delaunay refinement algorithm works		Chew’s second Delaunay refinement algorithm with offcenter Steiner vertices leads to practical improvement over Ruppert’s algorithm for quality mesh generation, but the most thorough theoretical analysis is known only for Ruppert’s algorithm. A detailed analysis of Chew’s second Delaunay refinement algorithm with offcenters is given, improving the guarantee of well-graded output for any minimum angle threshold α ≤ 28.60.	delaunay triangulation;mesh generation;refinement (computing);ruppert's algorithm;steiner tree problem	Alexander Rand	2011			mathematical optimization;combinatorics;ruppert's algorithm;mathematics;constrained delaunay triangulation;chew's second algorithm;algorithm	Theory	33.189607439280955	16.772582576104096	169415
3941aa85c3bab56efe6533c74f3f899d49d5e50d	choosing sample path length and number of sample paths when starting in steady state	politica optima;autocorrelation function;optimisation;metodo monte carlo;stochastic process;optimizacion;methode monte carlo;optimal policy;funcion autocorrelacion;fonction autocorrelation;monte carlo method;processus stochastique;optimization;proceso estocastico;monte carlo;politique optimale;steady state	Consider the problem of estimating the mean of a strictly stationary stochastic process by Monte Carlo sampling for the case in which the process has autocorrelation function {@a^|^s^|, |@a|  1 and t^*=1, n^* and t^*>1, and n^*>1. Results are presented in terms of @a and @q, a relative cost ratio. Also, we extend the analysis to autocorrelation functions that are convex combinations of geometrically decreasing quantities.	steady state	George S. Fishman	1994	Oper. Res. Lett.	10.1016/0167-6377(94)90070-1	stochastic process;econometrics;mathematical optimization;mathematics;statistics;monte carlo method	NLP	37.80354067304728	6.592900678709954	169426
9273bf843249cab777e36a5d32a2bdbb3f490aac	whittle index for partially observed binary markov decision processes	markov processes;process control;dynamic programming;aerospace electronics;dynamic scheduling	"""We consider the problem of dynamically scheduling <inline-formula><tex-math notation=""""LaTeX"""">$M$</tex-math> </inline-formula> out of <inline-formula><tex-math notation=""""LaTeX"""">$N$</tex-math></inline-formula> binary Markov chains when only noisy observations of state are available, with ergodic (equivalently, long run average) reward. By passing on to the equivalent problem of controlling the conditional distribution of state given observations and controls, it is cast as a restless bandit problem and its Whittle indexability is established."""	ergodicity;markov chain;markov decision process;multi-armed bandit;scheduling (computing)	Vivek S. Borkar	2017	IEEE Transactions on Automatic Control	10.1109/TAC.2017.2715329	mathematical optimization;ergodic theory;partially observable markov decision process;mathematics;dynamic priority scheduling;conditional probability distribution;markov model;markov decision process;markov process;markov chain	Metrics	38.711788057850995	4.941025986677362	169632
5f96e1246fb56229bdec6e68c49d5a9bf2549e6b	optimality and competitiveness of exploring polygons by mobile robots	poligono convexo;cercle;trajectoire;decalaje;competitive algorithm;mobile robot;competitividad;on line;benchmark;polygone convexe;en linea;forme convexe;convex shape;performance;competitive algorithms;on line exploration;exploracion;forma convexa;decalage;polygon;algorithme competitif;trajectory;informatique theorique;circulo;benchmarks;competitiveness;superficie;exploration;area;en ligne;51e12;trayectoria;shift;rendimiento;algoritmo optimo;algorithme optimal;circle;optimal algorithm;competitivite;robot;performance optimization;convex polygon;computer theory;polygon convexe;informatica teorica	A mobile robot, represented by a point moving along a polygonal line in the plane, has to explore an unknown polygon and return to the starting point. The robot has a sensing area which can be a circle or a square centered at the robot. This area shifts while the robot moves inside the polygon, and at each point of its trajectory the robot “sees” (explores) all points for which the segment between the robot and the point is contained in the polygon and in the sensing area. We focus on two tasks: exploring the entire polygon and exploring only its boundary. We consider several scenarios: both shapes of the sensing area and the Manhattan and the Euclidean metrics.We focus on two quality benchmarks for exploration performance: optimality (the length of the trajectory of the robot is equal to that of the optimal robot knowing the polygon) and competitiveness (the length of the trajectory of the robot is at most a constant multiple of that of the optimal robot knowing the polygon). Most of our results concern rectilinear polygons. We show that optimal exploration is possible in only one scenario, that of exploring the boundary by a robot with square sensing area, starting at the boundary and using the Manhattan metric. For this case we give an optimal exploration algorithm, and in all other scenarios we prove impossibility of optimal exploration. For competitiveness the situation is more optimistic: we show a competitive exploration algorithm for rectilinear polygons whenever the sensing area is a square, for both tasks, regardless of the metric and of the starting point. Finally, we show a competitive exploration algorithm for arbitrary convex polygons, for both shapes of the sensing area, regardless of the metric and of the starting point.	competitive analysis (online algorithm);mobile robot	Jurek Czyzowicz;Arnaud Labourel;Andrzej Pelc	2011	Inf. Comput.	10.1016/j.ic.2010.09.005	robot;mobile robot;mathematical optimization;simulation;point in polygon;benchmark;exploration;visibility polygon;performance;computer science;artificial intelligence;trajectory;polygon;mathematics;geometry;area;monotone polygon;polygon covering	Robotics	30.38257855131049	17.2926160916999	169819
ca4c99e5ee42db9283acf66c24715ba14acd64fe	beyond maximum independent set: an extended integer programming formulation for point labeling		Map labeling is a classical problem of cartography that has frequently been approached by combinatorial optimization. Given a set of features in a map and for each feature a set of label candidates, a common problem is to select an independent set of labels (that is, a labeling without label–label intersections) that contains as many labels as possible and at most one label for each feature. To obtain solutions of high cartographic quality, the labels can be weighted and one can maximize the total weight (rather than the number) of the selected labels. We argue, however, that when maximizing the weight of the labeling, the influences of labels on other labels are insufficiently addressed. Furthermore, in a maximum-weight labeling, the labels tend to be densely packed and thus the map background can be occluded too much. We propose extensions of an existing model to overcome these limitations. Since even without our extensions the problem is NP-hard, we cannot hope for an efficient exact algorithm for the problem. Therefore, we present a formalization of our model as an integer linear program (ILP). This allows us to compute optimal solutions in reasonable time, which we demonstrate both for randomly generated point sets and an existing data set of cities. Moreover, a relaxation of our ILP allows for a simple and efficient heuristic, which yielded near-optimal solutions for our instances.	benchmark (computing);cartography;combinatorial optimization;exact algorithm;experiment;heuristic;independent set (graph theory);integer programming;linear programming relaxation;loss function;map;mathematical optimization;maxima and minima;np-hardness;optimization problem;population;procedural generation;rounding;topography;weight function	Jan-Henrik Haunert;Alexander Wolff	2017	ISPRS Int. J. Geo-Information	10.3390/ijgi6110342	discrete mathematics;sequence labeling;automatic label placement;combinatorial optimization;independent set;integer programming;mathematical optimization;linear programming;mathematics;exact algorithm;integer	ML	24.69800310077643	6.8638881491205295	169917
78622b99a62c73105be77e9753ce561176598e05	deferred data structuring	68p20;query processing;computational geometry;68q20;68p10;half space intersection;linear programming;randomized algorithm;68p05;68u05;preprocessing;convex hull;dominance counting;data structure;lower bound	We consider the problem of answering a series of on-line queries on a static data set. The conventional approach to such problems involves a preprocessing phase which constructs a data structure with good search behavior. The data structure representing the data set then remains fixed throughout the processing of the queries. Our approach involves dynamic or query-driven structuring of the data set; our algorithm processes the data set only when doing so is required for answering a query. A data structure constructed progressively in this fashion is called a deferred data structure. We develop the notion of deferred data structures by solving the problem of answering membership queries on an ordered set. We obtain a randomized algorithm which achieves asymptotically optimal performance with high probability. We then present optimal deferred data structures for the following problems in the plane: testing convex-hull membership, half-plane intersection queries and fixed-constraint multi-objective linear programming. We also apply the deferred data structuring technique to multidimensional dominance query problems. Key words, data structure, preprocessing, query processing, lower bound, randomized algorithm, computational geometry, convex hull, linear programming, half-space intersection, dominance counting AMS(MOS) subject classifications. 68P05, 68P10, 68P20, 68Q20, 68U05	asymptotically optimal algorithm;computation;computational geometry;convex hull;data structure;database;linear programming;online and offline;preprocessor;randomized algorithm;with high probability	Richard M. Karp;Rajeev Motwani;Prabhakar Raghavan	1988	SIAM J. Comput.	10.1137/0217055	mathematical optimization;combinatorics;data structure;computational geometry;computer science;linear programming;theoretical computer science;convex hull;mathematics;geometry;upper and lower bounds;randomized algorithm;preprocessor;algorithm	Theory	27.85905033373988	16.194452271922064	169957
9a7cc024fbcaa80e5e0110a9cb8daae34b0b121f	an fpga implementation of genet for solving graph coloring problems	neural networks;time table scheduling;time complexity;turning;application software;processor scheduling;bandwidth allocation;neural net architecture;constraint satisfaction problems;genetics;computer networks;fpga implementation;graph coloring problem;car sequencing;genetic network;constraint programming;generic neural network model;constraint handling;neural net architecture field programmable gate arrays constraint handling graph colouring;graph coloring problems;computer science;read write memory;generic neural network model fpga implementation genet graph coloring problems constraint satisfaction problems time table scheduling bandwidth allocation car sequencing time complexity;field programmable gate arrays;field programmable gate arrays neural networks read write memory computer networks microprogramming computer science application software processor scheduling channel allocation turning;microprogramming;channel allocation;genet;graph colouring	for Solving Graph Coloring Problems T.K. Lee, P.H.W. Leong, K.H. Lee, K.T. Chan, S.K. Hui, H.K. Yeung, M.F. Lo, and J.H.M. Lee Department of Computer Science and Engineering The Chinese University of Hong Kong Shatin, N.T., Hong Kong CSPs and GENET Constraint satisfaction problems (CSPs) can be used to model problems in a wide variety of application areas, such as time-table scheduling, bandwidth allocation, and car-sequencing [2]. To solve a CSP means nding appropriate values for its set of variables such that all of the speci ed constraints are satis ed. Almost all CSPs have exponential time complexity and instances of them may require a prohibitively large amount of time to solve. Consequently, much research has been done in developing e cient methods to solve CSPs. In particular, a generic neural network (GENET) model, developed by Wang and Tsang [3], has been demonstrated to work extremely well in solving many CSPs, often nding solutions where other methods fail. Figure 1 shows how a graph coloring problem1 with 5 vertices (variables) and 3 colors (values) is transformed into a neural network under the GENET model. The nodes in the network are organized into clusters, one for each variable in the CSP. Each node in the cluster represents a possible value for the corresponding variable. Each node can be either ON or OFF. At any given time, each cluster has exactly one ON node, which represents the value currently assigned to the corresponding variable. A connection between two nodes in the neural network indicates a constraint that would be violated if both these nodes are ON. For instance, the upper left horizontal connection indicates that vertex z0 and vertex z1 cannot both be assigned the color 0. Once a CSP has been transformed into a network, the steps outlined below are performed to nd one of its solutions. First, each connection in the network is 1In the context of constraint programming, a solution for a graph-coloring problem is any consistent color assignment. Whether the number of colors used is minimal or not is of no importance. Cluster Domain 0 1 2	artificial neural network;color;computer cluster;computer science;constraint programming;constraint satisfaction;field-programmable gate array;graph coloring;schedule;scheduling (computing);time complexity	T. K. Lee;Philip Heng Wai Leong;K. H. Lee;K. T. Chan;S. K. Hui;H. K. Yeung;M. F. Lo;Jimmy Ho-Man Lee	1998		10.1109/FPGA.1998.707918	time complexity;constraint programming;application software;parallel computing;computer science;theoretical computer science;operating system;graph coloring;distributed computing;microcode;constraint satisfaction problem;artificial neural network;field-programmable gate array;bandwidth allocation	AI	30.234341801199953	4.737710022002489	170090
e994a15a9476ca92c4bf9c12ad89625401939d57	robust partially observable markov decision process		We seek to find the robust policy that maximizes the expected cumulative reward for the worst case when a partially observable Markov decision process (POMDP) has uncertain parameters whose values are only known to be in a given region. We prove that the robust value function, which represents the expected cumulative reward that can be obtained with the robust policy, is convex with respect to the belief state. Based on the convexity, we design a value-iteration algorithm for finding the robust policy. We prove that our value iteration converges for an infinite horizon. We also design point-based value iteration for fining the robust policy more efficiency possibly with approximation. Numerical experiments show that our point-based value iteration can adequately find robust policies.	algorithm;approximation;bellman equation;best, worst and average case;experiment;iteration;iterative method;markov chain;numerical analysis;partially observable markov decision process;partially observable system	Takayuki Osogami	2015			mathematical optimization;partially observable markov decision process;mathematics;mathematical economics	ML	36.62532341813852	4.691710776376971	170392
54fbbce020fc5b0194dbfc64257d06a793245d49	a pleasant stroll through the land of infinitely many creatures	fixed set;common problem;n process;brief summary;algorithmic technique	Many distributed algorithms are designed for a system with a fixed set of n processes. However, some systems may dynamically change and expand over time,so that the number of processes may grow to infinity as time tends to infinity. This paper considers such systems, and gives algorithms that are new and simple (but not necessarily efficient) for common problems. The reason for simplicity is to better expose some of the algorithmic techniques for dealing with infinitely many processes. A brief summary of existing work in the subject is also provided.	distributed algorithm;process (computing)	Marcos K. Aguilera	2004	SIGACT News	10.1145/992287.992298	mathematical optimization;combinatorics;discrete mathematics;mathematics;geometry;algorithm	Theory	36.94077355885786	18.104554873387052	170547
0ae39a5272b255a157dbde6d85100a4318c4006b	obstacle constrained total area coverage in wireless sensor networks	internet architecture;wireless sensor network;polynomial time algorithm	This paper deals with the accomplishment of total area coverage of an arbitrary region using sensors with a fini te sensing radius of rs. For a given region, we aim to obtain a deterministic placement of sensors which, apart from ensur ing that the entire region comes under the purview of at least a single sensor, minimises the number of sensors utilised. W e begin by considering regions devoid of obstacles and thus ha ving every location amenable for placement. Herein, we formalis e the popular notion that sensors at the centres of the hexagons of a hexagonal tessellation provide the most optimal placemen t. We then move on to regions which may comprise obstacles of arbitrary size at arbitrary locations. We recognise two distinct classes of obstacles, namely transparent and opaque obstac les, which are distinguished by their ability (or the lack of it) to permit sensing radiation through them. In the real world, transparent obstacles model lakes, ponds and swamps, while the opaque ones stand for, inter alia, hills, trees and walls. We propose a polynomial-time algorithm for achieving optimal placement in the aforesaid scenarios and we prove its convergence.	algorithm;asymptotically optimal algorithm;opaque pointer;polynomial;sensor;time complexity	M Shyam;Anurag Kumar	2010	CoRR		simulation;wireless sensor network;telecommunications;computer science;computer security	Mobile	30.95192558347039	17.439620836445396	170581
24ccb192a030d34427846071f985fc8f779cf330	thermodynamically favorable computation via tile self-assembly		The recently introduced Thermodynamic Binding Networks (TBN) model was developed with the purpose of studying self-assembling systems by focusing on their thermodynamically favorable final states, and ignoring the kinetic pathways through which they evolve. The model was intentionally developed to abstract away not only the notion of time, but also the constraints of geometry. Collections of monomers with binding domains which allow them to form polymers via complementary bonds are analyzed to determine their final, stable configurations, which are those which maximize the number of bonds formed (i.e. enthalpy) and the number of independent components (i.e. entropy). In this paper, we first develop a definition of what it means for a TBN to perform a computation, and then present a set of constructions which are capable of performing computations by simulating the behaviors of space-bounded Turing machines and boolean circuits. In contrast to previous TBN results, these constructions are robust to great variability in the counts of monomers existing in the systems and the numbers of polymers that form in parallel. Although the Turing machine simulating TBNs are inefficient in terms of the numbers of unique monomer types required, as compared to algorithmic self-assembling systems in the abstract Tile Assembly Model (aTAM), we then show that a general strategy of porting those aTAM system designs to TBNs produces TBNs which incorrectly simulate computations. Finally, we present a refinement of the TBN model which we call the Geometric Thermodynamic Binding Networks (GTBN) model in which monomers are defined with rigid geometries and form rigid bonds. ∗Department of Electrical and Computer Engineering, University of Texas at Austin, Austin, TX, USA ctchalk@utexas.edu This author’s research was supported in part by National Science Foundation Grants CCF-1618895 and CCF-1652824. †Department of Computer Science and Information Systems, University of Wisconsin River Falls, River Falls, WI, USA jacob.hendricks@uwrf.edu ‡Department of Computer Science and Computer Engineering, University of Arkansas, Fayetteville, AR, USA patitz@uark.edu This author’s research was supported in part by National Science Foundation Grants CCF-1422152 and CAREER-1553166. §Department of Computer Science and Computer Engineering, University of Arkansas, Fayetteville, AR, USA. mrs018@uark.edu. This author’s research was supported in part by National Science Foundation Grants CCF-1422152 and CAREER-1553166. 1 ar X iv :1 80 2. 02 68 6v 1 [ cs .E T ] 8 F eb 2 01 8 Utilizing the constraints imposed by geometry, we then provide a GTBN construction capable of simulating Turing machines as efficiently as in the aTAM.	boolean circuit;computation;computer engineering;computer science;information systems;refinement (computing);self-assembly;simulation;spatial variability;the baseball network;turing machine	Cameron T. Chalk;Jacob Hendricks;Matthew J. Patitz;Michael Sharp	2018		10.1007/978-3-319-92435-9_2	kinetic energy;theoretical computer science;enthalpy;discrete mathematics;computation;self-assembly;boolean circuit;turing machine;computer science	Theory	36.149541361167785	15.88516028239147	170676
64b63c73356444f19a881eef6d05bd5534e83686	a fast implementation of near neighbors queries for fréchet distance (gis cup)		This paper describes an implementation of fast near-neighbours queries (also known as range searching) with respect to the Fréchet distance. The algorithm is designed to be efficient on practical data such as GPS trajectories. Our approach is to use a quadtree data structure to enumerate all curves in the database that have similar start and endpoints as the query curve. On these curves we run positive and negative filters to narrow the set of potential results. Only for those trajectories where these heuristics fail, we compute the Fréchet distance exactly, by running a novel recursive variant of the classic free-space diagram algorithm.  Our implementation is among the top 3 submissions to the ACM SIGSPATIAL GIS Cup 2017 (more specific standings will be available on the competition's website).	algorithm;data structure;diagram;enumerated type;fréchet distance;geographic information system;heuristic (computer science);library (computing);negative feedback;quadtree;range searching;recursion;standard library;time complexity	Julian Baldus;Karl Bringmann	2017		10.1145/3139958.3140062	artificial intelligence;diagram;range searching;mathematics;machine learning;computational geometry;recursion;fréchet distance;heuristics;quadtree;data structure	DB	31.093805614569245	17.669515300226415	170757
5165d6f5e188675a7a1b9252f17404da9a017ce2	a stochastic proximal point algorithm for total variation regularization over large scale graphs	convergence;complexity theory;approximation algorithms;markov processes;tv;signal processing algorithms;programming	The total-variation (TV) regularizer is often used to promote the structured sparsity of a given real function over the vertices of a non-directed graph. Indeed, the proximity operator associated with TV regularizer promotes sparsity of the function discrete gradient. Although quite affordable in the special case of one-dimensional (1D) graphs, the computation of the proximity operator for general large scale graphs can be demanding. In this paper, we propose a stochastic algorithm for solving this problem over large graphs with a moderate iteration complexity. The algorithm consists in properly selecting random paths in the graph and computing 1D-proximity operators over these paths. Convergence of the algorithm is related to recent results on stochastic proximal point algorithms.	algorithm;computation;directed graph;gradient;graph (discrete mathematics);iteration;sparse matrix;total variation denoising	Adil Salim;Pascal Bianchi;Walid Hachem;Jérémie Jakubowicz	2016	2016 IEEE 55th Conference on Decision and Control (CDC)	10.1109/CDC.2016.7798952	programming;mathematical optimization;combinatorics;discrete mathematics;convergence;floyd–warshall algorithm;hopcroft–karp algorithm;mathematics;markov process;approximation algorithm	ML	34.04385010808231	6.908899316508788	170891
539c16a5b46c0bcb9101100410604df79b6f7f64	on the equivalence in complexity among three computation problems on maximum number of edge-disjoint s-t paths in a probabilistic graph	complexite calcul;expected maximum number of edge disjoint paths;fonction repartition;funcion densidad probabilidad;probability density function;edge disjoint;fonction densite probabilite;funcion distribucion;distribution function;complejidad computacion;computational complexity;probabilistic graph;probability distribution function;polynomial transformation	Abstract   This paper shows that three problems of computing probability density function, computing probability distribution function and computing expected value for the random maximum number of edge-disjoint  s − t  paths in a probabilistic graph with specified source  s  and sink  t  ( s ≠ t ) are equivalent in computational complexity.	computation;turing completeness	Peng Chen;Shigeru Masuyama	1994	Inf. Process. Lett.	10.1016/0020-0190(94)00076-X	probability distribution;random variable;probability density function;combinatorics;discrete mathematics;mathematics;geometry	Theory	38.8980936003928	15.279032833954258	171071
f1cef7fb07a6a8f21a33ed290834766e8c2f8e69	optimality of partial adiabatic search and its circuit model	optimality;partial adiabatic evolution;quantum circuit model;期刊论文;quantum computing	In this paper, we first uncover a fact that a partial adiabatic quantum search with O( √ N/M) time complexity is in fact optimal, in which N is the total number of elements in an unstructured database, and M(M ≥ 1) of them are the marked ones(one) (N M). We then discuss how to implement a partial adiabatic search algorithm on the quantum circuit model. From the implementing procedure on the circuit model, we can find out that the approximating steps needed are always in the same order of the time complexity of the adiabatic algorithm.	quantum circuit;search algorithm;time complexity	Ying Mei;Jie Sun;Songfeng Lu;Chao Gao	2014	Quantum Information Processing	10.1007/s11128-014-0770-6	combinatorics;theoretical computer science;mathematics;quantum computer;physics;algorithm;quantum mechanics;adiabatic quantum computation	Theory	30.488890016447737	11.347513416775834	171518
f6ab1b77dfab54a7badcfc89739664d052065058	on enclosing k points by a circle	smallest enclosing circle;complexite calcul;geometrie algorithmique;computational geometry;algorithme;algorithm;complejidad computacion;computational complexity;clustering;parametric search;randomized algorithm;geometria computacional;algoritmo	Abstract   We consider the problem of finding, for a given  n -point set  P  in the plane and an integer  k  ⩽  n , a smallest circle enclosing at least  k  points of  P . We present randomized algorithms with O( nk ) space and O( n  log  n  +  nk ) expected running time, resp. O( n ) space and O( n  log  n  +  nk  log  k ) time. This improves on previous results by logarithmic factors, and our algorithms are simpler and easier to implement.		Jirí Matousek	1995	Inf. Process. Lett.	10.1016/0020-0190(94)00190-A	combinatorics;computational geometry;computer science;mathematics;geometry;cluster analysis;randomized algorithm;computational complexity theory;algorithm	DB	29.59065582069089	17.646142125932116	171547
495ae7ef72a5b495a2f8155cf94120276714f17e	a binary integer programming model for global optimization of learning path discovery		This paper introduces a method based on graph theory and operations research techniques to optimize learning path discovery. In this method, learning objects are considered as nodes and competencies as vertices of a learning graph. A first step consists in reducing the solution space by obtaining an induced subgraph H. In a second step, the search of an optimal learning path in H is considered as a binary integer programming problem which we propose to solve using an exact method based on the well-known branch-and-bound algorithm. The method detailed in the paper takes into account the prerequisite and gained competencies as constraints of the optimization problem by minimizing the total competencies needed to reach the learning objective.	algorithm;branch and bound;feasible region;global optimization;graph theory;induced subgraph;integer programming;linear programming;mathematical optimization;operations research;optimization problem;programming model;whole earth 'lectronic link	Nabil Belacel;Guillaume Durand;François LaPlante	2014			binary integer decimal;graph theory;global optimization;branch and price;integer programming;induced subgraph;mathematical optimization;machine learning;programming paradigm;artificial intelligence;optimization problem;mathematics	ML	24.636897258191134	5.038819371844879	171834
87320d62778fadddd4f9350218e181b438bfd740	solution of 3 × 3 games using graphical method	game theory;graphical solution;simplex method;three dimensional;optimal strategy;3 n games;graphical method;3 3 games	While graphical solution of 2× n  games is described in all OR text books, solution of games of size 3×3 and higher sizes is obtained using simplex method only. This paper describes a method of solving games of size 3×3 graphically. The basic principle is to consider the problem as a three-dimensional model, and to convert it into plan and elevation, and thereby obtaining the solution. Each strategy is represented by a plane, and the common point where these planes intersect, is the solution point. The location of the plan of the solution point in relation to the strategies decides the probabilities with which they are to be played, and the height of the solution point gives the value of the game. Extension of this method for solving games of size 3× n  is also discussed.	list of graphical methods	K. G. K. Nair;G. Ranjith	1999	European Journal of Operational Research	10.1016/S0377-2217(97)00440-2	combinatorial game theory;three-dimensional space;game theory;mathematical optimization;combinatorics;simulation;mathematics;simplex algorithm	Robotics	29.410663025056387	12.427096089990206	171852
e038ef2dfd4ac93d98c2033668dd540b805b01f0	computing grasp functions	informatica;planning algorithm;mathematics;linear time algorithm;algebraic arcs;natuurwetenschappen;ordered by external client;parallel jaw gripper;width function;manufacturing;landbouwwetenschappen;wiskunde en informatica wiin	The grasp function of a planar part, characterized by extrema in the part's width function, can be used to analyze grasp mechanics. In particular, it can predict the final orientation of the part when it is grasped with a parallel-jaw gripper. This information allows us to derive a sequence of grasp angles that will orient the part up to symmetry in the grasp function. In previous papers the grasp function was assumed given as input; in this paper we present a linear-time algorithm for computing the grasp function of a part bounded by n algebraic arcs given in parametric form. We also show that the algorithm can be extended to compute a related function that describes the outcome of first pushing and then grasping the part.	algorithm;linear algebra;robot end effector;time complexity	Anil S. Rao;Kenneth Y. Goldberg	1996	Comput. Geom.	10.1016/0925-7721(95)00022-4	computer science;artificial intelligence;mathematics;manufacturing;algorithm	Robotics	33.648558546137984	18.11578246540449	172055
34c13d16ef44c2c5b8151d8b78b24fcf8dbc63e5	declaring independence via the sketching of sketches	equi partition;fairness;data stream;online scheduling;iron;non clairvoyant algorithm;distributed models;mutual information;precedences	"""We consider the problem of identifying correlations in data streams. Surprisingly, our work seems to be the first to consider this natural problem. In the centralized model, we consider a stream of pairs (i,j) ∈ [n]2 whose frequencies define a joint distribution (X,Y). In the distributed model, each coordinate of the pair may appear separately in the stream. We present a range of algorithms for approximating to what extent X and Y are independent, i.e., how close the joint distribution is to the product of the marginals. We consider various measures of closeness including ℓ1, ℓ2, and the mutual information between X and Y. Our algorithms are based on """"sketching sketches"""", i.e., composing small-space linear synopses of the distributions. Perhaps ironically, the biggest technical challenges that arise relate to ensuring that different components of our estimates are sufficiently independent."""	algorithm;centrality;centralized computing;mutual information	Piotr Indyk;Andrew McGregor	2008			mathematical optimization;combinatorics;discrete mathematics;theoretical computer science;mathematics;distributed computing;mutual information;iron;algorithm;statistics	Theory	31.55893542138954	10.456668886380767	172257
d4d28faa11f38f637619497c7a69b7f11a9d616c	perturbation of ctmc trapping probabilities with application to model repair		This paper studies properties of continuous-time Markov chains with one class of transient states and at least two absorbing states. We look at a perturbation of the chain that arises by uniformly decreasing all rates to absorption. For this situation, the monotonicity of the trapping probabilities is analysed, and their asymptotic limit is computed. The theoretical findings are then applied to a type of model repair problem, where a lower time-bounded and lower probability-bounded CSL until requirement needs to be satisfied. The paper presents an algorithm for this type of problem and proves its correctness.	absorbing markov chain;algorithm;asymptote;computerized speech lab;correctness (computer science);method of conditional probabilities;prototype	Alexander Gouberman;Markus Siegle;Bharath Siva Kumar Tati	2017		10.1145/3150928.3150947	distributed computing;correctness;mathematical optimization;computer science;monotonic function;markov chain;perturbation (astronomy)	PL	38.64052888143047	6.601738549505865	172328
383093379d9c7a8e5a7d66a01b024283f59f492b	the national energy modeling system: a large-scale energy-economic equilibrium model	energy policies general framework for determining energy policies;mathematical programming;complementarity energy economic application for the nonlinear complementarity problem government;mathematical programming complementarity energy economic application for the nonlinear complementarity problem;government energy policies general framework for determining energy policies	The National Energy Modeling System (NEMS) is a large-scale mathematical model that computes equilibrium fuel prices and quantities in the U.S. energy sector and is currently in use at the U.S. Department of Energy (DOE). At present, to generate these equilibrium values, NEMS iteratively solves a sequence of linear programs and nonlinear equations. This is a nonlinear Gauss-Seidel approach to arrive at estimates of market equilibrium fuel prices and quantities. In this paper, we present existence and uniqueness results for NEMS-type models based on a nonlinear complementarity/variational inequality problem format. Also, we document mathematically, for the first time, how the inputs and the outputs for each NEMS module link together.	calculus of variations;complementarity theory;computation;diagonally dominant matrix;energy modeling;fo (complexity);mathematical optimization;mathematical structure;microsoft outlook for mac;nl (complexity);nonlinear complementarity problem;nonlinear system;social inequality;variational inequality	Steven A. Gabriel;Andy S. Kydes;Peter Whitman	2001	Operations Research	10.1287/opre.49.1.14.11195	mathematical optimization;economics;mathematics;mixed complementarity problem;mathematical economics;welfare economics	Robotics	28.892254846226837	6.877863524274709	172366
1323d1c9f33110c21ae08a39150c1e9a2c71aed3	multi-dimensional pruning from the baumann point in an interval global optimization algorithm	pruning test;branch and bound algorithm;b method;upper bound;multi dimensional;branch and bound method;global optimization;interval arithmetic;lower bound	A new pruning method for interval branch and bound algorithms is presented.Inreliableglobaloptimizationmethodsthereareseveralapproachestomakethe algorithms faster. In minimization problems, interval B&B methods use a good upper boundofthefunctionattheglobalminimumandgoodlowerboundsofthefunctionatthe subproblemstodiscardmostofthem,buttheyneedefficientpruningmethodstodiscard regionsofthesubproblemsthatdonotcontainglobalminimizerpoints.Thenewpruning method presented here is based on the application of derivative information from the Baumann point. Numerical results were obtained by incorporating this new technique into a basic Interval B&B Algorithm in order to evaluate the achieved improvements.	algorithm;algorithmic efficiency;branch and bound;computation;global optimization;interval arithmetic	Boglárka G.-Tóth;Leocadio G. Casado	2007	J. Global Optimization	10.1007/s10898-006-9072-6	mathematical optimization;combinatorics;mathematics;upper and lower bounds;algorithm;global optimization	EDA	27.445654134864345	10.15379428173806	172655
2180ab0d1ac5b095defed2034f6ff66e65020a0a	efficient approximation of optimal control for continuous-time markov games	discretisation;liverpool;continuous time markov decision processes and games;repository;optimal control;university	We study the time-bounded reachability problem for continu ous-time Markov decision processes (CTMDPs) and games (CTMGs). Exis ting techniques for this problem use discretisation techniques to break tim e into discrete intervals, and optimal control is approximated for each interval separately. Current techniques provide an accuracy of O(ε) on each interval, which leads to an infeasibly large number of intervals. We propose a sequence of approximations that achieve accuracies of O(ε), O(ε), andO(ε), that allow us to drastically reduce the number of intervals that are considered. For CTMD Ps, the performance of the resulting algorithms is comparable to the heuri stic approach given by Buckholz and Schulz [6], while also being theoretically j ustified. All of our results generalise to CTMGs, where our results yield the firs t practically implementable algorithms for this problem. We also provide posit i nal strategies for both players that achieve similar error bounds.	approximation algorithm;discretization;markov chain;markov decision process;optimal control;reachability problem	John Fearnley;Markus N. Rabe;Sven Schewe;Lijun Zhang	2016	Inf. Comput.	10.1016/j.ic.2015.12.002	mathematical optimization;combinatorics;discrete mathematics;optimal control;discretization;mathematics;algorithm;statistics	Logic	37.346895803669035	5.232393546479446	172717
af46c5030450835f653c6e7b145d783bccd27a9f	automated assistance for search-based refactoring using unfolding of graph transformation systems	design process;combinatorial optimization problem;internal structure;graph transformation;objective function;quality measures	Refactoring has emerged as a successful technique to enhance the internal structure of software by a series of small, behaviour-preserving transformations [4]. However, due to complex dependencies and conflicts between the individual refactorings, it is difficult to choose the best sequence of refactoring steps in order to effect a specific improvement. In the case of large systems the situation becomes acute because existing tools offer only limited support for their automated application [8]. Therefore, search-based approaches have been suggested in order to provide automation in discovering appropriate refactoring sequences [6,11]. The idea is to see the design process as a combinatorial optimization problem, attempting to derive the best solution (with respect to a quality measure called objective function) from a given initial design [9].	code refactoring;graph rewriting;unfolding (dsp implementation)	Fawad Qayum	2010		10.1007/978-3-642-15928-2_34	optimization problem;mathematical optimization;combinatorics;discrete mathematics;wait-for graph;design process;computer science;mathematics	SE	26.80311462354105	6.816691093216898	173077
b3dbf007bd381e1d73c978da10867bc627b8e3a9	a polytope approach to the optimal assembly problem	system reliability;lower and upper bound;optimization technique;reliability function;geometric approach;article	The problem of assembling components into series modules to maximize the system reliability has been intensively studied in the literature. Invariably, the methods employed exploit special properties of the reliability function through standard analytical optimization techniques. We propose a geometric approach by exploiting the assembly polytope – a polytope generated by the potential assembly configurations. The new approach yields simpler proofs of known results, as well as new results about systems where the number of components in a module is not fixed, but subject to lower and upper bounds.		Frank K. Hwang;Uriel G. Rothblum	2006	J. Global Optimization	10.1007/s10898-005-3844-2	mathematical optimization;combinatorics;discrete mathematics;mathematics;survival function	Theory	27.37342490360466	10.504338952712912	173226
a706471629b0b8bb3e882215795d947853c512f2	optimizing the coherence of composite networks		We consider how to connect a set of disjoint networks to optimize the performance of the resulting composite network. We quantify this performance by the coherence of the composite network, which is defined by an H2 norm of the system. Two dynamics are considered: noisy consensus dynamics with and without stubborn agents. For noisy consensus dynamics without stubborn agents, we derive analytical expressions for the coherence of composite networks in terms of the coherence of the individual networks and the structure of their interconnections. We also identify optimal interconnection topologies and give bounds on coherence for general composite graphs. For noisy consensus dynamics with stubborn agents, we show that the coherence of a composite network is a submodular function over the set of potential edges between the disjoint networks. We leverage this submodularity to develop a non-combinatorial algorithm that identifies connecting edges such that the composite network coherence is within a provable bound of optimal.	algorithm;combinatorial optimization;consensus dynamics;h2 database engine;interconnection;optimizing compiler;provable security;submodular set function	Erika Mackin;Stacy Patterson	2017	2017 American Control Conference (ACC)	10.23919/ACC.2017.7963622	mathematical optimization;combinatorics;mathematics;distributed computing	Theory	36.123120911149044	12.648814733615842	173233
0fa74e81e3c7eec223e142be46aa03b165717cda	designing networks: a mixed-integer linear optimization approach		Designing networks with specified collective properties is useful in a variety of application areas, enabling the study of how given properties affect the behavior of network models, the downscaling of empirical networks to workable sizes, and the analysis of network evolution. Despite the importance of the task, there currently exists a gap in our ability to systematically generate networks that adhere to theoretical guarantees for the given property specifications. In this paper, we propose the use of Mixed-Integer Linear Optimization modeling and solution methodologies to address this Network Generation Problem. We present a number of useful modeling techniques and apply them to mathematically express and constrain network properties in the context of an optimization formulation. We then develop complete formulations for the generation of networks that attain specified levels of connectivity, spread, assortativity and robustness, and we illustrate these via a number of computational case studies.	assortativity;compiler;computation;downscaling;encode;integer programming;mathematical optimization;program optimization;programming paradigm	Chrysanthos E. Gounaris;Karthikeyan Rajendran;Ioannis G. Kevrekidis;Christodoulos A. Floudas	2016	Networks	10.1002/net.21699	mathematical optimization;combinatorics;simulation;computer science;theoretical computer science;mathematics;management science;algorithm	ML	27.825321216726092	7.053549405370923	173812
92db0fd1755ad4d3d4edb5f86ac1ed825915a564	exploiting the robustness on power-law networks	probabilistic analysis;robustness;power law networks	Many complex networks are discovered to follow the powerlaw distribution in degree sequence, ranging from the Internet, WWW to social networks. Unfortunately, there exist a great number of threats to these complex systems. In this context, it is crucial to understand the behaviors of power-law networks under various threats. Although power-law networks have been found robust under random failures but vulnerable to intentional attacks by experimental observations, it remains hard to theoretically assess their robustness so as to design a more stable complex network. In this paper, we assess the vulnerability of power-law networks with respect to their global pairwise connectivity, i.e. the number of connected node-pairs, where a pair of nodes are connected when there is a functional path between them. According to our in-depth probabilistic analysis under the theory of random power-law graph model, our results illustrate the best range of exponential factors in which the power-law networks are almost surely unaffected by any random failures and less likely to be destructed under adversarial attacks.	complex network;complex systems;degree (graph theory);existential quantification;internet;moore's law;probabilistic analysis of algorithms;social network;time complexity;www	Yilin Shen;Nam P. Nguyen;My T. Thai	2011		10.1007/978-3-642-22685-4_34	probabilistic analysis of algorithms;degree distribution;evolving networks;computer science;machine learning;data mining;mathematics;distributed computing;complex network;robustness	ML	37.103777068132274	14.024472964784334	173889
7e23abde7cd15506d2cdafc16f6eb45a81c98ac1	a stochastic cellular automata for modeling pedestrian groups distribution	settore inf 01 informatica		stochastic cellular automaton	Lorenza Manenti;Sara Manzoni;Stefania Bandini	2013	J. Cellular Automata		combinatorics;discrete mathematics;computer science;calculus;mathematics	Theory	38.13857738852164	9.43472936927357	174038
ce612c7277e890af86160ae217bd9883884ffa96	a simplex algorithm for minimum-cost network-flow problems in infinite networks	network flow;simplex algorithm;duality	We study minimum-cost network-flow problems in networks with a countably infinite number of nodes and arcs and integral flow data. This problem class contains many nonstationary planning problems over time where no natural finite planning horizon exists. We use an intuitive natural dual problem and show that weak and strong duality hold. Using recent results regarding the structure of basic solutions to infinite-dimensional network-flow problems we extend the well-known finite-dimensional network simplex method to the infinite-dimensional case. In addition, we study a class of infinite network-flow problems whose flow balance constraints are inequalities and show that the simplex method can be implemented in such a way that each pivot takes only a finite amount of time. © 2008 Wiley Periodicals, Inc. NETWORKS, Vol. 52(1), 14–31 2008	duality (optimization);flow network;john d. wiley;network simplex algorithm;strong duality;whole earth 'lectronic link	Thomas C. Sharkey;H. Edwin Romeijn	2008	Networks	10.1002/net.20221	mathematical optimization;combinatorics;calculus;mathematics;simplex algorithm	Theory	25.670573204335454	13.40697991390355	174664
cfc6a9f7ac5cad39ef03a84f5eca307591887a12	prune-and-search with limited workspace	minimum enclosing circle;low dimensional linear programming;read only memory algorithms;space efficient algorithms;prune and search;in place algorithms	Prune-and-search is an excellent algorithmic paradigm for solving various optimization problems. We provide a general scheme for prune-and-search technique and show how to implement it in space-efficient manner. We consider both the in-place and read-only model which have several advantages compared to the traditional model of computation. Our technique can be applied to a large number of problems which accept prune-and-search. For examples, we study the following problems each of which has tremendous practical usage apart from theoretical implication: • computing the minimum enclosing circle (MEC) of a set of n points in R2, and • linear programming problems with two and three variables and n constraints. In the in-place setting, all these problems can be solved in O(n) time using O(1) extra-space. In the read-only setup, the time and extra-space complexities of the proposed algorithms for all these problems are O(n polylog(n)) and O(polylog(n)), respectively.	algorithmic paradigm;in-place algorithm;linear programming;mathematical optimization;model of computation;programming paradigm;prune and search;raman scattering;read-only memory;selection algorithm;serial digital video out;workspace	Minati De;Subhas C. Nandy;Sasanka Roy	2015	J. Comput. Syst. Sci.	10.1016/j.jcss.2014.08.001	mathematical optimization;combinatorics;mathematics;algorithm	Theory	29.21199532830291	15.153462409836544	174686
fc3f5bfc0ba69dbd495a22dd635e803f99b7ab65	a new and efficient algorithm for a class of portfolio selection problems	efficient algorithm;portfolio selection	This paper proposes a new approach and develops an efficient algorithm for solving a class of (simplified) portfolio selection problems. The approach is based on the technique of parametric principal pivoting. The algorithm is particularly suited for problems with special structure and can handle potentially large problems. When specialized to the multiple index model, the algorithm achieves enormous savings in computer storage and computations.	algorithm	Jong-Shi Pang	1980	Operations Research	10.1287/opre.28.3.754	mathematical optimization;computer science;machine learning;mathematics;welfare economics	Theory	31.132292228137047	6.764879227026088	174989
b90f40193992a161ffb3c63b246dd7900599a044	on k-hulls and related problems	ham sandwich;algorithme lovasz;algoritmo busqueda;image processing;geometrie algorithmique;algorithme recherche;procesamiento de imagen;search algorithm;computational geometry;interseccion;traitement image;enveloppe convexe;parametric search;k set;intersection;convex hull	For any set X of points (in any dimension) and any $k = 1,2, \cdots $, we introduce the concept of the k-hull of X. The k-hull is the set of points p such that for any hyperplane containing p there are at least k points of X in each closed half-space determined by the hyperplane.Several computational problems related to k-hulls are studied here, including computing the k-hull and finding a point in the k-hull. Some of our algorithms are of interest in themselves because of the techniques employed; in particular, a “parametric” searching technique is used in a nontrivial way.		Richard Cole;Micha Sharir;Chee-Keng Yap	1987	SIAM J. Comput.	10.1137/0216005	mathematical optimization;combinatorics;image processing;computational geometry;convex hull;intersection;mathematics;geometry;search algorithm	Theory	29.889596337404427	17.092772392200892	175384
33aa8d70844c77f7675d762dc0fad808e672f18c	efficient computation of updated lower expectations for imprecise continuous-time hidden markov chains		We consider the problem of performing inference with imprecise continuous-time hidden Markov chains, that is, imprecise continuous-time Markov chains that are augmented with random output variables whose distribution depends on the hidden state of the chain. The prefix ‘imprecise’ refers to the fact that we do not consider a classical continuous-time Markov chain, but replace it with a robust extension that allows us to represent various types of model uncertainty, using the theory of imprecise probabilities. The inference problem amounts to computing lower expectations of functions on the state-space of the chain, given observations of the output variables. We develop and investigate this problem with very few assumptions on the output variables; in particular, they can be chosen to be either discrete or continuous random variables. Our main result is a polynomial runtime algorithm to compute the lower expectation of functions on the state-space at any given time-point, given a collection of observations of the output variables.	algorithm;computation;discrete mathematics;markov chain;polynomial;state space;time complexity	Thomas E. Krak;Jasper De Bock;Arno Siebes	2017			hidden semi-markov model;markov model	Logic	39.104345503698454	6.136874899413438	175752
b3d5a2fe3fa4d3bfa4a751fd536b9533b12f70eb	on relations between chance constrained and penalty function problems under discrete distributions	discrete distribution;calmness;exact penalization;90c15;penalty functions;chance constraints;asymptotic equivalence	We extend the theory of penalty functions to stochastic programming problems with nonlinear inequality constraints dependent on a random vector with known distribution. We show that the problems with penalty objective, penalty constraints and chance constraints are asymptotically equivalent under discretely distributed random parts. This is a complementary result to Branda (2012a), Branda and Dupačová (2012), and Ermoliev et al. (2000) where the theorems were restricted to continuous distributions only. We propose bounds on optimal values and convergence of optimal solutions. Moreover, we apply exact penalization under modified calmness property to improve the results.	function problem;mathematical optimization;nonlinear system;norm (social);penalty method;social inequality;stochastic optimization;stochastic programming;turing completeness	Martin Branda	2013	Math. Meth. of OR	10.1007/s00186-013-0428-7	probability distribution;mathematical optimization;mathematical analysis;calculus;penalty method;mathematics;statistics	ML	36.454700994387515	4.710108032468386	175958
8f936112e442fdfb1948558a5ef6c8b41f629427	ant colony optimization to minimal test cost reduction	graph theory;ant colony optimisation;ant colony optimization;heuristic programming;set theory;set theory ant colony optimisation graph theory heuristic programming learning artificial intelligence;positive region;classification algorithms;mushroom dataset ant colony optimization minimal test cost attribute reduction cost sensitive learning information gain based heuristic algorithm attribute set graph vertex graph attribute graph edge pheromone artificial ant population;cost sensitive learning;learning artificial intelligence;minimal test cost reduction cost sensitive learning positive region ant colony optimization;minimal test cost reduction	Minimal test cost attribute reduction is an important issue in cost-sensitive learning. Recently, an information gain based heuristic algorithm has been designed to this problem. However, the algorithm does not often find the optimal solution. In this paper, we develop an ant colony optimization algorithm to deal with this problem. First, the attribute set is represented as a graph with each vertex corresponding to an attribute and each edge concerning pheromone. Second, a population of artificial ants are generated. Third, each ant takes the set of core attributes, and travels a number of vertexes according to test costs of vertexes and pheromone of edges, until the positive region condition is met. Finally, a number of reducts are constructed from the last part of the ant colony and the one with least test cost is selected. The algorithm is tested with three representative test cost distributions on four VCI datasets. The experimental results indicate that our algorithm is significantly better than the existing one, especially on the mushroom dataset.	algorithm;ant colony optimization algorithms;artificial ants;heuristic (computer science);information gain in decision trees;kullback–leibler divergence;mathematical optimization;vertex (geometry)	Zilong Xu;Fan Min;Jiabin Liu;William Zhu	2012	2012 IEEE International Conference on Granular Computing	10.1109/GrC.2012.6468671	mathematical optimization;ant colony optimization algorithms;computer science;artificial intelligence;graph theory;machine learning;mathematics;metaheuristic;set theory	EDA	25.178034371615606	5.817650896178084	176206
06aea46f17e2f5b125fc3891ee55e2c8200e6c1c	nonserial dynamic programming and tree decomposition in discrete optimization	discrete optimization;dynamic program;tree decomposition	Solving discrete optimization problems (DOP) can be a rather hard task. Many real DOPs contain a huge number of variables and/or constraints that make the models intractable for currently available solvers. There are few approaches for solving DOPs: tree search approaches (e.g., branch and bound), relaxation and decomposition methods. Large DOPs can be solved due to their special structure. Among decomposition approaches we can mention poorly known local decomposition algorithms using the special block matrix structure of constraints and half-forgotten nonserial dynamic programming algorithms which can exploit sparsity in the dependency graph of a DOP. One of the promising approaches to cope with NP-hardness in solving DOPs is the construction of decomposition methods [7]. Decomposition techniques usually determine subproblems, whose solutions can be combined to create a solution of the initial DOP problem. Usually, DOPs from applications have a special structure, and the matrices of constraints for large-scale problems have a lot of zero elements (sparse matrices). This paper reviews main results for local decomposition algorithms in discrete programming and establishes some links between them, tree decompositions and nonserial dynamic programming.	algorithm;branch and bound;discrete optimization;dynamic programming;linear programming relaxation;mathematical optimization;np-hardness;optimization problem;program optimization;tree decomposition	Oleg Shcherbina	2006		10.1007/978-3-540-69995-8_26	discrete optimization;mathematical optimization;combinatorial optimization;mathematics;tree decomposition	AI	25.715481240483367	8.165048735842637	176817
33d2567414df6c4803b8dc61223cb2af7f7838a0	an exact solution method for quadratic matching: the one-quadratic-term technique and generalisations	polyhedral combinatorics;quadratic matching problem;reformulation linearisation technique rlt;binary quadratic optimisation problem	The quadratic matching problem (QMP) asks for a matching in a graph that optimises a quadratic objective in the edge variables. The QMP generalises the quadratic assignment problem as the latter can be seen as a perfect matching on a bipartite graph. In our approach, we strengthen the linearised IP-formulation by cutting planes that are derived from facets of the corresponding matching problem where only one quadratic term is present in the objective function (QMP1). As the influence of these cutting planes on the root bound decreases for instances with more quadratic terms in the objective, we present different methods to strengthen these cutting planes. Adopting the idea of the reformulation linearisation technique (RLT) we derive valid inequalities for the general QMP from QMP1 facets. Following the approach in Fomeni et al. [1] we replace cubic terms that appear in a reformulated QMP1 inequality by a quadratic estimator. Based on these methods we design and implement an exact branch-and-cut approach. When compared to solving the standard linearised IP formulation strengthened by cutting planes that result from the application of the RLT to the degree inequalities, we show that root bounds and cpu times for solving instances to optimality can significantly be improved, when the new method is applied.	branch and cut;cplex;central processing unit;computation;cubic function;heuristic (computer science);loss function;matching (graph theory);optimization problem;polyhedron;quadratic assignment problem;social inequality;technical support	Lena Hupp;Laura Klein;Frauke Liers	2015	Discrete Optimization	10.1016/j.disopt.2015.10.002	mathematical optimization;combinatorics;discrete mathematics;quadratic residuosity problem;polyhedral combinatorics;quadratic function;isotropic quadratic form;binary quadratic form;mathematics;quadratic programming	ML	25.544937409648877	15.173734120335421	176953
83d6db66ff3f9f0faf005f3f660c9c07496d8833	on solving boolean multilevel optimization problems	max sat;combinatorial optimization problem;satisfiability;artificial intelligent;optimization problem;boolean multilevel optimization;pseudo boolean optimization;logic in computer science;pseudo boolean	Many combinatorial optimization problems entail a number of hierarchically dependent optimization problems. An often used solution is to associate a suitably large cost with each individual optimization problem, such that the solution of the resulting aggregated optimization problem solves the original set of optimization problems. This paper starts by studying the package upgradeability problem in software distributions. Straightforward solutions based on Maximum Satisfiability (MaxSAT) and pseudo-Boolean (PB) optimization are shown to be ineffective, and unlikely to scale for large problem instances. Afterwards, the package upgradeability problem is related to multilevel optimization. The paper then develops new algorithms for Boolean Multilevel Optimization (BMO) and highlights a number of potential applications. The experimental results indicate that algorithms for BMO allow solving optimization problems that existing MaxSAT and PB solvers would otherwise be unable to solve.	algorithm;boolean satisfiability problem;combinatorial optimization;computation;computational problem;mathematical optimization;optimization problem;package manager;program optimization;software system;word lists by frequency	Josep Argelich;Inês Lynce;Joao Marques-Silva	2009	CoRR		probabilistic-based design optimization;discrete optimization;optimization problem;mathematical optimization;multi-swarm optimization;combinatorics;test functions for optimization;combinatorial optimization;computer science;derivative-free optimization;artificial intelligence;maximum satisfiability problem;stochastic optimization;mathematics;continuous optimization;vector optimization;l-reduction;bilevel optimization;algorithm;random optimization;3-opt;metaheuristic;global optimization;quadratic assignment problem;satisfiability	AI	26.929906889172095	6.109714360334612	177060
50a0ec59b3aa372ce1820b10faeae626c4af079f	analytic solutions for three-taxon mlmc trees with variable rates across sites	maximum likelihood;variable rate;moment generating function;inverse gaussian distribution;molecular clock;phylogenetic tree;rooted tree;analytic solution	We consider the problem of finding the maximum likelihood rooted tree under a molecular clock (MLMC), with three species and 2-state characters under a symmetric model of substitution. For identically distributed rates per site this is probably the simplest phylogenetic estimation problem, and it is readily solved numerically. Analytic solutions, on the other hand, were obtained only recently (Yang, 2000).#R##N##R##N#In this work we provide analytic solutions for any distribution of rates across sites (provided the moment generating function of the distribution is strictly increasing over the negative real numbers). This class of distributions includes, among others, identical rates across sites, as well as the Gamma, the uniform, and the inverse Gaussian distributions. Therefore, our work generalizes Yang's solution. In addition, our derivation of the analytic solution is substantially simpler. We employ the Hadamard conjugation (Hendy and Penny, 1993) and convexity of an entropy-like function.		Benny Chor;Michael D. Hendy;David Penny	2001		10.1007/3-540-44696-6_16	molecular clock;biology;closed-form expression;mathematical optimization;combinatorics;discrete mathematics;phylogenetic tree;mathematics;maximum likelihood;inverse gaussian distribution;moment-generating function;statistics	ML	38.58349623951181	11.891847943037163	177125
fff50ab881e9ca52d26e4b6eec222f4b7429086b	generalization error analysis for polynomial kernel methods - algebraic geometrical approach	metodo polinomial;generalization error;methode noyau;kernel function;intelligence artificielle;feature space;geometric approach;algebraic geometry;polynomial method;metodo nucleo;artificial intelligence;kernel method;geometria algebraica;inteligencia artificial;methode polynomiale;geometrie algebrique	The generalization properties of learning classifiers with a polynomial kernel function are examined here. We first show that the generalization error of the learning machine depends on the properties of the separating curve, that is, the intersection of the input surface and the true separating hyperplane in the feature space. When the input space is one-dimensional, the problem is decomposed to as many one-dimensional problems as the number of the intersecting points. Otherwise, the generalization error is determined by the class of the separating curve. Next, we consider how the class of the separating curve depends on the true separating function. The class is maximum when the true separating polynomial function is irreducible and smaller otherwise. In either case, the class depends only on the true function and does not on the dimension of the feature space. The results imply that the generalization error does not increase even when the dimension of the feature space gets larger and that the so-called overmodeling does not occur in the kernel learning.	generalization error;kernel method;polynomial kernel	Kazushi Ikeda	2003		10.1007/3-540-44989-2_25	kernel;kernel method;combinatorics;discrete mathematics;feature vector;algebraic geometry;computer science;artificial intelligence;machine learning;mathematics;geometry;algorithm;statistics;generalization error	ML	33.874895598593	13.508863108835575	177584
b8a1223d47ef4ab738b03d418d51e8f095859f1a	traditional pagerank versus network capacity bound		In a former paper [9] we simplified the proof of a theorem on personalized random walk that is fundamental to graph nodes clustering and generalized it to bipartite graphs for a specific case where the proobability of random jump was proprtional to the number of links of ”personally prefereed” nodes. In this paper we turn to the more complex issue of graphs in which the random jump follows uniform distribution.	cluster analysis;pagerank;personalization	Mieczyslaw A. Klopotek;Slawomir T. Wierzchon;Robert A. Klopotek;Elzbieta A. Klopotek	2017	CoRR		random walk;data mining;computer science;pagerank;cluster analysis;bipartite graph;graph;uniform distribution (continuous);jump	Theory	36.8771983501579	15.557685239953852	177899
50abe3f2094a16df4281d62f8ca0c132ee43c8e6	on the convergence of temporal-difference learning with linear function approximation	almost sure convergence;neuro dynamic programming;positive harris recurrence;reinforcement learning;linear functionals;upper bound;state space;asymptotic properties;asymptotic approximation;temporal difference learning;markov chains;markov chain	The asymptotic properties of temporal-difference learning algorithms with linear function approximation are analyzed in this paper. The analysis is carried out in the context of the approximation of a discounted cost-to-go function associated with an uncontrolled Markov chain with an uncountable finite-dimensional state-space. Under mild conditions, the almost sure convergence of temporal-difference learning algorithms with linear function approximation is established and an upper bound for their asymptotic approximation error is determined. The obtained results are a generalization and extension of the existing results related to the asymptotic behavior of temporal-difference learning. Moreover, they cover cases to which the existing results cannot be applied, while the adopted assumptions seem to be the weakest possible under which the almost sure convergence of temporal-difference learning algorithms is still possible to be demonstrated.	algorithm;approximation error;convex function;ergodicity;harris affine region detector;linear function;machine learning;markov chain;queueing theory;state space;temporal difference learning;uncontrolled format string	Vladislav Tadic	2001	Machine Learning	10.1023/A:1007609817671	temporal difference learning;markov chain;mathematical optimization;combinatorics;discrete mathematics;computer science;machine learning;mathematics;reinforcement learning;statistics	ML	38.156462020488206	5.184462485969841	178214
0bdb1f450fa1d69ca5d338b4a0e94177cc51eacc	determining type ii sensitivity ranges of the fractional assignment problem	fonction rationnelle;assignment problem;programacion fraccionaria;analisis sensibilidad;systeme degenere;optimisation;degeneracy;probleme affectation;optimizacion;labeling algorithm;programmation fractionnaire;fractional programming;degenerate system;sistema degenerado;programacion lineal;degeneration;sensitivity analysis;linear programming;programmation lineaire;analyse sensibilite;linear program;problema asignacion;optimization;funcion racional;rational function	This paper proposes iterative labeling algorithms to determine the Type II sensitivity ranges of the fractional assignment problem. Unlike the traditional sensitivity range which keeps the current optimal basis remaining optimal, the Type II sensitivity range is the range that keeps the current optimal assignment remaining optimal. Focusing only on the non-degenerate basic variables makes the Type II sensitivity range more practical. Three cases of perturbation, each with two kinds, are discussed. An example is presented to demonstrate the proposed algorithms.	assignment problem	Chi-Jen Lin	2011	Oper. Res. Lett.	10.1016/j.orl.2010.10.002	rational function;mathematical optimization;linear programming;calculus;mathematics;assignment problem;sensitivity analysis;degeneracy	Logic	26.39840875718849	11.561838440169268	178313
386e71eba94909962d6817a912814fe60d5cfdd0	efficient regret minimization in non-convex games		We consider regret minimization in repeated games with non-convex loss functions. Minimizing the standard notion of regret is computationally intractable. Thus, we define a natural notion of regret which permits efficient optimization and generalizes offline guarantees for convergence to an approximate local optimum. We give gradient-based methods that achieve optimal regret, which in turn guarantee convergence to equilibrium in this framework.	approximation algorithm;computational complexity theory;gradient;local optimum;loss function;mathematical optimization;online and offline;regret (decision theory)	Elad Hazan;Karan Singh;Cyril Zhang	2017			mathematics;local optimum;mathematical optimization;mathematical economics;regular polygon;minification;regret;repeated game;convergence (routing)	ML	33.00503452355812	5.679290594875756	178601
8b6dbbf67853c45f0cfffe323bcc1f57f8124fab	between a rock and a hard place: the two-to-one assignment problem	assignment problem;matching problem;approximate algorithm;transportation problem;efficient algorithm;triangle inequality;satisfiability;three dimensional;approximation;indexation;polynomial time approximation scheme	We describe the two-to-one assignment problem, a problem in between the axial three-index assignment problem and the three-dimensional matching problem, having applications in various domains. For the (relevant) case of decomposable costs satisfying the triangle inequality we provide, on the positive side, two constant factor approximation algorithms. These algorithms involve solving minimum weight matching problems and transportation problems, leading to a 2-approximation, and a $\frac32$-approximation. Moreover, we further show that the best of these two solutions is a $\frac43$-approximation for our problem. On the negative side, we show that the existence of a polynomial time approximation scheme for our problem would imply P=NP.	assignment problem	Dries R. Goossens;Sergey Polyakovskiy;Frits C. R. Spieksma;Gerhard J. Woeginger	2009		10.1007/978-3-642-12450-1_15	computational problem;optimization problem;transportation theory;three-dimensional space;mathematical optimization;partition problem;combinatorics;discrete mathematics;function problem;linear bottleneck assignment problem;polynomial-time approximation scheme;vertex cover;multi-commodity flow problem;generalized assignment problem;maximum satisfiability problem;cutting stock problem;approximation;p versus np problem;triangle inequality;mathematics;assignment problem;weapon target assignment problem;stable roommates problem;approximation algorithm;quadratic assignment problem;satisfiability	Crypto	25.146324082706187	15.795255276250598	179003
3e7ec94a1323f8193f6b3afdc575accadd007039	approximating smallest enclosing balls with applications to machine learning	smallest euclidean enclosing ball;machine learning;coresets;duality piercing covering	In this paper, we first survey prior work for computing exactly or approximately the smallest enclosing balls of point or ball sets in Euclidean spaces. We classify previous work into three categories: (1) purely combinatorial, (2) purely numerical, and (3) recent mixed hybrid algorithms based on coresets. We then describe two novel tailored algorithms for computing arbitrary close approximations of the smallest enclosing Euclidean ball of balls. These deterministic heuristics are based on solving relaxed decision problems using a primal-dual method. The primal-dual method is interpreted geometrically as solving for a minimum covering set, or dually as seeking for a minimum piercing set. Finally, we present some applications in machine learning of the exact and approximate smallest enclosing ball procedure, and discuss about its extension to non-Euclidean informationtheoretic spaces.	approximation algorithm;bounding sphere;convex set;coreset;decision problem;heuristic (computer science);hilbert space;machine learning;numerical analysis	Frank Nielsen;Richard Nock	2009	Int. J. Comput. Geometry Appl.	10.1142/S0218195909003039	mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	26.3180470255769	15.166520252382501	179178
38a5c472540785f3ad5b6e487922f252fa49774c	discriminating membrane proteins using the joint distribution of length sums of success and failure runs	bivariate markov embedded random variables of polynomial type;markov chain embeddable random variables;scans;runs;patterns;proteins analysis	Discriminating integral membrane proteins from water-soluble ones, has been over the past decades an important goal for computational molecular biology. A major drawback of methods appeared in the literature, is that most of the authors tried to solve the problem using machine learning techniques. Specifically, most of the proposed methods require an appropriate dataset for training, and consequently the results depend heavily on the suitability of the dataset, itself. Motivated by these facts, in this paper we develop a formal discrimination procedure that is based on appropriate theoretical observations on the sequence of hydrophobic and polar residues along the protein sequence and on the exact distribution of a two dimensional runs-related statistic defined on the same sequence. Specifically, for setting up our discrimination procedure, we study thoroughly the exact distribution of a bivariate random variable, which accumulates the exact lengths of both success and failure runs of at least a specific length in a sequence of Bernoulli trials. To investigate the properties of this bivariate random variable, we use the Markov chain embedding technique. Finally, we apply the new procedure to a well-defined dataset of proteins. B Sotirios Bersimis sbersim@unipi.gr Athanasios Sachlas asachlas@unipi.gr Pantelis G. Bagos pbagos@compgen.org 1 Department of Statistics and Insurance Science, University of Piraeus, 80 Karaoli and Dimitriou str., 185 34 Piraeus, Greece 2 Department of Computer Science and Biomedical Informatics, University of Thessaly, Papasiopoulou 2–4, Galaneika, 35100 Lamia, Greece	bernoulli polynomials;bivariate data;computation;computer science;informatics;machine learning;markov chain;polynomial	Sotirios Bersimis;Athanasios Sachlas;Pantelis G. Bagos	2017	Statistical Methods and Applications	10.1007/s10260-016-0370-y	econometrics;combinatorics;mathematics;geometry;pattern;statistics;variable-order markov model	ML	35.893757576622775	8.753303809832392	179471
34d389063e0b28f53eb06b8212778d69509d5bca	computable approximations for average markov decision processes in continuous time			approximation;computable function;markov chain;markov decision process	Jonatha Anselmi;François Dufour;Tomás Prieto-Rumeau	2018	J. Applied Probability	10.1017/jpr.2018.36	mathematics;combinatorics;markov decision process	Theory	38.223849579787625	5.017490812036777	179563
242fbde0834de8898e59a2562abf7005d651df82	solving multilabel graph cut problems with multilabel swap	de noising;graph theory;memory requirements;memory management;stereo image processing graph theory image denoising;image segmentation graph cuts;labeling computer vision probes stereo vision markov random fields iterative algorithms digital images computer applications australia noise reduction;approximation algorithms;probability density function;triangle inequality;keywords approximate solution;exact solution;markov random fields;energy functions;satisfiability;data mining;multi label;probes;markov random field;energy function;conference paper;approximate solution;graph cut;pixel;stereo image processing;labelling;triangle inequality multilabel graph cut problems multilabel swap binary graph cuts expansion denoising problem stereo problem s swap algorithm binary energy functions;optimization;image denoising;graph cuts;graph construction;labeling;markov random field graph cuts optimization labelling	Approximate solutions to labelling problems can be found using binary graph cuts and either the alpha-expansion or alpha-beta swap algorithms. In some specific cases, an exact solution can be computed by constructing a multilabel graph. However, in many practical applications the multilabel graph construction is infeasible due to its excessively large memory requirements. In this work, we expand the concept of alpha-beta swap to consider larger sets of labels at each iteration, and demonstrate how this approach is able to produce good approximate solutions to problems which can be solved using multilabel graph cuts. Furthermore, we show how alpha-expansion is a special case of multilabel swap, and from this new formulation, illustrate how alpha-expansion is now able to handle binary energy functions which do not satisfy the triangle inequality. Compared to alpha-beta swap, multilabel swap is able to produce an approximate solution in a shorter amount of time. We demonstrate the merits of our approach by considering the denoising and stereo problems. We illustrate how multilabel swap can be used in a recursive fashion to produce a good solution quickly and without requiring excessive amounts of memory.	approximation algorithm;concave function;convex optimization;cut (graph theory);feasible region;iteration;mathematical optimization;maxima and minima;noise reduction;paging;recursion;requirement;social inequality;submodular set function;the australian	Peter Carr;Richard I. Hartley	2009	2009 Digital Image Computing: Techniques and Applications	10.1109/DICTA.2009.90	mathematical optimization;combinatorics;cut;graph theory;machine learning;mathematics	AI	33.66026409424244	6.812635261420893	179587
97be35458529cbc62ae1ced3f108ea9fc67f7fdd	linear dependence of stationary distributions in ergodic markov decision processes	chaine markov;cadena markov;ergodicite;proceso markov;primary 90c40;decision markov;condicion estacionaria;condition stationnaire;secondary 60j20;stationary distribution;processus markov;markov process;stationary condition;ergodicidad;markov decision;60j10;markov decision process;ergodicity;markov chain	In ergodic MDPs we consider stationary distributions of policies that coincide in all but n states, in which one of two possible actions is chosen. We give conditions and formulas for linear dependence of the stationary distributions of n+2 such policies, and show some results about combinations and mixtures of policies.	distribution (mathematics);ergodicity;markov chain;markov decision process;stationary process	Ronald Ortner	2007	Oper. Res. Lett.	10.1016/j.orl.2006.12.001	markov decision process;econometrics;markov chain;combinatorics;stationary distribution;ergodicity;stationary ergodic process;mathematics;markov process;statistics	ML	38.418448566524575	5.938129665501335	179645
039ff3c8824a05651994c33bab29edeff15ed52b	an expander-based approach to geometric optimization	geometric optimization;expander graph	We present a new approach to problems in geometric optimization that are traditionally solved using the parametric searching technique of Megiddo. Our new approach is based on expander graphs and is conceptually much simpler and has more explicit geometric flavor. It does not require parallelization or randomization, and it exploits recent range-searching techniques of Matousˇek and others. We exemplify the technique on three problems, the <italic>slope selection</italic> problem, the planar <italic>distance selection</italic> problem, and the planar <italic>two-center</italic> problem. For the first problem we develop an <italic>O(n log<supscrpt>3</supscrpt>n)</italic>) solution, which, although suboptimal, is very simple. The second and third problems are more typical  examples of our approach. Our solutions have, respectively, running time <italic>O(n<supscrpt>4/3</supscrpt> log<supscrpt>3+δ</supscrpt> n)</italic>, for any δ > 0, and <italic>O(n<supscrpt>2</supscrpt> log<supscrpt>3</supscrpt> n)</italic>, comparable with the respective solutions of [2, 5].	exemplification;mathematical optimization;parallel computing;randomized algorithm;range searching;time complexity	Matthew J. Katz;Micha Sharir	1993		10.1145/160985.161137	mathematical optimization;combinatorics;expander graph;mathematics;geometry	Theory	29.634628828583068	15.086727437284154	180235
210fdc622d82b14b1333b51be37670c8daeadbbb	stochastic kronecker graphs	giant component;random graph model;generic model;heavy tail;degree distribution;large scale;phase transition;power law;kronecker product	A random graph model based on Kronecker products of probability matrices has been recently proposed as a generative model for large-scale real-world networks such as the web. This model simultaneously captures several well-known properties of real-world networks; in particular, it gives rise to a heavy-tailed degree distribution, has a low diameter, and obeys the densification power law. Most properties of Kronecker products of graphs (such as connectivity and diameter) are only rigorously analyzed in the deterministic case. In this paper, we study the basic properties of stochastic Kronecker products based on an initiator matrix of size two (which is the case that is shown to provide the best fit to many real-world networks). We will show a phase transition for the emergence of the giant component and another phase transition for connectivity, and prove that such graphs have constant diameters beyond the connectivity threshold, but are not searchable using a decentralized algorithm.	algorithm;curve fitting;degree distribution;emergence;emoticon;generative model;giant component;random graph;scsi initiator and target;the matrix;with high probability	Mohammad Mahdian;Ying Xu	2007	Random Struct. Algorithms	10.1007/978-3-540-77004-6_14	phase transition;power law;combinatorics;discrete mathematics;degree distribution;heavy-tailed distribution;theoretical computer science;mathematics;kronecker product;matrix addition;giant component;statistics	ML	37.39537564184229	14.25628064388903	180343
cea84a2bed651f9abccba2659149e0f2ab8198c2	size-dependent tile self-assembly: constant-height rectangles and stability		We introduce a new model of algorithmic tile self-assembly called size-dependent assembly. In previous models, supertiles are stable when the total strength of the bonds between any two halves exceeds some constant temperature. In this model, this constant temperature requirement is replaced by an nondecreasing temperature function τ : N→ N that depends on the size of the smaller of the two halves. This generalization allows supertiles to become unstable and break apart, and captures the increased forces that large structures may place on the bonds holding them together. We demonstrate the power of this model in two ways. First, we give fixed tile sets that assemble constant-height rectangles and squares of arbitrary input size given an appropriate temperature function. Second, we prove that deciding whether a supertile is stable is coNPcomplete. Both results contrast with known results for fixed temperature.	control theory;information;requirement;self-assembly	Sándor P. Fekete;Robert T. Schweller;Andrew Winslow	2015		10.1007/978-3-662-48971-0_26	combinatorics;discrete mathematics;mathematics	Theory	35.521516890231055	17.43517797087928	180380
891de76027034d1eed9359afe7cfdb74c510b050	three-dimensional constrained delaunay triangulation: a minimalist approach	computat ional geometry.;three-dimensional mesh generation;constrained triangulations;delaunay triangulation;industrial strength triangulations;tetrahedralization;mesh generation;three dimensional;constrained delaunay triangulation	In this paper we summarize our experiences with 3D constrained Delaunay triangulation algorithms for industrial applications. In addition, we report a robust implementat ion process for constructing 3D constrained triangulations from initial unconstrained triangulations, based on a minimalist approach, in which we minimize the use of geometrical operations such as intersections. This is achieved by inserting Steiner points on missing constraining edges and faces in the initial unconstrai ed triangulations. This approach allowed the generation of tetrahedral meshes for arbitrarily complex 3D domains.	algorithm;constrained delaunay triangulation;minimalism (computing);steiner tree problem	Paulo Roma Cavalcanti;Ulisses T. Mello	1999			pitteway triangulation;minimum-weight triangulation;bowyer–watson algorithm;mathematical optimization;constrained delaunay triangulation;delaunay triangulation;chew's second algorithm;ruppert's algorithm;surface triangulation;mathematics	Vision	33.15971177542798	16.89336045816077	180684
2929e4fcaf8bfaecedcb9fdc56fee17b4b375626	family of constrained codes for archival dna data storage		DNA-based data storage systems have evolved as a solution to accommodate data explosion. In this letter, some properties of DNA codewords that are essential for an archival DNA storage are considered for the design of codes. Constraint-based DNA codes, which avoid runs of nucleotides, have fixed GC-weight, and a specific minimum distance is presented. An altruistic algorithm that enumerates DNA codewords with the above constraints is provided. A theoretical bound on such DNA codewords is obtained. This bound is tight when there is no minimum distance constraint.	algorithm;code word;computer data storage;constraint logic programming	Dixita Limbachiya;Manish K. Gupta;Vaneet Aggarwal	2018	IEEE Communications Letters	10.1109/LCOMM.2018.2861867	dna digital data storage;real-time computing;dna;computer science;distributed computing;computer data storage	Theory	36.54379308909096	10.307547825190564	180825
0e3958270249653193e33a0b562d53e11a757e03	stochastic dual ascent for solving linear systems		We develop a new randomized iterative algorithm—stochastic dual ascent (SDA)—for finding the projection of a given vector onto the solution space of a linear system. The method is dual in nature: with the dual being a non-strongly concave quadratic maximization problem without constraints. In each iteration of SDA, a dual variable is updated by a carefully chosen point in a subspace spanned by the columns of a random matrix drawn independently from a fixed distribution. The distribution plays the role of a parameter of the method. Our complexity results hold for a wide family of distributions of random matrices, which opens the possibility to fine-tune the stochasticity of the method to particular applications. We prove that primal iterates associated with the dual process converge to the projection exponentially fast in expectation, and give a formula and an insightful lower bound for the convergence rate. We also prove that the same rate applies to dual function values, primal function values and the duality gap. Unlike traditional iterative methods, SDA converges under no additional assumptions on the system (e.g., rank, diagonal dominance) beyond consistency. In fact, our lower bound improves as the rank of the system matrix drops. Many existing randomized methods for linear systems arise as special cases of SDA, including randomized Kaczmarz, randomized Newton, randomized coordinate descent, Gaussian descent, and their variants. In special cases where our method specializes to a known algorithm, we either recover the best known rates, or improve upon them. Finally, we show that the framework can be applied to the distributed average consensus problem to obtain an array of new algorithms. The randomized gossip algorithm arises as a special case.	column (database);concave function;consensus (computer science);converge;coordinate descent;diagonally dominant matrix;duality (optimization);duality gap;expectation–maximization algorithm;feasible region;iteration;iterative method;linear system;newton;rate of convergence;times ascent	Robert Mansel Gower;Peter Richtárik	2015	CoRR		mathematical optimization;combinatorics;mathematical analysis;discrete mathematics;mathematics;algorithm;statistics;algebra	ML	34.64059310714146	6.057600865810626	180863
1166aefacca50efa96df68cdff87fef1444bd4e2	convex hulls under uncertainty	uncertainty;tukey depth;membership probability;convex hull	We study the convex-hull problem in a probabilistic setting, motivated by the need to handle data uncertainty inherent in many applications, including sensor databases, location-based services and computer vision. In our framework, the uncertainty of each input point is described by a probability distribution over a finite number of possible locations including a null location to account for non-existence of the point. Our results include both exact and approximation algorithms for computing the probability of a query point lying inside the convex hull of the input, time–space tradeoffs for the membership queries, a connection between Tukey depth and membership queries, as well as a new notion of $$\beta $$ β -hull that may be a useful representation of uncertain hulls.	approximation algorithm;computer vision;convex hull;data structure;database;fink;location-based service;subdivision surface	Pankaj K. Agarwal;Sariel Har-Peled;Subhash Suri;Hakan Yildiz;Wuzhou Zhang	2016	Algorithmica	10.1007/s00453-016-0195-y	mathematical optimization;combinatorics;discrete mathematics;uncertainty;convex hull;mathematics;geometry	DB	27.97954819256252	15.289092270407899	181111
b37558f060c384d20035cf1f359ae578f3581b7b	linear transformations to decrease computational requirements of solving some known linear programming models	degeneracy;duality;weights restrictions;dea;computational complexity;linear programming;linear transformation;linear program;data envelope analysis	Imposing additional weight restrictions increases the degeneracy and computational complexity of solving Data Envelopment Analysis (DEA) models, a known class of linear programming models. In this paper some linear transformations to reduce these problems are provided.	linear programming;requirement	Majid Soleimani-Damaneh;M. Zarepisheh	2009	Annals OR	10.1007/s10479-008-0473-4	system of linear equations;mathematical optimization;combinatorics;discrete mathematics;duality;linear-fractional programming;linear programming;data envelopment analysis;mathematics;linear map;computational complexity theory;degeneracy	Theory	25.37244687505832	13.123126874218707	181617
8c96f177162c3a5c96a8bacf555dbc381b677b09	a distance-based point-reassignment heuristic for the k-hyperplane clustering problem	nonlinear programming;data mining;heuristics	We consider the k-Hyperplane Clustering problem where, given a set of m points in Rn, we have to partition the set into k subsets (clusters) and determine a hyperplane for each of them, so as to minimize the sum of the squares of the Euclidean distances between the points and the hyperplane of the corresponding clusters. We give a nonconvex mixed-integer quadratically constrained quadratic programming formulation for the problem. Since even very small-size instances are challenging for state-of-the-art spatial branch-and-bound solvers like Couenne, we propose a heuristic in which many “critical” points are reassigned at each iteration. Such points, which are likely to be ill-assigned in the current solution, are identified using a distance-based criterion and their number is progressively decreased to zero. Our algorithm outperforms the best available one proposed by Bradley and Mangasarian on a set of real-world and structured randomly generated instances. For the largest instances, we obtain an average improvement in the solution quality of 54%.	cluster analysis;heuristic	Edoardo Amaldi;Stefano Coniglio	2013	European Journal of Operational Research	10.1016/j.ejor.2012.09.026	mathematical optimization;combinatorics;discrete mathematics;nonlinear programming;heuristics;mathematics	Theory	25.117267919469615	7.243149484983438	181669
4c0c5777f405f36c78ed8f04eb31f352444e171d	sampling graphs from a probabilistic generative model	graph theory;probability;sampling procedure sampling graphs probabilistic generative model sampling method graph node graph edge bernoulli distributions node occurrence probability edge occurrence probability;mathematical model computational modeling erbium eigenvalues and eigenfunctions barium probabilistic logic data models;sampling methods;sampling methods graph theory probability	In this paper we present a method of sampling from a probabilistic generative model for a set of graphs. Our method is based on the assumption that the nodes and edges of graphs arise under independent Bernoulli distributions. We sample graphs from the generative model according to the node and edge occurrence probabilities. We explain the construction of our generative model and then compute the node and edge occurrence probabilities which allow us to formulate a sampling procedure. We demonstrate experimentally to what extent the graphs sampled by our method reproduce the salient properties of the graphs in the original training sample.	bernoulli polynomials;experiment;generative model;sampling (signal processing)	Lin Han;Richard C. Wilson;Edwin R. Hancock;Lu Bai;Peng Ren	2012	Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)		1-planar graph;random graph;sampling;combinatorics;discrete mathematics;independent set;graph theory;pattern recognition;probability;mathematics;maximal independent set;generative model;indifference graph;statistics	Vision	38.65362040908881	15.036887921714593	182005
4cbfeab6fbc583352a225009f76c3f34a583bd9c	segmentation d'images par l'algorithme des flot maximum continu. (on continuous maximum flow image segmentation algorithm)		In recent years, with the advance of computing equipment and image acquisition techniques, the sizes, dimensions and content of acquired images have increased considerably. Unfortunately as time passes, there is a steadily increasing gap between the classical and parallel programming paradigms and their actual performance on modern computer hardware. In this thesis we consider in depth one particular algorithm, the continuous maximum flow computation. We review in detail why this algorithm is useful and interesting, and we propose efficient and portable implementations on various architectures. We also examine how it performs in the terms of segmentation quality on some recent problems of materials science and nano-scale biology.	algorithm;computation;computer hardware;gnu nano;image segmentation;maximum flow problem;parallel computing;programming paradigm	Laszlo Marak	2012				Vision	33.88067211653549	10.783208838921071	182376
0fe2ad03330f9f676319b7a7f947fdbbc23d6af0	amorphous packings of hard spheres in large space dimension	disordered systems theory;source and channel coding;glass transition;hard sphere;structural glasses theory;equation of state;free energy;cavity and replica method	In a recent paper we derived an expression for the replicated free energy of a liquid of hard spheres based on the HNC free energy functional. An approximate equation of state for the glass and an estimate of the random close packing density were obtained in d = 3. Here we show that the HNC approximation is not needed: the same expression can be obtained from the full diagrammatic expansion of the replicated free energy. Then, we consider the asymptotics of this expression when the space dimension d is very large. In this limit, the entropy of the hard sphere liquid has been computed exactly. Using this solution, we derive asymptotic expressions for the glass transition density and for the random close packing density for hard spheres in large space dimension.	approximation algorithm;diagram;markov chain;set packing	Giorgio Parisi;Francesco Zamponi	2006	CoRR	10.1088/1742-5468/2006/03/P03017	classical mechanics;combinatorics;glass transition;hard spheres;mathematics;geometry;equation of state;thermodynamics;physics	Theory	39.13024758601579	13.786439165731624	182967
a962f6e5f3ada48b5164411f13d7683dc2e39d37	solving stochastic dynamic programming problems by linear programming - an annotated bibliography	dynamic program;value iteration;linear program;decision process;stochastic dynamics;stochastic dynamic programming	As has been known for a long time, stochastic dynamic decision processes with finite state and action spaces can be handled by policy and value iteration, both typical dynamic programming techniques, as well as by linear programming. In the present paper, the most important results concerning the latter are reported and an outlook on more general settings is given.	dynamic programming;linear programming;stochastic programming	Wolf-Rüdiger Heilmann	1978	Zeitschr. für OR	10.1007/BF01917643	markov decision process;stochastic programming;mathematical optimization;continuous-time stochastic process;linear-fractional programming;reactive programming;computer science;linear programming;theoretical computer science;goal programming;mathematics;procedural programming;inductive programming;algorithm	Theory	37.36483823107402	4.738622865017732	183089
d694baa1589eaaa2b0cab9f7a48b0be15acd78ee	on the mean residual life function of coherent systems	coherent structures;order statistic;increasing failure rate;reliability theory;failure analysis;pareto distribution;failure rate;series parallel	We consider a coherent structure consisting of n components having the property that if it is known that at most r components (r < n) have failed, the system is still operating with probability 1. Some examples of the systems having this property are (n - k + 1)-out-of- n, some parallel-series, and some series-parallel structures. Depending on the structure, and the number of active components of the coherent systems at time t , the mean residual life function of the system is studied, by several authors. This paper investigates more properties of the mean residual life function of the coherent systems sharing the described property. We will show that, when the components of the system have increasing failure rate, the mean residual life function of the system is decreasing in time. Several examples, and illustrative graphs are also provided.	c date and time functions;coherent;failure rate;graph (discrete mathematics);series-parallel graph	Majid Asadi;S. Goliforushani	2008	IEEE Transactions on Reliability	10.1109/TR.2008.2007161	reliability engineering;failure analysis;lagrangian coherent structures;series and parallel circuits;order statistic;reliability theory;engineering;pareto distribution;failure rate;mathematics;statistics	Embedded	36.84303528748605	11.189933541335531	183544
31f815bdc009fbcaf0a181857013857786cbc43c	stability and interaction in flatline games	juego repetido;population model;population finie;evolutionary model;game theory;basin of attraction;teoria juego;theorie jeu;matriz simetrica;finite population;stationary state;jeu 2 personnes;symmetric matrix;jeu evolutionniste;repeated game;2 2;evolutionary game;juego 2 personas;evolutionary games;2 2 symmetric games;jeu repete;automate cellulaire;two person game;matrice symetrique;local interaction;cellular automata;game playing;cellular automaton;poblacion finita;automata celular	Starting from a given one-shot game played by a finite population of agents living in flatline, a circular or constrained grid structured by the classical definitions of neighborhood, we define transformation rules for cellular automata, which are determined by the best-reply behavior in standard two-person symmetric matrix games. Ameaningful concept of solution for the underlying population games will necessarily include robustness against any possible unilateral deviation undertaken by a single player. By excluding the invisible hand of mutation we obtain a purely deterministic population model. The resulting process of cellular transformation is then analyzed for chicken and stag-hunt type cellular games and finally compared with the outcomes of more prominent evolutionary models. Special emphasis is given to an exhaustive combinatorial description of the different basins of attraction corresponding to stable stationary states. ! 2004 Elsevier Ltd. All rights reserved.	automata theory;cellular automaton;chicken;evolutionary algorithm;mobile game;population model;selection algorithm;stationary process;stationary state	Alexander Mehlmann	2006	Computers & OR	10.1016/j.cor.2004.06.018	cellular automaton;stationary state;simulation;population model;computer science;artificial intelligence;repeated game;mathematics;symmetric matrix	AI	39.07529493728117	9.143773670805231	183717
369ed4a354c48f8aac950c6876c1e5a74e00c19a	rres: a novel approach to the partitioning problem for a typical subset of system graphs	signal image and speech processing;circuits and systems;control structures and microprogramming;electronic circuits and devices	The research field of system partitioning in modern electronic system design started to find strong advertence of scientists about fifteen years ago. Since a multitude of formulations for the partitioning problem exist, the same multitude could be found in the number of strategies that address this problem. Their feasibility is highly dependent on the platform abstraction and the degree of realism that it features. This work originated from the intention to identify the most mature and powerful approaches for system partitioning in order to integrate them into a consistent design framework for wireless embedded systems. Within this publication, a thorough characterisation of graph properties typical for task graphs in the field of wireless embedded system design has been undertaken and has led to the development of an entirely new approach for the system partitioning problem. The restricted range exhaustive search algorithm is introduced and compared to popular and well-reputed heuristic techniques based on tabu search, genetic algorithm, and the global criticality/local phase algorithm. It proves superior performance for a set of system graphs featuring specific properties found in human-made task graphs, since it exploits their typical characteristics such as locality, sparsity, and their degree of parallelism.	brute-force search;degree of parallelism;embedded system;genetic algorithm;graph property;heuristic;locality of reference;parallel computing;partition problem;run time (program lifecycle phase);search algorithm;self-organized criticality;sparse matrix;systems design;tabu search;vertex (graph theory);whole earth 'lectronic link	Bastian Knerr;Martin Holzer;Markus Rupp	2008	EURASIP J. Emb. Sys.	10.1155/2008/259686	embedded system;computer science;theoretical computer science;operating system;distributed computing;algorithm	Embedded	26.62210370702228	6.732370108437127	183851
8848874f40c01413712b8e6b2bd03c4f8232da67	hypervolume subset selection in two dimensions: formulations and algorithms	hypervolume;k link shortest path;subset selection;multiobjective optimization	The hypervolume subset selection problem consists of finding a subset, with a given cardinality k, of a set of nondominated points that maximizes the hypervolume indicator. This problem arises in selection procedures of evolutionary algorithms for multiobjective optimization, for which practically efficient algorithms are required. In this article, two new formulations are provided for the two-dimensional variant of this problem. The first is a (linear) integer programming formulation that can be solved by solving its linear programming relaxation. The second formulation is a k-link shortest path formulation on a special digraph with the Monge property that can be solved by dynamic programming in time. This improves upon the result of in Bader (2009), and slightly improves upon the result of in Bringmann et al. (2014b), which was developed independently from this work using different techniques. Numerical results are shown for several values of n and k.	cardinality;directed graph;dynamic programming;evolutionary algorithm;genetic selection;integer (number);integer programming;linear programming relaxation;mathematical optimization;multi-objective optimization;numerical method;selection algorithm;short;shortest path problem;subgroup	Tobias Kuhn;Carlos M. Fonseca;Luís Paquete;Stefan Ruzika;Miguel Duarte;José Rui Figueira	2016	Evolutionary Computation	10.1162/EVCO_a_00157	mathematical optimization;combinatorics;multi-objective optimization;machine learning;mathematics	Theory	25.34772179214734	5.271606491487533	184061
02b93e971fed601ced78831cf480ebbd10605f45	complexity and algorithms for the discrete fréchet distance upper bound with imprecise input		We study the problem of computing the upper bound of the discr ete Fréchet distance for imprecise input, and prove that the problem is NP-hard. This solves an open proble m posed in 2010 by Ahnet al. If shortcuts are allowed, we show that the upper bound of the discrete Fréche t distance with shortcuts for imprecise input can be computed in polynomial time and we present several efficient algorithms.	algorithm;eterna;fréchet distance;np-hardness;time complexity	Chenglin Fan;Binhai Zhu	2015	CoRR		discrete mathematics;combinatorics;mathematics;open problem;time complexity;fréchet distance;algorithm;upper and lower bounds	Theory	28.586593647237958	18.205025696153726	184427
ce12861d69031407e49ae597a8c1b6d3dc073bfd	a new method of minimum convex hull in computer applications	computer applications;期刊论文;minimum convex hull;new method	The minimum convex hull is one of the widely studied problems in computational geometry, as well as widely applied in many fields such as in GIS. This paper, through analysis of the disadvantage of the traditional minimum convex hull serial algorithm, puts forward a new method of the minimum convex hull. It introduces three new concepts such as ‘absolute vertex’, ‘absolutely wrong vertex’ and ‘possible vertex’ in minimum convex hull, which can improve the algorithm computing efficiency, and increase the applicability of minimum convex hull in large number of spatial data. In the end, through comparative analysis, compared to the traditional serial algorithm, this paper’s algorithm has obvious advantages on seeking minimum convex hull vertex in large number of spatial data, this algorithm execution efficiency is higher than the traditional serial algorithm for minimum convex hull, and it also can improved the applicability of minimum convex hull.	computation;computational geometry;computer;convex hull;geographic information system;qualitative comparative analysis;sequential algorithm;tree traversal	Bo Liu;Jian Ruan	2014	IJCAT	10.1504/IJCAT.2014.066728	convex analysis;subderivative;mathematical optimization;combinatorics;pseudotriangle;discrete mathematics;convex optimization;krein–milman theorem;convex polytope;convex combination;orthogonal convex hull;linear matrix inequality;convex conjugate;minimum bounding box;gift wrapping algorithm;convex hull;absolutely convex set;mathematics;convex set;computer applications;alpha shape;tight span;proper convex function;output-sensitive algorithm	AI	31.489306193092613	15.179198364018076	184467
7948ee28ce788eaa8b4bce6c04f4e12f99d53d3a	on the performance of triangulation-based multiple shooting method for 2d geometric shortest path problems		In this paper we describe an algorithm based on the idea of the direct multiple shooting method for solving approximately 2D geometric shortest path problems (introduced by An et al. in Journal of Computational and Applied Mathematics, 244 (2103), pp. 67-76). The algorithm divides the problem into suitable sub-problems, and then solves iteratively sub-problems. A so-called collinear condition for combining the sub-problems was constructed to obtain an approximate solution of the original problem. We discuss here the performance of the algorithm. In order to solve the sub-problems, a triangulation-based algorithm is used. The algorithms are implemented by C++ code. Numerical tests for An et al.’s algorithm are given to show that it runs significantly in terms of run time and memory usage.	shooting method;shortest path problem	Thanh An Phan;Nguyen Ngoc Hai;Tran Van Hoai;Le Hong Trang	2014	Trans. Large-Scale Data- and Knowledge-Centered Systems	10.1007/978-3-662-45947-8_4	mathematical optimization;combinatorics;constrained shortest path first;euclidean shortest path;yen's algorithm;shortest path problem;k shortest path routing;shortest path faster algorithm	Graphics	26.69947713497719	8.437485750671462	184473
214cec3977636a14d38c0496bb6766e95a67c264	parameterized clique on inhomogeneous random graphs	inhomogeneous random graph;power law graph;scale free network;clique	Finding cliques in graphs is a classical problem which is in general NP-hard and parameterized intractable. In typical applications like social networks or biological networks, however, the considered graphs are scale-free, i.e., their degree sequence follows a power law. Their specific structure can be algorithmically exploited and makes it possible to solve clique much more efficiently. We prove that on inhomogeneous random graphs with n nodes and power law exponent β, cliques of size k can be found in time O(n) for β ≥ 3 and in time O(nek4) for 2 < β < 3.	algorithm;biological network;clique (graph theory);complex network;degree (graph theory);np-hardness;pareto efficiency;random graph;samuel newman;social network;zipf's law	Tobias Friedrich;Anton Krohmer	2015	Discrete Applied Mathematics	10.1016/j.dam.2014.10.018	clique;block graph;random regular graph;pathwidth;random graph;mathematical optimization;split graph;combinatorics;clique graph;discrete mathematics;cograph;k-tree;clique problem;scale-free network;clique-sum;mathematics;treewidth;chordal graph;indifference graph	Theory	36.47034977195446	13.749547443666001	184499
2f91a4913bcfaa5ed5db8f9fc4f5b28a272e52e6	de-anonymizing scale-free social networks by percolation graph matching	social networking online complex networks graph theory network theory graphs;social network services algorithm design and analysis analytical models radiation detectors conferences computers privacy;real social network graphs scale free social network de anonymization problem percolation graph matching scale free graphs asymptotic mathematical analysis power law node degree distribution complex systems graph slicing technique	We address the problem of social network de-anonymization when relationships between people are described by scale-free graphs. In particular, we propose a rigorous, asymptotic mathematical analysis of the network de-anonymization problem while capturing the impact of power-law node degree distribution, which is a fundamental and quite ubiquitous feature of many complex systems such as social networks. By applying bootstrap percolation and a novel graph slicing technique, we prove that large inhomogeneities in the node degree lead to a dramatic reduction of the initial set of nodes that must be known a priori (the seeds) in order to successfully identify all other users. We characterize the size of this set when seeds are selected using different criteria, and we show that their number can be as small as n% for any small ε > 0. Our results are validated through simulation experiments on real social network graphs.	algorithm;bootstrap percolation;complex systems;data anonymization;de-anonymization;degree distribution;experiment;matching (graph theory);percolation theory;simulation;social network	Carla-Fabiana Chiasserini;Michele Garetto;Emilio Leonardi	2015	2015 IEEE Conference on Computer Communications (INFOCOM)	10.1109/INFOCOM.2015.7218536	network science;random graph;combinatorics;discrete mathematics;degree distribution;economic graph;bipartite graph;graph product;dynamic network analysis;clique-width;graph theory;theoretical computer science;machine learning;comparability graph;clustering coefficient;mathematics;modular decomposition;indifference graph;complex network	Vision	37.23812081592571	14.129386676119704	184633
13180921e1e692795ce07e830375f5f203789896	the subtree size profile of plane-oriented recursive trees		In this extended abstract, we outline how to derive limit theorems for the number of subtrees of size k on the fringe of random plane-oriented recursive trees. Our proofs are based on the method of moments, where a complex-analytic approach is used for constant k and an elementary approach for k which varies with n. Our approach is of some generality and can be applied to other simple classes of increasing trees as well.	boundary element method;recursion (computer science);tree (data structure)	Michael Fuchs	2011		10.1137/1.9781611973013.10	combinatorics;discrete mathematics;mathematics;algorithm	Theory	38.89106507603149	17.459167479699182	184670
cd314eed3c6a3d8dd2a3fa4f5f08b94400171fb7	a historical note on convex hull finding algorithms	algorithm complexity;algorithm analysis;geometrie calcul;computational geometry;monotone polygons;complexity;enveloppe convexe;complexite algorithme;pattern recognition;algorithms;analyse algorithme;reconnaissance forme;convex hull	Most of the progress made on the convex hull problem has been accomplished during and after the late 1970's. In the convex hull literature to date, Graham (1972) is credited with the first optimal O(n log n) algorithm for computing the convex hull of n points on the plane. In this note we bring to light a hidden and forgotten convex hull algorithm due to Bass and Schubert (1967). Although their description of the algorithm is somewhat vague and, as described, their algorithm is incorrect, it is shown here that their procedure nevertheless contains most of the key ideas that have appeared in the convex hull literature in recent years. Finally, although the authors did not provide either a proof of correctness or a complexity analysis, it is shown here that a suitable interpretation of their algorithm runs correctly in O(n log n) time and thus predates Graham's algorithm by five years.	algorithm;convex hull	Godfried T. Toussaint	1985	Pattern Recognition Letters	10.1016/0167-8655(85)90038-8	convex analysis;subderivative;mathematical optimization;combinatorics;pseudotriangle;complexity;convex polytope;convex combination;orthogonal convex hull;convex conjugate;computational geometry;computer science;gift wrapping algorithm;convex hull;gauss–lucas theorem;mathematics;geometry;convex set;alpha shape;tight span;algorithm;proper convex function;output-sensitive algorithm	Vision	30.97438420597644	15.661662451405432	184692
cfadb7c962728f20cfa80bf43ce0569ff677baba	an index calculus algorithm for plane curves of small degree	modelizacion;approximation asymptotique;plane curve;algorithm analysis;elliptic curve;heuristic method;metodo heuristico;cuerpo de clases;corps fini;courbe elliptique;corps de classes;finite field;modelisation;curva eliptica;indexation;campo finito;discrete logarithm problem;analyse algorithme;class field;hyperelliptic curve;methode heuristique;asymptotic approximation;modeling;analisis algoritmo;aproximacion asintotica;class group	We present an index calculus algorithm which is particularly well suited to solve the discrete logarithm problem (DLP) in degree 0 class groups of curves over finite fields which are represented by plane models of small degree. A heuristic analysis of our algorithm indicates that asymptotically for varying q, “almost all” instances of the DLP in degree 0 class groups of curves represented by plane models of a fixed degree d ≥ 4 over Fq can be solved in an expected time of Õ(q ). Additionally we provide a method to represent “sufficiently general” (non-hyperelliptic) curves of genus g ≥ 3 by plane models of degree g + 1. We conclude that on heuristic grounds, “almost all” instances of the DLP in degree 0 class groups of (non-hyperelliptic) curves of a fixed genus g ≥ 3 (represented initially by plane models of bounded degree) can be solved in an expected time of Õ(q).	algorithm;average-case complexity;degree (graph theory);digital light processing;discrete logarithm;genus (mathematics);google+;heuristic analysis	Claus Diem	2006		10.1007/11792086_38	discrete logarithm;plane curve;combinatorics;systems modeling;calculus;mathematics;geometry;elliptic curve;hyperelliptic curve;finite field;algorithm	Theory	29.48501738356121	15.905852213157507	184757
0e728d69ddfb98a7fbbde123b27573ce0b33dad4	sampling 3-colourings of regular bipartite graphs	mixing time 3 colouring potts model conductance glauber dynamics discrete hypercube;05c15 82b20	We show that if Σ = (V,E) is a regular bipartite graph for which the expansion of subsets of a single parity of V is reasonably good and which satisfies a certain local condition (that the union of the neighbourhoods of adjacent vertices does not contain too many pairwise non-adjacent vertices), and if M is a Markov chain on the set of proper 3-colourings of Σ which updates the colour of at most ρ|V | vertices at each step and whose stationary distribution is uniform, then for ρ ≈ .22 and d sufficiently large the convergence to stationarity ofM is (essentially) exponential in |V |. In particular, if Σ is the d-dimensional hypercube Qd (the graph on vertex set {0, 1}d in which two strings are adjacent if they differ on exactly one coordinate) then the convergence to stationarity of the well-known Glauber (single-site update) dynamics is exponentially slow in 2d/( √ d log d). A combinatorial corollary of our main result is that in a uniform 3-colouring of Qd there is an exponentially small probability (in 2 d) that there is a colour i such the proportion of vertices of the even subcube coloured i differs from the proportion of the odd subcube coloured i by at most .22. Our proof combines a conductance argument with combinatorial enumeration methods.	conductance (graph);glauber;graph (discrete mathematics);graph coloring;markov chain;neighbourhood (graph theory);stationary process;time complexity;vertex (geometry);whole earth 'lectronic link	David Galvin	2007	CoRR		combinatorics;discrete mathematics;topology;mathematics;neighbourhood	Theory	38.116003426560475	16.550259755120937	184800
a357635b8dee78c6e361bf7ae76269a6a386cde0	gathering two stateless mobile robots using very inaccurate compasses in finite time	potential symmetry;own instance;asynchronous identical mobile robot;stateless mobile robot;inaccurate compass;gathering problem;prior work;largest divergence;finite time;simple problem;asynchronous robot;larger divergence;mobile robot	This paper considers two asynchronous identical mobile robots in an environment devoid of any landmarks or common coordinate system. The robots, executing their own instance of the same algorithm, must cooperate to end up at the exact same location (not predetermined) within a finite time. The problem, known as gathering, is the simplest form of spontaneous agreement that can be reached between the robots. This simple problem is however notoriously impossible with two such robots. Surprisingly, the problem was shown to be solvable for three robots or more, adding a weak assumption to help break any potential symmetry in the system. Prior work has shown that the problem could be solved by letting each robot have access to some compass, provided that the divergence between the compasses is at most 45◦. The question remained open, however, as to whether the problem could still be solved with a larger divergence. In this paper, we present a distributed algorithm that solves the gathering problem with two asynchronous robots, when their compasses can differ by any angle less than 180◦, which is obviously the largest divergence for which the compasses can still bring any useful information.		Masafumi Yamashita;Samia Souissi;Xavier Défago	2007		10.1145/1377868.1377927	mobile robot;simulation;computer science;artificial intelligence	Robotics	32.50186808796466	12.41881993260932	184898
08b2765713454f53c855d8892fe985be4eea9abd	maximum principle for markov regime-switching forward-backward stochastic control system with jumps and relation to dynamic programming		This paper presents a sufficient stochastic maximum principle for a stochastic optimal control problem of Markov regime-switching forward–backward stochastic differential equations with jumps. The relationship between the stochastic maximum principle and the dynamic programming principle in a Markovian case is also established. Finally, applications of the main results to a recursive utility portfolio optimization problem in a financial market are discussed.	control system;dynamic programming;markov chain;stochastic control	Zhongyang Sun;Junyi Guo;Xin Zhang	2018	J. Optimization Theory and Applications	10.1007/s10957-017-1068-5	mathematical optimization;continuous-time stochastic process;stochastic modelling;stochastic optimization;forward–backward algorithm;mathematics;mathematical economics;stochastic	ML	38.6474166786862	4.9187929499901735	185105
1fa217b34332be504c152db9241b89ed9d38abd7	optimal constrained graph exploration	graph exploration;computational learning theory;lower bound	We address the problem of exploring an unknown graph <i>G</i> = (<i>V</i>, <i>E</i>) from a given start node <i>s</i> with either a tethered robot or a robot with a fuel tank of limited capacity, the former being a tighter constraint. In both variations of the problem, the robot can only move along the edges of the graph, i.e, it cannot jump between non-adjacent vertices. In the tethered robot case, if the tether (rope) has length <i>l</i>, then the robot must remain within distance <i>l</i> from the start node <i>s</i>. In the second variation, a fuel tank of limited capacity forces the robot to return to <i>s</i> after traversing <i>C</i> edges. The efficiency of algorithms for both variations of the problem is measured by the number of edges traversed during the exploration. We present an algorithm for a tethered robot which explores the graph in <i>&Ogr;</i>(¦<i>E</i>¦) edge traversals. The problem of exploration using a robot with a limited fuel tank capacity can be solved with a simple reduction from the tethered robot case and also yields a <i>&Ogr;</i>(¦<i>E</i>¦) algorithm. This improves on the previous best known bound of <i>&Ogr;</i>(¦<i>E</i>¦ + ¦<i>V</i>¦log <sup>2</sup>¦<i>V</i>¦) in [4]. Since the lower bound for the graph exploration problems is ¦<i>E</i>¦, our algorithm is optimal, thus answering the open problem of Awerbuch, Betke, Rivest, and Singh [3].	algorithm;graph (discrete mathematics);neighbourhood (graph theory);robot;rope (data structure)	Christian A. Duncan;Stephen G. Kobourov;V. S. Anil Kumar	2001	ACM Trans. Algorithms	10.1145/1159892.1159897	mathematical optimization;simulation;computer science;machine learning;mathematics;upper and lower bounds;computational learning theory	Theory	26.91126768830864	17.948815560840906	185198
2a7de43c13a6dd989ac80683e1d3a49ec17ded12	formulating logical implications in combinatorial optimisation	logical implication;cutting plane;operations research;linear constraint;constraint formulation;mip modelling;linear program;branch and bound;combinatorial optimisation;clustering problem	When practical problems are formulated as combinatorial optimisation models one must often include logical implications between decisions. It is useful to express these implications as linear constraints involving binary variables, since linear constraints offer the possibility of using linear programming and branch and bound as an initial solution method. Often this formulation step of the modelling process seems far from evident to many practitioners, students and researchers. Therefore it is of interest to make simple rules available to clarify and help in this process. In this educationally oriented tutorial paper we introduce such a rule, LIP, the logical implication principle. It offers an easy and automatic way to translate elementary logical implications involving 0–1 variables into linear constraints. Based on an extremely simple cutting plane, we demonstrate how the rule is extended using complementarity and implicit binary variables, and leads to a simple but powerful instrument. This is illustrated by several examples of applications in various fields of Operational Research. The paper culminates in the description of a novel set of constraints which fully eliminate all permutation-equivalent solutions to numbered clustering problems. 2002 Elsevier Science B.V. All rights reserved.	branch and bound;cluster analysis;combinatorial optimization;complementarity theory;cutting-plane method;integer programming;linear inequality;linear programming;mathematical optimization;model checking;operations research;social inequality	Frank Plastria	2002	European Journal of Operational Research	10.1016/S0377-2217(02)00073-5	mathematical optimization;combinatorics;logical consequence;computer science;linear programming;mathematics;branch and bound;algorithm;cutting-plane method	AI	25.264642608135542	10.753315028410253	185494
98306abac7a6151a2544df34659814ddc81dbc9c	a time efficient delaunay refinement algorithm	aspect ratio	In this paper we present a Delaunay refinement algorithm for generating good aspect ratio and optimal size triangulations. This is the first algorithm known to have sub-quadratic running time. The algorithm is based on the extremely popular Delaunay refinement algorithm of Ruppert. We know of no prior refinement algorithm with an analyzed subquadratic time bound. For many natural classes of meshing problems, our time bounds are comparable to know bounds for quadtree methods.	computation;computational geometry;delaunay triangulation;quadtree;refinement (computing);run time (program lifecycle phase);ruppert's algorithm;sorting;time complexity	Gary L. Miller	2004			mathematical optimization;combinatorics;aspect ratio;ruppert's algorithm;mathematics;constrained delaunay triangulation;chew's second algorithm;bowyer–watson algorithm;algorithm	Theory	32.73104376797713	16.492211058539077	185659
2f07722104a76d2ef433b2d89b46469c5a6a51ec	on the sequential search for spatially-distributed events	sequential search;spatial distribution	Events arise at two points according to independent Poisson processes; their durations are independent and identically distributed random variables. An observer can make visits to the two points, and, on any particular visit to either point, can detect all events then going on there. There is a “dead time” associated with travel from one point to the other. The problem is: What should the observer’s visiting strategy be if his goal is to maximize the steady-state fraction of events he observes at least once? We prove a series of theorems about an optimal search strategy that ultimately provide the basis of an algorithm shown to converge to that strategy.	linear search	Arnold Barnett;John Mazzarino	1980	SIAM J. Matrix Analysis Applications	10.1137/0601011	linear search;econometrics;mathematical optimization;mathematics;algorithm;statistics	Theory	38.171607321077275	6.962900771900845	185870
7b66a5a4d44d6ce5ae9ac842b59a0e675836721f	hausdorff distance driven l-shape matching based layout decomposition for e-beam lithography		Layout decomposition is a basic step in mask data preparation in e-beam lithography (EBL) writing. For larger throughput in EBL, L-shape-writing technique has recently been developed. It is termed as L-shape fracturing, similar in line with rectangular fracturing. However, implementation of this new technique may yield very thin/narrow features called slivers. For better manufacturability, it is preferable to minimize the overall sliver length. In this paper we propose a novel scheme based on Hausdorff distance metrics for L-shape fracturing with inherent sliver minimization. The proposed scheme starts with finding the concave corner vertices of input layout, and attempts to find a balanced partition of this set of concave corner points of the given layout. Subsequently, Hausdorff distance-based layout fracturing is performed. Experimental results demonstrate efficacy of our proposed algorithm.	beam robotics;hausdorff dimension	Arindam Sinharay;Pranab Roy;Hafizur Rahaman	2017		10.1007/978-981-10-7470-7_29	throughput;electronic engineering;mathematical optimization;electron-beam lithography;mask data preparation;hausdorff distance;vertex (geometry);computer science;partition (number theory);design for manufacturability;lithography	EDA	34.63066263786185	15.748471380942444	185917
15248a5ec05599d2f15cd385dbaf49b4f073570b	multidimensional binary search for contextual decision-making		We consider a multidimensional search problem that is motivated by questions in contextual decision-making, such as dynamic pricing and personalized medicine. Nature selects a state from a d-dimensional unit ball and then generates a sequence of d-dimensional directions. We are given access to the directions, but not access to the state. After receiving a direction, we have to guess the value of the dot product between the state and the direction. Our goal is to minimize the number of times when our guess is more than ε away from the true answer. We construct a polynomial time algorithm that we call Projected Volume achieving regret O(dlog(d/ε)), which is optimal up to a logd factor. The algorithm combines a volume cutting strategy with a new geometric technique that we call cylindrification.	binary search algorithm;cylindrification;decision problem;p (complexity);personalization;regret (decision theory);search problem	Ilan Lobel;Renato Paes Leme;Adrian Vladu	2017		10.1145/3033274.3085100	mathematical optimization;combinatorics;artificial intelligence;data mining;mathematics;algorithm;statistics	Theory	26.96412159396014	16.621903720070527	186060
7dbb0e881784e03b854821dda19e624ca356160f	an optimal algorithm for the intersection radius of a set of convex polygons	forme convexe;convex shape;temps lineaire;computational geometry;polygone;search strategy;forma convexa;algorithme;polygon;linear time;strategie recherche;segment droite;poligono;algorithms;segmento recta;line segment;algoritmo optimo;algorithme optimal;optimal algorithm;convex polygon;estrategia investigacion	The intersection radius of a finite collection of geometrical objects in the plane is the radius of the smallest closed disk that intersects all the objects in the collection. Bhattacharya et al. showed how the intersection radius can be found in linear time for a collection of line segments in the plane by combining the Ž Ž . Ž . prune-and-search strategy of Megiddo J. Assoc. Comput. Mach. 31 1 1984 , . Ž 114]127 with the strategy of replacing line segments by lines or points B. K. Bhattacharya, S. Jadhav, A. Mukhopadhyay, and J. M. Robert, in ‘‘Proceedings of the Seventh Annual ACM Symposium on Computational Geometry,’’ pp. 81]88, . 1991 . In this paper, we enlarge the scope of this technique by showing that it can also be used to find the intersection radius of a collection of convex polygons in Ž . O n time, where n is the total number of polygon vertices. Q 1996 Academic	algorithm;computation;convex set;mach;prune and search;symposium on computational geometry;time complexity	Shreesh Jadhav;Asish Mukhopadhyay;Binay K. Bhattacharya	1996	J. Algorithms	10.1006/jagm.1996.0013	intersection;combinatorics;computational geometry;polygon;mathematics;geometry;intersection;algorithm;line segment intersection	Theory	30.18209961230435	17.83024489752831	186106
b9c602a3fc2d18ca403ff4d62c5a3aa8a3d5d7a4	on the equivalence of two expected average cost criteria for semi-markov control processes	semi markov control models;average cost;borel state space;average cost optimality equation	The two expected average costs used in the theory of semi-Markov control processes with a Borel state space are considered. Under some stochastic stability conditions, we prove that the two criteria are equivalent in the sense that they lead to the same optimality equation.	markov chain;semiconductor industry;turing completeness	Anna Jaskiewicz	2004	Math. Oper. Res.	10.1287/moor.1030.0060	mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	38.3889615724835	5.292580702186527	186196
db8b78cfa14930d4510241cd698fadf6228049fb	approximations of inventory models	dynamic program;inventory model	"""A general procedure is presented for constructing approximations of discrete review single product dynamic inventory models. Bounds are derived for the approximations and compared with the ones of Hinderer [1978], Iqhitt [1978] for the approximation of a general dynamic program. """"Good"""" order-policies are constructed. Finally, conditions are given under which the sequence of bounds associated with a sequence of approximating models converges to zero."""	approximation algorithm;inventory theory	Karl-Heinz Waldmann	1981	Zeitschr. für OR	10.1007/BF01919299	mathematical optimization;mathematics;mathematical economics	ML	36.50756119935208	5.3943209517851045	186344
3709d05b92c439797c0e28c315c6dc36fac50cc2	control of continuous-time markov chains with safety upper bounds	continuous time;generators;probability;safe initial probability vectors;probability distributions;controlled markov chains;discrete time systems;controlled markov chain;safety control;continuous time horizon;continuous time markov chain;nontrivial extension;stochastic system;upper bound;continuous time systems;vectors;discrete time markov decision processes;necessary and sufficient condition;probability distribution;safety;markov processes safety probability distribution upper bound generators equations process control;process control;necessary and sufficient conditions;safety upper bounds;safe initial probability vectors continuous time markov chains safety upper bounds controlled markov chains continuous time horizon nontrivial extension safety control stochastic systems discrete time markov decision processes probability distributions unit interval valued vector state probability distribution vector necessary and sufficient conditions;vectors continuous time systems discrete time systems markov processes probability safety stochastic systems;markov processes;markov decision process;continuous time markov chains;stochastic systems;unit interval valued vector;state probability distribution vector	This work introduces the notion of safety for the controlled Markov chains in the continuous-time horizon. The concept is a non-trivial extension of safety control for stochastic systems modeled as discrete-time Markov decision processes, where the safety means that the probability distributions of the system states will not visit the given forbidden set at any time. In this paper an unit-interval valued vector that serves as an upper bound on the state probability distribution vector characterizes the forbidden set. A probability distribution is then called safe if it does not exceed the upper bound. Under mild conditions the author derives two results: 1) the necessary and sufficient conditions that guarantee the all-time safety of the probability distributions if the starting distribution is safe, and 2) the characterization of the supreme set of safe initial probability vectors that remain safe as time passes. In particular the paper identifies an upper bound on time and shows that if a distribution is always safe before that time, the distribution is safe at all times. Numerical examples are provided to illustrate the two results.	basic stamp;markov chain;markov decision process;numerical method;stochastic process	Shun-Pin Hsu	2010	2010 IEEE International Conference on Automation Science and Engineering	10.1109/COASE.2010.5584105	mathematical optimization;combinatorics;discrete mathematics;symmetric probability distribution;mathematics	Robotics	39.021760629087915	5.599101751501761	186463
60b97c5feeb1d6b066ef385bf0b74c7fecb5ee8b	fine-grained complexity analysis of some combinatorial data science problems		This thesis is concerned with analyzing the computational complexity of NPhard problems related to data science. For most of the problems considered in this thesis, the computational complexity has not been intensively studied before. We focus on the complexity of computing exact problem solutions and conduct a detailed analysis identifying tractable special cases. To this end, we adopt a parameterized viewpoint in which we spot several parameters which describe properties of a specific problem instance that allow to solve the instance efficiently. We develop specialized algorithms whose running times are polynomial if the corresponding parameter value is constant. We also investigate in which cases the problems remain intractable even for small parameter values. We thereby chart the border between tractability and intractability for some practically motivated problems which yields a better understanding of their computational complexity. In particular, we consider the following problems. General Position Subset Selection is the problem to select a maximum number of points in general position from a given set of points in the plane. Point sets in general position are well-studied in geometry and play a role in data visualization. We prove several computational hardness results and show how polynomial-time data reduction can be applied to solve the problem if the sought number of points in general position is very small or very large. The Distinct Vectors problem asks to select a minimum number of columns in a given matrix such that all rows in the selected submatrix are pairwise distinct. This problem is motivated by combinatorial feature selection. We prove a complexity dichotomy with respect to combinations of the minimum and the maximum pairwise Hamming distance of the rows for binary input matrices, thus separating polynomial-time solvable from NP-hard cases. Co-Clustering is a well-known matrix clustering problem in data mining where the goal is to partition a matrix into homogenous submatrices. We conduct an extensive multivariate complexity analysis revealing several NP-hard and some polynomial-time solvable and fixed-parameter tractable cases. The generic F -free Editing problem is a graph modification problem in which a given graph has to be modified by a minimum number of edge		Vincent Froese	2018			computational science;computer science	Theory	28.768492493009166	14.295466637252423	186677
43d42f99bb56852e94068b81ee7db7f92616abba	on random walks in direction-aware network problems	graph theory;theoretical framework;travel time;laplacian matrix;community networks;random walk;directed graph;graph model	Graph theory provides a powerful set of metrics and conceptual ideas to model and investigate the behavior of communication networks. Most graph-theoretical frameworks in the networking literature are based on undirected graph models, where a symmetric link weight is assigned to each link of the network. However, many communication networks must account for directionality of communication links. This paper reports on an effort to extend some of the existing results of symmetric graphs to asymmetric ones. In particular we are interested in the behavior of random-walk based algorithms in directed graphs and we find the average travel time of a random-walk as a function of an asymmetric Laplacian matrix, which is in turn a function of link weights.	algorithm;directed graph;graph (discrete mathematics);graph theory;laplacian matrix;symmetric graph;telecommunications network;turned a	Ali Tizghadam;Alberto Leon-Garcia	2010	SIGMETRICS Performance Evaluation Review	10.1145/1870178.1870182	graph power;random graph;combinatorics;discrete mathematics;directed graph;laplacian matrix;null graph;graph property;graph theory;machine learning;comparability graph;mathematics;voltage graph;graph;moral graph;butterfly graph;random geometric graph;complement graph;random walk;line graph;adjacency matrix	Theory	35.748330541022476	12.458838329247316	186733
c69260cb41df824f73c38e34ad128d1b7275ab90	the maximum traveling salesman problem under polyhedral norms	traveling salesman problem;polynomial time	We consider the traveling salesman problem when the cities are points in R for some fixed d and distances are computed according to a polyhedral norm. We show that for any such norm, the problem of finding a tour of maximum length can be solved in polynomial time. If arithmetic operations are assumed to take unit time, our algorithms run in time O(nf−2 log n), where f is the number of facets of the polyhedron determining the polyhedral norm. Thus for example we have O(n log n) algorithms for the cases of points in the plane under the Rectilinear and Sup norms. This is in contrast to the fact that finding a minimum length tour in each case is NP-hard.	algorithm;np-hardness;polyhedral;time complexity;travelling salesman problem	Alexander I. Barvinok;David S. Johnson;Gerhard J. Woeginger;Russell Woodroofe	1998		10.1007/3-540-69346-7_15	time complexity;nearest neighbour algorithm;2-opt;mathematics;mathematical economics;travelling salesman problem;3-opt;bottleneck traveling salesman problem	Theory	28.2875164208836	17.930592879315448	186773
ecab9d89b32b3a9b9c6c3ddee7434e4e6d2bc12d	the s-metric, the beichl-cloteaux approximation, and preferential attachment	cs si;math co;physics soc ph	Abstract. This paper presents an efficient algorithm to approximate the Smetric, which normalizes a graph’s assortativity by its maximum possible value. The algorithm is used to track in detail the assortative structure of growing preferential attachment trees, and to study the evolving structure and preferential attachment of several mathematics coauthorship graphs. These graphs’ behavior belies expectations; implications are discussed.	approximation algorithm;assortativity;attachments;business architecture;degree (graph theory);image scaling;sid meier's alpha centauri	Jason Cory Brunson	2013	CoRR		combinatorics;mathematics;geometry;algorithm;algebra	Theory	37.420730558904566	15.36572389932338	187082
406bfc4d0d6ab5f6aada90d4b56264698fe203a4	properties of a job search problem on a partially observable markov chain in a dynamic economy	stock market;partially observable markov chain;optimal policy;job search;computational mathematics;decision problem;bayesian learning;total positivity;probability distribution;state space;markov process;job search problem;modelling and simulation;applied mathematics;markov chain	"""This paper observes a job search problem on a partially observable Markov chain, which can be considered as an extension of a job search in a dynamic economy in [1]. This problem is formulated as the state changes according to a partially observable Markov chain, i.e., the current state cannot be observed but there exists some information regarding what a present state is. All information about the unobservable state are summarized by the probability distributions on the state space, and we employ the Bayes' theorem as a learning procedure. The total positivity of order two, or simply TP2, is a fundamental property to investigate sequential decision problems, and it also plays an important role in the Bayesian learning procedure for a partially observable Markov process. By using this property, we consider some relationships among prior and posterior information, and the optimal policy. """"We will also observe the probabilities to make a transition into each state after some additional transitions by employing the optimal policy. In the stock market, suppose that the states correspond to the business situation of one company and if there is a state designating the default, then the problem is what time the stocks are sold off before bankrupt, and the probability to become bankrupt will be also observed."""	decision problem;markov chain;partially observable system;search problem;state space	Toru Nakai	2006	Computers & Mathematics with Applications	10.1016/j.camwa.2005.11.019	probability distribution;markov chain;mathematical optimization;markov kernel;partially observable markov decision process;markov property;numerical analysis;state space;continuous-time markov chain;decision problem;mathematics;markov renewal process;additive markov chain;markov process;markov model;bayesian inference;statistics;variable-order markov model	AI	38.56200779500247	4.729859253580169	187586
187a5dbc46c127e26f90ddee001f3032bbc1ecd4	the object complexity model for hidden-surface removal	object complexity;computer graphics;segment tree;computational geometry;hidden surface removal;window visibility	We deene a new model of complexity, called object complexity, for measuring the performance of hidden-surface removal algorithms. This model is more appropriate for predicting the performance of these algorithms on current graphics rendering systems than the standard measure of scene complexity used in computational geometry. We also consider the problem of determining the set of visible windows in scenes consisting of n axis-parallel windows in R 3. We present an algorithm that runs in optimal (n log n) time. The algorithm solves in the object complexity model the same problem that Bern 3 addressed in the scene complexity model.	algorithm;apache axis;computational geometry;graphics;hidden surface determination;microsoft windows;rendering (computer graphics);time complexity	Edward F. Grove;T. M. Murali;Jeffrey Scott Vitter	1999	Int. J. Comput. Geometry Appl.	10.1142/S0218195999000145	computational problem;time complexity;segment tree;parameterized complexity;computer vision;probabilistic analysis of algorithms;average-case complexity;hidden surface determination;decision tree model;computational geometry;computer science;theoretical computer science;machine learning;worst-case complexity;mathematics;geometry;computer graphics	Theory	32.317815492458266	17.972299916944948	187862
72a26f2bc382c1599475045d4a36af269567ccf0	exactness of belief propagation for some graphical models with loops	maximum likelihood;message passing algorithms;exact results;analysis of algorithms;belief propagation;statistical inference;graphical model;linear program;lp relaxation;free energy;spin glasses theory	Abstract. It is well known that an arbitrary graphical model of statistical inference defined on a tree, i.e. on a graph without loops, is solved exactly and efficiently by an iterative Belief Propagation (BP) algorithm convergent to unique minimum of the so-called Bethe free energy functional. For a general graphical model on a loopy graph the functional may show multiple minima, the iterative BP algorithm may converge to one of the minima or may not converge at all, and the global minimum of the Bethe free energy functional is not guaranteed to correspond to the optimal MaximumLikelihood (ML) solution in the zero-temperature limit. However, there are exceptions to this general rule, discussed in [12] and [2] in two different contexts, where zerotemperature version of the BP algorithm finds ML solution for special models on graphs with loops. These two models share a key feature: their ML solutions can be found by an efficient Linear Programming (LP) algorithm with a Totally-Uni-Modular (TUM) matrix of constraints. Generalizing the two models we consider a class of graphical models reducible in the zero temperature limit to LP with TUM constraints. Assuming that a gedanken algorithm, g-BP, finding the global minimum of the Bethe free energy is available we show that in the limit of zero temperature g-BP outputs the ML solution. Our consideration is based on equivalence established between gapless Linear Programming (LP) relaxation of the graphical model in the T → 0 limit and respective LP version of the Bethe-Free energy minimization.	algorithm;backpropagation;belief propagation;casio loopy;converge;energy minimization;graphical model;iterative method;linear programming relaxation;maxima and minima;software propagation;turing completeness	Michael Chertkov	2008	CoRR	10.1088/1742-5468/2008/10/P10016	mathematical optimization;combinatorics;statistical inference;discrete mathematics;linear programming;linear programming relaxation;analysis of algorithms;mathematics;maximum likelihood;graphical model;statistics;belief propagation	ML	33.725285893165356	7.701046470950779	187970
b20ae9ae1b2f8d6153bd508ed8b9c7e5fca100ce	level of nodes in increasing trees revisited	generating function;random variable;limiting distribution	Abstract. Simply generated families of trees are described by the equation T (z) = φ(T (z)) for their generating function. If a tree has n nodes, we say that it is increasing if each node has a label ∈ {1, . . . , n}, no label occurs twice, and whenever we proceed from the root to a leaf, the labels are increasing. This leads to the concept of simple families of increasing trees. Three such families are especially important: recursive trees, heap ordered trees, and binary increasing trees. They belong to the subclass of very simple families of increasing trees, which can be characterized in 3 different ways. This paper contains results about these families as well as about polynomial families (the function φ(u) is just a polynomial). The random variable of interest is the level of the node (labelled) j, in random trees of size n ≥ j. For very simple families, this is independent of n, and the limiting distribution is Gaussian. For polynomial families, we can prove this as well for j, n →∞ such that n−j is fixed. Additional results are also given. These results follow from the study of certain trivariate generating functions and Hwang’s quasi power theorem. They unify and extend earlier results by Devroye, Mahmoud and others.	binary tree;gaussian (software);polynomial;recursion (computer science)	Alois Panholzer;Helmut Prodinger	2007	Random Struct. Algorithms	10.1002/rsa.20161	random variable;generating function;combinatorics;discrete mathematics;mathematics;asymptotic distribution;algorithm;statistics	Theory	39.09973438347711	16.764250423736527	188597
038e247ddfcd99b21322474b3ed60928d2074e22	optimal computation with non-unitary quantum walks	correlacion;ley uniforme;measurement;dynamique;fonction repartition;melangeage;ressource;optimization method;quantum walks;calculo automatico;metodo optimizacion;68wxx;computing;dinamica;algorithme;calcul automatique;ciclo;quantum algorithms;quantum walk;algorithm;funcion distribucion;distribution function;quantum correlations;quantum computer;quantum physics;dynamics;medida;random walk;informatique theorique;quantum algorithm;37a25;mixing time;methode optimisation;distribution uniforme;mesure;marcha aleatoria;correlation;quantum computing;mixing;cycle;loi uniforme;mezclado;recurso;marche aleatoire;uniform distribution;computer theory;algoritmo;resource;informatica teorica	Quantum versions of random walks on the line and the cycle show a quadratic improvement over classical random walks in their spreading rates and mixing times respectively. Non-unitary quantum walks can provide a useful optimisation of these properties, producing a more uniform distribution on the line, and faster mixing times on the cycle. We investigate the interplay between quantum and random dynamics by comparing the resources required, and examining numerically how the level of quantum correlations varies during the walk. We show numerically that the optimal non-unitary quantum walk proceeds such that the quantum correlations are nearly all removed at the point of the final measurement. This requires only O(log T ) random bits for a quantum walk of T steps.	computation;mathematical optimization;numerical analysis;quantum walk	Vivien M. Kendon;Olivier Maloyer	2008	Theor. Comput. Sci.	10.1016/j.tcs.2007.12.011	combinatorics;heterogeneous random walk in one dimension;quantum t-design;quantum capacity;calculus;mathematics;quantum walk;quantum computer;quantum algorithm;quantum phase estimation algorithm	Theory	38.33533676905535	13.514631374033502	188700
cf9bbbf489329fbba1257938fb51b3655a06702c	landscapes and the maximal constraint satisfaction problem	hamming distance;sampling technique;constraint satisfaction problem;landscape analysis;neighborhood search	 . Landscape is an important notion to study the difficulty of acombinatorial problem and the behavior of heuristics . In this paper, twonew measures for landscape analysis are introduced. These measures arebased on Hamming distance of iso-cost levels. Sampling techniques basedon neighborhood search are defined in order to carry out approximationof these measures. These measures and techniques are used to analyzeand characterize the properties of random landscapes of the Maximal... 	constraint satisfaction problem;maximal set	Meriema Belaidouni;Jin-Kao Hao	1999		10.1007/10721187_18	mathematical optimization;combinatorics;discrete mathematics;decomposition method;constraint graph;constraint satisfaction dual problem;mathematics;complexity of constraint satisfaction;hybrid algorithm;local consistency	Theory	25.957722541247872	16.359960123343672	188742
15465f36f84e9c03d7bf1d1849a8167b78ac590e	joint signature of two or more systems with applications to multistate systems made up of two-state components		The structure signature of a system made up of n components having continuous and i.i.d. lifetimes was defined in the eighties by Samaniego as the n-tuple whose k-th coordinate is the probability that the k-th component failure causes the system to fail. More recently, a bivariate version of this concept was considered as follows. The joint structure signature of a pair of systems built on a common set of components having continuous and i.i.d. lifetimes is a square matrix of order n whose (k, l)-entry is the probability that the k-th failure causes the first system to fail and the l-th failure causes the second system to fail. This concept was successfully used to derive a signaturebased decomposition of the joint reliability of the two systems. In the first part of this paper we provide an explicit formula to compute the joint structure signature of two or more systems and extend this formula to the general non-i.i.d. case, assuming only that the distribution of the component lifetimes has no ties. We also provide and discuss a necessary and sufficient condition on this distribution for the joint reliability of the systems to have a signature-based decomposition. In the second part of this paper we show how our results can be efficiently applied to the investigation of the reliability and signature of multistate systems made up of two-state components. The key observation is that the structure function of such a multistate system can always be additively decomposed into a sum of classical structure functions. Considering a multistate system then reduces to considering simultaneously several two-state systems.	bivariate data;failure cause	Jean-Luc Marichal;Pierre Mathonet;Jorge Navarro;Christian Paroissin	2017	European Journal of Operational Research	10.1016/j.ejor.2017.06.022	tuple;bivariate analysis;failure causes;mathematical optimization;mathematics;square matrix	OS	37.12535682090003	11.362996587440831	189116
79c0c2a92c3ed674c33b8c6d92b36a420b1760f7	extending partial suborders	branch and bound algorithm;necessary and sufficient condition;scheduling problem;partial order	Abstract   We consider the problem of finding a transitive orientation  T  of a comparability graph  G  = ( V ,  E ), such that a given partial order  P  is extended. Existing algorithms for this problem require the full knowledge of  E , so they are of limited use in the context of a branch-and-bound algorithm, where only parts of  E  may be known at any stage. We present a new approach to the problem by describing a pair of necessary and sufficient conditions for the existence of an orientation T, based on two simple forbidden subconfigurations. This allows it to solve higher-dimensional packing and scheduling problems of interesting size to optimality. We have implemented this approach and the computational results are convincing		Sándor P. Fekete;Ekkehard Köhler;Jürgen Teich	2001	Electronic Notes in Discrete Mathematics	10.1016/S1571-0653(05)80073-3	partially ordered set;job shop scheduling;mathematical optimization;combinatorics;discrete mathematics;mathematics;branch and bound	Theory	24.640271215172685	15.02340133699225	189342
00eb302e13e108ed056ad8173121f8c86c8f7ade	a nearly linear-time ptas for explicit fractional packing and covering linear programs	covering;koufogiannakis13nearly neal e young;approximation algorithm;packing;linear time;linear programming;lagrangian relaxation	We give an approximation algorithm for fractional packing and covering linear programs (linear programs with non-negative coefficients). Given a constraint matrix with n non-zeros, r rows, and c columns, the algorithm (with high probability) computes feasible primal and dual solutions whose costs are within a factor of 1+ε of opt (the optimal cost) in time O((r+c)log(n)/ε 2+n).	approximation algorithm;coefficient;column (database);duality (optimization);fractional fourier transform;linear programming;ptas reduction;set packing;time complexity;with high probability	Christos Koufogiannakis;Neal E. Young	2007	48th Annual IEEE Symposium on Foundations of Computer Science (FOCS'07)	10.1007/s00453-013-9771-6	time complexity;mathematical optimization;combinatorics;discrete mathematics;lagrangian relaxation;computer science;linear programming;mathematics;approximation algorithm	Theory	24.75032243939611	14.298065386479754	189462
c5f438ed1f116916151f5c8e63ec7b840a299082	grasp with path relinking for 2d-bandwidth minimization problem		The graph bandwidth minimization problem is an interesting problem that has become relevant in a wide range of domains. Networks communication, VLSI layout designs, parallel algorithms simulations, matrix decomposition, are some of which areas where the reduction of the bandwidth is very significant. The problem consists of embedding a graph G into a line with the aim of minimizing the maximum distance between adjacent vertices. In this paper, we are focused on the 2D bandwidth minimization variant, which considers embedding the graph in a two-dimensional grid instead of in a line. Specifically, we study the problem deeply analyzing its complexity and considering a survey of different approximate algorithms for graphs. The review concludes outlining the conceptual basis of the heuristic technique that we plan to apply to this graph problem.	approximation algorithm;grasp;graph (discrete mathematics);graph bandwidth;greedy algorithm;heuristic;local optimum;metaheuristic;neighbourhood (graph theory);optimization problem;parallel algorithm;relevance;simulation;very-large-scale integration	Miguel Ángel Rodríguez-García;Abraham Duarte;Jesús Sánchez-Oro	2018		10.1145/3230905.3230953	grid;mathematical optimization;parallel algorithm;heuristic;grasp;embedding;graph bandwidth;matrix decomposition;bandwidth (signal processing);mathematics	Theory	25.903920446236967	6.56943025379217	189520
1e3852539da6b891792dbf9055766f7f696ff55e	piece-wise linear approximation of functions of two variables	nonlinear programming;objective function;mixed integer program;piece wise linear;nonlinear problem;mathematical model;applied research;computational efficiency	The goal of increasing computational effÉciency is one of the fundamental challenges of both theoretical and applied research in mathematical modeling. The pursuit of this goal has lead to wide diversity of efforts to transform a specific mathematical problem into one that can be solved efficiently. Recent years have seen the emergence of highly efficient methods and software for solving Mixed Integer Programming Problems, such as those embodied in the packages CPLEX, MINTO, XPRESS-MP. The paper presents a method to develop a piece-wise linear approximation of an any desired accuracy to an arbitrary continuous function of two variables. The approximation generalizes the widely known model for approximating single variable functions, and significantly expands the set of nonlinear problems that can be efficiently solved by reducing them to Mixed Integer Programming Problems. By our development, any nonlinear programming problem, including non-convex ones, with an objective function (and/or constraints) that can be expressed as sums of component nonlinear functions of no more than two variables, can be efficiently approximated by a corresponding Mixed Integer Programming Problem.	approximation algorithm;cplex;emergence;integer programming;linear approximation;linear programming;loss function;minto;mathematical model;nonlinear programming;nonlinear system;optimization problem	Djangir A. Babayev	1997	J. Heuristics	10.1007/BF00132502	fractional programming;mathematical optimization;combinatorics;discrete mathematics;integer programming;linear-fractional programming;nonlinear programming;branch and price;mathematical model;mathematics	ML	27.837362448391758	6.842677540609038	189749
2a450f59fb3de9a6836bfbfc501dae75b618e5ff	rounding on the standard simplex: regular grids for global optimization	approximation;rounding;regular grid;proximal point;maximin distance	Given a point on the standard simplex, we calculate a proximal point on the regular grid which is closest with respect to any norm in a large class, including all `-norms for p ≥ 1. We show that the minimal `-distance to the regular grid on the standard simplex can exceed one, even for very fine mesh sizes in high dimensions. Furthermore, for p = 1, the maximum minimal distance approaches the `-diameter of the standard simplex. We also put our results into perspective with respect to the literature on approximating global optimization problems over the standard simplex by means of the regular grid.	approximation algorithm;global optimization;mathematical optimization;regular grid;rounding	Immanuel M. Bomze;Stefan Gollowitzer;E. Alper Yildirim	2014	J. Global Optimization	10.1007/s10898-013-0126-2	mathematical optimization;combinatorics;discrete mathematics;approximation;mathematics;rounding;regular grid	ML	27.93668858774164	18.219182719137702	189801
6cfbbc4d8c448d8352846976ca7289395c983ba7	computing lower bounds on basket option prices by discretizing semi-infinite linear programming	article;arbitrage upper bounds	The problem of finding static-arbitrage bounds on basket option prices has received a growing attention in the literature. In this paper, we focus on the lower bound case and propose a novel efficient solution procedure that is based on the separation problem. The computational burden of the proposed method is polynomial in the input data size. We also discuss the case of possibly negative weight vectors which can be applied to spread options.	algorithm;computation;decision problem;discretization;linear programming;numerical analysis;polynomial;semiconductor industry;subroutine;weight function	Hyunseok Cho;Kyoung-Kuk Kim;Kyungsik Lee	2016	Optimization Letters	10.1007/s11590-015-0987-z	mathematical optimization;mathematics;mathematical economics;no-arbitrage bounds	AI	31.73556209067629	7.72434906384772	189877
c1a13806723d8ce31ff2b5332659dc35893b2857	a simple and efficient algorithm for determining the symmetries of polyhedra	efficient algorithm	In this paper we present a simple and efficient algorithm for determining the rotational symmetries of polyhedral objects in o(m2) time using O(m) space, where m represents the number of edges of the object. Our algorithm is an extension of Weinberg’s algorithm for determining isomorphisms of planar triply connected graphs. The symmetry information detected by our algorithm can be utilized for various purposes in artificial intelligence, robotics, assembly planning, and machine vision.	algorithm;artificial intelligence;automated planning and scheduling;machine vision;polyhedron;robotics	Xiaoyi Jiang;Horst Bunke	1992	CVGIP: Graphical Model and Image Processing	10.1016/1049-9652(92)90037-X	mathematical optimization;combinatorics;computer science;mathematics;geometry;cornacchia's algorithm	AI	33.47881543782205	18.205735336285734	189885
2014c0511277cb9fae9fc79411a51564a3e7e6ae	a randomized approach to improve the accuracy of wildfire simulations using cellular automata	wildfire simulation;cellular automata		cellular automaton;computer simulation;randomized algorithm	Maria Vittoria Avolio;Salvatore Di Gregorio;Giuseppe A. Trunfio	2014	J. Cellular Automata		cellular automaton;simulation;computer science;mathematics;algorithm	Embedded	37.93096547750779	9.139724723789175	189953
70e1c7f7bc165c1bd4cf4deac250e2365441bbd6	coap 2008 best paper award: paper of p.m. hahn, b.-j. kim, m. guignard, j.m. smith and y.-r. zhu	combinatorial optimization;facility planning;quadratic assignment	Since 2004, the Computational Optimization and Applications (COAP) editorial board has selected a paper from the preceding year’s COAP publications for the “Best Paper Award.” The award competition among papers published in 2008 culminated in a tie between two papers. This article concerns the award winning work of Peter Hahn, Bum-Jin Kim, Monique Guignard-Spielberg and Yi-Rong Zhu at the University of Pennsylvania and J. MacGregor Smith at the University of Massachusetts, Amherst, and their paper “An algorithm for the generalized quadratic assignment problem,” published in Volume 40, Issue 3, pages 351–372. The authors’ interest in the subject of the award paper began in December 2003, when they came across the website of Sourour Elloumi [4], which showed her work and that of her colleagues on various quadratic 0-1 assignment problems. What intrigued the authors was the similarity of these problems to the Quadratic Assignment Problem (QAP), on which they had been working for years. They had learned two important facts about the QAP. First, dual ascent algorithms based on making subtractions in the objective function cost matrices make very fast lower bound calculations [1, 6]. Second, researchers have over the years ignored the advantages afforded by combining complementary variables (i.e., variables that are equal to each other) in quadratic problems. So, the authors set about to work on these new problems as a challenge. Months later they came across the work by Lee and Ma [11], who coined the term Generalized Quadratic Assignment Problem (GQAP) and had been the first to provide an exact solution algorithm.	algorithm;complementarity (physics);computation;constrained application protocol;loss function;optimization problem;quadratic assignment problem;quadratic programming;times ascent	B.-J. Kim;M Guignard;J. Mark Smith;Y.-R. Zhu;William W. Hager	2009	Comp. Opt. and Appl.	10.1007/s10589-009-9300-3	mathematical optimization;combinatorial optimization;mathematics;mathematical economics;operations research	ML	28.768556003733494	8.011472161695881	190064
10b7d05b04eae367016651ca5014c2e6efa3dd63	on iterative collision search for lpn and subset sum		Iterative collision search procedures play a key role in developing combinatorial algorithms for the subset sum and learning parity with noise (LPN) problems. In both scenarios, the single-list pair-wise iterative collision search finds the most solutions and offers the best efficiency. However, due to its complex probabilistic structure, no rigorous analysis for it appears to be available to the best of our knowledge. As a result, theoretical works often resort to overly constrained and sub-optimal iterative collision search variants in exchange for analytic simplicity. In this paper, we present rigorous analysis for the single-list pair-wise iterative collision search method and its applications in subset sum and LPN. In the LPN literature, the method is known as the LF2 heuristic. Besides LF2, we also present rigorous analysis of other LPN solving heuristics and show that they work well when combined with LF2. Putting it together, we significantly narrow the gap between theoretical and heuristic algorithms for LPN.	algorithm;heuristic (computer science);iteration;iterative method;learning with errors;parity learning;subset sum problem	Srinivas Devadas;Ling Ren;Hanshen Xiao	2017	IACR Cryptology ePrint Archive	10.1007/978-3-319-70503-3_24	mathematical optimization;collision;discrete mathematics;subset sum problem;computer science;probabilistic logic;parity (mathematics);heuristic;heuristics	ECom	28.488224525702364	5.629267787001437	190176
3f6c53bd73b70c9458fa7c8309bd7f503b2f2e29	solving multiobjective discrete optimization problems with propositional minimal model generation		We propose a propositional logic based approach to solve MultiObjective Discrete Optimization Problems (MODOPs). In our approach, there exists a one-to-one correspondence between a Pareto front point of MODOP and a P -minimal model of the CNF formula obtained from MODOP. This correspondence is achieved by adopting the order encoding as CNF encoding for multiobjective functions. Finding the Pareto front is done by enumerating all P-minimal models. The beauty of the approach is that each Pareto front point is blocked by a single clause that contains at most one literal for each objective function. We evaluate the effectiveness of our approach by empirically contrasting it to a state-of-the-art MODOP solving technique.	artificial intelligence;benchmark (computing);blocking (computing);boolean satisfiability problem;coefficient;conjunctive normal form;consistency model;covering problems;discrete optimization;experiment;feature selection;general-purpose modeling;heuristic (computer science);literal (mathematical logic);maximum satisfiability problem;one-to-one (data model);optimization problem;pareto efficiency;petabyte;point of view (computer hardware company);problem solving;propositional calculus;simultaneous multithreading;solver	Takehide Soh;Mutsunori Banbara;Naoyuki Tamura;Daniel Le Berre	2017		10.1007/978-3-319-66158-2_38	mathematical optimization;minimal model;discrete mathematics;existential quantification;propositional calculus;discrete optimization;multi-objective optimization;mathematics	AI	26.907766495525443	6.091137640602051	190233
e99914427a4440a31001c10406df620a524cc52b	optimization using neural networks	tratamiento paralelo;biology computing;optimisation;architecture systeme;simulation optimisation neural networks feedback design rule probabilistic resource allocation task high performance parallel processor;concurrent computing;traitement parallele;neural networks;optimizacion;multiprocessor;concepcion sistema;neural nets;resource allocation;simulation;resource management;simulacion;design optimization;computer networks;simulation feedback neural nets optimisation;design rules;optimization problem;feedback;computational modeling;system design;multiprocessor architecture;indexation;non linearite;no linealidad;arquitectura sistema;nonlinearity;probabilistic resource allocation task;optimization;recurrent neural network;design rule;neurofeedback;reseau neuronal;multiprocesador;system architecture;nonlinear optimization;neural networks biological neural networks design optimization biology computing computational modeling computer networks concurrent computing resource management neurofeedback parallel processing;high performance;red neuronal;high performance parallel processor;conception systeme;biological neural networks;parallel processing;neural network;multiprocesseur	The design of feedback (or recurrent) neural networks to produce good solutions to complex optimization problems is discussed. The theoretical basis for applying neural networks to optimization problems is reviewed, and a design rule that serves as a primitive for constructing a wide class of constraints is introduced. The use of the design rule is illustrated by developing a neural network for producing high-quality solutions to a probabilistic resource allocation task. The resulting neural network has been simulated on a high-performance parallel processor that has been optimized for neural network simulation. >	artificial neural network;neural network software;program optimization	Gene A. Tagliarini;J. Fury Christ;Edward W. Page	1991	IEEE Trans. Computers	10.1109/12.106220	stochastic neural network;nervous system network models;optimization problem;cellular neural network;parallel computing;probabilistic neural network;multiprocessing;multidisciplinary design optimization;types of artificial neural networks;resource allocation;computer science;artificial intelligence;recurrent neural network;theoretical computer science;machine learning;neurofeedback;time delay neural network;feedback;deep learning;computational model;artificial neural network;systems design	Robotics	30.19223597576045	4.508393493206758	190443
1fa139e5444747eacfafd946d2d2a72573545dfd	maximum distance between two sets of points in ed	espacio;maximo;geometrie algorithmique;computational geometry;espace;maximum;diametre;diameter;algorithme;algorithm;temps calcul;distancia;pattern recognition;diametro;geometria computacional;space;reconnaissance forme;tiempo computacion;computation time;reconocimiento patron;distance;algoritmo	Robert, J.-M., Maximum distance between two sets of points in E d. Pattern Recognition Letters 14 (1993) 733735. This note presents how to reduce the problem of computing the maximum distance between two sets of points in Ea to the problem of computing the diameters of Cd sets of points in E a, for some constant ca depending on d.	pattern recognition letters	Jean-Marc Robert	1993	Pattern Recognition Letters	10.1016/0167-8655(93)90144-3	computational geometry;diameter;mathematics;geometry;algorithm	Vision	29.634642806973904	17.046254228586307	190493
a3524ec457011c088400900044bb8301e3da9f7f	the traveling-salesman problem (abstract)	traveling salesman problem;optimal solution;shortest path;branch and bound algorithm;iterative algorithm;polynomial time;recursive algorithm;exponential growth;np complete problem	The traveling-salesman problem is one of the classical NP-Complete problems. No current algorithms are available which can solve these problems in polynomial time, that is, the number of steps grows as a polynomial according to the size of the input. The traveling-salesman problem involves a salesman who must make a tour of a number of cities using the shortest path available. For each number of cities n, the number of paths which must be explored is n!, causing this problem to grow exponentially rather than as a polynomial. Three separate algorithms are examined. These are an iterative algorithm, a recursive algorithm, and a branch and bound algorithm. The iterative algorithm generates tours as the permutations of the first n - 1 integers (where n is the number of cities). The recursive algorithm begins with a subtour consisting of the starting city and uses a recursive subroutine to build all tours. The branch and bound algorithm builds a tree which represents tours. Each node of the tree has an associated bound, and when the bound of a node becomes larger than the cost of the best tour found so far, that node is no longer eligible for exploration. This pruning process allows this algorithm to delay exponential growth longer than the other two algorithms do. Two algorithms which develop near optimal solutions are also examined.	algorithm;branch and bound;iterative method;karp's 21 np-complete problems;polynomial;recursion (computer science);shortest path problem;subroutine;time complexity;travelling salesman problem	Susan N. Twohig;Samuel O. Aletan	1990		10.1145/100348.100468	time complexity;nearest neighbour algorithm;mathematical optimization;exponential growth;np-complete;ramer–douglas–peucker algorithm;computer science;iterative method;shortest path problem;travelling salesman problem;branch and bound;nondeterministic algorithm;algorithm;recursion	Theory	24.95924439636198	17.19284590822602	190622
132e719bae06c86ae399760a94c0b28940a70992	alternative algorithms for counting all matchings in graphs	random graph;acoplamiento grafo;chaine markov;cadena markov;metodo monte carlo;efficient algorithm;error relativo;grafo aleatorio;methode monte carlo;graphe aleatoire;graph matching;couplage graphe;relative error;monte carlo method;vertex graph;echantillonnage importance;erreur relative;importance sampling;vertice grafo;sommet graphe;markov chain	We present two new methods for counting all matchings in a graph. Both methods are alternatives to methods based on the Markov Chains and both are unbiased. The first one is a generalization of a Godman-Godsil estimator. We show that it works in time O(1.0878n?-2) for general graphs. For dense graphs (every vertex is connected with at least (1/2 +?)n other vertices) it works in time O(n4+(6 ln 6)/??-2), where n is the number of vertices of a given graph and 0 < ? < 1 is an expected relative error. We also analyze the efficiency of the presented algorithm applied for random graphs. The second method uses importance sampling. This method works in exponential time but it can easily be enriched in some heuristics leading to very efficient algorithms in practice. Experiments show that our methods give better estimates than the Markow Chain approach.	algorithm;matching (graph theory)	Piotr Sankowski	2003		10.1007/3-540-36494-3_38	1-planar graph;random regular graph;pathwidth;random graph;markov chain;approximation error;combinatorics;discrete mathematics;independent set;importance sampling;metric dimension;vertex;mathematics;modular decomposition;chordal graph;indifference graph;statistics;matching;monte carlo method	Theory	38.46422558679364	14.913701654787133	190661
ba50e256fa43e5752fff132fae4be8b1282e2999	an algorithm for approximating piecewise linear concave functions from sample gradients	piecewise linear;almost sure convergence;resource allocation;approximation;stochastic gradient;stochastic gradient methods;adaptive estimation;concave functions	An e ective algorithm for solving stochastic resource allocation problems is to build piecewise linear, concave approximations of the recourse function based on sample gradient information. Algorithms based on this approach are proving useful in application areas such as the newsvendor problem, physical distribution and eet management. These algorithms require the adaptive estimation of the approximations of the recourse function that maintain concavity at every iteration. In this paper, we prove convergence for a particular version of an algorithm that produces approximations from stochastic gradient information while maintaining concavity.	algorithm;approximation;concave function;gradient descent;iteration;newsvendor model;piecewise linear continuation	Huseyin Topaloglu;Warren B. Powell	2003	Oper. Res. Lett.	10.1016/S0167-6377(02)00187-6	mathematical optimization;combinatorics;discrete mathematics;piecewise linear function;convergence of random variables;resource allocation;approximation;mathematics;concave function	ML	35.55796988089725	5.425509617477994	190815
4d892d6f7bfdb769a68ac431f63c9ab39818f0f8	some advanced techniques in reducing time for path planning based on visibility graph	graph theory;shortest path;path planning;geometry;reduction factor visibility graph polygon aggregation path planning combinatorial planning computing time;path planning geometry graph theory;path planning planning clustering algorithms complexity theory educational institutions robots merging;visibility graph;shortest path path planning visibility graph obstacle;obstacle	This paper describes some techniques based on polygon aggregation in reducing time for visibility graph in case of many obstacles. In path planning, the approaches are commonly used such as search-based, sampling-based or combinatorial planning. And visibility graph is one of the roadmaps of combinatorial planning. Building a visibility graph is a main phase in the whole process and theoretically it takes Î¸(nlogn). However, with some practical applications, for example one which has a large number of obstacles, this phase is very time-consuming. With the techniques proposed, the experiment result shows that the computing time gets a reduction factor of one-third approximately when the aggregation are used in preprocessing of building visibility graph.	motion planning;preprocessor;sampling (signal processing);visibility graph	Tran Thi Nhu Nguyet;Tran Van Hoai;Nguyen Anh Thi	2011	2011 Third International Conference on Knowledge and Systems Engineering	10.1109/KSE.2011.37	visibility graph;mathematical optimization;combinatorics;discrete mathematics;graph bandwidth;any-angle path planning;longest path problem;graph theory;mathematics;motion planning;shortest path problem	Robotics	30.596081069012286	14.871178804377477	190978
857c5ac2a4ecb1a9e686d77eee65ada2525ba00f	similarity-aware spectral sparsification by edge filtering		In recent years, spectral graph sparsification techniques that can compute ultra-sparse graph proxies have been extensively studied for accelerating various numerical and graph-related applications. Prior nearly-linear-time spectral sparsification methods first extract low-stretch spanning tree from the original graph to form the backbone of the sparsifier, and then recover small portions of spectrally-critical off-tree edges to the spanning tree to significantly improve the approximation quality. However, it is not clear how many off-tree edges should be recovered for achieving a desired spectral similarity level within the sparsifier. Motivated by recent graph signal processing techniques, this paper proposes a similarity-aware spectral graph sparsification framework that leverages efficient spectral off-tree edge embedding and filtering schemes to construct spectral sparsifiers with guaranteed spectral similarity (relative condition number) level. An iterative graph densification scheme is introduced to facilitate efficient and effective filtering of off-tree edges for highly ill-conditioned problems. The proposed method has been validated using various kinds of graphs obtained from public domain sparse matrix collections relevant to VLSI CAD, finite element analysis, as well as social and data networks frequently studied in many machine learning and data mining applications.	approximation;computer-aided design;condition number;data mining;file spanning;finite element method;internet backbone;iterative method;machine learning;numerical analysis;signal processing;spanning tree;sparse matrix;time complexity	Zhuo Feng	2018	2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)	10.1145/3195970.3196114	filter (signal processing);iterative method;discrete mathematics;sparse matrix;spectral graph theory;signal processing;finite element method;mathematics;graph partition;spanning tree;artificial intelligence;pattern recognition	EDA	35.17983796652422	10.420986923537022	191237
f289328f4e8e67c419c09cf67f0dc78830440ad2	facilities layout adjacency determination: an experimental comparison of three graph theoretic heuristics	482 applied graph theory;facility layout;184 facilities layout	The facilities layout problem is concerned with laying out facilities on a planar site in order to design systems that are as efficient as possible. One approach to the problem involves the use of REL charts, which are tables that estimate the desirability of locating facilities next to each other, and the construction of a maximum weight planar graph that represents an efficient layout design. This method is not a complete one, however, since it specifies only which facilities are to be adjacent. Nevertheless, whenever the analyst has a great deal of freedom of design, it is a useful tool in the initial stages of laying out a new system. In this paper, we describe three graph theoretic heuristics that attempt to determine an optimal planar adjacency graph from a REL chart. Our computational experience suggests that these methods can provide effective solutions to problems of the size frequently encountered in practice by designers.	heuristic (computer science)	Les R. Foulds;Peter B. Gibbons;J. W. Giffin	1985	Operations Research	10.1287/opre.33.5.1091	adjacency list;mathematical optimization;combinatorics;computer science	Vision	29.12366127700174	13.85158204381338	191740
34ae6ddde59c6e0ac4fb614484d19d824c912c39	two-dimensional cellular automata of radius one for density classification task p=1/2	second order;two dimensions;artificial tornasol classification;cellular automata 2d;density classification;binary classification;cellular automata	The study of a two-dimensional cellular automata (CA) able to perform density classification tasks for q 1⁄4 1 2 is hereby presented. Results show that, for two-dimension tasks, a perfect rule does not exist. However, the experiments show a good performance even when radius of neighborhood is minimum. The inherent space sensibility of CA is exploited for textural binary classification. Communication properties of CA are observed from the viewpoint of second order cybernetics. 2003 Elsevier B.V. All rights reserved.	automata theory;binary classification;cellular automaton;discrete system;experiment;francisco varela;humberto maturana;linear algebra;majority problem (cellular automaton);second-order cybernetics;simulation;transmitter	Rene Reynaga;Eligio Amthauer	2003	Pattern Recognition Letters	10.1016/S0167-8655(03)00143-0	binary classification;cellular automaton;two-dimensional space;computer science;theoretical computer science;machine learning;mathematics;second-order logic;algorithm	ML	39.09318414971699	8.870005816426428	191750
79c5b4dfcb405c482c47d2ac3d48aec530f31c3e	quality guarantees for region optimal dcop algorithms	approximate algorithm;bound;dcop;region optimality	kand t-optimality algorithms [9, 6] provide solutions to DCOPs that are optimal in regions characterized by its size and distance respectively. Moreover, they provide quality guarantees on their solutions. Here we generalise the kand t-optimal framework to introduce C-optimality, a flexible framework that provides reward-independent quality guarantees for optima in regions characterised by any arbitrary criterion. Therefore, C-optimality allows us to explore the space of criteria (beyond size and distance) looking for those that lead to better solution qualities. We benefit from this larger space of criteria to propose a new criterion, the socalled size-bounded-distance criterion, which outperforms kand t-optimality.	approximation algorithm;dcop;mathematical optimization	Meritxell Vinyals;Eric Anyung Shieh;Jesús Cerquides;Juan A. Rodríguez-Aguilar;Zhengyu Yin;Milind Tambe;Emma Bowring	2011			mathematical optimization	AI	29.45298156668992	8.636723455846528	191997
6cceb2e240e9012d63c15700037b5ca764762a3d	environment characterization for non-recontaminating frontier-based robotic exploration	mathematical morphology;graph combinatorics;frontier based exploration;environment characterization;game determinacy;distributed pursuitevasion;swarm robotics;planar geometry	This paper addresses the problem of obtaining a concise description of a physical environment for robotic exploration. We aim to determine the number of robots required to clear an environment using non-recontaminating exploration. We introduce the medial axis as a configuration space and derive a mathematical representation of a continuous environment that captures its underlying topology and geometry. We show that this representation provides a concise description of arbitrary environments, and that reasoning about points in this representation is equivalent to reasoning about robots in physical space. We leverage this to derive a lower bound on the number of required pursuers. We provide a transformation from this continuous representation into a symbolic representation. Finally, we present a generalized pursuit-evasion algorithm. Given an environment we can compute how many pursuers we need, and generate an optimal pursuit strategy that will guarantee the evaders are detected with the minimum number of pursuers.	algorithm;apache axis;approximation;blocking (computing);evasion (network security);exploration problem;graph (discrete mathematics);heuristic (computer science);ibm notes;medial graph;np-hardness;pursuit-evasion;robot;robotic spacecraft;smart;traverse	Mikhail Volkov;Alejandro Cornejo;Nancy A. Lynch;Daniela Rus	2011		10.1007/978-3-642-25044-6_5	swarm robotics;mathematical optimization;simulation;mathematical morphology;computer science;artificial intelligence;plane;distributed computing	AI	31.767228346696843	13.905632238602992	192001
47ef8d24fad0a7cd176ef89795a341eb2ec25c5e	a fast sequential method for polygonal approximation of digitized curves	algorithme rapide;image processing;algorithm analysis;traitement image;approximation polygonale;fast algorithm;analyse algorithme;polygonal approximation;courbe planaire	A new and very fast algorithm for polygonal approximation is presented. It uses a scan-along technique where the approximation depends on the area deviation for each line segment. The algorithm outputs a new line segment when the area deviation per length unit of the current segment exceeds a prespecified value. Pictures are included, showing the application of the algorithm to contour coding of binary objects. Some useful simplifications of the algorithms are suggested. 9 1984 Academic Press, Inc.	algorithm;approximation	Karin Wall;Per-Erik Danielsson	1984	Computer Vision, Graphics, and Image Processing	10.1016/S0734-189X(84)80023-7	computer vision;mathematical optimization;combinatorics;image processing;computer science;mathematics;algorithm	ML	30.627991042328397	16.11029907313704	192045
2dc32c43f4a9cb59d2ac7ad839eb0862cde3f1bc	geometric computations on indecisive and uncertain points		We study computing geometric problems on uncertain points. An uncertain point is a point that does not have a fixed location, but rather is described by a probability distribution. When these probability distributions are restricted to a finite number of locations, the points are called indecisive points. In particular, we focus on geometric shape-fitting problems and on building compact distributions to describe how the solutions to these problems vary with respect to the uncertainty in the points. Our main results are: (1) a simple and efficient randomized approximation algorithm for calculating the distribution of any statistic on uncertain data sets; (2) a polynomial, deterministic and exact algorithm for computing the distribution of answers for any LP-type problem on an indecisive point set; and (3) the development of shape inclusion probability (SIP) functions which captures the ambient distribution of shapes fit to uncertain or indecisive point sets and are admissible to the two algorithmic constructions.	approximation algorithm;computation;exact algorithm;lp-type problem;polynomial;randomized algorithm;uncertain data	Allan Jørgensen;Maarten Löffler;Jeff M. Phillips	2012	CoRR		mathematical optimization;combinatorics;discrete mathematics;mathematics;geometry	Theory	27.876169029521623	15.306315846441445	192898
22406af7b75589f454912ab35e1ab500029fd0f3	a unified framework for approximating and clustering data	epsilon nets;pac learning;epsilon approximation;k means;k median;optimization problem;regression;science learning;svd;clustering;linear time;cur;approximation scheme;coresets;clustered data;pca;vc dimension;approximating	"""Given a set F of n positive functions over a ground set X, we consider the problem of computing x* that minimizes the expression ∑f ∈ Ff(x), over x ∈ X. A typical application is shape fitting, where we wish to approximate a set P of n elements (say, points) by a shape x from a (possibly infinite) family X of shapes. Here, each point p ∈ P corresponds to a function f such that f(x) is the distance from p to x, and we seek a shape x that minimizes the sum of distances from each point in P. In the k-clustering variant, each x\in X is a tuple of k shapes, and f(x) is the distance from p to its closest shape in x.  Our main result is a unified framework for constructing coresets and approximate clustering for such general sets of functions. To achieve our results, we forge a link between the classic and well defined notion of ε-approximations from the theory of PAC Learning and VC dimension, to the relatively new (and not so consistent) paradigm of coresets, which are some kind of """"compressed representation"""" of the input set F. Using traditional techniques, a coreset usually implies an LTAS (linear time approximation scheme) for the corresponding optimization problem, which can be computed in parallel, via one pass over the data, and using only polylogarithmic space (i.e, in the streaming model). For several function families F for which coresets are known not to exist, or the corresponding (approximate) optimization problems are hard, our framework yields bicriteria approximations, or coresets that are large, but contained in a low-dimensional space.  We demonstrate our unified framework by applying it on projective clustering problems. We obtain new coreset constructions and significantly smaller coresets, over the ones that appeared in the literature during the past years, for problems such as: k-Median [Har-Peled and Mazumdar,STOC'04], [Chen, SODA'06], [Langberg and Schulman, SODA'10]; k-Line median [Feldman, Fiat and Sharir, FOCS'06], [Deshpande and Varadarajan, STOC'07]; Projective clustering [Deshpande et al., SODA'06] [Deshpande and Varadarajan, STOC'07]; Linear lp regression [Clarkson, Woodruff, STOC'09 ]; Low-rank approximation [Sarlos, FOCS'06]; Subspace approximation [Shyamalkumar and Varadarajan, SODA'07], [Feldman, Monemizadeh, Sohler and Woodruff, SODA'10], [Deshpande, Tulsiani, and Vishnoi, SODA'11].  The running times of the corresponding optimization problems are also significantly improved. We show how to generalize the results of our framework for squared distances (as in k-mean), distances to the qth power, and deterministic constructions."""	approximation algorithm;cluster analysis;coreset;entity–relationship model;forge;low-rank approximation;mathematical optimization;optimization problem;polylogarithmic function;probably approximately correct learning;programming paradigm;srinidhi varadarajan;time complexity;unified framework;vc dimension	Dan Feldman;Michael Langberg	2011		10.1145/1993636.1993712	time complexity;optimization problem;mathematical optimization;combinatorics;discrete mathematics;regression;vc dimension;computer science;machine learning;mathematics;cluster analysis;singular value decomposition;probably approximately correct learning;statistics;k-means clustering;principal component analysis	Theory	25.421394353364757	17.91682137898916	193280
b93758f9b5c904cff32de6ed526900da679bf4a1	iterative algorithm for finding frequent patterns in transactional databases	frequent pattern;iterative algorithm	A high-performance algorithm for searching for frequent patterns (FPs) in transactional databases is presented. The search for FPs is carried out by using an iterative sieve algorithm by computing the set of enclosed cycles. In each inner cycle of level m FPs composed of m elements are generated. The assigned number of enclosed cycles (the parameter of the problem) defines the maximum length of the desired FPs. The efficiency of the algorithm is produced by (i) the extremely simple logical searching scheme, (ii) the avoidance of recursive procedures, and (iii) the usage of only one-dimensional arrays of integers.	algorithm;database;iterative method;recursion	Gennady P. Berman;Vyacheslav N. Gorshkov;Edward P. MacKerrow;Xidi Wang	2005	CoRR		computer science;theoretical computer science;data mining;database;iterative method;algorithm	DB	29.11764810183219	12.962149381618518	193394
38cca85a3c6f7dba0c126dcd3c0373334f1273d7	random cluster dynamics for the ising model is rapidly mixing		We show that the mixing time of Glauber (single edge update) dynamics for the random cluster model at q = 2 is bounded by a polynomial in the size of the underlying graph. As a consequence, the Swendsen-Wang algorithm for the ferromagnetic Ising model at any temperature has the same polynomial mixing time bound.	directed graph;glauber;ising model;polynomial;swendsen–wang algorithm	Heng Guo;Mark Jerrum	2017			competitive analysis;online algorithm;mathematical optimization;combinatorics;discrete mathematics;computer science;theoretical computer science;mathematics	Theory	37.68749293893078	14.597807277161731	193726
2ac3013f6cd706d9b41f4a226f1a74891304054e	robust proximity queries: an illustration of degree-driven algorithm design	geometric computing;arithmetic operation;diagramme voronoi;geometrie algorithmique;operation arithmetique;computational geometry;algorithme;arithmetic precision;algorithm;proximity queries;robustesse;robustness;geometria computacional;68u05;65y25;diagrama voronoi;operation aritmetica;algorithm design;voronoi diagram;robustez;algoritmo	In the context of methodologies intended to confer robustness to geometric algorithms, we elaborate on the exact-computation paradigm and formalize the notion of degree of a geometric algorithm as a worst-case quantification of the precision (number of bits) to which arithmetic calculation have to be executed in order to guarantee topological correctness. We also propose a formalism for the expeditious evaluation of algorithmic degree. As an application of this paradigm and an illustration of our general approach where algorithm design is driven also by the degree, we consider the important classical problem of proximity queries in two and three dimensions and develop a new technique for the efficient and robust execution of such queries based on an implicit representation of Voronoi diagrams. Our new technique offers both low degree and fast query time and for 2D queries is optimal with respect to both cost measures of the paradigm, asymptotic number of operations, and arithmetic degree.	algorithm design	Giuseppe Liotta;Franco P. Preparata;Roberto Tamassia	1998	SIAM J. Comput.	10.1137/S0097539796305365	algorithm design;mathematical optimization;combinatorics;voronoi diagram;computational geometry;computer science;theoretical computer science;mathematics;algorithm;robustness	Theory	32.631033338903514	13.772938163338367	193942
cb47d46b250e2ccca1698fded4d83a2fbf60d3f1	image reconstruction on hypercube computers: application to electron microscopy	hypercube;computer program;representation tridimensionnelle;parallel computation;algorithme;algorithm;reconstruction image;electron microscopy;reconstruccion imagen;image reconstruction;three dimensional representation;computer application;filtered backprojection;programa computador;hypercube computers;representacion tridimensional;programme ordinateur;algoritmo;hipercubo	Abstract   Filtered backprojection is a popular algorithm for the reconstruction of  n -dimensional signals from their ( n  − 1)-dimensional projections (in the sense of line integrals). Here we specifically treat the problem of the 3-dimensional (3D) reconstruction of an object from its 2-dimensional (2D) projection images. In this work we perform the implementation of the filtered backprojection method in hypercube computers. The parallel algorithm is general in the sense that it does not impose any restriction in the problem space dimensions and is adaptable to any hypercube dimension. The flexibility of the algorithm is rooted in the methodology developed for embedding algorithms into hypercubes. Finally, we analyze the complexity of the parallel algorithm and apply the parallel algorithm to the 3-dimensional reconstruction of the oligomer formed by the chaperonin GroEL from E.coli.	computer;electron;iterative reconstruction	Emilio L. Zapata;José Ignacio Benavides Benítez;Francisco F. Rivera;Javier D. Bruguera;Tomás F. Pena;José María Carazo	1992	Signal Processing	10.1016/0165-1684(92)90111-9	iterative reconstruction;mathematical optimization;theoretical computer science;mathematics;geometry;algorithm;electron microscope;hypercube	Arch	34.10925587087324	11.612990418711885	194184
a2a1142636fc1e95c9ac2ed21549297ee780926f	test problem generator for the multidimensional assignment problem	linear assignment problem;multidimensional assignment problem;test problems	The multidimensional assignment problem (MAPs) is a higher dimensional version of the standard linear assignment problem. Test problems of known solution are useful in exercising solution methods. A method of generating an axial MAP of controllable size with a known unique solution is presented. Certain characteristics of the generated MAPs that determine realism and difficulty are investigated.	assignment problem	Don A. Grundel;Panos M. Pardalos	2005	Comp. Opt. and Appl.	10.1007/s10589-005-4558-6	mathematical optimization;combinatorics;discrete mathematics;linear bottleneck assignment problem;generalized assignment problem;mathematics;assignment problem;weapon target assignment problem;quadratic assignment problem	EDA	26.451610407140844	5.573092617835526	194233
be4a13fb4624abd071fd22ba8512921ad0de3e95	hidden surface removal with respect to a moving view point	hidden surface removal	We give two algorithms for hidden surface removal with respect to a moving viewpoint. The first algorithm preprocesses the set A of scene polygons to build a certain cylindrical partition H(A) of R3 of at most quadratic, but typically much smaller, size. After this any given viewpoiut v can be located in H(A) in poly-logarithmic time, and the view visible from v can be constructed in time proportional to the size of the complex View(o) n H(A) (up to a log factor), where View(v) is the star shaped polyhedron consisting of the visible points in R3. The second algorithm is designed to exploit coherence between successive scenes in case the view point is moving on a linear trajectory. Once the scene at the initial viewpoint is constructed, it generates all successive scenes on the the trajectory in time that is proportional to the number of “semi-opaquen topological changes in the scene, as the viewpoint is moved on its trajectory continuously. The algorithm is based on a certain View Dependent Partition (VDP) of R3 that is induced by the scene polygons. It depends on the position as well as the trajectory of the view point. It is used to predict efficiently when and where the topological changes in the scene occur. A nice feature of the above algorithm, based on VDP, is that it can be easily extended to deal with moving scene polygons as well, assuming that the trajectories of the scene vertices are rationally parametrized by t and have a bounded degree.	algorithm;hidden surface determination;polyhedron;semiconductor industry;time complexity;variable data publishing	Ketan Mulmuley	1991		10.1145/103418.103471	hidden surface determination;computer science;mathematics	Theory	32.13977536236736	18.179576137313816	194636
d5335ae3c20eec3122f3426a0614ddaf734c2ff2	on the structure of real-valued one-dimensional cellular automata	hypercyclicity;periodic points;real valued states;cellular automata	It is well-known that binary-valued cellular automata, which are defined by simple local rules, have the amazing feature of generating very complex patterns and having complicated dynamical behaviors. In this paper, we present a new type of cellular automaton based on real-valued states which produce an even greater amount of interesting structures such as fractal, chaotic and hypercyclic. We also give proofs to real-valued cellular systems which have fixed points and periodic solutions.	automata theory;binary data;chaos theory;dynamical system;elementary cellular automaton;fixed point (mathematics);fractal	Xu Xu;Stephen P. Banks;Mahdi Mahfouf	2011	I. J. Bifurcation and Chaos	10.1142/S0218127411029240	stochastic cellular automaton;cellular automaton;reversible cellular automaton;block cellular automaton;combinatorics;mathematical analysis;discrete mathematics;continuous spatial automaton;quantum cellular automaton;asynchronous cellular automaton;continuous automaton;mathematics;mobile automaton	Theory	39.14040245532305	9.067680803756188	195046
b0cc630003d7fb711b5d358cc34427a29bba42d8	active set methods with reoptimization for convex quadratic integer programming		We present a fast branch-and-bound algorithm for solving convex quadratic integer programs with few linear constraints. In each node, we solve the dual problem of the continuous relaxation using an infeasible active set method proposed by Kunisch and Rendl [11] to get a lower bound; this active set algorithm is well suited for reoptimization. Our algorithm generalizes a branch-and-bound approach for unconstrained convex quadratic integer programming proposed by Buchheim, Caprara and Lodi [5] to the presence of linear constraints. The main feature of the latter approach consists in a sophisticated preprocessing phase, leading to a fast enumeration of the branch-and-bound nodes. Experimental results for randomly generated instances are presented. The new approach significantly outperforms the MIQP solver of CPLEX 12.4 for instances with a small number of constraints.	active set method;algorithm;branch and bound;cplex;duality (optimization);integer programming;linear programming relaxation;objective-c;preprocessor;procedural generation;solver	Christoph Buchheim;Long Trieu	2014		10.1007/978-3-319-09174-7_11	mathematical optimization;combinatorics;discrete mathematics;mathematics;active set method	ML	25.987450945697947	8.159860239948046	195129
1c918ea82679b3a6d13aeb0d333c13c18f494916	a variation of two-stage sbm with leader-follower structure: an application to chinese commercial banks		The two-stage slack-based measure (SBM) model has many applications in the real world. Due to the limitations of the SBM model on which it is based, the two-stage SBM model unfortunately gives unrealistically low efficiencies and rather far projections (Tone in Eur J Oper Res 197(1):243–252, 2010) for inefficient decisionmaking unit. Based on the novel idea in Tone (2010), this paper proposes a variation of the two-stage SBM model by incorporating a leader–follower structure and applies the proposed approach to Chinese commercial banks. The results show that our proposed approach can increase efficiencies of inefficient banks and halve the projection distance of some inefficient banks. Journal of the Operational Research Society (2017). doi:10.1057/s41274-017-0262-z	academy;maxima and minima;maximal set;multidimensional digital pre-distortion;numerical analysis;radial (radio);slack variable;super bit mapping	Yuanchang Zhu;Yongjun Li;Liang Liang	2018	JORS	10.1057/s41274-017-0262-z	operations management;computer science	NLP	29.068365636170363	9.485991429829829	195234
dd21269158a550ddbbafb7ee6af5946f32c079d4	a probabilistic analysis of multidimensional bin packing problems	bin packing problem;digital disk;algorithm;digital region;compactness;uniform distribution;digital convexity	This paper gives probabilistic analyses of two kinds of multidimensional bin packing problems: vector packing and rectangle packing. In the vector packing problem each of the <italic>d</italic> dimensions can be interpreted as a resource. A given object <italic>i</italic> consumes <italic>a</italic><subscrpt>ij</subscrpt> units of the <italic>j</italic><supscrpt>th</supscrpt> resource, and the objects packed in any given bin may not collectively consume more than one unit of any resource. Subject to this constraint, the objects are to be packed into a minimum number of bins. The rectangle packing problem is more geometric in character. The <italic>i</italic><supscrpt>th</supscrpt> object is a <italic>d</italic>-dimensional box whose <italic>j</italic><supscrpt>th</supscrpt> side is of length <italic>a</italic><subscrpt>ij</subscrpt>, and the goal is to pack the objects into a minimum number of cubical boxes of side 1.  We study these problems on the assumption that the <italic>a</italic><subscrpt>ij</subscrpt> are drawn independently from the uniform distribution over [0,1]. We study a vector packing heuristic called VPACK that tries to place two objects in each bin and a rectangle packing heuristic called RPACK that tries to pack one object into each of the 2<supscrpt>d</supscrpt> corners of each bin. We show that each of these heuristics tends to produce packings in which very little of the capacity of the bins is wasted. In the case of rectangle packing, we show that the results can be extended to a wide class of distributions of the piece sizes.	bin packing problem;heuristic (computer science);probabilistic analysis of algorithms;set packing	Richard M. Karp;Michael Luby;Alberto Marchetti-Spaccamela	1984		10.1145/800057.808693	mathematical optimization;packing problems;combinatorics;bin packing problem;mathematics;geometry;uniform distribution;compact space;bin;algorithm;square packing in a square	Theory	28.336302610103463	15.569059226222862	195236
15cfa64511f169972feba1031ece0eee0925e03a	an iterated graph laplacian approach for ranking on manifolds	green s function;information retrieval;ranking function;analytical method;ranking algorithm;euclidean space;iterated graph laplacian;graph laplacian	"""Ranking is one of the key problems in information retrieval. Recently, there has been significant interest in a class of ranking algorithms based on the assumption that data is sampled from a low dimensional manifold embedded in a higher dimensional Euclidean space.  In this paper, we study a popular graph Laplacian based ranking algorithm [23] using an analytical method, which provides theoretical insights into the ranking algorithm going beyond the intuitive idea of """"diffusion."""" Our analysis shows that the algorithm is sensitive to a commonly used parameter due to the use of symmetric normalized graph Laplacian. We also show that the ranking function may diverge to infinity at the query point in the limit of infinite samples. To address these issues, we propose an improved ranking algorithm on manifolds using Green's function of an iterated unnormalized graph Laplacian, which is more robust and density adaptive, as well as pointwise continuous in the limit of infinite samples.  We also for the first time in the ranking literature empirically explore two variants from a family of twice normalized graph Laplacians. Experimental results on text and image data support our analysis, which also suggest the potential value of twice normalized graph Laplacians in practice."""	algorithm;embedded system;information retrieval;iteration;laplacian matrix;ranking (information retrieval)	Xueyuan Zhou;Mikhail Belkin;Nathan Srebro	2011		10.1145/2020408.2020556	algebraic connectivity;lattice graph;graph power;mathematical optimization;combinatorics;discrete mathematics;graph bandwidth;laplacian matrix;null graph;graph property;euclidean space;mathematics;voltage graph;green's function;spectral graph theory;quartic graph;complement graph;line graph;string graph;strength of a graph	ML	35.46044228840977	10.562188246947285	195723
5d115e1fa26a63ec77ed2b901d4dd69f2e994a29	on-line learning with linear loss constraints	constraint matrix;systeme avec perte;matrice contrainte;non linear programming;learning algorithm;teorema existencia;systeme apprentissage;programacion no lineal;linear constrait;coaccion;predicat contrainte;contrainte;existence theorem;programmation non lineaire;algorithme apprentissage;satisfiability;fonction perte;funcion perdida;constraint satisfaction;learning systems;satisfaction contrainte;constraint;mathematical programming;loss function;loss system;value function;contrainte lineaire;satisfaccion restriccion;algoritmo optimo;algorithme optimal;optimal algorithm;algoritmo aprendizaje;programmation mathematique;theoreme existence;programacion matematica;on line learning;constraint predicate;sistema con perdida	"""We consider a generalization of the mistake-bound model (for learning {0, 1}-valued functions) in which the learner must satisfy a general constraint on the number M""""+ of incorrect 1 predictions and the number M""""- of incorrect 0 predictions. We describe a general-purpose optimal algorithm for our formulation of this problem. We describe several applications of our general results, involving situations in which the learner wishes to satisfy linear inequalities in M""""+ and M""""-."""		David P. Helmbold;Nick Littlestone;Philip M. Long	2000	Inf. Comput.	10.1006/inco.2000.2871	constraint satisfaction;nonlinear programming;computer science;calculus;mathematics;bellman equation;constraint;algorithm;satisfiability;loss function	Theory	29.882424924595696	9.610189679744542	195915
16146a1280c283615de57c57e0547da1e6526e6b	exact and approximation algorithms for clustering	approximate algorithm;aproximacion;metric;calculo automatico;classification;computing;approximation;algorithme;calcul automatique;resolucion problema;algorithm;clustering;metrico;clasificacion;metrique;problem solving;resolution probleme;algoritmo	In this paper we present an n^ O(k 1-1/d ) -time algorithm for solving the k -center problem in \reals d , under L ∈ fty - and L 2 -metrics. The algorithm extends to other metrics, and to the discrete k -center problem. We also describe a simple (1+ɛ) -approximation algorithm for the k -center problem, with running time O(nlog  k) + (k/ɛ)^ O(k 1-1/d ) . Finally, we present an n^ O(k 1-1/d ) -time algorithm for solving the L -capacitated k -center problem, provided that L=Ω(n/k 1-1/d ) or L=O(1) .	approximation algorithm;time complexity	Pankaj K. Agarwal;Cecilia M. Procopiuc	1998	Algorithmica	10.1007/s00453-001-0110-y	combinatorics;computing;metric;computer science;approximation;calculus;mathematics;algorithm	Theory	28.660234734682007	16.50251291336167	196405
e4cfb44f1fe90a850657af7c7689dcac12e03727	metaheuristic algorithms and tree decomposition		This chapter deals with the application of evolutionary approaches and other metaheuristic techniques for generating tree decompositions. Tree decomposition is a concept introduced by Robertson and Seymour [34] and it is used to characterize the difficulty of constraint satisfaction and NP-hard problems that can be represented as a graph. Although in general no polynomial algorithms have been found for such problems, particular instances can be solved in polynomial time if the treewidth of their corresponding graph is bounded by a constant. The process of solving problems based on tree decomposition comprises two phases. First, a decomposition with small width is generated. Basically in this phase the problem is divided into several sub-problems, each included in one of the nodes of the tree decomposition. The second phase includes solving a problem (based on the generated tree decomposition) with a particular algorithm such as dynamic programming. The main idea is that by decomposing a problem into sub-problems of limited size, the whole problem can be solved more efficiently. The time for solving the problem based on its tree decomposition usually depends on the width of the tree decomposition. Thus it is of high interest to generate tree decompositions having small widths. Finding the treewidth of a graph is an NP-hard problem [2]. In order to solve this problem different algorithms have been proposed in the literature. Exact methods such as branch and bound techniques can be used only for small graphs. Therefore, metaheuristic algorithms based on genetic algorithms [18] , simulated annealing [22], tabu search [14], iterated local search [29] , and antcolony optimization ([7], [9]) have been proposed in the literature to generate good upper bounds for larger graphs. Such techniques have been applied very successfully and they are able to find the best existing upper bounds for many benchmark problems in the literature. In this chapter we will first introduce the concept of tree decomposition, and then give a survey on metaheuristic techniques used to generate tree decompositions. Three approaches based on genetic algorithms, iterated local search and ant-colony optimization that were proposed in the literature will be described in detail. Finally, we will also mention briefly two recent approaches that exploit tree decompositions within metaheuristic search.	ant colony optimization algorithms;benchmark (computing);branch and bound;constraint satisfaction;dynamic programming;genetic algorithm;heuristic;iterated local search;iteration;iterative method;local search (optimization);mathematical optimization;memetic algorithm;metaheuristic;np-hardness;polynomial;simulated annealing;tabu search;time complexity;tree decomposition;treewidth	Thomas Hammerl;Nysret Musliu;Werner Schafhauser	2015		10.1007/978-3-662-43505-2_64	parallel metaheuristic;tabu search	AI	24.669512111293837	4.698564261282708	196448
16799c678718add56d107c631f8c0f77d9cbddcb	on approximating the depth and related problems	disque;algorithme rapide;comptage;68w40;complejidad espacio;randomized algorithms;disk;plane;time complexity;random sampling;resolution math;computational geometry;plan;pregunta documental;depth;space time;espacio tiempo;contaje;68wxx;disco;approximation;complexite temps;probleme recouvrement;programacion lineal;problema recubrimiento;fast algorithm;muestreo aleatorio;plano;region;computational geometry 68w25;profundidad;counting;arreglo;linear programming;space complexity;query;programmation lineaire;resolucion matematica;arrangement;profondeur;complexite espace;covering problem;complejidad tiempo;echantillonnage aleatoire;05b40;68w25;solving;algoritmo rapido;espace temps;requete	In this paper, we study the problem of finding a disk covering the largest number of red points, while avoiding all the blue points. We reduce it to the question of finding a deepest point in an arrangement of pseudodisks and provide a near-linear expected-time randomized approximation algorithm for this problem. As an application of our techniques, we show how to solve linear programming with violations approximately. We also prove that approximate range counting has roughly the same time and space complexity as answering emptiness range queries.	approximation algorithm;counting problem (complexity);dspace;linear programming;randomized algorithm;range query (data structures)	Boris Aronov;Sariel Har-Peled	2005		10.1137/060669474	time complexity;sampling;region;computational geometry;linear programming;approximation;plane;calculus;space time;mathematics;geometry;dspace;randomized algorithm;plan;counting;algorithm	Theory	29.44967438196907	16.892605452764666	196834
c734d10c50c1da1aca7432e8c24ec2dc89292ad6	computing dominances in e^n	espacio n dimensiones;multidimensional space;espace n dimensions;dominance;complexite calcul;complejidad calculo;geometrie algorithmique;computational geometry;computing complexity;dominancia;computational complexity;espacio euclides;geometria algoritmica;espace euclide;euclid space	We show the following: Given an n-point set X⊆E n , the dominance relation on X can be computed in time O(n 3/2 M(n) 3/2 ), where M(n) denotes the time needed to multiply two nxn matrices over Z n+1 .		Jirí Matousek	1991	Inf. Process. Lett.	10.1016/0020-0190(91)90071-O	computational geometry;computer science;calculus;mathematics;geometry;dominance;computational complexity theory;algorithm	DB	29.807778742058613	16.251251131994486	197149
09043083a78c83e0f0cc7729819d01d318f2e315	nfl theorem is unusable on structured classes of problems	optimisation;graph colouring computational complexity optimisation combinatorial mathematics search problems;combinatorial optimization problem;optimization problem;testing polynomials np complete problem logic information theory compression algorithms mathematics data compression optimization methods application software;computational complexity;heuristics nfl theorem structured problem classes heuristic metaheuristics no free lunch theorem testbed problem combinatorial optimization problems optimization problem computational complexity k coloring problems polynomial reduction np complete problems structure preservation neighborhood search operators;structure preservation;search problems;neighborhood search;combinatorial mathematics;np complete problem;no free lunch;graph colouring	"""Nowadays, in the heuristic and metaheuristics community, there is a schism between researchers who say: """"we proved experimentally that a given heuristic provides good results on a given problem and we can guess that it is enough general to be applied for other problems"""", and those who claim: """"the No Free Lunch theorem (NFL) proves that there exists no absolute efficient heuristic"""". The formers suspect the existence of a structure in the solved problem and that structure can occur in other problems. The latters fear that heuristics are especially adapted for the testbed problem this paper addresses structural aspect of combinatorial optimization problems. In a first time, we recall some related works which provide a frame to our work. Particularly, we recall the existence of deceptive problems which are proved to be hard to optimize, the definitions of the five scenarios of knowledge in optimization problem, and some works which already discuss the reach of NFL theorem. In the next part, we give a short overview of how NFL works and discuss its significance with regards to complexity. This leads to the observation that the notion of structure of optimization problems is missing in NFL use. Then, we prove that k-coloring problems respect such a notion of structure, for any k. In the last part we discuss the relevance of our work on four points: the polynomial reduction of NP-complete problems and structure preservation, the connection between our work and the study which squeeze NFL using neighborhood search operators, the position of our study on the five scenarios of knowledge, and finally the difference between metaheuristics and heuristics."""	combinatorial optimization;complexity;experiment;graph coloring;heuristic (computer science);karp's 21 np-complete problems;local search (optimization);mathematical optimization;metaheuristic;nfl;no free lunch theorem;optimization problem;polynomial;relevance;testbed;usability	Benjamin Weinberg;El-Ghazali Talbi	2004	Proceedings of the 2004 Congress on Evolutionary Computation (IEEE Cat. No.04TH8753)	10.1109/CEC.2004.1330860	computational problem;no free lunch in search and optimization;optimization problem;mathematical optimization;combinatorics;np-complete;combinatorial optimization;computer science;artificial intelligence;machine learning;karp's 21 np-complete problems;mathematics;co-np-complete;computational complexity theory;l-reduction;algorithm;quadratic assignment problem	AI	27.577398149694638	4.7954657918123225	197184
911d048ae2025f87d93b18aab7783b3292aa71a2	exponential lower bounds for policy iteration	liverpool;repository;optimality criteria;infinite horizon;university;policy iteration;markov decision process;data structure;lower bound	We study policy iteration for infinite-horizon Markov decision processes. It has recently been shown policy iteration style algorithms have exponential lower bounds in a two player game setting. We extend these lower bounds to Markov decision processes with the total reward and average-reward optimality criteria.	algorithm;iteration;markov chain;markov decision process;time complexity	John Fearnley	2010		10.1007/978-3-642-14162-1_46	markov decision process;mathematical optimization;combinatorics;data structure;partially observable markov decision process;computer science;mathematics;markov model;mathematical economics;upper and lower bounds;programming language	ML	37.881356279667685	5.104012367423348	197285
0eec6f43f4f492a2ad141d5ef4a3a0d00f8abe24	new results on intersection query problems	search problem;intersections;geometrie algorithmique;two dimensions;computational geometry;problema investigacion;2 dimensional;algorithme;algorithms;intersection;probleme recherche	"""We present simple algorithms for three problems belonging to the class of intersection query problems. The """"rst algorithm deals with the static rectangle enclosure problem and can easily be extended to d dimensions, the second algorithm copes with the generalized c-oriented polygon intersection searching problem in two dimensions, while the third solves the static 2-dimensional dominance searching problem with respect to a set of obstacles. All algorithms are simple, are based on persistence and improve previous bounds. Also, as a corollary of the """"rst algorithm, we present a result for the static d-dimensional range searching problem."""	algorithm;persistence (computer science);range searching	Panayiotis Bozanis;Nectarios Kitsios;Christos Makris;Athanasios K. Tsakalidis	1997	Comput. J.	10.1093/comjnl/40.1.22	intersection;combinatorics;two-dimensional space;computational geometry;mathematics;geometry;algorithm	Theory	30.07168035466483	17.678882671569063	197382
0e75ff2a6341dfbbb6aa5a9b6da944703b2e630c	an exact minimum zone solution for sphericity evaluation	zone minimale;sphericite;diagramme voronoi;modele mathematique;modelo matematico;coordinate measuring machine;algorithme;algorithm;temps calcul;segment droite;mathematical model;segmento recta;evaluation;line segment;maquina medida coordenada;evaluacion;region maximale;tiempo computacion;computation time;max region;diagrama voronoi;sphericity;esfericidad;minimum zone;voronoi diagram;algoritmo;machine mesure coordonnee	Two theorems giving the exact minimum zone solution for sphericity evaluation are developed in this paper. These theorems build upon the 3D Voronoi diagrams to show that the exact sphericity is feasible using the max regions. The max region is a space bound by 3D Voronoi diagrams that the concentric circumscribed and inscribed spheres to the measured point set are determined by two control points of the max region. The first theorem shows that the exact minimum sphericity can be obtained only at an X-type vertex in the max region. Theorem 2 indicates that the minimum sphericity is determined by a small number of measured points that gives an efficient way to solve the minimum zone solution.		Jyunping Huang	1999	Computer-Aided Design	10.1016/S0010-4485(99)00072-X	combinatorics;sphericity;voronoi diagram;line segment;evaluation;calculus;mathematical model;mathematics;geometry	EDA	29.789807955044264	16.72906335455117	197959
2649d9022b6eebfac7f573c031659e22d86b5b21	a spectral clustering algorithm for manufacturing cell formation	graph clustering;spectral clustering;manufacturing cell formation;cell formation;bipartite graph;bipartite graph modeling	A graph clustering algorithm constructs groups of closely related parts and machines separately. After they are matched for the least intercell moves, a refining process runs on the initial cell formation to decrease the number of intercell moves. A simple modification of this main approach can deal with some practical constraints, such as the popular constraint of bounding the maximum number of machines in a cell. Our approach makes a big improvement in the computational time. More importantly, improvement is seen in the number of intercell moves when the computational results were compared with best known solutions from the literature.	algorithm;cluster analysis;spectral clustering	Sara Oliveira;J. F. F. Ribeiro;S. C. Seok	2009	Computers & Industrial Engineering	10.1016/j.cie.2009.04.008	correlation clustering;combinatorics;discrete mathematics;bipartite graph;computer science;simplex graph;machine learning;clustering coefficient;mathematics;voltage graph;spectral clustering	Robotics	25.0694768796754	6.327352491999207	198514
a9f01eb347c477bd5c5cd74e75cb791d574e2ca5	asymptotic theory of greedy approximations to minimal k-point random graphs	distribution;traveling salesman problem;graph theory;random graph;log weight;communication networks;quasi additive property;order statistic;minimal spanning tree;influence function;nonparametric estimation;contaminated mixture models;signal sampling;greedy approximations;1d rank order statistics;indexing terms;satisfiability;random communication network topologies;entropy estimation;epsiv;network topology;approximation theory;tree graphs;statistical distributions;robust nonparametric regression;graph weight;statistical analysis;independent and identically distributed;mixture model;almost sure limits;community networks;image registration asymptotic theory greedy approximations minimal k point random graphs independent identically distributed sample i i d sample multivariate distribution almost sure limits power weighted edge weight function partitioning method edge weight function quasi additive property k point minimal spanning tree steiner tree traveling salesman problem minimal weight function asymptotic sensitivity graph weight perturbations distribution influence function 1d rank order statistics log weight nonparametric estimate renyi entropy random communication network topologies mixing coefficient estimation spl epsiv contaminated mixture models outlier discrimination outlier rejection clustering pattern recognition robust nonparametric regression two sample matching;clustering;mixing coefficient estimation;image registration;travelling salesman problems;random processes;minimal k point random graphs;partitioning method;minimal weight function;pattern recognition;nonparametric regression;nonparametric estimate;perturbations;edge weight function;multivariate distribution;robustness;image analysis;two sample matching;power weighted edge weight function;pattern analysis;entropy;traveling salesman problems;weight function;k point minimal spanning tree;outlier discrimination;combinatorial optimization;i i d sample;asymptotic sensitivity;renyi entropy	Let Xn = fx1; : : : ; xng, be an i.i.d. sample having multivariate distribution P . We derive a.s. limits for the power weighted edge weight function of greedy approximations to a class of minimal graphs spanning k of the n samples. The class includes minimal k-point graphs constructed by the partitioning method of Ravi, Sundaram, Marathe, Rosenkrantz and Ravi [43] where the edge weight function satis es the quasi-additive property of Redmond and Yukich [45]. In particular this includes greedy approximations to the k-point minimal spanning tree (k-MST), Steiner tree (k-ST), and the traveling salesman problem (k-TSP). An expression for the in uence function of the minimal weight function is given which characterizes the asymptotic sensitivity of the graph weight to perturbations in the underlying distribution. The in uence function takes a form which indicates that the k-point minimal graph in d > 1 dimensions has robustness properties in IR which are analogous to those of rank order statistics in one dimension. A direct result of our theory is that the log-weight of the k-point minimal graph is a consistent nonparametric estimate of the R enyi entropy of the distribution P . Possible applications of this work include: analysis of random communication network topologies, estimation of the mixing coeÆcient in -contaminated mixture models, outlier discrimination and rejection, clustering and pattern recognition, robust non-parametric regression, two sample matching and image registration. Alfred Hero is with the University of Michigan at Ann Arbor, MI 48109-2122 (hero@eecs.umich.edu), and Olivier Michel is with the Ecole Normale Superieure, Lyon cedex 07, France (omichel@ens-lyon.fr). This research was supported in part by AFOSR grant F49620-97-0028. TO APPEAR IEEE TRANS ON IT 2	approximation;cluster analysis;file spanning;greedy algorithm;image registration;minimum spanning tree;mixture model;network topology;pattern recognition;random graph;rejection sampling;sieve of sundaram;steiner tree problem;telecommunications network;travelling salesman problem;utility functions on indivisible goods;weight function	Alfred O. Hero;Olivier J. J. Michel	1999	IEEE Trans. Information Theory	10.1109/18.782114	independent and identically distributed random variables;distribution;probability distribution;random graph;entropy;mathematical optimization;multivariate normal distribution;combinatorics;order statistic;image analysis;weight function;index term;perturbation;rényi entropy;entropy estimation;steiner tree problem;combinatorial optimization;image registration;graph theory;minimum spanning tree;mixture model;mathematics;cluster analysis;travelling salesman problem;asymptotic theory;nonparametric regression;tree;network topology;statistics;robustness;satisfiability;approximation theory	Theory	38.94287474549527	15.354902014677068	198570
884f39d41070a53ef26d2ace589407efee47493e	inverse optimization: towards the optimal parameter set of inverse lp with interval coefficients	parametric programming;interval optimization;inverse optimization;linear programming	Abstract We study the inverse optimization problem in the following formulation: given a family of parametrized optimization problems and a real number called demand, determine for which values of parameters the optimal value of the objective function equals to the demand. We formulate general questions and problems about the optimal parameter set and the optimal value function. Then we turn our attention to the case of linear programming, when parameters can be selected from given intervals (“inverse interval LP”). We prove that the problem is NP-hard not only in general, but even in a very special case. We inspect three special cases—the case when parameters appear in the right-hand sides, the case when parameters appear in the objective function, and the case when parameters appear in both the right-hand sides and the objective function. We design a technique based on parametric programming, which allows us to inspect the optimal parameter set. We illustrate the theory by examples.	coefficient;mathematical optimization	Michal Cerný;Milan Hladík	2016	CEJOR	10.1007/s10100-015-0402-y	stochastic programming;mathematical optimization;combinatorics;linear programming;mathematics;algorithm	ML	25.48477982942939	11.808415059773283	199167
